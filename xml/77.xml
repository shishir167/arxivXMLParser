<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:46:34Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|76001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03842</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03842</id><created>2015-04-15</created><authors><author><keyname>Bury</keyname><forenames>Marc</forenames></author></authors><title>OBDDs and (Almost) $k$-wise Independent Random Variables</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OBDD-based graph algorithms deal with the characteristic function of the edge
set E of a graph $G = (V,E)$ which is represented by an OBDD and solve
optimization problems by mainly using functional operations. We present an
OBDD-based algorithm which uses randomization for the first time. In
particular, we give a maximal matching algorithm with $O(\log^3 \vert V \vert)$
functional operations in expectation. This algorithm may be of independent
interest. The experimental evaluation shows that this algorithm outperforms
known OBDD-based algorithms for the maximal matching problem.
  In order to use randomization, we investigate the OBDD complexity of $2^n$
(almost) $k$-wise independent binary random variables. We give a OBDD
construction of size $O(n)$ for $3$-wise independent random variables and show
a lower bound of $2^{\Omega(n)}$ on the OBDD size for $k \geq 4$. The best
known lower bound was $\Omega(2^n/n)$ for $k \approx \log n$ due to Kabanets.
We also give a very simple construction of $2^n$ $(\varepsilon, k)$-wise
independent binary random variables by constructing a random OBDD of width $O(n
k^2/\varepsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03856</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03856</id><created>2015-04-15</created><updated>2015-04-15</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Priyanka</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author></authors><title>Sparse multivariate polynomial interpolation in the basis of Schubert
  polynomials</title><categories>cs.CC cs.DS math.CO</categories><comments>18 pages; some typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schubert polynomials were discovered by A. Lascoux and M. Sch\&quot;utzenberger in
the study of cohomology rings of flag manifolds in 1980's. These polynomials
form a linear basis of multivariate polynomials, and yield a natural
generalization of the classical Newton interpolation formula to the
multivariate case.
  In this paper we first show that evaluating Schubert polynomials over
nonnegative integral inputs is in $\#\mathrm{P}$. Our main result is a
deterministic algorithm that computes the expansion of a polynomial $f$ of
degree $d$ in $\mathbb{Z}[x_1, \dots, x_n]$ in the basis of Schubert
polynomials, assuming an oracle computing Schubert polynomials. This algorithm
runs in time polynomial in $n$, $d$, and the bit size of the expansion. This
generalizes the sparse interpolation algorithm of symmetric polynomials in the
Schur basis by Barvinok and Fomin (Advances in Applied Mathematics,
18(3):271--285).
  Combining the two results, we can compute the coefficients in the expansion
of products of two Schubert polynomials in the Schubert basis, of which the
Littlewood-Richardson coefficients are a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03864</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03864</id><created>2015-04-15</created><authors><author><keyname>Jecker</keyname><forenames>Isma&#xeb;l</forenames></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author></authors><title>Multi-Sequential Word Relations</title><categories>cs.FL</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rational relations are binary relations of finite words that are realised by
non-deterministic finite state transducers (NFT). A particular kind of rational
relations is the sequential functions. Sequential functions are the functions
that can be realised by input-deterministic transducers. Some rational
functions are not sequential. However, based on a property on transducers
called the twinning property, it is decidable in PTime whether a rational
function given by an NFT is sequential. In this paper, we investigate the
generalisation of this result to multi-sequential relations, i.e. relations
that are equal to a finite union of sequential functions. We show that given an
NFT, it is decidable in PTime whether the relation it defines is
multi-sequential, based on a property called the weak twinning property. If the
weak twinning property is satisfied, we give a procedure that effectively
constructs a finite set of input-deterministic transducers whose union defines
the relation. This procedure generalises to arbitrary NFT the determinisation
procedure of functional NFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03871</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03871</id><created>2015-04-15</created><updated>2015-05-03</updated><authors><author><keyname>Kheradpisheh</keyname><forenames>Saeed Reza</forenames></author><author><keyname>Ganjtabesh</keyname><forenames>Mohammad</forenames></author><author><keyname>Masquelier</keyname><forenames>Timoth&#xe9;e</forenames></author></authors><title>Bio-inspired Unsupervised Learning of Visual Features Leads to Robust
  Invariant Object Recognition</title><categories>cs.CV q-bio.NC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Retinal image of surrounding objects varies tremendously due to the changes
in position, size, pose, illumination condition, background context, occlusion,
noise, and nonrigid deformations. But despite these huge variations, our visual
system is able to invariantly recognize any object in just a fraction of a
second. To date, various computational models have been proposed to mimic the
hierarchical processing of the ventral visual pathway, with limited success.
Here, we show that combining a biologically inspired network architecture with
a biologically inspired learning rule significantly improves the models'
performance when facing challenging object recognition problems. Our model is
an asynchronous feedforward spiking neural network. When the network is
presented with natural images, the neurons in the entry layers detect edges,
and the most activated ones fire first, while neurons in higher layers are
equipped with spike timing-dependent plasticity. These neurons progressively
become selective to intermediate complexity visual features appropriate for
object categorization, as demonstrated using the 3D Object dataset provided by
Savarese et al. at CVGLab, Stanford University. The model reached 96%
categorization accuracy, which corresponds to two to three times fewer errors
than the previous state-of-the-art, demonstrating that it is able to accurately
recognize different instances of multiple object classes in various appearance
conditions (different views, scales, tilts, and backgrounds). Several
statistical analysis techniques are used to show that our model extracts class
specific and highly informative features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03872</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03872</id><created>2015-04-15</created><authors><author><keyname>Kaibel</keyname><forenames>Volker</forenames></author><author><keyname>Lee</keyname><forenames>Jon</forenames></author><author><keyname>Walter</keyname><forenames>Matthias</forenames></author><author><keyname>Weltge</keyname><forenames>Stefan</forenames></author></authors><title>Extended Formulations for Independence Polytopes of Regular Matroids</title><categories>cs.DM math.CO</categories><msc-class>52Bxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the independence polytope of every regular matroid has an
extended formulation of size quadratic in the size of its ground set. This
generalizes a similar statement for (co-)graphic matroids, which is a simple
consequence of Martin's extended formulation for the spanning-tree polytope. In
our construction, we make use of Seymour's decomposition theorem for regular
matroids. As a consequence, the extended formulations can be computed in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03874</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03874</id><created>2015-04-15</created><authors><author><keyname>Burger</keyname><forenames>Thomas</forenames></author></authors><title>Bridging belief function theory to modern machine learning</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning is a quickly evolving field which now looks really different
from what it was 15 years ago, when classification and clustering were major
issues. This document proposes several trends to explore the new questions of
modern machine learning, with the strong afterthought that the belief function
framework has a major role to play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03875</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03875</id><created>2015-04-15</created><authors><author><keyname>Roussel</keyname><forenames>K&#xe9;vin</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Song</keyname><forenames>Ye-Qiong</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Zendra</keyname><forenames>Olivier</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>RIOT OS Paves the Way for Implementation of High-Performance MAC
  Protocols</title><categories>cs.NI cs.OS cs.PF</categories><comments>SCITEPRESS. SENSORNETS 2015, Feb 2015, Angers, France.
  http://www.scitepress.org</comments><proxy>ccsd</proxy><doi>10.5220/0005237600050014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing new, high-performance MAC protocols requires real-time features,
to be able to synchronize correctly between different unrelated devices. Such
features are highly desirable for operating wireless sensor networks (WSN) that
are designed to be part of the Internet of Things (IoT). Unfortunately, the
operating systems commonly used in this domain cannot provide such features. On
the other hand, &quot;bare-metal&quot; development sacrifices portability, as well as the
mul-titasking abilities needed to develop the rich applications that are useful
in the domain of the Internet of Things. We describe in this paper how we
helped solving these issues by contributing to the development of a port of
RIOT OS on the MSP430 microcontroller, an architecture widely used in
IoT-enabled motes. RIOT OS offers rich and advanced real-time features,
especially the simultaneous use of as many hardware timers as the underlying
platform (microcontroller) can offer. We then demonstrate the effectiveness of
these features by presenting a new implementation, on RIOT OS, of S-CoSenS, an
efficient MAC protocol that uses very low processing power and energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03877</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03877</id><created>2015-04-15</created><authors><author><keyname>Lazaro</keyname><forenames>Christphe</forenames><affiliation>CITI, Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon</affiliation></author><author><keyname>M&#xe9;tayer</keyname><forenames>Daniel Le</forenames><affiliation>CITI, Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon</affiliation></author></authors><title>The control over personal data: True remedy or fairy tale ?</title><categories>cs.CY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research report undertakes an interdisciplinary review of the concept of
&quot;control&quot; (i.e. the idea that people should have greater &quot;control&quot; over their
data), proposing an analysis of this con-cept in the field of law and computer
science. Despite the omnipresence of the notion of control in the EU policy
documents, scholarly literature and in the press, the very meaning of this
concept remains surprisingly vague and under-studied in the face of
contemporary socio-technical environments and practices. Beyond the current
fashionable rhetoric of empowerment of the data subject, this report attempts
to reorient the scholarly debates towards a more comprehensive and refined
understanding of the concept of control by questioning its legal and technical
implications on data subject&#xe2;&#x80;&#x99;s agency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03878</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03878</id><created>2015-04-15</created><authors><author><keyname>Anceaume</keyname><forenames>Emmanuelle</forenames><affiliation>INRIA - SUPELEC, IRISA</affiliation></author><author><keyname>Busnel</keyname><forenames>Yann</forenames><affiliation>ENSAI, INRIA - IRISA</affiliation></author><author><keyname>Schulte-Geers</keyname><forenames>Ernst</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Sericola</keyname><forenames>Bruno</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Optimization results for a generalized coupon collector problem</title><categories>cs.DS cs.DC cs.IT math.IT math.PR</categories><comments>arXiv admin note: text overlap with arXiv:1402.5245</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper a generalized coupon collector problem, which consists
in analyzing the time needed to collect a given number of distinct coupons that
are drawn from a set of coupons with an arbitrary probability distribution. We
suppose that a special coupon called the null coupon can be drawn but never
belongs to any collection. In this context, we prove that the almost uniform
distribution, for which all the non-null coupons have the same drawing
probability, is the distribution which stochastically minimizes the time needed
to collect a fixed number of distinct coupons. Moreover, we show that in a
given closed subset of probability distributions, the distribution with all its
entries, but one, equal to the smallest possible value is the one, which
stochastically maximizes the time needed to collect a fixed number of distinct
coupons. An computer science application shows the utility of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03880</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03880</id><created>2015-04-15</created><updated>2015-12-07</updated><authors><author><keyname>Faymonville</keyname><forenames>Peter</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Parametric Linear Dynamic Logic (full version)</title><categories>cs.LO cs.FL</categories><comments>Accepted for publication at Information and Computation. A
  preliminary version of this work appeared in GandALF 2014 (arXiv:1408.5957)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Parametric Linear Dynamic Logic (PLDL), which extends Linear
Dynamic Logic (LDL) by temporal operators equipped with parameters that bound
their scope. LDL itself was proposed as an extension of Linear Temporal Logic
(LTL) that is able to express all omega-regular specifications while still
maintaining many of LTL's desirable properties like intuitive syntax and
semantics and a translation into non-deterministic B\&quot;uchi automata of
exponential size. But LDL lacks capabilities to express timing constraints. By
adding parameterized operators to LDL, we obtain a logic that is able to
express all omega-regular properties and that subsumes parameterized extensions
of LTL like Parametric LTL and PROMPT-LTL.
  Our main technical contribution is a translation of PLDL formulas into
non-deterministic B\&quot;uchi automata of exponential size via alternating
automata. This yields polynomial space algorithms for model checking and
assume-guarantee model checking and a realizability algorithm with
doubly-exponential running time. All three problems are also shown to be
complete for these complexity classes. Moreover, we give tight upper and lower
bounds on optimal parameter values for model checking and realizability. Using
these bounds, we present a polynomial space procedure for model checking
optimization and an algorithm with triply-exponential running time for
realizability optimization. Our results show that PLDL model checking and
realizability are no harder than their respective (parametric) LTL
counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03892</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03892</id><created>2015-04-15</created><authors><author><keyname>Tzelepis</keyname><forenames>Christos</forenames></author><author><keyname>Mezaris</keyname><forenames>Vasileios</forenames></author><author><keyname>Patras</keyname><forenames>Ioannis</forenames></author></authors><title>Linear Maximum Margin Classifier for Learning from Uncertain Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a maximum margin classifier that deals with
uncertainty in data input. Specifically, we reformulate the SVM framework such
that each input training entity is not solely a feature vector representation,
but a multi-dimensional Gaussian distribution with given probability density,
i.e., with a given mean and covariance matrix. The latter expresses the
uncertainty. We arrive at a convex optimization problem, which is solved in the
primal form using a gradient descent approach. The resulting classifier, which
we name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on synthetic
data, as well as on the problem of event detection in video using the
large-scale TRECVID MED 2014 dataset, and the problem of image classification
using the MNIST dataset of handwritten digits. Experimental results verify the
effectiveness of the proposed classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03899</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03899</id><created>2015-04-15</created><authors><author><keyname>Martinez-Cesena</keyname><forenames>Eduardo Alejandro</forenames></author><author><keyname>Mancarella</keyname><forenames>Pierluigi</forenames></author><author><keyname>Ndiaye</keyname><forenames>Mamadou</forenames></author><author><keyname>Schl&#xe4;pfer</keyname><forenames>Markus</forenames></author></authors><title>Using Mobile Phone Data for Electricity Infrastructure Planning</title><categories>physics.soc-ph cs.SY</categories><comments>'Data for Development' Challenge Senegal 2014-15. First Prize and
  Energy Prize</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detailed knowledge of the energy needs at relatively high spatial and
temporal resolution is crucial for the electricity infrastructure planning of a
region. However, such information is typically limited by the scarcity of data
on human activities, in particular in developing countries where
electrification of rural areas is sought. The analysis of society-wide mobile
phone records has recently proven to offer unprecedented insights into the
spatio-temporal distribution of people, but this information has never been
used to support electrification planning strategies anywhere and for rural
areas in developing countries in particular. The aim of this project is the
assessment of the contribution of mobile phone data for the development of
bottom-up energy demand models, in order to enhance energy planning studies and
existing electrification practices. More specifically, this work introduces a
framework that combines mobile phone data analysis, socioeconomic and
geo-referenced data analysis, and state-of-the-art energy infrastructure
engineering techniques to assess the techno-economic feasibility of different
centralized and decentralized electrification options for rural areas in a
developing country. Specific electrification options considered include
extensions of the existing medium voltage (MV) grid, diesel engine-based
community-level Microgrids, and individual household-level solar photovoltaic
(PV) systems. The framework and relevant methodology are demonstrated
throughout the paper using the case of Senegal and the mobile phone data made
available for the 'D4D-Senegal' innovation challenge. The results are extremely
encouraging and highlight the potential of mobile phone data to support more
efficient and economically attractive electrification plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03903</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03903</id><created>2015-04-15</created><authors><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Belmega</keyname><forenames>E. Veronica</forenames></author></authors><title>Learning to be green: robust energy efficiency maximization in dynamic
  MIMO-OFDM systems</title><categories>cs.IT cs.GT math.IT</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the maximization of energy efficiency (EE) in
next-generation multi-user MIMO-OFDM networks that evolve dynamically over time
- e.g. due to user mobility, fluctuations in the wireless medium, modulations
in the users' load, etc. Contrary to the static/stationary regime, the system
may evolve in an arbitrary manner so, targeting a fixed optimum state (either
static or in the mean) becomes obsolete; instead, users must adjust to changes
in the system &quot;on the fly&quot;, without being able to predict the state of the
system in advance. To tackle these issues, we propose a simple and distributed
online optimization policy that leads to no regret, i.e. it allows users to
match (and typically outperform) even the best fixed transmit policy in
hindsight, irrespective of how the system varies with time. Moreover, to
account for the scarcity of perfect channel state information (CSI) in massive
MIMO systems, we also study the algorithm's robustness in the presence of
measurement errors and observation noise. Importantly, the proposed policy
retains its no-regret properties under very mild assumptions on the error
statistics and, on average, it enjoys the same performance guarantees as in the
noiseless, deterministic case. Our analysis is supplemented by extensive
numerical simulations which show that, in realistic network environments, users
track their individually optimum transmit profile even under rapidly changing
channel conditions, achieving gains of up to 600% in energy efficiency over
uniform power allocation policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03912</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03912</id><created>2015-04-15</created><authors><author><keyname>Lin</keyname><forenames>Hui</forenames></author><author><keyname>Lin</keyname><forenames>Jianbiao</forenames></author><author><keyname>Ji</keyname><forenames>Ke</forenames></author><author><keyname>Wang</keyname><forenames>Jingjie</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author></authors><title>Promote the Industry Standard of Smart Home in China by Intelligent
  Router Technology</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The reason why smart home remains not popularized lies in bad product user
experience, purchasing cost, and compatibility, and a lack of industry
standard[1]. Echoing problems above, and having relentless devoted to software
and hardware innovation and practice, we have independently developed a set of
solution which is based on innovation and integration of router technology,
mobile Internet technology,Internet of things technology,communication
technology, digital-to-analog conversion and codec technology, and P2P
technology among others. We have also established relevant protocols (without
the application of protocols abroad). By doing this, we managed to establish a
system with low and moderate price, superior performance, all-inclusive
functions, easy installation, convenient portability, real-time reliability,
security encryption, and the capability to manage home furnitures in an
intelligent way. Only a new smart home system like this can inject new idea and
energy into smart home industry and thus vigorously promote the establishment
of smart home industry standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03916</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03916</id><created>2015-04-15</created><authors><author><keyname>Arikan</keyname><forenames>Erdal</forenames></author><author><keyname>Hassan</keyname><forenames>Najeeb ul</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Montorsi</keyname><forenames>Guido</forenames></author><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author></authors><title>Challenges and some new directions in channel coding</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the Journal of Communications and
  Networks, JCN copyright applies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three areas of ongoing research in channel coding are surveyed, and recent
developments are presented in each area: spatially coupled Low-Density
Parity-Check (LDPC) codes, non-binary LDPC codes, and polar coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03919</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03919</id><created>2015-04-15</created><authors><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author></authors><title>A new characterization of complete Heyting and co-Heyting algebras</title><categories>cs.LO math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new order-theoretic characterization of a complete Heyting and
co-Heyting algebra $C$. This result provides an unexpected relationship with
the field of Nash equilibria, being based on the so-called Veinott ordering
relation on subcomplete sublattices of $C$, which is crucially used in Topkis'
theorem for studying the order-theoretic stucture of Nash equilibria of
supermodular games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03923</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03923</id><created>2015-04-15</created><updated>2015-10-14</updated><authors><author><keyname>Huang</keyname><forenames>Sangxia</forenames></author></authors><title>$2^{(\log N)^{1/10-o(1)}}$ Hardness for Hypergraph Coloring</title><categories>cs.CC</categories><comments>The main theorem of Section 4 in the previous version contains a bug,
  replaced with a new construction. This gives a weaker hardness of
  2^{(logn)^{1/10}} than the 2^{(logn)^{1/4}} in the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that it is quasi-NP-hard to color 2-colorable 8-uniform hypergraphs
with $2^{(\log N)^{1/10-o(1)}}$ colors, where $N$ is the number of vertices.
There has been much focus on hardness of hypergraph coloring recently.
Guruswami, H{\aa}stad, Harsha, Srinivasan and Varma showed that it is
quasi-NP-hard to color 2-colorable 8-uniform hypergraphs with
$2^{2^{\Omega(\sqrt{\log\log N})}}$ colors. Their result is obtained by
composing standard Label Cover with an inner-verifier based on Low Degree Long
Code, using Reed-Muller code testing results by Dinur and Guruswami. Using a
different approach, Khot and Saket constructed a new variant of Label Cover,
and composed it with Quadratic Code to show quasi-NP-hardness of coloring
2-colorable 12-uniform hypergraphs with $2^{(\log N)^c}$ colors, for some $c$
around 1/20. Their construction of Label Cover is based on a new notion of
superposition complexity for CSP instances. The composition with inner-verifier
was subsequently improved by Varma, giving the same hardness result for
8-uniform hypergraphs.
  Our construction uses both Quadratic Code and Low Degree Long Code, and
builds upon the work by Khot and Saket. We present a different approach to
construct CSP instances with superposition hardness by observing that when the
number of assignments is odd, satisfying a constraint in superposition is the
same as &quot;odd-covering&quot; the constraint. We employ Low Degree Long Code in order
to keep the construction efficient. In the analysis, we also adapt and
generalize one of the key theorems by Dinur and Guruswami in the context of
analyzing probabilistically checkable proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03946</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03946</id><created>2015-04-15</created><updated>2015-04-16</updated><authors><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author><author><keyname>Sarwar</keyname><forenames>Joned</forenames></author></authors><title>An investigation of SUDOKU-inspired non-linear codes with local
  constraints</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes with local permutation constraints are described. Belief propagation
decoding is shown to require the computation of permanents, and trellis-based
methods for computing the permanents are introduced. New insights into the
asymptotic performance of such codes are presented. A universal encoder for
codes with local constraints is introduced, and simulation results for two code
structures, SUDOKU and semi-pandiagonal Latin squares, are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03947</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03947</id><created>2015-04-15</created><authors><author><keyname>Macedo</keyname><forenames>Nuno</forenames></author><author><keyname>Jorge</keyname><forenames>Tiago</forenames></author><author><keyname>Cunha</keyname><forenames>Alcino</forenames></author></authors><title>A Feature-based Classification of Model Repair Approaches</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistency management, the ability to detect, diagnose and handle
inconsistencies, is crucial during the development process in Model-driven
Engineering (MDE). As the popularity and application scenarios of MDE expanded,
a variety of different techniques were proposed to address these tasks in
specific contexts. Of the various stages of consistency management, this work
focuses on inconsistency fixing in MDE, where such task is embodied by model
repair techniques. This paper proposes a feature-based classification system
for model repair techniques, based on an systematic review of previously
proposed approaches. We expect this work to assist both the developers of novel
techniques and the MDE practitioners looking for suitable solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03957</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03957</id><created>2015-04-15</created><authors><author><keyname>Omidvar</keyname><forenames>Naeimeh</forenames></author><author><keyname>Liu</keyname><forenames>An</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Pakravan</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Optimal Hierarchical Radio Resource Management for HetNets with Flexible
  Backhaul</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (HetNets) have been considered as a promising
architecture for upcoming 5G networks due to their high energy and spectrum
efficiency. However, providing backhaul connectivity for all macro and pico
base stations (BSs) in HetNets constitutes a significant share of
infrastructure cost. Recently, the idea of flexible backhaul has drown a lot of
attention both from industry and academia. Under this architecture, not all the
pico BSs are connected to the backhaul, resulting in a significant reduction in
the infrastructure costs. In this regard, pico BSs without backhaul
connectivity need to communicate with their nearby BSs in order to have
indirect accessibility to the backhaul. This makes the radio resource
management (RRM) in such networks more complex and challenging. In this paper,
we address the problem of cross-layer RRM in HetNets with flexible backhaul. We
formulate the RRM problem as a two timescale non-convex stochastic optimization
problem which jointly optimizes flow control, routing control, interference
mitigation and link scheduling in order to maximize a generic network utility.
By exploiting the hidden convexity of this non-convex problem, we propose an
iterative algorithm which is converges to the global optimal solution. The
proposed solution has low complexity and requires low signalling and message
passing among different nodes, which makes it scalable. Moreover, due to the
proposed two-timescale design, it is robust to the backhaul signalling latency
as well. Simulation results demonstrate the significant performance gain of the
proposed solution over various baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03961</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03961</id><created>2015-04-15</created><authors><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Bahsoon</keyname><forenames>Rami</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>Online QoS Modeling in the Cloud: A Hybrid and Adaptive Multi-Learners
  Approach</title><categories>cs.DC</categories><comments>In the proceeding of the 7th IEEE/ACM International Conference on
  Utility and Cloud Computing (UCC), London, UK, 2014</comments><doi>10.1109/UCC.2014.42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the on-demand nature of cloud computing, managing cloud-based services
requires accurate modeling for the correlation between their Quality of Service
(QoS) and cloud configurations/resources. The resulted models need to cope with
the dynamic fluctuation of QoS sensitivity and interference. However, existing
QoS modeling in the cloud are limited in terms of both accuracy and
applicability due to their static and semi- dynamic nature. In this paper, we
present a fully dynamic multi- learners approach for automated and online QoS
modeling in the cloud. We contribute to a hybrid learners solution, which
improves accuracy while keeping model complexity adequate. To determine the
inputs of QoS model at runtime, we partition the inputs space into two
sub-spaces, each of which applies different symmetric uncertainty based
selection techniques, and we then combine the sub-spaces results. The learners
are also adaptive; they simultaneously allow several machine learning
algorithms to model QoS function and dynamically select the best model for
prediction on the fly. We experimentally evaluate our models using RUBiS
benchmark and realistic FIFA 98 workload. The results show that our
multi-learners approach is more accurate and effective in contrast to the other
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03965</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03965</id><created>2015-04-15</created><authors><author><keyname>Kraus</keyname><forenames>Elisabeth</forenames></author><author><keyname>Lentner</keyname><forenames>Simon D.</forenames></author></authors><title>Nash Equilibria And Partition Functions Of Games With Many Dependent
  Players</title><categories>cs.GT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss and solve a model for a game with many players, where a subset of
truely deciding players is embedded into a hierarchy of dependent agents.
  These interdependencies modify the game matrix and the Nash equilibria for
the deciding players. In a concrete example, we recognize the partition
function of the Ising model and for high dependency we observe a phase
transition to a new Nash equilibrium, which is the Pareto-efficient outcome.
  An example we have in mind is the game theory for major shareholders in a
stock market, where intermediate companies decide according to a majority vote
of their owners and compete for the final profit. In our model, these
interdependency eventually forces cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03967</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03967</id><created>2015-04-15</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Farag</keyname><forenames>Amal</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Turkbey</keyname><forenames>Evrim B.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Deep convolutional networks for pancreas segmentation in CT imaging</title><categories>cs.CV</categories><comments>SPIE Medical Imaging conference, Orlando, FL, USA: SPIE Proceedings |
  Volume 9413 | Classification</comments><journal-ref>Proc. SPIE 9413, Medical Imaging 2015: Image Processing, 94131G
  (20 March 2015)</journal-ref><doi>10.1117/12.2081420</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automatic organ segmentation is an important prerequisite for many
computer-aided diagnosis systems. The high anatomical variability of organs in
the abdomen, such as the pancreas, prevents many segmentation methods from
achieving high accuracies when compared to other segmentation of organs like
the liver, heart or kidneys. Recently, the availability of large annotated
training sets and the accessibility of affordable parallel computing resources
via GPUs have made it feasible for &quot;deep learning&quot; methods such as
convolutional networks (ConvNets) to succeed in image classification tasks.
These methods have the advantage that used classification features are trained
directly from the imaging data. We present a fully-automated bottom-up method
for pancreas segmentation in computed tomography (CT) images of the abdomen.
The method is based on hierarchical coarse-to-fine classification of local
image regions (superpixels). Superpixels are extracted from the abdominal
region using Simple Linear Iterative Clustering (SLIC). An initial probability
response map is generated, using patch-level confidences and a two-level
cascade of random forest classifiers, from which superpixel regions with
probabilities larger 0.5 are retained. These retained superpixels serve as a
highly sensitive initial input of the pancreas and its surroundings to a
ConvNet that samples a bounding box around each superpixel at different scales
(and random non-rigid deformations at training time) in order to assign a more
distinct probability of each superpixel region being pancreas or not. We
evaluate our method on CT images of 82 patients (60 for training, 2 for
validation, and 20 for testing). Using ConvNets we achieve average Dice scores
of 68%+-10% (range, 43-80%) in testing. This shows promise for accurate
pancreas segmentation, using a deep learning approach and compares favorably to
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03974</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03974</id><created>2015-04-15</created><authors><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Wireless Compressive Sensing Over Fading Channels with Distributed
  Sparse Random Projections</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of recovering a sparse signal observed by a resource
constrained wireless sensor network under channel fading. Sparse random
matrices are exploited to reduce the communication cost in forwarding
information to a fusion center. The presence of channel fading leads to
inhomogeneity and non Gaussian statistics in the effective measurement matrix
that relates the measurements collected at the fusion center and the sparse
signal being observed. We analyze the impact of channel fading on nonuniform
recovery of a given sparse signal by leveraging the properties of heavy-tailed
random matrices. We quantify the additional number of measurements required to
ensure reliable signal recovery in the presence of nonidentical fading channels
compared to that is required with identical Gaussian channels. Our analysis
provides insights into how to control the probability of sensor transmissions
at each node based on the channel fading statistics in order to minimize the
number of measurements collected at the fusion center for reliable sparse
signal recovery. We further discuss recovery guarantees of a given sparse
signal with any random projection matrix where the elements are sub-exponential
with a given sub-exponential norm. Numerical results are provided to
corroborate the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03975</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03975</id><created>2015-04-15</created><updated>2015-10-04</updated><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author></authors><title>Harnessing the Bethe free energy</title><categories>math.PR cs.DM</categories><comments>This version replaces version 1 and the RANDOM 2015 version of the
  paper, which contained critical errors affecting the main results</comments><msc-class>05C80, 82B44</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide class of problems in combinatorics, computer science and physics can
be described along the following lines. There are a large number of variables
ranging over a finite domain that interact through constraints that each bind a
few variables and either encourage or discourage certain value combinations.
Examples include the $k$-SAT problem or the Ising model. Such models naturally
induce a Gibbs measure on the set of assignments, which is characterised by its
partition function. The present paper deals with the partition function of
problems where the interactions between variables and constraints are induced
by a sparse random (hyper)graph. According to physics predictions, a generic
recipe called the &quot;replica symmetric cavity method&quot; yields the correct value of
the partition function if the underlying model enjoys certain properties
[Krzkala et al., PNAS 2007]. Guided by this conjecture, we prove general
sufficient conditions for the success of the cavity method. The proofs are
based on a &quot;regularity lemma&quot; for probability measures on sets of the form
$\Omega^n$ for a finite $\Omega$ and a large $n$ that may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03976</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03976</id><created>2015-04-15</created><updated>2016-02-08</updated><authors><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author><author><keyname>Kwasinski</keyname><forenames>Andres</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>A Survey on Applications of Model-Free Strategy Learning in Cognitive
  Wireless Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Communication Survey &amp; Tutorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-free learning has been considered as an efficient tool for designing
control mechanisms when the model of the system environment or the interaction
between the decision-making entities is not available as a-priori knowledge.
With model-free learning, the decision-making entities adapt their behaviors
based on the reinforcement from their interaction with the environment and are
able to (implicitly) build the understanding of the system through
trial-and-error mechanisms. Such characteristics of model-free learning is
highly in accordance with the requirement of cognition-based intelligence for
devices in cognitive wireless networks. Recently, model-free learning has been
considered as one key implementation approach to adaptive, self-organized
network control in cognitive wireless networks. In this paper, we provide a
comprehensive survey on the applications of the state-of-the-art model-free
learning mechanisms in cognitive wireless networks. According to the system
models that those applications are based on, a systematic overview of the
learning algorithms in the domains of single-agent system, multi-agent systems
and multi-player games is provided. Furthermore, the applications of model-free
learning to various problems in cognitive wireless networks are discussed with
the focus on how the learning mechanisms help to provide the solutions to these
problems and improve the network performance over the existing model-based,
non-adaptive methods. Finally, a broad
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03977</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03977</id><created>2015-04-15</created><updated>2015-04-20</updated><authors><author><keyname>Gustafson</keyname><forenames>Carl</forenames></author><author><keyname>Abbas</keyname><forenames>Taimoor</forenames></author><author><keyname>Bolin</keyname><forenames>David</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author></authors><title>Statistical Modeling and Estimation of Censored Pathloss Data</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 3 figures. Submitted to IEEE Wireless Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pathloss exponent and the large-scale fading variance are two parameters
that are of great importance when modeling and characterizing wireless
propagation channels. The pathloss is typically modeled using a log-distance
power law with a large-scale fading term that is log-normal. We investigate the
effect that the number of samples and the sampling distances have on the
standard errors of pathloss parameters estimated by ordinary least squares.
However, the received signal is affected by the dynamic range and noise floor
of the measurement system used to sound the channel, which can cause
measurement samples to be truncated or censored. We show that the path loss and
large scale fading estimates can be improved if the effects of censored
samples, i.e., samples below the noise floor, are taken into account in the
estimation. If the information about the censored samples are not included in
the estimation method, as in ordinary least squares estimation, it can result
in biased estimation of both the pathloss exponent and the large scale fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03985</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03985</id><created>2015-04-15</created><updated>2015-07-28</updated><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Rate Aware Instantly Decodable Network Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of reducing the delivery time of data
messages to cellular users using instantly decodable network coding (IDNC) with
physical-layer rate awareness. While most of the existing literature on IDNC
does not consider any physical layer complications and abstract the model as
equally slotted time for all users, this paper proposes a cross-layer scheme
that incorporates the different channel rates of the various users in the
decision process of both the transmitted message combinations and the rates
with which they are transmitted. The consideration of asymmetric rates for
receivers reflects more practical application scenarios and introduces a new
trade-off between the choice of coding combinations for various receivers and
the broadcasting rate for achieving shorter completion time. The completion
time minimization problem in such scenario is first shown to be intractable.
The problem is, thus, approximated by reducing, at each transmission, the
increase of an anticipated version of the completion time. The paper solves the
problem by formulating it as a maximum weight clique problem over a newly
designed rate aware IDNC (RA-IDNC) graph. The highest weight clique in the
created graph being potentially not unique, the paper further suggests a
multi-layer version of the proposed solution to improve the obtained results
from the employed completion time approximation. Simulation results indicate
that the cross-layer design largely outperforms the uncoded transmissions
strategies and the classical IDNC scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03987</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03987</id><created>2015-04-15</created><updated>2015-07-26</updated><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author></authors><title>Random Laplacian matrices and convex relaxations</title><categories>math.PR cs.DS cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The largest eigenvalue of a matrix is always larger or equal than its largest
diagonal entry. We show that for a large class of random Laplacian matrices,
this bound is essentially tight: the largest eigenvalue is, up to lower order
terms, often the size of the largest diagonal entry.
  Besides being a simple tool to obtain precise estimates on the largest
eigenvalue of a large class of random Laplacian matrices, our main result
settles a number of open problems related to the tightness of certain convex
relaxation-based algorithms. It easily implies the optimality of the
semidefinite relaxation approaches to problems such as $\mathbb{Z}_2$
Synchronization and Stochastic Block Model recovery. Interestingly, this result
readily implies the connectivity threshold for Erd\H{o}s-R\'{e}nyi graphs and
suggests that these three phenomena are manifestations of the same underlying
principle. The main tool is a recent estimate on the spectral norm of matrices
with independent entries by van Handel and the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03991</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03991</id><created>2015-04-15</created><updated>2015-07-18</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Theory of Dual-sparse Regularized Randomized Reduction</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study randomized reduction methods, which reduce
high-dimensional features into low-dimensional space by randomized methods
(e.g., random projection, random hashing), for large-scale high-dimensional
classification. Previous theoretical results on randomized reduction methods
hinge on strong assumptions about the data, e.g., low rank of the data matrix
or a large separable margin of classification, which hinder their applications
in broad domains. To address these limitations, we propose dual-sparse
regularized randomized reduction methods that introduce a sparse regularizer
into the reduced dual problem. Under a mild condition that the original dual
solution is a (nearly) sparse vector, we show that the resulting dual solution
is close to the original dual solution and concentrates on its support set. In
numerical experiments, we present an empirical study to support the analysis
and we also present a novel application of the dual-sparse regularized
randomized reduction methods to reducing the communication cost of distributed
learning from large-scale high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03995</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03995</id><created>2015-04-15</created><authors><author><keyname>Castellan</keyname><forenames>Simon</forenames></author><author><keyname>Clairambault</keyname><forenames>Pierre</forenames></author><author><keyname>Dybjer</keyname><forenames>Peter</forenames></author></authors><title>Undecidability of Equality in the Free Locally Cartesian Closed Category</title><categories>cs.LO</categories><acm-class>F.4.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a version of Martin-L\&quot;of type theory with extensional identity,
a unit type N_1, {\Sigma}, {\Pi},and a base type is a free category with
families (supporting these type formers) both in a 1- and a 2-categorical
sense. It follows that the underlying category of contexts is a free locally
cartesian closed category in a 2-categorical sense because of a previously
proved biequivalence. We then show that equality in this category is
undecidable by reducing it to the undecidability of convertibility in
combinatory logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04000</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04000</id><created>2015-04-15</created><authors><author><keyname>Sboui</keyname><forenames>Lokman</forenames></author><author><keyname>Rabah</keyname><forenames>Abdullatif</forenames></author></authors><title>Optimized UAV Communication Protocol Based on Prior Locations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we adopt a new communication protocol between the UAV and
fixed on-ground nodes. This protocol tends to reduce communication power
consumption by stopping communication if the channel is not good to communicate
(i.e. far nodes, obstacles, etc.) The communication is performed using the XBee
868M standard and Libelium wapsmotes. Our designed protocol is based on a new
communication model that we propose in this paper. The protocole decides wether
to communicate or not after computing the channel reliability through prior
RSSI measurement and nodes location data
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04003</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04003</id><created>2015-04-15</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Lee</keyname><forenames>Christopher T.</forenames></author><author><keyname>Shin</keyname><forenames>Hoo-Chang</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Kim</keyname><forenames>Lauren</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Anatomy-specific classification of medical images using deep
  convolutional nets</title><categories>cs.CV</categories><comments>Presented at: 2015 IEEE International Symposium on Biomedical
  Imaging, April 16-19, 2015, New York Marriott at Brooklyn Bridge, NY, USA</comments><journal-ref>Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium
  on Year: 2015 Pages: 101 - 104</journal-ref><doi>10.1109/ISBI.2015.7163826</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automated classification of human anatomy is an important prerequisite for
many computer-aided diagnosis systems. The spatial complexity and variability
of anatomy throughout the human body makes classification difficult. &quot;Deep
learning&quot; methods such as convolutional networks (ConvNets) outperform other
state-of-the-art methods in image classification tasks. In this work, we
present a method for organ- or body-part-specific anatomical classification of
medical images acquired using computed tomography (CT) with ConvNets. We train
a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical
classes. Key-images were mined from a hospital PACS archive, using a set of
1,675 patients. We show that a data augmentation approach can help to enrich
the data set and improve classification performance. Using ConvNets and data
augmentation, we achieve anatomy-specific classification error of 5.9 % and
area-under-the-curve (AUC) values of an average of 0.998 in testing. We
demonstrate that deep learning can be used to train very reliable and accurate
classifiers that could initialize further computer-aided diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04031</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04031</id><created>2015-04-15</created><authors><author><keyname>Arfaoui</keyname><forenames>Olfa</forenames></author><author><keyname>Hidri</keyname><forenames>Minyar Sassi</forenames></author></authors><title>Mining Semi-structured Data</title><categories>cs.DB</categories><journal-ref>The 5th International Conference on Web and Information
  Technologies (ICWIT), pp. 51-60, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for discovering knowledge from XML documents according to both
structure and content features has become challenging, due to the increase in
application contexts for which handling both structure and content information
in XML data is essential. So, the challenge is to find an hierarchical
structure which ensure a combination of data levels and their representative
structures. In this work, we will be based on the Formal Concept Analysis-based
views to index and query both content and structure. We evaluate given
structure in a querying process which allows the searching of user query
answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04044</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04044</id><created>2015-04-15</created><updated>2015-08-09</updated><authors><author><keyname>Khamis</keyname><forenames>Mahmoud Abo</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>FAQ: Questions Asked Frequently</title><categories>cs.DB cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define and study the FAQ (Functional Aggregate Query) problem, which
encompasses many frequently asked questions in constraint satisfaction,
databases, matrix operations, probabilistic graphical models and logic. This is
our main conceptual contribution.
  We then present a simple algorithm called &quot;InsideOut&quot; to solve this general
problem. &quot;InsideOut&quot; is a variation of the traditional dynamic programming
approach for constraint programming based on variable elimination. Our
variation adds a couple of simple twists to basic variable elimination in order
to deal with the generality of FAQ, as well as to take full advantage of the
(relatively) recent fractional edge cover framework of Grohe and Marx, and of
the analysis of recent worst-case optimal relational join algorithms.
  As is the case with constraint programming and graphical model inference, to
make &quot;InsideOut&quot; run efficiently we need to solve an optimization problem to
compute an appropriate variable ordering. The main technical contribution of
this work is a precise characterization of when a variable ordering is
&quot;semantically equivalent&quot; to the ordering given by the input FAQ expression.
Then, we design an approximation algorithm to find an equivalent ordering that
has the best &quot;fractional FAQ-width&quot;. Our results imply a host of known and a
few new results in graphical model inference, relational joins, and logic.
  FAQ can be thought of as a declarative language over functions. We examine
how the input and output function representations affect the tractability of
the problem. We show that the dynamic programming approach is still powerful in
some cases where the input functions are compactly represented; in particular,
we briefly explain how recent algorithms on beyond worst-case analysis for
joins and for solving SAT and #SAT can be viewed as variable elimination to
solve FAQ over compactly represented input functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04049</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04049</id><created>2015-04-15</created><updated>2015-09-23</updated><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Krejic</keyname><forenames>Natasa</forenames></author><author><keyname>Krklec-Jerinkic</keyname><forenames>Natasa</forenames></author></authors><title>Distributed Gradient Methods with Variable Number of Working Nodes</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to a journal on April 15, 2015; revised on September 23,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed optimization where $N$ nodes in a connected network
minimize the sum of their local costs subject to a common constraint set. We
propose a distributed projected gradient method where each node, at each
iteration $k$, performs an update (is active) with probability $p_k$, and stays
idle (is inactive) with probability $1-p_k$. Whenever active, each node
performs an update by weight-averaging its solution estimate with the estimates
of its active neighbors, taking a negative gradient step with respect to its
local cost, and performing a projection onto the constraint set; inactive nodes
perform no updates. Assuming that nodes' local costs are strongly convex, with
Lipschitz continuous gradients, we show that, as long as activation probability
$p_k$ grows to one asymptotically, our algorithm converges in the mean square
sense (MSS) to the same solution as the standard distributed gradient method,
i.e., as if all the nodes were active at all iterations. Moreover, when $p_k$
grows to one linearly, with an appropriately set convergence factor, the
algorithm has a linear MSS convergence, with practically the same factor as the
standard distributed gradient method. Simulations on both synthetic and real
world data sets demonstrate that, when compared with the standard distributed
gradient method, the proposed algorithm significantly reduces the overall
number of per-node communications and per-node gradient evaluations
(computational cost) for the same required accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04054</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04054</id><created>2015-04-15</created><authors><author><keyname>Pu</keyname><forenames>Yunchen</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>A Generative Model for Deep Convolutional Learning</title><categories>stat.ML cs.LG cs.NE</categories><comments>3 pages, 1 figure, ICLR workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generative model is developed for deep (multi-layered) convolutional
dictionary learning. A novel probabilistic pooling operation is integrated into
the deep model, yielding efficient bottom-up (pretraining) and top-down
(refinement) probabilistic learning. Experimental results demonstrate powerful
capabilities of the model to learn multi-layer features from images, and
excellent classification results are obtained on the MNIST and Caltech 101
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04056</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04056</id><created>2015-04-15</created><authors><author><keyname>Khasanvis</keyname><forenames>Santosh</forenames></author><author><keyname>Li</keyname><forenames>Mingyu</forenames></author><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author><author><keyname>Fashami</keyname><forenames>Mohammad Salehi</forenames></author><author><keyname>Biswas</keyname><forenames>Ayan K.</forenames></author><author><keyname>Atulasimha</keyname><forenames>Jayasimha</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Supriyo</forenames></author><author><keyname>Moritz</keyname><forenames>Csaba Andras</forenames></author></authors><title>Self-similar Magneto-electric Nanocircuit Technology for Probabilistic
  Inference Engines</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models are powerful mathematical formalisms for
machine learning and reasoning under uncertainty that are widely used for
cognitive computing. However they cannot be employed efficiently for large
problems (with variables in the order of 100K or larger) on conventional
systems, due to inefficiencies resulting from layers of abstraction and
separation of logic and memory in CMOS implementations. In this paper, we
present a magneto-electric probabilistic technology framework for implementing
probabilistic reasoning functions. The technology leverages Straintronic
Magneto-Tunneling Junction (S-MTJ) devices in a novel mixed-signal circuit
framework for direct computations on probabilities while enabling in-memory
computations with persistence. Initial evaluations of the Bayesian likelihood
estimation operation occurring during Bayesian Network inference indicate up to
127x lower area, 214x lower active power, and 70x lower latency compared to an
equivalent 45nm CMOS Boolean implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04061</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04061</id><created>2015-04-15</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author></authors><title>Synchronization over $Z_2$ and community detection in multiplex signed
  networks with constraints</title><categories>cs.SI math.OC physics.soc-ph</categories><comments>28 pages in Journal of Complex Networks, 2015</comments><doi>10.1093/comnet/cnu050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding group elements from noisy measurements of their pairwise ratios is
also known as the group synchronization problem, first introduced in the
context of the group SO(2) of planar rotations, whose usefulness has been
demonstrated recently in engineering and structural biology. Here, we focus on
synchronization over $\mathbb{Z}_2$, and consider the problem of identifying
communities in a multiplex network when the interaction between the nodes is
described by a signed (possibly weighted) measure of similarity, and when the
multiplex network has a natural partition into two communities. When one has
the additional information that certain subsets of nodes represent the same
unknown group element, we consider and compare several algorithms based on
spectral, semidefinite programming (SDP) and message passing algorithms. In
other words, all nodes within such a subset represent the same unknown group
element, and one has available noisy pairwise measurements between pairs of
nodes that belong to different non-overlapping subsets. Following a recent
analysis of the eigenvector method for synchronization over SO(2), we analyze
the robustness to noise of the eigenvector method for synchronization over
$\mathbb{Z}_2$, when the underlying graph of pairwise measurements is the
Erd\H{o}s-R\'{e}nyi random graph, using results from the random matrix theory
literature. We also propose a message passing synchronization algorithm that
outperforms the existing eigenvector synchronization algorithm only for certain
classes of graphs and noise models, and enjoys the flexibility of incorporating
constraints that may not be easily accommodated by other spectral or SDP-based
methods. We apply our methods both to several synthetic models and a real data
set of roll call voting patterns in the U.S. Congress across time, to identify
the two existing communities, i.e., the Democratic and Republican parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04070</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04070</id><created>2015-04-15</created><authors><author><keyname>Zaman</keyname><forenames>Nabil</forenames></author><author><keyname>Pippenger</keyname><forenames>Nicholas</forenames></author></authors><title>Asymptotic Analysis of Run-Length Encoding</title><categories>cs.IT math.IT</categories><comments>i+5 pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gallager and Van Voorhis have found optimal prefix-free codes $\kappa(K)$ for
a random variable $K$ that is geometrically distributed: $\Pr[K=k] = p(1-p)^k$
for $k\ge 0$. We determine the asymptotic behavior of the expected length ${\rm
Ex}[{\#\kappa(K)}]$ of these codes as $p\to 0$: $${\rm Ex}[{\#\kappa(K)}] =
\log_2 {1\over p} + \log_2 \log 2 + 2 + f\left(\log_2 {1\over p} + \log_2 \log
2\right) + O(p),$$ where $$f(z) = 4\cdot 2^{-2^{1-\{z\}}} - \{z\} - 1,$$ and
$\{z\} = z - \lfloor z\rfloor$ is the fractional part of $z$. The function
$f(z)$ is a periodic function (with period $1$) that exhibits small
oscillations (with magnitude less than $0.005$) about an even smaller average
value (less than $0.0005$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04073</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04073</id><created>2015-04-15</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>The Parametric Closure Problem</title><categories>cs.DS</categories><comments>12 pages, with a 5-page appendix and 3 figures. Extended version of a
  paper to appear at the 14th Algorithms and Data Structures Symposium (WADS),
  Victoria, BC, August 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the parametric closure problem, in which the input is a partially
ordered set whose elements have linearly varying weights and the goal is to
compute the sequence of minimum-weight lower sets of the partial order as the
weights vary. We give polynomial time solutions to many important special cases
of this problem including semiorders, reachability orders of bounded-treewidth
graphs, partial orders of bounded width, and series-parallel partial orders.
Our result for series-parallel orders provides a significant generalization of
a previous result of Carlson and Eppstein on bicriterion subtree problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04074</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04074</id><created>2015-04-15</created><authors><author><keyname>Wei</keyname><forenames>Xiaohan</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Power Aware Wireless File Downloading: A Lyapunov Indexing Approach to A
  Constrained Restless Bandit Problem</title><categories>cs.PF</categories><comments>Extended version submitted to IEEE Trans. Network. arXiv admin note:
  substantial text overlap with arXiv:1401.3824</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper treats power-aware throughput maxi-mization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy. In a special case with no
power-constraint, the multi-user heuristic is shown to be throughput optimal.
Simulations are used to demonstrate effectiveness of the heuristic in the
general case. For simple cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04076</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04076</id><created>2015-04-15</created><authors><author><keyname>Duan</keyname><forenames>Qiang</forenames></author><author><keyname>Wang</keyname><forenames>Chonggang</forenames></author><author><keyname>Li</keyname><forenames>Xiaolin</forenames></author></authors><title>End-to-End Service Delivery with QoS Guarantee in Software Defined
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Network (SDN) is expected to have a significant impact on
future networking. Although exciting progress has been made toward realizing
SDN, application of this new networking paradigm in the future Internet to
support end-to-end QoS provisioning faces some new challenges. The autonomous
network domains coexisting in the Internet and the diverse user applications
deployed upon the Internet call for a uniform Service Delivery Platform (SDP)
that enables high-level network abstraction and inter-domain collaboration for
end-to-end service provisioning. However, the currently available SDN
technologies lack effective mechanisms for supporting such a platform. In this
paper, we first present a SDP framework that applies the Network-as-a-Service
(NaaS) principle to provide network abstraction and orchestration for
end-to-end service provisioning in SDN-based future Internet. Then we focus our
study on two enabling technologies for such a SDP to achieve QoS guarantee;
namely a network abstraction model and an end-to-end resource allocation
scheme. Specifically we propose a general model for abstracting the service
capabilities offered by network domains and develop a technique for determining
the required amounts of bandwidth in network domains for end-to-end service
delivery with QoS guarantee. Both the analytical and numerical results obtained
in this paper indicate that the NaaS-based SDP not only simplifies SDN service
and resource management but also enhances bandwidth utilization for end-to-end
QoS provisioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04080</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04080</id><created>2015-04-15</created><authors><author><keyname>Hong</keyname><forenames>Y. -W. Peter</forenames></author><author><keyname>Li</keyname><forenames>Wei-Chiang</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Lee</keyname><forenames>Chia-Han</forenames></author></authors><title>Coordinated Multicasting with Opportunistic User Selection in Multicell
  Wireless Systems</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer multicasting with opportunistic user selection (OUS) is
examined for multicell multi-antenna wireless systems. By adopting a two-layer
encoding scheme, a rate-adaptive channel code is applied in each fading block
to enable successful decoding by a chosen subset of users (which varies over
different blocks) and an application layer erasure code is employed across
multiple blocks to ensure that every user is able to recover the message after
decoding successfully in a sufficient number of blocks. The transmit signal and
code-rate in each block determine opportunistically the subset of users that
are able to successfully decode and can be chosen to maximize the long-term
multicast efficiency. The employment of OUS not only helps avoid
rate-limitations caused by the user with the worst channel, but also helps
coordinate interference among different cells and multicast groups. In this
work, efficient algorithms are proposed for the design of the transmit
covariance matrices, the physical layer code-rates, and the target user subsets
in each block. In the single group scenario, the system parameters are
determined by maximizing the group-rate, defined as the physical layer
code-rate times the fraction of users that can successfully decode in each
block. In the multi-group scenario, the system parameters are determined by
considering a group-rate balancing optimization problem, which is solved by a
successive convex approximation (SCA) approach. To further reduce the feedback
overhead, we also consider the case where only part of the users feed back
their channel vectors in each block and propose a design based on the balancing
of the expected group-rates. In addition to SCA, a sample average approximation
technique is also introduced to handle the probabilistic terms arising in this
problem. The effectiveness of the proposed schemes is demonstrated by computer
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04085</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04085</id><created>2015-04-15</created><authors><author><keyname>Chen</keyname><forenames>Huaijin</forenames></author><author><keyname>Asif</keyname><forenames>M. Salman</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author></authors><title>FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave
  Infrared</title><categories>cs.CV</categories><comments>appears in IEEE Conf. Computer Vision and Pattern Recognition, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cameras for imaging in short and mid-wave infrared spectra are significantly
more expensive than their counterparts in visible imaging. As a result,
high-resolution imaging in those spectrum remains beyond the reach of most
consumers. Over the last decade, compressive sensing (CS) has emerged as a
potential means to realize inexpensive short-wave infrared cameras. One
approach for doing this is the single-pixel camera (SPC) where a single
detector acquires coded measurements of a high-resolution image. A
computational reconstruction algorithm is then used to recover the image from
these coded measurements. Unfortunately, the measurement rate of a SPC is
insufficient to enable imaging at high spatial and temporal resolutions.
  We present a focal plane array-based compressive sensing (FPA-CS)
architecture that achieves high spatial and temporal resolutions. The idea is
to use an array of SPCs that sense in parallel to increase the measurement
rate, and consequently, the achievable spatio-temporal resolution of the
camera. We develop a proof-of-concept prototype in the short-wave infrared
using a sensor with 64$\times$ 64 pixels; the prototype provides a 4096$\times$
increase in the measurement rate compared to the SPC and achieves a megapixel
resolution at video rate using CS techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04090</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04090</id><created>2015-04-15</created><authors><author><keyname>Tierney</keyname><forenames>Stephen</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author></authors><title>Segmentation of Subspaces in Sequential Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Ordered Subspace Clustering (OSC) to segment data drawn from a
sequentially ordered union of subspaces. Similar to Sparse Subspace Clustering
(SSC) we formulate the problem as one of finding a sparse representation but
include an additional penalty term to take care of sequential data. We test our
method on data drawn from infrared hyper spectral, video and motion capture
data. Experiments show that our method, OSC, outperforms the state of the art
methods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR)
and SSC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04092</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04092</id><created>2015-04-15</created><updated>2015-06-06</updated><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>One-Shot Mutual Covering Lemma and Marton's Inner Bound with a Common
  Message</title><categories>cs.IT math.IT</categories><comments>6 pages; extended version of ISIT paper</comments><journal-ref>2015 IEEE International Symposium on Information Theory (ISIT),
  14-19 June 2015, pages: 1457 - 1461, Hong Kong</journal-ref><doi>10.1109/ISIT.2015.7282697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By developing one-shot mutual covering lemmas, we derive a one-shot
achievability bound for broadcast with a common message which recovers Marton's
inner bound (with three auxiliary random variables) in the i.i.d.~case. The
encoder employed is deterministic. Relationship between the mutual covering
lemma and a new type of channel resolvability problem is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04103</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04103</id><created>2015-04-16</created><authors><author><keyname>Falahatgar</keyname><forenames>Moein</forenames></author><author><keyname>Jafarpour</keyname><forenames>Ashkan</forenames></author><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Pichapathi</keyname><forenames>Venkatadheeraj</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Faster Algorithms for Testing under Conditional Sampling</title><categories>cs.DS cs.CC cs.LG math.ST stat.TH</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been considerable recent interest in distribution-tests whose
run-time and sample requirements are sublinear in the domain-size $k$. We study
two of the most important tests under the conditional-sampling model where each
query specifies a subset $S$ of the domain, and the response is a sample drawn
from $S$ according to the underlying distribution.
  For identity testing, which asks whether the underlying distribution equals a
specific given distribution or $\epsilon$-differs from it, we reduce the known
time and sample complexities from $\tilde{\mathcal{O}}(\epsilon^{-4})$ to
$\tilde{\mathcal{O}}(\epsilon^{-2})$, thereby matching the information
theoretic lower bound. For closeness testing, which asks whether two
distributions underlying observed data sets are equal or different, we reduce
existing complexity from $\tilde{\mathcal{O}}(\epsilon^{-4} \log^5 k)$ to an
even sub-logarithmic $\tilde{\mathcal{O}}(\epsilon^{-5} \log \log k)$ thus
providing a better bound to an open problem in Bertinoro Workshop on Sublinear
Algorithms [Fisher, 2004].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04104</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04104</id><created>2015-04-16</created><authors><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author><author><keyname>Joshi</keyname><forenames>Shiv Dutt</forenames></author><author><keyname>Patney</keyname><forenames>Rakesh Kumar</forenames></author><author><keyname>Saha</keyname><forenames>Kaushik</forenames></author></authors><title>The Hilbert spectrum and the Energy Preserving Empirical Mode
  Decomposition</title><categories>cs.IT math.IT math.NA</categories><comments>23 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose algorithms which preserve energy in empirical mode
decomposition (EMD), generating finite $n$ number of band limited Intrinsic
Mode Functions (IMFs). In the first energy preserving EMD (EPEMD) algorithm, a
signal is decomposed into linearly independent (LI), non orthogonal yet energy
preserving (LINOEP) IMFs and residue (EPIMFs). It is shown that a vector in an
inner product space can be represented as a sum of LI and non orthogonal
vectors in such a way that Parseval's type property is satisfied. From the set
of $n$ IMFs, through Gram-Schmidt orthogonalization method (GSOM), $n!$ set of
orthogonal functions can be obtained. In the second algorithm, we show that if
the orthogonalization process proceeds from lowest frequency IMF to highest
frequency IMF, then the GSOM yields functions which preserve the properties of
IMFs and the energy of a signal. With the Hilbert transform, these IMFs yield
instantaneous frequencies and amplitudes as functions of time that reveal the
imbedded structures of a signal. The instantaneous frequencies and square of
amplitudes as functions of time produce a time-frequency-energy distribution,
referred as the Hilbert spectrum, of a signal. Simulations have been carried
out for the analysis of various time series and real life signals to show
comparison among IMFs produced by EMD, EPEMD, ensemble EMD and multivariate EMD
algorithms. Simulation results demonstrate the power of this proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04111</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04111</id><created>2015-04-16</created><authors><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Kelebek</keyname><forenames>Ismail G.</forenames></author></authors><title>The homogeneous weight for $R_k$, related Gray map and new binary
  quasicyclic codes</title><categories>cs.IT math.IT</categories><comments>Submitted to be published</comments><msc-class>94B15, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using theoretical results about the homogeneous weights for Frobenius rings,
we describe the homogeneous weight for the ring family $R_k$, a recently
introduced family of Frobenius rings which have been used extensively in coding
theory. We find an associated Gray map for the homogeneous weight using first
order Reed-Muller codes and we describe some of the general properties of the
images of codes over $R_k$ under this Gray map. We then discuss quasitwisted
codes over $R_k$ and their binary images under the homogeneous Gray map. In
this way, we find many optimal binary codes which are self-orthogonal and
quasicyclic. In particular, we find a substantial number of optimal binary
codes that are quasicyclic of index 8, 16 and 24, nearly all of which are new
additions to the database of quasicyclic codes kept by Chen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04113</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04113</id><created>2015-04-16</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author></authors><title>On the Performance of the Relay-ARQ Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Trans. Veh. Technol. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of relay networks in the presence of
hybrid automatic repeat request (ARQ) feedback and adaptive power allocation.
The throughput and the outage probability of different hybrid ARQ protocols are
studied for independent and spatially-correlated fading channels. The results
are obtained for the cases where there is a sum power constraint on the source
and the relay or when each of the source and the relay are power-limited
individually. With adaptive power allocation, the results demonstrate the
efficiency of relay-ARQ techniques in different conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04114</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04114</id><created>2015-04-16</created><authors><author><keyname>Levine</keyname><forenames>Nir</forenames></author><author><keyname>Mann</keyname><forenames>Timothy A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Actively Learning to Attract Followers on Twitter</title><categories>stat.ML cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter, a popular social network, presents great opportunities for on-line
machine learning research. However, previous research has focused almost
entirely on learning from passively collected data. We study the problem of
learning to acquire followers through normative user behavior, as opposed to
the mass following policies applied by many bots. We formalize the problem as a
contextual bandit problem, in which we consider retweeting content to be the
action chosen and each tweet (content) is accompanied by context. We design
reward signals based on the change in followers. The result of our month long
experiment with 60 agents suggests that (1) aggregating experience across
agents can adversely impact prediction accuracy and (2) the Twitter community's
response to different actions is non-stationary. Our findings suggest that
actively learning on-line can provide deeper insights about how to attract
followers than machine learning over passively collected data alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04115</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04115</id><created>2015-04-16</created><updated>2015-05-29</updated><authors><author><keyname>Gajarsk&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>FO Model Checking on Posets of Bounded Width</title><categories>cs.LO cs.DM</categories><comments>Minor correction, p.5, def. of \tau_{s+1}: instead of an induced
  subdigraph of D_s, we use the appropriate relational structure formed from
  this subdigrph</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades the main focus of research into first-order (FO)
model checking algorithms have been sparse relational structures-culminating in
the FPT-algorithm by Grohe, Kreutzer and Siebertz for FO model checking of
nowhere dense classes of graphs [STOC'14], with dense structures starting to
attract attention only recently. Bova, Ganian and Szeider [LICS'14] initiated
the study of the complexity of FO model checking on partially ordered sets
(posets). Bova, Ganian and Szeider showed that model checking existential FO
logic is fixed-parameter tractable (FPT) on posets of bounded width, where the
width of a poset is the size of the largest antichain in the poset. The
existence of an FPT algorithm for general FO model checking on posets of
bounded width, however, remained open. We resolve this question in the positive
by giving an algorithm that takes as its input an $n$-element poset $\cal P$ of
width $w$ and an FO logic formula $\phi$, and determines whether $\phi$ holds
on $\cal P$ in time $f(\phi,w)\cdot n^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04116</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04116</id><created>2015-04-16</created><authors><author><keyname>Afrin</keyname><forenames>Farzana</forenames></author><author><keyname>Rahaman</keyname><forenames>Mohammad Saiedur</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Saidur</forenames></author><author><keyname>Rahman</keyname><forenames>Mashiour</forenames></author></authors><title>Student Satisfaction mining in a typical core course of Computer Science</title><categories>cs.CY</categories><comments>AIUB Journal of Science and Engineering (AJSE) Vol.11 No.1 (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Students' satisfaction plays a vital role in success of an educational
institute. Hence, many educational institutes continuously improve their
service to produce a supportive learning environment to satisfy the student
need. For this reason, educational institutions collect student satisfaction
data to make decision about institutional quality, but till now it cannot be
determined because student satisfaction is a complex matter which is influenced
by variety of characteristics of students and institutions. There are many
studies have been performed to inspect student satisfaction in the form of
college services, programs, student accommodation facility, student-faculty
interaction, consulting hours etc. So, still we cannot have a standard method
to know what is going on about satisfaction in the case of a core course. In
this research we determined the attributes that heavily affect student
satisfaction in a core course of computer science and the current status of
other attributes as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04122</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04122</id><created>2015-04-16</created><authors><author><keyname>Battistelli</keyname><forenames>G.</forenames></author><author><keyname>Tesi</keyname><forenames>P.</forenames></author></authors><title>Detecting Topology Variations in Dynamical Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of detecting topology variations in
dynamical networks. We consider a network whose behavior can be represented via
a linear dynamical system. The problem of interest is then that of finding
conditions under which it is possible to detect node or link disconnections
from prior knowledge of the nominal network behavior and on-line measurements.
The considered approach makes use of analysis tools from switching systems
theory. A number of results are presented along with examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04123</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04123</id><created>2015-04-16</created><authors><author><keyname>Battistelli</keyname><forenames>G.</forenames></author><author><keyname>Tesi</keyname><forenames>P.</forenames></author></authors><title>Switching Control for Parameter Identifiability of Uncertain Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of identifying the parameters of an
uncertain linear system by means of feedback control. The problem is approached
by considering time-varying controllers. It is shown that even when the
uncertainty set is not finite, parameter identifiability can be generically
ensured by switching among a finite number of linear time-invariant
controllers. The results are shown to have several implications, ranging from
fault detection and isolation to adaptive and supervisory control. Practical
aspects of the problem are also discussed in details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04133</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04133</id><created>2015-04-16</created><authors><author><keyname>Li</keyname><forenames>Liping</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Hu</keyname><forenames>Yanjun</forenames></author></authors><title>On the Error Performance of Systematic Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematic polar codes are shown to outperform non-systematic polar codes in
terms of the bit-error-rate (BER) performance. However theoretically the
mechanism behind the better performance of systematic polar codes is not yet
clear. In this paper, we set the theoretical framework to analyze the
performance of systematic polar codes. The exact evaluation of the BER of
systematic polar codes conditioned on the BER of non-systematic polar codes
involves in $2^{NR}$ terms where $N$ is the code block length and $R$ is the
code rate, resulting in a prohibitive number of computations for large block
lengths. By analyzing the polar code construction and the
successive-cancellation (SC) decoding process, we use a statistical model to
quantify the advantage of systematic polar codes over non-systematic polar
codes, so called the systematic gain in this paper. A composite model is
proposed to approximate the dominant error cases in the SC decoding process.
This composite model divides the errors into independent regions and coupled
regions, controlled by a coupling coefficient. Based on this model, the
systematic gain can be conveniently calculated. Numerical simulations are
provided in the paper showing very close approximations of the proposed model
in quantifying the systematic gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04136</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04136</id><created>2015-04-16</created><updated>2015-06-21</updated><authors><author><keyname>Fijalkow</keyname><forenames>Nathana&#xeb;l</forenames><affiliation>LIAFA, Universit&#xe9; Paris 7 - Denis Diderot and University of Warsaw</affiliation></author><author><keyname>Gimbert</keyname><forenames>Hugo</forenames><affiliation>LaBRI, CNRS, Bordeaux</affiliation></author><author><keyname>Kelmendi</keyname><forenames>Edon</forenames><affiliation>LaBRI, Bordeaux</affiliation></author><author><keyname>Oualhadj</keyname><forenames>Youssouf</forenames><affiliation>LIF, Marseille</affiliation></author></authors><title>Deciding the value 1 problem for probabilistic leaktight automata</title><categories>cs.FL</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:12) 2015</journal-ref><doi>10.2168/LMCS-11(2:12)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The value 1 problem is a decision problem for probabilistic automata over
finite words: given a probabilistic automaton, are there words accepted with
probability arbitrarily close to 1? This problem was proved undecidable
recently; to overcome this, several classes of probabilistic automata of
different nature were proposed, for which the value 1 problem has been shown
decidable. In this paper, we introduce yet another class of probabilistic
automata, called leaktight automata, which strictly subsumes all classes of
probabilistic automata whose value 1 problem is known to be decidable. We prove
that for leaktight automata, the value 1 problem is decidable (in fact,
PSPACE-complete) by constructing a saturation algorithm based on the
computation of a monoid abstracting the behaviours of the automaton. We rely on
algebraic techniques developed by Simon to prove that this abstraction is
complete. Furthermore, we adapt this saturation algorithm to decide whether an
automaton is leaktight. Finally, we show a reduction allowing to extend our
decidability results from finite words to infinite ones, implying that the
value 1 problem for probabilistic leaktight parity automata is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04137</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04137</id><created>2015-04-16</created><authors><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Olmos</keyname><forenames>Pablo M.</forenames></author></authors><title>On Distributed Storage Allocations for Memory-Limited Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE GLOBECOM'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider distributed allocation problems with memory
constraint limits. Firstly, we propose a tractable relaxation to the problem of
optimal symmetric allocations from [1]. The approximated problem is based on
the Q-error function, and its solution approaches the solution of the initial
problem, as the number of storage nodes in the network grows. Secondly,
exploiting this relaxation, we are able to formulate and to solve the problem
for storage allocations for memory-limited DSS storing and arbitrary memory
profiles. Finally, we discuss the extension to the case of multiple data
objects, stored in the DSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04152</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04152</id><created>2015-04-16</created><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Interaction Graphs: Full Linear Logic</title><categories>cs.LO math.LO</categories><msc-class>03B70, 03F52, 28E15</msc-class><acm-class>F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction graphs were introduced as a general, uniform, construction of
dynamic models of linear logic, encompassing all &quot;Geometry of Interaction&quot;
(GoI) constructions introduced so far. This series of work was inspired from
Girard's hyperfinite GoI, and develops a quantitative approach that should be
understood as a dynamic version of weighted relational models. Until now, the
interaction graphs framework has been shown to deal with exponentials for the
constrained system ELL (Elementary Linear Logic) while keeping its quantitative
aspect. Adapting older constructions by Girard, one can clearly define &quot;full&quot;
exponentials, but at the cost of these quantitative features. We show here that
allowing interpretations of proofs to use continuous (yet finite in a
measure-theoretic sense) sets of states, as opposed to earlier Interaction
Graphs constructions were these sets of states were discrete (and finite),
provides a model for full linear logic with second order quantification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04167</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04167</id><created>2015-04-16</created><updated>2015-07-03</updated><authors><author><keyname>Atzeni</keyname><forenames>Italo</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Full-Duplex MIMO Small-Cell Networks: Performance Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex small-cell relays with multiple antennas constitute a core
element of the envisioned 5G network architecture. In this paper, we use
stochastic geometry to analyze the performance of wireless networks with
full-duplex multiple-antenna small cells, with particular emphasis on the
probability of successful transmission. To achieve this goal, we additionally
characterize the distribution of the self-interference power of the full-duplex
nodes. The proposed framework reveals useful insights on the benefits of
full-duplex with respect to half-duplex in terms of network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04169</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04169</id><created>2015-04-16</created><authors><author><keyname>Parter</keyname><forenames>Merav</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Fault Tolerant BFS Structures: A Reinforcement-Backup Tradeoff</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates the study of fault resilient network structures that mix
two orthogonal protection mechanisms: (a) {\em backup}, namely, augmenting the
structure with many (redundant) low-cost but fault-prone components, and (b)
{\em reinforcement}, namely, acquiring high-cost but fault-resistant
components. To study the trade-off between these two mechanisms in a concrete
setting, we address the problem of designing a $(b,r)$ {\em fault-tolerant} BFS
(or $(b,r)$ FT-BFS for short) structure, namely, a subgraph $H$ of the network
$G$ consisting of two types of edges: a set $E' \subseteq E$ of $r(n)$
fault-resistant {\em reinforcement} edges, which are assumed to never fail, and
a (larger) set $E(H) \setminus E'$ of $b(n)$ fault-prone {\em backup} edges,
such that subsequent to the failure of a single fault-prone backup edge $e \in
E \setminus E'$, the surviving part of $H$ still contains an BFS spanning tree
for (the surviving part of) $G$, satisfying $dist(s,v,H\setminus \{e\}) \leq
dist(s,v,G\setminus \{e\})$ for every $v \in V$ and $e \in E \setminus E'$. We
establish the following tradeoff between $b(n)$ and $r(n)$: For every real
$\epsilon \in (0,1]$, if $r(n) = {\tilde\Theta}(n^{1-\epsilon})$, then $b(n) =
{\tilde\Theta}(n^{1+\epsilon})$ is necessary and sufficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04171</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04171</id><created>2015-04-13</created><updated>2015-05-20</updated><authors><author><keyname>Hu</keyname><forenames>Chuangqiang</forenames></author><author><keyname>Zhao</keyname><forenames>Chang-An</forenames></author></authors><title>Multi-point Codes from Generalized Hermitian Curves</title><categories>cs.IT math.AG math.IT</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate multi-point algebraic geometric codes defined from curves
related to the generalized Hermitian curve introduced by Alp Bassa, Peter
Beelen, Arnaldo Garcia, and Henning Stichtenoth. Our main result is to find a
basis of the Riemann-Roch space of a series of divisors, which can be used to
construct multi-point codes explicitly. These codes turn out to have nice
properties similar to those of Hermitian codes, for example, they are easy to
describe, to encode and decode. It is shown that the duals are also such codes
and an explicit formula is given. In particular, this formula enables one to
calculate the parameters of these codes. Finally, we apply our results to
obtain linear codes attaining new records on the parameters. A new
record-giving $ [234,141,\geqslant 59] $-code over $ \mathbb{F}_{27} $ is
presented as one of the examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04174</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04174</id><created>2015-04-15</created><authors><author><keyname>Amaral</keyname><forenames>Ana Rita</forenames></author><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio Rodrigues</forenames></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames></author></authors><title>A parametric study on window-to-floor ratio of double window glazing and
  its shadowing using dynamic simulation</title><categories>cs.OH</categories><comments>7 pages, 2 figures, 3rd LAETA Young Researchers Meeting - 3EJIL,
  Coimbra, 7-8 May, 2015. arXiv admin note: text overlap with arXiv:1503.07016</comments><msc-class>68U20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When incorrectly designed, windows can be responsible for unnecessary energy
consumption in a building. This may result from its dimensions, orientation and
shadowing. In a moderate climate like the Portuguese, and considering an annual
thermal comfort assessment of a space, if windows are under-dimensioned or
over-shadowed, they can contribute to the increase of heating needs. However,
when over-dimensioned or under-shadowed, they contribute to the increase of
cooling requirements. Therefore, it is important to find the optimum design
that balances orientation, dimension and shadowing, contributing to minimize
both the heating and cooling needs. This study presents a parametric analysis
of a double glazing window in its orientation and dimension, located in the
Portuguese city of Coimbra. For each window orientation and dimension, the
optimum overhang depth is determined. The objective is to minimize degree-hours
of thermal discomfort. Results show that overhangs are mainly a corrective
mechanism to over-dimensioned openings, thus allowing that building
practitioners may choose a wider range of windows dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04179</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04179</id><created>2015-04-16</created><authors><author><keyname>Vabishchevich</keyname><forenames>P. N.</forenames></author></authors><title>Factorized schemes of second-order accuracy for numerical solving
  unsteady problems</title><categories>cs.NA math.NA</categories><comments>18 pages, 9 figures</comments><msc-class>65J08, 65M06, 65M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schemes with the second-order approximation in time are considered for
numerical solving the Cauchy problem for an evolutionary equation of first
order with a self-adjoint operator. The implicit two-level scheme based on the
Pad\'{e} polynomial approximation is unconditionally stable. It demonstrates
good asymptotic properties in time and provides an adequate evolution in time
for individual harmonics of the solution (has spectral mimetic stability). In
fact, the only drawback of this scheme is the necessity to solve an equation
with an operator polynomial of second degree at each time level. We consider
modifications of these schemes, which are based on solving equations with
operator polynomials of first degree. Such computational implementations occur,
for example, if we apply the fully implicit two-level scheme (the backward
Euler scheme). A three-level modification of the SM-stable scheme is proposed.
Its unconditional stability is established in the corresponding norms. The
emphasis is on the scheme, where the numerical algorithm involves two stages,
namely, the backward Euler scheme of first order at the first (prediction)
stage and the following correction of the approximate solution using a
factorized operator. The SM-stability is established for the proposed scheme.
To illustrate the theoretical results of the work, a model problem is solved
numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04181</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04181</id><created>2015-04-16</created><authors><author><keyname>Glasser</keyname><forenames>Christian</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>Constraint Satisfaction Problems around Skolem Arithmetic</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study interactions between Skolem Arithmetic and certain classes of
Constraint Satisfaction Problems (CSPs). We revisit results of Glass er et al.
in the context of CSPs and settle the major open question from that paper,
finding a certain satisfaction problem on circuits to be decidable. This we
prove using the decidability of Skolem Arithmetic. We continue by studying
first-order expansions of Skolem Arithmetic without constants, (N;*), where *
indicates multiplication, as CSPs. We find already here a rich landscape of
problems with non-trivial instances that are in P as well as those that are
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04184</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04184</id><created>2015-04-16</created><authors><author><keyname>Ollila</keyname><forenames>Esa</forenames></author></authors><title>Multichannel sparse recovery of complex-valued signals using Huber's
  criterion</title><categories>cs.IT math.IT stat.CO stat.ML</categories><comments>To appear in CoSeRa'15 (Pisa, Italy, June 16-19, 2015). arXiv admin
  note: text overlap with arXiv:1502.02441</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize Huber's criterion to multichannel sparse
recovery problem of complex-valued measurements where the objective is to find
good recovery of jointly sparse unknown signal vectors from the given multiple
measurement vectors which are different linear combinations of the same known
elementary vectors. This requires careful characterization of robust
complex-valued loss functions as well as Huber's criterion function for the
multivariate sparse regression problem. We devise a greedy algorithm based on
simultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlike
the conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, is
robust under heavy-tailed non-Gaussian noise conditions, yet has a negligible
performance loss compared to SNIHT under Gaussian noise. Usefulness of the
method is illustrated in source localization application with sensor arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04208</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04208</id><created>2015-04-16</created><authors><author><keyname>Koopman</keyname><forenames>Rob</forenames></author><author><keyname>Wang</keyname><forenames>Shenghui</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Contextualization of topics - browsing through terms, authors, journals
  and cluster allocations</title><categories>cs.DL cs.IR</categories><comments>proceedings of the ISSI 2015 conference (accepted)</comments><acm-class>H.3.3; D.2.2; H.3.4; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper builds on an innovative Information Retrieval tool, Ariadne. The
tool has been developed as an interactive network visualization and browsing
tool for large-scale bibliographic databases. It basically allows to gain
insights into a topic by contextualizing a search query (Koopman et al., 2015).
In this paper, we apply the Ariadne tool to a far smaller dataset of 111,616
documents in astronomy and astrophysics. Labeled as the Berlin dataset, this
data have been used by several research teams to apply and later compare
different clustering algorithms. The quest for this team effort is how to
delineate topics. This paper contributes to this challenge in two different
ways. First, we produce one of the different cluster solution and second, we
use Ariadne (the method behind it, and the interface - called LittleAriadne) to
display cluster solutions of the different group members. By providing a tool
that allows the visual inspection of the similarity of article clusters
produced by different algorithms, we present a complementary approach to other
possible means of comparison. More particular, we discuss how we can - with
LittleAriadne - browse through the network of topical terms, authors, journals
and cluster solutions in the Berlin dataset and compare cluster solutions as
well as see their context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04216</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04216</id><created>2015-04-16</created><authors><author><keyname>Ivanov</keyname><forenames>V. K.</forenames></author><author><keyname>Meskin</keyname><forenames>P. I.</forenames></author></authors><title>Genetic algorithm implementation for effective document subject search</title><categories>cs.IR cs.NE</categories><comments>in Russian</comments><journal-ref>Programmnye produkty i sistemy 4 (2014) 118-126</journal-ref><doi>10.15827/0236-235X.108.118-126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the software implementation of genetic algorithm for
identifying and selecting most relevant results received during sequentially
executed subject search operations. Simulated evolutionary process generates
sustainable and effective population of search queries, forms search pattern of
documents or semantic core, creates relevant sets of required documents, allows
automatic classification of search results. The paper discusses the features of
subject search, justifies the use of a genetic algorithm, describes arguments
of the fitness function and describes basic steps and parameters of the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04235</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04235</id><created>2015-04-16</created><updated>2015-04-17</updated><authors><author><keyname>K&#xfc;hnlenz</keyname><forenames>Florian</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author></authors><title>Dynamics of Complex Systems Built as Coupled Physical, Communication and
  Decision Layers</title><categories>cs.MA cs.SI cs.SY physics.soc-ph</categories><doi>10.1371/journal.pone.0145135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple model to capture the complexity of multi-layer
systems where their constituent layers affect, are affected by, each other. The
physical layer is a circuit composed by a power source and resistors in
parallel. Individual agents can add, remove or keep the resistors they have,
and their decisions aiming at maximising the delivered power - a non-linear
function dependent on the others' behaviour - based on their internal state,
their global state perception, the information received from their neighbours
in the communication network, and a randomised selfishness. We develop an
agent-based simulation to analyse the effects of number of agents (size of the
system), communication network topology, communication errors and the minimum
power gain that triggers a behavioural change. Our results show that a
wave-like behaviour at macro-level (caused by individual changes in the
decision layer) can only emerge for a specific system size, the ratio between
cooperators and defectors depends on minimum gain assumed - lower minimal gains
lead to less cooperation and vice-versa, different communication network
topologies lead to different levels of power utilisation and fairness at the
physical layer, and a certain level of error in the communication layer leads
to more cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04239</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04239</id><created>2015-04-16</created><updated>2015-04-18</updated><authors><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Li</keyname><forenames>Weiping</forenames></author></authors><title>Secrecy Communication with Security Rate Measure</title><categories>cs.IT math.IT</categories><comments>10 pages, submitted to IEEE ITW 2015. arXiv admin note: text overlap
  with arXiv:1410.2881 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new measure on secrecy, which is established based on
rate-distortion theory. It is named \emph{security rate}, which is the minimum
(infimum) of the additional rate needed to reconstruct the source within target
distortion level with any positive probability for wiretapper. It denotes the
minimum distance in information metric (bits) from what wiretapper has received
to any decrypted reconstruction (where decryption is defined as reconstruction
within target distortion level with some positive possibility). By source
coding theorem, it is equivalent to a distortion-based equivocation
$\mathop{\min}\limits
_{p\left(v^{n}|s^{n},m\right):Ed\left(S^{n},V^{n}\right)\le
D_{E}}\frac{1}{n}I\left(S^{n};V^{n}|M\right)$ which can be seen as a direct
extension of equivocation $\frac{1}{n}H\left(S^{n}|M\right)$ to lossy
decryption case, given distortion level $D_{E}$ and the received (encrypted)
message $M$ of wiretapper. In this paper, we study it in Shannon cipher system
with lossless communication, where a source is transmitted from sender to
legitimate receiver secretly and losslessly, and also eavesdropped by a
wiretapper. We characterize the admissible region of secret key rate, coding
rate of the source, wiretapper distortion, and security rate (distortion-based
equivocation). Since the security rate equals the distortion-based
equivocation, and the equivocation is a special case of the distortion-based
equivocation (with Hamming distortion measure and $D_{E}=0$), this gives an
answer for the meaning of the maximum equivocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04240</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04240</id><created>2015-04-16</created><authors><author><keyname>Yong</keyname><forenames>Wang</forenames></author><author><keyname>Guiming</keyname><forenames>Li</forenames></author></authors><title>A Framework of Stability Analysis for Multi-agent Systems on Arbitrary
  Topology Graph: Linear Systems</title><categories>cs.SY</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, from the structural perspective, we propose a new stability
analysis approach for the consensus of linear multi-agent systems. Different
from the general tools: the Laplacian matrix based method and the Lyapunov's
method, this approach treats the multi-agent system as the composition of many
isolated agents, and focuses on their special input and output relationship.
Through transforming the construction of a graph into a standard procedure only
including three basic structures, the stability analysis is recursive and
independent of the specific topology. Therefore, this approach can be used for
multi-agent systems on any topology graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04244</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04244</id><created>2015-04-16</created><updated>2015-12-17</updated><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Throughput Maximization in Multi-Hop Wireless Networks under Secrecy
  Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the throughput of industrial communication networks under
a secrecy constraint. The proposed scenario is composed by sensors that measure
some relevant information of the plant that is first processed by aggregator
node and then sent to the control unit. The sensor measurements, their
communication with the aggregetor and the information processing are all
assumed perfect. To reach the control unit, the message may travel through
relay nodes, forming a multi-hop, wireless link. At every hop, eavesdropper
nodes attempt to acquire the messages transmitted through the legitimate link.
The communication design problem posed here is how to maximize the multi-hop
throughput from the aggregator to the control unit by finding the best
combination of relay positions (i.e. hop length: short or long) and coding
rates (i.e. high or low spectral efficiency) so that the secrecy constraint is
satisfied. Using a stochastic-geometry approach, we show that the optimal
choice of coding rate depends only on the path- loss exponent and is normally
high while greater number of shorter hops are preferable to smaller number of
longer hops. For the scenarios of interest, we prove that the optimal
throughput subject to the secrecy constraint achieves the unconstrained optimal
performance if a feasible solution exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04263</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04263</id><created>2015-04-16</created><updated>2015-09-07</updated><authors><author><keyname>Benton</keyname><forenames>David M.</forenames></author></authors><title>Concurrent codes: A holographic-type encoding robust against noise and
  loss</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent coding is an encoding scheme with &quot;holographic&quot; type properties
that are shown here to be robust against a significant amount of noise and
signal loss. This single encoding scheme is able to correct for random errors
and burst errors simultaneously, but does not rely on cyclic codes. A simple
and practical scheme has been tested that displays perfect decoding when the
signal to noise ratio is of order -18dB. The same scheme also displays perfect
reconstruction when a contiguous block of 40% of the transmission is missing.
In addition this scheme is 50% more efficient in terms of transmitted power
requirements than equivalent cyclic codes. A simple model is presented that
describes the process of decoding and can determine the computational load that
would be expected, as well as describing the critical levels of noise and
missing data at which false messages begin to be generated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04297</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04297</id><created>2015-04-16</created><authors><author><keyname>Sohail</keyname><forenames>Hamza Bin</forenames></author><author><keyname>Vamanan</keyname><forenames>Balajee</forenames></author><author><keyname>Vijaykumar</keyname><forenames>T. N.</forenames></author></authors><title>MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is
emerging as a lead alternative for main memory technology. While PCM achieves
low energy due to various technology-specific advantages, PCM is significantly
slower than DRAM (especially for writes) and can endure far fewer writes before
wearing out. Previous work has proposed to use a large, DRAM-based hardware
cache to absorb writes and provide faster access. However, due to ineffectual
caching where blocks are evicted before sufficient number of accesses, hardware
caches incur significant overheads in energy and bandwidth, two key but scarce
resources in modern multicores. Because using hardware for detecting and
removing such ineffectual caching would incur additional hardware cost and
complexity, we leverage the OS virtual memory support for this purpose. We
propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on
demand from the PCM to DRAM. We call the DRAM part of our memory as
MigrantStore which includes two ideas. First, to reduce the energy, bandwidth,
and wear overhead of ineffectual migrations, we propose migration hysteresis.
Second, to reduce the software overhead of good replacement policies, we
propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track
the addresses of recently-accessed MigrantStore pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04306</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04306</id><created>2015-04-16</created><authors><author><keyname>Kumar</keyname><forenames>Uday</forenames></author><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Comparative Analysis of Cryptography Library in IoT</title><categories>cs.CR</categories><comments>5 pages, 14 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper aims to do a survey along with a comparative analysis of the
various cryptography libraries that are applicable in the field of Internet of
Things (IoT). The first half of the paper briefly introduces the various
cryptography libraries available in the field of cryptography along with a list
of all the algorithms contained within the libraries. The second half of the
paper deals with cryptography libraries specifically aimed for application in
the field of Internet of Things. The various libraries and their performance
analysis listed down in this paper are consolidated from various sources with
the aim of providing a single comprehensive repository for reference to the
various cryptography libraries and the comparative analysis of their features
in IoT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04309</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04309</id><created>2015-04-16</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Esteve</keyname><forenames>Chantal</forenames></author><author><keyname>Chirivella</keyname><forenames>Javier</forenames></author><author><keyname>Gagliardo</keyname><forenames>Pablo</forenames></author></authors><title>Preprint Clinical Feedback and Technology Selection of Game Based
  Dysphonic Rehabilitation Tool</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on 2015 9th International
  Conference on Pervasive Computing Technologies for Healthcare
  (PervasiveHealth2015)</comments><acm-class>B.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 2015 9th International
Conference on Pervasive Computing Technologies for Healthcare
(PervasiveHealth2015). An assistive training tool software for rehabilitation
of dysphonic patients is evaluated according to the practical clinical feedback
from the treatments. One stroke sufferer and one parkinson sufferer have
provided earnest suggestions for the improvement of our tool software. The
assistive tool employs a serious game as the attractive logic part, and running
on the tablet with normal microphone as input device. Seven pitch estimation
algorithms have been evaluated and compared with selected patients voice
database. A series of benchmarks have been generated during the evaluation
process for technology selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04311</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04311</id><created>2015-04-16</created><updated>2015-09-22</updated><authors><author><keyname>Stay</keyname><forenames>Mike</forenames></author><author><keyname>Meredith</keyname><forenames>Lucius Gregory</forenames></author></authors><title>Higher category models of the pi-calculus</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present an approach to modeling computational calculi using higher
category theory. Specifically we present a fully abstract semantics for the
pi-calculus. The interpretation is consistent with Curry-Howard, interpreting
terms as typed morphisms, while simultaneously providing an explicit
interpretation of the rewrite rules of standard operational presentations as
2-morphisms. One of the key contributions, inspired by catalysis in chemical
reactions, is a method of restricting the application of 2-morphisms
interpreting rewrites to specific contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04317</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04317</id><created>2015-04-16</created><authors><author><keyname>Jones</keyname><forenames>Corinne L.</forenames></author><author><keyname>Bridges</keyname><forenames>Robert A.</forenames></author><author><keyname>Huffer</keyname><forenames>Kelly</forenames></author><author><keyname>Goodall</keyname><forenames>John</forenames></author></authors><title>Towards a relation extraction framework for cyber-security concepts</title><categories>cs.IR cs.CL cs.CR</categories><comments>4 pages in Cyber &amp; Information Security Research Conference 2015, ACM</comments><acm-class>H.3.3</acm-class><doi>10.1145/2746266.2746277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to assist security analysts in obtaining information pertaining to
their network, such as novel vulnerabilities, exploits, or patches, information
retrieval methods tailored to the security domain are needed. As labeled text
data is scarce and expensive, we follow developments in semi-supervised Natural
Language Processing and implement a bootstrapping algorithm for extracting
security entities and their relationships from text. The algorithm requires
little input data, specifically, a few relations or patterns (heuristics for
identifying relations), and incorporates an active learning component which
queries the user on the most important decisions to prevent drifting from the
desired relations. Preliminary testing on a small corpus shows promising
results, obtaining precision of .82.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04319</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04319</id><created>2015-04-16</created><updated>2015-09-21</updated><authors><author><keyname>Baillieul</keyname><forenames>John</forenames></author><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Wang</keyname><forenames>Shuai</forenames></author></authors><title>The Kirchhoff-Braess Paradox and Its Implications for Smart Microgrids</title><categories>cs.SY</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Well known in the theory of network flows, Braess paradox states that in a
congested network, it may happen that adding a new path between destinations
can increase the level of congestion. In transportation networks the phenomenon
results from the decisions of network participants who selfishly seek to
optimize their own performance metrics. In an electric power distribution
network, an analogous increase in congestion can arise as a consequence
Kirchhoff's laws. Even for the simplest linear network of resistors and voltage
sources, the sudden appearance of congestion due to an additional conductive
line is a nonlinear phenomenon that results in a discontinuous change in the
network state. It is argued that the phenomenon can occur in almost any grid in
which they are loops, and with the increasing penetration of small-scale
distributed generation it suggests challenges ahead in the operation of
microgrids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04322</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04322</id><created>2015-04-16</created><authors><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>On the Capacity of Level and Type Modulation in Molecular Communication
  with Ligand Receptors</title><categories>cs.IT math.IT</categories><comments>18 pages, Accepted at ISIT conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the bacterial point-to-point communication problem
with one transmitter and one receiver by considering the ligand receptor
binding process. The most commonly investigated signalling model, referred to
as the Level Scenario (LS), uses one type of a molecule with different
concentration levels for signaling. An alternative approach is to employ
multiple types of molecules with a single concentration level, referred to as
the Type Scenario (TS). We investigate the trade-offs between the two scenarios
for the ligand receptor from the capacity point of view. For this purpose, we
evaluate the capacity using numerical algorithms. Moreover, we derive an upper
bound on the capacity of the ligand receptor using a Binomial Channel (BIC)
model using symmetrized Kullback-Leibler (KL) divergence. A lower bound is also
derived when the environment noise is negligible. Finally, we analyse the
effect of blocking of a receptor by a molecule of a different type, by
proposing a new Markov model in the multiple-type signalling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04325</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04325</id><created>2015-04-16</created><authors><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Channel Modeling between Seaborne MIMO Radar and MIMO Cellular System</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sharing between radars and cellular systems is an emerging area of
research. In this paper, we model channel between a seaborne MIMO radar and
MIMO cellular system. We model a 2D channel to capture the azimuth aspect of
the spectrum sharing scenario. Our channel modeling methodology allows MIMO
radar to place accurate nulls in the azimuth location of base stations (BS),
thus, protecting them from harmful radar interference. We use a projection
based approach, where radar waveform is projected onto null space of channel,
for mitigating radar interference to BSs. This is also known as an approach
based on eigen-nulling which is different from spatial-nulling commonly
employed by radars. We show through simulations that the proposed spatial
channel model allows eigen-nulling which performs superior to traditional
spatial-nulling for interference mitigation. The proposed channel model can be
leveraged to use eigen-nulling that enhances target detection and beampattern
resolution of MIMO radar while mitigating interference to BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04326</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04326</id><created>2015-04-16</created><authors><author><keyname>Ashraf</keyname><forenames>Mohammad</forenames></author><author><keyname>Mohammad</keyname><forenames>Ghulam</forenames></author></authors><title>On skew cyclic codes over $F_q+vF_q+v^2F_q$</title><categories>cs.IT math.IT math.RA</categories><msc-class>94B05, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we study skew cyclic codes over the ring
$F_{q}+vF_{q}+v^2F_{q}$, where $v^3=v,~q=p^m$ and $p$ is an odd prime. We
investigate the structural properties of skew cyclic codes over
$F_{q}+vF_{q}+v^2F_{q}$ using decomposition method. By defining a Gray map from
$F_{q}+vF_{q}+v^2F_{q}$ to $F_{q}^3$, it has been proved that the Gray image of
a skew cyclic code of length $n$ over $F_{q}+vF_{q}+v^2F_{q}$ is a skew
$3$-quasi cyclic code of length $3n$ over $F_{q}$. Further, it is shown that
the skew cyclic codes over $F_{q}+vF_{q}+v^2F_{q}$ are principally generated.
Finally, the idempotent generators of skew cyclic codes over
$F_{q}+vF_{q}+v^2F_{q}$ are also obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04327</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04327</id><created>2015-04-16</created><authors><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>Control and Communication Protocols that Enable Smart Building
  Microgrids</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent communication, computation, and technology advances coupled with
climate change concerns have transformed the near future prospects of
electricity transmission, and, more notably, distribution systems and
microgrids. Distributed resources (wind and solar generation, combined heat and
power) and flexible loads (storage, computing, EV, HVAC) make it imperative to
increase investment and improve operational efficiency. Commercial and
residential buildings, being the largest energy consumption group among
flexible loads in microgrids, have the largest potential and flexibility to
provide demand side management. Recent advances in networked systems and the
anticipated breakthroughs of the Internet of Things will enable significant
advances in demand response capabilities of intelligent load network of
power-consuming devices such as HVAC components, water heaters, and buildings.
In this paper, a new operating framework, called packetized direct load control
(PDLC), is proposed based on the notion of quantization of energy demand. This
control protocol is built on top of two communication protocols that carry
either complete or binary information regarding the operation status of the
appliances. We discuss the optimal demand side operation for both protocols and
analytically derive the performance differences between the protocols. We
propose an optimal reservation strategy for traditional and renewable energy
for the PDLC in both day-ahead and real time markets. In the end we discuss the
fundamental trade-off between achieving controllability and endowing
flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04333</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04333</id><created>2015-04-16</created><updated>2015-06-08</updated><authors><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Three-dimensional (3D) Channel Modeling between Seaborne MIMO Radar and
  MIMO Cellular System</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing radar spectrum with communication systems is an emerging area of
research. Deploying commercial wireless communication services in radar bands
give wireless operators the much needed additional spectrum to meet the growing
bandwidth demands. However, to enable spectrum sharing between these two
fundamentally different systems interference concerns must be addressed. In
order to assess interference concerns we design a three-dimensional (3D)
channel model between radar and cellular base stations (BSs) in which the radar
uses a two-dimensional (2D) antenna array and the BS uses a one-dimensional
(1D) antenna array. We formulate a line-of-sight (LoS) channel and then propose
an algorithm that mitigates radar interference to BSs. We extend the previously
proposed null space projection algorithm for 2D channels to 3D channels and
show that effective nulls can be placed by utilizing both the azimuth and
elevation angle information of BSs. This results in effective interference
mitigation. In addition we show that the 3D channel model allows us to
accurately classify the size of radar's search space when null space projection
algorithm is used for interference mitigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04339</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04339</id><created>2015-04-16</created><updated>2015-05-12</updated><authors><author><keyname>Bonaci</keyname><forenames>Tamara</forenames></author><author><keyname>Herron</keyname><forenames>Jeffrey</forenames></author><author><keyname>Yusuf</keyname><forenames>Tariq</forenames></author><author><keyname>Yan</keyname><forenames>Junjie</forenames></author><author><keyname>Kohno</keyname><forenames>Tadayoshi</forenames></author><author><keyname>Chizeck</keyname><forenames>Howard Jay</forenames></author></authors><title>To Make a Robot Secure: An Experimental Analysis of Cyber Security
  Threats Against Teleoperated Surgical Robots</title><categories>cs.RO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teleoperated robots are playing an increasingly important role in military
actions and medical services. In the future, remotely operated surgical robots
will likely be used in more scenarios such as battlefields and emergency
response. But rapidly growing applications of teleoperated surgery raise the
question; what if the computer systems for these robots are attacked, taken
over and even turned into weapons? Our work seeks to answer this question by
systematically analyzing possible cyber security attacks against Raven II, an
advanced teleoperated robotic surgery system. We identify a slew of possible
cyber security threats, and experimentally evaluate their scopes and impacts.
We demonstrate the ability to maliciously control a wide range of robots
functions, and even to completely ignore or override command inputs from the
surgeon. We further find that it is possible to abuse the robot's existing
emergency stop (E-stop) mechanism to execute efficient (single packet) attacks.
We then consider steps to mitigate these identified attacks, and experimentally
evaluate the feasibility of applying the existing security solutions against
these threats. The broader goal of our paper, however, is to raise awareness
and increase understanding of these emerging threats. We anticipate that the
majority of attacks against telerobotic surgery will also be relevant to other
teleoperated robotic and co-robotic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04343</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04343</id><created>2015-04-16</created><updated>2015-05-26</updated><authors><author><keyname>Hadjis</keyname><forenames>Stefan</forenames></author><author><keyname>Abuzaid</keyname><forenames>Firas</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Caffe con Troll: Shallow Ideas to Speed Up Deep Learning</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Caffe con Troll (CcT), a fully compatible end-to-end version of
the popular framework Caffe with rebuilt internals. We built CcT to examine the
performance characteristics of training and deploying general-purpose
convolutional neural networks across different hardware architectures. We find
that, by employing standard batching optimizations for CPU training, we achieve
a 4.5x throughput improvement over Caffe on popular networks like CaffeNet.
Moreover, with these improvements, the end-to-end training time for CNNs is
directly proportional to the FLOPS delivered by the CPU, which enables us to
efficiently train hybrid CPU-GPU systems for CNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04350</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04350</id><created>2015-04-16</created><authors><author><keyname>Kliuchnikov</keyname><forenames>Vadym</forenames></author><author><keyname>Yard</keyname><forenames>Jon</forenames></author></authors><title>A framework for exact synthesis</title><categories>quant-ph cs.ET</categories><comments>40 pages, preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact synthesis is a tool used in algorithms for approximating an arbitrary
qubit unitary with a sequence of quantum gates from some finite set. These
approximation algorithms find asymptotically optimal approximations in
probabilistic polynomial time, in some cases even finding the optimal solution
in probabilistic polynomial time given access to an oracle for factoring
integers. In this paper, we present a common mathematical structure underlying
all results related to the exact synthesis of qubit unitaries known to date,
including Clifford+T, Clifford-cyclotomic and V-basis gate sets, as well as
gates sets induced by the braiding of Fibonacci anyons in topological quantum
computing. The framework presented here also provides a means to answer
questions related to the exact synthesis of unitaries for wide classes of other
gate sets, such as Clifford+T+V and SU(2) level k anyons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04351</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04351</id><created>2015-04-16</created><authors><author><keyname>Budkuley</keyname><forenames>Amitalok J.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Dirty Paper Arbitrarily Varying Channel with a State-Aware Adversary</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we take an arbitrarily varying channel (AVC) approach to
examine the problem of writing on a dirty paper in the presence of an
adversary. We consider an additive white Gaussian noise (AWGN) channel with an
additive white Gaussian state, where the state is known non-causally to the
encoder and the adversary, but not the decoder. We determine the randomized
coding capacity of this AVC under the maximal probability of error criterion.
Interestingly, it is shown that the jamming adversary disregards the state
knowledge to choose a white Gaussian channel input which is independent of the
state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04357</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04357</id><created>2015-04-16</created><authors><author><keyname>Simmie</keyname><forenames>Donal</forenames></author><author><keyname>Thapen</keyname><forenames>Nicholas</forenames></author><author><keyname>Hankin</keyname><forenames>Chris</forenames></author></authors><title>DEFENDER: Detecting and Forecasting Epidemics using Novel Data-analytics
  for Enhanced Response</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years social and news media have increasingly been used to explain
patterns in disease activity and progression. Social media data, principally
from the Twitter network, has been shown to correlate well with official
disease case counts. This fact has been exploited to provide advance warning of
outbreak detection, tracking of disease levels and the ability to predict the
likelihood of individuals developing symptoms. In this paper we introduce
DEFENDER, a software system that integrates data from social and news media and
incorporates algorithms for outbreak detection, situational awareness,
syndromic case tracking and forecasting. As part of this system we have
developed a technique for creating a location network for any country or region
based purely on Twitter data. We also present a disease count tracking approach
which leverages counts from multiple symptoms, which was found to improve the
tracking of diseases by 37 percent over a model that used only previous case
data. Finally we attempt to forecast future levels of symptom activity based on
observed user movement on Twitter, finding a moderate gain of 5 percent over a
time series forecasting model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04359</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04359</id><created>2015-04-16</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Evolutionary games on multilayer networks: A colloquium</title><categories>physics.soc-ph cs.GT cs.SI nlin.AO q-bio.PE</categories><comments>14 two-column pages, 8 figures; accepted for publication in European
  Physical Journal B</comments><journal-ref>Eur. Phys. J. B 88 (2015) 124</journal-ref><doi>10.1140/epjb/e2015-60270-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks form the backbone of many complex systems, ranging from the Internet
to human societies. Accordingly, not only is the range of our interactions
limited and thus best described and modeled by networks, it is also a fact that
the networks that are an integral part of such models are often interdependent
or even interconnected. Networks of networks or multilayer networks are
therefore a more apt description of social systems. This colloquium is devoted
to evolutionary games on multilayer networks, and in particular to the
evolution of cooperation as one of the main pillars of modern human societies.
We first give an overview of the most significant conceptual differences
between single-layer and multilayer networks, and we provide basic definitions
and a classification of the most commonly used terms. Subsequently, we review
fascinating and counterintuitive evolutionary outcomes that emerge due to
different types of interdependencies between otherwise independent populations.
The focus is on coupling through the utilities of players, through the flow of
information, as well as through the popularity of different strategies on
different network layers. The colloquium highlights the importance of pattern
formation and collective behavior for the promotion of cooperation under
adverse conditions, as well as the synergies between network science and
evolutionary game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04363</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04363</id><created>2015-04-16</created><authors><author><keyname>Backman</keyname><forenames>Spencer</forenames></author><author><keyname>Huynh</keyname><forenames>Tony</forenames></author></authors><title>Transfinite Ford-Fulkerson on a Finite Network</title><categories>math.CO cs.DM</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the Ford-Fulkerson algorithm for finding a maximum flow
in a network need not terminate if we allow the arc capacities to take
irrational values. Every non-terminating example converges to a limit flow, but
this limit flow need not be a maximum flow. Hence, one may pass to the limit
and begin the algorithm again. In this way, we may view the Ford-Fulkerson
algorithm as a transfinite algorithm.
  We analyze the transfinite running-time of the Ford-Fulkerson algorithm using
ordinal numbers, and prove that the worst case running-time is
$\omega^{\Theta(|E|)}$. For the lower bound, we show that we can model the
Euclidean algorithm via Ford-Fulkerson on an auxiliary network. By running this
example on a pair of incommensurable numbers, we obtain a new robust
non-terminating example. We then describe how to glue $k$ copies of our
Euclidean example in parallel to obtain running-time $\omega^k$. An upper bound
of $\omega^{|E|}$ is established via induction on $|E|$. We conclude by
illustrating a close connection to transfinite chip-firing as previously
investigated by the first author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04387</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04387</id><created>2015-04-16</created><authors><author><keyname>Golbeck</keyname><forenames>Jennifer</forenames></author></authors><title>Benford's Law Applies To Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 2 figures</comments><doi>10.1371/journal.pone.0135169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benford's Law states that the frequency of first digits of numbers in
naturally occurring systems is not evenly distributed. Numbers beginning with a
1 occur roughly 30\% of the time, and are six times more common than numbers
beginning with a 9. We show that Benford's Law applies to social and behavioral
features of users in online social networks. We consider social data from five
major social networks: Facebook, Twitter, Google Plus, Pinterest, and Live
Journal. We show that the distribution of first significant digits of friend
and follower counts for users in these systems follow Benford's Law. The same
holds for the number of posts users make. We extend this to egocentric
networks, showing that friend counts among the people in an individual's social
network also follow the expected distribution. We discuss how this can be used
to detect suspicious or fraudulent activity online and to validate datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04406</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04406</id><created>2015-04-16</created><authors><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Babanezhad</keyname><forenames>Reza</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed Osama</forenames></author><author><keyname>Defazio</keyname><forenames>Aaron</forenames></author><author><keyname>Clifton</keyname><forenames>Ann</forenames></author><author><keyname>Sarkar</keyname><forenames>Anoop</forenames></author></authors><title>Non-Uniform Stochastic Average Gradient Method for Training Conditional
  Random Fields</title><categories>stat.ML cs.LG math.OC stat.CO</categories><comments>AI/Stats 2015, 24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply stochastic average gradient (SAG) algorithms for training
conditional random fields (CRFs). We describe a practical implementation that
uses structure in the CRF gradient to reduce the memory requirement of this
linearly-convergent stochastic gradient method, propose a non-uniform sampling
scheme that substantially improves practical performance, and analyze the rate
of convergence of the SAGA variant under non-uniform sampling. Our experimental
results reveal that our method often significantly outperforms existing methods
in terms of the training objective, and performs as well or better than
optimally-tuned stochastic gradient methods in terms of test error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04407</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04407</id><created>2015-04-16</created><updated>2015-11-16</updated><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose mS2GD: a method incorporating a mini-batching scheme for improving
the theoretical complexity and practical performance of semi-stochastic
gradient descent (S2GD). We consider the problem of minimizing a strongly
convex function represented as the sum of an average of a large number of
smooth convex functions, and a simple nonsmooth convex regularizer. Our method
first performs a deterministic step (computation of the gradient of the
objective function at the starting point), followed by a large number of
stochastic steps. The process is repeated a few times with the last iterate
becoming the new starting point. The novelty of our method is in introduction
of mini-batching into the computation of stochastic steps. In each step,
instead of choosing a single function, we sample $b$ functions, compute their
gradients, and compute the direction based on this. We analyze the complexity
of the method and show that it benefits from two speedup effects. First, we
prove that as long as $b$ is below a certain threshold, we can reach any
predefined accuracy with less overall work than without mini-batching. Second,
our mini-batching scheme admits a simple parallel implementation, and hence is
suitable for further acceleration by parallelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04419</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04419</id><created>2015-04-16</created><updated>2016-02-01</updated><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Wasserstein continuity of entropy and outer bounds for interference
  channels</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that under suitable regularity conditions, differential entropy
is a Lipschitz functional on the space of distributions on $n$-dimensional
Euclidean space with respect to the quadratic Wasserstein distance. Under
similar conditions, (discrete) Shannon entropy is shown to be Lipschitz
continuous in distributions over the product space with respect to Ornstein's
$\bar d$-distance (Wasserstein distance corresponding to the Hamming distance).
These results together with Talagrand's and Marton's transportation-information
inequalities allow one to replace the unknown multi-user interference with its
i.i.d. approximations. As an application, a new outer bound for the two-user
Gaussian interference channel is proved, which, in particular, settles the
&quot;missing corner point&quot; problem of Costa (1985).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04420</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04420</id><created>2015-04-16</created><authors><author><keyname>M</keyname><forenames>Manjunath</forenames></author><author><keyname>H</keyname><forenames>Manjaiah D.</forenames></author></authors><title>PAR: Petal ant routing algorithm for mobile ad hoc network</title><categories>cs.NI</categories><comments>14 Pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During route discovery of mobile ad hoc network, broadcasting of route
request and route reply packets are the essential operations for finding the
path between two ends. In such situations, intermediate node which may or may
not belongs will participate in route discovery process, update routing table
and rebroadcast the route discovery packets again to its neighboring nodes.
Finally optimal path is found with minimum hops. This simply upsurges overhead
and deteriorates the performance of routing. The proposed Petal Ant Routing
(PAR) algorithm offers a low overhead by optimizing FANT and BANT transmissions
in route discover process. The algorithm is an improved version of SARA and has
features extracted from petal routing. The algorithm is simulated on NS2,
compared with ACO frame work called SARA and classical routing protocols such
as AODV and AOMDV. The simulation results shows that PAR further reduces
overhead by eliminating redundant FANT transmission compared to other routing
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04421</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04421</id><created>2015-04-16</created><authors><author><keyname>Padhye</keyname><forenames>Nikhil</forenames></author><author><keyname>Mittal</keyname><forenames>Pulkit</forenames></author><author><keyname>Deb</keyname><forenames>Kalyanmoy</forenames></author></authors><title>Feasibility Preserving Constraint-Handling Strategies for Real Parameter
  Evolutionary Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Algorithms (EAs) are being routinely applied for a variety of
optimization tasks, and real-parameter optimization in the presence of
constraints is one such important area. During constrained optimization EAs
often create solutions that fall outside the feasible region; hence a viable
constraint- handling strategy is needed. This paper focuses on the class of
constraint-handling strategies that repair infeasible solutions by bringing
them back into the search space and explicitly preserve feasibility of the
solutions. Several existing constraint-handling strategies are studied, and two
new single parameter constraint-handling methodologies based on parent-centric
and inverse parabolic probability (IP) distribution are proposed. The existing
and newly proposed constraint-handling methods are first studied with PSO, DE,
GAs, and simulation results on four scalable test-problems under different
location settings of the optimum are presented. The newly proposed
constraint-handling methods exhibit robustness in terms of performance and also
succeed on search spaces comprising up-to 500 variables while locating the
optimum within an error of 10$^{-10}$. The working principle of the IP based
methods is also demonstrated on (i) some generic constrained optimization
problems, and (ii) a classic `Weld' problem from structural design and
mechanics. The successful performance of the proposed methods clearly exhibits
their efficacy as a generic constrained-handling strategy for a wide range of
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04423</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04423</id><created>2015-04-16</created><updated>2015-07-29</updated><authors><author><keyname>Khatamianfar</keyname><forenames>Arash</forenames></author></authors><title>Advanced Discrete-Time Control Methods for Industrial Applications</title><categories>cs.SY</categories><comments>PhD Thesis, 230 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis focuses on developing advanced control methods for two industrial
systems in discrete-time aiming to enhance their performance in delivering the
control objectives as well as considering the practical aspects. The first part
addresses wind power dispatch into the electricity network using a battery
energy storage system (BESS). To manage the amount of energy sold to the
electricity market, a novel control scheme is developed based on discrete-time
model predictive control (MPC) to ensure the optimal operation of the BESS in
the presence of practical constraints. The control scheme follows a decision
policy to sell more energy at peak demand times and store it at off-peaks in
compliance with the Australian National Electricity Market rules. The
performance of the control system is assessed under different scenarios using
actual wind farm and electricity price data in simulation environment. The
second part considers the control of overhead crane systems for automatic
operation. To achieve high-speed load transportation with high-precision and
minimum load swings, a new modeling approach is developed based on independent
joint control strategy which considers actuators as the main plant. The
nonlinearities of overhead crane dynamics are treated as disturbances acting on
each actuator. The resulting model enables us to estimate the unknown
parameters of the system including coulomb friction constants. A novel load
swing control is also designed based on passivity-based control to suppress
load swings. Two discrete-time controllers are then developed based on MPC and
state feedback control to track reference trajectories along with a feedforward
control to compensate for disturbances using computed torque control and a
novel disturbance observer. The practical results on an experimental overhead
crane setup demonstrate the high performance of the designed control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04424</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04424</id><created>2015-04-16</created><authors><author><keyname>Cooper</keyname><forenames>Joshua</forenames></author><author><keyname>Rorabaugh</keyname><forenames>Danny</forenames></author></authors><title>Density dichotomy in random words</title><categories>math.CO cs.DM</categories><comments>12 pages, submitted April 2015 to Combinatorics, Probability and
  Computing</comments><msc-class>05A05, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word $W$ is said to encounter word $V$ provided there is a homomorphism
$\phi$ mapping letters to nonempty words so that $\phi(V)$ is a substring of
$W$. For example, taking $\phi$ such that $\phi(h)=c$ and $\phi(u)=ien$, we see
that $science$ encounters $huh$ since $cienc=\phi(huh)$. The density of $V$ in
$W$ is the proportion of substrings of $W$ that are homomorphic images of $V$.
So the density of $huh$ in $science$ is $2/{8 \choose 2}$. A word is doubled if
every letter that appears in the word appears at least twice.
  The dichotomy: Let $V$ be a word over any alphabet, $\Sigma$ a finite
alphabet with at least 2 letters, and $W_n \in \Sigma^n$ chosen uniformly at
random. $V$ is doubled if and only if $\delta(V,W_n)=0$ asymptotically almost
surely.
  Other density results we prove include convergence for nondoubled words and
concentration of the limit distribution for doubled words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04426</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04426</id><created>2015-04-16</created><authors><author><keyname>Tsukada</keyname><forenames>Manabu</forenames></author><author><keyname>Santa</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Matsuura</keyname><forenames>Satoshi</forenames></author><author><keyname>Ernst</keyname><forenames>Thierry</forenames></author><author><keyname>Fujikawa</keyname><forenames>Kazutoshi</forenames></author></authors><title>On the Experimental Evaluation of Vehicular Networks: Issues,
  Requirements and Methodology Applied to a Real Use Case</title><categories>cs.NI</categories><comments>in EAI Endorsed Transactions on Industrial Networks and Intelligent
  Systems, 2014</comments><doi>10.4108/inis.1.1.e4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most challenging fields in vehicular communications has been the
experimental assessment of protocols and novel technologies. Researchers
usually tend to simulate vehicular scenarios and/or partially validate new
contributions in the area by using constrained testbeds and carrying out minor
tests. In this line, the present work reviews the issues that pioneers in the
area of vehicular communications and, in general, in telematics, have to deal
with if they want to perform a good evaluation campaign by real testing. The
key needs for a good experimental evaluation is the use of proper software
tools for gathering testing data, post-processing and generating relevant
figures of merit and, finally, properly showing the most important results. For
this reason, a key contribution of this paper is the presentation of an
evaluation environment called AnaVANET, which covers the previous needs. By
using this tool and presenting a reference case of study, a generic testing
methodology is described and applied. This way, the usage of the IPv6 protocol
over a vehicle-to-vehicle routing protocol, and supporting IETF-based network
mobility, is tested at the same time the main features of the AnaVANET system
are presented. This work contributes in laying the foundations for a proper
experimental evaluation of vehicular networks and will be useful for many
researchers in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04428</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04428</id><created>2015-04-16</created><updated>2016-02-23</updated><authors><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Optimal Dynamic Multicast Scheduling for Cache-Enabled Content-Centric
  Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>17 double-column pages; Shorter version appears in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching and multicasting at base stations are two promising approaches to
support massive content delivery over wireless networks. However, existing
scheduling designs do not make full use of the advantages of the two
approaches. In this paper, we consider the optimal dynamic multicast scheduling
to jointly minimize the average delay, power, and fetching costs for
cache-enabled content-centric wireless networks. We formulate this stochastic
optimization problem as an infinite horizon average cost Markov decision
process (MDP). It is well-known to be a difficult problem due to the curse of
dimensionality, and there generally only exist numerical solutions. By using
relative value iteration algorithm and the special structures of the request
queue dynamics, we analyze the properties of the value function and the
state-action cost function of the MDP for both the uniform and nonuniform
channel cases. Based on these properties, we show that the optimal policy,
which is adaptive to the request queue state, has a switch structure in the
uniform case and a partial switch structure in the nonuniform case. Moreover,
in the uniform case with two contents, we show that the switch curve is
monotonically non-decreasing. Then, by exploiting these structural properties
of the optimal policy, we propose two low-complexity optimal algorithms.
Motivated by the switch structures of the optimal policy, to further reduce the
complexity, we also propose a low-complexity suboptimal policy, which possesses
similar structural properties to the optimal policy, and develop a
low-complexity algorithm to compute this policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04432</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04432</id><created>2015-04-16</created><authors><author><keyname>Noorzad</keyname><forenames>Parham</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>On the Cost and Benefit of Cooperation (Extended Version)</title><categories>cs.IT math.IT</categories><comments>15 pages, 3 figures. To be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a cooperative coding scheme, network nodes work together to achieve higher
transmission rates. To obtain a better understanding of cooperation, we
consider a model in which two transmitters send rate-limited descriptions of
their messages to a &quot;cooperation facilitator&quot;, a node that sends back
rate-limited descriptions of the pair to each transmitter. This model includes
the conferencing encoders model and a prior model from the current authors as
special cases. We show that except for a special class of multiple access
channels, the gain in sum-capacity resulting from cooperation under this model
is quite large. Adding a cooperation facilitator to any such channel results in
a network that does not satisfy the edge removal property. An important special
case is the Gaussian multiple access channel, for which we explicitly
characterize the sum-rate cooperation gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04433</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04433</id><created>2015-04-16</created><updated>2015-04-23</updated><authors><author><keyname>Shao</keyname><forenames>Lu</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Jiang</keyname><forenames>Changjun</forenames></author></authors><title>STC: Coarse-Grained Vehicular Data Based Travel Speed Sensing by
  Leveraging Spatial-Temporal Correlation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important information for traffic condition evaluation, trip planning,
transportation management, etc., average travel speed for a road means the
average speed of vehicles travelling through this road in a given time
duration. Traditional ways for collecting travel-speed oriented traffic data
always depend on dedicated sensors and supporting infrastructures, and are
therefore financial costly. Differently, vehicular crowdsensing as an
infrastructure-free way, can be used to collect data including real-time
locations and velocities of vehicles for road travel speed estimation, which is
a quite low-cost way. However, vehicular crowdsensing data is always
coarse-grained. This coarseness can lead to the incompleteness of travel
speeds. Aiming to handle this problem as well as estimate travel speed
accurately, in this paper, we propose an approach named STC that exploits the
spatial-temporal correlation among travel speeds for roads by introducing the
time-lagged cross correlation function. The time lagging factor describes the
time consumption of traffic feature diffusion along roads. To properly
calculate cross correlation, we novelly make the determination of the time
lagging factor self-adaptive by recording the locations of vehicles at
different roads. Then, utilizing the local stationarity of cross correlation,
we further reduce the problem of single-road travel speed vacancy completion to
a minimization problem. Finally, we fill all the vacancies of travel speed for
roads in a recursive way using the geometric structure of road net. Elaborate
experiments based on real taxi trace data show that STC can settle the
incompleteness problem of vehicle crowdsensing data based travel speed
estimation and ensure the accuracy of estimated travel speed better, in
comparison with representative existing methods such as KNN, Kriging and ARIMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04449</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04449</id><created>2015-04-17</created><updated>2015-09-25</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Leditzky</keyname><forenames>Felix</forenames></author></authors><title>Decoding quantum information via the Petz recovery map</title><categories>quant-ph cs.IT math.IT</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain a lower bound on the maximum number of qubits, $Q^{n,
\varepsilon}({\mathcal{N}})$, which can be transmitted over $n$ uses of a
quantum channel $\mathcal{N}$, for a given non-zero error threshold
$\varepsilon$. To obtain our result, we first derive a bound on the one-shot
entanglement transmission capacity of the channel, and then compute its
asymptotic expansion up to the second order. In our method to prove this
achievability bound, the decoding map, used by the receiver on the output of
the channel, is chosen to be the Petz recovery map (also known as the transpose
channel). Our result, in particular, shows that this choice of the decoder can
be used to establish the coherent information as an achievable rate for quantum
information transmission. Applying our achievability bound to the 50-50 erasure
channel (which has zero quantum capacity), we find that there is a sharp error
threshold above which $Q^{n, \varepsilon}({\mathcal{N}})$ scales as $\sqrt{n}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04464</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04464</id><created>2015-04-17</created><authors><author><keyname>Xu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Gandhi</keyname><forenames>Praveen Kumar M.</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Chong</keyname><forenames>Peter Han Joo</forenames></author></authors><title>Two-Phase Cooperative Broadcasting Based on Batched Network Code</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the wireless broadcasting scenario with a source
node sending some common information to a group of closely located users, where
each link is subject to certain packet erasures. To ensure reliable information
reception by all users, the conventional approach generally requires repeated
transmission by the source until all the users are able to decode the
information, which is inefficient in many practical scenarios. In this paper,
by exploiting the close proximity among the users, we propose a novel two-phase
wireless broadcasting protocol with user cooperations based on an efficient
batched network code, known as batched sparse (BATS) code. In the first phase,
the information packets are encoded into batches with BATS encoder and
sequentially broadcasted by the source node until certain terminating criterion
is met. In the second phase, the users cooperate with each other by exchanging
the network-coded information via peer-to-peer (P2P) communications based on
their respective received packets. A fully distributed and light-weight
scheduling algorithm is proposed to improve the efficiency of the P2P
communication in the second phase. The performance of the proposed two-phase
protocol is analyzed and the channel rank distribution at the instance of
decoding is derived, based on which the optimal BATS code is designed.
Simulation results demonstrate that the proposed protocol significantly
outperforms the existing schemes. Lastly, the performance of the proposed
scheme is further verified via testbed experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04465</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04465</id><created>2015-04-17</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author></authors><title>Probabilistic Handshake in All-to-all Broadcast Coded Slotted ALOHA</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic handshake mechanism for all-to-all broadcast coded
slotted ALOHA. We consider a fully connected network where each user acts as
both transmitter and receiver in a half-duplex mode. Users attempt to exchange
messages with each other and to establish one-to-one handshakes, in the sense
that each user decides whether its packet was successfully received by the
other users: After performing decoding, each user estimates in which slots the
resolved users transmitted their packets and, based on that, decides if these
users successfully received its packet. The simulation results show that the
proposed handshake algorithm allows the users to reliably perform the
handshake. The paper also provides some analytical bounds on the performance of
the proposed algorithm which are in good agreement with the simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04470</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04470</id><created>2015-04-17</created><authors><author><keyname>Cao</keyname><forenames>Yixin</forenames></author></authors><title>Unit Interval Editing is Fixed-Parameter Tractable</title><categories>cs.DS</categories><comments>An extended abstract of this paper has appeared in the proceedings of
  ICALP 2015</comments><msc-class>05C75, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph~$G$ and integers $k_1$, $k_2$, and~$k_3$, the unit interval
editing problem asks whether $G$ can be transformed into a unit interval graph
by at most $k_1$ vertex deletions, $k_2$ edge deletions, and $k_3$ edge
additions. We give an algorithm solving the problem in time $2^{O(k\log
k)}\cdot (n+m)$, where $k := k_1 + k_2 + k_3$, and $n, m$ denote respectively
the numbers of vertices and edges of $G$. Therefore, it is fixed-parameter
tractable parameterized by the total number of allowed operations.
  This implies the fixed-parameter tractability of the unit interval edge
deletion problem, for which we also present a more efficient algorithm running
in time $O(4^k \cdot (n + m))$. Another result is an $O(6^k \cdot (n +
m))$-time algorithm for the unit interval vertex deletion problem,
significantly improving the best-known algorithm running in time $O(6^k \cdot
n^6)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04478</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04478</id><created>2015-04-17</created><updated>2015-09-15</updated><authors><author><keyname>Hartmann</keyname><forenames>Thomas</forenames></author><author><keyname>Zapilko</keyname><forenames>Benjamin</forenames></author><author><keyname>Wackerow</keyname><forenames>Joachim</forenames></author><author><keyname>Eckert</keyname><forenames>Kai</forenames></author></authors><title>Evaluating the Quality of RDF Data Sets on Common Vocabularies in the
  Social, Behavioral, and Economic Sciences</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From 2012 to 2015 together with other Linked Data community members and
experts from the social, behavioral, and economic sciences (SBE), we developed
diverse vocabularies to represent SBE metadata and tabular data in RDF. The
DDI-RDF Discovery Vocabulary (DDI-RDF) is designed to support the
dissemination, management, and reuse of unit-record data, i.e., data about
individuals, households, and businesses, collected in form of responses to
studies and archived for research purposes. The RDF Data Cube Vocabulary (QB)
is a W3C recommendation for expressing data cubes, i.e. multi-dimensional
aggregate data and its metadata. Physical Data Description (PHDD) is a
vocabulary to model data in rectangular format, i.e., tabular data. The data
could either be represented in records with character-separated values (CSV) or
fixed length. The Simple Knowledge Organization System (SKOS) is a vocabulary
to build knowledge organization systems such as thesauri, classification
schemes, and taxonomies. XKOS is a SKOS extension to describe formal
statistical classifications.
  To ensure high quality of and trust in both metadata and data, their
representation in RDF must satisfy certain criteria - specified in terms of RDF
constraints. In this paper, we evaluate the data quality of 15,694 data sets
(4.26 billion triples) of research data for the social, behavioral, and
economic sciences obtained from 33 SPARQL endpoints. We checked 115 constraints
on three different and representative SBE vocabularies (DDI-RDF, QB, and SKOS)
by means of the RDF Validator, a validation environment which is available at
http://purl.org/net/rdfval-demo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04479</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04479</id><created>2015-04-17</created><updated>2015-09-15</updated><authors><author><keyname>Hartmann</keyname><forenames>Thomas</forenames></author><author><keyname>Zapilko</keyname><forenames>Benjamin</forenames></author><author><keyname>Wackerow</keyname><forenames>Joachim</forenames></author><author><keyname>Eckert</keyname><forenames>Kai</forenames></author></authors><title>Constraints to Validate RDF Data Quality on Common Vocabularies in the
  Social, Behavioral, and Economic Sciences</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To ensure high quality of and trust in both metadata and data, their
representation in RDF must satisfy certain criteria - specified in terms of RDF
constraints. From 2012 to 2015 together with other Linked Data community
members and experts from the social, behavioral, and economic sciences (SBE),
we developed diverse vocabularies to represent SBE metadata and rectangular
data in RDF.
  The DDI-RDF Discovery Vocabulary (DDI-RDF) is designed to support the
dissemination, management, and reuse of unit-record data, i.e., data about
individuals, households, and businesses, collected in form of responses to
studies and archived for research purposes. The RDF Data Cube Vocabulary (QB)
is a W3C recommendation for expressing data cubes, i.e. multi-dimensional
aggregate data and its metadata. Physical Data Description (PHDD) is a
vocabulary to model data in rectangular format, i.e., tabular data. The data
could either be represented in records with character-separated values (CSV) or
fixed length. The Simple Knowledge Organization System (SKOS) is a vocabulary
to build knowledge organization systems such as thesauri, classification
schemes, and taxonomies. XKOS is a SKOS extension to describe formal
statistical classifications.
  In this paper, we describe RDF constraints to validate metadata on
unit-record data (DDI-RDF), aggregated data (QB), thesauri (SKOS), and
statistical classifications (XKOS) and to validate tabular data (PHDD) - all of
them represented in RDF. We classified these constraints according to the
severity of occurring constraint violations. This technical report is updated
continuously as modifying, adding, and deleting constraints remains ongoing
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04494</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04494</id><created>2015-04-17</created><updated>2015-12-30</updated><authors><author><keyname>Mojahedian</keyname><forenames>Mohammad Mahdi</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Perfectly Secure Index Coding</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures, submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the index coding problem in the presence of an
eavesdropper. Messages are to be sent from one transmitter to a number of
legitimate receivers who have side information about the messages, and share a
set of secret keys with the transmitter. We assume perfect secrecy, meaning
that the eavesdropper should not be able to retrieve any information about the
message set. We study the minimum key lengths for zero-error and perfectly
secure index coding problem. On one hand, this problem is a generalization of
the index coding problem (and thus a difficult one). On the other hand, it is a
generalization of the Shannon's cipher system. We show that a generalization of
Shannon's one-time pad strategy is optimal up to a multiplicative constant,
meaning that it obtains the entire boundary of the cone formed by looking at
the secure rate region from the origin. Finally, we consider relaxation of the
perfect secrecy and zero-error constraints to weak secrecy and asymptotically
vanishing probability of error, and provide a secure version of the result,
obtained by Langberg and Effros, on the equivalence of zero-error and
$\epsilon$-error regions in the conventional index coding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04498</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04498</id><created>2015-04-17</created><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Gavoille</keyname><forenames>Cyril</forenames></author><author><keyname>Halvorsen</keyname><forenames>Esben Bistrup</forenames></author><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>Simpler, faster and shorter labels for distances in graphs</title><categories>cs.DS</categories><acm-class>E.1; G.2.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how to assign labels to any undirected graph with n nodes such
that, given the labels of two nodes and no other information regarding the
graph, it is possible to determine the distance between the two nodes. The
challenge in such a distance labeling scheme is primarily to minimize the
maximum label lenght and secondarily to minimize the time needed to answer
distance queries (decoding). Previous schemes have offered different trade-offs
between label lengths and query time. This paper presents a simple algorithm
with shorter labels and shorter query time than any previous solution, thereby
improving the state-of-the-art with respect to both label length and query time
in one single algorithm. Our solution addresses several open problems
concerning label length and decoding time and is the first improvement of label
length for more than three decades.
  More specifically, we present a distance labeling scheme with label size (log
3)/2 + o(n) (logarithms are in base 2) and O(1) decoding time. This outperforms
all existing results with respect to both size and decoding time, including
Winkler's (Combinatorica 1983) decade-old result, which uses labels of size
(log 3)n and O(n/log n) decoding time, and Gavoille et al. (SODA'01), which
uses labels of size 11n + o(n) and O(loglog n) decoding time. In addition, our
algorithm is simpler than the previous ones. In the case of integral edge
weights of size at most W, we present almost matching upper and lower bounds
for label sizes. For r-additive approximation schemes, where distances can be
off by an additive constant r, we give both upper and lower bounds. In
particular, we present an upper bound for 1-additive approximation schemes
which, in the unweighted case, has the same size (ignoring second order terms)
as an adjacency scheme: n/2. We also give results for bipartite graphs and for
exact and 1-additive distance oracles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04499</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04499</id><created>2015-04-17</created><authors><author><keyname>Mishra</keyname><forenames>Manoj</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>On the Oblivious Transfer Capacity of the Degraded Wiretapped Binary
  Erasure Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>To be presented at the IEEE International Symposium on Information
  Theory (ISIT 2015), Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study oblivious transfer (OT) between Alice and Bob in the presence of an
eavesdropper Eve over a degraded wiretapped binary erasure channel from Alice
to Bob and Eve. In addition to the privacy goals of oblivious transfer between
Alice and Bob, we require privacy of Alice and Bob's private data from Eve. In
previous work we derived the OT capacity (in the honest-but-curious model) of
the wiretapped binary independent erasure channel where the erasure processes
of Bob and Eve are independent. Here we derive a lower bound on the OT capacity
in the same secrecy model when the wiretapped binary erasure channel is
degraded in favour of Bob.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04510</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04510</id><created>2015-04-17</created><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Zhou</keyname><forenames>Jieren</forenames></author><author><keyname>Liu</keyname><forenames>Tianci</forenames></author><author><keyname>Shao</keyname><forenames>Lu</forenames></author><author><keyname>Yan</keyname><forenames>Huiya</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Jiang</keyname><forenames>Changjun</forenames></author></authors><title>General Capacity for Deterministic Dissemination in Wireless Ad Hoc
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study capacity scaling laws of the deterministic
dissemination (DD) in random wireless networks under the generalized physical
model (GphyM). This is truly not a new topic. Our motivation to readdress this
issue is two-fold: Firstly, we aim to propose a more general result to unify
the network capacity for general homogeneous random models by investigating the
impacts of different parameters of the system on the network capacity.
Secondly, we target to close the open gaps between the upper and the lower
bounds on the network capacity in the literature. The generality of this work
lies in three aspects: (1) We study the homogeneous random network of a general
node density $\lambda \in [1,n]$, rather than either random dense network (RDN,
$\lambda=n$) or random extended network (REN, $\lambda=1$) as in the
literature. (2) We address the general deterministic dissemination sessions,
\ie, the general multicast sessions, which unify the capacities for unicast and
broadcast sessions by setting the number of destinations for each session as a
general value $n_d\in[1,n]$. (3) We allow the number of sessions to change in
the range $n_s\in(1,n]$, instead of assuming that $n_s=\Theta(n)$ as in the
literature. We derive the general upper bounds on the capacity for the
arbitrary case of $(\lambda, n_d, n_s)$ by introducing the Poisson Boolean
model of continuum percolation, and prove that they are tight according to the
existing general lower bounds constructed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04515</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04515</id><created>2015-04-17</created><updated>2015-07-16</updated><authors><author><keyname>Wu</keyname><forenames>Youlong</forenames></author></authors><title>Coding Schemes for Discrete Memoryless Multicast Networks with
  Rate-limited Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coding schemes for discrete memoryless multicast networks with rate-limited
feedback from the receivers and relays to the transmitter are proposed. The
coding schemes are based on block-Markov coding, {joint backward decoding} and
hybrid relaying strategy. In each block, the receivers and relays compress
their channel outputs and send the compression indices to the transmitter
through the feedback links. In the next block, after obtaining the compression
indices, the transmitter sends them together with the source message. Each
receiver uses backward decoding to jointly decode the source message and all
compression indices. Our coding schemes generalize Gabbai and Bross's results
for the single relay channel with partial feedback, where they proposed coding
schemes based on restricted decoding and deterministic partitioning. It is
shown that for the single relay channel with relay-transmitter feedback, our
coding schemes can strictly improve on noisy network coding, distributed
decode-forward coding and all known lower bounds on the achievable rate in the
absence of feedback. Furthermore, motivated by the feedback coding schemes, we
propose a new coding scheme for discrete memoryless multicast networks without
feedback, which also improves noisy network coding and distributed
decode-forward coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04517</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04517</id><created>2015-04-17</created><authors><author><keyname>Varloot</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Bouillard</keyname><forenames>Anne</forenames></author></authors><title>Speeding up Glauber Dynamics for Random Generation of Independent Sets</title><categories>cs.DM math.PR</categories><acm-class>G.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum independent set (MIS) problem is a well-studied combinatorial
optimization problem that naturally arises in many applications, such as
wireless communication, information theory and statistical mechanics.
  MIS problem is NP-hard, thus many results in the literature focus on fast
generation of maximal independent sets of high cardinality. One possibility is
to combine Gibbs sampling with coupling from the past arguments to detect
convergence to the stationary regime. This results in a sampling procedure with
time complexity that depends on the mixing time of the Glauber dynamics Markov
chain.
  We propose an adaptive method for random event generation in the Glauber
dynamics that considers only the events that are effective in the coupling from
the past scheme, accelerating the convergence time of the Gibbs sampling
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04531</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04531</id><created>2015-04-17</created><authors><author><keyname>Loncan</keyname><forenames>Laetitia</forenames></author><author><keyname>Almeida</keyname><forenames>Luis B.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Briottet</keyname><forenames>Xavier</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Fabre</keyname><forenames>Sophie</forenames></author><author><keyname>Liao</keyname><forenames>Wenzhi</forenames></author><author><keyname>Licciardi</keyname><forenames>Giorgio A.</forenames></author><author><keyname>Sim&#xf5;es</keyname><forenames>Miguel</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Veganzones</keyname><forenames>Miguel A.</forenames></author><author><keyname>Vivone</keyname><forenames>Gemine</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Yokoya</keyname><forenames>Naoto</forenames></author></authors><title>Hyperspectral pansharpening: a review</title><categories>cs.CV physics.data-an stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pansharpening aims at fusing a panchromatic image with a multispectral one,
to generate an image with the high spatial resolution of the former and the
high spectral resolution of the latter. In the last decade, many algorithms
have been presented in the literature for pansharpening using multispectral
data. With the increasing availability of hyperspectral systems, these methods
are now being adapted to hyperspectral images. In this work, we compare new
pansharpening techniques designed for hyperspectral data with some of the state
of the art methods for multispectral pansharpening, which have been adapted for
hyperspectral data. Eleven methods from different classes (component
substitution, multiresolution analysis, hybrid, Bayesian and matrix
factorization) are analyzed. These methods are applied to three datasets and
their effectiveness and robustness are evaluated with widely used performance
indicators. In addition, all the pansharpening techniques considered in this
paper have been implemented in a MATLAB toolbox that is made available to the
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04532</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04532</id><created>2015-04-17</created><updated>2016-01-27</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>Highest Trees of Random Mappings</title><categories>math.PR cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the exact asymptotic
$1-\left({\frac{2\pi}{3}-\frac{827}{288\pi}}+o(1)\right)/{\sqrt{n}}$ for the
probability that the underlying graph of a random mapping of $n$ elements
possesses a unique highest tree. The property of having a unique highest tree
turned out to be crucial in the solution of the famous Road Coloring Problem as
well as the generalization of this property in the proof of the author's result
about the probability of being synchronizable for a random automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04540</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04540</id><created>2015-04-17</created><updated>2015-05-05</updated><authors><author><keyname>Jacobsson</keyname><forenames>Sven</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Gustavsson</keyname><forenames>Ulf</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>One-Bit Massive MIMO: Channel Estimation and High-Order Modulations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the information-theoretic throughout achievable on a fading
communication link when the receiver is equipped with one-bit analog-to-digital
converters (ADCs). The analysis is conducted for the setting where neither the
transmitter nor the receiver have a priori information on the realization of
the fading channels. This means that channel-state information needs to be
acquired at the receiver on the basis of the one-bit quantized channel outputs.
We show that least-squares (LS) channel estimation combined with joint pilot
and data processing is capacity achieving in the single-user,
single-receive-antenna case.
  We also investigate the achievable uplink throughput in a massive
multiple-input multiple-output system where each element of the antenna array
at the receiver base-station feeds a one-bit ADC. We show that LS channel
estimation and maximum-ratio combining are sufficient to support both multiuser
operation and the use of high-order constellations. This holds in spite of the
severe nonlinearity introduced by the one-bit ADCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04541</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04541</id><created>2015-04-17</created><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames></author><author><keyname>Haddad</keyname><forenames>Serge</forenames></author><author><keyname>Picaronny</keyname><forenames>Claudine</forenames></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames></author><author><keyname>Sassolas</keyname><forenames>Mathieu</forenames></author></authors><title>Polynomial Interrupt Timed Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interrupt Timed Automata (ITA) form a subclass of stopwatch automata where
reachability and some variants of timed model checking are decidable even in
presence of parameters. They are well suited to model and analyze real-time
operating systems. Here we extend ITA with polynomial guards and updates,
leading to the class of polynomial ITA (PolITA). We prove the decidability of
the reachability and model checking of a timed version of CTL by an adaptation
of the cylindrical decomposition method for the first-order theory of reals.
Compared to previous approaches, our procedure handles parameters and clocks in
a unified way. Moreover, we show that PolITA are incomparable with stopwatch
automata. Finally additional features are introduced while preserving
decidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04548</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04548</id><created>2015-04-17</created><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Cusano</keyname><forenames>Claudio</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>Color Constancy Using CNNs</title><categories>cs.CV</categories><comments>Accepted at DeepVision: Deep Learning in Computer Vision 2015 (CVPR
  2015 workshop)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we describe a Convolutional Neural Network (CNN) to accurately
predict the scene illumination. Taking image patches as input, the CNN works in
the spatial domain without using hand-crafted features that are employed by
most previous methods. The network consists of one convolutional layer with max
pooling, one fully connected layer and three output nodes. Within the network
structure, feature learning and regression are integrated into one optimization
process, which leads to a more effective model for estimating scene
illumination. This approach achieves state-of-the-art performance on a standard
dataset of RAW images. Preliminary experiments on images with spatially varying
illumination demonstrate the stability of the local illuminant estimation
ability of our CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04553</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04553</id><created>2015-04-16</created><updated>2016-02-07</updated><authors><author><keyname>Gutierrez</keyname><forenames>Ismael</forenames></author><author><keyname>Molina</keyname><forenames>Ivan</forenames></author></authors><title>Some constructions of cyclic and quasi-cyclic subspaces codes</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct, using GAP System for Computational Discrete
Algebra, some cyclic subspace codes, specially an optimal code over the finite
field F_{2^{{10}}}. Further we present a definition and an example of the
$q$-analogous of a $m$-quasi-cyclic subspace code over F_{2^{{8}}}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04558</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04558</id><created>2015-04-17</created><authors><author><keyname>You</keyname><forenames>Quanzeng</forenames></author><author><keyname>Bhatia</keyname><forenames>Sumit</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>A Picture Tells a Thousand Words -- About You! User Interest Profiling
  from User Generated Visual Content</title><categories>cs.SI cs.IR cs.MM</categories><comments>7 pages, 6 Figures, 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference of online social network users' attributes and interests has been
an active research topic. Accurate identification of users' attributes and
interests is crucial for improving the performance of personalization and
recommender systems. Most of the existing works have focused on textual content
generated by the users and have successfully used it for predicting users'
interests and other identifying attributes. However, little attention has been
paid to user generated visual content (images) that is becoming increasingly
popular and pervasive in recent times. We posit that images posted by users on
online social networks are a reflection of topics they are interested in and
propose an approach to infer user attributes from images posted by them. We
analyze the content of individual images and then aggregate the image-level
knowledge to infer user-level interest distribution. We employ image-level
similarity to propagate the label information between images, as well as
utilize the image category information derived from the user created
organization structure to further propagate the category-level knowledge for
all images. A real life social network dataset created from Pinterest is used
for evaluation and the experimental results demonstrate the effectiveness of
our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04564</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04564</id><created>2015-04-17</created><updated>2015-08-05</updated><authors><author><keyname>Boku</keyname><forenames>Dereje Kifle</forenames></author><author><keyname>Fieker</keyname><forenames>Claus</forenames></author><author><keyname>Decker</keyname><forenames>Wolfram</forenames></author><author><keyname>Steenpass</keyname><forenames>Andreas</forenames></author></authors><title>Gr\&quot;obner Bases over Algebraic Number Fields</title><categories>math.AC cs.SC math.NT</categories><comments>16 pages, 1 figure, 1 table</comments><msc-class>13P10 (Primary), 11R09, 12F05 (Secondary)</msc-class><journal-ref>Proceedings of the 2015 International Workshop on Parallel
  Symbolic Computation, Bath. ACM (2015), 16-24 (J.-G. Dumas, E.L. Kaltofen)</journal-ref><doi>10.1145/2790282.2790284</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Buchberger's algorithm, in theory, allows us to compute Gr\&quot;obner
bases over any field, in practice, however, the computational efficiency
depends on the arithmetic of the ground field. Consider a field $K =
\mathbb{Q}(\alpha)$, a simple extension of $\mathbb{Q}$, where $\alpha$ is an
algebraic number, and let $f \in \mathbb{Q}[t]$ be the minimal polynomial of
$\alpha$. In this paper we present a new efficient method to compute Gr\&quot;obner
bases in polynomial rings over the algebraic number field $K$. Starting from
the ideas of Noro [Noro, 2006], we proceed by joining $f$ to the ideal to be
considered, adding $t$ as an extra variable. But instead of avoiding
superfluous S-pair reductions by inverting algebraic numbers, we achieve the
same goal by applying modular methods as in [Arnold, 2003; B\&quot;ohm et al., 2015;
Idrees et al., 2011], that is, by inferring information in characteristic zero
from information in characteristic $p &gt; 0$. For suitable primes $p$, the
minimal polynomial $f$ is reducible over $\mathbb{F}_p$. This allows us to
apply modular methods once again, on a second level, with respect to the
factors of $f$. The algorithm thus resembles a divide and conquer strategy and
is in particular easily parallelizable. At current state, the algorithm is
probabilistic in the sense that, as for other modular Gr\&quot;obner basis
computations, an effective final verification test is only known for
homogeneous ideals or for local monomial orderings. The presented timings show
that for most examples, our algorithm, which has been implemented in SINGULAR,
outperforms other known methods by far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04565</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04565</id><created>2015-04-17</created><authors><author><keyname>Pe&#xf1;aranda</keyname><forenames>Luis</forenames></author><author><keyname>Velho</keyname><forenames>Luiz</forenames></author><author><keyname>Sacht</keyname><forenames>Leonardo</forenames></author></authors><title>Real-time correction of panoramic images using hyperbolic M\&quot;obius
  transformations</title><categories>cs.GR</categories><comments>25 pages, 13 figures, 1 table, 1 algorithm. In &quot;Real-time Image
  Processing&quot;</comments><doi>10.1007/s11554-015-0502-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide-angle images gained a huge popularity in the last years due to the
development of computational photography and imaging technological advances.
They present the information of a scene in a way which is more natural for the
human eye but, on the other hand, they introduce artifacts such as bent lines.
These artifacts become more and more unnatural as the field of view increases.
  In this work, we present a technique aimed to improve the perceptual quality
of panorama visualization. The main ingredients of our approach are, on one
hand, considering the viewing sphere as a Riemann sphere, what makes natural
the application of M\&quot;obius (complex) transformations to the input image, and,
on the other hand, a projection scheme which changes in function of the field
of view used.
  We also introduce an implementation of our method, compare it against images
produced with other methods and show that the transformations can be done in
real-time, which makes our technique very appealing for new settings, as well
as for existing interactive panorama applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04579</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04579</id><created>2015-04-17</created><authors><author><keyname>Nabi</keyname><forenames>Syed Waqar</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>An Intermediate Language and Estimator for Automated Design Space
  Exploration on FPGAs</title><categories>cs.DC</categories><comments>Pre-print and extended version of poster paper accepted at
  international symposium on Highly Efficient Accelerators and Reconfigurable
  Technologies (HEART2015) Boston, MA, USA, June 1-2, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the TyTra-IR, a new intermediate language intended as a
compilation target for high-level language compilers and a front-end for HDL
code generators. We develop the requirements of this new language based on the
design-space of FPGAs that it should be able to express and the
estimation-space in which each configuration from the design-space should be
mappable in an automated design flow. We use a simple kernel to illustrate
multiple configurations using the semantics of TyTra-IR. The key novelty of
this work is the cost model for resource-costs and throughput for different
configurations of interest for a particular kernel. Through the realistic
example of a Successive Over-Relaxation kernel implemented both in TyTra-IR and
HDL, we demonstrate both the expressiveness of the IR and the accuracy of our
cost model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04586</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04586</id><created>2015-04-17</created><authors><author><keyname>Nabi</keyname><forenames>Syed Waqar</forenames></author><author><keyname>Hameed</keyname><forenames>Saji N.</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>A Reconfigurable Vector Instruction Processor for Accelerating a
  Convection Parametrization Model on FPGAs</title><categories>cs.AR</categories><comments>This is an extended pre-print version of work that was presented at
  the international symposium on Highly Efficient Accelerators and
  Reconfigurable Technologies (HEART2014), Sendai, Japan, June 911, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Performance Computing (HPC) platforms allow scientists to model
computationally intensive algorithms. HPC clusters increasingly use
General-Purpose Graphics Processing Units (GPGPUs) as accelerators; FPGAs
provide an attractive alternative to GPGPUs for use as co-processors, but they
are still far from being mainstream due to a number of challenges faced when
using FPGA-based platforms. Our research aims to make FPGA-based high
performance computing more accessible to the scientific community. In this work
we present the results of investigating the acceleration of a particular
atmospheric model, Flexpart, on FPGAs. We focus on accelerating the most
computationally intensive kernel from this model. The key contribution of our
work is the architectural exploration we undertook to arrive at a solution that
best exploits the parallelism available in the legacy code, and is also
convenient to program, so that eventually the compilation of high-level legacy
code to our architecture can be fully automated. We present the three different
types of architecture, comparing their resource utilization and performance,
and propose that an architecture where there are a number of computational
cores, each built along the lines of a vector instruction processor, works best
in this particular scenario, and is a promising candidate for a generic
FPGA-based platform for scientific computation. We also present the results of
experiments done with various configuration parameters of the proposed
architecture, to show its utility in adapting to a range of scientific
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04588</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04588</id><created>2015-04-17</created><authors><author><keyname>Goulet</keyname><forenames>James-A.</forenames></author></authors><title>The Nataf-Beta Random Field Classifier: An Extension of the Beta
  Conjugate Prior to Classification Problems</title><categories>cs.LG</categories><comments>17 pages, 4 figures, Submitted for publication in the Journal of
  Machine Learning Research</comments><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Nataf-Beta Random Field Classifier, a discriminative
approach that extends the applicability of the Beta conjugate prior to
classification problems. The approach's key feature is to model the probability
of a class conditional on attribute values as a random field whose marginals
are Beta distributed, and where the parameters of marginals are themselves
described by random fields. Although the classification accuracy of the
approach proposed does not statistically outperform the best accuracies
reported in the literature, it ranks among the top tier for the six benchmark
datasets tested. The Nataf-Beta Random Field Classifier is suited as a general
purpose classification approach for real-continuous and real-integer attribute
value problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04596</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04596</id><created>2015-04-17</created><updated>2015-04-20</updated><authors><author><keyname>Zhu</keyname><forenames>Yadong</forenames></author><author><keyname>Lan</keyname><forenames>Yanyan</forenames></author><author><keyname>Guo</keyname><forenames>Jiafeng</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Structural Learning of Diverse Ranking</title><categories>cs.IR</categories><comments>Discriminant Function, Diversity Feature, Learning Framework</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relevance and diversity are both crucial criteria for an effective search
system. In this paper, we propose a unified learning framework for
simultaneously optimizing both relevance and diversity. Specifically, the
problem is formalized as a structural learning framework optimizing
Diversity-Correlated Evaluation Measures (DCEM), such as ERR-IA, a-NDCG and
NRBP. Within this framework, the discriminant function is defined to be a
bi-criteria objective maximizing the sum of the relevance scores and
dissimilarities (or diversity) among the documents. Relevance and diversity
features are utilized to define the relevance scores and dissimilarities,
respectively. Compared with traditional methods, the advantages of our approach
lie in that: (1) Directly optimizing DCEM as the loss function is more
fundamental for the task; (2) Our framework does not rely on explicit diversity
information such as subtopics, thus is more adaptive to real application; (3)
The representation of diversity as the feature-based scoring function is more
flexible to incorporate rich diversity-based features into the learning
framework. Extensive experiments on the public TREC datasets show that our
approach significantly outperforms state-of-the-art diversification approaches,
which validate the above advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04599</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04599</id><created>2015-04-17</created><authors><author><keyname>Bhattacharya</keyname><forenames>Bhaswar B.</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Testing Closeness With Unequal Sized Samples</title><categories>cs.LG cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of closeness testing for two discrete distributions
in the practically relevant setting of \emph{unequal} sized samples drawn from
each of them. Specifically, given a target error parameter $\varepsilon &gt; 0$,
$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from
an unknown distribution $q$, we describe a test for distinguishing the case
that $p=q$ from the case that $||p-q||_1 \geq \varepsilon$. If $p$ and $q$ are
supported on at most $n$ elements, then our test is successful with high
probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =
\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt
n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this
range, to constant factors. These results extend the recent work of Chan et al.
who established the sample complexity when the two samples have equal sizes,
and tightens the results of Acharya et al. by polynomials factors in both $n$
and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the
mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses
$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a &quot;next node&quot; oracle, improving upon
the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we
note that the core of our testing algorithm is a relatively simple statistic
that seems to perform well in practice, both on synthetic data and on natural
language data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04615</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04615</id><created>2015-04-17</created><authors><author><keyname>Lashgari</keyname><forenames>Sina</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Avestimehr</keyname><forenames>Salman</forenames></author></authors><title>MISO Broadcast Channel with Hybrid CSIT: Beyond Two Users</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory; shorter version
  will be presented at IEEE ISIT, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of heterogeneity of channel-state-information available
at the transmitters (CSIT) on the capacity of broadcast channels with a
multiple-antenna transmitter and $k$ single-antenna receivers (MISO BC). In
particular, we consider the $k$-user MISO BC, where the CSIT with respect to
each receiver can be either instantaneous/perfect, delayed, or not available;
and we study the impact of this heterogeneity of CSIT on the degrees-of-freedom
(DoF) of such network. We first focus on the $3$-user MISO BC; and we
completely characterize the DoF region for all possible heterogeneous CSIT
configurations, assuming linear encoding strategies at the transmitters. The
result shows that the state-of-the-art achievable schemes in the literature are
indeed sum-DoF optimal, when restricted to linear encoding schemes. To prove
the result, we develop a novel bound, called Interference Decomposition Bound,
which provides a lower bound on the interference dimension at a receiver which
supplies delayed CSIT based on the average dimension of constituents of that
interference, thereby decomposing the interference into its individual
components. Furthermore, we extend our outer bound on the DoF region to the
general $k$-user MISO BC, and demonstrate that it leads to an approximate
characterization of linear sum-DoF to within an additive gap of $0.5$ for a
broad range of CSIT configurations. Moreover, for the special case where only
one receiver supplies delayed CSIT, we completely characterize the linear
sum-DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04616</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04616</id><created>2015-04-17</created><authors><author><keyname>Chikhi</keyname><forenames>Rayan</forenames></author><author><keyname>Medvedev</keyname><forenames>Paul</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author><author><keyname>Raskhodnikova</keyname><forenames>Sofya</forenames></author></authors><title>On the readability of overlap digraphs</title><categories>cs.DM cs.DS math.CO q-bio.GN</categories><comments>This is a full version of a conference paper of the same title at the
  26th Annual Symposium on Combinatorial Pattern Matching (CPM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the graph parameter readability and study it as a function of
the number of vertices in a graph. Given a digraph D, an injective overlap
labeling assigns a unique string to each vertex such that there is an arc from
x to y if and only if x properly overlaps y. The readability of D is the
minimum string length for which an injective overlap labeling exists. In
applications that utilize overlap digraphs (e.g., in bioinformatics),
readability reflects the length of the strings from which the overlap digraph
is constructed. We study the asymptotic behaviour of readability by casting it
in purely graph theoretic terms (without any reference to strings). We prove
upper and lower bounds on readability for certain graph families and general
graphs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04617</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04617</id><created>2015-04-17</created><updated>2015-05-28</updated><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author></authors><title>Quantum Coding with Finite Resources</title><categories>quant-ph cs.IT math.IT</categories><comments>v2: 24 pages, 6 figures, new author, new title, merged with
  arXiv:1504.05376v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum capacity of a memoryless channel is often used as a single figure
of merit to characterize its ability to transmit quantum information
coherently. The capacity determines the maximal rate at which we can code
reliably over asymptotically many uses of the channel. We argue that this
asymptotic treatment is insufficient to the point of being irrelevant in the
quantum setting where decoherence severely limits our ability to manipulate
large quantum systems in the encoder and decoder. For all practical purposes we
should instead focus on the trade-off between three parameters: the rate of the
code, the number of coherent uses of the channel, and the fidelity of the
transmission. The aim is then to specify the region determined by allowed
combinations of these parameters. Towards this goal, we find approximate and
exact characterizations of the region of allowed triplets for the qubit
dephasing channel and for the erasure channel with classical post-processing
assistance. In each case the region is parametrized by a second channel
parameter, the quantum channel dispersion. In the process we also develop
several general inner (achievable) and outer (converse) bounds on the coding
region that are valid for all finite-dimensional quantum channels and can be
computed efficiently. Applied to the depolarizing channel, this allows us to
determine a lower bound on the number of coherent uses of the channel necessary
to witness super-additivity of the coherent information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04640</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04640</id><created>2015-04-17</created><updated>2015-04-29</updated><authors><author><keyname>Dice</keyname><forenames>Dave</forenames></author><author><keyname>Harris</keyname><forenames>Tim</forenames></author><author><keyname>Kogan</keyname><forenames>Alex</forenames></author><author><keyname>Lev</keyname><forenames>Yossi</forenames></author></authors><title>The Influence of Malloc Placement on TSX Hardware Transactional Memory</title><categories>cs.OS</categories><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hardware transactional memory (HTM) implementation in Intel's i7-4770
&quot;Haswell&quot; processor tracks the transactional read-set in the L1 (level-1), L2
(level-2) and L3 (level-3) caches and the write-set in the L1 cache.
Displacement or eviction of read-set entries from the cache hierarchy or
write-set entries from the L1 results in abort. We show that the placement
policies of dynamic storage allocators -- such as those found in common
&quot;malloc&quot; implementations -- can influence the L1 conflict miss rate in the L1.
Conflict misses -- sometimes called mapping misses -- arise because of less
than ideal associativity and represent imbalanced distribution of active memory
blocks over the set of available L1 indices. Under transactional execution
conflict misses may manifest as aborts, representing wasted or futile effort
instead of a simple stall as would occur in normal execution mode.
  Furthermore, when HTM is used for transactional lock elision (TLE),
persistent aborts arising from conflict misses can force the offending thread
through the so-called &quot;slow path&quot;. The slow path is undesirable as the thread
must acquire the lock and run the critical section in normal execution mode,
precluding the concurrent execution of threads in the &quot;fast path&quot; that monitor
that same lock and run their critical sections in transactional mode. For a
given lock, multiple threads can concurrently use the transactional fast path,
but at most one thread can use the non-transactional slow path at any given
time. Threads in the slow path preclude safe concurrent fast path execution.
Aborts rising from placement policies and L1 index imbalance can thus result in
loss of concurrency and reduced aggregate throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04644</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04644</id><created>2015-04-17</created><authors><author><keyname>Danjuma</keyname><forenames>Kwetishe Joro</forenames></author><author><keyname>Onimode</keyname><forenames>Bayo Mohammed</forenames></author><author><keyname>Onche</keyname><forenames>Ochedikwu Jonah</forenames></author></authors><title>Gender Issues &amp; Information Communication Technology for Development
  (ICT4D): Prospects and Challenges for Women in Nigeria</title><categories>cs.CY</categories><comments>8 pages, 10 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information and Communication Technology is a compendium of interrelated
applications, products and services that can either deepen or alter gender
equality. It has successfully transformed education, businesses, healthcare,
entertainment, politics and good governance within the Global North; providing
equitable access to developmental framework driven by ICTs. However, in
drafting the core developmental objectives of sustainable development, there
has been considerable gender digital divide limiting women access to resources
based on their gender, ethnicity, socio-cultural bias and the rights to utilize
such resources for development. In realization of the United Nation Millennium
Development Goals within the Global South specifically Nigeria, women are often
marginalized or excluded from ICT policy drafting and imbalance associated with
ICT. This paper identifies and evaluates gender issues and information
communication technology, with focus on the challenges and prospects for women
empowerment in Nigeria. The study critically examined research literatures and
conducted research survey on the prospects and challenges of promoting gender
equality and women empowerment through ICTs; and identifies policy implication
for Nigeria. The research survey used a random sampling technique with a target
sample size of eighty respondents. Data gathered from the questionnaire was
analyzed using Statistical Package for Social Science version 19, and the
result was presented using ANOVA, and descriptive analysis. The study reveal
gender inclusiveness in policy drafting as a key driver for socio-economic
development, improved healthcare and women empowerment in Nigeria. We recommend
a deliberate ICT policy that attract and encourages women participation in ICT
developmental framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04646</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04646</id><created>2015-04-17</created><authors><author><keyname>Danjuma</keyname><forenames>Kwetishe Joro</forenames></author></authors><title>Performance Evaluation of Machine Learning Algorithms in Post-operative
  Life Expectancy in the Lung Cancer Patients</title><categories>cs.LG</categories><comments>11 pages, 3 figures, 2 tables, ISSN (Print): 1694-0814 | ISSN
  (Online): 1694-0784</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nature of clinical data makes it difficult to quickly select, tune and
apply machine learning algorithms to clinical prognosis. As a result, a lot of
time is spent searching for the most appropriate machine learning algorithms
applicable in clinical prognosis that contains either binary-valued or
multi-valued attributes. The study set out to identify and evaluate the
performance of machine learning classification schemes applied in clinical
prognosis of post-operative life expectancy in the lung cancer patients.
Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train
and test models on Thoracic Surgery datasets obtained from the University of
California Irvine machine learning repository. Stratified 10-fold
cross-validation was used to evaluate baseline performance accuracy of the
classifiers. The comparative analysis shows that multilayer perceptron
performed best with classification accuracy of 82.3%, J48 came out second with
classification accuracy of 81.8%, and Naive Bayes came out the worst with
classification accuracy of 74.4%. The quality and outcome of the chosen machine
learning algorithms depends on the ingenuity of the clinical miner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04650</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04650</id><created>2015-04-17</created><updated>2015-11-09</updated><authors><author><keyname>Jansen</keyname><forenames>Klaus</forenames></author><author><keyname>Kraft</keyname><forenames>Stefan Erich Julius</forenames></author></authors><title>A Faster FPTAS for the Unbounded Knapsack Problem</title><categories>cs.DS</categories><comments>30 pages, pdfLaTeX; typos corrected, additional smaller explanations
  to improve readability and to avoid confusion; full version of paper
  presented at IWOCA 2015, reviewer comments were taken into account</comments><acm-class>G.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unbounded Knapsack Problem (UKP) is a well-known variant of the famous
0-1 Knapsack Problem (0-1 KP). In contrast to 0-1 KP, an arbitrary number of
copies of every item can be taken in UKP. Since UKP is NP-hard, fully
polynomial time approximation schemes (FPTAS) are of great interest. Such
algorithms find a solution arbitrarily close to the optimum $\mathrm{OPT}(I)$,
i.e. of value at least $(1-\varepsilon) \mathrm{OPT}(I)$ for $\varepsilon &gt; 0$,
and have a running time polynomial in the input length and
$\frac{1}{\varepsilon}$. For over thirty years, the best FPTAS was due to
Lawler with a running time in $O(n + \frac{1}{\varepsilon^3})$ and a space
complexity in $O(n + \frac{1}{\varepsilon^2})$, where $n$ is the number of
knapsack items. We present an improved FPTAS with a running time in $O(n +
\frac{1}{\varepsilon^2} \log^3 \frac{1}{\varepsilon})$ and a space bound in
$O(n + \frac{1}{\varepsilon} \log^2 \frac{1}{\varepsilon})$. This directly
improves the running time of the fastest known approximation schemes for Bin
Packing and Strip Packing, which have to approximately solve UKP instances as
subproblems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04651</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04651</id><created>2015-04-17</created><authors><author><keyname>Jain</keyname><forenames>Anil K.</forenames></author><author><keyname>Arora</keyname><forenames>Sunpreet S.</forenames></author><author><keyname>Best-Rowden</keyname><forenames>Lacey</forenames></author><author><keyname>Cao</keyname><forenames>Kai</forenames></author><author><keyname>Sudhish</keyname><forenames>Prem Sewak</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Anjoo</forenames></author></authors><title>Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint
  Recognition for Infants and Toddlers</title><categories>cs.CV</categories><comments>Michigan State University Technical Report</comments><report-no>MSU-CSE-15-7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a number of emerging applications requiring biometric recognition of
children (e.g., tracking child vaccination schedules, identifying missing
children and preventing newborn baby swaps in hospitals), investigating the
temporal stability of biometric recognition accuracy for children is important.
The persistence of recognition accuracy of three of the most commonly used
biometric traits (fingerprints, face and iris) has been investigated for
adults. However, persistence of biometric recognition accuracy has not been
studied systematically for children in the age group of 0-4 years. Given that
very young children are often uncooperative and do not comprehend or follow
instructions, in our opinion, among all biometric modalities, fingerprints are
the most viable for recognizing children. This is primarily because it is
easier to capture fingerprints of young children compared to other biometric
traits, e.g., iris, where a child needs to stare directly towards the camera to
initiate iris capture. In this report, we detail our initiative to investigate
the persistence of fingerprint recognition for children in the age group of 0-4
years. Based on preliminary results obtained for the data collected in the
first phase of our study, use of fingerprints for recognition of 0-4 year-old
children appears promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04654</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04654</id><created>2015-04-17</created><authors><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author><author><keyname>Lim</keyname><forenames>Taehyung J.</forenames></author></authors><title>Information without rolling dice</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deterministic notions of capacity and entropy are studied in the context
of communication and storage of information using square-integrable,
bandlimited signals subject to perturbation. The $(\epsilon,\delta)$-capacity,
that extends the Kolmogorov $\epsilon$-capacity to packing sets of overlap at
most $\delta$, is introduced and compared to the Shannon capacity. The
functional form of the results indicates that in both Kolmogorov and Shannon's
settings, capacity and entropy grow linearly with the number of degrees of
freedom, but only logarithmically with the signal to noise ratio. This basic
insight transcends the details of the stochastic or deterministic description
of the information-theoretic model. For $\delta=0$, the analysis leads to new
bounds on the Kolmogorov $\epsilon$-capacity, and to a tight asymptotic
expression of the Kolmogorov $\epsilon$-entropy of bandlimited signals. A
deterministic notion of error exponent is introduced. Applications of the
theory are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04658</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04658</id><created>2015-04-17</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author><author><keyname>Roma</keyname><forenames>Gerard</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Deep Karaoke: Extracting Vocals from Musical Mixtures Using a
  Convolutional Deep Neural Network</title><categories>cs.SD cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification and extraction of singing voice from within musical mixtures
is a key challenge in source separation and machine audition. Recently, deep
neural networks (DNN) have been used to estimate 'ideal' binary masks for
carefully controlled cocktail party speech separation problems. However, it is
not yet known whether these methods are capable of generalizing to the
discrimination of voice and non-voice in the context of musical mixtures. Here,
we trained a convolutional DNN (of around a billion parameters) to provide
probabilistic estimates of the ideal binary mask for separation of vocal sounds
from real-world musical mixtures. We contrast our DNN results with more
traditional linear methods. Our approach may be useful for automatic removal of
vocal sounds from musical mixtures for 'karaoke' type applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04660</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04660</id><created>2015-04-17</created><authors><author><keyname>Hurlburt</keyname><forenames>Neal</forenames></author><author><keyname>Jaffey</keyname><forenames>Steve</forenames></author></authors><title>A spectral optical flow method for determining velocities from digital
  imagery</title><categories>cs.CV astro-ph.IM</categories><comments>12 pages, 5 figures. Submitted to Earth Science Informatics</comments><journal-ref>Earth Science Informatics: Volume 8, Issue 4 (2015), Page 959-965</journal-ref><doi>10.1007/s12145-015-0224-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for determining surface flows from solar images based
upon optical flow techniques. We apply the method to sets of images obtained by
a variety of solar imagers to assess its performance. The {\tt opflow3d}
procedure is shown to extract accurate velocity estimates when provided perfect
test data and quickly generates results consistent with completely distinct
methods when applied on global scales. We also validate it in detail by
comparing it to an established method when applied to high-resolution datasets
and find that it provides comparable results without the need to tune, filter
or otherwise preprocess the images before its application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04662</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04662</id><created>2015-04-17</created><updated>2015-06-29</updated><authors><author><keyname>Drager</keyname><forenames>Klaus</forenames><affiliation>Queen Mary, University of London</affiliation></author><author><keyname>Forejt</keyname><forenames>Vojtech</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Parker</keyname><forenames>David</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Ujma</keyname><forenames>Mateusz</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Permissive Controller Synthesis for Probabilistic Systems</title><categories>cs.LO cs.SY math.OC</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:16) 2015</journal-ref><doi>10.2168/LMCS-11(2:16)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel controller synthesis techniques for probabilistic systems
modelled using stochastic two-player games: one player acts as a controller,
the second represents its environment, and probability is used to capture
uncertainty arising due to, for example, unreliable sensors or faulty system
components. Our aim is to generate robust controllers that are resilient to
unexpected system changes at runtime, and flexible enough to be adapted if
additional constraints need to be imposed. We develop a permissive controller
synthesis framework, which generates multi-strategies for the controller,
offering a choice of control actions to take at each time step. We formalise
the notion of permissivity using penalties, which are incurred each time a
possible control action is disallowed by a multi-strategy. Permissive
controller synthesis aims to generate a multi-strategy that minimises these
penalties, whilst guaranteeing the satisfaction of a specified system property.
We establish several key results about the optimality of multi-strategies and
the complexity of synthesising them. Then, we develop methods to perform
permissive controller synthesis using mixed integer linear programming and
illustrate their effectiveness on a selection of case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04663</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04663</id><created>2015-04-17</created><updated>2015-10-20</updated><authors><author><keyname>Zhang</keyname><forenames>Jinxue</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Sun</keyname><forenames>Jingchao</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchao</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author></authors><title>TrueTop: A Sybil-Resilient System for User Influence Measurement on
  Twitter</title><categories>cs.SI</categories><comments>Accepted by IEEE/ACM Transactions on Networking. This is the Final
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influential users have great potential for accelerating information
dissemination and acquisition on Twitter. How to measure the influence of
Twitter users has attracted significant academic and industrial attention.
Existing influential measurement techniques, however, are vulnerable to sybil
users that are thriving on Twitter. Although sybil defenses for online social
networks have been extensively investigated, they commonly assume unique
mappings from human-established trust relationships to online social
associations and thus do not apply to Twitter where users can freely follow
each other. This paper presents TrueTop, the first sybil-resilient system to
measure the influence of Twitter users. TrueTop is firmly rooted in two
observations from real Twitter datasets. First, although non-sybil users may
incautiously follow strangers, they tend to be more careful and selective in
retweeting, replying to, and mentioning other Twitter users. Second,
influential users usually get much more retweets, replies, and mentions than
non-influential users. Detailed theoretical studies and synthetic simulations
show that TrueTop can generate very accurate influence measurement results and
also have strong resilience to sybil attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04666</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04666</id><created>2015-04-17</created><authors><author><keyname>Le</keyname><forenames>Phong</forenames></author><author><keyname>Zuidema</keyname><forenames>Willem</forenames></author></authors><title>Unsupervised Dependency Parsing: Let's Use Supervised Parsers</title><categories>cs.CL cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a self-training approach to unsupervised dependency parsing that
reuses existing supervised and unsupervised parsing algorithms. Our approach,
called `iterated reranking' (IR), starts with dependency trees generated by an
unsupervised parser, and iteratively improves these trees using the richer
probability models used in supervised parsing that are in turn trained on these
trees. Our system achieves 1.8% accuracy higher than the state-of-the-part
parser of Spitkovsky et al. (2013) on the WSJ corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04675</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04675</id><created>2015-04-17</created><updated>2015-09-18</updated><authors><author><keyname>Chen</keyname><forenames>Sitan</forenames></author><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Pseudorandomness for Read-Once, Constant-Depth Circuits</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Boolean functions computed by read-once, depth-$D$ circuits with
unbounded fan-in over the de Morgan basis, we present an explicit pseudorandom
generator with seed length $\tilde{O}(\log^{D+1} n)$. The previous best seed
length known for this model was $\tilde{O}(\log^{D+4} n)$, obtained by Trevisan
and Xue (CCC `13) for all of $AC^0$ (not just read-once). Our work makes use of
Fourier analytic techniques for pseudorandomness introduced by Reingold,
Steinke, and Vadhan (RANDOM `13) to show that the generator of Gopalan et al.
(FOCS `12) fools read-once $AC^0$. To this end, we prove a new Fourier growth
bound for read-once circuits, namely that for every $F: \{0,1\}^n\to\{0,1\}$
computed by a read-once, depth-$D$ circuit,
\begin{equation*}\sum_{s\subseteq[n], |s|=k}|\hat{F}[s]|\le
O(\log^{D-1}n)^k,\end{equation*} where $\hat{F}$ denotes the Fourier transform
of $F$ over $\mathbb{Z}^n_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04679</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04679</id><created>2015-04-17</created><updated>2015-10-09</updated><authors><author><keyname>Guo</keyname><forenames>Longkun</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author><author><keyname>Zhu</keyname><forenames>Wenxing</forenames></author></authors><title>Efficient Approximation Algorithms for Multi-Antennae Largest Weight
  Data Retrieval</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a mobile network, wireless data broadcast over $m$ channels (frequencies)
is a powerful means for distributed dissemination of data to clients who access
the channels through multi-antennae equipped on their mobile devices. The
$\delta$-antennae largest weight data retrieval ($\delta$ALWDR) problem is to
compute a schedule for downloading a subset of data items that has a maximum
total weight using $\delta$ antennae in a given time interval. In this paper,
we propose a ratio $1-\frac{1}{e}-\epsilon$ approximation algorithm for the
$\delta$-antennae largest weight data retrieval ($\delta$ALWDR) problem that
has the same ratio as the known result but a significantly improved time
complexity of $O(2^{\frac{1}{\epsilon}}\frac{1}{\epsilon}m^{7}T^{3.5}L)$ from
$O(\epsilon^{3.5}m^{\frac{3.5}{\epsilon}}T^{3.5}L)$ when $\delta=1$
\cite{lu2014data}. To our knowledge, our algorithm represents the first ratio
$1-\frac{1}{e}-\epsilon$ approximation solution to $\delta$ALWDR for the
general case of arbitrary $\delta$. To achieve this, we first give a ratio
$1-\frac{1}{e}$ algorithm for the $\gamma$-separated $\delta$ALWDR
($\delta$A$\gamma$LWDR) with runtime $O(m^{7}T^{3.5}L)$, under the assumption
that every data item appears at most once in each segment of
$\delta$A$\gamma$LWDR, for any input of maximum length $L$ on $m$ channels in
$T$ time slots. Then, we show that we can retain the same ratio for
$\delta$A$\gamma$LWDR without this assumption at the cost of increased time
complexity to $O(2^{\gamma}m^{7}T^{3.5}L)$. This result immediately yields an
approximation solution of same ratio and time complexity for $\delta$ALWDR,
presenting a significant improvement of the known time complexity of ratio
$1-\frac{1}{e}-\epsilon$ approximation to the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04684</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04684</id><created>2015-04-18</created><updated>2015-10-30</updated><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>A Framework for Robust Assessment of Power Grid Stability and Resiliency</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security assessment of large-scale, strongly nonlinear power grids containing
thousands to millions of interacting components is a computationally expensive
task. Targeting at reducing the computational cost, this paper introduces a
framework for constructing a robust assessment toolbox that can provide
mathematically rigorous certificates of the grids' stability with respect to
the variations in system parameters and the grids' ability to withstand a bunch
sources of faults. By this toolbox we can &quot;off-line&quot; screen a wide range of
contingencies in practice, without reassessing the system stability on a
regular basis. In particular, we formulate and solve two novel robust stability
and resiliency assessment problems of power grids subject to the uncertainty in
equilibrium points and uncertainty in faulton dynamics. Furthermore, we bring
in the quadratic Lyapunov functions approach to transient stability assessment,
offering realtime construction of stability/resiliency certificates and
real-time stability assessment. The effectiveness of the proposed techniques is
numerically illustrated on a number of IEEE test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04686</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04686</id><created>2015-04-18</created><authors><author><keyname>Bassily</keyname><forenames>Raef</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>Local, Private, Efficient Protocols for Succinct Histograms</title><categories>cs.CR cs.DS cs.LG</categories><acm-class>F.2.0</acm-class><doi>10.1145/2746539.2746632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give efficient protocols and matching accuracy lower bounds for frequency
estimation in the local model for differential privacy. In this model,
individual users randomize their data themselves, sending differentially
private reports to an untrusted server that aggregates them.
  We study protocols that produce a succinct histogram representation of the
data. A succinct histogram is a list of the most frequent items in the data
(often called &quot;heavy hitters&quot;) along with estimates of their frequencies; the
frequency of all other items is implicitly estimated as 0.
  If there are $n$ users whose items come from a universe of size $d$, our
protocols run in time polynomial in $n$ and $\log(d)$. With high probability,
they estimate the accuracy of every item up to error
$O\left(\sqrt{\log(d)/(\epsilon^2n)}\right)$ where $\epsilon$ is the privacy
parameter. Moreover, we show that this much error is necessary, regardless of
computational efficiency, and even for the simple setting where only one item
appears with significant frequency in the data set.
  Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for
this task either ran in time $\Omega(d)$ or had much worse error (about
$\sqrt[6]{\log(d)/(\epsilon^2n)}$), and the only known lower bound on error was
$\Omega(1/\sqrt{n})$.
  We also adapt a result of McGregor et al (2010) to the local setting. In a
model with public coins, we show that each user need only send 1 bit to the
server. For all known local protocols (including ours), the transformation
preserves computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04687</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04687</id><created>2015-04-18</created><updated>2015-04-30</updated><authors><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Sampling of graph signals with successive local aggregations</title><categories>cs.SI cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new scheme to sample signals defined in the nodes of a graph is proposed.
The underlying assumption is that such signals admit a sparse representation in
a frequency domain related to the structure of the graph, which is captured by
the so-called graph-shift operator. Most of the works that have looked at this
problem have focused on using the value of the signal observed at a subset of
nodes to recover the signal in the entire graph. Differently, the sampling
scheme proposed here uses as input observations taken at a single node. The
observations correspond to sequential applications of the graph-shift operator,
which are linear combinations of the information gathered by the neighbors of
the node. When the graph corresponds to a directed cycle (which is the support
of time-varying signals), our method is equivalent to the classical sampling in
the time domain. When the graph is more general, we show that the Vandermonde
structure of the sampling matrix, which is critical to guarantee recovery when
sampling time-varying signals, is preserved. Sampling and interpolation are
analyzed first in the absence of noise and then noise is considered. We then
study the recovery of the sampled signal when the specific set of frequencies
that is active is not known. Moreover, we present a more general sampling
scheme, under which, either our aggregation approach or the alternative
approach of sampling a graph signal by observing the value of the signal at a
subset of nodes can be both viewed as particular cases. The last part of the
paper presents numerical experiments that illustrate the results developed
through both synthetic graph signals and a real-world graph of the economy of
the United States.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04688</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04688</id><created>2015-04-18</created><authors><author><keyname>Niemeyer</keyname><forenames>Richard E.</forenames></author><author><keyname>Niemeyer</keyname><forenames>Robert G.</forenames></author></authors><title>Generating upward sweeps in population using the Turchin--Korotayev
  model</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 13 figures. Contains Matlab code for those interested in
  adapting and/or extending the model. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The works of [Cha-DunAlvInoNieCarFieLaw,Cha-Dun] describe upward sweeps in
populations of city-states and attempt to characterize such phenomenon. The
model proposed in both [TurKor,Tur] describes how the population, state
resources and internal conflict influence each other over time. We show that
one can obtain an upward sweep in the population by altering particular
parameters of the system of differential equations constituting the model given
in [TurKor,Tur]. Moreover, we show that such a system has a unstable critical
point and propose an approach for determining bifurcation points in the
parameter space for the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04690</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04690</id><created>2015-04-18</created><authors><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Skop</keyname><forenames>Eyal E.</forenames></author></authors><title>Efficient Vertex-Label Distance Oracles for Planar Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distance queries in vertex labeled planar graphs. For any fixed
$0 &lt; \epsilon \leq 1/2$ we show how to preprocess a planar graph with vertex
labels and edge lengths into a data structure that answers queries of the
following form. Given a vertex $u$ and a label $\lambda$ return a
$(1+O(\epsilon))$-approximation of the distance between $u$ and its closest
vertex with label $\lambda$. For an undirected $n$-vertex planar graph the
preprocessing time is $O(\epsilon^{-2}n\lg^{3}{n})$, the size is
$O(\epsilon^{-1}n\lg{n})$, and the query time is $O(\lg\lg{n} +
\epsilon^{-1})$. For a directed planar graph with arc lengths bounded by $N$,
the preprocessing time is $O(\epsilon^{-2}n\lg^{3}{n}\lg(nN))$, the data
structure size is $O(\epsilon^{-1}n\lg{n}\lg(nN))$, and the query time is
$O(\lg\lg{n}\lg\lg(nN) + \epsilon^{-1})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04697</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04697</id><created>2015-04-18</created><authors><author><keyname>Liu</keyname><forenames>Hongwu</forenames></author><author><keyname>Kim</keyname><forenames>Kyeong Jin</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Power Splitting for Full-Duplex Relay with Wireless Information and
  Power Transfer</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1503.04381</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates power splitting for full-duplex relay networks with
wireless information and energy transfer. By applying power splitting as a
relay transceiver architecture, the full duplex information relaying can be
powered by energy harvested from the source-emitted radio frequency signal. In
order to minimize outage probability, power splitting ratios have been
dynamically optimized according to full channel state information (CSI) and
partial CSI, respectively. Under strong loop interference, the proposed full
CSI-based and partial CSI-based power splitting schemes achieve the better
outage performance than the fixed power splitting scheme, whereas the partial
CSI-based power splitting scheme can ensure competitive outage performance
without requiring CSI of the second-hop link. It is also observed that the
worst outage performance is achieved when the relay is located midway between
the source and destination, whereas the outage performance of partial CSI-based
power splitting scheme approaches that of full CSI-based scheme when the relay
is placed close to the destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04708</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04708</id><created>2015-04-18</created><updated>2015-07-20</updated><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Mundhenk</keyname><forenames>Martin</forenames></author></authors><title>The model checking fingerprints of CTL operators</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study is to understand the inherent expressive power of CTL
operators. We investigate the complexity of model checking for all CTL
fragments with one CTL operator and arbitrary Boolean operators. This gives us
a fingerprint of each CTL operator. The comparison between the fingerprints
yields a hierarchy of the operators that mirrors their strength with respect to
model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04712</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04712</id><created>2015-04-18</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Liakata</keyname><forenames>Maria</forenames></author><author><keyname>Procter</keyname><forenames>Rob</forenames></author><author><keyname>Bontcheva</keyname><forenames>Kalina</forenames></author><author><keyname>Tolmie</keyname><forenames>Peter</forenames></author></authors><title>Towards Detecting Rumours in Social Media</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of false rumours during emergencies can jeopardise the well-being
of citizens as they are monitoring the stream of news from social media to stay
abreast of the latest updates. In this paper, we describe the methodology we
have developed within the PHEME project for the collection and sampling of
conversational threads, as well as the tool we have developed to facilitate the
annotation of these threads so as to identify rumourous ones. We describe the
annotation task conducted on threads collected during the 2014 Ferguson unrest
and we present and analyse our findings. Our results show that we can collect
effectively social media rumours and identify multiple rumours associated with
a range of stories that would have been hard to identify by relying on existing
techniques that need manual input of rumour-specific keywords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04714</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04714</id><created>2015-04-18</created><authors><author><keyname>Jacquelin</keyname><forenames>Mathias</forenames></author><author><keyname>Lin</keyname><forenames>Lin</forenames></author><author><keyname>Wichmann</keyname><forenames>Nathan</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author></authors><title>Enhancing the scalability and load balancing of the parallel selected
  inversion algorithm via tree-based asynchronous communication</title><categories>cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method for improving the parallel scalability of the recently
developed parallel selected inversion algorithm [Jacquelin, Lin and Yang 2014],
named PSelInv, on massively parallel distributed memory machines. In the
PSelInv method, we compute selected elements of the inverse of a sparse matrix
A that can be decomposed as A = LU, where L is lower triangular and U is upper
triangular. Updating these selected elements of A-1 requires restricted
collective communications among a subset of processors within each column or
row communication group created by a block cyclic distribution of L and U. We
describe how this type of restricted collective communication can be
implemented by using asynchronous point-to-point MPI communication functions
combined with a binary tree based data propagation scheme. Because multiple
restricted collective communications may take place at the same time in the
parallel selected inversion algorithm, we need to use a heuristic to prevent
processors participating in multiple collective communications from receiving
too many messages. This heuristic allows us to reduce communication load
imbalance and improve the overall scalability of the selected inversion
algorithm. For instance, when 6,400 processors are used, we observe over 5x
speedup for test matrices. It also mitigates the performance variability
introduced by an inhomogeneous network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04715</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04715</id><created>2015-04-18</created><authors><author><keyname>Konstantinidis</keyname><forenames>Stavros</forenames></author><author><keyname>Meijer</keyname><forenames>Casey</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Symbolic Manipulation of Code Properties</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FAdo system is a symbolic manipulator of formal languages objects,
implemented in Python. In this work, we extend its capabilities by implementing
methods to manipulate transducers and we go one level higher than existing
formal language systems and implement methods to manipulate objects
representing classes of independent languages (widely known as code
properties). Our methods allow users to define their own code properties and
combine them between themselves or with fixed properties such as prefix codes,
suffix codes, error detecting codes, etc. The satisfaction and maximality
decision questions are solvable for any of the definable properties. The new
online system LaSer allows to query about code properties and obtain the answer
in a batch mode. Our work is founded on independence theory as well as the
theory of rational relations and transducers and contributes with improveded
algorithms on these objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04716</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04716</id><created>2015-04-18</created><authors><author><keyname>Shukla</keyname><forenames>Vishal</forenames></author></authors><title>Gap Analysis of Natural Language Processing Systems with respect to
  Linguistic Modality</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modality is one of the important components of grammar in linguistics. It
lets speaker to express attitude towards, or give assessment or potentiality of
state of affairs. It implies different senses and thus has different
perceptions as per the context. This paper presents an account showing the gap
in the functionality of the current state of art Natural Language Processing
(NLP) systems. The contextual nature of linguistic modality is studied. In this
paper, the works and logical approaches employed by Natural Language Processing
systems dealing with modality are reviewed. It sees human cognition and
intelligence as multi-layered approach that can be implemented by intelligent
systems for learning. Lastly, current flow of research going on within this
field is talked providing futurology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04720</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04720</id><created>2015-04-18</created><updated>2016-02-18</updated><authors><author><keyname>Turilli</keyname><forenames>Matteo</forenames></author><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author><author><keyname>Merzky</keyname><forenames>Andre</forenames></author><author><keyname>Wilde</keyname><forenames>Michael</forenames></author><author><keyname>Weissman</keyname><forenames>Jon</forenames></author><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>Integrating Abstractions to Enhance the Execution of Distributed
  Applications</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the factors that limits the scale, performance, and sophistication of
distributed applications is the difficulty of concurrently executing them on
multiple distributed computing resources. In part, this is due to a poor
understanding of the general properties and performance of the coupling between
applications and dynamic resources. This paper addresses this issue by
integrating abstractions representing distributed applications, resources, and
execution processes into a pilot-based middleware. The middleware provides a
platform that can specify distributed applications, execute them on multiple
resource and for different configurations, and is instrumented to support
investigative analysis. We analyzed the execution of distributed applications
using experiments that measure the benefits of using multiple resources, the
late-binding of scheduling decisions, and the use of backfill scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04725</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04725</id><created>2015-04-18</created><authors><author><keyname>Zhang</keyname><forenames>Yan-Yu</forenames></author><author><keyname>Yu</keyname><forenames>Hong-Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Jian-Kang</forenames></author><author><keyname>Zhu</keyname><forenames>Yi-Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jin-Long</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Full Large-Scale Diversity Space Codes for MIMO Optical Wireless
  Communications</title><categories>cs.IT math.IT</categories><comments>accepted by ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiple-input-multiple-output optical wireless
communication (MIMO-OWC) system suffering from log-normal fading. In this
scenario, a general criterion for the design of full large-scale diversity
space code (FLDSC) with the maximum likelihood (ML) detector is developed.
Based on our criterion, FLDSC is attained if and only if all the entries of the
space coding matrix are positive. Particularly for $2\times 2$ MIMO-OWC with
unipolar pulse amplitude modulation (PAM), a closed-form linear FLDSC
satisfying this criterion is attained by smartly taking advantage of some
available properties as well as by developing some new interesting properties
on Farey sequences in number theory to rigorously attack the continuous and
discrete variables mixed max-min problem. In fact, this specific design not
only proves that a repetition code (RC) is the best linear FLDSC, but also
uncovers a significant difference between MIMO radio frequency (RF)
communications and MIMO-OWC that space-only transmission is sufficient for a
full diversity achievement. Computer simulations demonstrate that FLDSC
substantially outperforms spatial multiplexing with the same total optical
power and spectral efficiency and the latter obtains only the small-scale
diversity gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04730</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04730</id><created>2015-04-18</created><authors><author><keyname>Bhattacharya</keyname><forenames>Sourav</forenames></author><author><keyname>Huhta</keyname><forenames>Otto</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>LookAhead: Augmenting Crowdsourced Website Reputation Systems With
  Predictive Modeling</title><categories>cs.CR cs.IR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsafe websites consist of malicious as well as inappropriate sites, such as
those hosting questionable or offensive content. Website reputation systems are
intended to help ordinary users steer away from these unsafe sites. However,
the process of assigning safety ratings for websites typically involves humans.
Consequently it is time consuming, costly and not scalable. This has resulted
in two major problems: (i) a significant proportion of the web space remains
unrated and (ii) there is an unacceptable time lag before new websites are
rated.
  In this paper, we show that by leveraging structural and content-based
properties of websites, it is possible to reliably and efficiently predict
their safety ratings, thereby mitigating both problems. We demonstrate the
effectiveness of our approach using four datasets of up to 90,000 websites. We
use ratings from Web of Trust (WOT), a popular crowdsourced web reputation
system, as ground truth. We propose a novel ensemble classification technique
that makes opportunistic use of available structural and content properties of
webpages to predict their eventual ratings in two dimensions used by WOT:
trustworthiness and child safety. Ours is the first classification system to
predict such subjective ratings and the same approach works equally well in
identifying malicious websites. Across all datasets, our classification
performs well with average F$_1$-score in the 74--90\% range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04735</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04735</id><created>2015-04-18</created><authors><author><keyname>Matamalas</keyname><forenames>Joan T.</forenames></author><author><keyname>Poncela-Casasnovas</keyname><forenames>Julia</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Strategical incoherence regulates cooperation in social dilemmas on
  multiplex networks</title><categories>physics.soc-ph cs.GT cs.SI q-bio.PE</categories><comments>28 pages, 10 figures, to appear in Scientific Reports</comments><journal-ref>Scientific Reports 5 (2015) 9519</journal-ref><doi>10.1038/srep09519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation is a very common, yet not fully-understood phenomenon in natural
and human systems. The introduction of a network within the population is known
to affect the outcome of cooperative dynamics, allowing for the survival of
cooperation in adverse scenarios. Recently, the introduction of multiplex
networks has yet again modified the expectations for the outcome of the
Prisoner's Dilemma game, compared to the monoplex case. However, much remains
unstudied regarding other social dilemmas on multiplex, as well as the
unexplored microscopic underpinnings of it. In this paper, we systematically
study the evolution of cooperation in all four games in the $T-S$ plane on
multiplex. More importantly, we find some remarkable and previously unknown
features in the microscopic organization of the strategies, that are
responsible for the important differences between cooperative dynamics in
monoplex and multiplex. Specifically, we find that in the stationary state,
there are individuals that play the same strategy in all layers (coherent), and
others that don't (incoherent). This second group of players is responsible for
the surprising fact of a non full-cooperation in the Harmony Game on multiplex,
never observed before, as well as a higher-than-expected cooperation rates in
some regions of the other three social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04739</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04739</id><created>2015-04-18</created><authors><author><keyname>Jozefowicz</keyname><forenames>Rafal</forenames></author><author><keyname>Czarnecki</keyname><forenames>Wojciech Marian</forenames></author></authors><title>Fast optimization of Multithreshold Entropy Linear Classifier</title><categories>cs.LG stat.ML</categories><comments>Presented at Theoretical Foundations of Machine Learning 2015
  (http://tfml.gmum.net), final version published in Schedae Informaticae
  Journal</comments><doi>10.4467/20838476SI.14.005.3022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multithreshold Entropy Linear Classifier (MELC) is a density based model
which searches for a linear projection maximizing the Cauchy-Schwarz Divergence
of dataset kernel density estimation. Despite its good empirical results, one
of its drawbacks is the optimization speed. In this paper we analyze how one
can speed it up through solving an approximate problem. We analyze two methods,
both similar to the approximate solutions of the Kernel Density Estimation
querying and provide adaptive schemes for selecting a crucial parameters based
on user-specified acceptable error. Furthermore we show how one can exploit
well known conjugate gradients and L-BFGS optimizers despite the fact that the
original optimization problem should be solved on the sphere. All above methods
and modifications are tested on 10 real life datasets from UCI repository to
confirm their practical usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04740</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04740</id><created>2015-04-18</created><authors><author><keyname>Czarnecki</keyname><forenames>Wojciech Marian</forenames></author></authors><title>On the consistency of Multithreshold Entropy Linear Classifier</title><categories>cs.LG stat.ML</categories><comments>Presented at Theoretical Foundations of Machine Learning 2015
  (http://tfml.gmum.net), final version published in Schedae Informaticae
  Journal</comments><doi>10.4467/20838476SI.15.012.3034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multithreshold Entropy Linear Classifier (MELC) is a recent classifier idea
which employs information theoretic concept in order to create a multithreshold
maximum margin model. In this paper we analyze its consistency over
multithreshold linear models and show that its objective function upper bounds
the amount of misclassified points in a similar manner like hinge loss does in
support vector machines. For further confirmation we also conduct some
numerical experiments on five datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04750</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04750</id><created>2015-04-18</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Dilek</forenames></author><author><keyname>&#x130;nan</keyname><forenames>Tolga</forenames></author><author><keyname>Boyrazo&#x11f;lu</keyname><forenames>Burak</forenames></author><author><keyname>Buhan</keyname><forenames>Serkan</forenames></author><author><keyname>Salor</keyname><forenames>&#xd6;zg&#xfc;l</forenames></author><author><keyname>&#xc7;ad&#x131;rc&#x131;</keyname><forenames>I&#x15f;&#x131;k</forenames></author><author><keyname>Ermi&#x15f;</keyname><forenames>Muammer</forenames></author></authors><title>PQStream: A Data Stream Architecture for Electrical Power Quality</title><categories>cs.DB</categories><comments>Appears in Proceedings of International Workshop on Knowledge
  Discovery from Ubiquitous Data Streams of ECML/PKDD, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a data stream architecture is presented for electrical power
quality (PQ) which is called PQStream. PQStream is developed to process and
manage time-evolving data coming from the country-wide mobile measurements of
electrical PQ parameters of the Turkish Electricity Transmission System. It is
a full-fledged system with a data measurement module which carries out
processing of continuous PQ data, a stream database which stores the output of
the measurement module, and finally a Graphical User Interface for
retrospective analysis of the PQ data stored in the stream database. The
presented model is deployed and is available to PQ experts, academicians and
researchers of the area. As further studies, data mining methods such as
classification and clustering algorithms will be applied in order to deduce
useful PQ information from this database of PQ data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04751</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04751</id><created>2015-04-18</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Dilek</forenames></author><author><keyname>Y&#xf6;ndem</keyname><forenames>Meltem Turhan</forenames></author></authors><title>A Knowledge-poor Pronoun Resolution System for Turkish</title><categories>cs.CL</categories><comments>Appears in Proceedings of the 6th Discourse Anaphora and Anaphora
  Resolution Colloquium (DAARC), 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pronoun resolution system which requires limited syntactic knowledge to
identify the antecedents of personal and reflexive pronouns in Turkish is
presented. As in its counterparts for languages like English, Spanish and
French, the core of the system is the constraints and preferences determined
empirically. In the evaluation phase, it performed considerably better than the
baseline algorithm used for comparison. The system is significant for its being
the first fully specified knowledge-poor computational framework for pronoun
resolution in Turkish where Turkish possesses different structural properties
from the languages for which knowledge-poor systems had been developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04756</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04756</id><created>2015-04-18</created><authors><author><keyname>Marzen</keyname><forenames>Sarah E.</forenames></author><author><keyname>DeWeese</keyname><forenames>Michael R.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Time Resolution Dependence of Information Measures for Spiking Neurons:
  Atoms, Scaling, and Universality</title><categories>q-bio.NC cond-mat.dis-nn cs.NE math.PR nlin.CD</categories><comments>20 pages, 6 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/trdctim.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mutual information between stimulus and spike-train response is commonly
used to monitor neural coding efficiency, but neuronal computation broadly
conceived requires more refined and targeted information measures of
input-output joint processes. A first step towards that larger goal is to
develop information measures for individual output processes, including
information generation (entropy rate), stored information (statistical
complexity), predictable information (excess entropy), and active information
accumulation (bound information rate). We calculate these for spike trains
generated by a variety of noise-driven integrate-and-fire neurons as a function
of time resolution and for alternating renewal processes. We show that their
time-resolution dependence reveals coarse-grained structural properties of
interspike interval statistics; e.g., $\tau$-entropy rates that diverge less
quickly than the firing rate indicate interspike interval correlations. We also
find evidence that the excess entropy and regularized statistical complexity of
different types of integrate-and-fire neurons are universal in the
continuous-time limit in the sense that they do not depend on mechanism
details. This suggests a surprising simplicity in the spike trains generated by
these model neurons. Interestingly, neurons with gamma-distributed ISIs and
neurons whose spike trains are alternating renewal processes do not fall into
the same universality class. These results lead to two conclusions. First, the
dependence of information measures on time resolution reveals mechanistic
details about spike train generation. Second, information measures can be used
as model selection tools for analyzing spike train processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04757</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04757</id><created>2015-04-18</created><updated>2015-05-14</updated><authors><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>Tree Buffers</title><categories>cs.DS</categories><comments>CAV 2015 (The final publication is available at link.springer.com)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In runtime verification, the central problem is to decide if a given program
execution violates a given property. In online runtime verification, a monitor
observes a program's execution as it happens. If the program being observed has
hard real-time constraints, then the monitor inherits them. In the presence of
hard real-time constraints it becomes a challenge to maintain enough
information to produce error traces, should a property violation be observed.
In this paper we introduce a data structure, called tree buffer, that solves
this problem in the context of automata-based monitors: If the monitor itself
respects hard real-time constraints, then enriching it by tree buffers makes it
possible to provide error traces, which are essential for diagnosing defects.
We show that tree buffers are also useful in other application domains. For
example, they can be used to implement functionality of capturing groups in
regular expressions. We prove optimal asymptotic bounds for our data structure,
and validate them using empirical data from two sources: regular expression
searching through Wikipedia, and runtime verification of execution traces
obtained from the DaCapo test suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04759</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04759</id><created>2015-04-18</created><authors><author><keyname>Ramos</keyname><forenames>Arthur F.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Anjolina G.</forenames></author></authors><title>On the Identity Type as the Type of Computational Paths</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new way of formalizing the intensional identity type based on
the fact that a entity known as computational paths can be interpreted as terms
of the identity type. Our approach enjoys the fact that our elimination rule is
easy to understand and use. We make this point clear constructing terms of some
relevant types using our proposed elimination rule. We also show that the
identity type, as defined by our approach, induces a groupoid structure. This
result is on par with the fact that the traditional identity type induces a
groupoid, as exposed by Hofmann \&amp; Streicher (1994).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04763</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04763</id><created>2015-04-18</created><authors><author><keyname>Novotn&#xfd;</keyname><forenames>David</forenames></author><author><keyname>Larlus</keyname><forenames>Diane</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Understanding the Fisher Vector: a multimodal part model</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fisher Vectors and related orderless visual statistics have demonstrated
excellent performance in object detection, sometimes superior to established
approaches such as the Deformable Part Models. However, it remains unclear how
these models can capture complex appearance variations using visual codebooks
of limited sizes and coarse geometric information. In this work, we propose to
interpret Fisher-Vector-based object detectors as part-based models. Through
the use of several visualizations and experiments, we show that this is a
useful insight to explain the good performance of the model. Furthermore, we
reveal for the first time several interesting properties of the FV, including
its ability to work well using only a small subset of input patches and visual
words. Finally, we discuss the relation of the FV and DPM detectors, pointing
out differences and commonalities between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04767</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04767</id><created>2015-04-18</created><updated>2015-04-23</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Guruganesh</keyname><forenames>Guru</forenames></author></authors><title>On the Lov\'asz Theta function for Independent Sets in Sparse Graphs</title><categories>cs.DS</categories><comments>Extended abstract to appear at the STOC 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the maximum independent set problem on graphs with maximum
degree~$d$. We show that the integrality gap of the Lov\'asz
$\vartheta$-function based SDP is $\widetilde{O}(d/\log^{3/2} d)$. This
improves on the previous best result of $\widetilde{O}(d/\log d)$, and almost
matches the integrality gap of $\widetilde{O}(d/\log^2 d)$ recently shown for
stronger SDPs, namely those obtained using poly-$(\log(d))$ levels of the
$SA^+$ semidefinite hierarchy. The improvement comes from an improved
Ramsey-theoretic bound on the independence number of $K_r$-free graphs for
large values of $r$.
  We also show how to obtain an algorithmic version of the above-mentioned
$SA^+$-based integrality gap result, via a coloring algorithm of Johansson. The
resulting approximation guarantee of $\widetilde{O}(d/\log^2 d)$ matches the
best unique-games-based hardness result up to lower-order poly-$(\log\log d)$
factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04768</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04768</id><created>2015-04-18</created><updated>2015-09-07</updated><authors><author><keyname>Baelde</keyname><forenames>David</forenames></author><author><keyname>Delaune</keyname><forenames>St&#xe9;phanie</forenames></author><author><keyname>Hirschi</keyname><forenames>Lucca</forenames></author></authors><title>Partial Order Reduction for Security Protocols</title><categories>cs.CR cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Security protocols are concurrent processes that communicate using
cryptography with the aim of achieving various security properties. Recent work
on their formal verification has brought procedures and tools for deciding
trace equivalence properties (e.g., anonymity, unlinkability, vote secrecy) for
a bounded number of sessions. However, these procedures are based on a naive
symbolic exploration of all traces of the considered processes which,
unsurprisingly, greatly limits the scalability and practical impact of the
verification tools.
  In this paper, we overcome this difficulty by developing partial order
reduction techniques for the verification of security protocols. We provide
reduced transition systems that optimally eliminate redundant traces, and which
are adequate for model-checking trace equivalence properties of protocols by
means of symbolic execution. We have implemented our reductions in the tool
Apte, and demonstrated that it achieves the expected speedup on various
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04770</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04770</id><created>2015-04-18</created><authors><author><keyname>Rabinovich</keyname><forenames>Maxim</forenames></author><author><keyname>Archambeau</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Online Inference for Relation Extraction with a Reduced Feature Set</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to web-scale corpora is gradually bringing robust automatic knowledge
base creation and extension within reach. To exploit these large
unannotated---and extremely difficult to annotate---corpora, unsupervised
machine learning methods are required. Probabilistic models of text have
recently found some success as such a tool, but scalability remains an obstacle
in their application, with standard approaches relying on sampling schemes that
are known to be difficult to scale. In this report, we therefore present an
empirical assessment of the sublinear time sparse stochastic variational
inference (SSVI) scheme applied to RelLDA. We demonstrate that online inference
leads to relatively strong qualitative results but also identify some of its
pathologies---and those of the model---which will need to be overcome if SSVI
is to be used for large-scale relation extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04773</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04773</id><created>2015-04-18</created><authors><author><keyname>Gorodecky</keyname><forenames>Danila</forenames></author></authors><title>Reed-Muller Realization of X (mod P)</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a novel technique of X (mod P) realization. It is based
on the Reed-Muller polynomial expansion. The advantage of the approach
concludes in the capability to realize X (mod P) for an arbitrary P. The
approach is competitive with the known realizations on the speed processing.
Advantages and results of comparison with the known approaches for X [9:1] and
P=7 is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04785</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04785</id><created>2015-04-18</created><authors><author><keyname>Mashhadi</keyname><forenames>Mahdi Boloursaz</forenames></author><author><keyname>Asadi</keyname><forenames>Ehsan</forenames></author><author><keyname>Eskandari</keyname><forenames>Mohsen</forenames></author><author><keyname>Kiani</keyname><forenames>Shahrzad</forenames></author><author><keyname>Marvasti</keyname><forenames>Farrokh</forenames></author></authors><title>Heart Rate Tracking using Wrist-Type Photoplethysmographic (PPG) Signals
  during Physical Exercise with Simultaneous Accelerometry</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, 2 tables, submitted to IEEE signal processing
  letters</comments><doi>10.1109/LSP.2015.2509868</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of heart rate tracking during intensive
physical exercise using simultaneous 2 channel photoplethysmographic (PPG) and
3 dimensional (3D) acceleration signals recorded from wrist. This is a
challenging problem because the PPG signals recorded from wrist during exercise
are contaminated by strong Motion Artifacts (MAs). In this work, a novel
algorithm is proposed which consists of two main steps of MA Cancellation and
Spectral Analysis. The MA cancellation step cleanses the MA-contaminated PPG
signals utilizing the acceleration data and the spectral analysis step
estimates a higher resolution spectrum of the signal and selects the spectral
peaks corresponding to HR. Experimental results on datasets recorded from 12
subjects during fast running at the peak speed of 15 km/hour showed that the
proposed algorithm achieves an average absolute error of 1.25 beat per minute
(BPM). These experimental results also confirm that the proposed algorithm
keeps high estimation accuracies even in strong MA conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04788</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04788</id><created>2015-04-19</created><authors><author><keyname>Chen</keyname><forenames>Wenlin</forenames></author><author><keyname>Wilson</keyname><forenames>James T.</forenames></author><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author></authors><title>Compressing Neural Networks with the Hashing Trick</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As deep nets are increasingly used in applications suited for mobile devices,
a fundamental dilemma becomes apparent: the trend in deep learning is to grow
models to absorb ever-increasing data set sizes; however mobile devices are
designed with very little memory and cannot store such large models. We present
a novel network architecture, HashedNets, that exploits inherent redundancy in
neural networks to achieve drastic reductions in model sizes. HashedNets uses a
low-cost hash function to randomly group connection weights into hash buckets,
and all connections within the same hash bucket share a single parameter value.
These parameters are tuned to adjust to the HashedNets weight sharing
architecture with standard backprop during training. Our hashing procedure
introduces no additional memory overhead, and we demonstrate on several
benchmark data sets that HashedNets shrink the storage requirements of neural
networks substantially while mostly preserving generalization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04792</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04792</id><created>2015-04-19</created><updated>2015-04-23</updated><authors><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Gao</keyname><forenames>Bin-Bin</forenames></author><author><keyname>Liu</keyname><forenames>Guoqing</forenames></author></authors><title>Visual Recognition Using Directional Distribution Distance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, an entity such as an image or video is often represented
as a set of instance vectors, which can be SIFT, motion, or deep learning
feature vectors extracted from different parts of that entity. Thus, it is
essential to design efficient and effective methods to compare two sets of
instance vectors. Existing methods such as FV, VLAD or Super Vectors have
achieved excellent results. However, this paper shows that these methods are
designed based on a generative perspective, and a discriminative method can be
more effective in categorizing images or videos. The proposed D3
(discriminative distribution distance) method effectively compares two sets as
two distributions, and proposes a directional total variation distance (DTVD)
to measure how separated are they. Furthermore, a robust classifier-based
method is proposed to estimate DTVD robustly. The D3 method is evaluated in
action and image recognition tasks and has achieved excellent accuracy and
speed. D3 also has a synergy with FV. The combination of D3 and FV has
advantages over D3, FV, and VLAD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04796</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04796</id><created>2015-04-19</created><updated>2015-10-30</updated><authors><author><keyname>Luo</keyname><forenames>Wuqiong</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Leng</keyname><forenames>Mei</forenames></author></authors><title>Infection Spreading and Source Identification: A Hide and Seek Game</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of an infection source node (e.g., a rumor or computer virus source)
in a network is to spread its infection to as many nodes as possible, while
remaining hidden from the network administrator. On the other hand, the network
administrator aims to identify the source node based on knowledge of which
nodes have been infected. We model the infection spreading and source
identification problem as a strategic game, where the infection source and the
network administrator are the two players. As the Jordan center estimator is a
minimax source estimator that has been shown to be robust in recent works, we
assume that the network administrator utilizes a source estimation strategy
that can probe any nodes within a given radius of the Jordan center. Given any
estimation strategy, we design a best-response infection strategy for the
source. Given any infection strategy, we design a best-response estimation
strategy for the network administrator. We derive conditions under which a Nash
equilibrium of the strategic game exists. Simulations in both synthetic and
real-world networks demonstrate that our proposed infection strategy infects
more nodes while maintaining the same safety margin between the true source
node and the Jordan center source estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04797</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04797</id><created>2015-04-19</created><updated>2015-12-10</updated><authors><author><keyname>Li</keyname><forenames>Songze</forenames></author><author><keyname>Kao</keyname><forenames>David T. H.</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Rover-to-Orbiter Communication in Mars: Taking Advantage of the Varying
  Topology</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures. Accepted by IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the communication problem from rovers on Mars'
surface to Mars-orbiting satellites. We first justify that, to a good extent,
the rover-to-orbiter communication problem can be modelled as communication
over a $2 \times 2$ X-channel with the network topology varying over time. For
such a fading X-channel where transmitters are only aware of the time-varying
topology but not the time-varying channel state (i.e., no CSIT), we propose
coding strategies that code across topologies, and develop upper bounds on the
sum degrees-of-freedom (DoF) that is shown to be tight under certain pattern of
the topology variation. Furthermore we demonstrate that the proposed scheme
approximately achieves the ergodic sum-capacity of the network. Using the
proposed coding scheme, we numerically evaluate the ergodic rate gain over a
time-division-multiple-access (TDMA) scheme for Rayleigh and Rice fading
channels. We also numerically demonstrate that with practical orbital
parameters, a 9.6% DoF gain, as well as more than 11.6% throughput gain can be
achieved for a rover-to-orbiter communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04799</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04799</id><created>2015-04-19</created><authors><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author><author><keyname>Xi</keyname><forenames>Jiangtao</forenames></author></authors><title>Approximate Message Passing with Unitary Transformation</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message passing (AMP) and its variants, developed based on loopy
belief propagation, are attractive for estimating a vector x from a noisy
version of z = Ax, which arises in many applications. For a large A with i. i.
d. elements, AMP can be characterized by the state evolution and exhibits fast
convergence. However, it has been shown that, AMP mayeasily diverge for a
generic A. In this work, we develop a new variant of AMP based on a unitary
transformation of the original model (hence the variant is called UT-AMP),
where the unitary matrix is available for any matrix A, e.g., the conjugate
transpose of the left singular matrix of A, or a normalized DFT (discrete
Fourier transform) matrix for any circulant A. We prove that, in the case of
Gaussian priors, UT-AMP always converges for any matrix A. It is observed that
UT-AMP is much more robust than the original AMP for difficult A and exhibits
fast convergence.
  A special form of UT-AMP with a circulant A was used in our previous work
[13] for turbo equalization. This work extends it to a generic A, and provides
a theoretical investigation on the convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04800</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04800</id><created>2015-04-19</created><authors><author><keyname>Prokudin</keyname><forenames>Dmitry</forenames></author></authors><title>Design and Implementation of an Integrated Information System to Support
  Scientific Research</title><categories>cs.DL cs.CY cs.SI</categories><comments>in Russian</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Computerization of research activities led to the creation of large
specialized information resources, platforms, services and software to support
scientific research. However, their shortcomings do not allow to fully
realizing the comprehensive support of scientific activity, and the absence of
a single entry point to divide the scientific community fragmented groups
interests. The article based on analysing the existing solutions and approaches
to the tools of information and communication technologies of various types of
scientific activity, and taking into account the research lifecycle proposed
and formulated the basic principles of designing and implementing an integrated
information system to support scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04801</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04801</id><created>2015-04-19</created><updated>2015-06-13</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Can Intellectual Processes in the Sciences Also Be Simulated? The
  Anticipation and Visualization of Possible Future States</title><categories>cs.DL cs.CY</categories><comments>accepted for publication in Scientometrics (June 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Socio-cognitive action reproduces and changes both social and cognitive
structures. The analytical distinction between these dimensions of structure
provides us with richer models of scientific development. In this study, I
assume that (i) social structures organize expectations into belief structures
that can be attributed to individuals and communities; (ii) expectations are
specified in scholarly literature; and (iii) intellectually the sciences
(disciplines, specialties) tend to self-organize as systems of rationalized
expectations. Whereas social organizations remain localized, academic writings
can circulate, and expectations can be stabilized and globalized using
symbolically generalized codes of communication. The intellectual
restructuring, however, remains latent as a second-order dynamics that can be
accessed by participants only reflexively. Yet, the emerging &quot;horizons of
meaning&quot; provide feedback to the historically developing organizations by
constraining the possible future states as boundary conditions. I propose to
model these possible future states using incursive and hyper-incursive
equations from the computation of anticipatory systems. Simulations of these
equations enable us to visualize the couplings among the historical--i.e.,
recursive--progression of social structures along trajectories, the
evolutionary--i.e., hyper-incursive--development of systems of expectations at
the regime level, and the incursive instantiations of expectations in actions,
organizations, and texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04802</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04802</id><created>2015-04-19</created><authors><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames></author></authors><title>Gradual Classical Logic for Attributed Objects - Extended in
  Re-Presentation</title><categories>cs.AI cs.CL cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.6036</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our understanding about things is conceptual. By stating that we reason about
objects, it is in fact not the objects but concepts referring to them that we
manipulate. Now, so long just as we acknowledge infinitely extending notions
such as space, time, size, colour, etc, - in short, any reasonable quality -
into which an object is subjected, it becomes infeasible to affirm atomicity in
the concept referring to the object. However, formal/symbolic logics typically
presume atomic entities upon which other expressions are built. Can we reflect
our intuition about the concept onto formal/symbolic logics at all? I assure
that we can, but the usual perspective about the atomicity needs inspected. In
this work, I present gradual logic which materialises the observation that we
cannot tell apart whether a so-regarded atomic entity is atomic or is just
atomic enough not to be considered non-atomic. The motivation is to capture
certain phenomena that naturally occur around concepts with attributes,
including presupposition and contraries. I present logical particulars of the
logic, which is then mapped onto formal semantics. Two linguistically
interesting semantics will be considered. Decidability is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04803</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04803</id><created>2015-04-19</created><updated>2015-04-24</updated><authors><author><keyname>Cohen</keyname><forenames>Rami</forenames></author><author><keyname>Cassuto</keyname><forenames>Yuval</forenames></author></authors><title>Algorithms and Throughput Analysis for MDS-Coded Switches</title><categories>cs.IT math.IT</categories><comments>6 pages, an extended version of a paper accepted to the 2015 IEEE
  International Symposium on Information Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network switches and routers need to serve packet writes and reads at rates
that challenge the most advanced memory technologies. As a result, scaling the
switching rates is commonly done by parallelizing the packet I/Os using
multiple memory units. For improved read rates, packets can be coded with an
[n,k] MDS code, thus giving more flexibility at read time to achieve higher
utilization of the memory units. In the paper, we study the usage of [n,k] MDS
codes in a switching environment. In particular, we study the algorithmic
problem of maximizing the instantaneous read rate given a set of packet
requests and the current layout of the coded packets in memory. The most
interesting results from practical standpoint show how the complexity of
reaching optimal read rate depends strongly on the writing policy of the coded
packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04804</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04804</id><created>2015-04-19</created><authors><author><keyname>Pan</keyname><forenames>Yuechao</forenames></author><author><keyname>Wang</keyname><forenames>Yangzihao</forenames></author><author><keyname>Wu</keyname><forenames>Yuduo</forenames></author><author><keyname>Yang</keyname><forenames>Carl</forenames></author><author><keyname>Owens</keyname><forenames>John D.</forenames></author></authors><title>Multi-GPU Graph Analytics</title><categories>cs.DC</categories><comments>12 pages. Submitted to Supercomputing 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a multi-GPU graph processing library that allows programmers to
easily extend single-GPU graph algorithms to achieve scalable performance on
large graph datasets with billions of edges. Our design only requires users to
specify a few algorithm-dependent blocks, hiding most multi-GPU related
implementation details. Our design effectively overlaps computation and data
transfer and implements a just-enough memory allocation scheme that allows
memory usage to scale with more GPUs. We achieve ~20 GTEPS peak performance for
BFS, demonstrating a ~6X speed-up with ~2X total GPU memory consumption on 8
GPUs. We identify synchronization/data communication patterns, graph
topologies, and partitioning algorithms as limiting factors to further
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04806</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04806</id><created>2015-04-19</created><updated>2015-07-20</updated><authors><author><keyname>Thapa</keyname><forenames>Chandra</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Generalized Interlinked Cycle Cover for Index Coding</title><categories>cs.IT math.IT</categories><comments>Extended version of the paper which is to be presented at the IEEE
  Information Theory Workshop (ITW), 2015 Jeju</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A source coding problem over a noiseless broadcast channel where the source
is pre-informed about the contents of the cache of all receivers, is an index
coding problem. Furthermore, if each message is requested by one receiver, then
we call this an index coding problem with a unicast message setting. This
problem can be represented by a directed graph. In this paper, we first define
a structure (we call generalized interlinked cycles (GIC)) in directed graphs.
A GIC consists of cycles which are interlinked in some manner (i.e., not
disjoint), and it turns out that the GIC is a generalization of cliques and
cycles. We then propose a simple scalar linear encoding scheme with linear time
encoding complexity. This scheme exploits GICs in the digraph. We prove that
our scheme is optimal for a class of digraphs with message packets of any
length. Moreover, we show that our scheme can outperform existing techniques,
e.g., partial clique cover, local chromatic number, composite-coding, and
interlinked cycle cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04811</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04811</id><created>2015-04-19</created><authors><author><keyname>Tarasenko</keyname><forenames>Sergey</forenames></author></authors><title>Socializing Autonomous Units with the Reflexive Game Theory and
  Resonate-and-Fire neurons</title><categories>cs.MA</categories><comments>10 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study the concept of reflexia is applied to modeling behavior of
autonomous units. The relationship between reflexia, on the one hand, and
mirror neuron system and perception of emotions, on the other hand, is
introduced. The main method of using reflexia in a group of autonomous units is
Reflexive Game Theory (RGT). To embody RGT in a group of autonomous agents a
communication system is employed. This communication system uses frequency
domain multiplexing by means of Izhikevich's resonate-and-fire neural models.
The result of socialization of autonomous units by means of RGT and
communication system is illustrated in several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04813</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04813</id><created>2015-04-19</created><updated>2015-07-19</updated><authors><author><keyname>Ghazi</keyname><forenames>Badih</forenames></author><author><keyname>Komargodski</keyname><forenames>Ilan</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Communication with Contextual Uncertainty</title><categories>cs.CC cs.IT math.IT</categories><comments>20 pages + 1 title page</comments><acm-class>F.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple model illustrating the role of context in communication
and the challenge posed by uncertainty of knowledge of context. We consider a
variant of distributional communication complexity where Alice gets some
information $x$ and Bob gets $y$, where $(x,y)$ is drawn from a known
distribution, and Bob wishes to compute some function $g(x,y)$ (with high
probability over $(x,y)$). In our variant, Alice does not know $g$, but only
knows some function $f$ which is an approximation of $g$. Thus, the function
being computed forms the context for the communication, and knowing it
imperfectly models (mild) uncertainty in this context.
  A naive solution would be for Alice and Bob to first agree on some common
function $h$ that is close to both $f$ and $g$ and then use a protocol for $h$
to compute $h(x,y)$. We show that any such agreement leads to a large overhead
in communication ruling out such a universal solution.
  In contrast, we show that if $g$ has a one-way communication protocol with
complexity $k$ in the standard setting, then it has a communication protocol
with complexity $O(k \cdot (1+I))$ in the uncertain setting, where $I$ denotes
the mutual information between $x$ and $y$. In the particular case where the
input distribution is a product distribution, the protocol in the uncertain
setting only incurs a constant factor blow-up in communication and error.
  Furthermore, we show that the dependence on the mutual information $I$ is
required. Namely, we construct a class of functions along with a non-product
distribution over $(x,y)$ for which the communication complexity is a single
bit in the standard setting but at least $\Omega(\sqrt{n})$ bits in the
uncertain setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04818</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04818</id><created>2015-04-19</created><authors><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Compositional Correlation Quantization for Large-Scale Multimodal Search</title><categories>cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient similarity retrieval from large-scale multimodal database is
pervasive in current search systems with the big data tidal wave. To support
queries across content modalities, the system should enable cross-modal
correlation and computation-efficient indexing. While hashing methods have
shown great potential in approaching this goal, current attempts generally
failed to learn isomorphic hash codes in a seamless scheme, that is, they embed
multiple modalities into a continuous isomorphic space and then threshold
embeddings into binary codes, which incurred substantial loss of search
quality. In this paper, we establish seamless multimodal hashing by proposing a
novel Compositional Correlation Quantization (CCQ) model. Specifically, CCQ
jointly finds correlation-maximal mappings that transform different modalities
into an isomorphic latent space, and learns compositional quantizers that
quantize the isomorphic latent features into compact binary codes. An
optimization framework is developed to preserve both intra-modal similarity and
inter-modal correlation while minimizing both reconstruction and quantization
errors, which can be trained from both paired and unpaired data in linear time.
A comprehensive set of experiments clearly show the superior effectiveness and
efficiency of CCQ against the state-of-the-art techniques on both unimodal and
cross-modal search tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04826</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04826</id><created>2015-04-19</created><authors><author><keyname>Mbogo</keyname><forenames>Irina</forenames></author><author><keyname>Prokudin</keyname><forenames>Dmitry</forenames></author><author><keyname>Chugunov</keyname><forenames>Andrey</forenames></author></authors><title>Complex Integration of Digital Collections into Scientific Information
  Space</title><categories>cs.DL cs.CY cs.SI</categories><comments>in Russian</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The article considers the solution of problems of accumulation and
integration of scientific electronic collections into information space of
scientific researches. On the basis of the analysis of the existing standards
and solutions the choice of methodology and technologies of representation of
electronic materials of the scientific conference 'Internet and Modern Society'
locates. The concept of the project of integration of the created electronic
collection into the main world and domestic information systems and aggregators
of scientific information is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04839</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04839</id><created>2015-04-19</created><updated>2015-07-22</updated><authors><author><keyname>Vixie</keyname><forenames>Kevin R.</forenames></author></authors><title>Some Minimal Shape Decompositions Are Nice</title><categories>math.DG cs.CG</categories><comments>Corrections and acknowledgment added</comments><msc-class>49Q15 (Primary), 49Q20, 49Q05, 49Q10, 28A75, 58A25, 58C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some sense, the world is composed of shapes and words, of continuous
things and discrete things. The recognition and study of continuous objects in
the form of shapes occupies a significant part of the effort of unraveling many
geometric questions. Shapes can be rep- resented with great generality by
objects called currents. While the enormous variety and representational power
of currents is useful for representing a huge variety of phenomena, it also
leads to the problem that knowing something is a respectable current tells you
little about how nice or regular it is. In these brief notes I give an
intuitive explanation of a result that says that an important class of minimal
shape decompositions will be nice if the input shape (current) is nice. These
notes are an exposition of the paper by Ibrahim, Krishnamoorthy and Vixie which
can be found on the arXiv:1411.0882 and any reference to these notes, should
include a reference to that paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04850</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04850</id><created>2015-04-19</created><authors><author><keyname>Mitra</keyname><forenames>Adway</forenames></author></authors><title>Exploring Bayesian Models for Multi-level Clustering of Hierarchically
  Grouped Sequential Data</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of Bayesian models have been proposed for data that is divided
hierarchically into groups. These models aim to cluster the data at different
levels of grouping, by assigning a mixture component to each datapoint, and a
mixture distribution to each group. Multi-level clustering is facilitated by
the sharing of these components and distributions by the groups. In this paper,
we introduce the concept of Degree of Sharing (DoS) for the mixture components
and distributions, with an aim to analyze and classify various existing models.
Next we introduce a generalized hierarchical Bayesian model, of which the
existing models can be shown to be special cases. Unlike most of these models,
our model takes into account the sequential nature of the data, and various
other temporal structures at different levels while assigning mixture
components and distributions. We show one specialization of this model aimed at
hierarchical segmentation of news transcripts, and present a Gibbs Sampling
based inference algorithm for it. We also show experimentally that the proposed
model outperforms existing models for the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04859</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04859</id><created>2015-04-19</created><updated>2015-09-18</updated><authors><author><keyname>Salehi</keyname><forenames>&#xd6;zlem</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author></authors><title>Homing Vector Automata</title><categories>cs.FL</categories><comments>NCMA'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce homing vector automata, which are finite automata augmented by a
vector that is multiplied at each step by a matrix determined by the current
transition, and have to return the vector to its original setting in order to
accept the input. The computational power of the deterministic,
nondeterministic and blind versions of these real-time machines are examined
and compared to various related types of automata. A generalized version of the
Stern-Brocot encoding method, suitable for representing strings on arbitrary
alphabets, is also developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04867</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04867</id><created>2015-04-19</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Caviglione</keyname><forenames>Luca</forenames></author></authors><title>Information Hiding as a Challenge for Malware Detection</title><categories>cs.CR</categories><comments>9 pages, 1 table</comments><journal-ref>IEEE Security &amp; Privacy magazine, March/April, 2015, Volume:13,
  Issue: 2, pp. 89-93</journal-ref><doi>10.1109/MSP.2015.33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information hiding techniques are increasingly utilized by the current
malware to hide its existence and communication attempts. In this paper we
highlight this new trend by reviewing the most notable examples of malicious
software that shows this capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04869</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04869</id><created>2015-04-19</created><authors><author><keyname>Furma&#x144;czyk</keyname><forenames>Hanna</forenames></author></authors><title>Equitable total coloring of corona of cubic graphs</title><categories>cs.DM math.CO</categories><msc-class>05C15, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum number of total independent sets of $V \cup E$ of graph $G(V,E)$
is called the \emph{total chromatic number} of $G$, denoted by $\chi&quot;(G)$. If
difference of cardinalities of any two total independent sets is at most one,
then the minimum number of total independent partition sets of $V \cup E$ is
called the \emph{equitable total chromatic number}, and denoted by
$\chi&quot;_=(G)$.
  In this paper we consider equitable total coloring of corona of cubic graphs,
$G \circ H$. It turns out that, independly on equitable total chromatic numbers
of $G$ and $H$, equitable total chromatic number of corona $G \circ H$ is equal
to $\Delta(G \circ H) +1$. Thereby, we confirm TCC and ETCC conjectures for
coronas of cubic graphs. As a direct consequence we get that all coronas of
cubic graphs are of Type 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04871</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04871</id><created>2015-04-19</created><authors><author><keyname>Shankar</keyname><forenames>Sukrit</forenames></author><author><keyname>Garg</keyname><forenames>Vikas K.</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets</title><categories>cs.CV</categories><comments>10 pages, 8 figures, CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the approaches for discovering visual attributes in images demand
significant supervision, which is cumbersome to obtain. In this paper, we aim
to discover visual attributes in a weakly supervised setting that is commonly
encountered with contemporary image search engines. Deep Convolutional Neural
Networks (CNNs) have enjoyed remarkable success in vision applications
recently. However, in a weakly supervised scenario, widely used CNN training
procedures do not learn a robust model for predicting multiple attribute labels
simultaneously. The primary reason is that the attributes highly co-occur
within the training data. To ameliorate this limitation, we propose
Deep-Carving, a novel training procedure with CNNs, that helps the net
efficiently carve itself for the task of multiple attribute prediction. During
training, the responses of the feature maps are exploited in an ingenious way
to provide the net with multiple pseudo-labels (for training images) for
subsequent iterations. The process is repeated periodically after a fixed
number of iterations, and enables the net carve itself iteratively for
efficiently disentangling features. Additionally, we contribute a
noun-adjective pairing inspired Natural Scenes Attributes Dataset to the
research community, CAMIT - NSAD, containing a number of co-occurring
attributes within a noun category. We describe, in detail, salient aspects of
this dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset,
with weak supervision, clearly demonstrate that the Deep-Carved CNNs
consistently achieve considerable improvement in the precision of attribute
prediction over popular baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04873</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04873</id><created>2015-04-19</created><authors><author><keyname>Vana</keyname><forenames>Laura</forenames></author><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Hornik</keyname><forenames>Kurt</forenames></author></authors><title>Computing a consensus journal meta-ranking using paired comparisons and
  adaptive lasso estimators</title><categories>stat.AP cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a &quot;publish-or-perish culture&quot;, the ranking of scientific journals plays a
central role in assessing performance in the current research environment. With
a wide range of existing methods and approaches to deriving journal rankings,
meta-rankings have gained popularity as a means of aggregating different
information sources. In this paper, we propose a method to create a consensus
meta-ranking using heterogeneous journal rankings. Using a parametric model for
paired comparison data we estimate quality scores for 58 journals in the OR/MS
community, which together with a shrinkage procedure allows for the
identification of clusters of journals with similar quality. The use of paired
comparisons provides a flexible framework for deriving a consensus score while
eliminating the problem of data missingness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04884</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04884</id><created>2015-04-19</created><updated>2015-10-03</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>R.</forenames></author><author><keyname>Bentz</keyname><forenames>C.</forenames></author><author><keyname>Seguin</keyname><forenames>C.</forenames></author></authors><title>Compression and the origins of Zipf's law of abbreviation</title><categories>cs.IT cs.CL cs.SI math.IT physics.data-an</categories><comments>The article has expanded significantly. The proof on the relationship
  between Kendall tau and compression (the minimization of the cost function)
  has been corrected. New arguments involving optimal coding with nonsigular
  codes have been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Languages across the world exhibit Zipf's law of abbreviation, namely more
frequent words tend to be shorter. The generalized version of the law - an
inverse relationship between the frequency of a unit and its magnitude - holds
also for the behaviours of other species and the genetic code. The apparent
universality of this pattern in human language and its ubiquity in other
domains calls for a theoretical understanding of its origins. To this end, we
generalize the information theoretic concept of mean code length as a mean
energetic cost function over the probability and the magnitude of the types of
the repertoire. We show that the minimization of that cost function and a
negative correlation between probability and the magnitude of types are
intimately related.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04896</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04896</id><created>2015-04-19</created><authors><author><keyname>Cal-Braz</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Projection-based list detection in Generalized Spatial Modulation MIMO
  systems</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a novel detection strategy for Spatially-Multiplexed
Generalized Spatial Modulation systems. It is a multi-stage detection that
produces a list of candidates of the transmitted signal vector, sorted
according to the proximity of the data vector to one of the possible vector
subspaces. The quality metric and list-length metric selects the best candidate
and manages the list length, respectively. Performance results show that it
significantly reduces the performance gap to the optimal maximum likelihood
detector, while maintaining significant computational cost reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04909</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04909</id><created>2015-04-19</created><authors><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author></authors><title>Illuminating search spaces by mapping elites</title><categories>cs.AI cs.NE cs.RO q-bio.PE</categories><comments>Early draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many fields use search algorithms, which automatically explore a search space
to find high-performing solutions: chemists search through the space of
molecules to discover new drugs; engineers search for stronger, cheaper, safer
designs, scientists search for models that best explain data, etc. The goal of
search algorithms has traditionally been to return the single
highest-performing solution in a search space. Here we describe a new,
fundamentally different type of algorithm that is more useful because it
provides a holistic view of how high-performing solutions are distributed
throughout a search space. It creates a map of high-performing solutions at
each point in a space defined by dimensions of variation that a user gets to
choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)
algorithm illuminates search spaces, allowing researchers to understand how
interesting attributes of solutions combine to affect performance, either
positively or, equally of interest, negatively. For example, a drug company may
wish to understand how performance changes as the size of molecules and their
cost-to-produce vary. MAP-Elites produces a large diversity of high-performing,
yet qualitatively different solutions, which can be more helpful than a single,
high-performing solution. Interestingly, because MAP-Elites explores more of
the search space, it also tends to find a better overall solution than
state-of-the-art search algorithms. We demonstrate the benefits of this new
algorithm in three different problem domains ranging from producing modular
neural networks to designing simulated and real soft robots. Because MAP-
Elites (1) illuminates the relationship between performance and dimensions of
interest in solutions, (2) returns a set of high-performing, yet diverse
solutions, and (3) improves finding a single, best solution, it will advance
science and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04913</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04913</id><created>2015-04-19</created><authors><author><keyname>Lugo</keyname><forenames>Abdul</forenames></author><author><keyname>Calder&#xf3;n</keyname><forenames>Giovanni</forenames></author></authors><title>Un An\'alisis Comparativo de los M\'etodos Mim\'eticos, Diferencias
  Finitas y Elementos Finitos para problemas Estacionarios</title><categories>math.NA cs.NA</categories><comments>in Spanish</comments><msc-class>65Y20, 65N06, 65N12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical methods: mimetic finite differences and finite elements, are
analyzed from a numerical point of view. It seeks to conclude on the
efficiency, order of convergence and computational cost of these methods. The
analysis is done in boundary value problems one-dimensional
(convection-diffusion equation at steady) with different variations in the
gradient, diffusion coefficient and convective velocity.
  Key Words: Mimetics methods, Finite Element methods, Finite differences
methods, Conservative methods, Convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04914</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04914</id><created>2015-04-19</created><updated>2016-03-07</updated><authors><author><keyname>Tang</keyname><forenames>Ke</forenames></author><author><keyname>Yang</keyname><forenames>Peng</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>Negatively Correlated Search</title><categories>cs.NE cs.AI</categories><journal-ref>IEEE Journal on Selected Areas in Communications, Vol. 34, Issue
  3, pp. 1-9, March 2016</journal-ref><doi>10.1109/JSAC.2016.2525458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Algorithms (EAs) have been shown to be powerful tools for
complex optimization problems, which are ubiquitous in both communication and
big data analytics. This paper presents a new EA, namely Negatively Correlated
Search (NCS), which maintains multiple individual search processes in parallel
and models the search behaviors of individual search processes as probability
distributions. NCS explicitly promotes negatively correlated search behaviors
by encouraging differences among the probability distributions (search
behaviors). By this means, individual search processes share information and
cooperate with each other to search diverse regions of a search space, which
makes NCS a promising method for non-convex optimization. The cooperation
scheme of NCS could also be regarded as a novel diversity preservation scheme
that, different from other existing schemes, directly promotes diversity at the
level of search behaviors rather than merely trying to maintain diversity among
candidate solutions. Empirical studies showed that NCS is competitive to
well-established search methods in the sense that NCS achieved the best overall
performance on 20 multimodal (non-convex) continuous optimization problems. The
advantages of NCS over state-of-the-art approaches are also demonstrated with a
case study on the synthesis of unequally spaced linear antenna arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04916</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04916</id><created>2015-04-19</created><authors><author><keyname>Lou</keyname><forenames>Taishan</forenames></author></authors><title>Desensitized Kalman Filtering with Analytical Gain</title><categories>cs.IT math.IT</categories><comments>15 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possible methodologies to handle the uncertain parameter are reviewed.
The core idea of the desensitized Kalman filter is introduced. A new cost
function consisting of a posterior covariance trace and trace of a weighted
norm of the state error sensitivities matrix is minimizing to obtain a
well-known analytical gain matrix, which is different from the gain of the
desensitized Kalman filter. The pre-estimated uncertain parameter covariance is
set as a referential sensitivity-weighting matrix in the new framework, and the
rationality and validity of the covariance are tested. Then, these results are
extended to the linear continuous system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04920</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04920</id><created>2015-04-19</created><authors><author><keyname>Swenson</keyname><forenames>Brian</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author></authors><title>From Weak Learning to Strong Learning in Fictitious Play Type Algorithms</title><categories>math.OC cs.GT math.PR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the highly prototypical Fictitious Play (FP) algorithm, as
well as a broad class of learning processes based on best-response dynamics,
that we refer to as FP-type algorithms. A well-known shortcoming of FP is that,
while players may learn an equilibrium strategy in some abstract sense, there
are no guarantees that the period-by-period strategies generated by the
algorithm actually converge to equilibrium themselves. This issue is
fundamentally related to the discontinuous nature of the best response
correspondence and is inherited by many FP-type algorithms. Not only does it
cause problems in the interpretation of such algorithms as a mechanism for
economic and social learning, but it also greatly diminishes the practical
value of these algorithms for use in distributed control. We refer to forms of
learning in which players learn equilibria in some abstract sense only (to be
defined more precisely in the paper) as weak learning, and we refer to forms of
learning where players' period-by-period strategies converge to equilibrium as
strong learning. An approach is presented for modifying an FP-type algorithm
that achieves weak learning in order to construct a variant that achieves
strong learning. Theoretical convergence results are proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04923</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04923</id><created>2015-04-19</created><authors><author><keyname>Qiao</keyname><forenames>Ruizhi</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton von den</forenames></author></authors><title>Learning discriminative trajectorylet detector sets for accurate
  skeleton-based action recognition</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of low-cost RGB-D sensors has promoted the research in
skeleton-based human action recognition. Devising a representation suitable for
characterising actions on the basis of noisy skeleton sequences remains a
challenge, however. We here provide two insights into this challenge. First, we
show that the discriminative information of a skeleton sequence usually resides
in a short temporal interval and we propose a simple-but-effective local
descriptor called trajectorylet to capture the static and kinematic information
within this interval. Second, we further propose to encode each trajectorylet
with a discriminative trajectorylet detector set which is selected from a large
number of candidate detectors trained through exemplar-SVMs. The action-level
representation is obtained by pooling trajectorylet encodings. Evaluating on
standard datasets acquired from the Kinect sensor, it is demonstrated that our
method obtains superior results over existing approaches under various
experimental setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04926</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04926</id><created>2015-04-19</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Locally Encodable and Decodable Codes for Distributed Storage Systems</title><categories>cs.IT math.CO math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the locality of encoding and decoding operations in distributed
storage systems (DSS), and propose a new class of codes, called locally
encodable and decodable codes (LEDC), that provides a higher degree of
operational locality compared to currently known codes. For a given locality
structure, we derive an upper bound on the global distance and demonstrate the
existence of an optimal LEDC for sufficiently large field size. In addition, we
also construct two families of optimal LEDC for fields with size linear in code
length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04930</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04930</id><created>2015-04-20</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Connecting Multiple-unicast and Network Error Correction: Reduction and
  Unachievability</title><categories>cs.IT math.IT</categories><comments>ISIT 2015. arXiv admin note: text overlap with arXiv:1410.1905</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that solving a multiple-unicast network coding problem can be reduced
to solving a single-unicast network error correction problem, where an
adversary may jam at most a single edge in the network. Specifically, we
present an efficient reduction that maps a multiple-unicast network coding
instance to a network error correction instance while preserving feasibility.
The reduction holds for both the zero probability of error model and the
vanishing probability of error model. Previous reductions are restricted to the
zero-error case. As an application of the reduction, we present a constructive
example showing that the single-unicast network error correction capacity may
not be achievable, a result of separate interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04931</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04931</id><created>2015-04-20</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>McCarthy</keyname><forenames>J. Michael</forenames></author><author><keyname>Parrish</keyname><forenames>Brian E.</forenames></author></authors><title>Rooted Cycle Bases</title><categories>cs.DS</categories><comments>12 pages with 10 additional pages of appendices and 10 figures.
  Extended version of a paper to appear at the 14th Algorithms and Data
  Structures Symposium (WADS), Victoria, BC, August 2015</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cycle basis in an undirected graph is a minimal set of simple cycles whose
symmetric differences include all Eulerian subgraphs of the given graph. We
define a rooted cycle basis to be a cycle basis in which all cycles contain a
specified root edge, and we investigate the algorithmic problem of constructing
rooted cycle bases. We show that a given graph has a rooted cycle basis if and
only if the root edge belongs to its 2-core and the 2-core is
2-vertex-connected, and that constructing such a basis can be performed
efficiently. We show that in an unweighted or positively weighted graph, it is
possible to find the minimum weight rooted cycle basis in polynomial time.
Additionally, we show that it is NP-complete to find a fundamental rooted cycle
basis (a rooted cycle basis in which each cycle is formed by combining paths in
a fixed spanning tree with a single additional edge) but that the problem can
be solved by a fixed-parameter-tractable algorithm when parameterized by
clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04934</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04934</id><created>2015-04-20</created><authors><author><keyname>Wang</keyname><forenames>Qiming</forenames></author><author><keyname>Li</keyname><forenames>Liping</forenames></author></authors><title>On the Symmetry of Polar Codes for Symmetric Binary-Input Discrete
  Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, no figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the symmetry of polar codes on symmetric binary-input
discrete memoryless channels (B-DMC). The symmetry property of polar codes is
originally pointed out in Arikan's work for general B-DMC channels. With the
symmetry, the output vector $y_1^N$ ($N$ be the block length) can be divided
into equivalence classes in terms of their transition probabilities. In this
paper, we present a new frame of analysis on the symmetry of polar codes for
B-DMC channels. Theorems are provided to characterize the symmetries among the
received vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04938</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04938</id><created>2015-04-20</created><authors><author><keyname>Shahrokhi</keyname><forenames>Farhad</forenames></author></authors><title>A new separation theorem with geometric applications</title><categories>cs.CG</categories><comments>Proceedings of EuroCG 2010, Dortmund, Germany, March 22-24, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V(G), E(G))$ be an undirected graph with a measure function $\mu$
assigning non-negative values to subgraphs $H$ so that $\mu(H)$ does not exceed
the clique cover number of $H$. When $\mu$ satisfies some additional natural
conditions, we study the problem of separating $G$ into two subgraphs, each
with a measure of at most $2\mu(G)/3$ by removing a set of vertices that can be
covered with a small number of cliques $G$. When $E(G)=E(G_1)\cap E(G_2)$,
where $G_1=(V(G_1),E(G_1))$ is a graph with $V(G_1)=V(G)$, and $G_2=(V(G_2),
E(G_2))$ is a chordal graph with $V(G_2)=V(G)$, we prove that there is a
separator $S$ that can be covered with $O(\sqrt{l\mu(G)})$ cliques in $G$,
where $l=l(G,G_1)$ is a parameter similar to the bandwidth, which arises from
the linear orderings of cliques covers in $G_1$. The results and the methods
are then used to obtain exact and approximate algorithms which significantly
improve some of the past results for several well known NP-hard geometric
problems. In addition, the methods involve introducing new concepts and hence
may be of an independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04942</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04942</id><created>2015-04-20</created><authors><author><keyname>Benz</keyname><forenames>Samuel</forenames></author><author><keyname>de Sousa</keyname><forenames>Leandro Pacheco</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author></authors><title>Stretching Multi-Ring Paxos</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet-scale services rely on data partitioning and replication to provide
scalable performance and high availability. Moreover, to reduce user-perceived
response times and tolerate disasters (i.e., the failure of a whole
datacenter), services are increasingly becoming geographically distributed.
Data partitioning and replication, combined with local and geographical
distribution, introduce daunting challenges, including the need to carefully
order requests among replicas and partitions. One way to tackle this problem is
to use group communication primitives that encapsulate order requirements. This
paper presents a detailed performance evaluation of Multi-Ring Paxos, a
scalable group communication primitive. We focus our analysis on &quot;extreme
conditions&quot; with deployments including high-end 10 Gbps networks, a large
number of combined rings (i.e., independent Paxos instances), a large number of
replicas in a ring, and a global deployment. We also report on the performance
of recovery under peak load and present two novel extensions to boost
Multi-Ring Paxos's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04943</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04943</id><created>2015-04-20</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Wei</keyname><forenames>Xiu-shen</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author><author><keyname>Lu</keyname><forenames>Jiangbo</forenames></author><author><keyname>Nguyen</keyname><forenames>Viet-Anh</forenames></author><author><keyname>Do</keyname><forenames>Minh N.</forenames></author></authors><title>Weakly Supervised Fine-Grained Image Categorization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we categorize fine-grained images without using any object /
part annotation neither in the training nor in the testing stage, a step
towards making it suitable for deployments. Fine-grained image categorization
aims to classify objects with subtle distinctions. Most existing works heavily
rely on object / part detectors to build the correspondence between object
parts by using object or object part annotations inside training images. The
need for expensive object annotations prevents the wide usage of these methods.
Instead, we propose to select useful parts from multi-scale part proposals in
objects, and use them to compute a global image representation for
categorization. This is specially designed for the annotation-free fine-grained
categorization task, because useful parts have shown to play an important role
in existing annotation-dependent works but accurate part detectors can be
hardly acquired. With the proposed image representation, we can further detect
and visualize the key (most discriminative) parts in objects of different
classes. In the experiment, the proposed annotation-free method achieves better
accuracy than that of state-of-the-art annotation-free and most existing
annotation-dependent methods on two challenging datasets, which shows that it
is not always necessary to use accurate object / part annotations in
fine-grained image categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04945</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04945</id><created>2015-04-20</created><authors><author><keyname>Zhu</keyname><forenames>Yadong</forenames></author><author><keyname>Lan</keyname><forenames>Yanyan</forenames></author><author><keyname>Guo</keyname><forenames>Jiafeng</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Topic-focused Dynamic Information Filtering in Social Media</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the quick development of online social media such as twitter or sina
weibo in china, many users usually track hot topics to satisfy their desired
information need. For a hot topic, new opinions or ideas will be continuously
produced in the form of online data stream. In this scenario, how to
effectively filter and display information for a certain topic dynamically,
will be a critical problem. We call the problem as Topic-focused Dynamic
Information Filtering (denoted as TDIF for short) in social media. In this
paper, we start open discussions on such application problems. We first analyze
the properties of the TDIF problem, which usually contains several typical
requirements: relevance, diversity, recency and confidence. Recency means that
users want to follow the recent opinions or news. Additionally, the confidence
of information must be taken into consideration. How to balance these factors
properly in online data stream is very important and challenging. We propose a
dynamic preservation strategy on the basis of an existing feature-based utility
function, to solve the TDIF problem. Additionally, we propose new dynamic
diversity measures, to get a more reasonable evaluation for such application
problems. Extensive exploratory experiments have been conducted on TREC public
twitter dataset, and the experimental results validate the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04948</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04948</id><created>2015-04-20</created><updated>2015-09-17</updated><authors><author><keyname>Tian</keyname><forenames>Xiaohua</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>Defending Against DDoS Attacks in Bloom Filter based Multicasting</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Bloom filter (BF) based forwarding is an effective approach to implement
scalable multicasting in distributed systems. The forwarding BF carried by each
packet can encode either multicast tree or destination IP addresses, which are
termed as tree oriented approach (TOA) and destination oriented approach (DOA),
respectively. Recent studies have indicated that TOA based protocols have
serious vulnerabilities under some distributed denial-of-service (DDoS)
attacks, and raised doubt about deployability of BF based multicasting.
However, security analysis for DOA based protocols is still unavailable. In
this paper, we present a systematic analysis of security performance of BF
based multicasting. Important DDoS attacks and the corresponding defending
mechanisms are studied in the context of DOA. We have positive findings that
DOA, with convenient enhancement, has a robust performance in resisting a
variety of DDoS attacks that can deny service of TOA based protocols. Moreover,
we reveal that TOA based protocols are prone to flow duplication attack when
applied in the data center network (DCN). We propose a dynamic-sized BF
mechanism to defend against flow duplication attack for TOA based protocols in
the DCN. Simulation results are presented to validate our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04950</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04950</id><created>2015-04-20</created><updated>2015-04-27</updated><authors><author><keyname>Vereshchagin</keyname><forenames>Nikolay</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Algorithmic statistics revisited</title><categories>cs.IT math.IT</categories><msc-class>68Q30</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mission of statistics is to provide adequate statistical hypotheses
(models) for observed data. But what is an &quot;adequate&quot; model? To answer this
question, one needs to use the notions of algorithmic information theory. It
turns out that for every data string $x$ one can naturally define
&quot;stochasticity profile&quot;, a curve that represents a trade-off between complexity
of a model and its adequacy. This curve has four different equivalent
definitions in terms of (1)~randomness deficiency, (2)~minimal description
length, (3)~position in the lists of simple strings and (4)~Kolmogorov
complexity with decompression time bounded by busy beaver function. We present
a survey of the corresponding definitions and results relating them to each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04955</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04955</id><created>2015-04-20</created><authors><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Around Kolmogorov complexity: basic notions and results</title><categories>cs.IT math.IT math.LO</categories><msc-class>68Q30</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic information theory studies description complexity and randomness
and is now a well known field of theoretical computer science and mathematical
logic. There are several textbooks and monographs devoted to this theory where
one can find the detailed exposition of many difficult results as well as
historical references. However, it seems that a short survey of its basic
notions and main results relating these notions to each other, is missing.
  This report attempts to fill this gap and covers the basic notions of
algorithmic information theory: Kolmogorov complexity (plain, conditional,
prefix), Solomonoff universal a priori probability, notions of randomness
(Martin-L\&quot;of randomness, Mises--Church randomness), effective Hausdorff
dimension. We prove their basic properties (symmetry of information, connection
between a priori probability and prefix complexity, criterion of randomness in
terms of complexity, complexity characterization for effective dimension) and
show some applications (incompressibility method in computational complexity
theory, incompleteness theorems). It is based on the lecture notes of a course
at Uppsala University given by the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04960</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04960</id><created>2015-04-20</created><authors><author><keyname>Thomas</keyname><forenames>Anoop</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Vector Linear Error Correcting Index Codes and Discrete Polymatroids</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.05060</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connection between index coding and matroid theory have been well studied
in the recent past. El Rouayheb et al. established a connection between multi
linear representation of matroids and wireless index coding. Muralidharan and
Rajan showed that a vector linear solution to an index coding problem exists if
and only if there exists a representable discrete polymatroid satisfying
certain conditions. Recently index coding with erroneous transmission was
considered by Dau et al.. Error correcting index codes in which all receivers
are able to correct a fixed number of errors was studied. In this paper we
consider a more general scenario in which each receiver is able to correct a
desired number of errors, calling such index codes differential error
correcting index codes. We show that vector linear differential error
correcting index code exists if and only if there exists a representable
discrete polymatroid satisfying certain conditions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04970</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04970</id><created>2015-04-20</created><updated>2015-04-22</updated><authors><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Information-Theoretic Limits of Matrix Completion</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an information-theoretic framework for matrix completion. The
theory goes beyond the low-rank structure and applies to general matrices of
&quot;low description complexity&quot;. Specifically, we consider $m\times n$ random
matrices $\mathbf{X}$ of arbitrary distribution (continuous, discrete,
discrete-continuous mixture, or even singular). With $\mathcal{S}$ an
$\varepsilon$-support set of $\mathbf{X}$, i.e.,
$\mathrm{P}[\mathbf{X}\in\mathcal{S}]\geq 1-\varepsilon$, and
$\underline{\mathrm{dim}}_\mathrm{B}(\mathcal{S})$ denoting the lower Minkowski
dimension of $\mathcal{S}$, we show that $k&gt;
\underline{\mathrm{dim}}_\mathrm{B}(\mathcal{S})$ trace inner product
measurements with measurement matrices $A_i$, suffice to recover $\mathbf{X}$
with probability of error at most $\varepsilon$. The result holds for Lebesgue
a.a. $A_i$ and does not need incoherence between the $A_i$ and the unknown
matrix $\mathbf{X}$. We furthermore show that $k&gt;
\underline{\mathrm{dim}}_\mathrm{B}(\mathcal{S})$ measurements also suffice to
recover the unknown matrix $\mathbf{X}$ from measurements taken with rank-one
$A_i$, again this applies to a.a. rank-one $A_i$. Rank-one measurement matrices
are attractive as they require less storage space than general measurement
matrices and can be applied faster. Particularizing our results to the recovery
of low-rank matrices, we find that $k&gt;(m+n-r)r$ measurements are sufficient to
recover matrices of rank at most $r$. Finally, we construct a class of rank-$r$
matrices that can be recovered with arbitrarily small probability of error from
$k&lt;(m+n-r)r$ measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04971</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04971</id><created>2015-04-20</created><updated>2015-04-21</updated><authors><author><keyname>Plate</keyname><forenames>Henrik</forenames></author><author><keyname>Ponta</keyname><forenames>Serena Elisa</forenames></author><author><keyname>Sabetta</keyname><forenames>Antonino</forenames></author></authors><title>Impact assessment for vulnerabilities in open-source software libraries</title><categories>cs.CR cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software applications integrate more and more open-source software (OSS) to
benefit from code reuse. As a drawback, each vulnerability discovered in
bundled OSS potentially affects the application. Upon the disclosure of every
new vulnerability, the application vendor has to decide whether it is
exploitable in his particular usage context, hence, whether users require an
urgent application patch containing a non-vulnerable version of the OSS.
Current decision making is mostly based on high-level vulnerability
descriptions and expert knowledge, thus, effort intense and error prone. This
paper proposes a pragmatic approach to facilitate the impact assessment,
describes a proof-of-concept for Java, and examines one example vulnerability
as case study. The approach is independent from specific kinds of
vulnerabilities or programming languages and can deliver immediate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04974</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04974</id><created>2015-04-20</created><authors><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lixin</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Sun</keyname><forenames>Ninghui</forenames></author></authors><title>Understanding Big Data Analytic Workloads on Modern Processors</title><categories>cs.DC cs.PF</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.8013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data analytics applications play a significant role in data centers, and
hence it has become increasingly important to understand their behaviors in
order to further improve the performance of data center computer systems, in
which characterizing representative workloads is a key practical problem. In
this paper, after investigating three most impor- tant application domains in
terms of page views and daily visitors, we chose 11 repre- sentative data
analytics workloads and characterized their micro-architectural behaviors by
using hardware performance counters, so as to understand the impacts and
implications of data analytics workloads on the systems equipped with modern
superscalar out-of-order processors. Our study reveals that big data analytics
applications themselves share many inherent characteristics, which place them
in a different class from traditional workloads and scale-out services. To
further understand the characteristics of big data analytics work- loads we
performed a correlation analysis of CPI (cycles per instruction) with other
micro- architecture level characteristics and an investigation of the big data
software stack impacts on application behaviors. Our correlation analysis
showed that even though big data ana- lytics workloads own notable pipeline
front end stalls, the main factors affecting the CPI performance are long
latency data accesses rather than the front end stalls. Our software stack
investigation found that the typical big data software stack significantly
contributes to the front end stalls and incurs bigger working set. Finally we
gave several recommen- dations for architects, programmers and big data system
designers with the knowledge acquired from this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04975</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04975</id><created>2015-04-20</created><updated>2015-05-29</updated><authors><author><keyname>Ranganathan</keyname><forenames>Sudarsan V. S.</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>On the Girth of (3,L) Quasi-Cyclic LDPC Codes based on Complete
  Protographs</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, 5-page version to appear in the Proceedings of
  2015 IEEE International Symposium on Information Theory. Update 1 -
  05/29/2015 - Minor changes and added a reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing $(3,L)$ quasi-cyclic low-density
parity-check (LDPC) codes from complete protographs. A complete protograph is a
small bipartite graph with two disjoint vertex sets such that every vertex in
the variable-node set is connected to every vertex in the check-node set by a
unique edge. This paper analyzes the required lifting factor for achieving
girths of six or eight in the resulting quasi-cyclic codes with constraints on
lifting. The required lifting factors provide lower bounds on the block-length
of such codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04977</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04977</id><created>2015-04-20</created><authors><author><keyname>Qin</keyname><forenames>Xiaolin</forenames></author><author><keyname>Yang</keyname><forenames>Lu</forenames></author><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Bachmann</keyname><forenames>Bernhard</forenames></author><author><keyname>Fritzson</keyname><forenames>Peter</forenames></author></authors><title>Index reduction of differential algebraic equations by differential
  algebraic elimination</title><categories>cs.SC</categories><comments>19 pages, 1figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  High index differential algebraic equations (DAEs) are ordinary differential
equations (ODEs) with constraints and arise frequently from many mathematical
models of physical phenomenons and engineering fields. In this paper, we
generalize the idea of differential elimination with Dixon resultant to
polynomially nonlinear DAEs. We propose a new algorithm for index reduction of
DAEs and establish the notion of differential algebraic elimination, which can
provide the differential algebraic resultant of the enlarged system of original
equations. To make use of structure of DAEs, variable pencil technique is given
to determine the termination of differentiation. Moreover, we also provide a
heuristics method for removing the extraneous factors from differential
algebraic resultant. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.04993</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.04993</id><created>2015-04-20</created><authors><author><keyname>Kuhlmann</keyname><forenames>Marco</forenames></author></authors><title>Tabulation of Noncrossing Acyclic Digraphs</title><categories>cs.DS math.CO</categories><comments>9 pages, several figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present an algorithm that, given a number $n \geq 1$, computes a compact
representation of the set of all noncrossing acyclic digraphs with $n$ nodes.
This compact representation can be used as the basis for a wide range of
dynamic programming algorithms on these graphs. As an illustration, along with
this note I am releasing the implementation of an algorithm for counting the
number of noncrossing acyclic digraphs of a given size. The same tabulation can
be modified to count other classes of combinatorial structures, including
weakly connected noncrossing acyclic digraphs, general noncrossing digraphs,
noncrossing undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05008</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05008</id><created>2015-04-20</created><authors><author><keyname>R</keyname><forenames>Kavitha.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On the Number of Optimal Index Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Index coding there is a single sender with multiple messages and multiple
receivers each wanting a different set of messages and knowing a different set
of messages a priori. The Index Coding problem is to identify the minimum
number of transmissions (optimal length) to be made so that all receivers can
decode their wanted messages using the transmitted symbols and their respective
prior information and also the codes with optimal length. Recently it was shown
that different optimal length codes perform differently in a wireless channel.
Towards identifying the best optimal length index code one needs to know the
number of optimal length index codes. In this paper we present results on the
number of optimal length index codes making use of the representation of an
index coding problem by an equivalent network code. We give the minimum number
of codes possible with the optimal length. This is done using a simpler
algebraic formulation of the problem compared to the approach of Koetter and
Medard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05009</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05009</id><created>2015-04-20</created><authors><author><keyname>Adjiashvili</keyname><forenames>David</forenames></author></authors><title>Non-Uniform Robust Network Design in Planar Graphs</title><categories>math.OC cs.DS</categories><comments>17 pages, 2 figures</comments><acm-class>I.1.2; G.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimization is concerned with constructing solutions that remain
feasible also when a limited number of resources is removed from the solution.
Most studies of robust combinatorial optimization to date made the assumption
that every resource is equally vulnerable, and that the set of scenarios is
implicitly given by a single budget constraint. This paper studies a robustness
model of a different kind. We focus on \textbf{bulk-robustness}, a model
recently introduced~\cite{bulk} for addressing the need to model non-uniform
failure patterns in systems.
  We significantly extend the techniques used in~\cite{bulk} to design
approximation algorithm for bulk-robust network design problems in planar
graphs. Our techniques use an augmentation framework, combined with linear
programming (LP) rounding that depends on a planar embedding of the input
graph. A connection to cut covering problems and the dominating set problem in
circle graphs is established. Our methods use few of the specifics of
bulk-robust optimization, hence it is conceivable that they can be adapted to
solve other robust network design problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05018</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05018</id><created>2015-04-20</created><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames></author><author><keyname>Genaim</keyname><forenames>Samir</forenames></author></authors><title>Complexity of Bradley-Manna-Sipma Lexicographic Ranking Functions</title><categories>cs.PL cs.LO</categories><comments>Technical report for a corresponding CAV'15 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we turn the spotlight on a class of lexicographic ranking
functions introduced by Bradley, Manna and Sipma in a seminal CAV 2005 paper,
and establish for the first time the complexity of some problems involving the
inference of such functions for linear-constraint loops (without precondition).
We show that finding such a function, if one exists, can be done in polynomial
time in a way which is sound and complete when the variables range over the
rationals (or reals). We show that when variables range over the integers, the
problem is harder -- deciding the existence of a ranking function is
coNP-complete. Next, we study the problem of minimizing the number of
components in the ranking function (a.k.a. the dimension). This number is
interesting in contexts like computing iteration bounds and loop
parallelization. Surprisingly, and unlike the situation for some other classes
of lexicographic ranking functions, we find that even deciding whether a
two-component ranking function exists is harder than the unrestricted problem:
NP-complete over the rationals and $\Sigma^P_2$-complete over the integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05022</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05022</id><created>2015-04-20</created><updated>2015-09-13</updated><authors><author><keyname>Liu</keyname><forenames>Weifeng</forenames></author><author><keyname>Vinter</keyname><forenames>Brian</forenames></author></authors><title>A Framework for General Sparse Matrix-Matrix Multiplication on GPUs and
  Heterogeneous Processors</title><categories>cs.MS cs.DC math.NA</categories><comments>25 pages, 12 figures, published at Journal of Parallel and
  Distributed Computing (JPDC)</comments><msc-class>65F50</msc-class><acm-class>G.4; G.1.3</acm-class><doi>10.1016/j.jpdc.2015.06.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General sparse matrix-matrix multiplication (SpGEMM) is a fundamental
building block for numerous applications such as algebraic multigrid method
(AMG), breadth first search and shortest path problem. Compared to other sparse
BLAS routines, an efficient parallel SpGEMM implementation has to handle extra
irregularity from three aspects: (1) the number of nonzero entries in the
resulting sparse matrix is unknown in advance, (2) very expensive parallel
insert operations at random positions in the resulting sparse matrix dominate
the execution time, and (3) load balancing must account for sparse data in both
input matrices.
  In this work we propose a framework for SpGEMM on GPUs and emerging CPU-GPU
heterogeneous processors. This framework particularly focuses on the above
three problems. Memory pre-allocation for the resulting matrix is organized by
a hybrid method that saves a large amount of global memory space and
efficiently utilizes the very limited on-chip scratchpad memory. Parallel
insert operations of the nonzero entries are implemented through the GPU merge
path algorithm that is experimentally found to be the fastest GPU merge
approach. Load balancing builds on the number of necessary arithmetic
operations on the nonzero entries and is guaranteed in all stages.
  Compared with the state-of-the-art CPU and GPU SpGEMM methods, our approach
delivers excellent absolute performance and relative speedups on various
benchmarks multiplying matrices with diverse sparsity structures. Furthermore,
on heterogeneous processors, our SpGEMM approach achieves higher throughput by
using re-allocatable shared virtual memory.
  The source code of this work is available at
https://github.com/bhSPARSE/Benchmark_SpGEMM_using_CSR
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05025</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05025</id><created>2015-04-20</created><authors><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>Resource Management and Cell Planning in Millimeter-Wave Overlaid
  Ultra-Dense Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a cellular network exploiting millimeter-wave (mmWave)
and ultra-densified base stations (BSs) to achieve the far-reaching 5G aim in
downlink average rate. The mmWave overlaid network however incurs a pitfall
that its ample data rate is only applicable for downlink transmissions due to
the implementation difficulty at mobile users, leading to an immense difference
between uplink and downlink rates. We therefore turn our attention not only to
maximize downlink rate but also to ensure the minimum uplink rate. With this
end, we firstly derive the mmWave overlaid ultra-dense cellular network
spectral efficiencies for both uplink and downlink cases in closed forms by
using stochastic geometry via a lower bound approximation. In a practical
scenario, such tractable results of the proposed network reveal that incumbent
micro-wave ($\mu$Wave) cellular resource should be mostly dedicated to uplink
transmissions in order to correspond with the mmWave downlink rate improvement.
Furthermore, increasing uplink rate via $\mu$Wave BS densification cannot
solely cope with the mmWave downlink/uplink rate asymmetry, and thus requires
additional $\mu$Wave spectrum in 5G cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05032</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05032</id><created>2015-04-20</created><authors><author><keyname>Krauss</keyname><forenames>Patrick</forenames></author><author><keyname>Metzner</keyname><forenames>Claus</forenames></author><author><keyname>Tziridis</keyname><forenames>Konstantin</forenames></author><author><keyname>Schulze</keyname><forenames>Holger</forenames></author></authors><title>Adaptive stochastic resonance based on output autocorrelations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful detection of weak signals is a universal challenge for numerous
technical and biological systems and crucially limits signal transduction and
transmission. Stochastic resonance (SR) has been identified to have the
potential to tackle this problem, namely to enable non-linear systems to detect
small, otherwise sub-threshold signals by means of added non-zero noise. This
has been demonstrated within a wide range of systems in physical, technological
and biological contexts. Based on its ubiquitous importance, numerous
theoretical and technical approaches aim at an optimization of signal
transduction based on SR. Several quantities like mutual information,
signal-to-noise-ratio, or the cross-correlation between input stimulus and
resulting detector response have been used to determine optimal noise
intensities for SR. The fundamental shortcoming with all these measures is that
knowledge of the signal to be detected is required to compute them. This
dilemma prevents the use of adaptive SR procedures in any application where the
signal to be detected is unknown. We here show that the autocorrelation
function (AC) of the detector response fundamentally overcomes this drawback.
For a simplified model system, the equivalence of the output AC with the
measures mentioned above is proven analytically. In addition, we test our
approach numerically for a variety of systems comprising different input
signals and different types of detectors. The results indicate a strong
similarity between mutual information and output AC in terms of the optimal
noise intensity for SR. Hence, using the output AC to adaptively vary the
amount of added noise in order to maximize information transmission via SR
might be a fundamental processing principle in nature, in particular within
neural systems which could be implemented in future technical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05035</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05035</id><created>2015-04-20</created><authors><author><keyname>Wu</keyname><forenames>Xiaohe</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Zhu</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>F-SVM: Combination of Feature Transformation and SVM Learning via Convex
  Relaxation</title><categories>cs.LG cs.CV</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalization error bound of support vector machine (SVM) depends on the
ratio of radius and margin, while standard SVM only considers the maximization
of the margin but ignores the minimization of the radius. Several approaches
have been proposed to integrate radius and margin for joint learning of feature
transformation and SVM classifier. However, most of them either require the
form of the transformation matrix to be diagonal, or are non-convex and
computationally expensive. In this paper, we suggest a novel approximation for
the radius of minimum enclosing ball (MEB) in feature space, and then propose a
convex radius-margin based SVM model for joint learning of feature
transformation and SVM classifier, i.e., F-SVM. An alternating minimization
method is adopted to solve the F-SVM model, where the feature transformation is
updatedvia gradient descent and the classifier is updated by employing the
existing SVM solver. By incorporating with kernel principal component analysis,
F-SVM is further extended for joint learning of nonlinear transformation and
classifier. Experimental results on the UCI machine learning datasets and the
LFW face datasets show that F-SVM outperforms the standard SVM and the existing
radius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\mu}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05036</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05036</id><created>2015-04-20</created><authors><author><keyname>Aubel</keyname><forenames>C&#xe9;line</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Density Criteria for the Identification of Linear Time-Varying Systems</title><categories>cs.IT math.FA math.IT</categories><comments>IEEE International Symposium on Information Theory (ISIT), Hong Kong,
  China, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of identifying a linear time-varying (LTV)
system characterized by a (possibly infinite) discrete set of delays and
Doppler shifts. We prove that stable identifiability is possible if the upper
uniform Beurling density of the delay-Doppler support set is strictly smaller
than 1/2 and stable identifiability is impossible for densities strictly larger
than 1/2. The proof of this density theorem reveals an interesting relation
between LTV system identification and interpolation in the Bargmann-Fock space.
Finally, we introduce a subspace method for solving the system identification
problem at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05038</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05038</id><created>2015-04-20</created><updated>2015-04-21</updated><authors><author><keyname>Assaf</keyname><forenames>Ali</forenames></author></authors><title>Conservativity of embeddings in the lambda Pi calculus modulo rewriting
  (long version)</title><categories>cs.LO</categories><comments>Long version of TLCA 2015 paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The lambda Pi calculus can be extended with rewrite rules to embed any
functional pure type system. In this paper, we show that the embedding is
conservative by proving a relative form of normalization, thus justifying the
use of the lambda Pi calculus modulo rewriting as a logical framework for
logics based on pure type systems. This result was previously only proved under
the condition that the target system is normalizing. Our approach does not
depend on this condition and therefore also works when the source system is not
normalizing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05046</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05046</id><created>2015-04-20</created><authors><author><keyname>Calvin</keyname><forenames>Justus A.</forenames></author><author><keyname>Valeev</keyname><forenames>Edward F.</forenames></author></authors><title>Task-Based Algorithm for Matrix Multiplication: A Step Towards
  Block-Sparse Tensor Computing</title><categories>cs.DC</categories><comments>submitted to SC15 (9 pages, 8 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed-memory matrix multiplication (MM) is a key element of algorithms
in many domains (machine learning, quantum physics). Conventional algorithms
for dense MM rely on regular/uniform data decomposition to ensure load balance.
These traits conflict with the irregular structure (block-sparse or rank-sparse
within blocks) that is increasingly relevant for fast methods in quantum
physics. To deal with such irregular data we present a new MM algorithm based
on Scalable Universal Matrix Multiplication Algorithm (SUMMA). The novel
features are: (1) multiple-issue scheduling of SUMMA iterations, and (2)
fine-grained task-based formulation. The latter eliminates the need for
explicit internodal synchronization; with multiple-iteration scheduling this
allows load imbalance due to nonuniform matrix structure. For square MM with
uniform and nonuniform block sizes (the latter simulates matrices with general
irregular structure) we found excellent performance in weak and strong-scaling
regimes, on commodity and high-end hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05058</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05058</id><created>2015-04-20</created><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Markin</keyname><forenames>Nadya</forenames></author></authors><title>Constructions of Fast-Decodable Distributed Space-Time Codes</title><categories>cs.IT math.IT</categories><comments>Presented at the Fourth International Meeting on Coding Theory and
  Applications. To appear in CIM Series in Mathematical Sciences, Springer
  Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast-decodable distributed space-time codes are constructed by adapting the
iterative code construction introduced in [1] to the N -relay multiple-input
multiple-output channel, leading to the first fast-decodable distributed
space-time codes for more than one antenna per user. Explicit constructions are
provided alongside with a performance comparison to non-iterated (non-)
fast-decodable codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05059</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05059</id><created>2015-04-20</created><authors><author><keyname>Tschannen</keyname><forenames>Michael</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Nonparametric Nearest Neighbor Random Process Clustering</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>IEEE International Symposium on Information Theory (ISIT), June 2015,
  to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering noisy finite-length observations of
stationary ergodic random processes according to their nonparametric generative
models without prior knowledge of the model statistics and the number of
generative models. Two algorithms, both using the L1-distance between estimated
power spectral densities (PSDs) as a measure of dissimilarity, are analyzed.
The first algorithm, termed nearest neighbor process clustering (NNPC), to the
best of our knowledge, is new and relies on partitioning the nearest neighbor
graph of the observations via spectral clustering. The second algorithm, simply
referred to as k-means (KM), consists of a single k-means iteration with
farthest point initialization and was considered before in the literature,
albeit with a different measure of dissimilarity and with asymptotic
performance results only. We show that both NNPC and KM succeed with high
probability under noise and even when the generative process PSDs overlap
significantly, all provided that the observation length is sufficiently large.
Our results quantify the tradeoff between the overlap of the generative process
PSDs, the noise variance, and the observation length. Finally, we present
numerical performance results for synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05070</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05070</id><created>2015-04-20</created><updated>2015-04-27</updated><authors><author><keyname>Zhao</keyname><forenames>Han</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>Self-Adaptive Hierarchical Sentence Model</title><categories>cs.CL cs.LG cs.NE</categories><comments>8 pages, 7 figures, accepted as a full paper at IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to accurately model a sentence at varying stages (e.g.,
word-phrase-sentence) plays a central role in natural language processing. As
an effort towards this goal we propose a self-adaptive hierarchical sentence
model (AdaSent). AdaSent effectively forms a hierarchy of representations from
words to phrases and then to sentences through recursive gated local
composition of adjacent segments. We design a competitive mechanism (through
gating networks) to allow the representations of the same sentence to be
engaged in a particular learning task (e.g., classification), therefore
effectively mitigating the gradient vanishing problem persistent in other
recursive models. Both qualitative and quantitative analysis shows that AdaSent
can automatically form and select the representations suitable for the task at
hand during training, yielding superior classification performance over
competitor models on 5 benchmark data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05073</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05073</id><created>2015-04-20</created><authors><author><keyname>Dirksen</keyname><forenames>Sjoerd</forenames></author><author><keyname>Lecu&#xe9;</keyname><forenames>Guillaume</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author></authors><title>On the gap between RIP-properties and sparse recovery conditions</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering sparse vectors from underdetermined
linear measurements via $\ell_p$-constrained basis pursuit. Previous analyses
of this problem based on generalized restricted isometry properties have
suggested that two phenomena occur if $p\neq 2$. First, one may need
substantially more than $s \log(en/s)$ measurements (optimal for $p=2$) for
uniform recovery of all $s$-sparse vectors. Second, the matrix that achieves
recovery with the optimal number of measurements may not be Gaussian (as for
$p=2$). We present a new, direct analysis which shows that in fact neither of
these phenomena occur. Via a suitable version of the null space property we
show that a standard Gaussian matrix provides $\ell_q/\ell_1$-recovery
guarantees for $\ell_p$-constrained basis pursuit in the optimal measurement
regime. Our result extends to several heavier-tailed measurement matrices. As
an application, we show that one can obtain a consistent reconstruction from
uniform scalar quantized measurements in the optimal measurement regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05078</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05078</id><created>2015-04-20</created><authors><author><keyname>Marcote</keyname><forenames>Sebastian R. Lamelas</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Automatic Repair of Infinite Loops</title><categories>cs.SE</categories><report-no>hal-01144026</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on automatic software repair is concerned with the development of
systems that automatically detect and repair bugs. One well-known class of bugs
is the infinite loop. Every computer programmer or user has, at least once,
experienced this type of bug. We state the problem of repairing infinite loops
in the context of test-suite based software repair: given a test suite with at
least one failing test, generate a patch that makes all test cases pass.
Consequently, repairing infinites loop means having at least one test case that
hangs by triggering the infinite loop. Our system to automatically repair
infinite loops is called $Infinitel$. We develop a technique to manipulate
loops so that one can dynamically analyze the number of iterations of loops;
decide to interrupt the loop execution; and dynamically examine the state of
the loop on a per-iteration basis. Then, in order to synthesize a new loop
condition, we encode this set of program states as a code synthesis problem
using a technique based on Satisfiability Modulo Theory (SMT). We evaluate our
technique on seven seeded-bugs and on seven real-bugs. $Infinitel$ is able to
repair all of them, within seconds up to one hour on a standard laptop
configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05086</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05086</id><created>2015-04-20</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author></authors><title>A Modified KZ Reduction Algorithm</title><categories>cs.IT math.IT</categories><comments>has been accepted by IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Korkine-Zolotareff (KZ) reduction has been used in communications and
cryptography. In this paper, we modify a very recent KZ reduction algorithm
proposed by Zhang et al., resulting in a new algorithm, which can be much
faster and more numerically reliable, especially when the basis matrix is ill
conditioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05095</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05095</id><created>2015-04-20</created><authors><author><keyname>AlKindy</keyname><forenames>Bassam</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Salomon</keyname><forenames>Michel</forenames></author><author><keyname>Parisod</keyname><forenames>Christian</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well
  Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes</title><categories>cs.AI cs.NE q-bio.PE q-bio.QM</categories><comments>15 pages, 7 figures, 2nd International Conference on Algorithms for
  Computational Biology, AlCoB 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of completely sequenced chloroplast genomes increases rapidly
every day, leading to the possibility to build large scale phylogenetic trees
of plant species. Considering a subset of close plant species defined according
to their chloroplasts, the phylogenetic tree that can be inferred by their core
genes is not necessarily well supported, due to the possible occurrence of
&quot;problematic&quot; genes (i.e., homoplasy, incomplete lineage sorting, horizontal
gene transfers, etc.) which may blur phylogenetic signal. However, a
trustworthy phylogenetic tree can still be obtained if the number of
problematic genes is low, the problem being to determine the largest subset of
core genes that produces the best supported tree. To discard problematic genes
and due to the overwhelming number of possible combinations, we propose an
hybrid approach that embeds both genetic algorithms and statistical tests.
Given a set of organisms, the result is a pipeline of many stages for the
production of well supported phylogenetic trees. The proposal has been applied
to different cases of plant families, leading to encouraging results for these
families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05100</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05100</id><created>2015-04-20</created><authors><author><keyname>G&#xf6;lo&#x11f;lu</keyname><forenames>Faruk</forenames></author><author><keyname>Lember</keyname><forenames>J&#xfc;ri</forenames></author><author><keyname>Riet</keyname><forenames>Ago-Erik</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>New Bounds for Permutation Codes in Ulam Metric</title><categories>cs.IT math.CO math.IT</categories><comments>To be presented at ISIT 2015, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New bounds on the cardinality of permutation codes equipped with the Ulam
distance are presented. First, an integer-programming upper bound is derived,
which improves on the Singleton-type upper bound in the literature for some
lengths. Second, several probabilistic lower bounds are developed, which
improve on the known lower bounds for large minimum distances. The results of a
computer search for permutation codes are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05103</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05103</id><created>2015-04-20</created><authors><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Optimizing Age-of-Information in a Multi-class Queueing System</title><categories>math.OC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the age-of-information in a multi-class $M/G/1$ queueing system,
where each class generates packets containing status information. Age of
information is a relatively new metric that measures the amount of time that
elapsed between status updates, thus accounting for both the queueing delay and
the delay between packet generation. This gives rise to a tradeoff between
frequency of status updates, and queueing delay. In this paper, we study this
tradeoff in a system with heterogenous users modeled as a multi-class $M/G/1$
queue. To this end, we derive the exact peak age-of-Information (PAoI) profile
of the system, which measures the &quot;freshness&quot; of the status information. We
then seek to optimize the age of information, by formulating the problem using
quasiconvex optimization, and obtain structural properties of the optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05110</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05110</id><created>2015-04-20</created><updated>2015-09-26</updated><authors><author><keyname>Ahmad</keyname><forenames>Rizwan</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Iteratively Reweighted $\ell_1$ Approaches to Sparse Composite
  Regularization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the observation that a given signal $\boldsymbol{x}$ admits
sparse representations in multiple dictionaries $\boldsymbol{\Psi}_d$ but with
varying levels of sparsity across dictionaries, we propose two new algorithms
for the reconstruction of (approximately) sparse signals from noisy linear
measurements. Our first algorithm, Co-L1, extends the well-known lasso
algorithm from the L1 regularizer $\|\boldsymbol{\Psi x}\|_1$ to composite
regularizers of the form $\sum_d \lambda_d \|\boldsymbol{\Psi}_d
\boldsymbol{x}\|_1$ while self-adjusting the regularization weights
$\lambda_d$. Our second algorithm, Co-IRW-L1, extends the well-known
iteratively reweighted L1 algorithm to the same family of composite
regularizers. We provide several interpretations of both algorithms: i)
majorization-minimization (MM) applied to a non-convex log-sum-type penalty,
ii) MM applied to an approximate $\ell_0$-type penalty, iii) MM applied to
Bayesian MAP inference under a particular hierarchical prior, and iv)
variational expectation-maximization (VEM) under a particular prior with
deterministic unknown parameters. A detailed numerical study suggests that our
proposed algorithms yield significantly improved recovery SNR when compared to
their non-composite L1 and IRW-L1 counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05122</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05122</id><created>2015-04-20</created><authors><author><keyname>Muriel</keyname><forenames>Reinaldo Uribe</forenames></author><author><keyname>Lozando</keyname><forenames>Fernando</forenames></author><author><keyname>Anderson</keyname><forenames>Charles</forenames></author></authors><title>Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes
  as a Minimal Sequence of Cumulative Tasks</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel method to solve average-reward semi-Markov
decision processes, by reducing them to a minimal sequence of cumulative reward
problems. The usual solution methods for this type of problems update the gain
(optimal average reward) immediately after observing the result of taking an
action. The alternative introduced, optimal nudging, relies instead on setting
the gain to some fixed value, which transitorily makes the problem a
cumulative-reward task, solving it by any standard reinforcement learning
method, and only then updating the gain in a way that minimizes uncertainty in
a minmax sense. The rule for optimal gain update is derived by exploiting the
geometric features of the w-l space, a simple mapping of the space of policies.
The total number of cumulative reward tasks that need to be solved is shown to
be small. Some experiments are presented to explore the features of the
algorithm and to compare its performance with other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05133</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05133</id><created>2015-04-20</created><updated>2015-04-29</updated><authors><author><keyname>Ng</keyname><forenames>Joe Yue-Hei</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Exploiting Local Features from Deep Networks for Image Retrieval</title><categories>cs.CV</categories><comments>CVPR DeepVision Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have been successfully applied to image
classification tasks. When these same networks have been applied to image
retrieval, the assumption has been made that the last layers would give the
best performance, as they do in classification. We show that for instance-level
image retrieval, lower layers often perform better than the last layers in
convolutional neural networks. We present an approach for extracting
convolutional features from different layers of the networks, and adopt VLAD
encoding to encode features into a single vector for each image. We investigate
the effect of different layers and scales of input images on the performance of
convolutional features using the recent deep networks OxfordNet and GoogLeNet.
Experiments demonstrate that intermediate layers or higher layers with finer
scales produce better results for image retrieval, compared to the last layer.
When using compressed 128-D VLAD descriptors, our method obtains
state-of-the-art results and outperforms other VLAD and CNN based approaches on
two out of three test datasets. Our work provides guidance for transferring
deep networks trained on image classification to image retrieval tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05137</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05137</id><created>2015-04-15</created><authors><author><keyname>Valenzuela</keyname><forenames>V. V. Vermehren</forenames></author><author><keyname>Lins</keyname><forenames>R. D.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Application of Enhanced-2D-CWT in Topographic Images for Mapping
  Landslide Risk Areas</title><categories>cs.CV physics.geo-ph</categories><comments>8 pages, 8 figures; Lecture Notes in Computer Science LNCS 7950,
  pp.380-388, 2013 Springer-Verlag, Heidelberg ISBN: 978-3-642-39093-7</comments><doi>10.1007/978-3-642-39094-4_43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been lately a number of catastrophic events of landslides and
mudslides in the mountainous region of Rio de Janeiro, Brazil. Those were
caused by intense rain in localities where there was unplanned occupation of
slopes of hills and mountains. Thus, it became imperative creating an inventory
of landslide risk areas in densely populated cities. This work presents a way
of demarcating risk areas by using the bidimensional Continuous Wavelet
Transform (2D-CWT) applied to high resolution topographic images of the
mountainous region of Rio de Janeiro.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05140</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05140</id><created>2015-04-20</created><authors><author><keyname>Bast</keyname><forenames>Hannah</forenames></author><author><keyname>Delling</keyname><forenames>Daniel</forenames></author><author><keyname>Goldberg</keyname><forenames>Andrew</forenames></author><author><keyname>M&#xfc;ller-Hannemann</keyname><forenames>Matthias</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Route Planning in Transportation Networks</title><categories>cs.DS</categories><comments>This is an updated version of the technical report MSR-TR-2014-4,
  previously published by Microsoft Research. This work was mostly done while
  the authors Daniel Delling, Andrew Goldberg, and Renato F. Werneck were at
  Microsoft Research Silicon Valley</comments><acm-class>G.2.1; G.2.2; G.2.3; H.2.8; H.3.5; H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey recent advances in algorithms for route planning in transportation
networks. For road networks, we show that one can compute driving directions in
milliseconds or less even at continental scale. A variety of techniques provide
different trade-offs between preprocessing effort, space requirements, and
query time. Some algorithms can answer queries in a fraction of a microsecond,
while others can deal efficiently with real-time traffic. Journey planning on
public transportation systems, although conceptually similar, is a
significantly harder problem due to its inherent time-dependent and
multicriteria nature. Although exact algorithms are fast enough for interactive
queries on metropolitan transit systems, dealing with continent-sized instances
requires simplifications or heavy preprocessing. The multimodal route planning
problem, which seeks journeys combining schedule-based transportation (buses,
trains) with unrestricted modes (walking, driving), is even harder, relying on
approximate solutions even for metropolitan inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05143</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05143</id><created>2015-04-20</created><authors><author><keyname>Kappel</keyname><forenames>David</forenames></author><author><keyname>Habenschuss</keyname><forenames>Stefan</forenames></author><author><keyname>Legenstein</keyname><forenames>Robert</forenames></author><author><keyname>Maass</keyname><forenames>Wolfgang</forenames></author></authors><title>Network Plasticity as Bayesian Inference</title><categories>cs.NE q-bio.NC</categories><comments>33 pages, 5 figures, the supplement is available on the author's web
  page http://www.igi.tugraz.at/kappel</comments><doi>10.1371/journal.pcbi.1004485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General results from statistical learning theory suggest to understand not
only brain computations, but also brain plasticity as probabilistic inference.
But a model for that has been missing. We propose that inherently stochastic
features of synaptic plasticity and spine motility enable cortical networks of
neurons to carry out probabilistic inference by sampling from a posterior
distribution of network configurations. This model provides a viable
alternative to existing models that propose convergence of parameters to
maximum likelihood values. It explains how priors on weight distributions and
connection probabilities can be merged optimally with learned experience, how
cortical networks can generalize learned information so well to novel
experiences, and how they can compensate continuously for unforeseen
disturbances of the network. The resulting new theory of network plasticity
explains from a functional perspective a number of experimental data on
stochastic aspects of synaptic plasticity that previously appeared to be quite
puzzling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05150</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05150</id><created>2015-04-20</created><updated>2015-04-21</updated><authors><author><keyname>Kaminski</keyname><forenames>Mark</forenames></author><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author></authors><title>Computing Horn Rewritings of Description Logics Ontologies</title><categories>cs.AI cs.LO</categories><comments>15 pages. To appear in IJCAI-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of rewriting an ontology O1 expressed in a DL L1 into an
ontology O2 in a Horn DL L2 such that O1 and O2 are equisatisfiable when
extended with an arbitrary dataset. Ontologies that admit such rewritings are
amenable to reasoning techniques ensuring tractability in data complexity.
After showing undecidability whenever L1 extends ALCF, we focus on devising
efficiently checkable conditions that ensure existence of a Horn rewriting. By
lifting existing techniques for rewriting Disjunctive Datalog programs into
plain Datalog to the case of arbitrary first-order programs with function
symbols, we identify a class of ontologies that admit Horn rewritings of
polynomial size. Our experiments indicate that many real-world ontologies
satisfy our sufficient conditions and thus admit polynomial Horn rewritings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05155</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05155</id><created>2015-04-20</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Grier</keyname><forenames>Daniel</forenames></author><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author></authors><title>The Classification of Reversible Bit Operations</title><categories>quant-ph cs.CC</categories><comments>68 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complete classification of all possible sets of classical
reversible gates acting on bits, in terms of which reversible transformations
they generate, assuming swaps and ancilla bits are available for free. Our
classification can be seen as the reversible-computing analogue of Post's
lattice, a central result in mathematical logic from the 1940s. It is a step
toward the ambitious goal of classifying all possible quantum gate sets acting
on qubits. Our theorem implies a linear-time algorithm (which we have
implemented), that takes as input the truth tables of reversible gates G and H,
and that decides whether G generates H. Previously, this problem was not even
known to be decidable. The theorem also implies that any n-bit reversible
circuit can be &quot;compressed&quot; to an equivalent circuit, over the same gates, that
uses at most 2^n*poly(n) gates and O(1) ancilla bits; these are the first upper
bounds on these quantities known, and are close to optimal. Finally, the
theorem implies that every non-degenerate reversible gate can implement either
every reversible transformation, or every affine transformation, when
restricted to an &quot;encoded subspace.&quot; Briefly, the theorem says that every set
of reversible gates generates either all reversible transformations on n-bit
strings (as the Toffoli gate does); no transformations; all transformations
that preserve Hamming weight (as the Fredkin gate does); all transformations
that preserve Hamming weight mod k for some k; all affine transformations (as
the Controlled-NOT gate does); all affine transformations that preserve Hamming
weight mod 2 or mod 4, inner products mod 2, or a combination thereof; or a
previous class augmented by a NOT or NOTNOT gate. Ruling out the possibility of
additional classes, not in the list, requires some arguments about polynomials,
lattices, and Diophantine equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05158</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05158</id><created>2015-04-20</created><authors><author><keyname>Szwed</keyname><forenames>Piotr</forenames></author><author><keyname>Chmiel</keyname><forenames>Wojciech</forenames></author></authors><title>Multi-swarm PSO algorithm for the Quadratic Assignment Problem: a
  massive parallel implementation on the OpenCL platform</title><categories>cs.NE</categories><comments>2 tables 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a multi-swarm PSO algorithm for the Quadratic Assignment
Problem (QAP) implemented on OpenCL platform. Our work was motivated by results
of time efficiency tests performed for single-swarm algorithm implementation
that showed clearly that the benefits of a parallel execution platform can be
fully exploited, if the processed population is large. The described algorithm
can be executed in two modes: with independent swarms or with migration. We
discuss the algorithm construction, as well as we report results of tests
performed on several problem instances from the QAPLIB library. During the
experiments the algorithm was configured to process large populations. This
allowed us to collect statistical data related to values of goal function
reached by individual particles. We use them to demonstrate on two test cases
that although single particles seem to behave chaotically during the
optimization process, when the whole population is analyzed, the probability
that a particle will select a near-optimal solution grows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05159</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05159</id><created>2015-04-20</created><updated>2015-11-18</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Complexity of Suffix-Free Regular Languages</title><categories>cs.FL</categories><comments>25 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study various complexity properties of suffix-free regular languages. The
quotient complexity of a regular language $L$ is the number of left quotients
of $L$; this is the same as the state complexity of $L$. A regular language
$L'$ is a dialect of a regular language $L$ if it differs only slightly from
$L$. The quotient complexity of an operation on regular languages is the
maximal quotient complexity of the result of the operation expressed as a
function of the quotient complexities of the operands. A sequence
$(L_k,L_{k+1},\dots)$ of regular languages in some class ${\mathcal C}$, where
$n$ is the quotient complexity of $L_n$, is called a stream. A stream is most
complex in class ${\mathcal C}$ if its languages $L_n$ meet the complexity
upper bounds for all basic measures. It is known that there exist such most
complex streams in the class of regular languages, in the class of prefix-free
languages, and also in the classes of right, left, and two-sided ideals. In
contrast to this, we prove that there does not exist a most complex stream in
the class of suffix-free regular languages. However, we do exhibit one ternary
suffix-free stream that meets the bound for product and whose restrictions to
binary alphabets meet the bounds for star and boolean operations. We also
exhibit a quinary stream that meets the bounds for boolean operations,
reversal, size of syntactic semigroup, and atom complexities. Moreover, we
solve an open problem about the bound for the product of two languages of
quotient complexities $m$ and $n$ in the binary case by showing that it can be
met for infinitely many $m$ and $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05163</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05163</id><created>2015-04-20</created><authors><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Trend of Narratives in the Age of Misinformation</title><categories>cs.SI cs.HC physics.soc-ph</categories><doi>10.1371/journal.pone.0134641</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media enabled a direct path from producer to consumer of contents
changing the way users get informed, debate, and shape their worldviews. Such a
{\em disintermediation} weakened consensus on social relevant issues in favor
of rumors, mistrust, and fomented conspiracy thinking -- e.g., chem-trails
inducing global warming, the link between vaccines and autism, or the New World
Order conspiracy.
  In this work, we study through a thorough quantitative analysis how different
conspiracy topics are consumed in the Italian Facebook. By means of a
semi-automatic topic extraction strategy, we show that the most discussed
contents semantically refer to four specific categories: {\em environment},
{\em diet}, {\em health}, and {\em geopolitics}. We find similar patterns by
comparing users activity (likes and comments) on posts belonging to different
semantic categories. However, if we focus on the lifetime -- i.e., the distance
in time between the first and the last comment for each user -- we notice a
remarkable difference within narratives -- e.g., users polarized on geopolitics
are more persistent in commenting, whereas the less persistent are those
focused on diet related topics. Finally, we model users mobility across various
topics finding that the more a user is active, the more he is likely to join
all topics. Once inside a conspiracy narrative users tend to embrace the
overall corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05171</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05171</id><created>2015-04-20</created><authors><author><keyname>Efremenko</keyname><forenames>Klim</forenames></author><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author><author><keyname>Schenck</keyname><forenames>Hal</forenames></author><author><keyname>Weyman</keyname><forenames>Jerzy</forenames></author></authors><title>On minimal free resolutions and the method of shifted partial
  derivatives in complexity theory</title><categories>cs.CC math.AC math.AG</categories><msc-class>68Q17, 13D02, 14L30, 20B30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimal free resolution of the Jacobian ideals of the determinant
polynomial were computed by Lascoux, and it is an active area of research to
understand the Jacobian ideals of the permanent. As a step in this direction we
compute several new cases and completely determine the linear strands of the
minimal free resolutions of the ideals generated by sub-permanents. Our
motivation is an exploration of the utility and limits of the method of shifted
partial derivatives introduced by Kayal and Gupta-Kamath-Kayal-Saptharishi. The
method of shifted partial derivatives amounts to computing Hilbert functions of
Jacobian ideals, and the Hilbert functions are in turn the Euler
characteristics of the minimal free resolutions of the Jacobian ideals. We
compute several such Hilbert functions relevant for complexity theory. We show
that the method of shifted partial derivatives alone cannot prove the size m
padded permanent cannotbe realized inside the orbit closure of the size n
determinant when m&lt; 1.5n^2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05182</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05182</id><created>2015-04-20</created><authors><author><keyname>Jog</keyname><forenames>Varun</forenames></author><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author></authors><title>A Geometric Analysis of the AWGN channel with a $(\sigma, \rho)$-Power
  Constraint</title><categories>cs.IT math.IT</categories><comments>55 pages, 5 figures. Contains results submitted to ISIT 2014 and ISIT
  2015. Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the AWGN channel with a power constraint called
the $(\sigma, \rho)$-power constraint, which is motivated by energy harvesting
communication systems. Given a codeword, the constraint imposes a limit of
$\sigma + k \rho$ on the total power of any $k\geq 1$ consecutive transmitted
symbols. Such a channel has infinite memory and evaluating its exact capacity
is a difficult task. Consequently, we establish an $n$-letter capacity
expression and seek bounds for the same. We obtain a lower bound on capacity by
considering the volume of ${\cal S}_n(\sigma, \rho) \subseteq \mathbb{R}^n$,
which is the set of all length $n$ sequences satisfying the $(\sigma,
\rho)$-power constraints. For a noise power of $\nu$, we obtain an upper bound
on capacity by considering the volume of ${\cal S}_n(\sigma, \rho) \oplus
B_n(\sqrt{n\nu})$, which is the Minkowski sum of ${\cal S}_n(\sigma, \rho)$ and
the $n$-dimensional Euclidean ball of radius $\sqrt{n\nu}$. We analyze this
bound using a result from convex geometry known as Steiner's formula, which
gives the volume of this Minkowski sum in terms of the intrinsic volumes of
${\cal S}_n(\sigma, \rho)$. We show that as the dimension $n$ increases, the
logarithm of the sequence of intrinsic volumes of $\{{\cal S}_n(\sigma,
\rho)\}$ converges to a limit function under an appropriate scaling. The upper
bound on capacity is then expressed in terms of this limit function. We derive
the asymptotic capacity in the low and high noise regime for the $(\sigma,
\rho)$-power constrained AWGN channel, with strengthened results for the
special case of $\sigma = 0$, which is the amplitude constrained AWGN channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05208</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05208</id><created>2015-04-20</created><authors><author><keyname>Blomberg</keyname><forenames>Niclas</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>Approximate Regularization Paths for Nuclear Norm Minimization Using
  Singular Value Bounds -- With Implementation and Extended Appendix</title><categories>cs.SY</categories><comments>Also in conference version submitted to Signal Processing Education
  Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widely used nuclear norm heuristic for rank minimization problems
introduces a regularization parameter which is difficult to tune. We have
recently proposed a method to approximate the regularization path, i.e., the
optimal solution as a function of the parameter, which requires solving the
problem only for a sparse set of points. In this paper, we extend the algorithm
to provide error bounds for the singular values of the approximation. We
exemplify the algorithms on large scale benchmark examples in model order
reduction. Here, the order of a dynamical system is reduced by means of
constrained minimization of the nuclear norm of a Hankel matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05218</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05218</id><created>2015-04-20</created><authors><author><keyname>Solovey</keyname><forenames>Kiril</forenames></author><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>Zamir</keyname><forenames>Or</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Motion Planning for Unlabeled Discs with Optimality Guarantees</title><categories>cs.CG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of path planning for unlabeled (indistinguishable)
unit-disc robots in a planar environment cluttered with polygonal obstacles. We
introduce an algorithm which minimizes the total path length, i.e., the sum of
lengths of the individual paths. Our algorithm is guaranteed to find a solution
if one exists, or report that none exists otherwise. It runs in time
$\tilde{O}(m^4+m^2n^2)$, where $m$ is the number of robots and $n$ is the total
complexity of the workspace. Moreover, the total length of the returned
solution is at most $\text{OPT}+4m$, where OPT is the optimal solution cost. To
the best of our knowledge this is the first algorithm for the problem that has
such guarantees. The algorithm has been implemented in an exact manner and we
present experimental results that attest to its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05222</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05222</id><created>2015-04-20</created><authors><author><keyname>Song</keyname><forenames>Yangbo</forenames></author></authors><title>Social Learning with Endogenous Network Formation</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I study the problem of social learning in a model where agents move
sequentially. Each agent receives a private signal about the underlying state
of the world, observes the past actions in a neighborhood of individuals, and
chooses her action attempting to match the true state. Earlier research in this
field emphasizes that herding behavior occurs with a positive probability in
certain special cases; recent studies show that asymptotic learning is
achievable under a more general observation structure. In particular, with
unbounded private beliefs, asymptotic learning occurs if and only if agents
observe a close predecessor, i.e., the action of a close predecessor reveals
the true state in the limit. However, a prevailing assumption in these studies
is that the observation structure in the society is exogenous. In contrast to
most of the previous literature, I assume in this paper that observation is
endogenous and costly. More specifically, each agent must pay a specific cost
to make any observation and can strategically choose the set of actions to
observe privately. I introduce the notion of maximal learning (relative to
cost) as a natural extension of asymptotic learning: society achieves maximal
learning when agents can learn the true state with probability 1 in the limit
after paying the cost of observation. I show that observing only a close
predecessor is no longer sufficient for learning the true state with unbounded
private beliefs and positive costs. Instead, maximal learning occurs if and
only if the size of the observations extends to infinity. I provide interesting
comparative statics as to how various settings in the model affect the learning
probability. For instance, the probability to learn the true state may be
higher under positive costs than under zero cost; in addition, the probability
to learn the true state may be higher under weaker private signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05225</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05225</id><created>2015-04-20</created><authors><author><keyname>Nelson</keyname><forenames>Peter</forenames></author><author><keyname>van Zwam</keyname><forenames>Stefan H. M.</forenames></author></authors><title>The maximum-likelihood decoding threshold for graphic codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a class $\mathcal{C}$ of binary linear codes, we write
$\theta_{\mathcal{C}}\colon (0,1) \to [0,\frac{1}{2}]$ for the
maximum-likelihood decoding threshold function of $\mathcal{C}$, the function
whose value at $R \in (0,1)$ is the largest bit-error rate $p$ that codes in
$\mathcal{C}$ can tolerate with a negligible probability of maximum-likelihood
decoding error across a binary symmetric channel. We show that, if
$\mathcal{C}$ is the class of cycle codes of graphs, then
$\theta_{\mathcal{C}}(R) \le \frac{(1-\sqrt{R})^2}{2(1+R)}$ for each $R$, and
show that equality holds only when $R$ is asymptotically achieved by cycle
codes of regular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05226</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05226</id><created>2015-04-20</created><authors><author><keyname>Caragata</keyname><forenames>Daniel</forenames></author></authors><title>On the Security of a Revised Fragile Watermarking Scheme</title><categories>cs.MM cs.CR</categories><comments>To be submitted to AEU INT J ELECTRON C</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes a revised fragile watermarking scheme proposed by Botta
et al. which was developed as a revision of the watermarking scheme previously
proposed by Rawat et al. A new attack is presented that allows an attacker to
apply a valid watermark on tampered images, therefore circumventing the
protection that the watermarking scheme under study was supposed to offer.
Furthermore, the presented attack has very low computational and memory
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05227</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05227</id><created>2015-04-20</created><authors><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Fully Quantum Source Compression with a Quantum Helper</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, one figure, submitted to ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study source compression with a helper in the fully quantum regime,
extending our earlier result on classical source compression with a quantum
helper [arXiv:1501.04366, 2015]. We characterise the quantum resources involved
in this problem, and derive a single-letter expression of the achievable rate
region when entanglement assistance is available. The direct coding proof is
based on a combination of two fundamental protocols, namely the quantum state
merging protocol and the quantum reverse Shannon theorem (QRST). This result
demonstrates an unexpected connection between distributed source compression
with the QRST protocol, a quantum protocol that consumes noiseless resources to
simulate a noisy quantum channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05229</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05229</id><created>2015-04-20</created><updated>2015-09-11</updated><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Poisson Matrix Recovery and Completion</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>Submitted to IEEE Journal. Parts of the paper have appeared in
  GlobalSIP 2013, GlobalSIP 2014, and ISIT 2015. arXiv admin note: substantial
  text overlap with arXiv:1501.06243</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the theory of low-rank matrix recovery and completion to the case
when Poisson observations for a linear combination or a subset of the entries
of a matrix are available, which arises in various applications with count
data. We consider the usual matrix recovery formulation through maximum
likelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,
and establish theoretical upper and lower bounds on the recovery error. Our
bounds for matrix completion are nearly optimal up to a factor on the order of
$\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by combing techniques
for compressed sensing for sparse vectors with Poisson noise and for analyzing
low-rank matrices, as well as adapting the arguments used for one-bit matrix
completion \cite{davenport20121} (although these two problems are different in
nature) and the adaptation requires new techniques exploiting properties of the
Poisson likelihood function and tackling the difficulties posed by the locally
sub-Gaussian characteristic of the Poisson distribution. Our results highlight
a few important distinctions of the Poisson case compared to the prior work
including having to impose a minimum signal-to-noise requirement on each
observed entry and a gap in the upper and lower bounds. We also develop a set
of efficient iterative algorithms and demonstrate their good performance on
synthetic examples and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05240</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05240</id><created>2015-04-20</created><authors><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Tsai</keyname><forenames>Meng-Tsung</forenames></author></authors><title>On the complexity of computing prime tables</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many large arithmetic computations rely on tables of all primes less than
$n$. For example, the fastest algorithms for computing $n!$ takes time
$O(M(n\log n) + P(n))$, where $M(n)$ is the time to multiply two $n$-bit
numbers, and $P(n)$ is the time to compute a prime table up to $n$. The fastest
algorithm to compute $\binom{n}{n/2}$ also uses a prime table. We show that it
takes time $O(M(n) + P(n))$.
  In various models, the best bound on $P(n)$ is greater than $M(n\log n)$,
given advances in the complexity of multiplication \cite{Furer07,De08}. In this
paper, we give two algorithms to computing prime tables and analyze their
complexity on a multitape Turing machine, one of the standard models for
analyzing such algorithms. These two algorithms run in time $O(M(n\log n))$ and
$O(n\log^2 n/\log \log n)$, respectively. We achieve our results by speeding up
Atkin's sieve.
  Given that the current best bound on $M(n)$ is $n\log n 2^{O(\log^*n)}$, the
second algorithm is faster and improves on the previous best algorithm by a
factor of $\log^2\log n$. Our fast prime-table algorithms speed up both the
computation of $n!$ and $\binom{n}{n/2}$.
  Finally, we show that computing the factorial takes $\Omega(M(n \log^{4/7 -
\epsilon} n))$ for any constant $\epsilon &gt; 0$ assuming only multiplication is
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05241</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05241</id><created>2015-04-20</created><authors><author><keyname>Hou</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Hong</forenames></author><author><keyname>Zhou</keyname><forenames>Shilin</forenames></author></authors><title>Convolutional Neural Network-Based Image Representation for Visual Loop
  Closure Detection</title><categories>cs.RO cs.CV</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (CNN) have recently been shown in many
computer vision and pattern recog- nition applications to outperform by a
significant margin state- of-the-art solutions that use traditional
hand-crafted features. However, this impressive performance is yet to be fully
exploited in robotics. In this paper, we focus one specific problem that can
benefit from the recent development of the CNN technology, i.e., we focus on
using a pre-trained CNN model as a method of generating an image representation
appropriate for visual loop closure detection in SLAM (simultaneous
localization and mapping). We perform a comprehensive evaluation of the outputs
at the intermediate layers of a CNN as image descriptors, in comparison with
state-of-the-art image descriptors, in terms of their ability to match images
for detecting loop closures. The main conclusions of our study include: (a)
CNN-based image representations perform comparably to state-of-the-art hand-
crafted competitors in environments without significant lighting change, (b)
they outperform state-of-the-art competitors when lighting changes
significantly, and (c) they are also significantly faster to extract than the
state-of-the-art hand-crafted features even on a conventional CPU and are two
orders of magnitude faster on an entry-level GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05255</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05255</id><created>2015-04-20</created><updated>2015-07-22</updated><authors><author><keyname>Gagliardoni</keyname><forenames>Tommaso</forenames></author><author><keyname>H&#xfc;lsing</keyname><forenames>Andreas</forenames></author><author><keyname>Schaffner</keyname><forenames>Christian</forenames></author></authors><title>Semantic Security and Indistinguishability in the Quantum World</title><categories>cs.CR quant-ph</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At CRYPTO 2013, Boneh and Zhandry initiated the study of quantum-secure
encryption. They proposed first indistinguishability definitions for the
quantum world where the actual indistinguishability only holds for classical
messages, and they provide arguments why it might be hard to achieve a stronger
notion. In this work, we show that stronger notions are achievable, where the
indistinguishability holds for quantum superpositions of messages. We
investigate exhaustively the possibilities and subtle differences in defining
such a quantum indistinguishability notion for symmetric-key encryption
schemes. We justify our stronger definition by showing its equivalence to novel
quantum semantic-security notions that we introduce. Furthermore, we show that
our new security definitions cannot be achieved by a large class of ciphers --
those which are quasi-preserving the message length. On the other hand, we
provide a secure construction based on quantum-resistant pseudorandom
permutations; this construction can be used as a generic transformation for
turning a large class of encryption schemes into quantum indistinguishable and
hence quantum semantically secure ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05262</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05262</id><created>2015-04-20</created><updated>2015-04-23</updated><authors><author><keyname>Lira</keyname><forenames>M. M. S.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Wavelets for Elliptical Waveguide Problems</title><categories>math.CA cs.NA</categories><comments>5 pages, 4 figures. in: 2002 WSEAS International Conference on
  Wavelet Analysis and Multirate Systems, Vouliagmeni, Greece. arXiv admin
  note: substantial text overlap with arXiv:1501.07255</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New elliptic cylindrical wavelets are introduced, which exploit the
relationship between analysing filters and Floquet's solution of Mathieu
differential equations. It is shown that the transfer function of both
multiresolution filters is related to the solution of a Mathieu equation of odd
characteristic exponent. The number of notches of these analysing filters can
be easily designed. Wavelets derived by this method have potential application
in the fields of optics, microwaves and electromagnetism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05268</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05268</id><created>2015-04-20</created><authors><author><keyname>Ataei</keyname><forenames>Mohammad R.</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author><author><keyname>Kunz</keyname><forenames>Thomas</forenames></author></authors><title>Energy-Efficient Broadcasting for Cross Wireless Ad-Hoc Networks</title><categories>cs.NI</categories><comments>35 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose solutions for the energy-efficient broadcasting
over cross networks, where N nodes are located on two perpendicular lines. Our
solutions consist of an algorithm which finds the optimal range assignment in
polynomial time (O(N^12)), a near-optimal algorithm with linear complexity
(O(N)), and a distributed algorithm with complexity O(1). To the best of our
knowledge, this is the first study presenting an optimal solution for the
minimum-energy broadcasting problem for a 2-D network (with cross
configuration). We compare our algorithms with the broadcast incremental power
(BIP) algorithm, one of the most commonly used methods for solving this problem
with complexity O(N^2). We demonstrate that our near-optimal algorithm
outperforms BIP, and that the distributed algorithm performs close to it.
Moreover, the proposed distributed algorithm can be used for more general
two-dimensional networks, where the nodes are located on a grid consisting of
perpendicular line-segments. The performance of the proposed near-optimal and
distributed algorithms tend to be closer to the optimal solution for larger
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05276</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05276</id><created>2015-04-20</created><authors><author><keyname>Hussain</keyname><forenames>Rasheed</forenames></author><author><keyname>Kim</keyname><forenames>Donghyun</forenames></author><author><keyname>Nogueira</keyname><forenames>Michele</forenames></author><author><keyname>Son</keyname><forenames>Junggab</forenames></author><author><keyname>Tokuta</keyname><forenames>Alade O.</forenames></author><author><keyname>Oh</keyname><forenames>Heekuck</forenames></author></authors><title>PBF: A New Privacy-Aware Billing Framework for Online Electric Vehicles
  with Bidirectional Auditability</title><categories>cs.CR</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently an online electric vehicle (OLEV) concept has been introduced, where
vehicles are propelled through the wirelessly transmitted electrical power from
the infrastructure installed under the road while moving. The absence of
secure-and-fair billing is one main hurdle to widely adopt this promising
technology. This paper introduces a secure and privacy-aware fair billing
framework for OLEV on the move through the charging plates installed under the
road. We first propose two extreme lightweight mutual authentication
mechanisms, a direct authentication and a hash chain-based authentication
between vehicles and the charging plates that can be used for different
vehicular speeds on the road. Second we propose a secure and privacy-aware
wireless power transfer on move for the vehicles with bidirectional
auditability guarantee by leveraging game-theoretic approach. Each charging
plate transfers a fixed amount of energy to the vehicle and bills the vehicle
in a privacy-aware way accordingly. Our protocol guarantees secure,
privacy-aware, and fair billing mechanism for the OLEVs while receiving
electric power from the road. Moreover our proposed framework can play a vital
role in eliminating the security and privacy challenges in the deployment of
power transfer technology to the OLEVs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05277</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05277</id><created>2015-04-20</created><updated>2015-04-22</updated><authors><author><keyname>Gao</keyname><forenames>Bin-Bin</forenames></author><author><keyname>Wei</keyname><forenames>Xiu-Shen</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author></authors><title>Deep Spatial Pyramid: The Devil is Once Again in the Details</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that by carefully making good choices for various
detailed but important factors in a visual recognition framework using deep
learning features, one can achieve a simple, efficient, yet highly accurate
image classification system. We first list 5 important factors, based on both
existing researches and ideas proposed in this paper. These important detailed
factors include: 1) $\ell_2$ matrix normalization is more effective than
unnormalized or $\ell_2$ vector normalization, 2) the proposed natural deep
spatial pyramid is very effective, and 3) a very small $K$ in Fisher Vectors
surprisingly achieves higher accuracy than normally used large $K$ values.
Along with other choices (convolutional activations and multiple scales), the
proposed DSP framework is not only intuitive and efficient, but also achieves
excellent classification accuracy on many benchmark datasets. For example,
DSP's accuracy on SUN397 is 59.78%, significantly higher than previous
state-of-the-art (53.86%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05283</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05283</id><created>2015-04-20</created><updated>2015-04-24</updated><authors><author><keyname>Wu</keyname><forenames>Yueping</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>User-Centric Interference Nulling in Downlink Multi-Antenna
  Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>Shorter version to appear in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (HetNets) have strong interference due to spectrum
reuse. This affects the signal-to-interference ratio (SIR) of each user, and
hence is one of the limiting factors of network performance. However, in
previous works, interference management approaches in HetNets are mainly based
on interference level, and thus cannot effectively utilize the limited resource
to improve network performance. In this paper, we propose a user-centric
interference nulling (IN) scheme in downlink two-tier HetNets to improve
network performance by improving each user's SIR. This scheme has three design
parameters: the maximum degree of freedom for IN (IN DoF), and the IN
thresholds for the macro and pico users, respectively. Using tools from
stochastic geometry, we first obtain a tractable expression of the coverage
(equivalently outage) probability. Then, we characterize the asymptotic
behavior of the outage probability in the high reliability regime. The
asymptotic results show that the maximum IN DoF can affect the order gain of
the asymptotic outage probability, while the IN thresholds only affect the
coefficient of the asymptotic outage probability. Moreover, we show that the IN
scheme can linearly improve the outage performance, and characterize the
optimal maximum IN DoF which minimizes the asymptotic outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05287</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05287</id><created>2015-04-20</created><authors><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author></authors><title>Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares
  Algorithms</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor rank and low-rank tensor decompositions have many applications in
learning and complexity theory. Most known algorithms use unfoldings of tensors
and can only handle rank up to $n^{\lfloor p/2 \rfloor}$ for a $p$-th order
tensor in $\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose
3rd order tensors when the rank is super-linear in the dimension. Using ideas
from sum-of-squares hierarchy, we give the first quasi-polynomial time
algorithm that can decompose a random 3rd order tensor decomposition when the
rank is as large as $n^{3/2}/\textrm{polylog} n$.
  We also give a polynomial time algorithm for certifying the injective norm of
random low rank tensors. Our tensor decomposition algorithm exploits the
relationship between injective norm and the tensor components. The proof relies
on interesting tools for decoupling random variables to prove better matrix
concentration bounds, which can be useful in other settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05289</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05289</id><created>2015-04-20</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Distance-based species tree estimation: information-theoretic trade-off
  between number of loci and sequence length under the coalescent</title><categories>math.PR cs.LG math.ST q-bio.PE stat.TH</categories><comments>Submitted. 26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reconstruction of a phylogeny from multiple genes under the
multispecies coalescent. We establish a connection with the sparse signal
detection problem, where one seeks to distinguish between a distribution and a
mixture of the distribution and a sparse signal. Using this connection, we
derive an information-theoretic trade-off between the number of genes, $m$,
needed for an accurate reconstruction and the sequence length, $k$, of the
genes. Specifically, we show that to detect a branch of length $f$, one needs
$m = \Theta(1/[f^{2} \sqrt{k}])$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05294</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05294</id><created>2015-04-20</created><updated>2015-11-15</updated><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>On Approximating the Sum-Rate for Multiple-Unicasts</title><categories>cs.IT math.IT</categories><comments>10 pages; Shorter version appeared at ISIT (International Symposium
  on Information Theory) 2015; some typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study upper bounds on the sum-rate of multiple-unicasts. We approximate
the Generalized Network Sharing Bound (GNS cut) of the multiple-unicasts
network coding problem with $k$ independent sources. Our approximation
algorithm runs in polynomial time and yields an upper bound on the joint source
entropy rate, which is within an $O(\log^2 k)$ factor from the GNS cut. It
further yields a vector-linear network code that achieves joint source entropy
rate within an $O(\log^2 k)$ factor from the GNS cut, but \emph{not} with
independent sources: the code induces a correlation pattern among the sources.
  Our second contribution is establishing a separation result for vector-linear
network codes: for any given field $\mathbb{F}$ there exist networks for which
the optimum sum-rate supported by vector-linear codes over $\mathbb{F}$ for
independent sources can be multiplicatively separated by a factor of
$k^{1-\delta}$, for any constant ${\delta&gt;0}$, from the optimum joint entropy
rate supported by a code that allows correlation between sources. Finally, we
establish a similar separation result for the asymmetric optimum vector-linear
sum-rates achieved over two distinct fields $\mathbb{F}_{p}$ and
$\mathbb{F}_{q}$ for independent sources, revealing that the choice of field
can heavily impact the performance of a linear network code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05298</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05298</id><created>2015-04-21</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pham</keyname><forenames>Duc-Son</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Viewpoint distortion compensation in practical surveillance systems</title><categories>cs.CV</categories><comments>International Conference on Multimedia &amp; Expo, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim is to estimate the perspective-effected geometric distortion of a
scene from a video feed. In contrast to all previous work we wish to achieve
this using from low-level, spatio-temporally local motion features used in
commercial semi-automatic surveillance systems. We: (i) describe a dense
algorithm which uses motion features to estimate the perspective distortion at
each image locus and then polls all such local estimates to arrive at the
globally best estimate, (ii) present an alternative coarse algorithm which
subdivides the image frame into blocks, and uses motion features to derive
block-specific motion characteristics and constrain the relationships between
these characteristics, with the perspective estimate emerging as a result of a
global optimization scheme, and (iii) report the results of an evaluation using
nine large sets acquired using existing close-circuit television (CCTV)
cameras. Our findings demonstrate that both of the proposed methods are
successful, their accuracy matching that of human labelling using complete
visual data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05299</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05299</id><created>2015-04-21</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pham</keyname><forenames>Duc-Son</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Groupwise registration of aerial images</title><categories>cs.CV</categories><comments>International Joint Conference on Artificial Intelligence, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the task of time separated aerial image registration.
The ability to solve this problem accurately and reliably is important for a
variety of subsequent image understanding applications. The principal challenge
lies in the extent and nature of transient appearance variation that a land
area can undergo, such as that caused by the change in illumination conditions,
seasonal variations, or the occlusion by non-persistent objects (people, cars).
Our work introduces several novelties: (i) unlike all previous work on aerial
image registration, we approach the problem using a set-based paradigm; (ii) we
show how local, pair-wise constraints can be used to enforce a globally good
registration using a constraints graph structure; (iii) we show how a simple
holistic representation derived from raw aerial images can be used as a basic
building block of the constraints graph in a manner which achieves both high
registration accuracy and speed. We demonstrate: (i) that the proposed method
outperforms the state-of-the-art for pair-wise registration already, achieving
greater accuracy and reliability, while at the same time reducing the
computational cost of the task; and (ii) that the increase in the number of
available images in a set consistently reduces the average registration error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05302</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05302</id><created>2015-04-21</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pham</keyname><forenames>Duc-Son</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>The adaptable buffer algorithm for high quantile estimation in
  non-stationary data streams</title><categories>cs.CV</categories><comments>International Joint Conference on Neural Networks, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to estimate a particular quantile of a distribution is an important
problem which frequently arises in many computer vision and signal processing
applications. For example, our work was motivated by the requirements of many
semi-automatic surveillance analytics systems which detect abnormalities in
close-circuit television (CCTV) footage using statistical models of low-level
motion features. In this paper we specifically address the problem of
estimating the running quantile of a data stream with non-stationary
stochasticity when the memory for storing observations is limited. We make
several major contributions: (i) we derive an important theoretical result
which shows that the change in the quantile of a stream is constrained
regardless of the stochastic properties of data, (ii) we describe a set of
high-level design goals for an effective estimation algorithm that emerge as a
consequence of our theoretical findings, (iii) we introduce a novel algorithm
which implements the aforementioned design goals by retaining a sample of data
values in a manner adaptive to changes in the distribution of data and
progressively narrowing down its focus in the periods of quasi-stationary
stochasticity, and (iv) we present a comprehensive evaluation of the proposed
algorithm and compare it with the existing methods in the literature on both
synthetic data sets and three large `real-world' streams acquired in the course
of operation of an existing commercial surveillance system. Our findings
convincingly demonstrate that the proposed method is highly successful and
vastly outperforms the existing alternatives, especially when the target
quantile is high valued and the available buffer capacity severely limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05305</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05305</id><created>2015-04-21</created><authors><author><keyname>Zhang</keyname><forenames>Ying</forenames></author></authors><title>A Simple and General Problem and its Optimal Randomized Online Algorithm
  Design with Competitive Analysis</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online algorithm design was proposed to handle the caching problem when
the future information is unknown. And currently, it draws more and more
attentions from the researchers from the areas of microgrid, where the
production of renewables are unpredictable.
  In this note, we present a framework of randomized online algorithm design
for the \textit{simple and tractable} problem. This framework hopes to provide
a tractable design to design a randomized online algorithm, which can be proved
to achieve the best competitive ratio by \textit{Yao's Principle}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05308</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05308</id><created>2015-04-21</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Automatic Face Recognition from Video</title><categories>cs.CV</categories><comments>Doctor of Philosophy (PhD) dissertation, University of Cambridge,
  2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work is to automatically recognize faces from video
sequences in a realistic, unconstrained setup in which illumination conditions
are extreme and greatly changing, viewpoint and user motion pattern have a wide
variability, and video input is of low quality. At the centre of focus are face
appearance manifolds: this thesis presents a significant advance of their
understanding and application in the sphere of face recognition. The two main
contributions are the Generic Shape-Illumination Manifold recognition algorithm
and the Anisotropic Manifold Space clustering. The Generic Shape-Illumination
Manifold is evaluated on a large data corpus acquired in real-world conditions
and its performance is shown to greatly exceed that of state-of-the-art methods
in the literature and the best performing commercial software. Empirical
evaluation of the Anisotropic Manifold Space clustering on a popular situation
comedy is also described with excellent preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05311</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05311</id><created>2015-04-21</created><updated>2015-09-18</updated><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Lu</keyname><forenames>Xuanxuan</forenames></author></authors><title>Transceiver Design for Clustered Wireless Sensor Networks --- Towards
  SNR Maximization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the transceiver design problem in a noisy-sensing
noisy-transmission multi-input multi-output (MIMO) wireless sensor network.
Consider a cluster-based network, where multiple sensors scattering across
several clusters will first send their noisy observations to their respective
cluster-heads (CH), who will then forward the data to one common fusion center
(FC). The cluster-heads and the fusion center collectively form a coherent-sum
multiple access channel (MAC) that is affected by fading and additive noise.
Our goal is to jointly design the linear transceivers at the CHs and the FC to
maximize the signal-to-noise ratio (SNR) of the recovered signal. We develop
three iterative block coordinated ascent (BCA) algorithms: 2-block BCA based on
semidefinite relaxation (SDR) and rank reduction via randomization or solving
linear equations, 2-block BCA based on iterative second-order cone programming
(SOCP), and multi-block BCA that lends itself to efficient closed-form
solutions in specific but important scenarios. We show that all of these
methods optimize SNR very well but each has different efficiency
characteristics that are tailored for different network setups. Convergence
analysis is carried out and extensive numerical results are presented to
confirm our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05318</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05318</id><created>2015-04-21</created><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author><author><keyname>Ramadan</keyname><forenames>Mohammed</forenames></author></authors><title>Compressive Random Access Using A Common Overloaded Control Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a &quot;one shot&quot; random access procedure where users can send a
message without a priori synchronizing with the network. In this procedure a
common overloaded control channel is used to jointly detect sparse user
activity and sparse channel profiles. The detected information is subsequently
used to demodulate the data in dedicated frequency slots. We analyze the system
theoretically and provide a link between achievable rates and standard
compressing sensing estimates in terms of explicit expressions and scaling
laws. Finally, we support our findings with simulations in an LTE-A-like
setting allowing &quot;one shot&quot; sparse random access of 100 users in 1ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05319</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05319</id><created>2015-04-21</created><updated>2015-05-20</updated><authors><author><keyname>Qu</keyname><forenames>Lizhen</forenames></author><author><keyname>Ferraro</keyname><forenames>Gabriela</forenames></author><author><keyname>Zhou</keyname><forenames>Liyuan</forenames></author><author><keyname>Hou</keyname><forenames>Weiwei</forenames></author><author><keyname>Schneider</keyname><forenames>Nathan</forenames></author><author><keyname>Baldwin</keyname><forenames>Timothy</forenames></author></authors><title>Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word:
  The Impact of Word Representation on Sequence Labelling Tasks</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Word embeddings -- distributed word representations that can be learned from
unlabelled data -- have been shown to have high utility in many natural
language processing applications. In this paper, we perform an extrinsic
evaluation of five popular word embedding methods in the context of four
sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE
identification. A particular focus of the paper is analysing the effects of
task-based updating of word representations. We show that when using word
embeddings as features, as few as several hundred training instances are
sufficient to achieve competitive results, and that word embeddings lead to
improvements over OOV words and out of domain. Perhaps more surprisingly, our
results indicate there is little difference between the different word
embedding methods, and that simple Brown clusters are often competitive with
word embeddings across all tasks we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05321</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05321</id><created>2015-04-21</created><updated>2015-11-11</updated><authors><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author><author><keyname>Valiant</keyname><forenames>Paul</forenames></author></authors><title>Instance Optimal Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following basic learning task: given independent draws from
an unknown distribution over a discrete support, output an approximation of the
distribution that is as accurate as possible in $\ell_1$ distance (i.e. total
variation or statistical distance). Perhaps surprisingly, it is often possible
to &quot;de-noise&quot; the empirical distribution of the samples to return an
approximation of the true distribution that is significantly more accurate than
the empirical distribution, without relying on any prior assumptions on the
distribution. We present an instance optimal learning algorithm which optimally
performs this de-noising for every distribution for which such a de-noising is
possible. More formally, given $n$ independent draws from a distribution $p$,
our algorithm returns a labelled vector whose expected distance from $p$ is
equal to the minimum possible expected error that could be obtained by any
algorithm that knows the true unlabeled vector of probabilities of distribution
$p$ and simply needs to assign labels, up to an additive subconstant term that
is independent of $p$ and goes to zero as $n$ gets large. One conceptual
implication of this result is that for large samples, Bayesian assumptions on
the &quot;shape&quot; or bounds on the tail probabilities of a distribution over discrete
support are not helpful for the task of learning the distribution.
  As a consequence of our techniques, we also show that given a set of $n$
samples from an arbitrary distribution, one can accurately estimate the
expected number of distinct elements that will be observed in a sample of any
size up to $n \log n$. This sort of extrapolation is practically relevant,
particularly to domains such as genomics where it is important to understand
how much more might be discovered given larger sample sizes, and we are
optimistic that our approach is practically viable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05323</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05323</id><created>2015-04-21</created><authors><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author></authors><title>A Novel Variable Step Size NLMS Algorithm Based on the Power Estimate of
  the System Noise</title><categories>cs.SY</categories><comments>6 pages, 8 figures, submitted to Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the tradeoff of the conventional normalized least mean square
(NLMS) algorithm between fast convergence rate and low steady-state
misalignment, this paper proposes a variable step size (VSS) NLMS algorithm by
devising a new strategy to update the step size. In this strategy, the input
signal power and the cross-correlation between the input signal and the error
signal are used to estimate the true tracking error power, reducing the effect
of the system noise on the algorithm performance. Moreover, the steady-state
performances of the algorithm are provided for Gaussian white input signal and
are verified by simulations. Finally, simulation results in the context of the
system identification and acoustic echo cancellation (AEC) have demonstrated
that the proposed algorithm has lower steady-state misalignment than other VSS
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05331</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05331</id><created>2015-04-21</created><authors><author><keyname>Moussaid</keyname><forenames>Mehdi</forenames></author><author><keyname>Brighton</keyname><forenames>Henry</forenames></author><author><keyname>Gaissmaier</keyname><forenames>Wolfgang</forenames></author></authors><title>The amplification of risk in experimental diffusion chains</title><categories>physics.soc-ph cs.SI</categories><comments>Published online in PNAS Early Edition (open-access):
  http://www.pnas.org/content/early/2015/04/14/1421883112</comments><doi>10.1073/pnas.1421883112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how people form and revise their perception of risk is central
to designing efficient risk communication methods, eliciting risk awareness,
and avoiding unnecessary anxiety among the public. However, public responses to
hazardous events such as climate change, contagious outbreaks, and terrorist
threats are complex and difficult-to-anticipate phenomena. Although many
psychological factors influencing risk perception have been identified in the
past, it remains unclear how perceptions of risk change when propagated from
one person to another and what impact the repeated social transmission of
perceived risk has at the population scale. Here, we study the social dynamics
of risk perception by analyzing how messages detailing the benefits and harms
of a controversial antibacterial agent undergo change when passed from one
person to the next in 10-subject experimental diffusion chains. Our analyses
show that when messages are propagated through the diffusion chains, they tend
to become shorter, gradually inaccurate, and increasingly dissimilar between
chains. In contrast, the perception of risk is propagated with higher fidelity
due to participants manipulating messages to fit their preconceptions, thereby
influencing the judgments of subsequent participants. Computer simulations
implementing this simple influence mechanism show that small judgment biases
tend to become more extreme, even when the injected message contradicts
preconceived risk judgments. Our results provide quantitative insights into the
social amplification of risk perception, and can help policy makers better
anticipate and manage the public response to emerging threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05349</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05349</id><created>2015-04-21</created><authors><author><keyname>Bartz</keyname><forenames>Hannes</forenames></author><author><keyname>Sidorenko</keyname><forenames>Vladimir</forenames></author></authors><title>List and Probabilistic Unique Decoding of Folded Subspace Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure, accepted for ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of folded subspace codes for noncoherent network coding is
presented. The codes can correct insertions and deletions beyond the unique
decoding radius for any code rate $R\in[0,1]$. An efficient interpolation-based
decoding algorithm for this code construction is given which allows to correct
insertions and deletions up to the normalized radius $s(1-((1/h+h)/(h-s+1))R)$,
where $h$ is the folding parameter and $s\leq h$ is a decoding parameter. The
algorithm serves as a list decoder or as a probabilistic unique decoder that
outputs a unique solution with high probability. An upper bound on the average
list size of (folded) subspace codes and on the decoding failure probability is
derived. A major benefit of the decoding scheme is that it enables
probabilistic unique decoding up to the list decoding radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05351</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05351</id><created>2015-04-21</created><authors><author><keyname>Bora</keyname><forenames>Siddharth</forenames></author><author><keyname>Singh</keyname><forenames>Harvineet</forenames></author><author><keyname>Sen</keyname><forenames>Anirban</forenames></author><author><keyname>Bagchi</keyname><forenames>Amitabha</forenames></author><author><keyname>Singla</keyname><forenames>Parag</forenames></author></authors><title>On the Role of Conductance, Geography and Topology in Predicting Hashtag
  Virality</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Soc. Netw. Anal. Min. 5(1):57, December 2015</journal-ref><doi>10.1007/s13278-015-0300-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on three aspects of the early spread of a hashtag in order to
predict whether it will go viral: the network properties of the subset of users
tweeting the hashtag, its geographical properties, and, most importantly, its
conductance-related properties. One of our significant contributions is to
discover the critical role played by the conductance based features for the
successful prediction of virality. More specifically, we show that the first
derivative of the conductance gives an early indication of whether the hashtag
is going to go viral or not. We present a detailed experimental evaluation of
the effect of our various categories of features on the virality prediction
task. When compared to the baselines and the state of the art techniques
proposed in the literature our feature set is able to achieve significantly
better accuracy on a large dataset of 7.7 million users and all their tweets
over a period of month, as well as on existing datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05353</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05353</id><created>2015-04-21</created><authors><author><keyname>Ikarashi</keyname><forenames>Dai</forenames></author><author><keyname>Kikuchi</keyname><forenames>Ryo</forenames></author><author><keyname>Chida</keyname><forenames>Koji</forenames></author><author><keyname>Takahashi</keyname><forenames>Katsumi</forenames></author></authors><title>k-anonymous Microdata Release via Post Randomisation Method</title><categories>cs.CR</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of the release of anonymized microdata is an important topic in
the fields of statistical disclosure control (SDC) and privacy preserving data
publishing (PPDP), and yet it remains sufficiently unsolved. In these research
fields, k-anonymity has been widely studied as an anonymity notion for mainly
deterministic anonymization algorithms, and some probabilistic relaxations have
been developed. However, they are not sufficient due to their limitations,
i.e., being weaker than the original k-anonymity or requiring strong parametric
assumptions. First we propose Pk-anonymity, a new probabilistic k-anonymity,
and prove that Pk-anonymity is a mathematical extension of k-anonymity rather
than a relaxation. Furthermore, Pk-anonymity requires no parametric
assumptions. This property has a significant meaning in the viewpoint that it
enables us to compare privacy levels of probabilistic microdata release
algorithms with deterministic ones. Second, we apply Pk-anonymity to the post
randomization method (PRAM), which is an SDC algorithm based on randomization.
PRAM is proven to satisfy Pk-anonymity in a controlled way, i.e, one can
control PRAM's parameter so that Pk-anonymity is satisfied. On the other hand,
PRAM is also known to satisfy ${\varepsilon}$-differential privacy, a recent
popular and strong privacy notion. This fact means that our results
significantly enhance PRAM since it implies the satisfaction of both important
notions: k-anonymity and ${\varepsilon}$-differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05358</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05358</id><created>2015-04-21</created><updated>2015-04-21</updated><authors><author><keyname>Kim</keyname><forenames>Jeemin</forenames></author><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Ko</keyname><forenames>Seung-Woo</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>User Attraction via Wireless Charging in Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong motivation of charging depleted battery can be an enabler for
network capacity increase. In this light we propose a spatial attraction
cellular network (SAN) consisting of macro cells overlaid with small cell base
stations that wirelessly charge user batteries. Such a network makes battery
depleting users move toward the vicinity of small cell base stations. With a
fine adjustment of charging power, this user spatial attraction (SA) yields
improvement in spectral efficiency (SE) as well as load balancing (LB). We
jointly optimize both enhancements thanks to SA, and derive the corresponding
optimal charging power in a closed form by using a stochastic geometric
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05369</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05369</id><created>2015-04-21</created><authors><author><keyname>Zecha</keyname><forenames>Dan</forenames></author><author><keyname>Lienhart</keyname><forenames>Rainer</forenames></author></authors><title>Key-Pose Prediction in Cyclic Human Motion</title><categories>cs.CV</categories><comments>Accepted at WACV 2015, 8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of estimating innercyclic time intervals
within repetitive motion sequences of top-class swimmers in a swimming channel.
Interval limits are given by temporal occurrences of key-poses, i.e.
distinctive postures of the body. A key-pose is defined by means of only one or
two specific features of the complete posture. It is often difficult to detect
such subtle features directly. We therefore propose the following method: Given
that we observe the swimmer from the side, we build a pictorial structure of
poselets to robustly identify random support poses within the regular motion of
a swimmer. We formulate a maximum likelihood model which predicts a key-pose
given the occurrences of multiple support poses within one stroke. The maximum
likelihood can be extended with prior knowledge about the temporal location of
a key-pose in order to improve the prediction recall. We experimentally show
that our models reliably and robustly detect key-poses with a high precision
and that their performance can be improved by extending the framework with
additional camera views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05372</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05372</id><created>2015-04-21</created><authors><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>Inferring Program Transformations from Type Transformations for
  Partitioning of Ordered Sets</title><categories>cs.PL</categories><comments>This work is supported by the EPSRC through the TyTra project
  (EP/L00058X/1)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper I introduce a mechanism to derive program transforma- tions
from order-preserving transformations of vector types. The purpose of this work
is to allow automatic generation of correct-by-construction instances of
programs in a streaming data processing paradigm suitable for FPGA processing.
We show that for it is possible to automatically derive instances for programs
based on combinations of opaque element- processing functions combined using
foldl and map, purely from the type transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05376</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05376</id><created>2015-04-21</created><updated>2015-05-29</updated><authors><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author></authors><title>A Minimax Converse for Quantum Channel Coding</title><categories>quant-ph cs.IT math.IT</categories><comments>merged with arXiv:1504.04617 version 1 ; see version 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a one-shot &quot;minimax&quot; converse bound for quantum channel coding
assisted by positive partial transpose channels between sender and receiver.
The bound is similar in spirit to the converse by Polyanskiy, Poor, and Verdu
[IEEE Trans. Info. Theory 56, 2307-2359 (2010)] for classical channel coding,
and also enjoys the saddle point property enabling the order of optimizations
to be interchanged. Equivalently, the bound can be formulated as a semidefinite
program satisfying strong duality. The convex nature of the bound implies
channel symmetries can substantially simplify the optimization, enabling us to
explicitly compute the finite blocklength behavior for several simple qubit
channels. In particular, we find that finite blocklength converse statements
for the classical erasure channel apply to the assisted quantum erasure
channel, while bounds for the classical binary symmetric channel apply to both
the assisted dephasing and depolarizing channels. This implies that these qubit
channels inherit statements regarding the asymptotic limit of large
blocklength, such as the strong converse or second-order converse rates, from
their classical counterparts. Moreover, for the dephasing channel, the finite
blocklength bounds are as tight as those for the classical binary symmetric
channel, since coding for classical phase errors yields equivalently-performing
unassisted quantum codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05381</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05381</id><created>2015-04-21</created><updated>2016-01-26</updated><authors><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames></author></authors><title>How do you revise your belief set with %$;@*?</title><categories>cs.AI</categories><comments>Corrected the following: 1. In Definition 1, the function I and Assoc
  were both defined to map into 2^Props x 2^Props, but they should be clearly
  into 2^{Props x Props}. 2. In Definition 1, one disjunctive case was being
  omitted. One case (5th item) was inserted to complete the picture</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic AGM belief revision theory, beliefs are static and do not
change their own shape. For instance, if p is accepted by a rational agent, it
will remain p to the agent. But such rarely happens to us. Often, when we
accept some information p, what is actually accepted is not the whole p, but
only a portion of it; not necessarily because we select the portion but because
p must be perceived. Only the perceived p is accepted; and the perception is
subject to what we already believe (know). What may, however, happen to the
rest of p that initially escaped our attention? In this work we argue that the
invisible part is also accepted to the agent, if only unconsciously. Hence some
parts of p are accepted as visible, while some other parts as latent, beliefs.
The division is not static. As the set of beliefs changes, what were hidden may
become visible. We present a perception-based belief theory that incorporates
latent beliefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05383</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05383</id><created>2015-04-21</created><authors><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author></authors><title>HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis</title><categories>q-bio.PE cs.SI stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05400</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05400</id><created>2015-04-21</created><authors><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author></authors><title>Ergodic convergence of a stochastic proximal point algorithm</title><categories>math.OC cs.NA</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to establish the almost sure weak ergodic
convergence of a sequence of iterates $(x_n)$ given by $x_{n+1} = (I+\lambda_n
A(\xi_{n+1},\,.\,))^{-1}(x_n)$ where $(A(s,\,.\,):s\in E)$ is a collection of
maximal monotone operators on a separable Hilbert space, $(\xi_n)$ is an
independent identically distributed sequence of random variables on $E$ and
$(\lambda_n)$ is a positive sequence in $\ell^2\backslash \ell^1$. The weighted
averaged sequence of iterates is shown to converge weakly to a zero (assumed to
exist) of the Aumann expectation ${\mathbb E}(A(\xi_1,\,.\,))$ under the
assumption that the latter is maximal. We consider applications to stochastic
optimization problems of the form $\min {\mathbb E}(f(\xi_1,x))$ w.r.t. $x\in
\bigcap_{i=1}^m X_i$ where $f$ is a normal convex integrand and $(X_i)$ is a
collection of closed convex sets. In this case, the iterations are closely
related to a stochastic proximal algorithm recently proposed by Wang and
Bertsekas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05401</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05401</id><created>2015-04-21</created><updated>2015-04-24</updated><authors><author><keyname>Karthick</keyname><forenames>T.</forenames></author></authors><title>Weighted Independent Sets in a Subclass of $P_6$-free Graphs</title><categories>cs.DM math.CO</categories><comments>arXiv admin note: text overlap with arXiv:1503.06025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Weight Independent Set (MWIS) problem on graphs with vertex
weights asks for a set of pairwise nonadjacent vertices of maximum total
weight. The complexity of the MWIS problem for $P_6$-free graphs is unknown. In
this note, we show that the MWIS problem can be solved in time $O(n^3m)$ for
($P_6$, banner)-free graphs by analyzing the structure of subclasses of these
class of graphs. This extends the existing results for ($P_5$, banner)-free
graphs, and ($P_6$, $C_4$)-free graphs. Here, $P_t$ denotes the chordless path
on $t$ vertices, and a banner is the graph obtained from a chordless cycle on
four vertices by adding a vertex that has exactly one neighbor on the cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05408</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05408</id><created>2015-04-21</created><authors><author><keyname>Tao</keyname><forenames>Hong</forenames></author><author><keyname>Hou</keyname><forenames>Chenping</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Jiao</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Yi</keyname><forenames>Dongyun</forenames></author></authors><title>Effective Discriminative Feature Selection with Non-trivial Solutions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection and feature transformation, the two main ways to reduce
dimensionality, are often presented separately. In this paper, a feature
selection method is proposed by combining the popular transformation based
dimensionality reduction method Linear Discriminant Analysis (LDA) and sparsity
regularization. We impose row sparsity on the transformation matrix of LDA
through ${\ell}_{2,1}$-norm regularization to achieve feature selection, and
the resultant formulation optimizes for selecting the most discriminative
features and removing the redundant ones simultaneously. The formulation is
extended to the ${\ell}_{2,p}$-norm regularized case: which is more likely to
offer better sparsity when $0&lt;p&lt;1$. Thus the formulation is a better
approximation to the feature selection problem. An efficient algorithm is
developed to solve the ${\ell}_{2,p}$-norm based optimization problem and it is
proved that the algorithm converges when $0&lt;p\le 2$. Systematical experiments
are conducted to understand the work of the proposed method. Promising
experimental results on various types of real-world data sets demonstrate the
effectiveness of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05411</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05411</id><created>2015-04-21</created><authors><author><keyname>Nyga</keyname><forenames>Daniel</forenames></author><author><keyname>Beetz</keyname><forenames>Michael</forenames></author></authors><title>Reasoning about Unmodelled Concepts - Incorporating Class Taxonomies in
  Probabilistic Relational Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in the application of first-order probabilistic methods is the
enormous size of graphical models they imply. The size results from the
possible worlds that can be generated by a domain of objects and relations. One
of the reasons for this explosion is that so far the approaches do not
sufficiently exploit the structure and similarity of possible worlds in order
to encode the models more compactly. We propose fuzzy inference in Markov logic
networks, which enables the use of taxonomic knowledge as a source of imposing
structure onto possible worlds. We show that by exploiting this structure,
probability distributions can be represented more compactly and that the
reasoning systems become capable of reasoning about concepts not contained in
the probabilistic knowledge base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05427</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05427</id><created>2015-04-21</created><updated>2015-05-29</updated><authors><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Varma</keyname><forenames>Rohan</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Signal Recovery on Graphs: Random versus Experimentally Designed
  Sampling</title><categories>cs.IT math.IT stat.ML</categories><comments>Correct some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study signal recovery on graphs based on two sampling strategies: random
sampling and experimentally designed sampling. We propose a new class of smooth
graph signals, called approximately bandlimited, which generalizes the
bandlimited class and is similar to the globally smooth class. We then propose
two recovery strategies based on random sampling and experimentally designed
sampling. The proposed recovery strategy based on experimentally designed
sampling is similar to the leverage scores used in the matrix approximation. We
show that while both strategies are unbiased estimators for the low-frequency
components, the convergence rate of experimentally designed sampling is much
faster than that of random sampling when a graph is irregular. We validate the
proposed recovery strategies on three specific graphs: a ring graph, an
Erd\H{o}s-R\'enyi graph, and a star graph. The simulation results support the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05429</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05429</id><created>2015-04-21</created><updated>2015-09-05</updated><authors><author><keyname>Kim</keyname><forenames>Bryce M.</forenames></author></authors><title>Analog Computer Understanding of Hamiltonian Paths</title><categories>cs.OH</categories><comments>8 pages, 1 figure</comments><acm-class>C.1.3; F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores finding existence of undirected hamiltonian paths in a
graph using lumped/ideal circuits, specifically low-pass filters. While other
alternatives are possible, a first-order RC low-pass filter is chosen to
describe the process. The paper proposes a way of obtaining time complexity for
counting the number of hamiltonian paths in a graph, and then shows that the
time complexity of the circuits is around $O(n \log n)$ where $n$ is the number
of vertices in a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05430</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05430</id><created>2015-04-21</created><updated>2015-06-04</updated><authors><author><keyname>Haralabopoulos</keyname><forenames>Giannis</forenames></author></authors><title>An Anonymous Social Network of Opinions</title><categories>cs.SI physics.soc-ph</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research interest on Online Social Networks (OSNs), has increased
dramatically over the last decade, mainly because online networks provide a
vast source of social information. Graph structure, user connections, growth,
information exposure and diffusion, are some of the most frequently researched
subjects. However, some areas of these networks, such as anonymity, equality
and bias are overlooked or even unconsidered. In the related bibliography, such
features seem to be influential to social interactions. Based on these studies,
we aim at determining how universal anonymity affects bias, user equality,
information propagation, sharing and exposure, connection establishment, as
well as network structure. Thus, we propose a new Anonymous Online Social
Network, which will facilitate a variety of monitoring and data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05431</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05431</id><created>2015-04-21</created><authors><author><keyname>Hauteville</keyname><forenames>Adrien</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>New algorithms for decoding in the rank metric and an attack on the LRPC
  cryptosystem</title><categories>cs.CR cs.IT math.IT</categories><comments>A shortened version of this paper will be published in the
  proceedings of the IEEE International Symposium on Information Theory 2015
  (ISIT 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the decoding problem or the problem of finding low weight
codewords for rank metric codes. We show how additional information about the
codeword we want to find under the form of certain linear combinations of the
entries of the codeword leads to algorithms with a better complexity. This is
then used together with a folding technique for attacking a McEliece scheme
based on LRPC codes. It leads to a feasible attack on one of the parameters
suggested in \cite{GMRZ13}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05451</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05451</id><created>2015-04-21</created><updated>2015-04-21</updated><authors><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Zhang</keyname><forenames>Kaihua</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author></authors><title>Adaptive Compressive Tracking via Online Vector Boosting Feature
  Selection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the compressive tracking (CT) method has attracted much attention
due to its high efficiency, but it cannot well deal with the large scale target
appearance variations due to its data-independent random projection matrix that
results in less discriminative features. To address this issue, in this paper
we propose an adaptive CT approach, which selects the most discriminative
features to design an effective appearance model. Our method significantly
improves CT in three aspects: Firstly, the most discriminative features are
selected via an online vector boosting method. Secondly, the object
representation is updated in an effective online manner, which preserves the
stable features while filtering out the noisy ones. Finally, a simple and
effective trajectory rectification approach is adopted that can make the
estimated location more accurate. Extensive experiments on the CVPR2013
tracking benchmark demonstrate the superior performance of our algorithm
compared over state-of-the-art tracking algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05455</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05455</id><created>2015-04-21</created><authors><author><keyname>Nadeem</keyname><forenames>Qurrat-Ul-Ain</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>A Generalized Spatial Correlation Model for 3D MIMO Channels based on
  the Fourier Coefficients of Power Spectrums</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Transactions on signal processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies have confirmed the adverse impact of fading correlation on
the mutual information (MI) of two-dimensional (2D) multiple-input
multiple-output (MIMO) systems. More recently, the trend is to enhance the
system performance by exploiting the channel's degrees of freedom in the
elevation, which necessitates the derivation and characterization of
three-dimensional (3D) channels in the presence of spatial correlation. In this
paper, an exact closed-form expression for the Spatial Correlation Function
(SCF) is derived for 3D MIMO channels. This novel SCF is developed for a
uniform linear array of antennas with nonisotropic antenna patterns. The
proposed method resorts to the spherical harmonic expansion (SHE) of plane
waves and the trigonometric expansion of Legendre and associated Legendre
polynomials. The resulting expression depends on the underlying arbitrary
angular distributions and antenna patterns through the Fourier Series (FS)
coefficients of power azimuth and elevation spectrums. The novelty of the
proposed method lies in the SCF being valid for any 3D propagation environment.
The developed SCF determines the covariance matrices at the transmitter and the
receiver that form the Kronecker channel model. In order to quantify the
effects of correlation on the system performance, the information-theoretic
deterministic equivalents of the MI for the Kronecker model are utilized in
both mono-user and multi-user cases. Numerical results validate the proposed
analytical expressions and elucidate the dependence of the system performance
on azimuth and elevation angular spreads and antenna patterns. Some useful
insights into the behaviour of MI as a function of downtilt angles are
provided. The derived model will help evaluate the performance of correlated 3D
MIMO channels in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05457</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05457</id><created>2015-04-21</created><updated>2015-05-13</updated><authors><author><keyname>Kashnitsky</keyname><forenames>Yury</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author></authors><title>Graphlet-based lazy associative graph classification</title><categories>cs.DS cs.AI</categories><comments>This paper has been withdrawn by the author due to the incomplete set
  of necessary definitions and experiments</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the graph classification problem and introduces a
modification of the lazy associative classification method to efficiently
handle intersections of graphs. Graph intersections are approximated with all
common subgraphs up to a fixed size similarly to what is done with graphlet
kernels. We explain the idea of the algorithm with a toy example and describe
our experiments with a predictive toxicology dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05469</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05469</id><created>2015-04-21</created><authors><author><keyname>Kashnitsky</keyname><forenames>Yury</forenames></author></authors><title>Visual analytics in FCA-based clustering</title><categories>cs.IR cs.AI</categories><comments>11 pages, 3 figures, 2 algorithms, 3rd International Conference on
  Analysis of Images, Social Networks and Texts (AIST'2014). in Supplementary
  Proceedings of the 3rd International Conference on Analysis of Images, Social
  Networks and Texts (AIST 2014), Vol. 1197, CEUR-WS.org, 2014</comments><msc-class>62-07, 00A66</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual analytics is a subdomain of data analysis which combines both human
and machine analytical abilities and is applied mostly in decision-making and
data mining tasks. Triclustering, based on Formal Concept Analysis (FCA), was
developed to detect groups of objects with similar properties under similar
conditions. It is used in Social Network Analysis (SNA) and is a basis for
certain types of recommender systems. The problem of triclustering algorithms
is that they do not always produce meaningful clusters. This article describes
a specific triclustering algorithm and a prototype of a visual analytics
platform for working with obtained clusters. This tool is designed as a testing
frameworkis and is intended to help an analyst to grasp the results of
triclustering and recommender algorithms, and to make decisions on
meaningfulness of certain triclusters and recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05473</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05473</id><created>2015-04-21</created><authors><author><keyname>Kashnitsky</keyname><forenames>Yury</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>Can FCA-based Recommender System Suggest a Proper Classifier?</title><categories>cs.IR cs.LG stat.ML</categories><comments>10 pages, 1 figure, 4 tables, ECAI 2014, workshop &quot;What FCA can do
  for &quot;Artifficial Intelligence&quot;</comments><msc-class>62-07</msc-class><journal-ref>CEUR Workshop Proceedings, 1257, pp. 17-26 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper briefly introduces multiple classifier systems and describes a new
algorithm, which improves classification accuracy by means of recommendation of
a proper algorithm to an object classification. This recommendation is done
assuming that a classifier is likely to predict the label of the object
correctly if it has correctly classified its neighbors. The process of
assigning a classifier to each object is based on Formal Concept Analysis. We
explain the idea of the algorithm with a toy example and describe our first
experiments with real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05474</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05474</id><created>2015-04-21</created><updated>2015-07-13</updated><authors><author><keyname>Limmer</keyname><forenames>Steffen</forenames></author><author><keyname>Mohammadi</keyname><forenames>Jafar</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>A Simple Algorithm for Approximation by Nomographic Functions</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, 2 tables. v2: various improvements and minor
  corrections, added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel algorithmic solution for the approximation of a
given multivariate function by a nomographic function that is composed of a
one-dimensional continuous and monotone outer function and a sum of univariate
continuous inner functions. We show that a suitable approximation can be
obtained by solving a cone-constrained Rayleigh-Quotient optimization problem.
The proposed approach is based on a combination of a dimensionwise function
decomposition known as Analysis of Variance (ANOVA) and optimization over a
class of monotone polynomials. An example is given to show that the proposed
algorithm can be applied to solve problems in distributed function computation
over multiple-access channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05476</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05476</id><created>2015-04-21</created><authors><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Optimal parameterized algorithms for planar facility location problems
  using Voronoi diagrams</title><categories>cs.DS cs.CC cs.CG</categories><comments>64 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general family of facility location problems defined on planar
graphs and on the 2-dimensional plane. In these problems, a subset of $k$
objects has to be selected, satisfying certain packing (disjointness) and
covering constraints. Our main result is showing that, for each of these
problems, the $n^{O(k)}$ time brute force algorithm of selecting $k$ objects
can be improved to $n^{O(\sqrt{k})}$ time. The algorithm is based on an idea
that was introduced recently in the design of geometric QPTASs, but was not yet
used for exact algorithms and for planar graphs. We focus on the Voronoi
diagram of a hypothetical solution of $k$ objects, guess a balanced separator
cycle of this Voronoi diagram to obtain a set that separates the solution in a
balanced way, and then recurse on the resulting subproblems. We complement our
study by giving evidence that packing problems have $n^{O(\sqrt{k})}$ time
algorithms for a much more general class of objects than covering problems
have.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05477</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05477</id><created>2015-04-21</created><updated>2015-10-30</updated><authors><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Musco</keyname><forenames>Christopher</forenames></author></authors><title>Randomized Block Krylov Methods for Stronger and Faster Approximate
  Singular Value Decomposition</title><categories>cs.DS cs.LG cs.NA</categories><comments>Neural Information Processing Systems 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko,
Martinsson, and Tropp, randomized Simultaneous Power Iteration has become the
method of choice for approximate singular value decomposition. It is more
accurate than simpler sketching algorithms, yet still converges quickly for any
matrix, independently of singular value gaps. After $\tilde{O}(1/\epsilon)$
iterations, it gives a low-rank approximation within $(1+\epsilon)$ of optimal
for spectral norm error.
  We give the first provable runtime improvement on Simultaneous Iteration: a
simple randomized block Krylov method, closely related to the classic Block
Lanczos algorithm, gives the same guarantees in just
$\tilde{O}(1/\sqrt{\epsilon})$ iterations and performs substantially better
experimentally. Despite their long history, our analysis is the first of a
Krylov subspace method that does not depend on singular value gaps, which are
unreliable in practice.
  Furthermore, while it is a simple accuracy benchmark, even $(1+\epsilon)$
error for spectral norm low-rank approximation does not imply that an algorithm
returns high quality principal components, a major issue for data applications.
We address this problem for the first time by showing that both Block Krylov
Iteration and a minor modification of Simultaneous Iteration give nearly
optimal PCA for any matrix. This result further justifies their strength over
non-iterative sketching methods.
  Finally, we give insight beyond the worst case, justifying why both
algorithms can run much faster in practice than predicted. We clarify how
simple techniques can take advantage of common matrix properties to
significantly improve runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05479</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05479</id><created>2015-04-21</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author></authors><title>A Proof of the Convergence of the Hegselmann-Krause Dynamics on the
  Circle</title><categories>cs.SY</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we give a complete proof that Hegselmann-Krause systems
converge on the circle following the proof strategy developed by Hegarty,
Martinsson, and Wedin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05487</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05487</id><created>2015-04-21</created><authors><author><keyname>Wiatowski</keyname><forenames>Thomas</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Deep Convolutional Neural Networks Based on Semi-Discrete Frames</title><categories>cs.LG cs.IT math.FA math.IT stat.ML</categories><comments>Proc. of IEEE International Symposium on Information Theory (ISIT),
  Hong Kong, China, June 2015, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have led to breakthrough results in
practical feature extraction applications. The mathematical analysis of these
networks was pioneered by Mallat, 2012. Specifically, Mallat considered
so-called scattering networks based on identical semi-discrete wavelet frames
in each network layer, and proved translation-invariance as well as deformation
stability of the resulting feature extractor. The purpose of this paper is to
develop Mallat's theory further by allowing for different and, most
importantly, general semi-discrete frames (such as, e.g., Gabor frames,
wavelets, curvelets, shearlets, ridgelets) in distinct network layers. This
allows to extract wider classes of features than point singularities resolved
by the wavelet transform. Our generalized feature extractor is proven to be
translation-invariant, and we develop deformation stability results for a
larger class of deformations than those considered by Mallat. For Mallat's
wavelet-based feature extractor, we get rid of a number of technical
conditions. The mathematical engine behind our results is continuous frame
theory, which allows us to completely detach the invariance and deformation
stability proofs from the particular algebraic structure of the underlying
frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05492</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05492</id><created>2015-04-21</created><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>An information diffusion Fano inequality</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><msc-class>94A17, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present an information diffusion inequality derived from an
elementary argument, which gives rise to a very general Fano-type inequality.
The latter unifies and generalizes the distance-based Fano inequality and the
continuous Fano inequality established in [Corollary 1, Propositions 1 and 2,
arXiv:1311.2669v2], as well as the generalized Fano inequality in [Equation
following (10); T. S. Han and S. Verd\'u. Generalizing the Fano inequality.
IEEE Transactions on Information Theory, 40(4):1247-1251, July 1994].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05498</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05498</id><created>2015-04-21</created><updated>2015-06-17</updated><authors><author><keyname>Torrellas</keyname><forenames>Marc</forenames></author><author><keyname>Agustin</keyname><forenames>Adrian</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>DoF-Delay Trade-Off for the $K$-user MIMO Interference Channel With
  Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>31 pages, 10 figures, 3 tables, submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) of the $K$-user multiple-input multiple-output
(MIMO) interference channel are studied when perfect, but delayed channel state
information is available at the transmitter side (delayed CSIT). Recent works
have proposed schemes that achieve increasing DoF values, but at the cost of
long communication delays. This work proposes three linear precoding
strategies, formulated in such a way that the achievable DoF can be derived as
a function of the transmission delay, thus elucidating its achievable DoF-delay
trade-off. All strategies are based on the concept of interference alignment,
and built upon three main ingredients: delayed CSIT precoding, user scheduling,
and redundancy transmission. In this respect, the interference alignment is
realized by exploiting delayed CSIT in order to align the interference at the
non-intended receivers along the space-time domain. Finally, the latter part of
this work settles that all the proposed strategies work also for constant
channels, except for SISO. In such a case, the schemes can be made feasible by
resorting to asymmetric complex signaling concepts. This conclusion removes the
time-varying channels assumption unnecessarily common along all the literature
on delayed CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05513</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05513</id><created>2015-04-21</created><updated>2015-10-10</updated><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Astefanoaei</keyname><forenames>Lacramioara</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author><author><keyname>Rayana</keyname><forenames>Souha Ben</forenames></author><author><keyname>Bensalem</keyname><forenames>Saddek</forenames></author></authors><title>Timed Orchestration for Component-based Systems</title><categories>cs.FL cs.LO cs.SC</categories><comments>Timestamp of the work, with evaluation added by creating MES
  orchestration examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individual machines in flexible production lines explicitly expose
capabilities at their interfaces by means of parametric skills. Given such a
set of configurable machines, a line integrator is faced with the problem of
finding and tuning parameters for each machine such that the overall production
line implements given safety and temporal requirements in an optimized and
robust fashion. We formalize this problem of configuring and orchestrating
flexible production lines as a parameter synthesis problem for systems of
parametric timed automata, where interactions are based on skills. Parameter
synthesis problems for interaction-level LTL properties are translated to
parameter synthesis problems for state-based safety properties. For safety
properties, synthesis problems are solved by checking satisfiability of
$\exists\forall$SMT constraints. For constraint generation, we provide a set of
computationally cheap over-approximations of the set of reachable states,
together with fence constructions as sufficient conditions for safety formulas.
We demonstrate the feasibility of our approach by solving typical machine
configuration problems as encountered in industrial automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05515</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05515</id><created>2015-04-21</created><authors><author><keyname>Baste</keyname><forenames>Julien</forenames></author><author><keyname>Faria</keyname><forenames>Luerbio</forenames></author><author><keyname>Klein</keyname><forenames>Sulamita</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author></authors><title>Parameterized complexity dichotomy for $(r,\ell)$-Vertex Deletion</title><categories>cs.DS cs.CC</categories><comments>16 pages, 2 figures</comments><msc-class>68R10, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For two integers $r, \ell \geq 0$, a graph $G = (V, E)$ is an
$(r,\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\ell$
cliques. In the parameterized $(r,\ell)$-Vertex Deletion problem, given a graph
$G$ and an integer $k$, one has to decide whether at most $k$ vertices can be
removed from $G$ to obtain an $(r,\ell)$-graph. This problem is NP-hard if
$r+\ell \geq 1$ and encompasses several relevant problems such as Vertex Cover
and Odd Cycle Transversal. The parameterized complexity of $(r,\ell)$-Vertex
Deletion was known for all values of $(r,\ell)$ except for $(2,1)$, $(1,2)$,
and $(2,2)$. We prove that each of these three cases is FPT and, furthermore,
solvable in single-exponential time, which is asymptotically optimal in terms
of $k$. We consider as well the version of $(r,\ell)$-Vertex Deletion where the
set of vertices to be removed has to induce an independent set, and provide
also a parameterized complexity dichotomy for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05517</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05517</id><created>2015-04-21</created><authors><author><keyname>Pardo</keyname><forenames>Juan</forenames></author><author><keyname>Zamora-Martinez</keyname><forenames>Francisco</forenames></author><author><keyname>Botella-Rocamora</keyname><forenames>Paloma</forenames></author></authors><title>Online Learning Algorithm for Time Series Forecasting Suitable for Low
  Cost Wireless Sensor Networks Nodes</title><categories>cs.NI cs.LG cs.SY</categories><comments>28 pages, Published 21 April 2015 at MDPI's journal &quot;Sensors&quot;</comments><journal-ref>Sensors 2015, 15(4), 9277-9304</journal-ref><doi>10.3390/s150409277</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Time series forecasting is an important predictive methodology which can be
applied to a wide range of problems. Particularly, forecasting the indoor
temperature permits an improved utilization of the HVAC (Heating, Ventilating
and Air Conditioning) systems in a home and thus a better energy efficiency.
With such purpose the paper describes how to implement an Artificial Neural
Network (ANN) algorithm in a low cost system-on-chip to develop an autonomous
intelligent wireless sensor network. The present paper uses a Wireless Sensor
Networks (WSN) to monitor and forecast the indoor temperature in a smart home,
based on low resources and cost microcontroller technology as the 8051MCU. An
on-line learning approach, based on Back-Propagation (BP) algorithm for ANNs,
has been developed for real-time time series learning. It performs the model
training with every new data that arrive to the system, without saving enormous
quantities of data to create a historical database as usual, i.e., without
previous knowledge. Consequently to validate the approach a simulation study
through a Bayesian baseline model have been tested in order to compare with a
database of a real application aiming to see the performance and accuracy. The
core of the paper is a new algorithm, based on the BP one, which has been
described in detail, and the challenge was how to implement a computational
demanding algorithm in a simple architecture with very few hardware resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05519</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05519</id><created>2015-04-21</created><authors><author><keyname>Guo</keyname><forenames>Longkun</forenames></author><author><keyname>Liao</keyname><forenames>Kewen</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author></authors><title>Efficient Approximation Algorithms for Computing \emph{k} Disjoint
  Restricted Shortest Paths</title><categories>cs.DM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network applications, such as multimedia streaming and video conferencing,
impose growing requirements over Quality of Service (QoS), including bandwidth,
delay, jitter, etc. Meanwhile, networks are expected to be load-balanced,
energy-efficient, and resilient to some degree of failures. It is observed that
the above requirements could be better met with multiple disjoint QoS paths
than a single one. Let $G=(V,\, E)$ be a digraph with nonnegative integral cost
and delay on every edge, $s,\, t\in V$ be two specified vertices, and
$D\in\mathbb{Z}_{0}^{+}$ be a delay bound (or some other constraint), the
\emph{$k$ Disjoint Restricted Shortest Path} ($k$\emph{RSP})\emph{ Problem} is
computing $k$ disjoint paths between $s$ and $t$ with total cost minimized and
total delay bounded by $D$. Few efficient algorithms have been developed
because of the hardness of the problem.
  In this paper, we propose efficient algorithms with provable performance
guarantees for the $k$RSP problem. We first present a pseudo-polynomial-time
approximation algorithm with a bifactor approximation ratio of $(1,\,2)$, then
improve the algorithm to polynomial time with a bifactor ratio of
$(1+\epsilon,\,2+\epsilon)$ for any fixed $\epsilon&gt;0$, which is better than
the current best approximation ratio $(O(1+\gamma),\, O(1+\frac{1}{\gamma})\})$
for any fixed $\gamma&gt;0$ \cite{orda2004efficient}. To the best of our
knowledge, this is the first constant-factor algorithm that almost strictly
obeys the constraint for the $k$RSP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05522</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05522</id><created>2015-04-21</created><authors><author><keyname>Gunturu</keyname><forenames>Rupesh</forenames></author></authors><title>Survey of Sybil Attacks in Social Networks</title><categories>cs.CR</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews the Sybil attack in social networks, which has the
potential to compromise the whole distributed network. In the Sybil attack, the
malicious user claims multiple identities to compromise the network. Sybil
attacks can be used to change the overall ranking in voting applications,
bad-mouth an opinion, access resources or to break the trust mechanism behind a
P2P network. In this paper, different defense mechanisms used to mitigate Sybil
attacks are also reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05524</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05524</id><created>2015-04-21</created><authors><author><keyname>Wang</keyname><forenames>Heng</forenames></author><author><keyname>Oneata</keyname><forenames>Dan</forenames></author><author><keyname>Verbeek</keyname><forenames>Jakob</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>A robust and efficient video representation for action recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a state-of-the-art video representation and applies it
to efficient action recognition and detection. We first propose to improve the
popular dense trajectory features by explicit camera motion estimation. More
specifically, we extract feature point matches between frames using SURF
descriptors and dense optical flow. The matches are used to estimate a
homography with RANSAC. To improve the robustness of homography estimation, a
human detector is employed to remove outlier matches from the human body as
human motion is not constrained by the camera. Trajectories consistent with the
homography are considered as due to camera motion, and thus removed. We also
use the homography to cancel out camera motion from the optical flow. This
results in significant improvement on motion-based HOF and MBH descriptors. We
further explore the recent Fisher vector as an alternative feature encoding
approach to the standard bag-of-words histogram, and consider different ways to
include spatial layout information in these encodings. We present a large and
varied set of evaluations, considering (i) classification of short basic
actions on six datasets, (ii) localization of such actions in feature-length
movies, and (iii) large-scale recognition of complex events. We find that our
improved trajectory features significantly outperform previous dense
trajectories, and that Fisher vectors are superior to bag-of-words encodings
for video recognition tasks. In all three tasks, we show substantial
improvements over the state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05526</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05526</id><created>2015-04-21</created><updated>2015-04-22</updated><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Secret Key Generation with One Communicator and a One-Shot Converse via
  Hypercontractivity</title><categories>cs.IT math.IT</categories><comments>slightly extended version of ISIT 2015 paper</comments><journal-ref>Proc. 2015 IEEE Int. Symposium on Information Theory, Hong-Kong,
  June 15--19, 2015, pp 1457--1461</journal-ref><doi>10.1109/ISIT.2015.7282697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model of multi-party secret key agreement is proposed, in which one
terminal called the communicator can transmit public messages to other
terminals before all terminals agree on a secret key. A single-letter
characterization of the achievable region is derived in the stationary
memoryless case. The new model generalizes some other (old and new) models of
key agreement. In particular, key generation with an omniscient helper is the
special case where the communicator knows all sources, for which we derive a
zero-rate one-shot converse for the secret key per bit of communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05535</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05535</id><created>2015-04-21</created><updated>2015-09-28</updated><authors><author><keyname>Ganardi</keyname><forenames>Moses</forenames></author><author><keyname>Hucke</keyname><forenames>Danny</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Noeth</keyname><forenames>Eric</forenames></author></authors><title>Tree compression using string grammars</title><categories>cs.FL cs.DS</categories><msc-class>68P30, 68Q42, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the compressed representation of a ranked tree by a (string)
straight-line program (SLP) for its preorder traversal, and compare it with the
well-studied representation by straight-line context free tree grammars (which
are also known as tree straight-line programs or TSLPs). Although SLPs turn out
to be exponentially more succinct than TSLPs, we show that many simple tree
queries can still be performed efficiently on SLPs, such as computing the
height and Horton-Strahler number of a tree, tree navigation, or evaluation of
Boolean expressions. Other problems on tree traversals turn out to be
intractable, e.g. pattern matching and evaluation of tree automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05538</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05538</id><created>2015-04-21</created><updated>2015-06-23</updated><authors><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Joint Source-Channel Secrecy Using Hybrid Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secrecy performance of a source-channel model is studied in the context
of lossy source compression over a noisy broadcast channel. The source is
causally revealed to the eavesdropper during decoding. The fidelity of the
transmission to the legitimate receiver and the secrecy performance at the
eavesdropper are both measured by a distortion metric. Two achievability
schemes using the technique of hybrid coding are analyzed and compared with an
operationally separate source-channel coding scheme. A numerical example is
provided and the comparison results show that the hybrid coding schemes
outperform the operationally separate scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05539</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05539</id><created>2015-04-21</created><authors><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author><author><keyname>Tanner</keyname><forenames>Brian</forenames></author></authors><title>Temporal-Difference Networks</title><categories>cs.LG</categories><comments>8 pages, 3 figures, presented at the 2004 conference on Neural
  Information Processing Systems. in Advances in Neural Information Processing
  Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and
  Bottou, L. (Eds)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalization of temporal-difference (TD) learning to
networks of interrelated predictions. Rather than relating a single prediction
to itself at a later time, as in conventional TD methods, a TD network relates
each prediction in a set of predictions to other predictions in the set at a
later time. TD networks can represent and apply TD learning to a much wider
class of predictions than has previously been possible. Using a random-walk
example, we show that these networks can be used to learn to predict by a fixed
interval, which is not possible with conventional TD methods. Secondly, we show
that if the inter-predictive relationships are made conditional on action, then
the usual learning-efficiency advantage of TD methods over Monte Carlo
(supervised learning) methods becomes particularly pronounced. Thirdly, we
demonstrate that TD networks can learn predictive state representations that
enable exact solution of a non-Markov problem. A very broad range of
inter-predictive temporal relationships can be expressed in these networks.
Overall we argue that TD networks represent a substantial extension of the
abilities of TD methods and bring us closer to the goal of representing world
knowledge in entirely predictive, grounded terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05553</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05553</id><created>2015-04-21</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Lang</keyname><forenames>Harry</forenames></author><author><keyname>Levin</keyname><forenames>Keith</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Morteza</forenames></author></authors><title>A Unified Approach for Clustering Problems on Sliding Windows</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore clustering problems in the streaming sliding window model in both
general metric spaces and Euclidean space. We present the first polylogarithmic
space $O(1)$-approximation to the metric $k$-median and metric $k$-means
problems in the sliding window model, answering the main open problem posed by
Babcock, Datar, Motwani and O'Callaghan, which has remained unanswered for over
a decade. Our algorithm uses $O(k^3 \log^6 n)$ space and
$\operatorname{poly}(k, \log n)$ update time. This is an exponential
improvement on the space required by the technique due to Babcock, et al. We
introduce a data structure that extends smooth histograms as introduced by
Braverman and Ostrovsky to operate on a broader class of functions. In
particular, we show that using only polylogarithmic space we can maintain a
summary of the current window from which we can construct an $O(1)$-approximate
clustering solution.
  Merge-and-reduce is a generic method in computational geometry for adapting
offline algorithms to the insertion-only streaming model. Several well-known
coreset constructions are maintainable in the insertion-only streaming model
using this method, including well-known coreset techniques for the $k$-median,
$k$-means in both low-and high-dimensional Euclidean spaces. Previous work has
adapted these techniques to the insertion-deletion model, but translating them
to the sliding window model has remained a challenge. We give the first
algorithm that, given an insertion-only streaming coreset construction of space
$s$, maintains a $(1\pm\epsilon)$-approximate coreset in the sliding window
model using $O(s^2\epsilon^{-2}\log n)$ space.
  For clustering problems, our results constitute the first significant step
towards resolving problem number 20 from the List of Open Problems in Sublinear
Algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05556</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05556</id><created>2015-04-21</created><updated>2015-05-25</updated><authors><author><keyname>Bhangale</keyname><forenames>Amey</forenames></author><author><keyname>Saptharishi</keyname><forenames>Ramprasad</forenames></author><author><keyname>Varma</keyname><forenames>Girish</forenames></author><author><keyname>Venkat</keyname><forenames>Rakesh</forenames></author></authors><title>On Fortification of Projection Games</title><categories>cs.CC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent result of Moshkovitz \cite{Moshkovitz14} presented an ingenious
method to provide a completely elementary proof of the Parallel Repetition
Theorem for certain projection games via a construction called fortification.
However, the construction used in \cite{Moshkovitz14} to fortify arbitrary
label cover instances using an arbitrary extractor is insufficient to prove
parallel repetition. In this paper, we provide a fix by using a stronger graph
that we call fortifiers. Fortifiers are graphs that have both $\ell_1$ and
$\ell_2$ guarantees on induced distributions from large subsets. We then show
that an expander with sufficient spectral gap, or a bi-regular extractor with
stronger parameters (the latter is also the construction used in an independent
update \cite{Moshkovitz15} of \cite{Moshkovitz14} with an alternate argument),
is a good fortifier. We also show that using a fortifier (in particular
$\ell_2$ guarantees) is necessary for obtaining the robustness required for
fortification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05566</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05566</id><created>2015-04-21</created><updated>2015-04-22</updated><authors><author><keyname>Mishra</keyname><forenames>Shaunak</forenames></author><author><keyname>Shoukry</keyname><forenames>Yasser</forenames></author><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Secure State Estimation: Optimal Guarantees against Sensor Attacks in
  the Presence of Noise</title><categories>math.OC cs.CR cs.IT cs.SY math.IT</categories><comments>A shorter version of this work will appear in the proceedings of ISIT
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the need to secure cyber-physical systems against attacks, we
consider the problem of estimating the state of a noisy linear dynamical system
when a subset of sensors is arbitrarily corrupted by an adversary. We propose a
secure state estimation algorithm and derive (optimal) bounds on the achievable
state estimation error. In addition, as a result of independent interest, we
give a coding theoretic interpretation for prior work on secure state
estimation against sensor attacks in a noiseless dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05567</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05567</id><created>2015-04-21</created><authors><author><keyname>Cozzo</keyname><forenames>Emanuele</forenames></author><author><keyname>de Arruda</keyname><forenames>Guilherme Ferraz</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Multilayer networks: metrics and spectral properties</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Chapter contribution to the book &quot;Interconnected networks&quot;, edited by
  F. Schweitzer and A. Garas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayer networks represent systems in which there are several topological
levels each one representing one kind of interaction or interdependency between
the systems' elements. These networks have attracted a lot of attention
recently because their study allows considering different dynamical modes
concurrently. Here, we revise the main concepts and tools developed up to date.
Specifically, we focus on several metrics for multilayer network
characterization as well as on the spectral properties of the system, which
ultimately enable for the dynamical characterization of several critical
phenomena. The theoretical framework is also applied for description of
real-world multilayer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05574</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05574</id><created>2015-04-21</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Gaussian Quadrature Inference for Multicarrier Continuous-Variable
  Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>46 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the Gaussian quadrature inference (GQI) method for multicarrier
continuous-variable quantum key distribution (CVQKD). A multicarrier CVQKD
protocol utilizes Gaussian subcarrier quantum continuous variables (CV) for
information transmission. The GQI framework provides a minimal error estimate
of the quadratures of the CV quantum states from the discrete, measured noisy
subcarrier variables. GQI utilizes the fundamentals of regularization theory
and statistical information processing. We characterize GQI for multicarrier
CVQKD, and define a method for the statistical modeling and processing of noisy
Gaussian subcarrier quadratures. We demonstrate the results through the
adaptive multicarrier quadrature division (AMQD) scheme. We define direct GQI
(DGQI), and prove that it achieves a theoretical minimal magnitude error. We
introduce the terms statistical secret key rate and statistical private
classical information, which quantities are derived purely by the statistical
functions of GQI. We prove the secret key rate formulas for a multiple access
multicarrier CVQKD via the AMQD-MQA (multiuser quadrature allocation) scheme.
The GQI and DGQI frameworks can be established in an arbitrary CVQKD protocol
and measurement setting, and are implementable by standard low-complexity
statistical functions, which is particularly convenient for an experimental
CVQKD scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05597</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05597</id><created>2015-04-21</created><authors><author><keyname>Zuiddam</keyname><forenames>Jeroen</forenames></author></authors><title>A note on the gap between rank and border rank</title><categories>math.AC cs.DM quant-ph</categories><comments>Comments are welcome</comments><msc-class>68Q17, 15A69, 16Z05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the tensor rank of a certain algebra. As a result we find a sequence
of tensors with a large gap between rank and border rank, and thus a
counterexample to a conjecture of Rhodes. We also obtain a new lower bound on
the tensor rank of powers of the generalized W-state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05603</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05603</id><created>2015-04-21</created><updated>2015-11-30</updated><authors><author><keyname>Oesterheld</keyname><forenames>Caspar</forenames></author></authors><title>Formalizing Preference Utilitarianism in Physical World Models</title><categories>cs.CY cs.AI</categories><comments>14 pages, 3 figures</comments><doi>10.1007/s11229-015-0883-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most ethical work is done at a low level of formality. This makes practical
moral questions inaccessible to formal and natural sciences and can lead to
misunderstandings in ethical discussion. In this paper, we use Bayesian
inference to introduce a formalization of preference utilitarianism in physical
world models, specifically cellular automata. Even though our formalization is
not immediately applicable, it is a first step in providing ethics and
ultimately the question of how to &quot;make the world better&quot; with a formal basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05606</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05606</id><created>2015-04-21</created><authors><author><keyname>Teeti</keyname><forenames>Mohammed</forenames></author><author><keyname>Sun</keyname><forenames>Jun</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Liu</keyname><forenames>Yingzhuang</forenames></author></authors><title>The Impact of Physical Channel on Performance of Subspace-Based Channel
  Estimation in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figures</comments><journal-ref>Wireless Communications, IEEE Transactions on (Volume:14 , Issue:
  9 ) , pp. 4743 - 4756, 22 April 2015</journal-ref><doi>10.1109/TWC.2015.2425401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A subspace method for channel estimation has been recently proposed [1] for
tackling the pilot contamination effect, which is regarded by some researchers
as a bottleneck in massive MIMO systems. It was shown in [1] that if the power
ratio between the desired signal and interference is kept above a certain
value, the received signal spectrum splits into signal and interference
eigenvalues, namely, the &quot;pilot contamination&quot; effect can be completely
eliminated. However, [1] assumes an independently distributed (i.d.) channel,
which is actually not much the case in practice. Considering this, a more
sensible finite-dimensional physical channel model (i.e., a finite scattering
environment, where signals impinge on the base station (BS) from a finite
number of angles of arrival (AoA)) is employed in this paper. Via asymptotic
spectral analysis, it is demonstrated that, compared with the i.d. channel, the
physical channel imposes a penalty in the form of an increased power ratio
between the useful signal and the interference. Furthermore, we demonstrate an
interesting &quot;antenna saturation&quot; effect, i.e., when the number of the BS
antennas approaches infinity, the performance under the physical channel with P
AoAs is limited by and nearly the same as the performance under the i.d.
channel with P receive antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05616</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05616</id><created>2015-04-21</created><authors><author><keyname>Mokhtarinezhad</keyname><forenames>Farshid</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Lossy Compression with Privacy Constraints: Optimality of Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lossy source coding problem with privacy constraint is studied in which two
correlated discrete sources $X$ and $Y$ are compressed into a reconstruction
$\hat{X}$ with some prescribed distortion $D$. In addition, a privacy
constraint is specified as the equivocation between the lossy reconstruction
$\hat{X}$ and $Y$. This models the situation where a certain amount of source
information from one user is provided as utility (given by the fidelity of its
reconstruction) to another user or the public, while some other correlated part
of the source information $Y$ must be kept private. In this work, we show that
polar codes are able, possibly with the aid of time sharing, to achieve any
point in the optimal rate-distortion-equivocation region identified by
Yamamoto, thus providing a constructive scheme that obtains the optimal
tradeoff between utility and privacy in this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05618</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05618</id><created>2015-04-21</created><authors><author><keyname>Tripathy</keyname><forenames>Ardhendu</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Capacity of Sum-networks for Different Message Alphabets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sum-network is a directed acyclic network in which all terminal nodes
demand the `sum' of the independent information observed at the source nodes.
Many characteristics of the well-studied multiple-unicast network communication
problem also hold for sum-networks due to a known reduction between instances
of these two problems. Our main result is that unlike a multiple unicast
network, the coding capacity of a sum-network is dependent on the message
alphabet. We demonstrate this using a construction procedure and show that the
choice of a message alphabet can reduce the coding capacity of a sum-network
from $1$ to close to $0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05619</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05619</id><created>2015-04-21</created><authors><author><keyname>Tizhoosh</keyname><forenames>Hamid R.</forenames></author><author><keyname>Rahnamayan</keyname><forenames>Shahryar</forenames></author></authors><title>Learning Opposites with Evolving Rules</title><categories>cs.NE cs.LG</categories><comments>Accepted for publication in The 2015 IEEE International Conference on
  Fuzzy Systems (FUZZ-IEEE 2015), August 2-5, 2015, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of opposition-based learning was introduced 10 years ago. Since then
a noteworthy group of researchers has used some notions of oppositeness to
improve existing optimization and learning algorithms. Among others,
evolutionary algorithms, reinforcement agents, and neural networks have been
reportedly extended into their opposition-based version to become faster and/or
more accurate. However, most works still use a simple notion of opposites,
namely linear (or type- I) opposition, that for each $x\in[a,b]$ assigns its
opposite as $\breve{x}_I=a+b-x$. This, of course, is a very naive estimate of
the actual or true (non-linear) opposite $\breve{x}_{II}$, which has been
called type-II opposite in literature. In absence of any knowledge about a
function $y=f(\mathbf{x})$ that we need to approximate, there seems to be no
alternative to the naivety of type-I opposition if one intents to utilize
oppositional concepts. But the question is if we can receive some level of
accuracy increase and time savings by using the naive opposite estimate
$\breve{x}_I$ according to all reports in literature, what would we be able to
gain, in terms of even higher accuracies and more reduction in computational
complexity, if we would generate and employ true opposites? This work
introduces an approach to approximate type-II opposites using evolving fuzzy
rules when we first perform opposition mining. We show with multiple examples
that learning true opposites is possible when we mine the opposites from the
training data to subsequently approximate $\breve{x}_{II}=f(\mathbf{x},y)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05623</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05623</id><created>2015-04-21</created><authors><author><keyname>Greminger</keyname><forenames>Michael A.</forenames></author></authors><title>Median and Mode Ellipse Parameterization for Robust Contour Fitting</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems that require the parameterization of closed contours arise
frequently in computer vision applications. This article introduces a new curve
parameterization algorithm that is able to fit a closed curve to a set of
points while being robust to the presence of outliers and occlusions in the
data. This robustness property makes this algorithm applicable to computer
vision applications where misclassification of features may lead to outliers.
The algorithm starts by fitting ellipses to numerous five point subsets from
the source data. The closed curve is parameterized by determining the median
perimeter of the set of ellipses. The resulting curve is not an ellipse,
allowing arbitrary closed contours to be parameterized. The use of the modal
perimeter rather than the median perimeter is also explored. A detailed
comparison is made between the proposed curve fitting algorithm and existing
robust ellipse fitting algorithms. Finally, the utility of the algorithm for
computer vision applications is demonstrated through the parameterization of
the boundary of fuel droplets during combustion. The performance of the
proposed algorithm and the performance of existing algorithms are compared to a
ground truth segmentation of the fuel droplet images, which demonstrates
improved performance for both area quantification and edge deviation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05628</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05628</id><created>2015-04-21</created><authors><author><keyname>Sasaki</keyname><forenames>Hiroaki</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Uyematsu</keyname><forenames>Tomohiko</forenames></author></authors><title>Key Rate of the B92 Quantum Key Distribution Protocol with Finite Qubits</title><categories>quant-ph cs.IT math.IT</categories><comments>4 pages, 2 figures, IEEEtran.cls. Some overlap with arXiv:1301.5083.
  Accepted for presentation in 2015 IEEE International Symposium on Information
  Theory, Hong Kong, June 14-19, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key rate of the B92 quantum key distribution protocol had not been
reported before this research when the number of qubits is finite. We compute
it by using the security analysis framework proposed by Scarani and Renner in
2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05632</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05632</id><created>2015-04-21</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Wang</keyname><forenames>Zhaowen</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Han</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Self-Tuned Deep Super Resolution</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has been successfully applied to image super resolution (SR).
In this paper, we propose a deep joint super resolution (DJSR) model to exploit
both external and self similarities for SR. A Stacked Denoising Convolutional
Auto Encoder (SDCAE) is first pre-trained on external examples with proper data
augmentations. It is then fine-tuned with multi-scale self examples from each
input, where the reliability of self examples is explicitly taken into account.
We also enhance the model performance by sub-model training and selection. The
DJSR model is extensively evaluated and compared with state-of-the-arts, and
show noticeable performance improvements both quantitatively and perceptually
on a wide range of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05639</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05639</id><created>2015-04-21</created><updated>2015-11-12</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>Asymmetric quantum convolutional codes</title><categories>quant-ph cs.IT math.IT</categories><comments>Accepted for publication in Quantum Information Processing. arXiv
  admin note: text overlap with arXiv:1212.4654</comments><journal-ref>Quantum Information Processing January 2016, Vol 15, Issue 1, pp
  167--183</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct the first families of asymmetric quantum
convolutional codes (AQCC)'s. These new AQCC's are constructed by means of the
CSS-type construction applied to suitable families of classical convolutional
codes, which are also constructed here. The new codes have noncatastrophic
generator matrices and they present great asymmetry. Since our constructions
are performed algebraically, it is possible to derive several families of such
codes and not only codes with specifi?c parameters. Additionally, several
different types of such codes are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05646</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05646</id><created>2015-04-21</created><updated>2015-06-05</updated><authors><author><keyname>Halderman</keyname><forenames>J. Alex</forenames><affiliation>University of Michigan</affiliation></author><author><keyname>Teague</keyname><forenames>Vanessa</forenames><affiliation>University of Melbourne</affiliation></author></authors><title>The New South Wales iVote System: Security Failures and Verification
  Flaws in a Live Online Election</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the world's largest-ever deployment of online voting, the iVote Internet
voting system was trusted for the return of 280,000 ballots in the 2015 state
election in New South Wales, Australia. During the election, we performed an
independent security analysis of parts of the live iVote system and uncovered
severe vulnerabilities that could be leveraged to manipulate votes, violate
ballot privacy, and subvert the verification mechanism. These vulnerabilities
do not seem to have been detected by the election authorities before we
disclosed them, despite a pre-election security review and despite the system
having run in a live state election for five days. One vulnerability, the
result of including analytics software from an insecure external server,
exposed some votes to complete compromise of privacy and integrity. At least
one parliamentary seat was decided by a margin much smaller than the number of
votes taken while the system was vulnerable. We also found protocol flaws,
including vote verification that was itself susceptible to manipulation. This
incident underscores the difficulty of conducting secure elections online and
carries lessons for voters, election officials, and the e-voting research
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05647</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05647</id><created>2015-04-21</created><updated>2015-06-29</updated><authors><author><keyname>Aloraini</keyname><forenames>Bushra</forenames></author><author><keyname>Johnson</keyname><forenames>Daryl</forenames></author><author><keyname>Stackpole</keyname><forenames>Bill</forenames></author><author><keyname>Mishra</keyname><forenames>Sumita</forenames></author></authors><title>A New Covert Channel over Cellular Voice Channel in Smartphones</title><categories>cs.CR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Investigating network covert channels in smartphones has become increasingly
important as smartphones have recently replaced the role of traditional
computers. Smartphones are subject to traditional computer network covert
channel techniques. Smartphones also introduce new sets of covert channel
techniques as they add more capabilities and multiple network connections. This
work presents a new network covert channel in smartphones. The research studies
the ability to leak information from the smartphones applications by reaching
the cellular voice stream, and it examines the ability to employ the cellular
voice channel to be a potential medium of information leakage through carrying
modulated speech-like data covertly. To validate the theory, an Android
software audio modem has been developed and it was able to leak data
successfully through the cellular voice channel stream by carrying modulated
data with a throughput of 13 bps with 0.018% BER. Moreover, Android security
policies are investigated and broken in order to implement a user-mode rootkit
that opens the voice channels by stealthily answering an incoming voice call.
Multiple scenarios are conducted to verify the effectiveness of the proposed
covert channel. This study identifies a new potential smartphone covert
channel, and discusses some security vulnerabilities in Android OS that allow
the use of this channel demonstrating the need to set countermeasures against
this kind of breach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05651</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05651</id><created>2015-04-22</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Zhang</keyname><forenames>Jiji</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Distinguishing Cause from Effect Based on Exogeneity</title><categories>cs.AI stat.ME</categories><comments>11 pages, 4 figures, published in Proceedings of the 15th conference
  on Theoretical Aspects of Rationality and Knowledge (TARK'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in structural equation modeling have produced several
methods that can usually distinguish cause from effect in the two-variable
case. For that purpose, however, one has to impose substantial structural
constraints or smoothness assumptions on the functional causal models. In this
paper, we consider the problem of determining the causal direction from a
related but different point of view, and propose a new framework for causal
direction determination. We show that it is possible to perform causal
inference based on the condition that the cause is &quot;exogenous&quot; for the
parameters involved in the generating process from the cause to the effect. In
this way, we avoid the structural constraints required by the SEM-based
approaches. In particular, we exploit nonparametric methods to estimate
marginal and conditional distributions, and propose a bootstrap-based approach
to test for the exogeneity condition; the testing results indicate the causal
direction between two variables. The proposed method is validated on both
synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05653</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05653</id><created>2015-04-22</created><authors><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author><author><keyname>Meir</keyname><forenames>Or</forenames></author><author><keyname>Ron-Zewi</keyname><forenames>Noga</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>High rate locally-correctable and locally-testable codes with
  sub-polynomial query complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we construct the first locally-correctable codes (LCCs), and
locally-testable codes (LTCs) with constant rate, constant relative distance,
and sub-polynomial query complexity. Specifically, we show that there exist
binary LCCs and LTCs with block length $n$, constant rate (which can even be
taken arbitrarily close to 1), constant relative distance, and query complexity
$\exp(\tilde{O}(\sqrt{\log n}))$. Previously such codes were known to exist
only with $\Omega(n^{\beta})$ query complexity (for constant $\beta &gt; 0$), and
there were several, quite different, constructions known.
  Our codes are based on a general distance-amplification method of Alon and
Luby~\cite{AL96_codes}. We show that this method interacts well with local
correctors and testers, and obtain our main results by applying it to suitably
constructed LCCs and LTCs in the non-standard regime of \emph{sub-constant
relative distance}.
  Along the way, we also construct LCCs and LTCs over large alphabets, with the
same query complexity $\exp(\tilde{O}(\sqrt{\log n}))$, which additionally have
the property of approaching the Singleton bound: they have almost the
best-possible relationship between their rate and distance. This has the
surprising consequence that asking for a large alphabet error-correcting code
to further be an LCC or LTC with $\exp(\tilde{O}(\sqrt{\log n}))$ query
complexity does not require any sacrifice in terms of rate and distance! Such a
result was previously not known for any $o(n)$ query complexity.
  Our results on LCCs also immediately give locally-decodable codes (LDCs) with
the same parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05655</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05655</id><created>2015-04-22</created><authors><author><keyname>Kurka</keyname><forenames>David Burth</forenames></author><author><keyname>Godoy</keyname><forenames>Alan</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author></authors><title>Online Social Network Analysis: A Survey of Research Applications in
  Computer Science</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence and popularization of online social networks suddenly made
available a large amount of data from social organization, interaction and
human behaviour. All this information opens new perspectives and challenges to
the study of social systems, being of interest to many fields. Although most
online social networks are recent (less than fifteen years old), a vast amount
of scientific papers was already published on this topic, dealing with a broad
range of analytical methods and applications. This work describes how
computational researches have approached this subject and the methods used to
analyse such systems. Founded on a wide though non-exaustive review of the
literature, a taxonomy is proposed to classify and describe different
categories of research. Each research category is described and the main works,
discoveries and perspectives are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05657</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05657</id><created>2015-04-22</created><updated>2015-05-04</updated><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>Low-Complexity CFO Estimation for Multi-User Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-complexity carrier frequency offset (CFO) estimation and compensation in
multi-user massive multiple-input multiple-output (MIMO) systems is a
challenging problem. The existing CFO estimation algorithms incur tremendous
increase in complexity with increasing number of base station (BS) antennas,
$M$ and number of user terminals (UTs) $K$ (i.e. massive MIMO regime). In this
paper, we address this problem by proposing a novel low-complexity algorithm
for CFO estimation which uses the pilot signal received at the BS during
special uplink slots. The total per-channel use complexity of the proposed
algorithm increases only linearly with increasing $M$ and is independent of
$K$. Analysis reveals that the CFO estimation accuracy can be considerably
improved by increasing $M$ and $K$ (i.e. massive MIMO regime). For example, for
a fixed $K$ and a fixed training length, the required per-user radiated power
during uplink training decreases as $\frac{1}{\sqrt{M}}$ with increasing $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05662</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05662</id><created>2015-04-22</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Weakly Secure MDS Codes for Simple Multiple Access Networks</title><categories>cs.IT math.CO math.IT</categories><comments>Accepted at ISIT'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple multiple access network (SMAN), where $k$ sources of
unit rates transmit their data to a common sink via $n$ relays. Each relay is
connected to the sink and to certain sources. A coding scheme (for the relays)
is weakly secure if a passive adversary who eavesdrops on less than $k$
relay-sink links cannot reconstruct the data from each source. We show that
there exists a weakly secure maximum distance separable (MDS) coding scheme for
the relays if and only if every subset of $\ell$ relays must be collectively
connected to at least $\ell+1$ sources, for all $0 &lt; \ell &lt; k$. Moreover, we
prove that this condition can be verified in polynomial time in $n$ and $k$.
Finally, given a SMAN satisfying the aforementioned condition, we provide
another polynomial time algorithm to trim the network until it has a sparsest
set of source-relay links that still supports a weakly secure MDS coding
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05663</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05663</id><created>2015-04-22</created><updated>2015-08-11</updated><authors><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Chen</keyname><forenames>Erkai</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Content-Centric Multicast Beamforming in Cache-Enabled Cloud Radio
  Access Networks</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast transmission and wireless caching are effective ways of reducing
air and backhaul traffic load in wireless networks. This paper proposes to
incorporate these two key ideas for content-centric multicast transmission in a
cloud radio access network (RAN) where multiple base stations (BSs) are
connected to a central processor (CP) via finite-capacity backhaul links. Each
BS has a cache with finite storage size and is equipped with multiple antennas.
The BSs cooperatively transmit contents, which are either stored in the local
cache or fetched from the CP, to multiple users in the network. Users
requesting a same content form a multicast group and are served by a same
cluster of BSs cooperatively using multicast beamforming. Assuming fixed cache
placement, this paper investigates the joint design of multicast beamforming
and content-centric BS clustering by formulating an optimization problem of
minimizing the total network cost under the quality-of-service (QoS)
constraints for each multicast group. The network cost involves both the
transmission power and the backhaul cost. We model the backhaul cost using the
mixed $\ell_0/\ell_2$-norm of beamforming vectors. To solve this non-convex
problem, we first approximate it using the semidefinite relaxation (SDR) method
and concave smooth functions. We then propose a difference of convex functions
(DC) programming algorithm to obtain suboptimal solutions and show the
connection of three smooth functions. Simulation results validate the advantage
of multicasting and show the effects of different cache size and caching
policies in cloud RAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05665</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05665</id><created>2015-04-22</created><authors><author><keyname>Hayashi</keyname><forenames>Kohei</forenames></author><author><keyname>Maeda</keyname><forenames>Shin-ichi</forenames></author><author><keyname>Fujimaki</keyname><forenames>Ryohei</forenames></author></authors><title>Rebuilding Factorized Information Criterion: Asymptotically Accurate
  Marginal Likelihood</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factorized information criterion (FIC) is a recently developed approximation
technique for the marginal log-likelihood, which provides an automatic model
selection framework for a few latent variable models (LVMs) with tractable
inference algorithms. This paper reconsiders FIC and fills theoretical gaps of
previous FIC studies. First, we reveal the core idea of FIC that allows
generalization for a broader class of LVMs, including continuous LVMs, in
contrast to previous FICs, which are applicable only to binary LVMs. Second, we
investigate the model selection mechanism of the generalized FIC. Our analysis
provides a formal justification of FIC as a model selection criterion for LVMs
and also a systematic procedure for pruning redundant latent variables that
have been removed heuristically in previous studies. Third, we provide an
interpretation of FIC as a variational free energy and uncover a few
previously-unknown their relationships. A demonstrative study on Bayesian
principal component analysis is provided and numerical experiments support our
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05666</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05666</id><created>2015-04-22</created><updated>2016-01-29</updated><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Venkatakrishnan</keyname><forenames>Shaileshh</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Information Complexity Density and Simulation of Protocols</title><categories>cs.IT cs.CC math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two parties observing correlated random variables seek to run an interactive
communication protocol. How many bits must they exchange to simulate the
protocol, namely to produce a view with a joint distribution within a fixed
statistical distance of the joint distribution of the input and the transcript
of the original protocol? We present an information spectrum approach for this
problem whereby the information complexity of the protocol is replaced by its
information complexity density. Our single-shot bounds relate the communication
complexity of simulating a protocol to tail bounds for information complexity
density. As a consequence, we obtain a strong converse and characterize the
second-order asymptotic term in communication complexity for indepedent and
identically distributed observation sequences. Furthermore, we obtain a general
formula for the rate of communication complexity which applies to any sequence
of observations and protocols. Connections with results from theoretical
computer science and implications for the function computation problem are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05670</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05670</id><created>2015-04-22</created><updated>2015-09-28</updated><authors><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Anastasopoulos</keyname><forenames>Achilleas</forenames></author></authors><title>Mechanism Design for Fair Allocation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanism design for a social utility being the sum of agents' utilities
(SoU) is a well-studied problem. There are, however, a number of problems of
theoretical and practical interest where a designer may have a different
objective than maximization of the SoU. One motivation for this is the desire
for more equitable allocation of resources among agents. A second, more subtle,
motivation is the fact that a fairer allocation indirectly implies less
variation in taxes which can be desirable in a situation where (implicit)
individual agent budgetary constraints make payment of large taxes unrealistic.
In this paper we study a family of social utilities that provide fair
allocation (with SoU being subsumed as an extreme case) and derive conditions
under which Bayesian and Dominant strategy implementation is possible.
Furthermore, it is shown how a simple modification of the above mechanism can
guarantee full Bayesian implementation. Through a numerical example it is shown
that the proposed method can result in significant gains both in allocation
fairness and tax reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05679</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05679</id><created>2015-04-22</created><authors><author><keyname>Lin</keyname><forenames>Shih-Chun</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author></authors><title>On Two-Pair Two-Way Relay Channel with an Intermittently Available Relay</title><categories>cs.IT math.IT</categories><comments>extended version of ISIT 2015 paper</comments><doi>10.1109/ISIT.2015.7282786</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple users share the same resource for physical layer cooperation
such as relay terminals in their vicinities, this shared resource may not be
always available for every user, and it is critical for transmitting terminals
to know whether other users have access to that common resource in order to
better utilize it. Failing to learn this critical piece of information may
cause severe issues in the design of such cooperative systems. In this paper,
we address this problem by investigating a two-pair two-way relay channel with
an intermittently available relay. In the model, each pair of users need to
exchange their messages within their own pair via the shared relay. The shared
relay, however, is only intermittently available for the users to access. The
accessing activities of different pairs of users are governed by independent
Bernoulli random processes. Our main contribution is the characterization of
the capacity region to within a bounded gap in a symmetric setting, for both
delayed and instantaneous state information at transmitters. An interesting
observation is that the bottleneck for information flow is the quality of state
information (delayed or instantaneous) available at the relay, not those at the
end users. To the best of our knowledge, our work is the first result regarding
how the shared intermittent relay should cooperate with multiple pairs of users
in such a two-way cooperative network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05692</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05692</id><created>2015-04-22</created><authors><author><keyname>Hadzieva</keyname><forenames>Elena</forenames></author><author><keyname>Simevski</keyname><forenames>Aleksandar</forenames></author></authors><title>Theoretical Aspects of a Design Method for Programmable NMR Voters</title><categories>cs.DC</categories><comments>11 pages, 3 figures</comments><msc-class>00A69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all dependable systems use some form of redundancy in order to
increase fault-tolerance. Very popular are the $N$-Modular Redundant (NMR)
systems in which a majority voter chooses the voting output. However, elaborate
systems require fault-tolerant voters which further give additional information
besides the voting output, e.g., how many module outputs agree. Dynamically
defining which set of inputs should be considered for voting is also crucial.
Earlier we showed a practical implementation of programmable NMR voters that
self-report the voting outcome and do self-checks. Our voter design method uses
a binary matrix with specific properties that enable easy scaling of the design
regarding the number of voter inputs N. Thus, an automated construction of NMR
systems is possible, given the basic module and arbitrary redundancy $N$. In
this paper we present the mathematical aspects of the method, i.e., we analyze
the properties of the matrix that characterizes the method. We give the
characteristic polynomials of the properly and erroneously built matrices in
their explicit forms. We further give their eigenvalues and corresponding
eigenvectors, which reveal a lot of useful information about the system. At the
end, we give relations between the voter outputs and eigenpairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05694</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05694</id><created>2015-04-22</created><authors><author><keyname>Lee</keyname><forenames>Linda</forenames></author><author><keyname>Egelman</keyname><forenames>Serge</forenames></author><author><keyname>Lee</keyname><forenames>Joong Hwa</forenames></author><author><keyname>Wagner</keyname><forenames>David</forenames></author></authors><title>Risk Perceptions for Wearable Devices</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable devices, or &quot;wearables,&quot; bring great benefits but also potential
risks that could expose users' activities with- out their awareness or consent.
In this paper, we report findings from the first large-scale survey conducted
to investigate user security and privacy concerns regarding wearables. We
surveyed 1,782 Internet users in order to identify risks that are particularly
concerning to them; these risks are inspired by the sensor inputs and
applications of popular wearable technologies. During this experiment, our
questions controlled for the effects of what data was being accessed and with
whom it was being shared. We also investigated how these emergent threats
compared to existent mobile threats, how upcoming capabilities and artifacts
compared to existing technologies, and how users ranked technical and
nontechnical concerns to sketch a concrete and broad view of the wearable
device landscape. We hope that this work will inform the design of future user
notification, permission management, and access control schemes for wearables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05696</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05696</id><created>2015-04-22</created><updated>2015-09-05</updated><authors><author><keyname>Shanahan</keyname><forenames>Murray</forenames></author></authors><title>Ascribing Consciousness to Artificial Intelligence</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper critically assesses the anti-functionalist stance on consciousness
adopted by certain advocates of integrated information theory (IIT), a
corollary of which is that human-level artificial intelligence implemented on
conventional computing hardware is necessarily not conscious. The critique
draws on variations of a well-known gradual neuronal replacement thought
experiment, as well as bringing out tensions in IIT's treatment of
self-knowledge. The aim, though, is neither to reject IIT outright nor to
champion functionalism in particular. Rather, it is suggested that both ideas
have something to offer a scientific understanding of consciousness, as long as
they are not dressed up as solutions to illusory metaphysical problems. As for
human-level AI, we must await its development before we can decide whether or
not to ascribe consciousness to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05705</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05705</id><created>2015-04-22</created><authors><author><keyname>Achdou</keyname><forenames>Yves</forenames><affiliation>LJLL</affiliation></author><author><keyname>Porretta</keyname><forenames>Alessio</forenames><affiliation>DIPMAT</affiliation></author></authors><title>Convergence of a finite difference scheme to weak solutions of the
  system of partial differential equation arising in mean field games</title><categories>cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean field type models describing the limiting behavior of stochastic
differential games as the number of players tends to +$\infty$, have been
recently introduced by J-M. Lasry and P-L. Lions. Under suitable assumptions,
they lead to a system of two coupled partial differential equations, a forward
Bellman equation and a backward Fokker-Planck equations. Finite difference
schemes for the approximation of such systems have been proposed in previous
works. Here, we prove the convergence of these schemes towards a weak solution
of the system of partial differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05707</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05707</id><created>2015-04-22</created><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author></authors><title>Towards More Data-Aware Application Integration (extended version)</title><categories>cs.DB</categories><comments>18 Pages, extended version of the contribution to British
  International Conference on Databases (BICOD), 2015, Edinburgh, Scotland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although most business application data is stored in relational databases,
programming languages and wire formats in integration middleware systems are
not table-centric. Due to costly format conversions, data-shipments and faster
computation, the trend is to &quot;push-down&quot; the integration operations closer to
the storage representation.
  We address the alternative case of de?ning declarative, table-centric
integration semantics within standard integration systems. For that, we replace
the current operator implementations for the well-known Enterprise Integration
Patterns by equivalent &quot;in-memory&quot; table processing, and show a practical
realization in a conventional integration system for a non-reliable,
&quot;data-intensive&quot; messaging example. The results of the runtime analysis show
that table-centric processing is promising already in standard, &quot;single-record&quot;
message routing and transformations, and can potentially excel the message
throughput for &quot;multi-record&quot; table messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05723</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05723</id><created>2015-04-22</created><authors><author><keyname>Saha</keyname><forenames>Saikat</forenames></author></authors><title>Noise Robust Online Inference for Linear Dynamic Systems</title><categories>stat.CO cs.RO cs.SY q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the Bayesian online inference problems for the linear dynamic
systems (LDS) under non- Gaussian environment. The noises can naturally be
non-Gaussian (skewed and/or heavy tailed) or to accommodate spurious
observations, noises can be modeled as heavy tailed. However, at the cost of
such noise robustness, the performance may degrade when such spurious
observations are absent. Therefore, any inference engine should not only be
robust to noise outlier, but also be adaptive to potentially unknown and time
varying noise parameters; yet it should be scalable and easy to implement.
  To address them, we envisage here a new noise adaptive Rao-Blackwellized
particle filter (RBPF), by leveraging a hierarchically Gaussian model as a
proxy for any non-Gaussian (process or measurement) noise density. This leads
to a conditionally linear Gaussian model (CLGM), that is tractable. However,
this framework requires a valid transition kernel for the intractable state,
targeted by the particle filter (PF). This is typically unknown. We outline how
such kernel can be constructed provably, at least for certain classes
encompassing many commonly occurring non-Gaussian noises, using auxiliary
latent variable approach. The efficacy of this RBPF algorithm is demonstrated
through numerical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05727</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05727</id><created>2015-04-22</created><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>ZhenYang</keyname></author></authors><title>Coding vs. Spreading for Narrow-Band Interference Suppression</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, accepted for publication in IEEE Trans. Vech.
  Tech</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of active narrow-band interference (NBI) suppression in
direct-sequence spread-spectrum (DS-SS) communications has been extensively
studied. In this paper, we address the problem of optimum coding-spreading
tradeoff for NBI suppression. With maximum likelihood decoding, we first derive
upper bounds on the error probability of coded systems in the presence of a
special class of NBI, namely, multi-tone interference with orthogonal
signatures. By employing the well-developed bounding techniques, we show there
is no advantage in spreading, and hence a low-rate full coding approach is
always preferred. Then, we propose a practical low-rate turbo-Hadamard coding
approach, in which the NBI suppression is naturally achieved through iterative
decoding. The proposed turbo-Hadamard coding approach employs a kind of coded
spread-spectrum signalling with time-varying spreading sequences, which is
sharply compared with the code-aided DS-SS approach. With a spreading sequence
of length 32 and a fixed bandwidth allocated for both approaches, it is shown
through extensive simulations that the proposed turbo-Hadamard coding approach
outperforms the code-aided DS-SS approach for three types of NBI, even when its
transmission information rate is about 5 times higher than that of the
code-aided DS-SS approach. The use of the proposed turbo-Hadmard coding
approach in multipath fading channels is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05739</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05739</id><created>2015-04-22</created><updated>2016-03-03</updated><authors><author><keyname>Daca</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Petrov</keyname><forenames>Tatjana</forenames></author></authors><title>Faster Statistical Model Checking for Unbounded Temporal Properties</title><categories>cs.LO</categories><comments>Published in the proceedings of 22nd International Conference on
  Tools and Algorithms for the Construction and Analysis of Systems (TACAS),
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for the statistical model checking of Markov
chains with respect to unbounded temporal properties, such as reachability and
full linear temporal logic. The main idea is that we monitor each simulation
run on the fly, in order to detect quickly if a bottom strongly connected
component is entered with high probability, in which case the simulation run
can be terminated early. As a result, our simulation runs are often much
shorter than required by termination bounds that are computed a priori for a
desired level of confidence and size of the state space. In comparison to
previous algorithms for statistical model checking, for a given level of
confidence, our method is not only faster in many cases but also requires less
information about the system, namely, only the minimum transition probability
that occurs in the Markov chain, thus enabling almost complete black-box
verification. In addition, our method can be generalised to unbounded
quantitative properties such as mean-payoff bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05740</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05740</id><created>2015-04-22</created><authors><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Yucovich</keyname><forenames>Alexander</forenames></author><author><keyname>Maor</keyname><forenames>Gal</forenames></author><author><keyname>Yadgar</keyname><forenames>Gala</forenames></author></authors><title>When Do WOM Codes Improve the Erasure Factor in Flash Memories?</title><categories>cs.IT math.IT</categories><comments>to be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flash memory is a write-once medium in which reprogramming cells requires
first erasing the block that contains them. The lifetime of the flash is a
function of the number of block erasures and can be as small as several
thousands. To reduce the number of block erasures, pages, which are the
smallest write unit, are rewritten out-of-place in the memory. A Write-once
memory (WOM) code is a coding scheme which enables to write multiple times to
the block before an erasure. However, these codes come with significant rate
loss. For example, the rate for writing twice (with the same rate) is at most
0.77.
  In this paper, we study WOM codes and their tradeoff between rate loss and
reduction in the number of block erasures, when pages are written uniformly at
random. First, we introduce a new measure, called erasure factor, that reflects
both the number of block erasures and the amount of data that can be written on
each block. A key point in our analysis is that this tradeoff depends upon the
specific implementation of WOM codes in the memory. We consider two systems
that use WOM codes; a conventional scheme that was commonly used, and a new
recent design that preserves the overall storage capacity. While the first
system can improve the erasure factor only when the storage rate is at most
0.6442, we show that the second scheme always improves this figure of merit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05743</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05743</id><created>2015-04-22</created><authors><author><keyname>Lawyer</keyname><forenames>Glenn</forenames></author></authors><title>Stochasticity in pandemic spread over the World Airline Network
  explained by local flight connections</title><categories>cs.SI physics.soc-ph</categories><comments>article text: 6 pages, 5 figures, 28 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive growth in human mobility has dramatically increased the risk and rate
of pandemic spread. Macro-level descriptors of the topology of the World
Airline Network (WAN) explains middle and late stage dynamics of pandemic
spread mediated by this network, but necessarily regard early stage variation
as stochastic. We propose that much of early stage variation can be explained
by appropriately characterizing the local topology surrounding the debut
location of an outbreak. We measure for each airport the expected force of
infection (AEF) which a pandemic originating at that airport would generate. We
observe, for a subset of world airports, the minimum transmission rate at which
a disease becomes pandemically competent at each airport. We also observe, for
a larger subset, the time until a pandemically competent outbreak achieves
pandemic status given its debut location. Observations are generated using a
highly sophisticated metapopulation reaction-diffusion simulator under a
disease model known to well replicate the 2009 influenza pandemic. The
robustness of the AEF measure to model misspecification is examined by
degrading the network model. AEF powerfully explains pandemic risk, showing
correlation of 0.90 to the transmission level needed to give a disease pandemic
competence, and correlation of 0.85 to the delay until an outbreak becomes a
pandemic. The AEF is robust to model misspecification. For 97% of airports,
removing 15% of airports from the model changes their AEF metric by less than
1%. Appropriately summarizing the size, shape, and diversity of an airport's
local neighborhood in the WAN accurately explains much of the macro-level
stochasticity in pandemic outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05750</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05750</id><created>2015-04-22</created><updated>2015-12-06</updated><authors><author><keyname>Fusaroli</keyname><forenames>Riccardo</forenames></author><author><keyname>Bj&#xf8;rndahl</keyname><forenames>Johanne S.</forenames></author><author><keyname>Roepstorff</keyname><forenames>Andreas</forenames></author><author><keyname>Tyl&#xe9;n</keyname><forenames>Kristian</forenames></author></authors><title>A Heart for Interaction: Shared Physiological Dynamics and Behavioral
  Coordination in a Collective, Creative Construction Task</title><categories>physics.soc-ph cs.MA nlin.AO</categories><comments>34 pages, 6 figures, 7 tables. Minor revisions due to peer review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpersonally shared physiological dynamics are increasingly argued to
underlie rapport, empathy and even team performance. Inspired by the model of
interpersonal synergy, we critically investigate the presence, temporal
development, possible mechanisms and impact of shared interpersonal heart rate
dynamics during individual and collective creative LEGO construction tasks. In
Study 1 we show how shared HR dynamics are driven by a plurality of sources
including task constraints and behavioral coordination. Generally, shared HR
dynamics are more prevalent in individual trials (involving participants doing
the same things) than in collective ones (involving participants taking turns
and performing complementary actions). However, when contrasted against virtual
pairs, collective trials display more stable shared HR dynamics suggesting that
online social interaction plays an important role. Furthermore, in contrast to
individual trials, shared HR dynamics are found to increase across collective
trials. Study 2 investigates which aspects of social interaction might drive
these effects. We show that shared HR dynamics are statistically predicted by
interpersonal speech and building coordination. In Study 3, we explore the
relation between HR dynamics, behavioral coordination, and self-reported
measures of rapport and group competence. While behavioral coordination
predicts rapport and group competence, shared HR dynamics do not. Although
shared physiological dynamics were reliably observed in our study, our results
warrant not to consider HR dynamics a general driving mechanism of social
coordination. Behavioral coordination - on the other hand - seems to be more
informative of both shared physiological dynamics and collective experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05756</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05756</id><created>2015-04-22</created><authors><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>A Large Deviations Approach to Secure Lossy Compression</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Shannon cipher system for memoryless sources, in which
distortion is allowed at the legitimate decoder. The source is compressed using
a rate distortion code secured by a shared key, which satisfies a constraint on
the compression rate, as well as a constraint on the exponential rate of the
excess-distortion probability at the legitimate decoder. Secrecy is measured by
the exponential rate of the exiguous-distortion probability at the
eavesdropper, rather than by the traditional measure of equivocation. We define
the perfect secrecy exponent as the maximal exiguous-distortion exponent
achievable when the key rate is unlimited. Under limited key rate, we prove
that the maximal achievable exiguous-distortion exponent is equal to the
minimum between the average key rate and the perfect secrecy exponent, for a
fairly general class of variable key rate codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05764</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05764</id><created>2015-04-22</created><authors><author><keyname>Moreno-Pozas</keyname><forenames>Laureano</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author><author><keyname>Paris</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Martos-Naya</keyname><forenames>Eduardo</forenames></author></authors><title>The $\kappa$-$\mu$ Shadowed Fading Model: Unifying the $\kappa$-$\mu$
  and $\eta$-$\mu$ Distributions</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the recently proposed $\kappa$-$\mu$ shadowed fading
model includes, besides the $\kappa$-$\mu$ model, the $\eta$-$\mu$ fading model
as a particular case. This has important relevance in practice, as it allows
for the unification of these popular fading distributions through a more
general, yet equally tractable, model. The convenience of new underlying
physical models is discussed. Then, we derive simple and novel closed-form
expressions for the asymptotic ergodic capacity in $\kappa$-$\mu$ shadowed
fading channels, which illustrate the effects of the different fading
parameters on the system performance. By exploiting the unification here
unveiled, the asymptotic capacity expressions for the $\kappa$-$\mu$ and
$\eta$-$\mu$ fading models are also obtained in closed-form as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05766</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05766</id><created>2015-04-22</created><authors><author><keyname>D&#xfc;&#x11f;enci</keyname><forenames>Muharrem</forenames></author></authors><title>Honeybees-inspired heuristic algorithms for numerical optimisation</title><categories>cs.NE</categories><comments>17 pages, 3 Figures, 6 Tables</comments><report-no>KU-IE-MD-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm intelligence is all about developing collective behaviours to solve
complex, ill-structured and large-scale problems. Efficiency in collective
behaviours depends on how to harmonise the individual contributions so that a
complementary collective effort can be achieved to offer a useful solution. The
main points in organising the harmony remains as managing the diversification
and intensification actions appropriately, where the efficiency of collective
behaviours depends on blending these two actions appropriately. In this study,
two swarm intelligence algorithms inspired of natural honeybee colonies have
been overviewed with many respects and two new revisions and a hybrid version
have been studied to improve the efficiencies in solving numerical optimisation
problems, which are well-known hard benchmarks. Consequently, the revisions and
especially the hybrid algorithm proposed have outperformed the two original bee
algorithms in solving these very hard numerical optimisation benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05767</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05767</id><created>2015-04-22</created><authors><author><keyname>Muller</keyname><forenames>Lorenz K.</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author></authors><title>Rounding Methods for Neural Networks with Low Resolution Synaptic
  Weights</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network algorithms simulated on standard computing platforms typically
make use of high resolution weights, with floating-point notation. However, for
dedicated hardware implementations of such algorithms, fixed-point synaptic
weights with low resolution are preferable. The basic approach of reducing the
resolution of the weights in these algorithms by standard rounding methods
incurs drastic losses in performance. To reduce the resolution further, in the
extreme case even to binary weights, more advanced techniques are necessary. To
this end, we propose two methods for mapping neural network algorithms with
high resolution weights to corresponding algorithms that work with low
resolution weights and demonstrate that their performance is substantially
better than standard rounding. We further use these methods to investigate the
performance of three common neural network algorithms under fixed memory size
of the weight matrix with different weight resolutions. We show that dedicated
hardware systems, whose technology dictates very low weight resolutions (be
they electronic or biological) could in principle implement the algorithms we
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05770</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05770</id><created>2015-04-22</created><authors><author><keyname>Nishimura</keyname><forenames>Ryota</forenames></author><author><keyname>Wada</keyname><forenames>Takahiro</forenames></author><author><keyname>Sugiyama</keyname><forenames>Seiji</forenames></author></authors><title>Haptic Shared Control in Steering Operation Based on Cooperative Status
  Between a Driver and a Driver Assistance System</title><categories>cs.RO</categories><comments>Accepted for International Journal of Human Robot Interaction</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Haptic shared control is expected to achieve a smooth collaboration between
humans and automated systems, because haptics facilitate mutual communication.
A methodology for sharing a given task is important to achieve effective shared
control. Therefore, the appropriate cooperative relationship between a human
operator and automated system should be considered. This paper proposes a
methodology to evaluate the cooperative status between the operator and the
automated system in the haptic shared control of a steering operation using a
pseudo-power pair of torque from each agent and the vehicle lateral velocity as
each agent's contribution to vehicle motion. This method allows us to estimate
cooperative status based on two axes: the initiative holder and the intent
consistency between the two agents. A control method for a lane-keeping assist
system (LKAS) that enables drivers to change lanes smoothly is proposed based
on the estimated cooperative status. A gain-tuning control method based on the
estimated cooperative status is proposed to decrease the assistance system's
pseudo-power when intent inconsistency occurs. A method for switching the
followed lane to match the driver's and assistance system's intentions is also
proposed. A user study using a driving simulator is conducted to demonstrate
the effectiveness of the proposed methods. The results demonstrate that the
proposed methods facilitate smooth driver-initiated lane changes without
significantly affecting the driver's torque or steering wheel angle while
significantly improve lane-keeping performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05773</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05773</id><created>2015-04-22</created><updated>2015-07-17</updated><authors><author><keyname>Enright</keyname><forenames>Jessica</forenames></author><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>Deleting edges to restrict the size of an epidemic</title><categories>cs.DS math.CO</categories><comments>Extra literature references; new figure illustrating treewidth of
  real networks; small improvement to running time of algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in network epidemiology, we consider the problem of
determining whether it is possible to delete at most $k$ edges from a given
input graph (of small treewidth) so that the maximum component size in the
resulting graph is at most $h$. While this problem is NP-complete in general,
we provide evidence that many of the real-world networks of interest are likely
to have small treewidth, and we describe an algorithm which solves the problem
in time $O((wh)^{2w}n)$ on an input graph having $n$ vertices and whose
treewidth is bounded by a fixed constant $w$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05776</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05776</id><created>2015-04-22</created><updated>2016-02-13</updated><authors><author><keyname>Pustelnik</keyname><forenames>Nelly</forenames></author><author><keyname>Wendt</keyname><forenames>Herwig</forenames></author><author><keyname>Abry</keyname><forenames>Patrice</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author></authors><title>Combining local regularity estimation and total variation optimization
  for scale-free texture segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture segmentation constitutes a standard image processing task, crucial to
many applications. The present contribution focuses on the particular subset of
scale-free textures and its originality resides in the combination of three key
ingredients: First, texture characterization relies on the concept of local
regularity ; Second, estimation of local regularity is based on new multiscale
quantities referred to as wavelet leaders ; Third, segmentation from local
regularity faces a fundamental bias variance trade-off: In nature, local
regularity estimation shows high variability that impairs the detection of
changes, while a posteriori smoothing of regularity estimates precludes from
locating correctly changes. Instead, the present contribution proposes several
variational problem formulations based on total variation and proximal
resolutions that effectively circumvent this trade-off. Estimation and
segmentation performance for the proposed procedures are quantified and
compared on synthetic as well as on real-world textures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05782</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05782</id><created>2015-04-21</created><authors><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>Ensembles based on the Rich-Club and how to use them to build
  soft-communities</title><categories>cs.SI physics.data-an</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensembles of networks are used as null-models to discriminate network
structures. We present an efficient algorithm, based on the maximal entropy
method to generate network ensembles defined by the degree sequence and the
rich-club coefficient. The method is applicable for unweighted, undirected
networks. The ensembles are used to generate correlated and uncorrelated
null--models of a real networks. These ensembles can be used to define the
partition of a network into soft communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05793</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05793</id><created>2015-04-22</created><updated>2015-05-02</updated><authors><author><keyname>Ar&#x131;kan</keyname><forenames>Erdal</forenames></author></authors><title>A Packing Lemma for Polar Codes</title><categories>cs.IT math.IT</categories><comments>5 pages. To be presented at 2015 IEEE International Symposium on
  Information Theory, June 14-19, 2015, Hong Kong. Minor corrections to v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A packing lemma is proved using a setting where the channel is a binary-input
discrete memoryless channel $(\mathcal{X},w(y|x),\mathcal{Y})$, the code is
selected at random subject to parity-check constraints, and the decoder is a
joint typicality decoder. The ensemble is characterized by (i) a pair of fixed
parameters $(H,q)$ where $H$ is a parity-check matrix and $q$ is a channel
input distribution and (ii) a random parameter $S$ representing the desired
parity values. For a code of length $n$, the constraint is sampled from $p_S(s)
= \sum_{x^n\in {\mathcal{X}}^n} \phi(s,x^n)q^n(x^n)$ where $\phi(s,x^n)$ is the
indicator function of event $\{s = x^n H^T\}$ and $q^n(x^n) =
\prod_{i=1}^nq(x_i)$. Given $S=s$, the codewords are chosen conditionally
independently from $p_{X^n|S}(x^n|s) \propto \phi(s,x^n) q^n(x^n)$. It is shown
that the probability of error for this ensemble decreases exponentially in $n$
provided the rate $R$ is kept bounded away from $I(X;Y)-\frac{1}{n}I(S;Y^n)$
with $(X,Y)\sim q(x)w(y|x)$ and $(S,Y^n)\sim p_S(s)\sum_{x^n} p_{X^n|S}(x^n|s)
\prod_{i=1}^{n} w(y_i|x_i)$. In the special case where $H$ is the parity-check
matrix of a standard polar code, it is shown that the rate penalty
$\frac{1}{n}I(S;Y^n)$ vanishes as $n$ increases. The paper also discusses the
relation between ordinary polar codes and random codes based on polar
parity-check matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05797</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05797</id><created>2015-04-22</created><updated>2015-09-02</updated><authors><author><keyname>Tutu</keyname><forenames>Ionut</forenames><affiliation>Department of Computer Science, Royal Holloway University of London</affiliation></author><author><keyname>Fiadeiro</keyname><forenames>Jose Luiz</forenames><affiliation>Department of Computer Science, Royal Holloway University of London</affiliation></author></authors><title>Service-Oriented Logic Programming</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:3) 2015</journal-ref><doi>10.2168/LMCS-11(3:3)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop formal foundations for notions and mechanisms needed to support
service-oriented computing. Our work builds on recent theoretical advancements
in the algebraic structures that capture the way services are orchestrated and
in the processes that formalize the discovery and binding of services to given
client applications by means of logical representations of required and
provided services. We show how the denotational and the operational semantics
specific to conventional logic programming can be generalized using the theory
of institutions to address both static and dynamic aspects of service-oriented
computing. Our results rely upon a strong analogy between the discovery of a
service that can be bound to an application and the search for a clause that
can be used for computing an answer to a query; they explore the manner in
which requests for external services can be described as service queries, and
explain how the computation of their answers can be performed through
service-oriented derivatives of unification and resolution, which characterize
the binding of services and the reconfiguration of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05800</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05800</id><created>2015-04-22</created><updated>2015-11-09</updated><authors><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author></authors><title>On the Generalization Properties of Differential Privacy</title><categories>cs.LG cs.CR</categories><comments>This paper was merged with another manuscript and is now subsumed by
  arXiv:1511.02513</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new line of work, started with Dwork et al., studies the task of answering
statistical queries using a sample and relates the problem to the concept of
differential privacy. By the Hoeffding bound, a sample of size $O(\log
k/\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\alpha$,
where the answers are computed by evaluating the statistical queries on the
sample. This argument fails when the queries are chosen adaptively (and can
hence depend on the sample). Dwork et al. showed that if the answers are
computed with $(\epsilon,\delta)$-differential privacy then $O(\epsilon)$
accuracy is guaranteed with probability $1-O(\delta^\epsilon)$. Using the
Private Multiplicative Weights mechanism, they concluded that the sample size
can still grow polylogarithmically with the $k$.
  Very recently, Bassily et al. presented an improved bound and showed that (a
variant of) the private multiplicative weights algorithm can answer $k$
adaptively chosen statistical queries using sample complexity that grows
logarithmically in $k$. However, their results no longer hold for every
differentially private algorithm, and require modifying the private
multiplicative weights algorithm in order to obtain their high probability
bounds.
  We greatly simplify the results of Dwork et al. and improve on the bound by
showing that differential privacy guarantees $O(\epsilon)$ accuracy with
probability $1-O(\delta\log(1/\epsilon)/\epsilon)$. It would be tempting to
guess that an $(\epsilon,\delta)$-differentially private computation should
guarantee $O(\epsilon)$ accuracy with probability $1-O(\delta)$. However, we
show that this is not the case, and that our bound is tight (up to logarithmic
factors).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05803</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05803</id><created>2015-04-22</created><updated>2016-01-29</updated><authors><author><keyname>Pardo</keyname><forenames>Diego</forenames></author><author><keyname>M&#xf6;ller</keyname><forenames>Lukas</forenames></author><author><keyname>Neunert</keyname><forenames>Michael</forenames></author><author><keyname>Winkler</keyname><forenames>Alexander W.</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author></authors><title>Evaluating direct transcription and nonlinear optimization methods for
  robot motion planning</title><categories>math.OC cs.RO</categories><doi>10.1109/LRA.2016.2527062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies existing direct transcription methods for trajectory
optimization applied to robot motion planning. There are diverse alternatives
for the implementation of direct transcription. In this study we analyze the
effects of such alternatives when solving a robotics problem. Different
parameters such as integration scheme, number of discretization nodes,
initialization strategies and complexity of the problem are evaluated. We
measure the performance of the methods in terms of computational time, accuracy
and quality of the solution. Additionally, we compare two optimization
methodologies frequently used to solve the transcribed problem, namely
Sequential Quadratic Programming (SQP) and Interior Point Method (IPM). As a
benchmark, we solve different motion tasks on an underactuated and
non-minimal-phase ball-balancing robot with a 10 dimensional state space and 3
dimensional input space. Additionally, we validate the results on a simulated
3D quadrotor. Finally, as a verification of using direct transcription methods
for trajectory optimization on real robots, we present hardware experiments on
a motion task including path constraints and actuation limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05809</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05809</id><created>2015-04-22</created><authors><author><keyname>Qi</keyname><forenames>Xianbiao</forenames></author><author><keyname>Zhao</keyname><forenames>Guoying</forenames></author><author><keyname>Shen</keyname><forenames>Linlin</forenames></author><author><keyname>Li</keyname><forenames>Qingquan</forenames></author><author><keyname>Pietikainen</keyname><forenames>Matti</forenames></author></authors><title>LOAD: Local Orientation Adaptive Descriptor for Texture and Material
  Classification</title><categories>cs.CV</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel local feature, called Local Orientation
Adaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD,
we proposed to define point description on an Adaptive Coordinate System (ACS),
adopt a binary sequence descriptor to capture relationships between one point
and its neighbors and use multi-scale strategy to enhance the discriminative
power of the descriptor. The proposed LOAD enjoys not only discriminative power
to capture the texture information, but also has strong robustness to
illumination variation and image rotation. Extensive experiments on benchmark
data sets of texture classification and real-world material recognition show
that the proposed LOAD yields the state-of-the-art performance. It is worth to
mention that we achieve a 65.4\% classification accuracy-- which is, to the
best of our knowledge, the highest record by far --on Flickr Material Database
by using a single feature. Moreover, by combining LOAD with the feature
extracted by Convolutional Neural Networks (CNN), we obtain significantly
better performance than both the LOAD and CNN. This result confirms that the
LOAD is complementary to the learning-based features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05811</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05811</id><created>2015-04-22</created><authors><author><keyname>Colledanchise</keyname><forenames>Michele</forenames></author><author><keyname>Parasuraman</keyname><forenames>Ramviyas</forenames></author><author><keyname>&#xd6;gren</keyname><forenames>Petter</forenames></author></authors><title>Learning of Behavior Trees for Autonomous Agents</title><categories>cs.RO cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Definition of an accurate system model for Automated Planner (AP) is often
impractical, especially for real-world problems. Conversely, off-the-shelf
planners fail to scale up and are domain dependent. These drawbacks are
inherited from conventional transition systems such as Finite State Machines
(FSMs) that describes the action-plan execution generated by the AP. On the
other hand, Behavior Trees (BTs) represent a valid alternative to FSMs
presenting many advantages in terms of modularity, reactiveness, scalability
and domain-independence. In this paper, we propose a model-free AP framework
using Genetic Programming (GP) to derive an optimal BT for an autonomous agent
to achieve a given goal in unknown (but fully observable) environments. We
illustrate the proposed framework using experiments conducted with an open
source benchmark Mario AI for automated generation of BTs that can play the
game character Mario to complete a certain level at various levels of
difficulty to include enemies and obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05816</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05816</id><created>2015-04-22</created><authors><author><keyname>Soos</keyname><forenames>Sandor</forenames><affiliation>Dept. Science Policy and Scientometrics, Library and Information Centre of the Hungarian Academy of Sciences, MTA</affiliation></author></authors><title>A new generation of science overlay maps with an application to the
  history of biosystematics</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a text-mining based analytical framework aiming at the
cognitive organization of complex scientific discourses. The approach is based
on models recently developed in science mapping, being a generalization of the
so-called Science Overlay Mapping methodology, referred to as Topic Overlay
Mapping (TOM). It is shown that via applications of TOM in visualization,
document clustering, time series analysis etc. the in-depth exploration and
even the measurement of cognitive complexity and its dynamics is feasible for
scientific domains. As a use case, an empirical study is presented into the
discovery of a long-standing complex, interdisciplinary discourse, the debate
on the species concept in biosystematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05821</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05821</id><created>2015-04-22</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Factor Colorings of Linearly Recurrent Words</title><categories>math.CO cs.DM</categories><comments>7 pages</comments><msc-class>68R15, 05D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short article, we study factor colorings of aperiodic linearly
recurrent infinite words. We show that there always exists a coloring which
does not admit a monochromatic factorization of the word into factors of
increasing lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05823</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05823</id><created>2015-04-22</created><updated>2015-06-02</updated><authors><author><keyname>Cowan</keyname><forenames>Wesley</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Normal Bandits of Unknown Means and Variances: Asymptotic Optimality,
  Finite Horizon Regret Bounds, and a Solution to an Open Problem</title><categories>stat.ML cs.LG</categories><comments>15 pages 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of sampling sequentially from a finite number of $N \geq
2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and
$k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the
$k^{th}$ time it is sampled. It is assumed that for each fixed $i$,
  $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. normal random variables,
with unknown mean $\mu_i$ and unknown variance $\sigma_i^2$.
  The objective is to have a policy $\pi$ for deciding from which of the $N$
populations to sample form at any time $n=1,2,\ldots$ so as to maximize the
expected sum of outcomes of $n$ samples or equivalently to minimize the regret
due to lack on information of the parameters $\mu_i$ and $\sigma_i^2$. In this
paper, we present a simple inflated sample mean (ISM) index policy that is
asymptotically optimal in the sense of Theorem 4 below. This resolves a
standing open problem from Burnetas and Katehakis (1996). Additionally, finite
horizon regret bounds are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05830</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05830</id><created>2015-04-22</created><authors><author><keyname>Besser</keyname><forenames>Bert</forenames></author><author><keyname>Werth</keyname><forenames>Bastian</forenames></author></authors><title>On the Approximation Performance of Degree Heuristics for Matching</title><categories>cs.DS</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the design of greedy algorithms for the maximum cardinality matching
problem the utilization of degree information when selecting the next edge is a
well established and successful approach.
  We define the class of &quot;degree sensitive&quot; greedy matching algorithms, which
allows us to analyze many well-known heuristics, and provide tight
approximation guarantees under worst case tie breaking. We exhibit algorithms
in this class with optimal approximation guarantee for bipartite graphs. In
particular the Karp-Sipser algorithm, which picks an edge incident with a
degree-1 node if possible and otherwise an arbitrary edge, turns out to be
optimal with approximation guarantee D/(2D-2), where D is the maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05837</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05837</id><created>2015-04-22</created><authors><author><keyname>Nguyen</keyname><forenames>Thi Le Thu</forenames></author><author><keyname>Septier</keyname><forenames>Francois</forenames></author><author><keyname>Rajaona</keyname><forenames>Harizo</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Delignon</keyname><forenames>Yves</forenames></author></authors><title>New Perspectives on Multiple Source Localization in Wireless Sensor
  Networks</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the challenging problem of multiple source
localization in Wireless Sensor Networks (WSN). We develop an efficient
statistical algorithm, based on the novel application of Sequential Monte Carlo
(SMC) sampler methodology, that is able to deal with an unknown number of
sources given quantized data obtained at the fusion center from different
sensors with imperfect wireless channels. We also derive the Posterior
Cram\'er-Rao Bound (PCRB) of the source location estimate. The PCRB is used to
analyze the accuracy of the proposed SMC sampler algorithm and the impact that
quantization has on the accuracy of location estimates of the sources.
Extensive experiments show that the benefits of the proposed scheme in terms of
the accuracy of the estimation method that are required for model selection
(i.e., the number of sources) and the estimation of the source characteristics
compared to the classical importance sampling method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05840</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05840</id><created>2015-04-22</created><updated>2015-05-05</updated><authors><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The Dynamics of Triads in Aggregated Journal-Journal Citation Relations:
  Specialty Developments at the Above-Journal Level</title><categories>cs.DL</categories><comments>Accepted for publication in Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dyads of journals related by citations can agglomerate into specialties
through the mechanism of triadic closure. Using the Journal Citation Reports
2011, 2012, and 2013, we analyze triad formation as indicators of integration
(specialty growth) and disintegration (restructuring). The strongest
integration is found among the large journals that report on studies in
different scientific specialties, such as PLoS ONE, Nature Communications,
Nature, and Science. This tendency towards large-scale integration has not yet
stabilized. Using the Islands algorithm, we also distinguish 51 local maxima of
integration. We zoom into the cited articles that carry the integration for:
(i) a new development within high-energy physics and (ii) an emerging interface
between the journals Applied Mathematical Modeling and the International
Journal of Advanced Manufacturing Technology. In the first case, integration is
brought about by a specific communication reaching across specialty boundaries,
whereas in the second, the dyad of journals indicates an emerging interface
between specialties. These results suggest that integration picks up
substantive developments at the specialty level. An advantage of the bottom-up
method is that no ex ante classification of journals is assumed in the dynamic
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05843</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05843</id><created>2015-04-22</created><authors><author><keyname>Yang</keyname><forenames>Hao</forenames></author><author><keyname>Zhou</keyname><forenames>Joey Tianyi</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Gao</keyname><forenames>Bin-Bin</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author></authors><title>Can Partial Strong Labels Boost Multi-label Object Recognition?</title><categories>cs.CV cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNN) have shown great performance as a global
representation for object recognition. However, for multi-label images that
contain multiple objects from different categories, scales and locations,
single CNN features might not be be optimal. To enhance the robustness and
discriminative power of CNN features for multi-label object recognition
problem, we propose a multi-view multi-instance framework. This framework
transforms the multi-label classification problem into a multi-class
multi-instance learning problem by extracting object proposals from images. A
multi-view pipeline is then applied to generate a two-view representation of
each proposal by exploiting two levels of labels in multi-label recognition
problem. The proposed framework not only has the flexibility of utilizing both
weak and strong labels or just weak labels, but also holds the generalization
ability to boost the performance of unseen categories by available strong
labels. Our framework is extensively compared with state-of-the-art
hand-crafted feature based and CNN based methods on two multi-label benchmark
datasets. The experimental results validate the discriminative power and
generalization ability of the proposed framework. When combined with a
very-deep network, we can achieve state-of-the-art results in both datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05846</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05846</id><created>2015-04-22</created><authors><author><keyname>Caldwell</keyname><forenames>James</forenames></author><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author><author><keyname>Nightingale</keyname><forenames>Peter</forenames></author></authors><title>Generalized Support and Formal Development of Constraint Propagators</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of support is pervasive in constraint programming. Traditionally,
when a domain value ceases to have support, it may be removed because it takes
part in no solutions. Arc-consistency algorithms such as AC2001 make use of
support in the form of a single domain value. GAC algorithms such as GAC-Schema
use a tuple of values to support each literal. We generalize these notions of
support in two ways. First, we allow a set of tuples to act as support. Second,
the supported object is generalized from a set of literals (GAC-Schema) to an
entire constraint or any part of it.
  We design a methodology for developing correct propagators using generalized
support. A constraint is expressed as a family of support properties, which may
be proven correct against the formal semantics of the constraint. Using
Curry-Howard isomorphism to interpret constructive proofs as programs, we show
how to derive correct propagators from the constructive proofs of the support
properties. The framework is carefully designed to allow efficient algorithms
to be produced. Derived algorithms may make use of dynamic literal triggers or
watched literals for efficiency. Finally, two case studies of deriving
efficient algorithms are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05854</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05854</id><created>2015-04-22</created><authors><author><keyname>Frecon</keyname><forenames>Jordan</forenames></author><author><keyname>Pustelnik</keyname><forenames>Nelly</forenames></author><author><keyname>Abry</keyname><forenames>Patrice</forenames></author><author><keyname>Condat</keyname><forenames>Laurent</forenames></author></authors><title>On-the-fly Approximation of Multivariate Total Variation Minimization</title><categories>cs.LG cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of change-point detection, addressed by Total Variation
minimization strategies, an efficient on-the-fly algorithm has been designed
leading to exact solutions for univariate data. In this contribution, an
extension of such an on-the-fly strategy to multivariate data is investigated.
The proposed algorithm relies on the local validation of the Karush-Kuhn-Tucker
conditions on the dual problem. Showing that the non-local nature of the
multivariate setting precludes to obtain an exact on-the-fly solution, we
devise an on-the-fly algorithm delivering an approximate solution, whose
quality is controlled by a practitioner-tunable parameter, acting as a
trade-off between quality and computational cost. Performance assessment shows
that high quality solutions are obtained on-the-fly while benefiting of
computational costs several orders of magnitude lower than standard iterative
procedures. The proposed algorithm thus provides practitioners with an
efficient multivariate change-point detection on-the-fly procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05862</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05862</id><created>2015-04-22</created><authors><author><keyname>Babaheidarian</keyname><forenames>Parisa</forenames></author><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author></authors><title>Compute-and-Forward Can Buy Secrecy Cheap</title><categories>cs.CR cs.IT math.IT</categories><comments>Accepted to ISIT 2015, 5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Gaussian multiple access channel with $K$ transmitters, a
(intended) receiver and an external eavesdropper. The transmitters wish to
reliably communicate with the receiver while concealing their messages from the
eavesdropper. This scenario has been investigated in prior works using two
different coding techniques; the random i.i.d. Gaussian coding and the signal
alignment coding. Although, the latter offers promising results in a very high
SNR regime, extending these results to the finite SNR regime is a challenging
task. In this paper, we propose a new lattice alignment scheme based on the
compute-and-forward framework which works at any finite SNR. We show that our
achievable secure sum rate scales with $\log(\mathrm{SNR})$ and hence, in most
SNR regimes, our scheme outperforms the random coding scheme in which the
secure sum rate does not grow with power. Furthermore, we show that our result
matches the prior work in the infinite SNR regime. Additionally, we analyze our
result numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05872</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05872</id><created>2015-04-22</created><authors><author><keyname>Peel</keyname><forenames>Leto</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Predicting sports scoring dynamics with restoration and anti-persistence</title><categories>physics.data-an cs.CY stat.AP</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Professional team sports provide an excellent domain for studying the
dynamics of social competitions. These games are constructed with simple,
well-defined rules and payoffs that admit a high-dimensional set of possible
actions and nontrivial scoring dynamics. The resulting gameplay and efforts to
predict its evolution are the object of great interest to both sports
professionals and enthusiasts. In this paper, we consider two online prediction
problems for team sports:~given a partially observed game Who will score next?
and ultimately Who will win? We present novel interpretable generative models
of within-game scoring that allow for dependence on lead size (restoration) and
on the last team to score (anti-persistence). We then apply these models to
comprehensive within-game scoring data for four sports leagues over a ten year
period. By assessing these models' relative goodness-of-fit we shed new light
on the underlying mechanisms driving the observed scoring dynamics of each
sport. Furthermore, in both predictive tasks, the performance of our models
consistently outperforms baselines models, and our models make quantitative
assessments of the latent team skill, over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05880</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05880</id><created>2015-04-22</created><authors><author><keyname>Kasiviswanathan</keyname><forenames>Shiva Prasad</forenames></author><author><keyname>Rudelson</keyname><forenames>Mark</forenames></author></authors><title>Spectral Norm of Random Kernel Matrices with Applications to Privacy</title><categories>stat.ML cs.CR cs.LG</categories><comments>16 pages, 1 Figure</comments><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods are an extremely popular set of techniques used for many
important machine learning and data analysis applications. In addition to
having good practical performances, these methods are supported by a
well-developed theory. Kernel methods use an implicit mapping of the input data
into a high dimensional feature space defined by a kernel function, i.e., a
function returning the inner product between the images of two data points in
the feature space. Central to any kernel method is the kernel matrix, which is
built by evaluating the kernel function on a given sample dataset.
  In this paper, we initiate the study of non-asymptotic spectral theory of
random kernel matrices. These are n x n random matrices whose (i,j)th entry is
obtained by evaluating the kernel function on $x_i$ and $x_j$, where
$x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Our
main contribution is to obtain tight upper bounds on the spectral norm (largest
eigenvalue) of random kernel matrices constructed by commonly used kernel
functions based on polynomials and Gaussian radial basis.
  As an application of these results, we provide lower bounds on the distortion
needed for releasing the coefficients of kernel ridge regression under
attribute privacy, a general privacy notion which captures a large class of
privacy definitions. Kernel ridge regression is standard method for performing
non-parametric regression that regularly outperforms traditional regression
approaches in various domains. Our privacy distortion lower bounds are the
first for any kernel technique, and our analysis assumes realistic scenarios
for the input, unlike all previous lower bounds for other release problems
which only hold under very restrictive input settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05891</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05891</id><created>2015-04-22</created><authors><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Exponent Function for One Helper Source Coding Problem at Rates outside
  the Rate Region</title><categories>cs.IT math.IT</categories><comments>Short version of this paper is accepted for presentation at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the one helper source coding problem posed and investigated by
Ahlswede, K\&quot;orner and Wyner. In this system, the error probability of decoding
goes to one as the source block length $n$ goes to infinity. This implies that
we have a strong converse theorem for the one helper source coding problem. In
this paper we provide a much stronger version of this strong converse theorem
for the one helper source coding problem. We prove that the error probability
of decoding tends to one exponentially and derive an explicit lower bound of
this exponent function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05895</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05895</id><created>2015-04-22</created><authors><author><keyname>Dashdorj</keyname><forenames>Zolzaya</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author><author><keyname>Antonelli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Semantic Enrichment of Mobile Phone Data Records Using Background
  Knowledge</title><categories>cs.AI cs.IT math.IT</categories><comments>40 pages, 34 figures</comments><msc-class>68</msc-class><acm-class>H.1.2; H.2.8; H.3.3; I.2.3; I.2.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every day, billions of mobile network events (i.e. CDRs) are generated by
cellular phone operator companies. Latent in this data are inspiring insights
about human actions and behaviors, the discovery of which is important because
context-aware applications and services hold the key to user-driven,
intelligent services, which can enhance our everyday lives such as social and
economic development, urban planning, and health prevention. The major
challenge in this area is that interpreting such a big stream of data requires
a deep understanding of mobile network events' context through available
background knowledge. This article addresses the issues in context awareness
given heterogeneous and uncertain data of mobile network events missing
reliable information on the context of this activity. The contribution of this
research is a model from a combination of logical and statistical reasoning
standpoints for enabling human activity inference in qualitative terms from
open geographical data that aimed at improving the quality of human behaviors
recognition tasks from CDRs. We use open geographical data, Openstreetmap
(OSM), as a proxy for predicting the content of human activity in the area. The
user study performed in Trento shows that predicted human activities (top
level) match the survey data with around 93% overall accuracy. The extensive
validation for predicting a more specific economic type of human activity
performed in Barcelona, by employing credit card transaction data. The analysis
identifies that appropriately normalized data on points of interest (POI) is a
good proxy for predicting human economical activities, with 84% accuracy on
average. So the model is proven to be efficient for predicting the context of
human activity, when its total level could be efficiently observed from cell
phone data records, missing contextual information however.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05898</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05898</id><created>2015-04-22</created><authors><author><keyname>Karakus</keyname><forenames>Can</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Opportunistic Scheduling for Full-Duplex Uplink-Downlink Networks</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, to appear at IEEE International Symposium on
  Information Theory (ISIT) '15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study opportunistic scheduling and the sum capacity of cellular networks
with a full-duplex multi-antenna base station and a large number of
single-antenna half-duplex users. Simultaneous uplink and downlink over the
same band results in uplink-to-downlink interference, degrading performance. We
present a simple opportunistic joint uplink-downlink scheduling algorithm that
exploits multiuser diversity and treats interference as noise. We show that in
homogeneous networks, our algorithm achieves the same sum capacity as what
would have been achieved if there was no uplink-to-downlink interference,
asymptotically in the number of users. The algorithm does not require
interference CSI at the base station or uplink users. It is also shown that for
a simple class of heterogeneous networks without sufficient channel diversity,
it is not possible to achieve the corresponding interference-free system
capacity. We discuss the potential for using device-to-device side-channels to
overcome this limitation in heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05900</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05900</id><created>2015-04-22</created><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>The Degraded Gaussian Diamond-Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>26 pages, 6 figures, a short version will appear in Proc. IEEE ISIT
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present nontrivial upper and lower bounds on the secrecy
capacity of the degraded Gaussian diamond-wiretap channel and identify several
ranges of channel parameters where these bounds coincide with useful
intuitions. Furthermore, we investigate the effect of the presence of an
eavesdropper on the capacity. We consider the following two scenarios regarding
the availability of randomness: 1) a common randomness is available at the
source and the two relays and 2) a randomness is available only at the source
and there is no available randomness at the relays. We obtain the upper bound
by taking into account the correlation between the two relay signals and the
availability of randomness at each encoder. For the lower bound, we propose two
types of coding schemes: 1) a decode-and-forward scheme where the relays
cooperatively transmit the message and the fictitious message and 2) a partial
DF scheme incorporated with multicoding in which each relay sends an
independent partial message and the whole or partial fictitious message using
dependent codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05905</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05905</id><created>2015-04-22</created><updated>2016-01-11</updated><authors><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author></authors><title>An FPT algorithm and a polynomial kernel for Linear Rankwidth-1 Vertex
  Deletion</title><categories>cs.DS</categories><comments>29 pages, 9 figures, An extended abstract appeared in IPEC2015</comments><msc-class>05C85</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear rankwidth is a linearized variant of rankwidth, introduced by Oum and
Seymour [Approximating clique-width and branch-width. J. Combin. Theory Ser. B,
96(4):514--528, 2006]. Motivated from recent development on graph modification
problems regarding classes of graphs of bounded treewidth or pathwidth, we
study the Linear Rankwidth-1 Vertex Deletion problem (shortly, LRW1-Vertex
Deletion). In the LRW1-Vertex Deletion problem, given an $n$-vertex graph $G$
and a positive integer $k$, we want to decide whether there is a set of at most
$k$ vertices whose removal turns $G$ into a graph of linear rankwidth at most
$1$ and find such a vertex set if one exists. While the meta-theorem of
Courcelle, Makowsky, and Rotics implies that LRW1-Vertex Deletion can be solved
in time $f(k)\cdot n^3$ for some function $f$, it is not clear whether this
problem allows a running time with a modest exponential function.
  We first establish that LRW1-Vertex Deletion can be solved in time $8^k\cdot
n^{\mathcal{O}(1)}$. The major obstacle to this end is how to handle a long
induced cycle as an obstruction. To fix this issue, we define necklace graphs
and investigate their structural properties. Later, we reduce the polynomial
factor by refining the trivial branching step based on a cliquewidth expression
of a graph, and obtain an algorithm that runs in time $2^{\mathcal{O}(k)}\cdot
n^4$. We also prove that the running time cannot be improved to $2^{o(k)}\cdot
n^{\mathcal{O}(1)}$ under the Exponential Time Hypothesis assumption. Lastly,
we show that the LRW1-Vertex Deletion problem admits a polynomial kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05908</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05908</id><created>2015-04-21</created><updated>2015-04-23</updated><authors><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Kuhlmann</keyname><forenames>Marco</forenames></author></authors><title>Maximum Pagenumber-k Subgraph is NP-Complete</title><categories>cs.CC</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ with a total order defined on its vertices, the Maximum
Pagenumber-$k$ Subgraph Problem asks for a maximum subgraph $G'$ of $G$ such
that $G'$ can be embedded into a $k$-book when the vertices are placed on the
spine according to the specified total order. We show that this problem is
NP-complete for $k \geq 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05910</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05910</id><created>2015-04-22</created><updated>2015-12-23</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Sen</keyname><forenames>Subhabrata</forenames></author></authors><title>Semidefinite Programs on Sparse Random Graphs and their Application to
  Community Detection</title><categories>cs.DM math.PR</categories><comments>43 pages (v3 contains a small section with consequences on
  estimation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denote by $A$ the adjacency matrix of an Erdos-Renyi graph with bounded
average degree. We consider the problem of maximizing $\langle
A-E\{A\},X\rangle$ over the set of positive semidefinite matrices $X$ with
diagonal entries $X_{ii}=1$. We prove that for large (bounded) average degree
$d$, the value of this semidefinite program (SDP) is --with high probability--
$2n\sqrt{d} + n\, o(\sqrt{d})+o(n)$. For a random regular graph of degree $d$,
we prove that the SDP value is $2n\sqrt{d-1}+o(n)$, matching a spectral upper
bound. Informally, Erdos-Renyi graphs appear to behave similarly to random
regular graphs for semidefinite programming.
  We next consider the sparse, two-groups, symmetric community detection
problem (also known as planted partition). We establish that SDP achieves the
information-theoretically optimal detection threshold for large (bounded)
degree. Namely, under this model, the vertex set is partitioned into subsets of
size $n/2$, with edge probability $a/n$ (within group) and $b/n$ (across). We
prove that SDP detects the partition with high probability provided
$(a-b)^2/(4d)&gt; 1+o_{d}(1)$, with $d= (a+b)/2$. By comparison, the information
theoretic threshold for detecting the hidden partition is $(a-b)^2/(4d)&gt; 1$:
SDP is nearly optimal for large bounded average degree.
  Our proof is based on tools from different research areas: $(i)$ A new
`higher-rank' Grothendieck inequality for symmetric matrices; $(ii)$ An
interpolation method inspired from statistical physics; $(iii)$ An analysis of
the eigenvectors of deformed Gaussian random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05916</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05916</id><created>2015-04-22</created><updated>2015-06-01</updated><authors><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author><author><keyname>Mehrmanesh</keyname><forenames>Laura</forenames></author><author><keyname>Mili</keyname><forenames>Lamine</forenames></author></authors><title>On the Definition of Cyber-Physical Resilience in Power Systems</title><categories>cs.SY</categories><comments>20 pages. This is a modified version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, advanced sensors, intelligent automation, communication
networks, and information technologies have been integrated into the electric
grid to enhance its performance and efficiency. Integrating these new
technologies has resulted in more interconnections and interdependencies
between the physical and cyber components of the grid. Natural disasters and
man-made perturbations have begun to threaten grid integrity more often. Urban
infrastructure networks are highly reliant on the electric grid and
consequently, the vulnerability of infrastructure networks to electric grid
outages is becoming a major global concern. In order to minimize the economic,
social, and political impacts of power system outages, the grid must be
resilient. The concept of a power system cyber-physical resilience centers
around maintaining system states at a stable level in the presence of
disturbances. Resilience is a multidimensional property of the electric grid,
it requires managing disturbances originating from physical component failures,
cyber component malfunctions, and human attacks. In the electric grid
community, there is not a clear and universally accepted definition of
cyber-physical resilience. This paper focuses on the definition of resilience
for the electric grid and reviews key concepts related to system resilience.
This paper aims to advance the field not only by adding cyber-physical
resilience concepts to power systems vocabulary, but also by proposing a new
way of thinking about grid operation with unexpected disturbances and hazards
and leveraging distributed energy resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05926</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05926</id><created>2015-04-22</created><authors><author><keyname>Cavraro</keyname><forenames>Guido</forenames></author><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author></authors><title>Distribution Network Topology Detection with Time Series Measurement
  Data Analysis</title><categories>cs.SY</categories><comments>This paper has been submitted to the IEEE Transactions on Power
  Systems. arXiv admin note: text overlap with arXiv:1504.00724</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach for detecting the topology of
distribution networks based on the analysis of time series measurements. The
time-based analysis approach draws on data from high-precision phasor
measurement units (PMUs or synchrophasors) for distribution systems. A key fact
is that time-series data taken from a dynamic system show specific patterns
regarding state transitions such as opening or closing switches, as a kind of
signature from each topology change. The proposed algorithm here is based on
the comparison of the actual signature of a recent state transition against a
library of signatures derived from topology simulations. The IEEE 33-bus model
is used for initial algorithm validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05929</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05929</id><created>2015-04-22</created><updated>2015-09-25</updated><authors><author><keyname>Yang</keyname><forenames>Bishan</forenames></author><author><keyname>Cardie</keyname><forenames>Claire</forenames></author><author><keyname>Frazier</keyname><forenames>Peter</forenames></author></authors><title>A Hierarchical Distance-dependent Bayesian Model for Event Coreference
  Resolution</title><categories>cs.CL stat.ML</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hierarchical distance-dependent Bayesian model for event
coreference resolution. While existing generative models for event coreference
resolution are completely unsupervised, our model allows for the incorporation
of pairwise distances between event mentions -- information that is widely used
in supervised coreference models to guide the generative clustering processing
for better event clustering both within and across documents. We model the
distances between event mentions using a feature-rich learnable distance
function and encode them as Bayesian priors for nonparametric clustering.
Experiments on the ECB+ corpus show that our model outperforms state-of-the-art
methods for both within- and cross-document event coreference resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05931</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05931</id><created>2015-04-22</created><authors><author><keyname>Hachem</keyname><forenames>Jad</forenames></author><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Effect of Number of Users in Multi-level Coded Caching</title><categories>cs.IT math.IT</categories><comments>13 pages; 2 figures. A shorter version is to appear in IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been recently established that joint design of content delivery and
storage (coded caching) can significantly improve performance over conventional
caching. This has also been extended to the case when content has non-uniform
popularity through several models. In this paper we focus on a multi-level
popularity model, where content is divided into levels based on popularity. We
consider two extreme cases of user distribution across caches for the
multi-level popularity model: a single user per cache (single-user setup)
versus a large number of users per cache (multi-user setup). When the capacity
approximation is universal (independent of number of popularity levels as well
as number of users, files and caches), we demonstrate a dichotomy in the
order-optimal strategies for these two extreme cases. In the multi-user case,
sharing memory among the levels is order-optimal, whereas for the single-user
case clustering popularity levels and allocating all the memory to them is the
order-optimal scheme. In proving these results, we develop new
information-theoretic lower bounds for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05932</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05932</id><created>2015-04-22</created><authors><author><keyname>Mackin</keyname><forenames>Erika</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Allocating Indivisible Items in Categorized Domains</title><categories>cs.GT cs.AI</categories><acm-class>J.4; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a general class of allocation problems called categorized domain
allocation problems (CDAPs), where indivisible items from multiple categories
are allocated to agents without monetary transfer and each agent gets at least
one item per category.
  We focus on basic CDAPs, where the number of items in each category is equal
to the number of agents. We characterize serial dictatorships for basic CDAPs
by a minimal set of three axiomatic properties: strategy-proofness,
non-bossiness, and category-wise neutrality. Then, we propose a natural
extension of serial dictatorships called categorial sequential allocation
mechanisms (CSAMs), which allocate the items in multiple rounds: in each round,
the active agent chooses an item from a designated category. We fully
characterize the worst-case rank efficiency of CSAMs for optimistic and
pessimistic agents, and provide a bound for strategic agents. We also conduct
experiments to compare expected rank efficiency of various CSAMs w.r.t. random
generated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05940</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05940</id><created>2015-04-22</created><authors><author><keyname>Trillingsgaard</keyname><forenames>Kasper Fl&#xf8;e</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Broadcasting a Common Message with Variable-Length Stop-Feedback Codes</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the maximum coding rate achievable over a two-user broadcast
channel for the scenario where a common message is transmitted using
variable-length stop-feedback codes. Specifically, upon decoding the common
message, each decoder sends a stop signal to the encoder, which transmits
continuously until it receives both stop signals. For the point-to-point case,
Polyanskiy, Poor, and Verd\'u (2011) recently demonstrated that variable-length
coding combined with stop feedback significantly increases the speed at which
the maximum coding rate converges to capacity. This speed-up manifests itself
in the absence of a square-root penalty in the asymptotic expansion of the
maximum coding rate for large blocklengths, a result a.k.a. zero dispersion. In
this paper, we show that this speed-up does not necessarily occur for the
broadcast channel with common message. Specifically, there exist scenarios for
which variable-length stop-feedback codes yield a positive dispersion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05941</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05941</id><created>2015-04-22</created><authors><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Strong Converse Exponent for Degraded Broadcast Channels at Rates
  outside the Capacity Region</title><categories>cs.IT math.IT</categories><comments>Short version of this paper is accepted for presentation at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the discrete memoryless degraded broadcast channels. We prove
that the error probability of decoding tends to one exponentially for rates
outside the capacity region and derive an explicit lower bound of this exponent
function. We shall demonstrate that the information spectrum approach is quite
useful for investigating this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05948</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05948</id><created>2015-04-22</created><authors><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Strong Converse Theorems for Degraded Broadcast Channels with Feedback</title><categories>cs.IT math.IT</categories><comments>Short version of this paper is accepted for presentation at ISIT
  2015. arXiv admin note: substantial text overlap with arXiv:1504.05941</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the discrete memoryless degraded broadcast channels with
feedback. We prove that the error probability of decoding tends to one
exponentially for rates outside the capacity region and derive an explicit
lower bound of this exponent function. We shall demonstrate that the
information spectrum approach is quite useful for investigating this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05967</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05967</id><created>2015-04-22</created><authors><author><keyname>Song</keyname><forenames>Daniel</forenames></author><author><keyname>Zhao</keyname><forenames>Jisheng</forenames></author><author><keyname>Burke</keyname><forenames>Michael</forenames></author><author><keyname>Sb&#xee;rlea</keyname><forenames>Drago&#x15f;</forenames></author><author><keyname>Wallach</keyname><forenames>Dan</forenames></author><author><keyname>Sarkar</keyname><forenames>Vivek</forenames></author></authors><title>Finding Tizen security bugs through whole-system static analysis</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tizen is a new Linux-based open source platform for consumer devices
including smartphones, televisions, vehicles, and wearables. While Tizen
provides kernel-level mandatory policy enforcement, it has a large collection
of libraries, implemented in a mix of C and C++, which make their own security
checks. In this research, we describe the design and engineering of a static
analysis engine which drives a full information flow analysis for apps and a
control flow analysis for the full library stack. We implemented these static
analyses as extensions to LLVM, requiring us to improve LLVM's native analysis
features to get greater precision and scalability, including knotty issues like
the coexistence of C++ inheritance with C function pointer use. With our tools,
we found several unexpected behaviors in the Tizen system, including paths
through the system libraries that did not have inline security checks. We show
how our tools can help the Tizen app store to verify important app properties
as well as helping the Tizen development process avoid the accidental
introduction of subtle vulnerabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05984</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05984</id><created>2015-04-22</created><updated>2015-08-05</updated><authors><author><keyname>Zhang</keyname><forenames>Qiaosheng</forenames></author><author><keyname>Kadhe</keyname><forenames>Swanand</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Coding against a Limited-view Adversary: The Effect of Causality and
  Feedback</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communication over a multi-path network in the
presence of a causal adversary. The limited-view causal adversary is able to
eavesdrop on a subset of links and also jam on a potentially overlapping subset
of links based on the current and past information. To ensure that the
communication takes place reliably and secretly, resilient network codes with
necessary redundancy are needed. We study two adversarial models - additive and
overwrite jamming and we optionally assume passive feedback from decoder to
encoder, i.e., the encoder sees everything that the decoder sees. The problem
assumes transmissions are in the large alphabet regime. For both jamming
models, we find the capacity under four scenarios - reliability without
feedback, reliability and secrecy without feedback, reliability with passive
feedback, reliability and secrecy with passive feedback. We observe that, in
comparison to the non-causal setting, the capacity with a causal adversary is
strictly increased for a wide variety of parameter settings and present our
intuition through several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05996</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05996</id><created>2015-04-22</created><updated>2015-05-01</updated><authors><author><keyname>Variani</keyname><forenames>Ehsan</forenames></author><author><keyname>Lahouel</keyname><forenames>Kamel</forenames></author><author><keyname>Bar-Hen</keyname><forenames>Avner</forenames></author><author><keyname>Jedynak</keyname><forenames>Bruno</forenames></author></authors><title>Non-Adaptive Policies for 20 Questions Target Localization</title><categories>cs.IT cs.AI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of target localization with noise is addressed. The target is a
sample from a continuous random variable with known distribution and the goal
is to locate it with minimum mean squared error distortion. The localization
scheme or policy proceeds by queries, or questions, weather or not the target
belongs to some subset as it is addressed in the 20-question framework. These
subsets are not constrained to be intervals and the answers to the queries are
noisy. While this situation is well studied for adaptive querying, this paper
is focused on the non adaptive querying policies based on dyadic questions. The
asymptotic minimum achievable distortion under such policies is derived.
Furthermore, a policy named the Aurelian1 is exhibited which achieves
asymptotically this distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05997</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05997</id><created>2015-04-22</created><authors><author><keyname>Su</keyname><forenames>Dong</forenames></author><author><keyname>Cao</keyname><forenames>Jianneng</forenames></author><author><keyname>Li</keyname><forenames>Ninghui</forenames></author></authors><title>Differentially Private Projected Histograms of Multi-Attribute Data for
  Classification</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we tackle the problem of constructing a differentially private
synopsis for the classification analyses. Several the state-of-the-art methods
follow the structure of existing classification algorithms and are all
iterative, which is suboptimal due to the locally optimal choices and the
over-divided privacy budget among many sequentially composed steps. Instead, we
propose a new approach, PrivPfC, a new differentially private method for
releasing data for classification. The key idea is to privately select an
optimal partition of the underlying dataset using the given privacy budget in
one step. Given one dataset and the privacy budget, PrivPfC constructs a pool
of candidate grids where the number of cells of each grid is under a data-aware
and privacy-budget-aware threshold. After that, PrivPfC selects an optimal grid
via the exponential mechanism by using a novel quality function which minimizes
the expected number of misclassified records on which a histogram classifier is
constructed using the published grid. Finally, PrivPfC injects noise into each
cell of the selected grid and releases the noisy grid as the private synopsis
of the data. If the size of the candidate grid pool is larger than the
processing capability threshold set by the data curator, we add a step in the
beginning of PrivPfC to prune the set of attributes privately. We introduce a
modified $\chi^2$ quality function with low sensitivity and use it to evaluate
an attribute's relevance to the classification label variable. Through
extensive experiments on real datasets, we demonstrate PrivPfC's superiority
over the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05998</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05998</id><created>2015-04-22</created><authors><author><keyname>Su</keyname><forenames>Dong</forenames></author><author><keyname>Cao</keyname><forenames>Jianneng</forenames></author><author><keyname>Li</keyname><forenames>Ninghui</forenames></author><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author><author><keyname>Jin</keyname><forenames>Hongxia</forenames></author></authors><title>Differentially Private $k$-Means Clustering</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are two broad approaches for differentially private data analysis. The
interactive approach aims at developing customized differentially private
algorithms for various data mining tasks. The non-interactive approach aims at
developing differentially private algorithms that can output a synopsis of the
input dataset, which can then be used to support various data mining tasks. In
this paper we study the tradeoff of interactive vs. non-interactive approaches
and propose a hybrid approach that combines interactive and non-interactive,
using $k$-means clustering as an example. In the hybrid approach to
differentially private $k$-means clustering, one first uses a non-interactive
mechanism to publish a synopsis of the input dataset, then applies the standard
$k$-means clustering algorithm to learn $k$ cluster centroids, and finally uses
an interactive approach to further improve these cluster centroids. We analyze
the error behavior of both non-interactive and interactive approaches and use
such analysis to decide how to allocate privacy budget between the
non-interactive step and the interactive step. Results from extensive
experiments support our analysis and demonstrate the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.05999</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.05999</id><created>2015-04-22</created><authors><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author></authors><title>Securing Data against Limited-Knowledge Adversaries in Distributed
  Storage Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted for ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing secure regenerating codes that protect
data integrity in distributed storage systems (DSS) in which some nodes may be
compromised by a malicious adversary. The adversary can corrupt the data stored
on and transmitted by the nodes under its control. The &quot;damage&quot; incurred by the
actions of the adversary depends on how much information it knows about the
data in the whole DSS. We focus on the limited-knowledge model in which the
adversary knows only the data on the nodes under its control. The only secure
capacity-achieving codes known in the literature for this model are for the
bandwidth-limited regime and repair degree $d=n-1$, i.e., when a node fails in
a DSS with $n$ nodes all the remaining $n-1$ nodes are contacted for repair. We
extend these results to the more general case of $d\leq n-1$ in the
bandwidth-limited regime. Our capacity-achieving scheme is based on the use of
product-matrix codes with special hashing functions and allow the
identification of the compromised nodes and their elimination from the DSS
while preserving the data integrity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06002</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06002</id><created>2015-04-22</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Majumdar</keyname><forenames>Anirudha</forenames></author></authors><title>Some Applications of Polynomial Optimization in Operations Research and
  Real-Time Decision Making</title><categories>math.OC cs.RO cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate applications of algebraic techniques that optimize and certify
polynomial inequalities to problems of interest in the operations research and
transportation engineering communities. Three problems are considered: (i)
wireless coverage of targeted geographical regions with guaranteed signal
quality and minimum transmission power, (ii) computing real-time certificates
of collision avoidance for a simple model of an unmanned vehicle (UV)
navigating through a cluttered environment, and (iii) designing a nonlinear
hovering controller for a quadrotor UV, which has recently been used for load
transportation. On our smaller-scale applications, we apply the sum of squares
(SOS) relaxation and solve the underlying problems with semidefinite
programming. On the larger-scale or real-time applications, we use our recently
introduced &quot;SDSOS Optimization&quot; techniques which result in second order cone
programs. To the best of our knowledge, this is the first study of real-time
applications of sum of squares techniques in optimization and control. No
knowledge in dynamics and control is assumed from the reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06003</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06003</id><created>2015-04-22</created><authors><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Belyi</keyname><forenames>Alexander</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Arias</keyname><forenames>Juan Murillo</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Scaling of city attractiveness for foreign visitors through big data of
  human economical and social media activity</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 3 figures, 1 table</comments><msc-class>91D30</msc-class><doi>10.1109/BigDataCongress.2015.92</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific studies investigating laws and regularities of human behavior are
nowadays increasingly relying on the wealth of widely available digital
information produced by human social activity. In this paper we leverage big
data created by three different aspects of human activity (i.e., bank card
transactions, geotagged photographs and tweets) in Spain for quantifying city
attractiveness for the foreign visitors. An important finding of this papers is
a strong superlinear scaling of city attractiveness with its population size.
The observed scaling exponent stays nearly the same for different ways of
defining cities and for different data sources, emphasizing the robustness of
our finding. Temporal variation of the scaling exponent is also considered in
order to reveal seasonal patterns in the attractiveness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06010</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06010</id><created>2015-04-22</created><authors><author><keyname>Farnia</keyname><forenames>Farzan</forenames></author><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Minimum HGR Correlation Principle: From Marginals to Joint Distribution</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given low order moment information over the random variables $\mathbf{X} =
(X_1,X_2,\ldots,X_p)$ and $Y$, what distribution minimizes the
Hirschfeld-Gebelein-R\'{e}nyi (HGR) maximal correlation coefficient between
$\mathbf{X}$ and $Y$, while remains faithful to the given moments? The answer
to this question is important especially in order to fit models over
$(\mathbf{X},Y)$ with minimum dependence among the random variables
$\mathbf{X}$ and $Y$. In this paper, we investigate this question first in the
continuous setting by showing that the jointly Gaussian distribution achieves
the minimum HGR correlation coefficient among distributions with the given
first and second order moments. Then, we pose a similar question in the
discrete scenario by fixing the pairwise marginals of the random variables
$\mathbf{X}$ and $Y$. To answer this question in the discrete setting, we first
derive a lower bound for the HGR correlation coefficient over the class of
distributions with fixed pairwise marginals. Then we show that this lower bound
is tight if there exists a distribution with certain {\it additive} structure
satisfying the given pairwise marginals. Moreover, the distribution with the
additive structure achieves the minimum HGR correlation coefficient. Finally,
we conclude by showing that the event of obtaining pairwise marginals
containing an additive structured distribution has a positive Lebesgue measure
over the probability simplex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06015</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06015</id><created>2015-04-22</created><authors><author><keyname>Li</keyname><forenames>Yuanxin</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Super-Resolution of Mutually Interfering Signals</title><categories>cs.IT math.IT</categories><comments>ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider simultaneously identifying the membership and locations of point
sources that are convolved with different low-pass point spread functions, from
the observation of their superpositions. This problem arises in
three-dimensional super-resolution single-molecule imaging, neural spike
sorting, multi-user channel identification, among others. We propose a novel
algorithm, based on convex programming, and establish its near-optimal
performance guarantee for exact recovery by exploiting the sparsity of the
point source model as well as incoherence between the point spread functions.
Numerical examples are provided to demonstrate the effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06018</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06018</id><created>2015-04-22</created><updated>2015-09-01</updated><authors><author><keyname>Kao</keyname><forenames>David T. H.</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Blind Index Coding</title><categories>cs.IT math.IT</categories><comments>Parts of this paper were presented at ISIT 2015 and ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the blind index coding (BIC) problem, in which a single sender
communicates distinct messages to multiple users over a shared channel. Each
user has partial knowledge of each message as side information. However, unlike
classic index coding, in BIC, the sender is uncertain of what side information
is available to each user. In particular, the sender only knows the amount of
bits in each user's side information but not its content. This problem can
arise naturally in caching and wireless networks. In order to blindly exploit
side information in the BIC problem, we develop a hybrid coding scheme that
XORs uncoded bits of a subset of messages with random combinations of bits from
other messages. This scheme allows us to strike the right balance between
maximizing the transmission rate to each user and minimizing the interference
leakage to others. We also develop a general outer bound, which relies on a
strong data processing inequality to effectively capture the senders
uncertainty about the users' side information. Additionally, we consider the
case where communication takes place over a shared wireless medium, modeled by
an erasure broadcast channel, and show that surprisingly, combining repetition
coding with hybrid coding improves the achievable rate region and outperforms
alternative strategies of coping with channel erasure and while blindly
exploiting side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06025</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06025</id><created>2015-04-22</created><authors><author><keyname>Xu</keyname><forenames>Jingwei</forenames></author><author><keyname>Che</keyname><forenames>Tiben</forenames></author><author><keyname>Choi</keyname><forenames>Gwan</forenames></author></authors><title>XJ-BP: Express Journey Belief Propagation Decoding for Polar Codes</title><categories>cs.IT math.IT</categories><comments>submitted to GLOBECOMM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel propagation (BP) based decoding algorithm for
polar codes. The proposed algorithm facilitates belief propagation by utilizing
the specific constituent codes that exist in the factor graph, which results in
an express journey (XJ) for belief information to propagate in each decoding
iteration. In addition, this XJ-BP decoder employs a novel round-trip message
passing scheduling method for the increased efficiency. The proposed method
simplifies min-sum (MS) BP decoder by 40.6%. Along with the round-trip
scheduling, the XJ-BP algorithm reduces the computational complexity of MS BP
decoding by 90.4%; this enables an energy-efficient hardware implementation of
BP decoding in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06028</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06028</id><created>2015-04-22</created><authors><author><keyname>Xu</keyname><forenames>Aolin</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Converses for distributed estimation via strong data processing
  inequalities</title><categories>cs.IT math.IT</categories><comments>6 pages; to be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed estimation, where local processors
observe independent samples conditioned on a common random parameter of
interest, map the observations to a finite number of bits, and send these bits
to a remote estimator over independent noisy channels. We derive converse
results for this problem, such as lower bounds on Bayes risk. The main
technical tools include a lower bound on the Bayes risk via mutual information
and small ball probability, as well as strong data processing inequalities for
the relative entropy. Our results can recover and improve some existing results
on distributed estimation with noiseless channels, and also capture the effect
of noisy channels on the estimation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06029</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06029</id><created>2015-04-22</created><authors><author><keyname>Lee</keyname><forenames>Jaeho</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author></authors><title>On MMSE estimation from quantized observations in the nonasymptotic
  regime</title><categories>cs.IT math.IT</categories><comments>5 pages; to be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies MMSE estimation on the basis of quantized noisy
observations. It presents nonasymptotic bounds on MMSE regret due to
quantization for two settings: (1) estimation of a scalar random variable given
a quantized vector of $n$ conditionally independent observations, and (2)
estimation of a $p$-dimensional random vector given a quantized vector of $n$
observations (not necessarily independent) when the full MMSE estimator has a
subgaussian concentration property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06036</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06036</id><created>2015-04-23</created><authors><author><keyname>Brustolin</keyname><forenames>Andrew F. C.</forenames></author></authors><title>Edge Detection Based on Global and Local Parameters of the Image</title><categories>cs.CV</categories><comments>13 pages, 26 figures</comments><acm-class>I.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an edge detection method based on global and local
parameters of the image, which produces satisfactory results on the edge
detection of complex images and has a simple structure for execution. The local
and global parameters of the image are arithmetic means and standard
deviations, the former acquired from a three sized window representing five
pixels, the latter acquired from the entire row or column. We obtain the
differences of grayscale intensities between two adjacent pixels and the sum of
the modulus of these differences from the horizontal and vertical scans of the
image. Using these obtained values, we calculate the local and global
parameters. After the gathering of the local and global parameters, we compare
each sum of the modulus of differences with its own local and global parameter.
In the case of the comparison is true, the consecutive pixel to the modulus sum
of differences index is marked as an edge. We present the results of the tests
with grayscale images using different parameters and discuss the advantages and
disadvantages of each parameter value and algorithm structure chosen on the
edge processing. There is a comparison of results between this papers detector
and Canny, where we evaluate the quality of the presented detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06043</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06043</id><created>2015-04-23</created><authors><author><keyname>Ramaswamy</keyname><forenames>Arunselvan</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Stability of Stochastic Approximations with `Controlled Markov' Noise
  and Temporal Difference Learning</title><categories>cs.SY stat.ML</categories><comments>18 pages</comments><msc-class>62L20, 93E03, 93E35, 34A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a `stability theorem' for stochastic approximation
(SA) algorithms with `controlled Markov' noise. Such algorithms were first
studied by Borkar in 2006. Specifically, sufficient conditions are presented
which guarantee the stability of the iterates. Further, under these conditions
the iterates are shown to track a solution to the differential inclusion
defined in terms of the ergodic occupation measures associated with the
`controlled Markov' process. As an application to our main result we present an
improvement to a general form of temporal difference learning algorithms.
Specifically, we present sufficient conditions for their stability and
convergence using our framework. This paper builds on the works of Borkar as
well as Benveniste, Metivier and Priouret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06044</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06044</id><created>2015-04-23</created><updated>2015-06-20</updated><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author><author><keyname>Kotulski</keyname><forenames>Leszek</forenames></author></authors><title>Towards a better understanding and behavior recognition of inhabitants
  in smart cities. A public transport case</title><categories>cs.CY</categories><comments>Proceedings of 14th International Conference on Arificial Inteligence
  and Soft Computing (ICAISC 2015), 14-18 June, 2015, Zakopane, Poland; Lecture
  Notes in Computer Science, vol. 9120, pp.237-246. Springer Verlag 2015</comments><doi>10.1007/978-3-319-19369-4_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of modern urban systems and smart cities requires monitoring and
careful analysis of different signals. Such signals can originate from
different sources and one of the most promising is the BTS, i.e. base
transceiver station, an element of mobile carrier networks. This paper presents
the fundamental problems of elicitation, classification and understanding of
such signals so as to develop context-aware and pro-active systems in urban
areas. These systems are characterized by the omnipresence of computing which
is strongly focused on providing on-line support to users/inhabitants of smart
cities. A method of analyzing selected elements of mobile phone datasets
through understanding inhabitants' behavioral fingerprints to obtain smart
scenarios for public transport is proposed. Some scenarios are outlined. A
multi-agent system is proposed. A formalism based on graphs that allows
reasoning about inhabitant behaviors is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06045</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06045</id><created>2015-04-23</created><updated>2015-09-21</updated><authors><author><keyname>Hori</keyname><forenames>Yutaka</forenames></author><author><keyname>Miyazako</keyname><forenames>Hiroki</forenames></author><author><keyname>Kumagai</keyname><forenames>Soichiro</forenames></author><author><keyname>Hara</keyname><forenames>Shinji</forenames></author></authors><title>Coordinated Spatial Pattern Formation in Biomolecular Communication
  Networks</title><categories>q-bio.MN cs.SY nlin.PS</categories><comments>26 pages, 7 figures, 3 movies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a control theoretic framework to model and analyze the
self-organized pattern formation of molecular concentrations in biomolecular
communication networks, emerging applications in synthetic biology. In
biomolecular communication networks, bio-nanomachines, or biological cells,
communicate with each other using a cell-to-cell communication mechanism
mediated by a diffusible signaling molecule, thereby the dynamics of molecular
concentrations are approximately modeled as a reaction-diffusion system with a
single diffuser. We first introduce a feedback model representation of the
reaction-diffusion system and provide a systematic local stability/instability
analysis tool using the root locus of the feedback system. The instability
analysis then allows us to analytically derive the conditions for the
self-organized spatial pattern formation, or Turing pattern formation, of the
bio-nanomachines. We propose a novel synthetic biocircuit motif called
activator-repressor-diffuser system and show that it is one of the minimum
biomolecular circuits that admit self-organized patterns over cell population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06046</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06046</id><created>2015-04-23</created><authors><author><keyname>Lai</keyname><forenames>Ching-Yi</forenames></author><author><keyname>Duan</keyname><forenames>Runyao</forenames></author></authors><title>On the One-Shot Zero-Error Classical Capacity of Classical-Quantum
  Channels Assisted by Quantum Non-signalling Correlations</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Duan and Winter studied the one-shot zero-error classical capacity of a
quantum channel assisted by quantum non-signalling correlations, and formulated
this problem as a semidefinite program depending only on the Kraus operator
space of the channel. For the class of classical-quantum channels, they showed
that the asymptotic zero-error classical capacity assisted by quantum
non-signalling correlations, minimized over all classical-quantum channels with
a confusability graph $G$, is exactly $\log \vartheta(G)$, where $\vartheta(G)$
is the celebrated Lov\'{a}sz theta function. In this paper, we show that the
same result holds in the one-shot setting for a class of circulant graphs
defined by equal-sized cyclotomic cosets, which include the cycle graphs of odd
length, the Paley graphs of prime vertices, and the cubit residue graphs of
prime vertices. Examples of other graphs are also discussed. This endows the
Lov\'{a}sz $\theta$ function with a more straightforward operational meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06049</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06049</id><created>2015-04-23</created><authors><author><keyname>Ruderman</keyname><forenames>Michael</forenames></author></authors><title>State-space formulation of scalar Preisach hysteresis model for rapid
  computation in time domain</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A state-space formulation of classical scalar Preisach model (CSPM) of
hysteresis is proposed. The introduced state dynamics and memory interface
allow to use the state equation, which is rapid in calculation, instead of the
original Preisach equation. The main benefit of the proposed modeling approach
is the reduced computational effort which requires only a single integration
over the instantaneous line segment in the Preisach plane. Numerical
evaluations of the computation time and model accuracy are provided in
comparison to the CSPM which is taken as a reference model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06054</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06054</id><created>2015-04-23</created><authors><author><keyname>Anjum</keyname><forenames>Muhammad Ali Raza</forenames></author></authors><title>A New Approach to Adaptive Signal Processing</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unified linear algebraic approach to adaptive signal processing (ASP) is
presented. Starting from just Ax=b, key ASP algorithms are derived in a simple,
systematic, and integrated manner without requiring any background knowledge to
the field. Algorithms covered are Steepest Descent, LMS, Normalized LMS,
Kaczmarz, Affine Projection, RLS, Kalman filter, and MMSE/Least Square Wiener
filters. By following this approach, readers will discover a synthesis; they
will learn that one and only one equation is involved in all these algorithms.
They will also learn that this one equation forms the basis of more advanced
algorithms like reduced rank adaptive filters, extended Kalman filter, particle
filters, multigrid methods, preconditioning methods, Krylov subspace methods
and conjugate gradients. This will enable them to enter many sophisticated
realms of modern research and development. Eventually, this one equation will
not only become their passport to ASP but also to many highly specialized areas
of computational science and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06055</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06055</id><created>2015-04-23</created><authors><author><keyname>Wang</keyname><forenames>Naiyan</forenames></author><author><keyname>Shi</keyname><forenames>Jianping</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author></authors><title>Understanding and Diagnosing Visual Tracking Systems</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several benchmark datasets for visual tracking research have been proposed in
recent years. Despite their usefulness, whether they are sufficient for
understanding and diagnosing the strengths and weaknesses of different trackers
remains questionable. To address this issue, we propose a framework by breaking
a tracker down into five constituent parts, namely, motion model, feature
extractor, observation model, model updater, and ensemble post-processor. We
then conduct ablative experiments on each component to study how it affects the
overall result. Surprisingly, our findings are discrepant with some common
beliefs in the visual tracking research community. We find that the feature
extractor plays the most important role in a tracker. On the other hand,
although the observation model is the focus of many studies, we find that it
often brings no significant improvement. Moreover, the motion model and model
updater contain many details that could affect the result. Also, the ensemble
post-processor can improve the result substantially when the constituent
trackers have high diversity. Based on our findings, we put together some very
elementary building blocks to give a basic tracker which is competitive in
performance to the state-of-the-art trackers. We believe our framework can
provide a solid baseline when conducting controlled experiments for visual
tracking research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06058</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06058</id><created>2015-04-23</created><updated>2015-05-04</updated><authors><author><keyname>Xu</keyname><forenames>Haifeng</forenames></author><author><keyname>Jiang</keyname><forenames>Albert X.</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author><author><keyname>Rabinovich</keyname><forenames>Zinovi</forenames></author><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Tambe</keyname><forenames>Milind</forenames></author></authors><title>Security Games with Information Leakage: Modeling and Computation</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most models of Stackelberg security games assume that the attacker only knows
the defender's mixed strategy, but is not able to observe (even partially) the
instantiated pure strategy. Such partial observation of the deployed pure
strategy -- an issue we refer to as information leakage -- is a significant
concern in practical applications. While previous research on patrolling games
has considered the attacker's real-time surveillance, our settings, therefore
models and techniques, are fundamentally different. More specifically, after
describing the information leakage model, we start with an LP formulation to
compute the defender's optimal strategy in the presence of leakage. Perhaps
surprisingly, we show that a key subproblem to solve this LP (more precisely,
the defender oracle) is NP-hard even for the simplest of security game models.
We then approach the problem from three possible directions: efficient
algorithms for restricted cases, approximation algorithms, and heuristic
algorithms for sampling that improves upon the status quo. Our experiments
confirm the necessity of handling information leakage and the advantage of our
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06063</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06063</id><created>2015-04-23</created><updated>2015-08-29</updated><authors><author><keyname>Ma</keyname><forenames>Lin</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Shang</keyname><forenames>Lifeng</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>Multimodal Convolutional Neural Networks for Matching Image and Sentence</title><categories>cs.CV cs.CL cs.NE</categories><comments>Accepted by ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose multimodal convolutional neural networks (m-CNNs)
for matching image and sentence. Our m-CNN provides an end-to-end framework
with convolutional architectures to exploit image representation, word
composition, and the matching relations between the two modalities. More
specifically, it consists of one image CNN encoding the image content, and one
matching CNN learning the joint representation of image and sentence. The
matching CNN composes words to different semantic fragments and learns the
inter-modal relations between image and the composed fragments at different
levels, thus fully exploit the matching relations between image and sentence.
Experimental results on benchmark databases of bidirectional image and sentence
retrieval demonstrate that the proposed m-CNNs can effectively capture the
information necessary for image and sentence matching. Specifically, our
proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and
Microsoft COCO databases achieve the state-of-the-art performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06066</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06066</id><created>2015-04-23</created><authors><author><keyname>Ren</keyname><forenames>Shaoqing</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Object Detection Networks on Convolutional Feature Maps</title><categories>cs.CV</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most object detectors contain two important components: a feature extractor
and an object classifier. The feature extractor has rapidly evolved with
significant research efforts leading to better deep ConvNet architectures. The
object classifier, however, has not received much attention and most
state-of-the-art systems (like R-CNN) use simple multi-layer perceptrons. This
paper demonstrates that carefully designing deep networks for object
classification is just as important. We take inspiration from traditional
object classifiers, such as DPM, and experiment with deep networks that have
part-like filters and reason over latent variables. We discover that on
pre-trained convolutional feature maps, even randomly initialized deep
classifiers produce excellent results, while the improvement due to fine-tuning
is secondary; on HOG features, deep classifiers outperform DPMs and produce the
best HOG-only results without external data. We believe these findings provide
new insight for developing object detection systems. Our framework, called
Networks on Convolutional feature maps (NoC), achieves outstanding results on
the PASCAL VOC 2007 (73.3% mAP) and 2012 (68.8% mAP) benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06075</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06075</id><created>2015-04-23</created><updated>2015-09-22</updated><authors><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Veerman</keyname><forenames>J. J. P.</forenames></author></authors><title>Transients of platoons with asymmetric and different Laplacians</title><categories>cs.SY cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an asymmetric control of platoons of identical vehicles with
nearest-neighbor interaction. Recent results show that if the vehicle uses
different asymmetries for position and velocity errors, the platoon has a short
transient and low overshoots. In this paper we investigate the properties of
vehicles with friction. To achieve consensus, an integral part is added to the
controller, making the vehicle a third-order system. We show that the
parameters can be chosen so that the platoon behaves as a wave equation with
different wave velocities. Simulations suggest that our system has a better
performance than other nearest-neighbor scenarios. Moreover, an
optimization-based procedure is used to find the controller properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06077</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06077</id><created>2015-04-23</created><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author><author><keyname>Andro</keyname><forenames>Mathieu</forenames></author><author><keyname>Corbi&#xe8;re</keyname><forenames>Roselyne</forenames></author><author><keyname>Phan</keyname><forenames>Tien T.</forenames></author></authors><title>Open Data Platform for Knowledge Access in Plant Health Domain : VESPA
  Mining</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Important data are locked in ancient literature. It would be uneconomic to
produce these data again and today or to extract them without the help of text
mining technologies. Vespa is a text mining project whose aim is to extract
data on pest and crops interactions, to model and predict attacks on crops, and
to reduce the use of pesticides. A few attempts proposed an agricultural
information access. Another originality of our work is to parse documents with
a dependency of the document architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06078</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06078</id><created>2015-04-23</created><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author><author><keyname>Phan</keyname><forenames>Tien</forenames></author></authors><title>x.ent: R Package for Entities and Relations Extraction based on
  Unsupervised Learning and Document Structure</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relation extraction with accurate precision is still a challenge when
processing full text databases. We propose an approach based on cooccurrence
analysis in each document for which we used document organization to improve
accuracy of relation extraction. This approach is implemented in a R package
called \emph{x.ent}. Another facet of extraction relies on use of extracted
relation into a querying system for expert end-users. Two datasets had been
used. One of them gets interest from specialists of epidemiology in plant
health. For this dataset usage is dedicated to plant-disease exploration
through agricultural information news. An open-data platform exploits exports
from \emph{x.ent} and is publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06080</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06080</id><created>2015-04-23</created><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author></authors><title>svcR: An R Package for Support Vector Clustering improved with Geometric
  Hashing applied to Lexical Pattern Discovery</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new R package which takes a numerical matrix format as data
input, and computes clusters using a support vector clustering method (SVC). We
have implemented an original 2D-grid labeling approach to speed up cluster
extraction. In this sense, SVC can be seen as an efficient cluster extraction
if clusters are separable in a 2-D map. Secondly we showed that this SVC
approach using a Jaccard-Radial base kernel can help to classify well enough a
set of terms into ontological classes and help to define regular expression
rules for information extraction in documents; our case study concerns a set of
terms and documents about developmental and molecular biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06093</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06093</id><created>2015-04-23</created><updated>2015-04-27</updated><authors><author><keyname>Vigneri</keyname><forenames>Luigi</forenames></author><author><keyname>Chandrashekar</keyname><forenames>Jaideep</forenames></author><author><keyname>Pefkianakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Heen</keyname><forenames>Olivier</forenames></author></authors><title>Taming the Android AppStore: Lightweight Characterization of Android
  Applications</title><categories>cs.NI cs.CY</categories><comments>20 pages, single column</comments><report-no>RR-15-305</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are over 1.2 million applications on the Google Play store today with a
large number of competing applications for any given use or function. This
creates challenges for users in selecting the right application. Moreover, some
of the applications being of dubious origin, there are no mechanisms for users
to understand who the applications are talking to, and to what extent. In our
work, we first develop a lightweight characterization methodology that can
automatically extract descriptions of application network behavior, and apply
this to a large selection of applications from the Google App Store. We find
several instances of overly aggressive communication with tracking websites, of
excessive communication with ad related sites, and of communication with sites
previously associated with malware activity. Our results underscore the need
for a tool to provide users more visibility into the communication of apps
installed on their mobile devices. To this end, we develop an Android
application to do just this; our application monitors outgoing traffic,
associates it with particular applications, and then identifies destinations in
particular categories that we believe suspicious or else important to reveal to
the end-user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06103</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06103</id><created>2015-04-23</created><updated>2016-03-04</updated><authors><author><keyname>Vojir</keyname><forenames>Tomas</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author><author><keyname>Noskova</keyname><forenames>Jana</forenames></author></authors><title>Online Adaptive Hidden Markov Model for Multi-Tracker Fusion</title><categories>cs.CV</categories><comments>27 pages, 9 figures, submitted to CVIU journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel method for visual object tracking called
HMMTxD. The method fuses observations from complementary out-of-the box
trackers and a detector by utilizing a hidden Markov model whose latent states
correspond to a binary vector expressing the failure of individual trackers.
The Markov model is trained in an unsupervised way, relying on an online
learned detector to provide a source of tracker-independent information for a
modified Baum- Welch algorithm that updates the model w.r.t. the partially
annotated data.
  We show the effectiveness of the proposed method on combination of two and
three tracking algorithms. The performance of HMMTxD is evaluated on two
standard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly
available sequences. The HMMTxD outperforms the state-of-the-art, often
significantly, on all datasets in almost all criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06105</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06105</id><created>2015-04-23</created><authors><author><keyname>Kartzow</keyname><forenames>Alexander</forenames></author><author><keyname>Weidner</keyname><forenames>Thomas</forenames></author></authors><title>Model Checking Constraint LTL over Trees</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint automata are an adaptation of B\&quot;uchi-automata that process data
words where the data comes from some relational structure S. Every transition
of such an automaton comes with constraints in terms of the relations of S. A
transition can only be fired if the current and the next data values satisfy
all constraints of this transition. These automata have been used in the
setting where S is a linear order for deciding constraint LTL with constraints
over S. In this paper, S is the infinitely branching infinite order tree T. We
provide a PSPACE algorithm for emptiness of T-constraint automata. This result
implies PSPACE-completeness of the satisfiability and the model checking
problem for constraint LTL with constraints over T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06106</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06106</id><created>2015-04-23</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>A Short Survey on Arithmetic Transforms and the Arithmetic Hartley
  Transform</title><categories>math.CA cs.NA</categories><comments>12 pages, 5 figures</comments><msc-class>42Bxx, 65Txx</msc-class><journal-ref>Revista da Sociedade Brasileira de Telecomunica\c{c}\~{o}es
  (Journal of Communication and Information Systems), v. 19, pp. 68--79, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arithmetic complexity has a main role in the performance of algorithms for
spectrum evaluation. Arithmetic transform theory offers a method for computing
trigonometrical transforms with minimal number of multiplications. In this
paper, the proposed algorithms for the arithmetic Fourier transform are
surveyed. A new arithmetic transform for computing the discrete Hartley
transform is introduced: the Arithmetic Hartley transform. The interpolation
process is shown to be the key element of the arithmetic transform theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06110</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06110</id><created>2015-04-23</created><authors><author><keyname>AlKindy</keyname><forenames>Bassam</forenames></author><author><keyname>Al-Nayyef</keyname><forenames>Huda</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Salomon</keyname><forenames>Michel</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Improved Core Genes Prediction for Constructing well-supported
  Phylogenetic Trees in large sets of Plant Species</title><categories>q-bio.GN cs.CE</categories><comments>12 pages, 7 figures, IWBBIO 2015 (3rd International Work-Conference
  on Bioinformatics and Biomedical Engineering)</comments><journal-ref>Springer LNBI 9043, 2015, 379--390</journal-ref><doi>10.1007/978-3-319-16483-0_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The way to infer well-supported phylogenetic trees that precisely reflect the
evolutionary process is a challenging task that completely depends on the way
the related core genes have been found. In previous computational biology
studies, many similarity based algorithms, mainly dependent on calculating
sequence alignment matrices, have been proposed to find them. In these kinds of
approaches, a significantly high similarity score between two coding sequences
extracted from a given annotation tool means that one has the same genes. In a
previous work article, we presented a quality test approach (QTA) that improves
the core genes quality by combining two annotation tools (namely NCBI, a
partially human-curated database, and DOGMA, an efficient annotation algorithm
for chloroplasts). This method takes the advantages from both sequence
similarity and gene features to guarantee that the core genome contains correct
and well-clustered coding sequences (\emph{i.e.}, genes). We then show in this
article how useful are such well-defined core genes for biomolecular
phylogenetic reconstructions, by investigating various subsets of core genes at
various family or genus levels, leading to subtrees with strong bootstraps that
are finally merged in a well-supported supertree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06117</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06117</id><created>2015-04-23</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Otop</keyname><forenames>Jan</forenames></author></authors><title>Nested Weighted Automata</title><categories>cs.FL</categories><comments>The full version of the paper &quot;Nested Weighted Automata&quot; accepted to
  LICS 2015</comments><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been a significant effort to handle quantitative
properties in formal verification and synthesis. While weighted automata over
finite and infinite words provide a natural and flexible framework to express
quantitative properties, perhaps surprisingly, some basic system properties
such as average response time cannot be expressed using weighted automata, nor
in any other know decidable formalism. In this work, we introduce nested
weighted automata as a natural extension of weighted automata which makes it
possible to express important quantitative properties such as average response
time. In nested weighted automata, a master automaton spins off and collects
results from weighted slave automata, each of which computes a quantity along a
finite portion of an infinite word. Nested weighted automata can be viewed as
the quantitative analogue of monitor automata, which are used in run-time
verification. We establish an almost complete decidability picture for the
basic decision problems about nested weighted automata, and illustrate their
applicability in several domains. In particular, nested weighted automata can
be used to decide average response time properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06122</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06122</id><created>2015-04-23</created><updated>2015-11-30</updated><authors><author><keyname>Geppert</keyname><forenames>Leo N.</forenames></author><author><keyname>Ickstadt</keyname><forenames>Katja</forenames></author><author><keyname>Munteanu</keyname><forenames>Alexander</forenames></author><author><keyname>Quedenfeld</keyname><forenames>Jens</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Random projections for Bayesian regression</title><categories>stat.CO cs.DS</categories><doi>10.1007/s11222-015-9608-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with random projections applied as a data reduction
technique for Bayesian regression analysis. We show sufficient conditions under
which the entire $d$-dimensional distribution is approximately preserved under
random projections by reducing the number of data points from $n$ to $k\in
O(\operatorname{poly}(d/\varepsilon))$ in the case $n\gg d$. Under mild
assumptions, we prove that evaluating a Gaussian likelihood function based on
the projected data instead of the original data yields a
$(1+O(\varepsilon))$-approximation in terms of the $\ell_2$ Wasserstein
distance. Our main result shows that the posterior distribution of Bayesian
linear regression is approximated up to a small error depending on only an
$\varepsilon$-fraction of its defining parameters. This holds when using
arbitrary Gaussian priors or the degenerate case of uniform distributions over
$\mathbb{R}^d$ for $\beta$. Our empirical evaluations involve different
simulated settings of Bayesian linear regression. Our experiments underline
that the proposed method is able to recover the regression model up to small
error while considerably reducing the total running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06130</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06130</id><created>2015-04-23</created><updated>2015-06-12</updated><authors><author><keyname>Durand</keyname><forenames>Bruno</forenames></author><author><keyname>Romashchenko</keyname><forenames>Andrei</forenames></author></authors><title>Quasiperiodicity and non-computability in tilings</title><categories>cs.DM</categories><comments>v3: the version accepted to MFCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study tilings of the plane that combine strong properties of different
nature: combinatorial and algorithmic. We prove existence of a tile set that
accepts only quasiperiodic and non-recursive tilings. Our construction is based
on the fixed point construction; we improve this general technique and make it
enforce the property of local regularity of tilings needed for
quasiperiodicity. We prove also a stronger result: any effectively closed set
can be recursively transformed into a tile set so that the Turing degrees of
the resulted tilings consists exactly of the upper cone based on the Turing
degrees of the later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06133</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06133</id><created>2015-04-23</created><authors><author><keyname>Nicolaou</keyname><forenames>Anguelos</forenames></author><author><keyname>Bagdanov</keyname><forenames>Andrew D.</forenames></author><author><keyname>Liwicki</keyname><forenames>Marcus</forenames></author><author><keyname>Karatzas</keyname><forenames>Dimosthenis</forenames></author></authors><title>Sparse Radial Sampling LBP for Writer Identification</title><categories>cs.CV</categories><comments>Submitted to the 13th International Conference on Document Analysis
  and Recognition (ICDAR 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the use of Sparse Radial Sampling Local Binary
Patterns, a variant of Local Binary Patterns (LBP) for text-as-texture
classification. By adapting and extending the standard LBP operator to the
particularities of text we get a generic text-as-texture classification scheme
and apply it to writer identification. In experiments on CVL and ICDAR 2013
datasets, the proposed feature-set demonstrates State-Of-the-Art (SOA)
performance. Among the SOA, the proposed method is the only one that is based
on dense extraction of a single local feature descriptor. This makes it fast
and applicable at the earliest stages in a DIA pipeline without the need for
segmentation, binarization, or extraction of multiple features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06135</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06135</id><created>2015-04-23</created><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Complexity of Propositional Independence and Inclusion Logic</title><categories>cs.LO cs.CC math.LO</categories><comments>13 pages + 3 pages appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify the computational complexity of the satisfiability, validity and
model-checking problems for propositional independence and inclusion logic and
their extensions by the classical negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06136</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06136</id><created>2015-04-23</created><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Broadcast Channels with Privacy Leakage Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcast channel (BC) with one common and two private messages with
leakage constraints is studied, where leakage refers to the normalized mutual
information between a message and a channel symbol string. Each private message
is destined for a different user and the leakage to the other receiver must
satisfy a constraint. This model captures several scenarios concerning secrecy,
i.e., when both, either or neither of the private messages are secret. Inner
and outer bounds on the leakage-capacity region are derived. Without leakage
constraints the inner bound recovers Marton's region and the outer bound
reduces to the UVW-outer bound. The bounds match for semi-deterministic (SD)
and physically degraded (PD) BCs, as well as for BCs with a degraded message
set. The leakage-capacity regions of the SD-BC and the BC with a degraded
message set recover past results for different secrecy scenarios. A Blackwell
BC example illustrates the results and shows how its leakage-capacity region
changes from the capacity region without secrecy to the secrecy-capacity
regions for different secrecy scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06151</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06151</id><created>2015-04-23</created><authors><author><keyname>Shahid</keyname><forenames>Nauman</forenames></author><author><keyname>Kalofolias</keyname><forenames>Vassilis</forenames></author><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Robust Principal Component Analysis on Graphs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal Component Analysis (PCA) is the most widely used tool for linear
dimensionality reduction and clustering. Still it is highly sensitive to
outliers and does not scale well with respect to the number of data samples.
Robust PCA solves the first issue with a sparse penalty term. The second issue
can be handled with the matrix factorization model, which is however
non-convex. Besides, PCA based clustering can also be enhanced by using a graph
of data similarity. In this article, we introduce a new model called &quot;Robust
PCA on Graphs&quot; which incorporates spectral graph regularization into the Robust
PCA framework. Our proposed model benefits from 1) the robustness of principal
components to occlusions and missing values, 2) enhanced low-rank recovery, 3)
improved clustering property due to the graph smoothness assumption on the
low-rank matrix, and 4) convexity of the resulting optimization problem.
Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with
corruptions clearly reveal that our model outperforms 10 other state-of-the-art
models in its clustering and low-rank recovery tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06158</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06158</id><created>2015-04-23</created><authors><author><keyname>Mirbel</keyname><forenames>Isabelle</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Crescenzo</keyname><forenames>Pierre</forenames><affiliation>I3S</affiliation></author></authors><title>From End-User's Requirements to Web Services Retrieval: A Semantic and
  Intention-Driven Approach</title><categories>cs.SE cs.AI</categories><comments>{\'e}galement rapport de recherche I3S/RR--2010-03--FR in
  Computational Materials Science (2015). arXiv admin note: substantial text
  overlap with arXiv:1502.06735</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-14319-9_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present SATIS, a framework to derive Web Service
specifications from end-user's requirements in order to opera-tionalise
business processes in the context of a specific application domain. The aim of
SATIS is to provide to neuroscientists, which are not familiar with computer
science, a complete solution to easily find a set of Web Services to implement
an image processing pipeline. More precisely, our framework offers the
capability to capture high-level end-user's requirements in an iterative and
incremental way and to turn them into queries to retrieve Web Services
description. The whole framework relies on reusable and combinable elements
which can be shared out by a community of users sharing some interest or
problems for a given topic. In our approach, we adopt Web semantic languages
and models as a unified framework to deal with end-user's requirements and Web
Service descriptions in order to take advantage of their reasoning and
traceability capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06165</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06165</id><created>2015-04-23</created><authors><author><keyname>Gupta</keyname><forenames>Nitish</forenames></author><author><keyname>Singh</keyname><forenames>Sameer</forenames></author></authors><title>Collectively Embedding Multi-Relational Data for Predicting User
  Preferences</title><categories>cs.LG cs.IR</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix factorization has found incredible success and widespread application
as a collaborative filtering based approach to recommendations. Unfortunately,
incorporating additional sources of evidence, especially ones that are
incomplete and noisy, is quite difficult to achieve in such models, however, is
often crucial for obtaining further gains in accuracy. For example, additional
information about businesses from reviews, categories, and attributes should be
leveraged for predicting user preferences, even though this information is
often inaccurate and partially-observed. Instead of creating customized methods
that are specific to each type of evidences, in this paper we present a generic
approach to factorization of relational data that collectively models all the
relations in the database. By learning a set of embeddings that are shared
across all the relations, the model is able to incorporate observed information
from all the relations, while also predicting all the relations of interest.
Our evaluation on multiple Amazon and Yelp datasets demonstrates effective
utilization of additional information for held-out preference prediction, but
further, we present accurate models even for the cold-starting businesses and
products for which we do not observe any ratings or reviews. We also illustrate
the capability of the model in imputing missing information and jointly
visualizing words, categories, and attribute factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06170</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06170</id><created>2015-04-23</created><updated>2015-04-24</updated><authors><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>Small width, low distortions: quasi-isometric embeddings with quantized
  sub-Gaussian random projections</title><categories>cs.IT math.IT</categories><comments>Keywords: quantization, restricted isometry property, compressed
  sensing, dimensionality reduction. 26 pages, 1 figure. Remark: The abstract
  above has been shortened compared to the one given in the pdf file (due to a
  1920 characters limitation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under which conditions a subset $\mathcal K$ of $\mathbb R^N$ can be embedded
in another one of $\delta \mathbb Z^M$ for some resolution $\delta&gt;0$? We
address this general question through the specific use of a quantized random
linear mapping ${\bf A}:\mathbb R^N \to \delta \mathbb Z^M$ combining a linear
projection of $\mathbb R^N$ in $\mathbb R^M$ associated to a random matrix
$\boldsymbol \Phi \in \mathbb R^{M\times N}$ with a uniform scalar (dithered)
quantization $\mathcal Q$ of $\mathbb R^M$ in $\delta\mathbb Z^M$. The targeted
embedding relates the $\ell_2$-distance of any pair of vectors in $\mathcal K$
with the $\ell_1$-distance of their respective mappings in $\delta \mathbb
Z^M$, allowing for both multiplicative and additive distortions between these
two quantities, i.e., describing a $\ell_2/\ell_1$-quasi-isometric embedding.
We show that the sought conditions depend on the Gaussian mean width
$w(\mathcal K)$ of the subset $\mathcal K$. In particular, given a symmetric
sub-Gaussian distribution $\varphi$ and a precision $\epsilon &gt; 0$, if $M \geq
C \epsilon^{-5} w(\mathcal K)^2$ and if the sensing matrix $\boldsymbol \Phi$
has entries i.i.d. as $\varphi$, then, with high probability, the mapping $\bf
A$ provides a $\ell_2/\ell_1$-quasi-isometry between $\mathcal K$ and its image
in $\delta \mathbb Z^M$. Moreover, in this embedding, the additive distortion
is of order $\delta\epsilon$ while the multiplicative one grows with
$\epsilon$. For non-Gaussian random $\boldsymbol \Phi$, the multiplicative
error is also impacted by the sparsity of the vectors difference, i.e., being
smaller for &quot;not too sparse&quot; difference. When $\mathcal K$ is the set of
bounded $K$-sparse vectors in any orthonormal basis, then only $M \geq C
\epsilon^{-2} \log(c N/K\epsilon^{3/2})$ measurements suffice. Remark: all
values $C,c&gt;0$ above only depend on $\delta$ and on the distribution $\varphi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06177</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06177</id><created>2015-04-23</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Rawlinson S.</forenames></author><author><keyname>Barreto</keyname><forenames>Raimundo da Silva</forenames></author></authors><title>State of the Art of the Intra-Task Dynamic Voltage and Frequency Scaling
  Technique</title><categories>cs.OH</categories><comments>94 pages, in Portuguese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been an increasing use of embedded systems because
of advances in technology, the reduction of the costs of electronic equipment
and mainly the popularity of mobile devices. Many of these systems implement
low power consumption policies to extend their autonomy, usually because they
have a reduced amount of resources and the great majority of them use electric
power from batteries. One way to minimize the power consumption of these
devices is through of the application of low power consumption techniques. From
the various techniques presented in the literature - the intra-task Dynamic
Voltage and Frequency Scaling (DVFS) has played an important role. The main aim
of DVFS is to allow each task to manage the minimum resources necessary for
tasks execution, this way reducing the processor power consumption and, at the
same time, respecting the task deadlines when considered a real-time system
context. Therefore, this paper aims to apply a systematic literature review
with the goal of identifying and presenting the main methods using the
intra-task DVFS technique, applied in the context of real-time systems to
reduce energy consumption on the processor. Finally, this work will show the
advantages and disadvantages of each cataloged methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06187</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06187</id><created>2015-04-23</created><updated>2015-09-22</updated><authors><author><keyname>L&#xfc;ck</keyname><forenames>Martin</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author></authors><title>LTL Fragments are Hard for Standard Parameterisations</title><categories>cs.LO cs.CC</categories><comments>TIME 2015 conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify the complexity of the LTL satisfiability and model checking
problems for several standard parameterisations. The investigated parameters
are temporal depth, number of propositional variables and formula treewidth,
resp., pathwidth. We show that all operator fragments of LTL under the
investigated parameterisations are intractable in the sense of parameterised
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06201</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06201</id><created>2015-04-23</created><updated>2015-09-21</updated><authors><author><keyname>Bertasius</keyname><forenames>Gedas</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>High-for-Low and Low-for-High: Efficient Boundary Detection from Deep
  Object Features and its Applications to High-Level Vision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the current boundary detection systems rely exclusively on low-level
features, such as color and texture. However, perception studies suggest that
humans employ object-level reasoning when judging if a particular pixel is a
boundary. Inspired by this observation, in this work we show how to predict
boundaries by exploiting object-level features from a pretrained
object-classification network. Our method can be viewed as a &quot;High-for-Low&quot;
approach where high-level object features inform the low-level boundary
detection process. Our model achieves state-of-the-art performance on an
established boundary detection benchmark and it is efficient to run.
  Additionally, we show that due to the semantic nature of our boundaries we
can use them to aid a number of high-level vision tasks. We demonstrate that
using our boundaries we improve the performance of state-of-the-art methods on
the problems of semantic boundary labeling, semantic segmentation and object
proposal generation. We can view this process as a &quot;Low-for-High&quot; scheme, where
low-level boundaries aid high-level vision tasks.
  Thus, our contributions include a boundary detection system that is accurate,
efficient, generalizes well to multiple datasets, and is also shown to improve
existing state-of-the-art high-level vision methods on three distinct tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06203</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06203</id><created>2015-04-23</created><authors><author><keyname>Ferrarotti</keyname><forenames>Flavio</forenames></author><author><keyname>Schewe</keyname><forenames>Klaus-Dieter</forenames></author><author><keyname>Tec</keyname><forenames>Loredana</forenames></author><author><keyname>Wang</keyname><forenames>Qing</forenames></author></authors><title>A New Thesis concerning Synchronised Parallel Computing - Simplified
  Parallel ASM Thesis</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A behavioural theory consists of machine-independent postulates
characterizing a particular class of algorithms or systems, an abstract machine
model that provably satisfies these postulates, and a rigorous proof that any
algorithm or system stipulated by the postulates is captured by the abstract
machine model. The class of interest in this article is that of synchronous
parallel algorithms. For this class a behavioural theory has already been
developed by Blass and Gurevich, which unfortunately, though mathematically
correct, fails to be convincing, as it is not intuitively clear that the
postulates really capture the essence of (synchronous) parallel algorithms.
  In this article we present a much simpler (and presumably more convincing)
set of four postulates for (synchronous) parallel algorithms, which are rather
close to those used in Gurevich's celebrated sequential ASM thesis, i.e. the
behavioural theory of sequential algorithms. The key difference is made by an
extension of the bounded exploration postulate using multiset comprehension
terms instead of ground terms formulated over the signature of the states. In
addition, all implicit assumptions are made explicit, which amounts to
considering states of a parallel algorithm to be represented by meta-finite
first-order structures.
  The article first provides the necessary evidence that the axiomatization
presented in this article characterizes indeed the whole class of
deterministic, synchronous, parallel algorithms, then formally proves that
parallel algorithms are captured by Abstract State Machines (ASMs). The proof
requires some recourse to methods from finite model theory, by means of which
it can be shown that if a critical tuple defines an update in some update set,
then also every other tuple that is logically indistinguishable defines an
update in that update set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06204</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06204</id><created>2015-04-23</created><authors><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Gati</keyname><forenames>Rudolf</forenames></author></authors><title>Order Reduction of the Radiative Heat Transfer Model for the Simulation
  of Plasma Arcs</title><categories>math.OC cs.SY physics.comp-ph</categories><doi>10.1016/j.jqsrt.2015.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to derive low-complexity models describing thermal radiation for
the sake of simulating the behavior of electric arcs in switchgear systems is
presented. The idea is to approximate the (high dimensional) full-order
equations, modeling the propagation of the radiated intensity in space, with a
model of much lower dimension, whose parameters are identified by means of
nonlinear system identification techniques. The low-order model preserves the
main structural aspects of the full-order one, and its parameters can be
straightforwardly used in arc simulation tools based on computational fluid
dynamics. In particular, the model parameters can be used together with the
common approaches to resolve radiation in magnetohydrodynamic simulations,
including the discrete-ordinate method, the P-N methods and photohydrodynamics.
The proposed order reduction approach is able to systematically compute the
partitioning of the electromagnetic spectrum in frequency bands, and the
related absorption coefficients, that yield the best matching with respect to
the finely resolved absorption spectrum of the considered gaseous medium. It is
shown how the problem's structure can be exploited to improve the computational
efficiency when solving the resulting nonlinear optimization problem. In
addition to the order reduction approach and the related computational aspects,
an analysis by means of Laplace transform is presented, providing a
justification to the use of very low orders in the reduction procedure as
compared with the full-order model. Finally, comparisons between the full-order
model and the reduced-order ones are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06206</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06206</id><created>2015-04-23</created><authors><author><keyname>Figueiredo</keyname><forenames>Isabel N.</forenames></author><author><keyname>Leal</keyname><forenames>Carlos</forenames></author><author><keyname>Pinto</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Figueiredo</keyname><forenames>Pedro N.</forenames></author><author><keyname>Tsai</keyname><forenames>Richard</forenames></author></authors><title>An Elastic Image Registration Approach for Wireless Capsule Endoscope
  Localization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits
physicians to examine all the areas of the Gastrointestinal (GI) tract. It is
especially important for the small intestine, where traditional invasive
endoscopies cannot reach. Although WCE represents an extremely important
advance in medical imaging, a major drawback that remains unsolved is the WCE
precise location in the human body during its operating time. This is mainly
due to the complex physiological environment and the inherent capsule effects
during its movement. When an abnormality is detected, in the WCE images,
medical doctors do not know precisely where this abnormality is located
relative to the intestine and therefore they can not proceed efficiently with
the appropriate therapy. The primary objective of the present paper is to give
a contribution to WCE localization, using image-based methods. The main focus
of this work is on the description of a multiscale elastic image registration
approach, its experimental application on WCE videos, and comparison with a
multiscale affine registration. The proposed approach includes registrations
that capture both rigid-like and non-rigid deformations, due respectively to
the rigid-like WCE movement and the elastic deformation of the small intestine
originated by the GI peristaltic movement. Under this approach a qualitative
information about the WCE speed can be obtained, as well as the WCE location
and orientation via projective geometry. The results of the experimental tests
with real WCE video frames show the good performance of the proposed approach,
when elastic deformations of the small intestine are involved in successive
frames, and its superiority with respect to a multiscale affine image
registration, which accounts for rigid-like deformations only and discards
elastic deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06213</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06213</id><created>2015-04-23</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>Sums of products of polynomials in few variables : lower bounds and
  polynomial identity testing</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of representing polynomials as a sum of products of
polynomials in few variables. More precisely, we study representations of the
form $$P = \sum_{i = 1}^T \prod_{j = 1}^d Q_{ij}$$ such that each $Q_{ij}$ is
an arbitrary polynomial that depends on at most $s$ variables. We prove the
following results.
  1. Over fields of characteristic zero, for every constant $\mu$ such that $0
\leq \mu &lt; 1$, we give an explicit family of polynomials $\{P_{N}\}$, where
$P_{N}$ is of degree $n$ in $N = n^{O(1)}$ variables, such that any
representation of the above type for $P_{N}$ with $s = N^{\mu}$ requires $Td
\geq n^{\Omega(\sqrt{n})}$. This strengthens a recent result of Kayal and Saha
[KS14a] which showed similar lower bounds for the model of sums of products of
linear forms in few variables. It is known that any asymptotic improvement in
the exponent of the lower bounds (even for $s = \sqrt{n}$) would separate VP
and VNP[KS14a].
  2. We obtain a deterministic subexponential time blackbox polynomial identity
testing (PIT) algorithm for circuits computed by the above model when $T$ and
the individual degree of each variable in $P$ are at most $\log^{O(1)} N$ and
$s \leq N^{\mu}$ for any constant $\mu &lt; 1/2$. We get quasipolynomial running
time when $s &lt; \log^{O(1)} N$. The PIT algorithm is obtained by combining our
lower bounds with the hardness-randomness tradeoffs developed in [DSY09, KI04].
To the best of our knowledge, this is the first nontrivial PIT algorithm for
this model (even for the case $s=2$), and the first nontrivial PIT algorithm
obtained from lower bounds for small depth circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06231</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06231</id><created>2015-04-23</created><updated>2015-08-07</updated><authors><author><keyname>Pedersen</keyname><forenames>Jesper</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>Repair Scheduling in Wireless Distributed Storage with D2D Communication</title><categories>cs.IT math.IT</categories><comments>To be presented at IEEE Information Theory Workshop (ITW) 2015, Jeju
  Island, Korea, October 2015</comments><doi>10.1109/ITWF.2015.7360736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed storage (DS) for a wireless network where mobile
devices arrive and depart according to a Poisson random process. Content is
stored in a number of mobile devices, using an erasure correcting code. When
requesting a piece of content, a user retrieves the content from the mobile
devices using device-to-device communication or, if not possible, from the base
station (BS), at the expense of a higher communication cost. We consider the
repair problem when a device that stores data leaves the network. In
particular, we introduce a repair scheduling where repair is performed (from
storage devices or the BS) periodically. We derive analytical expressions for
the overall communication cost of repair and download as a function of the
repair interval. We illustrate the analysis by giving results for maximum
distance separable codes and regenerating codes. Our results indicate that DS
can reduce the overall communication cost with respect to the case where
content is only downloaded from the BS, provided that repairs are performed
frequently enough. The required repair frequency depends on the code used for
storage and the network parameters. In particular, minimum bandwidth
regenerating codes require very frequent repairs, while maximum distance
separable codes give better performance if repair is performed less frequently.
We also show that instantaneous repair is not always optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06234</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06234</id><created>2015-04-23</created><authors><author><keyname>Chen</keyname><forenames>Jijuan</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Huiqin</forenames></author></authors><title>Acyclic chromatic index of triangle-free $1$-planar graphs</title><categories>math.CO cs.DM</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1302.2405, arXiv:1405.0713</comments><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that
every cycle is colored with at least three colors. The acyclic chromatic index
$\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic
edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) +
2$ for any simple graph $G$ with maximum degree $\Delta(G)$. A graph is {\em
$1$-planar} if it can be drawn on the plane such that every edge is crossed by
at most one other edge. In this paper, we prove that every triangle-free
$1$-planar graph $G$ admits an acyclic edge coloring with $\Delta(G) + 17$
colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06236</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06236</id><created>2015-04-23</created><updated>2015-04-24</updated><authors><author><keyname>Sheikhahmadi</keyname><forenames>Amir</forenames></author><author><keyname>Nematbakhsh</keyname><forenames>Mohammad A.</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Arman</forenames></author></authors><title>Improving detection of influential nodes in complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 8 figures, 5 tables . . . accepted for publication in
  Physica A: Statistical Mechanics and its Applications</comments><doi>10.1016/j.physa.2015.04.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently an increasing amount of research is devoted to the question of how
the most influential nodes (seeds) can be found effectively in a complex
network. There are a number of measures proposed for this purpose, for
instance, high-degree centrality measure reflects the importance of the network
topology and has a reasonable runtime performance to find a set of nodes with
highest degree, but they do not have a satisfactory dissemination potentiality
in the network due to having many common neighbors ($\mbox{CN}^{(1)}$) and
common neighbors of neighbors ($\mbox{CN}^{(2)}$). This flaw holds in other
measures as well. In this paper, we compare high-degree centrality measure with
other well-known measures using ten datasets in order to find a proportion for
the common seeds in the seed sets obtained by them. We, thereof, propose an
improved high-degree centrality measure (named DegreeDistance) and improve it
to enhance accuracy in two phases, FIDD and SIDD, by putting a threshold on the
number of common neighbors of already-selected seed nodes and a non-seed node
which is under investigation to be selected as a seed as well as considering
the influence score of seed nodes directly or through their common neighbors
over the non-seed node. To evaluate the accuracy and runtime performance of
DegreeDistance, FIDD, and SIDD, they are applied to eight large-scale networks
and it finally turns out that SIDD dramatically outperforms other well-known
measures and evinces comparatively more accurate performance in identifying the
most influential nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06238</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06238</id><created>2015-04-23</created><authors><author><keyname>Cai</keyname><forenames>Xing Shi</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author></authors><title>The graph structure of a deterministic automaton chosen at random: full
  version</title><categories>math.PR cs.FL math.CO</categories><comments>47 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deterministic finite automaton (DFA) of $n$ states over a $k$-letter
alphabet can be seen as a digraph with $n$ vertices which all have exactly $k$
labeled out-arcs ($k$-out digraph). In 1973 Grusho first proved that with high
probability (whp) in a random $k$-out digraph there is a strongly connected
component (SCC) of linear size that is reachable from all vertices, i.e., a
giant. He also proved that the size of the giant follows a central limit law.
We show that whp the part outside the giant contains at most a few short cycles
and mostly consists of overlapping tree-like structures. Thus the directed
acyclic graph (DAG) of a random $k$-out digraph is almost the same as the
digraph with the giant contracted into one vertex. These findings lead to a
new, concise and self-contained proof of Grusho's theorem. This work also
contains some other results including the structure outside the giant, the
phase transition phenomenon in strong connectivity, the typical distance, and
an extension to simple digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06240</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06240</id><created>2015-04-23</created><authors><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>A Computable Measure of Algorithmic Probability by Finite Approximations</title><categories>cs.IT cs.CC cs.FL math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study formal properties of a Levin-inspired measure $m$ calculated from
the output distribution of small Turing machines. We introduce and justify
finite approximations $m_k$ that have already been used in applications as an
alternative to lossless compression algorithms for approximating algorithmic
(Kolmogorov-Chaitin) complexity. We provide proofs of the relevant properties
of both $m$ and $m_k$ and compare them to Levin's Universal Distribution.
Finally, we provide error estimations of $m_k$ with respect to $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06242</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06242</id><created>2015-04-23</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Fontaine</keyname><forenames>Allyx</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>Dictionary matching in a stream</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of dictionary matching in a stream. Given a set of
strings, known as a dictionary, and a stream of characters arriving one at a
time, the task is to report each time some string in our dictionary occurs in
the stream. We present a randomised algorithm which takes O(log log(k + m))
time per arriving character and uses O(k log m) words of space, where k is the
number of strings in the dictionary and m is the length of the longest string
in the dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06243</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06243</id><created>2015-04-23</created><authors><author><keyname>Shen</keyname><forenames>Yang</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Yan</keyname><forenames>Junchi</forenames></author><author><keyname>Xu</keyname><forenames>Mingliang</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>Person Re-identification with Correspondence Structure Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of handling spatial misalignments due to
camera-view changes or human-pose variations in person re-identification. We
first introduce a boosting-based approach to learn a correspondence structure
which indicates the patch-wise matching probabilities between images from a
target camera pair. The learned correspondence structure can not only capture
the spatial correspondence pattern between cameras but also handle the
viewpoint or human-pose variation in individual images. We further introduce a
global-based matching process. It integrates a global matching constraint over
the learned correspondence structure to exclude cross-view misalignments during
the image patch matching process, hence achieving a more reliable matching
score between images. Experimental results on various datasets demonstrate the
effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06247</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06247</id><created>2015-04-23</created><updated>2015-09-28</updated><authors><author><keyname>Che</keyname><forenames>Tiben</forenames></author><author><keyname>Xu</keyname><forenames>Jingwei</forenames></author><author><keyname>Choi</keyname><forenames>Gwan</forenames></author></authors><title>TC: Throughput Centric Successive Cancellation Decoder Hardware
  Implementation for Polar Codes</title><categories>cs.IT math.IT</categories><comments>submitted to ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hardware architecture of fast simplified successive
cancellation (fast-SSC) algorithm for polar codes, which significantly reduces
the decoding latency and dramatically increases the throughput.
Algorithmically, fast-SSC algorithm suffers from the fact that its decoder
scheduling and the consequent architecture depends on the code rate; this is a
challenge for rate-compatible system. However, by exploiting the
homogeneousness between the decoding processes of fast constituent polar codes
and regular polar codes, the presented design is compatible with any rate. The
scheduling plan and the intendedly designed process core are also described.
Results show that, compared with the state-of-art decoder, proposed design can
achieve at least 60% latency reduction for the codes with length N = 1024. By
using Nangate FreePDK 45nm process, proposed design can reach throughput up to
5.81 Gbps and 2.01 Gbps for (1024, 870) and (1024, 512) polar code,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06249</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06249</id><created>2015-04-23</created><updated>2015-08-27</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Kiani</keyname><forenames>Narsis A.</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>Quantifying Loss of Information in Network-based Dimensionality
  Reduction Techniques</title><categories>q-bio.MN cs.IT math.IT q-bio.QM</categories><comments>29 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cope with the complexity of large networks, a number of dimensionality
reduction techniques for graphs have been developed. However, the extent to
which information is lost or preserved when these techniques are employed has
not yet been clear. Here we develop a framework, based on algorithmic
information theory, to quantify the extent to which information is preserved
when network motif analysis, graph spectra and spectral sparsification methods
are applied to over twenty different biological and artificial networks. We
find that the spectral sparsification is highly sensitive to high number of
edge deletion, leading to significant inconsistencies, and that graph spectral
methods are the most irregular, capturing algebraic information in a condensed
fashion but largely losing most of the information content of the original
networks. However, the approach shows that network motif analysis excels at
preserving the relative algorithmic information content of a network, hence
validating and generalizing the remarkable fact that despite their inherent
combinatorial possibilities, local regularities preserve information to such an
extent that essential properties are fully recoverable across different
networks to determine their family group to which they belong to (eg genetic vs
social network). Our algorithmic information methodology thus provides a
rigorous framework enabling a fundamental assessment and comparison between
different data dimensionality reduction methods thereby facilitating the
identification and evaluation of the capabilities of old and new methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06260</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06260</id><created>2015-04-23</created><updated>2015-10-01</updated><authors><author><keyname>Paix&#xe3;o</keyname><forenames>Tiago</forenames></author><author><keyname>Heredia</keyname><forenames>Jorge P&#xe9;rez</forenames></author><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author><author><keyname>Trubenov&#xe1;</keyname><forenames>Barbora</forenames></author></authors><title>First Steps Towards a Runtime Comparison of Natural and Artificial
  Evolution</title><categories>cs.NE</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms (EAs) form a popular optimisation paradigm inspired
by natural evolution. In recent years the field of evolutionary computation has
developed a rigorous analytical theory to analyse their runtime on many
illustrative problems. Here we apply this theory to a simple model of natural
evolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime the
time between occurrence of new mutations is much longer than the time it takes
for a new beneficial mutation to take over the population. In this situation,
the population only contains copies of one genotype and evolution can be
modelled as a (1+1)-type process where the probability of accepting a new
genotype (improvements or worsenings) depends on the change in fitness.
  We present an initial runtime analysis of SSWM, quantifying its performance
for various parameters and investigating differences to the (1+1)EA. We show
that SSWM can have a moderate advantage over the (1+1)EA at crossing fitness
valleys and study an example where SSWM outperforms the (1+1)EA by taking
advantage of information on the fitness gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06262</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06262</id><created>2015-04-23</created><updated>2015-06-23</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Lemieux</keyname><forenames>Yves</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>40 Gbps Access for Metro networks: Implications in terms of
  Sustainability and Innovation from an LCA Perspective</title><categories>cs.CY cs.MM cs.NI</categories><comments>10 pages, 6 Tables, 1 Figure. Accepted to be presented at the
  ICT4S'15 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the implications of new technologies, more specifically the new
optical FTTH technologies, are studied both from the functional and
non-functional perspectives. In particular, some direct impacts are listed in
the form of abandoning non-functional technologies, such as micro-registration,
which would be implicitly required for having a functioning operation before
arrival the new high-bandwidth access technologies. It is shown that such
abandonment of non-functional best practices, which are mainly at the
management level of ICT, immediately results in additional consumption and
environmental footprint, and also there is a chance that some other new
innovations might be 'missed.' Therefore, unconstrained deployment of these
access technologies is not aligned with a possible sustainable ICT picture,
except if they are regulated. An approach to pricing the best practices,
including both functional and non-functional technologies, is proposed in order
to develop a regulation and policy framework for a sustainable broadband
access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06266</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06266</id><created>2015-04-23</created><authors><author><keyname>Othman</keyname><forenames>Ahmed</forenames></author><author><keyname>Tizhoosh</keyname><forenames>Hamid R.</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author></authors><title>Evolving Fuzzy Image Segmentation with Self-Configuration</title><categories>cs.CV</categories><comments>Benchmark data (35 breast ultrasound images with gold standard
  segments) available; 11 pages, 4 algorithms, 6 figures, 5 tables;</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Current image segmentation techniques usually require that the user tune
several parameters in order to obtain maximum segmentation accuracy, a
computationally inefficient approach, especially when a large number of images
must be processed sequentially in daily practice. The use of evolving fuzzy
systems for designing a method that automatically adjusts parameters to segment
medical images according to the quality expectation of expert users has been
proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS
suffers from a few limitations when used in practice mainly due to some fixed
parameters. For instance, EFIS depends on auto-detection of the object of
interest for feature calculation, a task that is highly application-dependent.
This shortcoming limits the applicability of EFIS, which was proposed with the
ultimate goal of offering a generic but adjustable segmentation scheme. In this
paper, a new version of EFIS is proposed to overcome these limitations. The new
EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to
self-estimate the parameters that are fixed in EFIS. As well, the proposed
SC-EFIS relies on a feature selection process that does not require
auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same
segmentation algorithms and the same dataset as for EFIS. The results show that
SC-EFIS can provide the same results as EFIS but with a higher level of
automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06274</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06274</id><created>2015-04-23</created><authors><author><keyname>Mao</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author></authors><title>A new approach for physiological time series</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a new approach for the analysis of physiological time series. An
iterative convolution filter is used to decompose the time series into various
components. Statistics of these components are extracted as features to
characterize the mechanisms underlying the time series. Motivated by the
studies that show many normal physiological systems involve irregularity while
the decrease of irregularity usually implies the abnormality, the statistics
for &quot;outliers&quot; in the components are used as features measuring irregularity.
Support vector machines are used to select the most relevant features that are
able to differentiate the time series from normal and abnormal systems. This
new approach is successfully used in the study of congestive heart failure by
heart beat interval time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06305</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06305</id><created>2015-04-23</created><authors><author><keyname>Slawski</keyname><forenames>Martin</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Regularization-free estimation in trace regression with symmetric
  positive semidefinite matrices</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, trace regression models have received considerable
attention in the context of matrix completion, quantum state tomography, and
compressed sensing. Estimation of the underlying matrix from
regularization-based approaches promoting low-rankedness, notably nuclear norm
regularization, have enjoyed great popularity. In the present paper, we argue
that such regularization may no longer be necessary if the underlying matrix is
symmetric positive semidefinite (\textsf{spd}) and the design satisfies certain
conditions. In this situation, simple least squares estimation subject to an
\textsf{spd} constraint may perform as well as regularization-based approaches
with a proper choice of the regularization parameter, which entails knowledge
of the noise level and/or tuning. By contrast, constrained least squares
estimation comes without any tuning parameter and may hence be preferred due to
its simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06314</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06314</id><created>2015-04-23</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author></authors><title>Finding Any Nontrivial Coarse Correlated Equilibrium Is Hard</title><categories>cs.GT</categories><comments>21 pages</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most appealing aspects of the (coarse) correlated equilibrium
concept is that natural dynamics quickly arrive at approximations of such
equilibria, even in games with many players. In addition, there exist
polynomial-time algorithms that compute exact (coarse) correlated equilibria.
In light of these results, a natural question is how good are the (coarse)
correlated equilibria that can arise from any efficient algorithm or dynamics.
  In this paper we address this question, and establish strong negative
results. In particular, we show that in multiplayer games that have a succinct
representation, it is NP-hard to compute any coarse correlated equilibrium (or
approximate coarse correlated equilibrium) with welfare strictly better than
the worst possible. The focus on succinct games ensures that the underlying
complexity question is interesting; many multiplayer games of interest are in
fact succinct. Our results imply that, while one can efficiently compute a
coarse correlated equilibrium, one cannot provide any nontrivial welfare
guarantee for the resulting equilibrium, unless P=NP. We show that analogous
hardness results hold for correlated equilibria, and persist under the
egalitarian objective or Pareto optimality.
  To complement the hardness results, we develop an algorithmic framework that
identifies settings in which we can efficiently compute an approximate
correlated equilibrium with near-optimal welfare. We use this framework to
develop an efficient algorithm for computing an approximate correlated
equilibrium with near-optimal welfare in aggregative games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06316</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06316</id><created>2015-04-23</created><updated>2015-08-13</updated><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>Hayes</keyname><forenames>Thomas P.</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author><author><keyname>Young</keyname><forenames>Maxwell</forenames></author></authors><title>Interactive Communication with Unknown Noise Rate</title><categories>cs.DS cs.DC cs.IT cs.NI math.IT</categories><comments>Made substantial improvements to the algorithm and analysis. Previous
  version had a subtle error involving the adversary's ability to attack
  fingerprints</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alice and Bob want to run a protocol over a noisy channel, where a certain
number of bits are flipped adversarially. Several results take a protocol
requiring $L$ bits of noise-free communication and make it robust over such a
channel. In a recent breakthrough result, Haeupler described an algorithm that
sends a number of bits that is conjectured to be near optimal in such a model.
However, his algorithm critically requires $a \ priori$ knowledge of the number
of bits that will be flipped by the adversary.
  We describe an algorithm requiring no such knowledge. If an adversary flips
$T$ bits, our algorithm sends $L + O\left(\sqrt{L(T+1)\log L} + T\right)$ bits
in expectation and succeeds with high probability in $L$. It does so without
any $a \ priori$ knowledge of $T$. Assuming a conjectured lower bound by
Haeupler, our result is optimal up to logarithmic factors.
  Our algorithm critically relies on the assumption of a private channel. We
show that privacy is necessary when the amount of noise is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06320</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06320</id><created>2015-04-23</created><updated>2015-11-24</updated><authors><author><keyname>Wiley</keyname><forenames>Keith B.</forenames></author><author><keyname>Koene</keyname><forenames>Randal A.</forenames></author></authors><title>The Fallacy of Favoring Gradual Replacement Mind Uploading Over
  Scan-and-Copy</title><categories>cs.OH</categories><comments>14 pages, 0 figures; this version was resubmitted to the journal
  after the first review phase; the final version has been accepted for 2016
  publication but cannot be publicly provided due to copyright</comments><acm-class>I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mind uploading speculation and debate often concludes that a procedure
described as gradual in-place replacement preserves personal identity while a
procedure described as destructive scan-and-copy produces some other identity
in the target substrate such that personal identity is lost along with the
biological brain. This paper demonstrates a chain of reasoning that establishes
metaphysical equivalence between these two methods in terms of preserving
personal identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06329</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06329</id><created>2015-04-23</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Grothendieck</keyname><forenames>John</forenames></author></authors><title>Analysis of Stopping Active Learning based on Stabilizing Predictions</title><categories>cs.LG cs.CL stat.ML</categories><comments>10 pages, 8 tables; appeared in Proceedings of the Seventeenth
  Conference on Computational Natural Language Learning, August 2013</comments><acm-class>I.5.1; I.5.4; G.3; I.2.7; I.2.6</acm-class><journal-ref>In Proceedings of the Seventeenth Conference on Computational
  Natural Language Learning, pages 10-19, Sofia, Bulgaria, August 2013.
  Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the natural language processing (NLP) community, active learning has
been widely investigated and applied in order to alleviate the annotation
bottleneck faced by developers of new NLP systems and technologies. This paper
presents the first theoretical analysis of stopping active learning based on
stabilizing predictions (SP). The analysis has revealed three elements that are
central to the success of the SP method: (1) bounds on Cohen's Kappa agreement
between successively trained models impose bounds on differences in F-measure
performance of the models; (2) since the stop set does not have to be labeled,
it can be made large in practice, helping to guarantee that the results
transfer to previously unseen streams of examples at test/application time; and
(3) good (low variance) sample estimates of Kappa between successive models can
be obtained. Proofs of relationships between the level of Kappa agreement and
the difference in performance between consecutive models are presented.
Specifically, if the Kappa agreement between two models exceeds a threshold T
(where $T&gt;0$), then the difference in F-measure performance between those
models is bounded above by $\frac{4(1-T)}{T}$ in all cases. If precision of the
positive conjunction of the models is assumed to be $p$, then the bound can be
tightened to $\frac{4(1-T)}{(p+1)T}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06341</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06341</id><created>2015-04-23</created><authors><author><keyname>Schipper</keyname><forenames>Burkhard C.</forenames></author></authors><title>Strategic Teaching and Learning in Games</title><categories>cs.GT cs.AI cs.LG</categories><proxy>Burkhard Schipper</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that there are uncoupled learning heuristics leading to Nash
equilibrium in all finite games. Why should players use such learning
heuristics and where could they come from? We show that there is no uncoupled
learning heuristic leading to Nash equilibrium in all finite games that a
player has an incentive to adopt, that would be evolutionary stable or that
could &quot;learn itself&quot;. Rather, a player has an incentive to strategically teach
such a learning opponent in order secure at least the Stackelberg leader
payoff. The impossibility result remains intact when restricted to the classes
of generic games, two-player games, potential games, games with strategic
complements or 2x2 games, in which learning is known to be &quot;nice&quot;. More
generally, it also applies to uncoupled learning heuristics leading to
correlated equilibria, rationalizable outcomes, iterated admissible outcomes,
or minimal curb sets. A possibility result restricted to &quot;strategically
trivial&quot; games fails if some generic games outside this class are considered as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06350</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06350</id><created>2015-04-23</created><authors><author><keyname>Gibson</keyname><forenames>Matt</forenames></author><author><keyname>Krohn</keyname><forenames>Erik</forenames></author><author><keyname>Wang</keyname><forenames>Qing</forenames></author></authors><title>A Characterization of Visibility Graphs for Pseudo-Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a characterization of the visibility graphs of
pseudo-polygons. We first identify some key combinatorial properties of
pseudo-polygons, and we then give a set of five necessary conditions based off
our identified properties. We then prove that these necessary conditions are
also sufficient via a reduction to a characterization of vertex-edge visibility
graphs given by O'Rourke and Streinu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06353</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06353</id><created>2015-04-23</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>A System Structure for Adaptive Mobile Applications</title><categories>cs.SE</categories><comments>In Proc. of the Sixth IEEE Int.l Symposium on a World of Wireless,
  Mobile and Multimedia Networks (WoWMoM 2005)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system structure for adaptive mobile applications is introduced and
discussed, together with a compliant architecture and a prototypic
implementation. A methodology is also introduced, which exploits our structure
to decompose the behavior of non stable systems into a set of quasi-stable
scenarios. Within each of these scenarios we can exploit the knowledge of the
available QoS figures to express simpler and better adaptation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06355</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06355</id><created>2015-04-23</created><updated>2016-01-09</updated><authors><author><keyname>Decker</keyname><forenames>Normann</forenames></author><author><keyname>Thoma</keyname><forenames>Daniel</forenames></author></authors><title>On Freeze LTL with Ordered Attributes</title><categories>cs.LO</categories><comments>Extended version of article published in proceedings of FoSSaCS 2016</comments><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with Freeze LTL, a temporal logic on data words with
registers. In a (multi-attributed) data word each position carries a letter
from a finite alphabet and assigns a data value to a fixed, finite set of
attributes. The satisfiability problem of Freeze LTL is undecidable if more
than one register is available or tuples of data values can be stored and
compared arbitrarily. Starting from the decidable one-register fragment we
propose an extension that allows for specifying a dependency relation on
attributes. This restricts in a flexible way how collections of attribute
values can be stored and compared. This conceptual dimension is orthogonal to
the number of registers or the available temporal operators. The extension is
strict. Admitting arbitrary dependency relations satisfiability becomes
undecidable. Tree-like relations, however, induce a family of decidable
fragments escalating the ordinal-indexed hierarchy of fast-growing complexity
classes, a recently introduced framework for non-primitive recursive
complexities. This results in completeness for the class ${\bf
F}_{\epsilon_0}$. We employ nested counter systems and show that they relate to
the hierarchy in terms of the nesting depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06357</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06357</id><created>2015-04-23</created><authors><author><keyname>Hollis</keyname><forenames>Simon J.</forenames></author><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author></authors><title>Overview of Swallow --- A Scalable 480-core System for Investigating the
  Performance and Energy Efficiency of Many-core Applications and Operating
  Systems</title><categories>cs.DC cs.AR cs.DS</categories><comments>An open source release of the Swallow system design and code will
  follow and references to these will be added at a later date</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Swallow, a scalable many-core architecture, with a current
configuration of 480 x 32-bit processors.
  Swallow is an open-source architecture, designed from the ground up to
deliver scalable increases in usable computational power to allow
experimentation with many-core applications and the operating systems that
support them.
  Scalability is enabled by the creation of a tile-able system with a
low-latency interconnect, featuring an attractive communication-to-computation
ratio and the use of a distributed memory configuration.
  We analyse the energy and computational and communication performances of
Swallow. The system provides 240GIPS with each core consuming 71--193mW,
dependent on workload. Power consumption per instruction is lower than almost
all systems of comparable scale.
  We also show how the use of a distributed operating system (nOS) allows the
easy creation of scalable software to exploit Swallow's potential. Finally, we
show two use case studies: modelling neurons and the overlay of shared memory
on a distributed memory system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06359</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06359</id><created>2015-04-23</created><updated>2015-09-07</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Halawani</keyname><forenames>Alaa</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author><author><keyname>Rehman</keyname><forenames>Shafiq ur</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Preprint Touch-less Interactive Augmented Reality Game on Vision Based
  Wearable Device</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on Personal and Ubiquitous
  Computing</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on Personal and Ubiquitous
Computing. There is an increasing interest in creating pervasive games based on
emerging interaction technologies. In order to develop touch-less, interactive
and augmented reality games on vision-based wearable device, a touch-less
motion interaction technology is designed and evaluated in this work. Users
interact with the augmented reality games with dynamic hands/feet gestures in
front of the camera, which triggers the interaction event to interact with the
virtual object in the scene. Three primitive augmented reality games with
eleven dynamic gestures are developed based on the proposed touch-less
interaction technology as proof. At last, a comparing evaluation is proposed to
demonstrate the social acceptability and usability of the touch-less approach,
running on a hybrid wearable framework or with Google Glass, as well as
workload assessment, user's emotions and satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06363</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06363</id><created>2015-04-23</created><authors><author><keyname>Neumann</keyname><forenames>Frank</forenames></author><author><keyname>Witt</keyname><forenames>Carsten</forenames></author></authors><title>On the Runtime of Randomized Local Search and Simple Evolutionary
  Algorithms for Dynamic Makespan Scheduling</title><categories>cs.DS cs.NE</categories><comments>Conference version appears at IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms have been frequently used for dynamic optimization
problems. With this paper, we contribute to the theoretical understanding of
this research area. We present the first computational complexity analysis of
evolutionary algorithms for a dynamic variant of a classical combinatorial
optimization problem, namely makespan scheduling. We study the model of a
strong adversary which is allowed to change one job at regular intervals.
Furthermore, we investigate the setting of random changes. Our results show
that randomized local search and a simple evolutionary algorithm are very
effective in dynamically tracking changes made to the problem instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06366</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06366</id><created>2015-04-23</created><authors><author><keyname>Sakthithasan</keyname><forenames>Sripirakas</forenames></author><author><keyname>Pears</keyname><forenames>Russel</forenames></author><author><keyname>Bifet</keyname><forenames>Albert</forenames></author><author><keyname>Pfahringer</keyname><forenames>Bernhard</forenames></author></authors><title>Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in
  Data Streams</title><categories>cs.AI cs.LG</categories><comments>This paper has been accepted for IJCNN 2015 conference, Ireland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, we apply ensembles of Fourier encoded spectra to capture
and mine recurring concepts in a data stream environment. Previous research
showed that compact versions of Decision Trees can be obtained by applying the
Discrete Fourier Transform to accurately capture recurrent concepts in a data
stream. However, in highly volatile environments where new concepts emerge
often, the approach of encoding each concept in a separate spectrum is no
longer viable due to memory overload and thus in this research we present an
ensemble approach that addresses this problem. Our empirical results on real
world data and synthetic data exhibiting varying degrees of recurrence reveal
that the ensemble approach outperforms the single spectrum approach in terms of
classification accuracy, memory and execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06374</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06374</id><created>2015-04-23</created><authors><author><keyname>Cornelio</keyname><forenames>Cristina</forenames></author><author><keyname>Loreggia</keyname><forenames>Andrea</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author></authors><title>Logical Conditional Preference Theories</title><categories>cs.AI</categories><comments>15 pages, 1 figure, submitted to CP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CP-nets represent the dominant existing framework for expressing qualitative
conditional preferences between alternatives, and are used in a variety of
areas including constraint solving. Over the last fifteen years, a significant
literature has developed exploring semantics, algorithms, implementation and
use of CP-nets. This paper introduces a comprehensive new framework for
conditional preferences: logical conditional preference theories (LCP
theories). To express preferences, the user specifies arbitrary (constraint)
Datalog programs over a binary ordering relation on outcomes. We show how LCP
theories unify and generalize existing conditional preference proposals, and
leverage the rich semantic, algorithmic and implementation frameworks of
Datalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06375</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06375</id><created>2015-04-23</created><updated>2015-10-03</updated><authors><author><keyname>Xie</keyname><forenames>Saining</forenames></author><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author></authors><title>Holistically-Nested Edge Detection</title><categories>cs.CV</categories><comments>v2 Add appendix A for updated results (ODS=0.790) on BSDS-500 in a
  new experiment setting. Fix typos and reorganize formulations. Add Table 2 to
  discuss the role of deep supervision. Add links to publicly available
  repository for code, models and data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new edge detection algorithm that tackles two important issues
in this long-standing vision problem: (1) holistic image training and
prediction; and (2) multi-scale and multi-level feature learning. Our proposed
method, holistically-nested edge detection (HED), performs image-to-image
prediction by means of a deep learning model that leverages fully convolutional
neural networks and deeply-supervised nets. HED automatically learns rich
hierarchical representations (guided by deep supervision on side responses)
that are important in order to approach the human ability resolve the
challenging ambiguity in edge and object boundary detection. We significantly
advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and
the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed
(0.4 second per image) that is orders of magnitude faster than some recent
CNN-based edge detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06378</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06378</id><created>2015-04-23</created><updated>2015-05-06</updated><authors><author><keyname>Supancic</keyname><forenames>James Steven</forenames><suffix>III</suffix></author><author><keyname>Rogez</keyname><forenames>Gregory</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Shotton</keyname><forenames>Jamie</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Depth-based hand pose estimation: methods, data, and challenges</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand pose estimation has matured rapidly in recent years. The introduction of
commodity depth sensors and a multitude of practical applications have spurred
new advances. We provide an extensive analysis of the state-of-the-art,
focusing on hand pose estimation from a single depth frame. To do so, we have
implemented a considerable number of systems, and will release all software and
evaluation code. We summarize important conclusions here: (1) Pose estimation
appears roughly solved for scenes with isolated hands. However, methods still
struggle to analyze cluttered scenes where hands may be interacting with nearby
objects and surfaces. To spur further progress we introduce a challenging new
dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves
with disparate criteria, making comparisons difficult. We define a consistent
evaluation criteria, rigorously motivated by human experiments. (3) We
introduce a simple nearest-neighbor baseline that outperforms most existing
systems. This implies that most systems do not generalize beyond their training
sets. This also reinforces the under-appreciated point that training data is as
important as the model itself. We conclude with directions for future progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06387</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06387</id><created>2015-04-24</created><updated>2016-01-10</updated><authors><author><keyname>Narasimha</keyname><forenames>Srinath</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author><author><keyname>Sunny</keyname><forenames>Albert</forenames></author></authors><title>Throughput Optimal and Fast Near-Optimal Scheduling with Heterogeneously
  Delayed Network-State Information (Extended Version)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed scheduling in wireless networks where
heterogeneously delayed information about queue lengths and channel states of
all links are available at all the transmitters. In an earlier work (by Reddy
et al. in Queueing Systems, 2012), a throughput optimal scheduling policy
(which we refer to henceforth as the R policy) for this setting was proposed.
We study the R policy, and examine its two drawbacks -- (i) its huge
computational complexity, and (ii) its non-optimal average per-packet queueing
delay. We show that the R policy unnecessarily constrains itself to work with
information that is more delayed than that afforded by the system. We propose a
new policy that fully exploits the commonly available information, thereby
greatly improving upon the computational complexity and the delay performance
of the R policy. We show that our policy is throughput optimal. Our main
contribution in this work is the design of two fast and near-throughput-optimal
policies for this setting, whose explicit throughput and runtime performances
we characterize analytically. While the R policy takes a few milliseconds to
several tens of seconds to compute the schedule once (for varying number of
links in the network), the running times of the proposed
near-throughput-optimal algorithms range from a few microseconds to only a few
hundred microseconds, and are thus suitable for practical implementation in
networks with heterogeneously delayed information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06391</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06391</id><created>2015-04-24</created><authors><author><keyname>Haber</keyname><forenames>Eben M.</forenames></author></authors><title>On the Stability of Online Language Features: How Much Text do you Need
  to know a Person?</title><categories>cs.CL</categories><comments>4 pages, 4 figures, not published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, numerous studies have inferred personality and other traits
from people's online writing. While these studies are encouraging, more
information is needed in order to use these techniques with confidence. How do
linguistic features vary across different online media, and how much text is
required to have a representative sample for a person? In this paper, we
examine several large sets of online, user-generated text, drawn from Twitter,
email, blogs, and online discussion forums. We examine and compare
population-wide results for the linguistic measure LIWC, and the inferred
traits of Big5 Personality and Basic Human Values. We also empirically measure
the stability of these traits across different sized samples for each
individual. Our results highlight the importance of tuning models to each
online medium, and include guidelines for the minimum amount of text required
for a representative result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06394</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06394</id><created>2015-04-24</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Shen</keyname><forenames>Jie</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author></authors><title>Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion</title><categories>cs.SI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social trust prediction addresses the significant problem of exploring
interactions among users in social networks. Naturally, this problem can be
formulated in the matrix completion framework, with each entry indicating the
trustness or distrustness. However, there are two challenges for the social
trust problem: 1) the observed data are with sign (1-bit) measurements; 2) they
are typically sampled non-uniformly. Most of the previous matrix completion
methods do not well handle the two issues. Motivated by the recent progress of
max-norm, we propose to solve the problem with a 1-bit max-norm constrained
formulation. Since max-norm is not easy to optimize, we utilize a reformulation
of max-norm which facilitates an efficient projected gradient decent algorithm.
We demonstrate the superiority of our formulation on two benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06395</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06395</id><created>2015-04-24</created><authors><author><keyname>Jung</keyname><forenames>Sang Yeob</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Viability of Reverse Pricing in Cellular Networks: A New Outlook on
  Resource Management</title><categories>cs.NI</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reverse pricing has been recognized as an effective tool to handle the
uncertainty of users' demands in the travel industry (e.g., airlines and
hotels). To investigate its viability in cellular networks, we study the
practical limitations of (operator-driven) time-dependent pricing that has been
recently introduced, taking into account demand uncertainty. Then, we endeavor
to design the reverse pricing mechanism to resolve the weakness of the
time-dependent pricing scheme. We show that the proposed pricing scheme can
achieve &quot;triple-win&quot; solutions: an increase in the total revenue of the
operator; higher resource utilization efficiency; and an increment in the total
payoff of the users. Our findings provide a new outlook on resource management,
and design guidelines for adopting the reverse pricing scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06409</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06409</id><created>2015-04-24</created><updated>2015-06-12</updated><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Modal Inclusion Logic: Being Lax is Simpler than Being Strict</title><categories>cs.LO cs.CC</categories><comments>MFCS 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of the satisfiability problem of
modal inclusion logic. We distinguish two variants of the problem: one for
strict and another one for lax semantics. The complexity of the lax version
turns out to be complete for EXPTIME, whereas with strict semantics, the
problem becomes NEXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06416</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06416</id><created>2015-04-24</created><authors><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author></authors><title>On the Minimum Distance of Array-Based Spatially-Coupled Low-Density
  Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>5 pages. To be presented at the 2015 IEEE International Symposium on
  Information Theory, June 14-19, 2015, Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An array low-density parity-check (LDPC) code is a quasi-cyclic LDPC code
specified by two integers $q$ and $m$, where $q$ is an odd prime and $m \leq
q$. The exact minimum distance, for small $q$ and $m$, has been calculated, and
tight upper bounds on it for $m \leq 7$ have been derived. In this work, we
study the minimum distance of the spatially-coupled version of these codes. In
particular, several tight upper bounds on the optimal minimum distance for
coupling length at least two and $m=3,4,5$, that are independent of $q$ and
that are valid for all values of $q \geq q_0$ where $q_0$ depends on $m$, are
presented. Furthermore, we show by exhaustive search that by carefully
selecting the edge spreading or unwrapping procedure, the minimum distance
(when $q$ is not very large) can be significantly increased, especially for
$m=5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06423</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06423</id><created>2015-04-24</created><updated>2015-05-06</updated><authors><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>White</keyname><forenames>Ryen</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Information Gathering in Networks via Active Exploration</title><categories>cs.AI</categories><comments>Longer version of IJCAI'15 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How should we gather information in a network, where each node's visibility
is limited to its local neighborhood? This problem arises in numerous
real-world applications, such as surveying and task routing in social networks,
team formation in collaborative networks and experimental design with
dependency constraints. Often the informativeness of a set of nodes can be
quantified via a submodular utility function. Existing approaches for
submodular optimization, however, require that the set of all nodes that can be
selected is known ahead of time, which is often unrealistic. In contrast, we
propose a novel model where we start our exploration from an initial node, and
new nodes become visible and available for selection only once one of their
neighbors has been chosen. We then present a general algorithm NetExp for this
problem, and provide theoretical bounds on its performance dependent on
structural properties of the underlying network. We evaluate our methodology on
various simulated problem instances as well as on data collected from social
question answering system deployed within a large enterprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06431</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06431</id><created>2015-04-24</created><authors><author><keyname>Denkovski</keyname><forenames>Daniel</forenames></author><author><keyname>Rakovic</keyname><forenames>Valentin</forenames></author><author><keyname>Atanasovski</keyname><forenames>Vladimir</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author><author><keyname>M&#xe4;h&#xf6;nen</keyname><forenames>Petri</forenames></author></authors><title>Generic Multiuser Coordinated Beamforming for Underlay Spectrum Sharing</title><categories>cs.IT math.IT physics.data-an</categories><comments>30 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beamforming techniques have been recently studied as possible enablers
for underlay spectrum sharing. The existing beamforming techniques have several
common limitations: they are usually system model specific, cannot operate with
arbitrary number of transmit/receive antennas, and cannot serve arbitrary
number of users. Moreover, the beamforming techniques for underlay spectrum
sharing do not consider the interference originating from the incumbent primary
system. This work extends the common underlay sharing model by incorporating
the interference originating from the incumbent system into generic combined
beamforming design that can be applied on interference, broadcast or multiple
access channels. The paper proposes two novel multiuser beamforming algorithms
for user fairness and sum rate maximization, utilizing newly derived convex
optimization problems for transmit and receive beamformers calculation in a
recursive optimization. Both beamforming algorithms provide efficient operation
for the interference, broadcast and multiple access channels, as well as for
arbitrary number of antennas and secondary users in the system. Furthermore,
the paper proposes a successive transmit/receive optimization approach that
reduces the computational complexity of the proposed recursive algorithms. The
results show that the proposed complexity reduction significantly improves the
convergence rates and can facilitate their operation in scenarios which require
agile beamformers computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06434</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06434</id><created>2015-04-24</created><authors><author><keyname>Uijlings</keyname><forenames>Jasper</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Situational Object Boundary Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitively, the appearance of true object boundaries varies from image to
image. Hence the usual monolithic approach of training a single boundary
predictor and applying it to all images regardless of their content is bound to
be suboptimal. In this paper we therefore propose situational object boundary
detection: We first define a variety of situations and train a specialized
object boundary detector for each of them using [Dollar and Zitnick 2013]. Then
given a test image, we classify it into these situations using its context,
which we model by global image appearance. We apply the corresponding
situational object boundary detectors, and fuse them based on the
classification probabilities. In experiments on ImageNet, Microsoft COCO, and
Pascal VOC 2012 segmentation we show that our situational object boundary
detection gives significant improvements over a monolithic approach.
Additionally, our method substantially outperforms [Hariharan et al. 2011] on
semantic contour detection on their SBD dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06443</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06443</id><created>2015-04-24</created><authors><author><keyname>Imachi</keyname><forenames>Hiroto</forenames></author><author><keyname>Hoshi</keyname><forenames>Takeo</forenames></author></authors><title>Hybrid Numerical Solvers for Massively Parallel Eigenvalue Computation
  and Their Benchmark with Electronic Structure Calculations</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.NA math.NA</categories><comments>9 pages, 8 figures</comments><msc-class>65F15</msc-class><journal-ref>J. Info. Process.24, pp.164-172 (2016)</journal-ref><doi>10.2197/ipsjjip.24.164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimally hybrid numerical solvers were constructed for massively parallel
generalized eigenvalue problem (GEP).The strong scaling benchmark was carried
out on the K computer and other supercomputers for electronic structure
calculation problems in the matrix sizes of M = 10^4-10^6 with upto 105 cores.
The procedure of GEP is decomposed into the two subprocedures of the reducer to
the standard eigenvalue problem (SEP) and the solver of SEP. A hybrid solver is
constructed, when a routine is chosen for each subprocedure from the three
parallel solver libraries of ScaLAPACK, ELPA and EigenExa. The hybrid solvers
with the two newer libraries, ELPA and EigenExa, give better benchmark results
than the conventional ScaLAPACK library. The detailed analysis on the results
implies that the reducer can be a bottleneck in next-generation (exa-scale)
supercomputers, which indicates the guidance for future research. The code was
developed as a middleware and a mini-application and will appear online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06451</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06451</id><created>2015-04-24</created><updated>2015-05-05</updated><authors><author><keyname>Meimaris</keyname><forenames>Marios</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Pateritsas</keyname><forenames>Christos</forenames></author><author><keyname>Galani</keyname><forenames>Theodora</forenames></author><author><keyname>Stavrakas</keyname><forenames>Yannis</forenames></author></authors><title>A Framework for Managing Evolving Information Resources on the Data Web</title><categories>cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1504.01891</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web of data has brought forth the need to preserve and sustain evolving
information within linked datasets; however, a basic requirement of data
preservation is the maintenance of the datasets' structural characteristics as
well. As open data are often found using different and/or heterogeneous data
models and schemata from one source to another, there is a need to reconcile
these mismatches and provide common denominations of interpretation on a
multitude of levels, in order to be able to preserve and manage the evolution
of the generated resources. In this paper, we present a linked data approach
for the preservation and archiving of open heterogeneous datasets that evolve
through time, at both the structural and the semantic layer. We first propose a
set of re-quirements for modelling evolving linked datasets. We then proceed on
concep-tualizing a modelling framework for evolving entities and place these in
a 2x2 model space that consists of the semantic and the temporal dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06454</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06454</id><created>2015-04-24</created><authors><author><keyname>Calamoneri</keyname><forenames>Tiziana</forenames></author><author><keyname>Sinaimeri</keyname><forenames>Blerina</forenames></author><author><keyname>Gastaldello</keyname><forenames>Mattia</forenames></author></authors><title>On Pairwise Compatibility of Some Graph (Super)Classes</title><categories>cs.DM</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G=(V,E) is a pairwise compatibility graph (PCG) if there exists an
edge-weighted tree T and two non-negative real numbers `d' and `D' such that
each leaf `u' of T is a node of V and the edge `(u,v) belongs to E' iff `d &lt;=
d_T(u, v) &lt;= D' where d_T(u, v) is the sum of weights of the edges on the
unique path from `u' to `v' in T. The main issue on these graphs consists in
characterizing them.
  In this note we prove the inclusion in the PCG class of threshold tolerance
graphs and the non-inclusion of a number of intersection graphs, such as disk
and grid intersection graphs, circular arc and tolerance graphs. The
non-inclusion of some superclasses (trapezoid, permutation and rectangle
intersection graphs) follows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06457</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06457</id><created>2015-04-24</created><authors><author><keyname>Duff</keyname><forenames>I. Pontes</forenames></author><author><keyname>Poussot-Vassal</keyname><forenames>C.</forenames></author><author><keyname>Seren</keyname><forenames>C.</forenames></author></authors><title>Realization independent single time-delay dynamical model interpolation
  and $\mathcal{H}_2$-optimal approximation</title><categories>cs.SY math.NA math.OC</categories><comments>14 pages, 4 figures, submitted to CDC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the realization-free model approximation problem, as stated in
\cite{mayo2007framework,beattie2012realization}, is revisited in the case where
the interpolating model might be time-delay dependent. To this aim, the Loewner
framework, initially settled for delay-free realization, is firstly generalized
to the single delay case. Secondly, the (infinite) model approximation
$\mathcal{H}_2$ optimality conditions are established through the use of the
Lambert functions. Finally, a numerically effective iterative scheme, named
\textbf{dTF-IRKA}, similar to the \textbf{TF-IRKA}
\cite{beattie2012realization}, is proposed to reach a part of the
aforementioned optimality conditions. The proposed method validity and interest
are assessed on different numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06461</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06461</id><created>2015-04-24</created><authors><author><keyname>Lin</keyname><forenames>Jinbiao</forenames></author><author><keyname>Song</keyname><forenames>Shiji</forenames></author><author><keyname>You</keyname><forenames>Keyou</forenames></author><author><keyname>Wu</keyname><forenames>Cheng</forenames></author></authors><title>3-D Velocity Regulation for Nonholonomic Source Seeking Without Position
  Measurement</title><categories>cs.RO</categories><comments>submitted to IEEE TCST;12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a three-dimensional problem of steering a nonholonomic vehicle to
seek an unknown source of a spatially distributed signal field without any
position measurement. In the literature, there exists an extremum seeking-based
strategy under a constant forward velocity and tunable pitch and yaw
velocities. Obviously, the vehicle with a constant forward velocity may exhibit
certain overshoots in the seeking process and can not slow down even it
approaches the source. To resolve this undesired behavior, this paper proposes
a regulation strategy for the forward velocity along with the pitch and yaw
velocities. Under such a strategy, the vehicle slows down near the source and
stays within a small area as if it comes to a full stop, and controllers for
angular velocities become succinct. We prove the local exponential convergence
via the averaging technique. Finally, the theoretical results are illustrated
with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06474</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06474</id><created>2015-04-24</created><updated>2015-09-14</updated><authors><author><keyname>Liu</keyname><forenames>Weifeng</forenames></author><author><keyname>Vinter</keyname><forenames>Brian</forenames></author></authors><title>Speculative Segmented Sum for Sparse Matrix-Vector Multiplication on
  Heterogeneous Processors</title><categories>cs.MS cs.DC math.NA</categories><comments>22 pages, 8 figures, Published at Parallel Computing (PARCO)</comments><msc-class>65F50</msc-class><acm-class>G.4; G.1.3</acm-class><doi>10.1016/j.parco.2015.04.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse matrix-vector multiplication (SpMV) is a central building block for
scientific software and graph applications. Recently, heterogeneous processors
composed of different types of cores attracted much attention because of their
flexible core configuration and high energy efficiency. In this paper, we
propose a compressed sparse row (CSR) format based SpMV algorithm utilizing
both types of cores in a CPU-GPU heterogeneous processor. We first
speculatively execute segmented sum operations on the GPU part of a
heterogeneous processor and generate a possibly incorrect results. Then the CPU
part of the same chip is triggered to re-arrange the predicted partial sums for
a correct resulting vector. On three heterogeneous processors from Intel, AMD
and nVidia, using 20 sparse matrices as a benchmark suite, the experimental
results show that our method obtains significant performance improvement over
the best existing CSR-based SpMV algorithms. The source code of this work is
downloadable at https://github.com/bhSPARSE/Benchmark_SpMV_using_CSR
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06475</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06475</id><created>2015-04-24</created><updated>2015-09-07</updated><authors><author><keyname>Reitzig</keyname><forenames>Raphael</forenames></author><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author></authors><title>A Practical and Worst-Case Efficient Algorithm for Divisor Methods of
  Apportionment</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proportional apportionment is the problem of assigning seats to parties
according to their relative share of votes. Divisor methods are the de-facto
standard solution, used in many countries.
  In recent literature, there are two algorithms that implement divisor
methods: one by Cheng and Eppstein (ISAAC, 2014) has worst-case optimal running
time but is complex, while the other (Pukelsheim, 2014) is relatively simple
and fast in practice but does not offer worst-case guarantees.
  We demonstrate that the former algorithm is much slower than the other in
practice and propose a novel algorithm that avoids the shortcomings of both. We
investigate the running-time behavior of the three contenders in order to
determine which is most useful in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06484</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06484</id><created>2015-04-24</created><authors><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author></authors><title>Recent Advances in Real Geometric Reasoning</title><categories>cs.SC cs.CG</categories><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2; F.2.2; G.4</acm-class><journal-ref>Automated Deduction in Geometry (LNCS 9201), pp. 37-52. Springer
  International, 2015</journal-ref><doi>10.1007/978-3-319-21362-0_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 1930s Tarski showed that real quantifier elimination was possible, and
in 1975 Collins gave a remotely practicable method, albeit with
doubly-exponential complexity, which was later shown to be inherent. We discuss
some of the recent major advances in Collins method: such as an alternative
approach based on passing via the complexes, and advances which come closer to
&quot;solving the question asked&quot; rather than &quot;solving all problems to do with these
polynomials&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06488</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06488</id><created>2015-04-24</created><authors><author><keyname>Trodden</keyname><forenames>Paul</forenames></author></authors><title>A One-step Approach to Computing a Polytopic Robust Positively Invariant
  Set</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A procedure and theoretical results are presented for the problem of
determining a minimal robust positively invariant (RPI) set for a linear
discrete-time system subject to unknown, bounded disturbances. The procedure
computes, via the solving of a single LP, a polytopic RPI set that is minimal
with respect to the family of RPI sets generated from a finite number of
inequalities with pre-defined normal vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06494</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06494</id><created>2015-04-24</created><authors><author><keyname>Georgatzis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Williams</keyname><forenames>Christopher K. I.</forenames></author></authors><title>Discriminative Switching Linear Dynamical Systems applied to
  Physiological Condition Monitoring</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Discriminative Switching Linear Dynamical System (DSLDS) applied
to patient monitoring in Intensive Care Units (ICUs). Our approach is based on
identifying the state-of-health of a patient given their observed vital signs
using a discriminative classifier, and then inferring their underlying
physiological values conditioned on this status. The work builds on the
Factorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which
has been previously used in a similar setting. The FSLDS is a generative model,
whereas the DSLDS is a discriminative model. We demonstrate on two real-world
datasets that the DSLDS is able to outperform the FSLDS in most cases of
interest, and that an $\alpha$-mixture of the two models achieves higher
performance than either of the two models separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06501</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06501</id><created>2015-04-24</created><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>McCauley</keyname><forenames>Samuel</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames><affiliation>University of Massachusetts, Amherst</affiliation></author><author><keyname>Singh</keyname><forenames>Shikha</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>Vu</keyname><forenames>Hoa T.</forenames><affiliation>University of Massachusetts, Amherst</affiliation></author></authors><title>Run Generation Revisited: What Goes Up May or May Not Come Down</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the classic problem of run generation. Run
generation is the first phase of external-memory sorting, where the objective
is to scan through the data, reorder elements using a small buffer of size M ,
and output runs (contiguously sorted chunks of elements) that are as long as
possible.
  We develop algorithms for minimizing the total number of runs (or
equivalently, maximizing the average run length) when the runs are allowed to
be sorted or reverse sorted. We study the problem in the online setting, both
with and without resource augmentation, and in the offline setting.
  (1) We analyze alternating-up-down replacement selection (runs alternate
between sorted and reverse sorted), which was studied by Knuth as far back as
1963. We show that this simple policy is asymptotically optimal. Specifically,
we show that alternating-up-down replacement selection is 2-competitive and no
deterministic online algorithm can perform better.
  (2) We give online algorithms having smaller competitive ratios with resource
augmentation. Specifically, we exhibit a deterministic algorithm that, when
given a buffer of size 4M , is able to match or beat any optimal algorithm
having a buffer of size M . Furthermore, we present a randomized online
algorithm which is 7/4-competitive when given a buffer twice that of the
optimal.
  (3) We demonstrate that performance can also be improved with a small amount
of foresight. We give an algorithm, which is 3/2-competitive, with
foreknowledge of the next 3M elements of the input stream. For the extreme case
where all future elements are known, we design a PTAS for computing the optimal
strategy a run generation algorithm must follow.
  (4) Finally, we present algorithms tailored for nearly sorted inputs which
are guaranteed to have optimal solutions with sufficiently long runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06507</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06507</id><created>2015-04-24</created><authors><author><keyname>Baltaxe</keyname><forenames>Michael</forenames></author><author><keyname>Meer</keyname><forenames>Peter</forenames></author><author><keyname>Lindenbaum</keyname><forenames>Michael</forenames></author></authors><title>Local Variation as a Statistical Hypothesis Test</title><categories>cs.CV</categories><doi>10.1007/s11263-015-0855-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of image oversegmentation is to divide an image into several pieces,
each of which should ideally be part of an object. One of the simplest and yet
most effective oversegmentation algorithms is known as local variation (LV)
(Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm and
show that algorithms similar to LV can be devised by applying different
statistical models and decisions, thus providing further theoretical
justification and a well-founded explanation for the unexpected high
performance of the LV approach. Some of these algorithms are based on
statistics of natural images and on a hypothesis testing decision; we denote
these algorithms probabilistic local variation (pLV). The best pLV algorithm,
which relies on censored estimation, presents state-of-the-art results while
keeping the same computational complexity of the LV algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06526</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06526</id><created>2015-04-24</created><updated>2016-03-01</updated><authors><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Towards Massive, Ultra-Reliable, and Low-Latency Wireless Communication
  with Short Packets</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures; to appear in the Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the recent advances in the design of high-speed wireless systems are
based on information-theoretic principles that demonstrate how to efficiently
transmit long data packets. However, the upcoming wireless systems, notably the
5G system, will need to support novel traffic types that use short packets. For
example, short packets represent the most common form of traffic generated by
sensors and other devices involved in Machine-to-Machine (M2M) communications.
Furthermore, there are emerging applications in which small packets are
expected to carry critical information that should be received with low latency
and ultra-high reliability.
  Current wireless systems are not designed to support short-packet
transmissions. For example, the design of current systems relies on the
assumption that the metadata (control information) is of negligible size
compared to the actual information payload. Hence, transmitting metadata using
heuristic methods does not affect the overall system performance. However, when
the packets are short, metadata may be of the same size as the payload, and the
conventional methods to transmit it may be highly suboptimal.
  In this article, we review recent advances in information theory, which
provide the theoretical principles that govern the transmission of short
packets. We then apply these principles to three exemplary scenarios (the
two-way channel, the downlink broadcast channel, and the uplink random access
channel), thereby illustrating how the transmission of control information can
be optimized when the packets are short. The insights brought by these examples
suggest that new principles are needed for the design of wireless protocols
supporting short packets. These principles will have a direct impact on the
system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06529</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06529</id><created>2015-04-24</created><authors><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Kharlamov</keyname><forenames>Evgeny</forenames></author><author><keyname>Kostylev</keyname><forenames>Egor V.</forenames></author><author><keyname>Zheleznyakov</keyname><forenames>Dmitriy</forenames></author></authors><title>Controlled Query Evaluation for Datalog and OWL 2 Profile Ontologies</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study confidentiality enforcement in ontologies under the Controlled Query
Evaluation framework, where a policy specifies the sensitive information and a
censor ensures that query answers that may compromise the policy are not
returned. We focus on censors that ensure confidentiality while maximising
information access, and consider both Datalog and the OWL 2 profiles as
ontology languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06534</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06534</id><created>2015-04-24</created><authors><author><keyname>Aiswarya</keyname><forenames>C.</forenames></author><author><keyname>Bollig</keyname><forenames>Benedikt</forenames></author><author><keyname>Gastin</keyname><forenames>Paul</forenames></author></authors><title>An Automata-Theoretic Approach to the Verification of Distributed
  Algorithms</title><categories>cs.LO cs.FL</categories><comments>26 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an automata-theoretic method for the verification of distributed
algorithms running on ring networks. In a distributed algorithm, an arbitrary
number of processes cooperate to achieve a common goal (e.g., elect a leader).
Processes have unique identifiers (pids) from an infinite, totally ordered
domain. An algorithm proceeds in synchronous rounds, each round allowing a
process to perform a bounded sequence of actions such as send or receive a pid,
store it in some register, and compare register contents wrt. the associated
total order. An algorithm is supposed to be correct independently of the number
of processes. To specify correctness properties, we introduce a logic that can
reason about processes and pids. Referring to leader election, it may say that,
at the end of an execution, each process stores the maximum pid in some
dedicated register. Since the verification of distributed algorithms is
undecidable, we propose an underapproximation technique, which bounds the
number of rounds. This is an appealing approach, as the number of rounds needed
by a distributed algorithm to conclude is often exponentially smaller than the
number of processes. We provide an automata-theoretic solution, reducing model
checking to emptiness for alternating two-way automata on words. Overall, we
show that round-bounded verification of distributed algorithms over rings is
PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06537</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06537</id><created>2015-04-24</created><authors><author><keyname>Gambuzza</keyname><forenames>L. V.</forenames></author><author><keyname>Fortuna</keyname><forenames>L.</forenames></author><author><keyname>Frasca</keyname><forenames>M.</forenames></author><author><keyname>Gale</keyname><forenames>E.</forenames></author></authors><title>Experimental evidence of chaos from memristors</title><categories>nlin.CD cs.ET</categories><comments>Accepted for publication in International Journal of Bifurcation and
  Chaos</comments><doi>10.1142/S0218127415501011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until now, most memristor-based chaotic circuits proposed in the literature
are based on mathematical models which assume ideal characteristics such as
piece-wise linear or cubic non-linearities. The idea, illustrated here and
originating from the experimental approach for device characterization, is to
realize a chaotic system exploiting the non-linearity of only one memristor
with a very simple experimental set-up using feedback. In this way a simple
circuit is obtained and chaos is experimentally observed and is confirmed by
the calculation of the largest Lyapunov exponent. Numerical results using the
Strukov model support the existence of robust chaos in our circuit. This is the
first experimental demonstration of chaos in a real memristor circuit and
suggests that memristors are well placed for hardware encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06540</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06540</id><created>2015-04-24</created><updated>2015-06-30</updated><authors><author><keyname>Eades</keyname><forenames>Peter</forenames></author><author><keyname>Hong</keyname><forenames>Seok-Hee</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Poon</keyname><forenames>Sheung-Hung</forenames></author></authors><title>Straight-line Drawability of a Planar Graph Plus an Edge</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate straight-line drawings of topological graphs that consist of a
planar graph plus one edge, also called almost-planar graphs. We present a
characterization of such graphs that admit a straight-line drawing. The
characterization enables a linear-time testing algorithm to determine whether
an almost-planar graph admits a straight-line drawing, and a linear-time
drawing algorithm that constructs such a drawing, if it exists. We also show
that some almost-planar graphs require exponential area for a straight-line
drawing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06541</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06541</id><created>2015-04-24</created><authors><author><keyname>Gonzalez</keyname><forenames>Elias</forenames></author><author><keyname>Balog</keyname><forenames>Robert S.</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Resource requirements and speed versus geometry of unconditionally
  secure physical key exchanges</title><categories>cs.CR</categories><comments>13 pages, 7 figures, MDPI Entropy</comments><journal-ref>Entropy 2015, 17(4), pp. 2010-2024</journal-ref><doi>10.3390/e17042010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The imperative need for unconditional secure key exchange is expounded by the
increasing connectivity of networks and by the increasing number and level of
sophistication of cyberattacks. Two concepts that are information theoretically
secure are quantum key distribution (QKD) and Kirchoff-law-Johnson-noise
(KLJN). However, these concepts require a dedicated connection between hosts in
peer-to-peer (P2P) networks which can be impractical and or cost prohibitive. A
practical and cost effective method is to have each host share their respective
cable(s) with other hosts such that two remote hosts can realize a secure key
exchange without the need of an additional cable or key exchanger. In this
article we analyze the cost complexities of cable, key exchangers, and time
required in the star network. We mentioned the reliability of the star network
and compare it with other network geometries. We also conceived a protocol and
equation for the number of secure bit exchange periods needed in a star
network. We then outline other network geometries and trade-off possibilities
that seem interesting to explore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06543</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06543</id><created>2015-04-22</created><authors><author><keyname>Hirokawa</keyname><forenames>Sachio</forenames></author><author><keyname>Ito</keyname><forenames>Eisuke</forenames></author></authors><title>Power Law and Entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon(1951) and Yavuz(1974) estimated the entropy of real documents. This
note drives an upper bound of entropy from the parameter of the power law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06544</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06544</id><created>2015-04-24</created><authors><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Gouleakis</keyname><forenames>Themis</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Sampling Correctors</title><categories>cs.DS cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many situations, sample data is obtained from a noisy or imperfect source.
In order to address such corruptions, this paper introduces the concept of a
sampling corrector. Such algorithms use structure that the distribution is
purported to have, in order to allow one to make &quot;on-the-fly&quot; corrections to
samples drawn from probability distributions. These algorithms then act as
filters between the noisy data and the end user.
  We show connections between sampling correctors, distribution learning
algorithms, and distribution property testing algorithms. We show that these
connections can be utilized to expand the applicability of known distribution
learning and property testing algorithms as well as to achieve improved
algorithms for those tasks.
  As a first step, we show how to design sampling correctors using proper
learning algorithms. We then focus on the question of whether algorithms for
sampling correctors can be more efficient in terms of sample complexity than
learning algorithms for the analogous families of distributions. When
correcting monotonicity, we show that this is indeed the case when also granted
query access to the cumulative distribution function. We also obtain sampling
correctors for monotonicity without this stronger type of access, provided that
the distribution be originally very close to monotone (namely, at a distance
$O(1/\log^2 n)$). In addition to that, we consider a restricted error model
that aims at capturing &quot;missing data&quot; corruptions. In this model, we show that
distributions that are close to monotone have sampling correctors that are
significantly more efficient than achievable by the learning approach.
  We also consider the question of whether an additional source of independent
random bits is required by sampling correctors to implement the correction
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06560</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06560</id><created>2015-04-24</created><authors><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author><author><keyname>Shi</keyname><forenames>Cong</forenames></author></authors><title>Approximation Algorithms for Inventory Problems with Submodular or
  Routing Costs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following two deterministic inventory optimization problems
over a finite planning horizon $T$ with non-stationary demands.
  (a) Submodular Joint Replenishment Problem: This involves multiple item types
and a single retailer who faces demands. In each time step, any subset of
item-types can be ordered incurring a joint ordering cost which is submodular.
Moreover, items can be held in inventory while incurring a holding cost. The
objective is find a sequence of orders that satisfies all demands and minimizes
the total ordering and holding costs.
  (b) Inventory Routing Problem: This involves a single depot that stocks
items, and multiple retailer locations facing demands. In each time step, any
subset of locations can be visited using a vehicle originating from the depot.
There is also cost incurred for holding items at any retailer. The objective
here is to satisfy all demands while minimizing the sum of routing and holding
costs.
  We present a unified approach that yields $\mathcal{O}\left(\frac{\log
T}{\log\log T}\right)$-factor approximation algorithms for both problems when
the holding costs are polynomial functions. A special case is the classic
linear holding cost model, wherein this is the first sub-logarithmic
approximation ratio for either problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06567</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06567</id><created>2015-04-24</created><authors><author><keyname>Salvador</keyname><forenames>Amaia</forenames></author><author><keyname>Zeppelzauer</keyname><forenames>Matthias</forenames></author><author><keyname>Manchon-Vizuete</keyname><forenames>Daniel</forenames></author><author><keyname>Calafell</keyname><forenames>Andrea</forenames></author><author><keyname>Giro-i-Nieto</keyname><forenames>Xavier</forenames></author></authors><title>Cultural Event Recognition with Visual ConvNets and Temporal Models</title><categories>cs.CV cs.CY</categories><comments>Initial version of the paper accepted at the CVPR Workshop ChaLearn
  Looking at People 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our contribution to the ChaLearn Challenge 2015 on
Cultural Event Classification. The challenge in this task is to automatically
classify images from 50 different cultural events. Our solution is based on the
combination of visual features extracted from convolutional neural networks
with temporal information using a hierarchical classifier scheme. We extract
visual features from the last three fully connected layers of both CaffeNet
(pretrained with ImageNet) and our fine tuned version for the ChaLearn
challenge. We propose a late fusion strategy that trains a separate low-level
SVM on each of the extracted neural codes. The class predictions of the
low-level SVMs form the input to a higher level SVM, which gives the final
event scores. We achieve our best result by adding a temporal refinement step
into our classification scheme, which is applied directly to the output of each
low-level SVM. Our approach penalizes high classification scores based on
visual features when their time stamp does not match well an event-specific
temporal distribution learned from the training and validation data. Our system
achieved the second best result in the ChaLearn Challenge 2015 on Cultural
Event Classification with a mean average precision of 0.767 on the test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06580</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06580</id><created>2015-04-24</created><updated>2015-05-24</updated><authors><author><keyname>Santos</keyname><forenames>Cicero Nogueira dos</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Classifying Relations by Ranking with Convolutional Neural Networks</title><categories>cs.CL cs.LG cs.NE</categories><comments>Accepted as a long paper in the 53rd Annual Meeting of the
  Association for Computational Linguistics (ACL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relation classification is an important semantic processing task for which
state-ofthe-art systems still rely on costly handcrafted features. In this work
we tackle the relation classification task using a convolutional neural network
that performs classification by ranking (CR-CNN). We propose a new pairwise
ranking loss function that makes it easy to reduce the impact of artificial
classes. We perform experiments using the the SemEval-2010 Task 8 dataset,
which is designed for the task of classifying the relationship between two
nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art
for this dataset and achieve a F1 of 84.1 without using any costly handcrafted
features. Additionally, our experimental results show that: (1) our approach is
more effective than CNN followed by a softmax classifier; (2) omitting the
representation of the artificial class Other improves both precision and
recall; and (3) using only word embeddings as input features is enough to
achieve state-of-the-art results if we consider only the text between the two
target nominals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06582</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06582</id><created>2015-04-24</created><updated>2015-05-05</updated><authors><author><keyname>Gribov</keyname><forenames>Alexander</forenames></author></authors><title>Approximate Fitting of Circular Arcs when Two Points are Known</title><categories>cs.CG</categories><comments>Added new section and references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of approximation of points with circular arcs is performed in many
applications, such as polyline compression, noise filtering, and feature
recognition. However, development of algorithms that perform a significant
amount of circular arcs fitting require an efficient way of fitting circular
arcs with complexity O(1). The elegant solution to this task based on an
eigenvector problem for a square nonsymmetrical matrix is described in [1]. For
the compression algorithm described in [2], it is necessary to solve this task
when two points on the arc are known. This paper describes a different approach
to efficiently fitting the arcs and solves the task when one or two points are
known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06584</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06584</id><created>2015-04-24</created><authors><author><keyname>Gribov</keyname><forenames>Alexander</forenames></author></authors><title>Searching for a Compressed Polyline with a Minimum Number of Vertices</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many practical applications that require simplification of
polylines. Some of the goals are to reduce the amount of information necessary
to store, improve processing time, or simplify editing. The simplification is
usually done by removing some of the vertices, making the resultant polyline go
through a subset of the source polyline vertices. However, such approaches do
not necessarily produce a new polyline with the minimum number of vertices. The
approximate solution to find a polyline, within a specified tolerance, with the
minimum number of vertices is described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06586</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06586</id><created>2015-04-24</created><authors><author><keyname>Laurent</keyname><forenames>Monique</forenames></author><author><keyname>Seminaroti</keyname><forenames>Matteo</forenames></author></authors><title>A Lex-BFS-based recognition algorithm for Robinsonian matrices</title><categories>cs.DM math.OC</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robinsonian matrices arise in the classical seriation problem and play an
important role in many applications where unsorted similarity (or
dissimilarity) information must be reordered. We present a new polynomial time
algorithm to recognize Robinsonian matrices based on a new characterization of
Robinsonian matrices in terms of straight enumerations of unit interval graphs.
The algorithm is simple and is based essentially on lexicographic breadth-first
search (Lex-BFS), using a divide-and-conquer strategy. When applied to a
nonnegative symmetric $n\times n$ matrix with $m$ nonzero entries and given as
a weighted adjacency list, it runs in $O(d(n+m))$ time, where $d$ is the depth
of the recursion tree, which is at most the number of distinct nonzero entries
of $A$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06587</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06587</id><created>2015-04-24</created><authors><author><keyname>Reddy</keyname><forenames>N. Dinesh</forenames></author><author><keyname>Singhal</keyname><forenames>Prateek</forenames></author><author><keyname>Krishna</keyname><forenames>K. Madhava</forenames></author></authors><title>Semantic Motion Segmentation Using Dense CRF Formulation</title><categories>cs.CV</categories><doi>10.1145/2683483.2683539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the literature has been fairly dense in the areas of scene
understanding and semantic labeling there have been few works that make use of
motion cues to embellish semantic performance and vice versa. In this paper, we
address the problem of semantic motion segmentation, and show how semantic and
motion priors augments performance. We pro- pose an algorithm that jointly
infers the semantic class and motion labels of an object. Integrating semantic,
geometric and optical ow based constraints into a dense CRF-model we infer both
the object class as well as motion class, for each pixel. We found improvement
in performance using a fully connected CRF as compared to a standard
clique-based CRFs. For inference, we use a Mean Field approximation based
algorithm. Our method outperforms recently pro- posed motion detection
algorithms and also improves the semantic labeling compared to the
state-of-the-art Automatic Labeling Environment algorithm on the challenging
KITTI dataset especially for object classes such as pedestrians and cars that
are critical to an outdoor robotic navigation scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06591</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06591</id><created>2015-04-24</created><authors><author><keyname>Mopuri</keyname><forenames>Konda Reddy</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Object Level Deep Feature Pooling for Compact Image Representation</title><categories>cs.CV</categories><comments>Deep Vision 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Network (CNN) features have been successfully employed
in recent works as an image descriptor for various vision tasks. But the
inability of the deep CNN features to exhibit invariance to geometric
transformations and object compositions poses a great challenge for image
search. In this work, we demonstrate the effectiveness of the objectness prior
over the deep CNN features of image regions for obtaining an invariant image
representation. The proposed approach represents the image as a vector of
pooled CNN features describing the underlying objects. This representation
provides robustness to spatial layout of the objects in the scene and achieves
invariance to general geometric transformations, such as translation, rotation
and scaling. The proposed approach also leads to a compact representation of
the scene, making each image occupy a smaller memory footprint. Experiments
show that the proposed representation achieves state of the art retrieval
results on a set of challenging benchmark image datasets, while maintaining a
compact representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06593</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06593</id><created>2015-04-24</created><authors><author><keyname>Papadopoulos</keyname><forenames>Athanasios</forenames></author><author><keyname>Czap</keyname><forenames>Laszlo</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>LP formulations for secrecy over erasure networks with feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design polynomial time schemes for secure message transmission over
arbitrary networks, in the presence of an eavesdropper, and where each edge
corresponds to an erasure channel with public feedback. Our schemes are
described through linear programming (LP) formulations, that explicitly select
(possibly different) sets of paths for key-generation and message sending.
Although our LPs are not always capacity-achieving, they outperform the best
known alternatives in the literature, and extend to incorporate several
interesting scenaria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06598</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06598</id><created>2015-04-24</created><updated>2015-09-29</updated><authors><author><keyname>Wahls</keyname><forenames>Sander</forenames></author><author><keyname>Le</keyname><forenames>Son T.</forenames></author><author><keyname>Prilepsky</keyname><forenames>Jaroslaw E.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Turitsyn</keyname><forenames>Sergei K.</forenames></author></authors><title>Digital Backpropagation in the Nonlinear Fourier Domain</title><categories>cs.IT math.IT physics.optics</categories><comments>Invited paper presented in the special session on &quot;Signal Processing,
  Coding, and Information Theory for Optical Communications&quot; at IEEE SPAWC
  2015. Minor changes</comments><journal-ref>Proceedings of the 2015 IEEE 16th International Workshop on Signal
  Processing Advances in Wireless Communications (SPAWC), Stockholm, Sweden,
  Jun. 2015, pp.445-449</journal-ref><doi>10.1109/SPAWC.2015.7227077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear and dispersive transmission impairments in coherent fiber-optic
communication systems are often compensated by reverting the nonlinear
Schr\&quot;odinger equation, which describes the evolution of the signal in the
link, numerically. This technique is known as digital backpropagation. Typical
digital backpropagation algorithms are based on split-step Fourier methods in
which the signal has to be discretized in time and space. The need to
discretize in both time and space however makes the real-time implementation of
digital backpropagation a challenging problem. In this paper, a new fast
algorithm for digital backpropagation based on nonlinear Fourier transforms is
presented. Aiming at a proof of concept, the main emphasis will be put on
fibers with normal dispersion in order to avoid the issue of solitonic
components in the signal. However, it is demonstrated that the algorithm also
works for anomalous dispersion if the signal power is low enough. Since the
spatial evolution of a signal governed by the nonlinear Schr\&quot;odinger equation
can be reverted analytically in the nonlinear Fourier domain through simple
phase-shifts, there is no need to discretize the spatial domain. The proposed
algorithm requires only $\mathcal{O}(D\log^{2}D)$ floating point operations to
backpropagate a signal given by $D$ samples, independently of the fiber's
length, and is therefore highly promising for real-time implementations. The
merits of this new approach are illustrated through numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06602</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06602</id><created>2015-04-24</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Arkadev</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>The Range of Topological Effects on Communication</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the study of communication cost of computing functions when
inputs are distributed among $k$ processors, each of which is located at one
vertex of a network/graph called a terminal. Every other node of the network
also has a processor, with no input. The communication is point-to-point and
the cost is the total number of bits exchanged by the protocol, in the worst
case, on all edges.
  Chattopadhyay, Radhakrishnan and Rudra (FOCS'14) recently initiated a study
of the effect of topology of the network on the total communication cost using
tools from $L_1$ embeddings. Their techniques provided tight bounds for simple
functions like Element-Distinctness (ED), which depend on the 1-median of the
graph. This work addresses two other kinds of natural functions. We show that
for a large class of natural functions like Set-Disjointness the communication
cost is essentially $n$ times the cost of the optimal Steiner tree connecting
the terminals. Further, we show for natural composed functions like $\text{ED}
\circ \text{XOR}$ and $\text{XOR} \circ \text{ED}$, the naive protocols
suggested by their definition is optimal for general networks. Interestingly,
the bounds for these functions depend on more involved topological parameters
that are a combination of Steiner tree and 1-median costs.
  To obtain our results, we use some new tools in addition to ones used in
Chattopadhyay et. al. These include (i) viewing the communication constraints
via a linear program; (ii) using tools from the theory of tree embeddings to
prove topology sensitive direct sum results that handle the case of composed
functions and (iii) representing the communication constraints of certain
problems as a family of collection of multiway cuts, where each multiway cut
simulates the hardness of computing the function on the star topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06603</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06603</id><created>2015-04-24</created><updated>2015-05-12</updated><authors><author><keyname>Mishkin</keyname><forenames>Dmytro</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author><author><keyname>Perdoch</keyname><forenames>Michal</forenames></author><author><keyname>Lenc</keyname><forenames>Karel</forenames></author></authors><title>WxBS: Wide Baseline Stereo Generalizations</title><categories>cs.CV</categories><comments>Descriptor and detector evaluation expanded</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have presented a new problem -- the wide multiple baseline stereo (WxBS)
-- which considers matching of images that simultaneously differ in more than
one image acquisition factor such as viewpoint, illumination, sensor type or
where object appearance changes significantly, e.g. over time. A new dataset
with the ground truth for evaluation of matching algorithms has been introduced
and will be made public.
  We have extensively tested a large set of popular and recent detectors and
descriptors and show than the combination of RootSIFT and HalfRootSIFT as
descriptors with MSER and Hessian-Affine detectors works best for many
different nuisance factors. We show that simple adaptive thresholding improves
Hessian-Affine, DoG, MSER (and possibly other) detectors and allows to use them
on infrared and low contrast images.
  A novel matching algorithm for addressing the WxBS problem has been
introduced. We have shown experimentally that the WxBS-M matcher dominantes the
state-of-the-art methods both on both the new and existing datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06607</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06607</id><created>2015-04-15</created><authors><author><keyname>Bacci</keyname><forenames>Giacomo</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author></authors><title>Understanding Game Theory via Wireless Power Control</title><categories>cs.GT cs.NI</categories><comments>Accepted for publication as lecture note in IEEE Signal Processing
  Magazine, 13 pages, 4 figures. The results can be reproduced using the
  following Matlab code: https://github.com/lucasanguinetti/ ln-game-theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this lecture note, we introduce the basic concepts of game theory (GT), a
branch of mathematics traditionally studied and applied in the areas of
economics, political science, and biology, which has emerged in the last
fifteen years as an effective framework for communications, networking, and
signal processing (SP). The real catalyzer has been the blooming of all issues
related to distributed networks, in which the nodes can be modeled as players
in a game competing for system resources. Some relevant notions of GT are
introduced by elaborating on a simple application in the context of wireless
communications, notably the power control in an interference channel (IC) with
two transmitters and two receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06608</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06608</id><created>2015-04-14</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author></authors><title>Leveraging disjoint communities for detecting overlapping community
  structure</title><categories>cs.SI physics.soc-ph</categories><comments>19 pages, 8 figures, 4 tables in Journal of Statistical Mechanics:
  Theory and Experiment (JSTAT), 2015. arXiv admin note: text overlap with
  arXiv:1110.5813 by other authors</comments><doi>10.1088/1742-5468/2015/05/P05017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network communities represent mesoscopic structure for understanding the
organization of real-world networks, where nodes often belong to multiple
communities and form overlapping community structure in the network. Due to
non-triviality in finding the exact boundary of such overlapping communities,
this problem has become challenging, and therefore huge effort has been devoted
to detect overlapping communities from the network.
  In this paper, we present PVOC (Permanence based Vertex-replication algorithm
for Overlapping Community detection), a two-stage framework to detect
overlapping community structure. We build on a novel observation that
non-overlapping community structure detected by a standard disjoint community
detection algorithm from a network has high resemblance with its actual
overlapping community structure, except the overlapping part. Based on this
observation, we posit that there is perhaps no need of building yet another
overlapping community finding algorithm; but one can efficiently manipulate the
output of any existing disjoint community finding algorithm to obtain the
required overlapping structure. We propose a new post-processing technique that
by combining with any existing disjoint community detection algorithm, can
suitably process each vertex using a new vertex-based metric, called
permanence, and thereby finds out overlapping candidates with their community
memberships. Experimental results on both synthetic and large real-world
networks show that PVOC significantly outperforms six state-of-the-art
overlapping community detection algorithms in terms of high similarity of the
output with the ground-truth structure. Thus our framework not only finds
meaningful overlapping communities from the network, but also allows us to put
an end to the constant effort of building yet another overlapping community
detection algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06622</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06622</id><created>2015-04-24</created><authors><author><keyname>Chen</keyname><forenames>Tianran</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author></authors><title>An index-resolved fixed-point homotopy and potential energy landscapes</title><categories>cond-mat.soft cond-mat.mtrl-sci cs.NA math.DS math.OC</categories><comments>7 pages, 2 figures</comments><report-no>ADP-15-15/T917</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stationary points (SPs) of the potential energy landscapes can be classified
by their Morse index, i.e., the number of negative eigenvalues of the Hessian
evaluated at the SPs. In understanding chemical clusters through their
potential energy landscapes, only SPs of a particular Morse index are needed.
We propose a modification of the &quot;fixed-point homotopy&quot; method which can be
used to directly target stationary points of a specified Morse index. We
demonstrate the effectiveness of our approach by applying it to the
Lennard-Jones clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06631</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06631</id><created>2015-04-24</created><updated>2015-11-25</updated><authors><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Solovey</keyname><forenames>Kiril</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Motion Planning for Multi-Link Robots by Implicit Configuration-Space
  Tiling</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of motion-planning for free-flying multi-link robots and
develop a sampling-based algorithm that is specifically tailored for the task.
Our work is based on the simple observation that the set of configurations for
which the robot is self-collision free is independent of the obstacles or of
the exact placement of the robot. This allows to eliminate the need to perform
costly self-collision checks online when solving motion-planning problems,
assuming some offline preprocessing. In particular, given a specific robot type
our algorithm precomputes a tiling roadmap, which efficiently and implicitly
encodes the self-collision free (sub-)space over the entire configuration
space, where the latter can be infinite for that matter. To answer any query,
in any given scenario, we traverse the tiling roadmap while only testing for
collisions with obstacles. Our algorithm suggests more flexibility than the
prevailing paradigm in which a precomputed roadmap depends both on the robot
and on the scenario at hand. We show through various simulations the
effectiveness of this approach on open and closed-chain multi-link robots,
where in some settings our algorithm is more than fifty times faster than the
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06634</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06634</id><created>2015-04-24</created><updated>2015-12-11</updated><authors><author><keyname>Heydari</keyname><forenames>Babak</forenames></author><author><keyname>Mosleh</keyname><forenames>Mohsen</forenames></author><author><keyname>Dalili</keyname><forenames>Kia</forenames></author></authors><title>Efficient Network Structures with Separable Heterogeneous Connection
  Costs</title><categories>q-fin.EC cs.SI physics.soc-ph</categories><comments>9 pages</comments><journal-ref>Economics Letters, Vol. 134, September 2015, 82-85</journal-ref><doi>10.1016/j.econlet.2015.06.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a heterogeneous connection model for network formation to
capture the effect of cost heterogeneity on the structure of efficient
networks. In the proposed model, connection costs are assumed to be separable,
which means the total connection cost for each agent is uniquely proportional
to its degree. For these sets of networks, we provide the analytical solution
for the efficient network and discuss stability impli- cations. We show that
the efficient network exhibits a core-periphery structure, and for a given
density, we find a lower bound for clustering coefficient of the efficient
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06647</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06647</id><created>2015-04-24</created><updated>2015-09-10</updated><authors><author><keyname>Fischer</keyname><forenames>Johannes</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author></authors><title>Approximating LZ77 via Small-Space Multiple-Pattern Matching</title><categories>cs.DS</categories><comments>preliminary version presented at ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Karp-Rabin string matching to handle multiple patterns in
$\mathcal{O}(n \log n + m)$ time and $\mathcal{O}(s)$ space, where $n$ is the
length of the text and $m$ is the total length of the $s$ patterns, returning
correct answers with high probability. As a prime application of our algorithm,
we show how to approximate the LZ77 parse of a string of length $n$. If the
optimal parse consists of $z$ phrases, using only $\mathcal{O}(z)$ working
space we can return a parse consisting of at most $(1+\varepsilon)z$ phrases in
$\mathcal{O}(\varepsilon^{-1}n\log n)$ time, for any $\varepsilon\in (0,1]$. As
previous quasilinear-time algorithms for LZ77 use $\Omega(n/\textrm{polylog
}n)$ space, but $z$ can be exponentially small in $n$, these improvements in
space are substantial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06650</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06650</id><created>2015-04-24</created><authors><author><keyname>Neelakantan</keyname><forenames>Arvind</forenames></author><author><keyname>Collins</keyname><forenames>Michael</forenames></author></authors><title>Learning Dictionaries for Named Entity Recognition using Minimal
  Supervision</title><categories>cs.CL stat.ML</categories><comments>In 14th Conference of the European Chapter of the Association for
  Computational Linguistic, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an approach for automatic construction of dictionaries
for Named Entity Recognition (NER) using large amounts of unlabeled data and a
few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower
dimensional embeddings (representations) for candidate phrases and classify
these phrases using a small number of labeled examples. Our method achieves
16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER
respectively. We also show that by adding candidate phrase embeddings as
features in a sequence tagger gives better performance compared to using word
embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06654</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06654</id><created>2015-04-24</created><authors><author><keyname>Neelakantan</keyname><forenames>Arvind</forenames></author><author><keyname>Shankar</keyname><forenames>Jeevan</forenames></author><author><keyname>Passos</keyname><forenames>Alexandre</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Efficient Non-parametric Estimation of Multiple Embeddings per Word in
  Vector Space</title><categories>cs.CL stat.ML</categories><comments>In Conference on Empirical Methods in Natural Language Processing,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is rising interest in vector-space word embeddings and their use in
NLP, especially given recent methods for their fast estimation at very large
scale. Nearly all this work, however, assumes a single vector per word type
ignoring polysemy and thus jeopardizing their usefulness for downstream tasks.
We present an extension to the Skip-gram model that efficiently learns multiple
embeddings per word type. It differs from recent related work by jointly
performing word sense discrimination and embedding learning, by
non-parametrically estimating the number of senses per word type, and by its
efficiency and scalability. We present new state-of-the-art results in the word
similarity in context task and demonstrate its scalability by training with one
machine on a corpus of nearly 1 billion tokens in less than 6 hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06658</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06658</id><created>2015-04-24</created><authors><author><keyname>Neelakantan</keyname><forenames>Arvind</forenames></author><author><keyname>Chang</keyname><forenames>Ming-Wei</forenames></author></authors><title>Inferring Missing Entity Type Instances for Knowledge Base Completion:
  New Dataset and Methods</title><categories>cs.CL stat.ML</categories><comments>North American Chapter of the Association for Computational
  Linguistics- Human Language Technologies, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of previous work in knowledge base (KB) completion has focused on the
problem of relation extraction. In this work, we focus on the task of inferring
missing entity type instances in a KB, a fundamental task for KB competition
yet receives little attention. Due to the novelty of this task, we construct a
large-scale dataset and design an automatic evaluation methodology. Our
knowledge base completion method uses information within the existing KB and
external information from Wikipedia. We show that individual methods trained
with a global objective that considers unobserved cells from both the entity
and the type side gives consistently higher quality predictions compared to
baseline methods. We also perform manual evaluation on a small subset of the
data to verify the effectiveness of our knowledge base completion methods and
the correctness of our proposed automatic evaluation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06660</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06660</id><created>2015-04-24</created><updated>2015-06-12</updated><authors><author><keyname>Kalajdzic</keyname><forenames>Kenan</forenames></author><author><keyname>Jegourel</keyname><forenames>Cyrille</forenames></author><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Smolka</keyname><forenames>Scott A.</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author></authors><title>Model Checking as Control: Feedback Control for Statistical Model
  Checking of Cyber-Physical Systems</title><categories>cs.SY</categories><comments>There are somethings to be checked more carefully</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce feedback-control statistical system checking (FC-SSC), a new
approach to statistical model checking that exploits principles of
feedback-control for the analysis of cyber-physical systems (CPS). FC-SSC uses
stochastic system identification to learn a CPS model, importance sampling to
estimate the CPS state, and importance splitting to control the CPS so that the
probability that the CPS satisfies a given property can be efficiently
inferred. We illustrate the utility of FC-SSC on two example applications, each
of which is simple enough to be easily understood, yet complex enough to
exhibit all of FC-SCC's features. To the best of our knowledge, FC-SSC is the
first statistical system checker to efficiently estimate the probability of
rare events in realistic CPS applications or in any complex probabilistic
program whose model is either not available, or is infeasible to derive through
static-analysis techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06662</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06662</id><created>2015-04-24</created><updated>2015-05-27</updated><authors><author><keyname>Neelakantan</keyname><forenames>Arvind</forenames></author><author><keyname>Roth</keyname><forenames>Benjamin</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Compositional Vector Space Models for Knowledge Base Completion</title><categories>cs.CL stat.ML</categories><comments>The 53rd Annual Meeting of the Association for Computational
  Linguistics and The 7th International Joint Conference of the Asian
  Federation of Natural Language Processing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge base (KB) completion adds new facts to a KB by making inferences
from existing facts, for example by inferring with high likelihood
nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop
relational synonyms like this, or use as evidence a multi-hop relational path
treated as an atomic feature, like bornIn(X,Z) -&gt; containedIn(Z,Y). This paper
presents an approach that reasons about conjunctions of multi-hop relations
non-atomically, composing the implications of a path using a recursive neural
network (RNN) that takes as inputs vector embeddings of the binary relation in
the path. Not only does this allow us to generalize to paths unseen at training
time, but also, with a single high-capacity RNN, to predict new relation types
not seen when the compositional model was trained (zero-shot learning). We
assemble a new dataset of over 52M relational triples, and show that our method
improves over a traditional classifier by 11%, and a method leveraging
pre-trained embeddings by 7%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06663</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06663</id><created>2015-04-24</created><authors><author><keyname>Shi</keyname><forenames>Z.</forenames></author></authors><title>A Scheduling Model of Battery-powered Embedded System</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental theory on battery-powered cyber-physical systems (CPS) calls for
dynamic models that are able to describe and predict the status of processors
and batteries at any given time. We believe that the idealized system of single
processor powered by single battery (SPSB) can be viewed as a generic case for
the modeling effort. This paper introduces a dynamic model for multiple
aperiodic tasks on a SPSB system under a scheduling algorithm that resembles
the rate monotonic scheduling (RMS) within finite time windows. The model
contains two major modules. The first module is an online battery capacity
model based on the Rakhmatov-Vrudhula-Wallach (RVW) model. This module provides
predictions of remaining battery capacity based on the knowledge of the battery
discharging current. The second module is a dynamical scheduling model that can
predict the scheduled behavior of tasks within any finite time window, without
the need to store all past information about each task before the starting time
of the finite time window. The module provides a complete analytical
description of the relationship among tasks and it delineates all possible
modes of the processor utilization as square-wave functions of time. The two
modules i.e. the scheduling model and the battery model are integrated to
obtain a hybrid scheduling model that describes the dynamic behaviors of the
SPSB system. Our effort may have demonstrated that through dynamic modeling,
different components of CPS may be integrated under a unified theoretical
framework centered around hybrid systems theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06664</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06664</id><created>2015-04-24</created><authors><author><keyname>He</keyname><forenames>Q.</forenames></author></authors><title>Fast and Rigorous DC Solution in Finite Element Method for Integrated
  Circuit Analysis</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale circuit simulation, such as power delivery network analysis, has
become increasingly challenge in the VLSI design verification flow. Power
delivery network can be simulated by both SPICE-type circuit-based model and
eletromagnetics-based model when full-wave accuracy is desired. In the early
time of the time domain finite element simulation for integrated circuit, the
modes having the highest eigenvalues supported by the numerical system will be
excited. Because of the band limited source, after the early time, the modes
having a resonance frequency well beyond the input frequency band will die
down, and all physically important high-order modes and DC mode will show up
and become dominant. Among these modes, the DC mode is the last one to show up.
Although the convergence criterion is not applied on the DC mode, the existence
of DC mode in the field solution will deteriorate the convergence rate of the
first several high order modes. Therefore, this paper first analyzed the
mathematic characteristics of the DC mode and proposed a rigorous and fast
solution to extract the DC mode from the numerical system in order to speed up
the convergence rate. Experimental results demonstrated the robustness and
superior performance of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06665</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06665</id><created>2015-04-24</created><updated>2015-04-28</updated><authors><author><keyname>Pust</keyname><forenames>Michael</forenames></author><author><keyname>Hermjakob</keyname><forenames>Ulf</forenames></author><author><keyname>Knight</keyname><forenames>Kevin</forenames></author><author><keyname>Marcu</keyname><forenames>Daniel</forenames></author><author><keyname>May</keyname><forenames>Jonathan</forenames></author></authors><title>Using Syntax-Based Machine Translation to Parse English into Abstract
  Meaning Representation</title><categories>cs.CL cs.AI</categories><comments>10 pages, 8 figures</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parser for Abstract Meaning Representation (AMR). We treat
English-to-AMR conversion within the framework of string-to-tree, syntax-based
machine translation (SBMT). To make this work, we transform the AMR structure
into a form suitable for the mechanics of SBMT and useful for modeling. We
introduce an AMR-specific language model and add data and features drawn from
semantic resources. Our resulting AMR parser improves upon state-of-the-art
results by 7 Smatch points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06667</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06667</id><created>2015-04-24</created><updated>2015-08-11</updated><authors><author><keyname>Fish</keyname><forenames>Benjamin</forenames></author><author><keyname>Caceres</keyname><forenames>Rajmonda S.</forenames></author></authors><title>Handling oversampling in dynamic networks using link prediction</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>ECML/PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oversampling is a common characteristic of data representing dynamic
networks. It introduces noise into representations of dynamic networks, but
there has been little work so far to compensate for it. Oversampling can affect
the quality of many important algorithmic problems on dynamic networks,
including link prediction. Link prediction seeks to predict edges that will be
added to the network given previous snapshots. We show that not only does
oversampling affect the quality of link prediction, but that we can use link
prediction to recover from the effects of oversampling. We also introduce a
novel generative model of noise in dynamic networks that represents
oversampling. We demonstrate the results of our approach on both synthetic and
real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06672</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06672</id><created>2015-04-24</created><authors><author><keyname>Lin</keyname><forenames>Jian-Hong</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Locating influential nodes via dynamics-sensitive centrality</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>6 pages, 1 table and 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With great theoretical and practical significance, locating influential nodes
of complex networks is a promising issues. In this paper, we propose a
dynamics-sensitive (DS) centrality that integrates topological features and
dynamical properties. The DS centrality can be directly applied in locating
influential spreaders. According to the empirical results on four real networks
for both susceptible-infected-recovered (SIR) and susceptible-infected (SI)
spreading models, the DS centrality is much more accurate than degree,
$k$-shell index and eigenvector centrality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06678</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06678</id><created>2015-04-24</created><authors><author><keyname>Veeriah</keyname><forenames>Vivek</forenames></author><author><keyname>Zhuang</keyname><forenames>Naifan</forenames></author><author><keyname>Qi</keyname><forenames>Guo-Jun</forenames></author></authors><title>Differential Recurrent Neural Networks for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long short-term memory (LSTM) neural network is capable of processing
complex sequential information since it utilizes special gating schemes for
learning representations from long input sequences. It has the potential to
model any sequential time-series data, where the current hidden state has to be
considered in the context of the past hidden states. This property makes LSTM
an ideal choice to learn the complex dynamics of various actions.
Unfortunately, the conventional LSTMs do not consider the impact of
spatio-temporal dynamics corresponding to the given salient motion patterns,
when they gate the information that ought to be memorized through time. To
address this problem, we propose a differential gating scheme for the LSTM
neural network, which emphasizes on the change in information gain caused by
the salient motions between the successive frames. This change in information
gain is quantified by Derivative of States (DoS), and thus the proposed LSTM
model is termed as differential Recurrent Neural Network (dRNN). We demonstrate
the effectiveness of the proposed model by automatically recognizing actions
from the real-world 2D and 3D human action datasets. Our study is one of the
first works towards demonstrating the potential of learning complex time-series
representations via high-order derivatives of states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06681</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06681</id><created>2015-04-25</created><authors><author><keyname>Chen</keyname><forenames>Niangjun</forenames></author><author><keyname>Agarwal</keyname><forenames>Anish</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Andrew</keyname><forenames>Lachlan L. H.</forenames></author></authors><title>Online Convex Optimization Using Predictions</title><categories>cs.LG</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making use of predictions is a crucial, but under-explored, area of online
algorithms. This paper studies a class of online optimization problems where we
have external noisy predictions available. We propose a stochastic prediction
error model that generalizes prior models in the learning and stochastic
control communities, incorporates correlation among prediction errors, and
captures the fact that predictions improve as time passes. We prove that
achieving sublinear regret and constant competitive ratio for online algorithms
requires the use of an unbounded prediction window in adversarial settings, but
that under more realistic stochastic prediction error models it is possible to
use Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear
regret and constant competitive ratio in expectation using only a
constant-sized prediction window. Furthermore, we show that the performance of
AFHC is tightly concentrated around its mean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06692</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06692</id><created>2015-04-25</created><updated>2015-10-01</updated><authors><author><keyname>Mao</keyname><forenames>Junhua</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Learning like a Child: Fast Novel Visual Concept Learning from Sentence
  Descriptions of Images</title><categories>cs.CV cs.CL cs.LG</categories><comments>ICCV 2015 camera ready version. We add much more novel visual
  concepts in the NVC dataset and have released it, see
  http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html</comments><acm-class>I.2.6; I.2.7; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the task of learning novel visual concepts, and
their interactions with other concepts, from a few images with sentence
descriptions. Using linguistic context and visual features, our method is able
to efficiently hypothesize the semantic meaning of new words and add them to
its word dictionary so that they can be used to describe images which contain
these novel concepts. Our method has an image captioning module based on m-RNN
with several improvements. In particular, we propose a transposed weight
sharing scheme, which not only improves performance on image captioning, but
also makes the model more suitable for the novel concept learning task. We
propose methods to prevent overfitting the new concepts. In addition, three
novel concept datasets are constructed for this new task. In the experiments,
we show that our method effectively learns novel visual concepts from a few
examples without disturbing the previously learned concepts. The project page
is http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06700</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06700</id><created>2015-04-25</created><authors><author><keyname>Mu</keyname><forenames>Kedian</forenames></author><author><keyname>Wang</keyname><forenames>Kewen</forenames></author><author><keyname>Wen</keyname><forenames>Lian</forenames></author></authors><title>Preferential Multi-Context Systems</title><categories>cs.AI</categories><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-context systems (MCS) presented by Brewka and Eiter can be considered
as a promising way to interlink decentralized and heterogeneous knowledge
contexts. In this paper, we propose preferential multi-context systems (PMCS),
which provide a framework for incorporating a total preorder relation over
contexts in a multi-context system. In a given PMCS, its contexts are divided
into several parts according to the total preorder relation over them,
moreover, only information flows from a context to ones of the same part or
less preferred parts are allowed to occur. As such, the first $l$ preferred
parts of an PMCS always fully capture the information exchange between contexts
of these parts, and then compose another meaningful PMCS, termed the
$l$-section of that PMCS. We generalize the equilibrium semantics for an MCS to
the (maximal) $l_{\leq}$-equilibrium which represents belief states at least
acceptable for the $l$-section of an PMCS. We also investigate inconsistency
analysis in PMCS and related computational complexity issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06709</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06709</id><created>2015-04-25</created><updated>2015-05-07</updated><authors><author><keyname>Hien</keyname><forenames>L. V.</forenames></author><author><keyname>Trinh</keyname><forenames>H.</forenames></author></authors><title>Exponential stability of time-delay systems via new weighted integral
  inequalities</title><categories>math.OC cs.SY</categories><comments>Submitted</comments><msc-class>34D20, 34K20, 93D05</msc-class><journal-ref>Applied Mathematics and Computation 275 (2016), pp. 335-344</journal-ref><doi>10.1016/j.amc.2015.11.076</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new weighted integral inequalities (WIIs) are first derived by
refining the Jensen single and double inequalities. It is shown that the newly
derived inequalities in this paper encompass both the Jensen inequality and its
most recent improvements based on Wirtinger integral inequality. The potential
capability of the proposed WIIs is demonstrated through applications in
exponential stability analysis for some classes of time-delay systems in the
framework of linear matrix inequalities (LMIs). The effectiveness and least
conservativeness of the derived stability conditions using WIIs are shown by
various numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06712</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06712</id><created>2015-04-25</created><updated>2015-06-08</updated><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Faster Lightweight Lempel-Ziv Parsing</title><categories>cs.DS</categories><comments>16 pages, 5 figures, accepted to MFCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that computes the Lempel-Ziv decomposition in
$O(n(\log\sigma + \log\log n))$ time and $n\log\sigma + \epsilon n$ bits of
space, where $\epsilon$ is a constant rational parameter, $n$ is the length of
the input string, and $\sigma$ is the alphabet size. The $n\log\sigma$ bits in
the space bound are for the input string itself which is treated as read-only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06719</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06719</id><created>2015-04-25</created><authors><author><keyname>Marvaniya</keyname><forenames>Smit</forenames></author><author><keyname>Gupta</keyname><forenames>Raj</forenames></author><author><keyname>Mittal</keyname><forenames>Anurag</forenames></author></authors><title>Adaptive Locally Affine-Invariant Shape Matching</title><categories>cs.CV</categories><comments>submitted to Image and Vision Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching deformable objects using their shapes is an important problem in
computer vision since shape is perhaps the most distinguishable characteristic
of an object. The problem is difficult due to many factors such as intra-class
variations, local deformations, articulations, viewpoint changes and missed and
extraneous contour portions due to errors in shape extraction. While small
local deformations has been handled in the literature by allowing some leeway
in the matching of individual contour points via methods such as Chamfer
distance and Hausdorff distance, handling more severe deformations and
articulations has been done by applying local geometric corrections such as
similarity or affine. However, determining which portions of the shape should
be used for the geometric corrections is very hard, although some methods have
been tried. In this paper, we address this problem by an efficient search for
the group of contour segments to be clustered together for a geometric
correction using Dynamic Programming by essentially searching for the
segmentations of two shapes that lead to the best matching between them. At the
same time, we allow portions of the contours to remain unmatched to handle
missing and extraneous contour portions. Experiments indicate that our method
outperforms other algorithms, especially when the shapes to be matched are more
complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06726</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06726</id><created>2015-04-25</created><authors><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Valicov</keyname><forenames>Petru</forenames></author></authors><title>Planar digraphs without large acyclic sets</title><categories>math.CO cs.DM</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed graph, an acyclic set is a set of vertices inducing a
subgraph with no directed cycle. In this note we show that there exist oriented
planar graphs of order $n$ for which the size of the maximum acyclic set is at
most $\lceil \frac{n+1}{2} \rceil$, for any $n$. This disproves a conjecture of
Harutyunyan and shows that a question of Albertson is best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06729</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06729</id><created>2015-04-25</created><updated>2015-11-22</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Zhong</keyname><forenames>Peilin</forenames></author></authors><title>Optimal Principal Component Analysis in Distributed and Streaming Models</title><categories>cs.DS</categories><comments>working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Principal Component Analysis (PCA) problem in the distributed
and streaming models of computation. Given a matrix $A \in R^{m \times n},$ a
rank parameter $k &lt; rank(A)$, and an accuracy parameter $0 &lt; \epsilon &lt; 1$, we
want to output an $m \times k$ orthonormal matrix $U$ for which $$ || A - U U^T
A ||_F^2 \le \left(1 + \epsilon \right) \cdot || A - A_k||_F^2, $$ where $A_k
\in R^{m \times n}$ is the best rank-$k$ approximation to $A$.
  This paper provides improved algorithms for distributed PCA and streaming
PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06731</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06731</id><created>2015-04-25</created><authors><author><keyname>Morizumi</keyname><forenames>Hiroki</forenames></author></authors><title>Lower Bounds for the Size of Nondeterministic Circuits</title><categories>cs.CC</categories><comments>the submitted version to COCOON'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nondeterministic circuits are a nondeterministic computation model in circuit
complexity theory. In this paper, we prove a $3(n-1)$ lower bound for the size
of nondeterministic $U_2$-circuits computing the parity function. It is known
that the minimum size of (deterministic) $U_2$-circuits computing the parity
function exactly equals $3(n-1)$. Thus, our result means that nondeterministic
computation is useless to compute the parity function by $U_2$-circuits and
cannot reduce the size from $3(n-1)$. To the best of our knowledge, this is the
first nontrivial lower bound for the size of nondeterministic circuits
(including formulas, constant depth circuits, and so on) with unlimited
nondeterminism for an explicit Boolean function. We also discuss an approach to
proving lower bounds for the size of deterministic circuits via lower bounds
for the size of nondeterministic restricted circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06734</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06734</id><created>2015-04-25</created><authors><author><keyname>Kochnev</keyname><forenames>Anton</forenames></author><author><keyname>Savelov</keyname><forenames>Nicolai</forenames></author></authors><title>Symmetric matrix inversion using modified Gaussian elimination</title><categories>cs.MS</categories><comments>5 pages, 6 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present two different variants of method for symmetric
matrix inversion, based on modified Gaussian elimination. Both methods avoid
computation of square roots and have a reduced machine time's spending.
Further, both of them can be used efficiently not only for positive (semi-)
definite, but for any non-singular symmetric matrix inversion. We use
simulation to verify results, which represented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06736</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06736</id><created>2015-04-25</created><updated>2015-09-03</updated><authors><author><keyname>Kunjir</keyname><forenames>Mayuresh</forenames></author><author><keyname>Fain</keyname><forenames>Brandon</forenames></author><author><keyname>Munagala</keyname><forenames>Kamesh</forenames></author><author><keyname>Babu</keyname><forenames>Shivnath</forenames></author></authors><title>ROBUS: Fair Cache Allocation for Multi-tenant Data-parallel Workloads</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems for processing big data---e.g., Hadoop, Spark, and massively parallel
databases---need to run workloads on behalf of multiple tenants simultaneously.
The abundant disk-based storage in these systems is usually complemented by a
smaller, but much faster, {\em cache}. Cache is a precious resource: Tenants
who get to use cache can see two orders of magnitude performance improvement.
Cache is also a limited and hence shared resource: Unlike a resource like a CPU
core which can be used by only one tenant at a time, a cached data item can be
accessed by multiple tenants at the same time. Cache, therefore, has to be
shared by a multi-tenancy-aware policy across tenants, each having a unique set
of priorities and workload characteristics.
  In this paper, we develop cache allocation strategies that speed up the
overall workload while being {\em fair} to each tenant. We build a novel
fairness model targeted at the shared resource setting that incorporates not
only the more standard concepts of Pareto-efficiency and sharing incentive, but
also define envy freeness via the notion of {\em core} from cooperative game
theory. Our cache management platform, ROBUS, uses randomization over small
time batches, and we develop a proportionally fair allocation mechanism that
satisfies the core property in expectation. We show that this algorithm and
related fair algorithms can be approximated to arbitrary precision in
polynomial time. We evaluate these algorithms on a ROBUS prototype implemented
on Spark with RDD store used as cache. Our evaluation on a synthetically
generated industry-standard workload shows that our algorithms provide a
speedup close to performance optimal algorithms while guaranteeing fairness
across tenants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06740</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06740</id><created>2015-04-25</created><authors><author><keyname>Srivastava</keyname><forenames>Siddharth</forenames></author></authors><title>SIFT Vs SURF: Quantifying the Variation in Transformations</title><categories>cs.CV</categories><report-no>v01.0</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the robustness of SIFT and SURF against different image
transforms (rigid body, similarity, affine and projective) by quantitatively
analyzing the variations in the extent of transformations. Previous studies
have been comparing the two techniques on absolute transformations rather than
the specific amount of deformation caused by the transformation. The paper
establishes an exhaustive empirical analysis of such deformations and matching
capability of SIFT and SURF with variations in matching parameters and the
amount of tolerance. This is helpful in choosing the specific use case for
applying these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06741</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06741</id><created>2015-04-25</created><authors><author><keyname>Levin</keyname><forenames>Stanislav</forenames></author><author><keyname>Yehudai</keyname><forenames>Amiram</forenames></author></authors><title>Collaborative Real Time Coding or How to Avoid the Dreaded Merge</title><categories>cs.SE</categories><comments>This paper was written on 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software engineers who collaborate to develop software in teams often have to
manually merge changes they made to a module (e.g. a class), because the change
conflicts with one that has just been made by another engineer to the same or
another module (e.g. a supplier class). This is due to the fact that engineers
edit code separately, and loosely coordinate their work via a source control or
a software configuration management system (SCM). This work proposes to
eliminate almost all the need to manually merge a recent change, by proposing a
Collaborative Real Time Coding approach. In this approach, valid changes to the
code are seen by others in real time, but intermediate changes (that cause the
code not to compile) result in blocking other engineers from making changes
related to the entity (e.g. method) being modified, while allowing them to work
on most of the system. The subject of collaborative real time editing systems
has been studied for the past 20 years. Research in this field has mostly
concentrated on collaborative textual and graphical editing. In this work we
address the challenges involved in designing a collaborative real time coding
system, as well as present the major differences when compared to collaborative
editing of plain text. We then present a prototype plug in for the Eclipse
Integrated Development Environment (IDE) that allows for a collaborative coding
to take place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06742</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06742</id><created>2015-04-25</created><updated>2015-04-28</updated><authors><author><keyname>Levin</keyname><forenames>Stanislav</forenames></author><author><keyname>Yehudai</keyname><forenames>Amiram</forenames></author></authors><title>Improving software team collaboration with Synchronized Software
  Development</title><categories>cs.SE</categories><comments>This paper was written on 2012, added a footnote acknowledging ISF's
  support. arXiv admin note: text overlap with arXiv:1504.06741</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective collaboration is a key factor in the success of a software project
developed by a team. In this work, we suggest the approach of Synchronized
Software Development (SSD), which promotes a new mechanism of collaboration in
general, and for code synchronization in particular. In SSD, code changes made
by one developer are automatically propagated to others as long as they keep
the code free of compilation errors. Changes that introduce compilation errors
are not propagated until the errors are fixed. Moreover, other developers are
restricted from concurrently editing the entities involved in these changes.
While in this state, developers are, however, free to modify the rest of the
entities. The novelty of our approach is that it actively synchronizes
developers with the latest error free version of the source code, preventing
possible conflicts and merges that may arise due to concurrent changes made by
fellow team members. SSD also allows for a more transparent an practically near
real time awareness of new code that is being introduced by multiple
developers. We built CSI (Code Synchronizing Intelligence), a prototype
demonstrating key features of SSD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06743</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06743</id><created>2015-04-25</created><authors><author><keyname>Chae</keyname><forenames>Sung Ho</forenames></author><author><keyname>Jeong</keyname><forenames>Cheol</forenames></author></authors><title>Degrees of Freedom of Interference Channels with Hybrid Beam-forming</title><categories>cs.IT math.IT</categories><comments>6 figures, 27pages, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sum degrees of freedom (DoF) of interference channels with
hybrid beam-forming in which hybrid beam-forming composed of analog and digital
precodings is employed at each node. For the two-user case, we completely
characterize the sum DoF for an arbitrary number of antennas and RF chains by
developing an achievable scheme optimized for the hybrid beam-forming structure
and deriving its matching upper bound. For a general K-user case, we focus on a
symmetric case and obtain lower and upper bounds on the sum DoF, which are
tight for certain cases. The results show that hybrid beam-forming can increase
the sum DoF of interference channel under certain conditions while it cannot
improve the sum DoFs of point-to-point channel, multiple access channel, and
broadcast channel. The key insights on this gain is that hybrid beam-forming
enables users to manage inter user interference better, and thus each user can
increase the dimension of interference-free signal space for its own desired
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06744</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06744</id><created>2015-04-25</created><updated>2015-07-15</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Haddad</keyname><forenames>Axel</forenames></author><author><keyname>Monmege</keyname><forenames>Benjamin</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Renault</keyname><forenames>Gabriel</forenames></author></authors><title>Quantitative Games under Failures</title><categories>cs.GT</categories><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generalisation of sabotage games, a model of dynamic network games
introduced by van Benthem. The original definition of the game is inherently
finite and therefore does not allow one to model infinite processes. We propose
an extension of the sabotage games in which the first player (Runner) traverses
an arena with dynamic weights determined by the second player (Saboteur). In
our model of quantitative sabotage games, Saboteur is now given a budget that
he can distribute amongst the edges of the graph, whilst Runner attempts to
minimise the quantity of budget witnessed while completing his task. We show
that, on the one hand, for most of the classical cost functions considered in
the literature, the problem of determining if Runner has a strategy to ensure a
cost below some threshold is EXPTIME-complete. On the other hand, if the budget
of Saboteur is fixed a priori, then the problem is in PTIME for most cost
functions. Finally, we show that restricting the dynamics of the game also
leads to better complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06746</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06746</id><created>2015-04-25</created><updated>2015-05-12</updated><authors><author><keyname>Lemos</keyname><forenames>Jo&#xe3;o S.</forenames></author><author><keyname>Ros&#xe1;rio</keyname><forenames>Francisco</forenames></author><author><keyname>Monteiro</keyname><forenames>Francisco A.</forenames></author><author><keyname>Xavier</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Rodrigues</keyname><forenames>Ant&#xf3;nio</forenames></author></authors><title>Massive MIMO Full-Duplex Relaying with Optimal Power Allocation for
  Independent Multipairs</title><categories>cs.IT math.IT</categories><comments>Accepted to the 16th IEEE International Workshop on Signal Processing
  Advances in Wireless Communications - SPAWC, Stockholm, Sweden 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the help of an in-band full-duplex relay station, it is possible to
simultaneously transmit and receive signals from multiple users. The
performance of such system can be greatly increased when the relay station is
equipped with a large number of antennas on both transmitter and receiver
sides. In this paper, we exploit the use of massive arrays to effectively
suppress the loopback interference (LI) of a decode-and-forward relay (DF) and
evaluate the performance of the end-to-end (e2e) transmission. This paper
assumes imperfect channel state information is available at the relay and
designs a minimum mean-square error (MMSE) filter to mitigate the interference.
Subsequently, we adopt zero-forcing (ZF) filters for both detection and
beamforming. The performance of such system is evaluated in terms of bit error
rate (BER) at both relay and destinations, and an optimal choice for the
transmission power at the relay is shown. We then propose a complexity
efficient optimal power allocation (OPA) algorithm that, using the channel
statistics, computes the minimum power that satisfies the rate constraints of
each pair. The results obtained via simulation show that when both MMSE
filtering and OPA method are used, better values for the energy efficiency are
attained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06749</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06749</id><created>2015-04-25</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Energy-Efficient Symbol-Level Precoding in Multiuser MISO Based on
  Relaxed Detection Region</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE transactions on Wireless Communications. arXiv
  admin note: substantial text overlap with arXiv:1408.4700</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of exploiting interference among
simultaneous multiuser transmissions in the downlink of multiple-antenna
systems. Using symbol-level precoding, a new approach towards addressing the
multiuser interference is discussed through jointly utilizing the channel state
information (CSI) and data information (DI). The interference among the data
streams is transformed under certain conditions to a useful signal that can
improve the signal-to-interference noise ratio (SINR) of the downlink
transmissions and as a result the system's energy efficiency. In this context,
new constructive interference precoding techniques that tackle the transmit
power minimization (min power) with individual SINR constraints at each user's
receiver have been proposed. In this paper, we generalize the CI precoding
design under the assumption that the received MPSK symbol can reside in a
relaxed region in order to be correctly detected. Moreover, a weighted
maximization of the minimum SNR among all users is studied taking into account
the relaxed detection region. Symbol error rate analysis (SER) for the proposed
precoding is discussed to characterize the tradeoff between transmit power
reduction and SER increase due to the relaxation. Based on this tradeoff, the
energy efficiency performance of the proposed technique is analyzed. Finally,
extensive numerical results show that the proposed schemes outperform other
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06750</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06750</id><created>2015-04-25</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Constructive Interference through Symbol Level Precoding for Multi-level
  Modulation</title><categories>cs.IT math.IT</categories><comments>6 pages, submitted for possible publication in Globecom. arXiv admin
  note: substantial text overlap with arXiv:1408.4700</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constructive interference concept in the downlink of multiple-antenna
systems is addressed in this paper. The concept of the joint exploitation of
the channel state information (CSI) and data information (DI) is discussed.
Using symbol-level precoding, the interference between data streams is
transformed Under certain conditions into useful signal that can improve the
signal to interference noise ratio (SINR) of the downlink transmissions. In the
previous work, different constructive interference precoding techniques have
been proposed for the MPSK scenario. In this context, a novel constructive
interference precoding technique that tackles the transmit power minimization
(min-power) with individual SINR constraints at each user's receivers is
proposed assuming MQAM modulation. Extensive simulations are performed to
validate the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06755</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06755</id><created>2015-04-25</created><updated>2015-05-20</updated><authors><author><keyname>Xu</keyname><forenames>Pingmei</forenames></author><author><keyname>Ehinger</keyname><forenames>Krista A</forenames></author><author><keyname>Zhang</keyname><forenames>Yinda</forenames></author><author><keyname>Finkelstein</keyname><forenames>Adam</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking</title><categories>cs.CV</categories><comments>9 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional eye tracking requires specialized hardware, which means
collecting gaze data from many observers is expensive, tedious and slow.
Therefore, existing saliency prediction datasets are order-of-magnitudes
smaller than typical datasets for other vision recognition tasks. The small
size of these datasets limits the potential for training data intensive
algorithms, and causes overfitting in benchmark evaluation. To address this
deficiency, this paper introduces a webcam-based gaze tracking system that
supports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical
Turk (AMTurk). By a combination of careful algorithm and gaming protocol
design, our system obtains eye tracking data for saliency prediction comparable
to data gathered in a traditional lab setting, with relatively lower cost and
less effort on the part of the researchers. Using this tool, we build a
saliency dataset for a large number of natural images. We will open-source our
tool and provide a web server where researchers can upload their images to get
eye tracking results from AMTurk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06760</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06760</id><created>2015-04-25</created><updated>2015-08-17</updated><authors><author><keyname>Arbabjolfaei</keyname><forenames>Fatemeh</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>On Critical Index Coding Problems</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted to 2015 IEEE Information Theory Workshop (ITW),
  Jeju Island, Korea</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of under what condition some side information for index coding
can be removed without affecting the capacity region is studied, which was
originally posed by Tahmasbi, Shahrasbi, and Gohari. To answer this question,
the notion of unicycle for the side information graph is introduced and it is
shown that any edge that belongs to a unicycle is critical, namely, it cannot
be removed without reducing the capacity region. Although this sufficient
condition for criticality is not necessary in general, a partial converse is
established, which elucidates the connection between the notion of unicycle and
the maximal acylic induced subgraph outer bound on the capacity region by
Bar-Yossef, Birk, Jayram, and Kol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06761</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06761</id><created>2015-04-25</created><authors><author><keyname>Arbabjolfaei</keyname><forenames>Fatemeh</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Structural Properties of Index Coding Capacity Using Fractional Graph
  Theory</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in the 2015 IEEE International Symposium on
  Information Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity region of the index coding problem is characterized through the
notion of confusion graph and its fractional chromatic number. Based on this
multiletter characterization, several structural properties of the capacity
region are established, some of which are already noted by Tahmasbi, Shahrasbi,
and Gohari, but proved here with simple and more direct graph-theoretic
arguments. In particular, the capacity region of a given index coding problem
is shown to be simple functionals of the capacity regions of smaller
subproblems when the interaction between the subproblems is none, one-way, or
complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06766</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06766</id><created>2015-04-25</created><authors><author><keyname>Alechina</keyname><forenames>Natasha</forenames></author><author><keyname>Logan</keyname><forenames>Brian</forenames></author><author><keyname>Nguyen</keyname><forenames>Hoang Nga</forenames></author><author><keyname>Raimondi</keyname><forenames>Franco</forenames></author></authors><title>Technical Report: Model-Checking for Resource-Bounded ATL with
  Production and Consumption of Resources</title><categories>cs.MA cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several logics for expressing coalitional ability under resource bounds have
been proposed and studied in the literature. Previous work has shown that if
only consumption of resources is considered or the total amount of resources
produced or consumed on any path in the system is bounded, then the
model-checking problem for several standard logics, such as Resource-Bounded
Coalition Logic (RB-CL) and Resource-Bounded Alternating-Time Temporal Logic
(RB-ATL) is decidable. However, for coalition logics with unbounded resource
production and consumption, only some undecidability results are known. In this
paper, we show that the model-checking problem for RB-ATL with unbounded
production and con- sumption of resources is decidable but EXPSPACE-hard. We
also investigate some tractable cases and provide a detailed comparison to a
variant of the resource logic RAL, together with new complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06773</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06773</id><created>2015-04-25</created><authors><author><keyname>Kandiah</keyname><forenames>V.</forenames></author><author><keyname>Escaith</keyname><forenames>H.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>D. L.</forenames></author></authors><title>Google matrix of the world network of economic activities</title><categories>q-fin.ST cs.SI physics.soc-ph</categories><comments>more data at http://www.quantware.ups-tlse.fr/QWLIB/wneamatrix/</comments><journal-ref>Eur. Phys. J. B (2015) 88: 186</journal-ref><doi>10.1140/epjb/e2015-60324-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the new data from the OECD-WTO world network of economic activities we
construct the Google matrix $G$ of this directed network and perform its
detailed analysis. The network contains 58 countries and 37 activity sectors
for years 1995 and 2008. The construction of $G$, based on Markov chain
transitions, treats all countries on equal democratic grounds while the
contribution of activity sectors is proportional to their exchange monetary
volume. The Google matrix analysis allows to obtain reliable ranking of
countries and activity sectors and to determine the sensitivity of
CheiRank-PageRank commercial balance of countries in respect to price
variations and labor cost in various countries. We demonstrate that the
developed approach takes into account multiplicity of network links with
economy interactions between countries and activity sectors thus being more
efficient compared to the usual export-import analysis. The spectrum and
eigenstates of $G$ are also analyzed being related to specific activity
communities of countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06778</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06778</id><created>2015-04-25</created><authors><author><keyname>Marin</keyname><forenames>Mike A.</forenames></author><author><keyname>Brown</keyname><forenames>Jay A.</forenames></author></authors><title>Implementing a Case Management Modeling and Notation (CMMN) System using
  a Content Management Interoperability Services (CMIS) compliant repository</title><categories>cs.SE</categories><comments>35 pages, 12 figures, 5 tables, and complete Java pseudo-code</comments><acm-class>D.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes how a Case Management Modeling and Notation (CMMN)
implementation can use Content Management Interoperability Services (CMIS) to
implement the CMMN information model. The interaction between CMMN and CMIS is
described in detail, and two implementation alternatives are presented. An
integration alternative where any external CMIS repository is used. This
alternative is useful to process technology vendors looking to integrate with
CMIS compliant repositories. An embedded alternative where a CMIS repository is
embedded within the CMMN engine. This alternative is useful to content
management vendors implementing CMMN. In both alternatives a CMIS folder is
used as the case file containing the case instance data. The CMIS repository
can also be used to store the CMMN models to take advantage of CMIS versioning
and meta-data. Extensive Java pseudocode is provided as an example of how a
CMMN implementation can use a CMIS repository to implement the CMMN information
model. No extensions to CMIS are needed, and only minor extensions to CMMN are
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06779</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06779</id><created>2015-04-25</created><authors><author><keyname>Machado</keyname><forenames>Emerson Lopes</forenames></author><author><keyname>Miosso</keyname><forenames>Cristiano Jacques</forenames></author><author><keyname>von Borries</keyname><forenames>Ricardo</forenames></author><author><keyname>Coutinho</keyname><forenames>Murilo</forenames></author><author><keyname>Berger</keyname><forenames>Pedro de Azevedo</forenames></author><author><keyname>Marques</keyname><forenames>Thiago</forenames></author><author><keyname>Jacobi</keyname><forenames>Ricardo Pezzuol</forenames></author></authors><title>Computational Cost Reduction in Learned Transform Classifications</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theoretical analysis and empirical evaluations of a novel set of
techniques for computational cost reduction of classifiers that are based on
learned transform and soft-threshold. By modifying optimization procedures for
dictionary and classifier training, as well as the resulting dictionary
elements, our techniques allow to reduce the bit precision and to replace each
floating-point multiplication by a single integer bit shift. We also show how
the optimization algorithms in some dictionary training methods can be modified
to penalize higher-energy dictionaries. We applied our techniques with the
classifier Learning Algorithm for Soft-Thresholding, testing on the datasets
used in its original paper. Our results indicate it is feasible to use solely
sums and bit shifts of integers to classify at test time with a limited
reduction of the classification accuracy. These low power operations are a
valuable trade off in FPGA implementations as they increase the classification
throughput while decrease both energy consumption and manufacturing cost.
Moreover, our techniques reduced 50% of the bit precision in almost all
datasets we tested, enabling to use 32-bit operations instead of 64-bit in
GPUs, which can almost double the classification throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06782</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06782</id><created>2015-04-26</created><updated>2015-10-31</updated><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Khargonekar</keyname><forenames>P. P.</forenames></author></authors><title>An Approximately Optimal Algorithm for Scheduling Phasor Data
  Transmissions in Smart Grid Networks</title><categories>cs.IT math.IT</categories><comments>8 pages, published in IEEE Transactions on Smart Grid, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we devise a scheduling algorithm for ordering transmission of
synchrophasor data from the substation to the control center in as short a time
frame as possible, within the realtime hierarchical communications
infrastructure in the electric grid. The problem is cast in the framework of
the classic job scheduling with precedence constraints. The optimization setup
comprises the number of phasor measurement units (PMUs) to be installed on the
grid, a weight associated with each PMU, processing time at the control center
for the PMUs, and precedence constraints between the PMUs. The solution to the
PMU placement problem yields the optimum number of PMUs to be installed on the
grid, while the processing times are picked uniformly at random from a
predefined set. The weight associated with each PMU and the precedence
constraints are both assumed known. The scheduling problem is provably NP-hard,
so we resort to approximation algorithms which provide solutions that are
suboptimal yet possessing polynomial time complexity. A lower bound on the
optimal schedule is derived using branch and bound techniques, and its
performance evaluated using standard IEEE test bus systems. The scheduling
policy is power grid-centric, since it takes into account the electrical
properties of the network under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06785</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06785</id><created>2015-04-26</created><updated>2015-11-17</updated><authors><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Complete Dictionary Recovery over the Sphere</title><categories>cs.IT cs.CV cs.LG math.IT math.OC stat.ML</categories><comments>104 pages, 5 figures. Due to length constraint of publication, this
  long paper are subsequently divided into two papers (arXiv:1511.03607 and
  arXiv:1511.04777). Further updates will be made only to the two papers</comments><msc-class>68P30, 58C05, 94A12, 94A08, 68T05, 90C26, 90C48, 90C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a complete (i.e., square and
invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb R^{n \times p}$
with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is
sufficiently sparse. This recovery problem is central to the theoretical
understanding of dictionary learning, which seeks a sparse representation for a
collection of input signals, and finds numerous applications in modern signal
processing and machine learning. We give the first efficient algorithm that
provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per
column, under suitable probability model for $\mathbf X_0$. In contrast, prior
results based on efficient algorithms provide recovery guarantees when $\mathbf
X_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta
\in (0, 1)$.
  Our algorithmic pipeline centers around solving a certain nonconvex
optimization problem with a spherical constraint, and hence is naturally
phrased in the language of manifold optimization. To show this apparently hard
problem is tractable, we first provide a geometric characterization of the
high-dimensional objective landscape, which shows that with high probability
there are no &quot;spurious&quot; local minima. This particular geometric structure
allows us to design a Riemannian trust region algorithm over the sphere that
provably converges to one local minimizer with an arbitrary initialization,
despite the presence of saddle points. The geometric approach we develop here
may also shed light on other problems arising from nonconvex recovery of
structured signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06786</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06786</id><created>2015-04-26</created><updated>2015-05-06</updated><authors><author><keyname>Nafchi</keyname><forenames>Hossein Ziaei</forenames></author><author><keyname>Hedjam</keyname><forenames>Rachid</forenames></author><author><keyname>Shahkolaei</keyname><forenames>Atena</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Deviation Based Pooling Strategies For Full Reference Image Quality
  Assessment</title><categories>cs.MM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-the-art pooling strategies for perceptual image quality
assessment (IQA) are based on the mean and the weighted mean. They are robust
pooling strategies which usually provide a moderate to high performance for
different IQAs. Recently, standard deviation (SD) pooling was also proposed.
Although, this deviation pooling provides a very high performance for a few
IQAs, its performance is lower than mean poolings for many other IQAs. In this
paper, we propose to use the mean absolute deviation (MAD) and show that it is
a more robust and accurate pooling strategy for a wider range of IQAs. In fact,
MAD pooling has the advantages of both mean pooling and SD pooling. The joint
computation and use of the MAD and SD pooling strategies is also considered in
this paper. Experimental results provide useful information on the choice of
the proper deviation pooling strategy for different IQA models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06787</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06787</id><created>2015-04-26</created><updated>2015-12-14</updated><authors><author><keyname>Li</keyname><forenames>Chongxuan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Shi</keyname><forenames>Tianlin</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Max-margin Deep Generative Models</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep generative models (DGMs) are effective on learning multilayered
representations of complex data and performing inference of input data by
exploring the generative ability. However, little work has been done on
examining or empowering the discriminative ability of DGMs on making accurate
predictions. This paper presents max-margin deep generative models (mmDGMs),
which explore the strongly discriminative principle of max-margin learning to
improve the discriminative power of DGMs, while retaining the generative
capability. We develop an efficient doubly stochastic subgradient algorithm for
the piecewise linear objective. Empirical results on MNIST and SVHN datasets
demonstrate that (1) max-margin learning can significantly improve the
prediction performance of DGMs and meanwhile retain the generative ability; and
(2) mmDGMs are competitive to the state-of-the-art fully discriminative
networks by employing deep convolutional neural networks (CNNs) as both
recognition and generative models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06793</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06793</id><created>2015-04-26</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author><author><keyname>Marik</keyname><forenames>Radek</forenames></author><author><keyname>Khlevnoy</keyname><forenames>Vladimir A.</forenames></author></authors><title>A Formal Approach to Network/Distributed Systems Complex Testing</title><categories>cs.DC cs.SE</categories><comments>5 pages, 1 figure</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V22(2):76-80, April 2015. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V22P115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of network/distributed systems sets high requirements for
procedures, tools and approaches for the complex testing of these systems. This
work provides a survey of testing activities with regard to these systems based
on standards and actual practices for both software-based and distribution
(network) aspects. On the basis of this survey, we determine formal testing
procedures/processes which cover these aspects, but which are not contrary to
both aspects. The next step, based on the analysis of the implementation phase
of System Development Life Cycle, determines a formal model for these processes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06794</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06794</id><created>2015-04-26</created><updated>2015-07-06</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Brandt</keyname><forenames>Rasmus</forenames></author><author><keyname>Ghauch</keyname><forenames>Hadi</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Overhead-Aware Distributed CSI Selection in the MIMO Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. to appear at EUSIPCO 2015, Special Session on
  Algorithms for Distributed Coordination and Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a MIMO interference channel in which the transmitters and
receivers operate in frequency-division duplex mode. In this setting,
interference management through coordinated transceiver design necessitates
channel state information at the transmitters (CSI-T). The acquisition of CSI-T
is done through feedback from the receivers, which entitles a loss in degrees
of freedom, due to training and feedback. This loss increases with the amount
of CSI-T. In this work, after formulating an overhead model for CSI acquisition
at the transmitters, we propose a distributed mechanism to find for each
transmitter a subset of the complete CSI, which is used to perform interference
management. The mechanism is based on many-to-many stable matching. We prove
the existence of a stable matching and exploit an algorithm to reach it.
Simulation results show performance improvement compared to full and minimal
CSI-T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06796</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06796</id><created>2015-04-26</created><updated>2015-06-28</updated><authors><author><keyname>Kozdoba</keyname><forenames>Mark</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Overlapping Communities Detection via Measure Space Embedding</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for community detection. The algorithm uses random
walks to embed the graph in a space of measures, after which a modification of
$k$-means in that space is applied. The algorithm is therefore fast and easily
parallelizable. We evaluate the algorithm on standard random graph benchmarks,
including some overlapping community benchmarks, and find its performance to be
better or at least as good as previously known algorithms. We also prove a
linear time (in number of edges) guarantee for the algorithm on a
$p,q$-stochastic block model with $p \geq c\cdot N^{-\frac{1}{2} + \epsilon}$
and $p-q \geq c' \sqrt{p N^{-\frac{1}{2} + \epsilon} \log N}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06797</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06797</id><created>2015-04-26</created><authors><author><keyname>Rashid</keyname><forenames>Haroon</forenames></author><author><keyname>Turuk</keyname><forenames>Ashok Kumar</forenames></author></authors><title>Dead Reckoning Localization Technique for Mobile Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>Journal Paper, IET Wireless Sensor Systems, 2015</comments><doi>10.1049/iet-wss.2014.0043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization in wireless sensor networks not only provides a node with its
geographical location but also a basic requirement for other applications such
as geographical routing. Although a rich literature is available for
localization in static WSN, not enough work is done for mobile WSNs, owing to
the complexity due to node mobility. Most of the existing techniques for
localization in mobile WSNs uses Monte-Carlo localization, which is not only
time-consuming but also memory intensive. They, consider either the unknown
nodes or anchor nodes to be static. In this paper, we propose a technique
called Dead Reckoning Localization for mobile WSNs. In the proposed technique
all nodes (unknown nodes as well as anchor nodes) are mobile. Localization in
DRLMSN is done at discrete time intervals called checkpoints. Unknown nodes are
localized for the first time using three anchor nodes. For their subsequent
localizations, only two anchor nodes are used. The proposed technique estimates
two possible locations of a node Using Bezouts theorem. A dead reckoning
approach is used to select one of the two estimated locations. We have
evaluated DRLMSN through simulation using Castalia simulator, and is compared
with a similar technique called RSS-MCL proposed by Wang and Zhu .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06798</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06798</id><created>2015-04-26</created><authors><author><keyname>Kozdoba</keyname><forenames>Mark</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Overlapping Community Detection by Online Cluster Aggregation</title><categories>cs.LG cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new online algorithm for detecting overlapping communities. The
main ingredients are a modification of an online k-means algorithm and a new
approach to modelling overlap in communities. An evaluation on large benchmark
graphs shows that the quality of discovered communities compares favorably to
several methods in the recent literature, while the running time is
significantly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06804</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06804</id><created>2015-04-26</created><updated>2015-09-15</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>High Speed Hashing for Integers and Strings</title><categories>cs.DS</categories><comments>Fixed a few typos from first version. Please send comments/typos to
  me at mikkel2thorup@gmail.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes describe the most efficient hash functions currently known for
hashing integers and strings. These modern hash functions are often an order of
magnitude faster than those presented in standard text books. They are also
simpler to implement, and hence a clear win in practice, but their analysis is
harder. Some of the most practical hash functions have only appeared in theory
papers, and some of them requires combining results from different theory
papers. The goal here is to combine the information in lecture-style notes that
can be used by theoreticians and practitioners alike, thus making these
practical fruits of theory more widely accessible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06810</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06810</id><created>2015-04-26</created><authors><author><keyname>Senejohnny</keyname><forenames>Danial</forenames></author><author><keyname>Tesi</keyname><forenames>Pietro</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Self-triggered Coordination over a Shared Network under
  Denial-of-Service</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of security has become ever more prevalent in the analysis and
design of cyber-physical systems. In this paper, we analyze a consensus network
in the presence of Denial-of-Service (DoS) attacks, namely attacks that prevent
communication among the network agents. By introducing a notion of
Persistency-of-Communication (PoC), we provide a characterization of DoS
frequency and duration such that consensus is not destroyed. An example is
given to substantiate the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06812</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06812</id><created>2015-04-26</created><updated>2015-10-06</updated><authors><author><keyname>Araniti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Condoluci</keyname><forenames>Massimo</forenames></author><author><keyname>Orsino</keyname><forenames>Antonino</forenames></author><author><keyname>Iera</keyname><forenames>Antonio</forenames></author><author><keyname>Molinaro</keyname><forenames>Antonella</forenames></author><author><keyname>Cosmas</keyname><forenames>John</forenames></author></authors><title>Evaluating the Performance of Multicast Resource Allocation Policies
  over LTE Systems</title><categories>cs.NI cs.MM</categories><comments>We just realized that the submitted version is not compliant with the
  final version of the manuscript. In addition, there are also crucial error in
  the formulation of the analytical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a multi-criteria decision method properly designed to
effectively evaluate the most performing strategy for multicast content
delivery in Long Term Evolution (LTE) and beyond systems. We compared the
legacy conservative-based approach with other promising strategies in
literature, i.e., opportunistic multicasting and subgroup-based policies
tailored to exploit different cost functions, such as maximum throughput,
proportional fairness and the multicast dissatisfaction index (MDI). We provide
a comparison among above schemes in terms of aggregate data rate (ADR),
fairness and spectral efficiency. We further design a multi-criteria decision
making method, namely TOPSIS, to evaluate through a single mark the overall
performance of considered strategies. The obtained results show that the MDI
subgrouping strategy represents the most suitable approach for multicast
content delivery as it provides the most promising trade-off between the
fairness and the throughput achieved by the multicast members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06813</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06813</id><created>2015-04-26</created><updated>2015-10-06</updated><authors><author><keyname>Militano</keyname><forenames>Leonardo</forenames></author><author><keyname>Orsino</keyname><forenames>Antonino</forenames></author><author><keyname>Araniti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molinaro</keyname><forenames>Antonella</forenames></author><author><keyname>Iera</keyname><forenames>Antonio</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author></authors><title>Efficient Spectrum Management Exploiting D2D Communication in 5G Systems</title><categories>cs.NI cs.MM</categories><comments>We just realized that the submitted version is not compliant with the
  final version of the manuscript. In addition, there are also crucial error in
  the formulation of the analytical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the future standardization of the 5G networks, in Long Term Evolution
(LTE) Release 13 and beyond, Device-to-Device communications (D2D) is
recognized as one of the key technologies that will support the 5G
architecture. In fact, D2D can be exploited for different proximity-based
services (ProSe) where the users discover their neighbors and benefit form
different services like social applications, advertisement, public safety, and
warning messages. In such a scenario, the aim is to manage in a proper way the
radio spectrum and the energy consumption to provide high Quality of Experience
(QoE) and better Quality of Services (QoS). To reach this goal, in this paper
we propose a novel D2D-based uploading scheme in order to decrease the amount
of radio resources needed to upload to the eNodeB a certain multimedia content.
As a further improvement, the proposed scheme enhances the energy consumption
of the users in the network, without affects the content uploading time. The
obtained results show that our scheme achieves a gain of about 35\% in term of
mean radio resources used with respect to the standard LTE cellular approach.
In addition, it is also 40 times more efficient in terms of energy consumption
needed to upload the multimedia content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06817</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06817</id><created>2015-04-26</created><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a theoretical analysis of the nuclear-norm
regularized least squares for full-rank matrix completion. Although similar
formulations have been examined by previous studies, their results are
unsatisfactory because only additive upper bounds are provided. Under the
assumption that the top eigenspaces of the target matrix are incoherent, we
derive a relative upper bound for recovering the best low-rank approximation of
the unknown matrix. Our relative upper bound is tighter than previous additive
bounds of other methods if the mass of the target matrix is concentrated on its
top eigenspaces, and also implies perfect recovery if it is low-rank. The
analysis is built upon the optimality condition of the regularized formulation
and existing guarantees for low-rank matrix completion. To the best of our
knowledge, this is first time such a relative bound is proved for the
regularized formulation of matrix completion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06825</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06825</id><created>2015-04-26</created><authors><author><keyname>Glauner</keyname><forenames>Patrick O.</forenames></author></authors><title>Comparison of Training Methods for Deep Neural Networks</title><categories>cs.LG cs.AI</categories><comments>50 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes the difficulties of training neural networks and in
particular deep neural networks. It then provides a literature review of
training methods for deep neural networks, with a focus on pre-training. It
focuses on Deep Belief Networks composed of Restricted Boltzmann Machines and
Stacked Autoencoders and provides an outreach on further and alternative
approaches. It also includes related practical recommendations from the
literature on training them. In the second part, initial experiments using some
of the covered methods are performed on two databases. In particular,
experiments are performed on the MNIST hand-written digit dataset and on facial
emotion data from a Kaggle competition. The results are discussed in the
context of results reported in other research papers. An error rate lower than
the best contribution to the Kaggle competition is achieved using an optimized
Stacked Autoencoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06827</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06827</id><created>2015-04-26</created><authors><author><keyname>Kryvasheyeu</keyname><forenames>Yury</forenames></author><author><keyname>Chen</keyname><forenames>Haohui</forenames></author><author><keyname>Obradovich</keyname><forenames>Nick</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Fowler</keyname><forenames>James</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Nowcasting Disaster Damage</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Could social media data aid in disaster response and damage assessment?
Countries face both an increasing frequency and intensity of natural disasters
due to climate change. And during such events, citizens are turning to social
media platforms for disaster-related communication and information. Social
media improves situational awareness, facilitates dissemination of emergency
information, enables early warning systems, and helps coordinate relief
efforts. Additionally, spatiotemporal distribution of disaster-related messages
helps with real-time monitoring and assessment of the disaster itself. Here we
present a multiscale analysis of Twitter activity before, during, and after
Hurricane Sandy. We examine the online response of 50 metropolitan areas of the
United States and find a strong relationship between proximity to Sandy's path
and hurricane-related social media activity. We show that real and perceived
threats -- together with the physical disaster effects -- are directly
observable through the intensity and composition of Twitter's message stream.
We demonstrate that per-capita Twitter activity strongly correlates with the
per-capita economic damage inflicted by the hurricane. Our findings suggest
that massive online social networks can be used for rapid assessment
(&quot;nowcasting&quot;) of damage caused by a large-scale disaster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06828</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06828</id><created>2015-04-26</created><authors><author><keyname>Yaji</keyname><forenames>Vinayaka</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>A bi-convex optimization problem to compute Nash equilibrium in n-player
  games and an algorithm</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present optimization problems with biconvex objective
function and linear constraints such that the set of global minima of the
optimization problems is the same as the set of Nash equilibria of a n-player
general-sum normal form game. We further show that the objective function is an
invex function and consider a projected gradient descent algorithm. We prove
that the projected gradient descent scheme converges to a partial optimum of
the objective function. We also present simulation results on certain test
cases showing convergence to a Nash equilibrium strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06830</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06830</id><created>2015-04-26</created><authors><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>Information Complexity and the Quest for Interactive Compression (A
  Survey)</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information complexity is the interactive analogue of Shannon's classical
information theory. In recent years this field has emerged as a powerful tool
for proving strong communication lower bounds, and for addressing some of the
major open problems in communication complexity and circuit complexity. A
notable achievement of information complexity is the breakthrough in
understanding of the fundamental direct sum and direct product conjectures,
which aim to quantify the power of parallel computation. This survey provides a
brief introduction to information complexity, and overviews some of the recent
progress on these conjectures and their tight relationship with the fascinating
problem of compressing interactive protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06833</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06833</id><created>2015-04-26</created><authors><author><keyname>Reed</keyname><forenames>Joel</forenames></author><author><keyname>Archuleta</keyname><forenames>Jeremy</forenames></author><author><keyname>Brim</keyname><forenames>Michael J.</forenames></author><author><keyname>Lothian</keyname><forenames>Joshua</forenames></author></authors><title>Evaluating Dynamic File Striping For Lustre</title><categories>cs.OS cs.DC</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define dynamic striping as the ability to assign different Lustre striping
characteristics to contiguous segments of a file as it grows. In this paper, we
evaluate the effects of dynamic striping using a watermark-based strategy where
the stripe count or width is increased once a file's size exceeds one of the
chosen watermarks. To measure the performance of this strategy we used a
modified version of the IOR benchmark, a netflow analysis workload, and the
blastn algorithm from NCBI BLAST. The results indicate that dynamic striping is
beneficial to tasks with unpredictable data file size and large sequential
reads, but are less conclusive for workloads with significant random read
phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06836</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06836</id><created>2015-04-26</created><authors><author><keyname>Brim</keyname><forenames>Michael J.</forenames></author><author><keyname>Lothian</keyname><forenames>Joshua K.</forenames></author></authors><title>Monitoring Extreme-scale Lustre Toolkit</title><categories>cs.DC cs.OS</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the design and ongoing development of the Monitoring Extreme-scale
Lustre Toolkit (MELT), a unified Lustre performance monitoring and analysis
infrastructure that provides continuous, low-overhead summary information on
the health and performance of Lustre, as well as on-demand, in- depth problem
diagnosis and root-cause analysis. The MELT infrastructure leverages a
distributed overlay network to enable monitoring of center-wide Lustre
filesystems where clients are located across many network domains. We preview
interactive command-line utilities that help administrators and users to
observe Lustre performance at various levels of resolution, from individual
servers or clients to whole filesystems, including job-level reporting.
Finally, we discuss our future plans for automating the root-cause analysis of
common Lustre performance problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06837</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06837</id><created>2015-04-26</created><updated>2015-12-30</updated><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>Davis</keyname><forenames>Jesse</forenames></author><author><keyname>De Smet</keyname><forenames>Frank</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Assessing binary classifiers using only positive and unlabeled data</title><categories>stat.ML cs.IR cs.LG</categories><comments>14 pages, 8 figures</comments><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing the performance of a learned model is a crucial part of machine
learning. However, in some domains only positive and unlabeled examples are
available, which prohibits the use of most standard evaluation metrics. We
propose an approach to estimate any metric based on contingency tables,
including ROC and PR curves, using only positive and unlabeled data. Estimating
these performance metrics is essentially reduced to estimating the fraction of
(latent) positives in the unlabeled set, assuming known positives are a random
sample of all positives. We provide theoretical bounds on the quality of our
estimates, illustrate the importance of estimating the fraction of positives in
the unlabeled set and demonstrate empirically that we are able to reliably
estimate ROC and PR curves on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06840</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06840</id><created>2015-04-26</created><authors><author><keyname>Addario-Berry</keyname><forenames>Louigi</forenames></author><author><keyname>Balle</keyname><forenames>Borja</forenames></author><author><keyname>Perarnau</keyname><forenames>Guillem</forenames></author></authors><title>Diameter and Stationary Distribution of Random $r$-out Digraphs</title><categories>math.PR cs.DM math.CO</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $D(n,r)$ be a random $r$-out regular directed multigraph on the set of
vertices $\{1,\ldots,n\}$. In this work, we establish that for every $r \ge 2$,
there exists $\eta_r&gt;0$ such that
$\text{diam}(D(n,r))=(1+\eta_r+o(1))\log_r{n}$. Our techniques also allow us to
bound some extremal quantities related to the stationary distribution of a
simple random walk on $D(n,r)$. In particular, we determine the asymptotic
behaviour of $\pi_{\max}$ and $\pi_{\min}$, the maximum and the minimum values
of the stationary distribution. We show that with high probability $\pi_{\max}
= n^{-1+o(1)}$ and $\pi_{\min}=n^{-(1+\eta_r)+o(1)}$. Our proof shows that the
vertices with $\pi(v)$ near to $\pi_{\min}$ lie at the top of &quot;narrow, slippery
towers&quot;, such vertices are also responsible for increasing the diameter from
$(1+o(1))\log_r n$ to $(1+\eta_r+o(1))\log_r{n}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06842</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06842</id><created>2015-04-26</created><authors><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author><author><keyname>Sysikaski</keyname><forenames>Mikko</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>An Optimal Algorithm for Minimum-Link Rectilinear Paths in Triangulated
  Rectilinear Domains</title><categories>cs.CG cs.DS</categories><comments>23 pages, 10 figures; an extended abstract to appear in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding minimum-link rectilinear paths in
rectilinear polygonal domains in the plane. A path or a polygon is rectilinear
if all its edges are axis-parallel. Given a set $\mathcal{P}$ of $h$
pairwise-disjoint rectilinear polygonal obstacles with a total of $n$ vertices
in the plane, a minimum-link rectilinear path between two points is a
rectilinear path that avoids all obstacles with the minimum number of edges. In
this paper, we present a new algorithm for finding minimum-link rectilinear
paths among $\mathcal{P}$. After the plane is triangulated, with respect to any
source point $s$, our algorithm builds an $O(n)$-size data structure in
$O(n+h\log h)$ time, such that given any query point $t$, the number of edges
of a minimum-link rectilinear path from $s$ to $t$ can be computed in $O(\log
n)$ time and the actual path can be output in additional time linear in the
number of the edges of the path. The previously best algorithm computes such a
data structure in $O(n\log n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06844</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06844</id><created>2015-04-26</created><authors><author><keyname>Huang</keyname><forenames>Xiao</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author></authors><title>Index Coding and Network Coding via Rank Minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Index codes reduce the number of bits broadcast by a wireless transmitter to
a number of receivers with different demands and with side information. It is
known that the problem of finding optimal linear index codes is NP-hard. We
investigate the performance of different heuristics based on rank minimization
and matrix completion methods, such as alternating projections and alternating
minimization, for constructing linear index codes over the reals. As a summary
of our results, the alternating projections method gives the best results in
terms of minimizing the number of broadcast bits and convergence rate and leads
to up to 13% savings in average communication cost compared to graph coloring
algorithms studied in the literature. Moreover, we describe how the proposed
methods can be used to construct linear network codes for non-multicast
networks. Our computer code is available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06846</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06846</id><created>2015-04-26</created><updated>2015-04-29</updated><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Memetic Elitist Pareto Evolutionary Algorithm for Virtual Network
  Embedding</title><categories>cs.DC</categories><comments>URL: http://dx.doi.org/10.5539/cis.v8n2p73. ISSN 1913-8989 E-ISSN
  1913-8997,Published by Canadian Center of Science and Education</comments><journal-ref>Computer and Information Science journal; Vol. 8, No. 2; 2015, pg.
  73-88</journal-ref><doi>10.5539/cis.v8n2p73</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assigning virtual network resources to physical network components, called
Virtual Network Embedding, is a major challenge in cloud computing platforms.
In this paper, we propose a memetic elitist pareto evolutionary algorithm for
virtual network embedding problem, which is called MEPE-VNE. MEPE-VNE applies a
non-dominated sorting-based multi-objective evolutionary algorithm, called
NSGA-II, to reduce computational complexity of constructing a hierarchy of
non-dominated Pareto fronts and assign a rank value to each virtual network
embedding solution based on its dominance level and crowding distance value.
Local search is applied to enhance virtual network embedding solutions and
speed up convergence of the proposed algorithm. To reduce loss of good
solutions, MEPE-VNE ensures elitism by passing virtual network embedding
solutions with best fitness values to next generation. Performance of the
proposed algorithm is evaluated and compared with existing algorithms using
extensive simulations, which show that the proposed algorithm improves virtual
network embedding by increasing acceptance ratio and revenue while decreasing
the cost incurred by substrate network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06848</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06848</id><created>2015-04-26</created><authors><author><keyname>Tolpin</keyname><forenames>David</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Maximum a Posteriori Estimation by Search in Probabilistic Programs</title><categories>cs.AI stat.ML</categories><comments>To appear in proceedings of SOCS15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approximate search algorithm for fast maximum a posteriori
probability estimation in probabilistic programs, which we call Bayesian ascent
Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with
varying number of mutually dependent finite, countable, and continuous random
variables. BaMC is an anytime MAP search algorithm applicable to any
combination of random variables and dependencies. We compare BaMC to other MAP
estimation algorithms and show that BaMC is faster and more robust on a range
of probabilistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06851</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06851</id><created>2015-04-26</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas J.</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Rubin</keyname><forenames>Natan</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Stable Delaunay Graphs</title><categories>cs.CG math.MG</categories><comments>This is a revision of the paper arXiv:1104.0622 presented in SoCG
  2010. The revised analysis relies on results reported in the companion paper
  arXiv:1404.4851</comments><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in $\mathrm{R}^2$, and let $\mathrm{DT}(P)$
denote its Euclidean Delaunay triangulation. We introduce the notion of an edge
of $\mathrm{DT}(P)$ being {\it stable}. Defined in terms of a parameter
$\alpha&gt;0$, a Delaunay edge $pq$ is called $\alpha$-stable, if the (equal)
angles at which $p$ and $q$ see the corresponding Voronoi edge $e_{pq}$ are at
least $\alpha$. A subgraph $G$ of $\mathrm{DT}(P)$ is called {\it $(c\alpha,
\alpha)$-stable Delaunay graph} ($\mathrm{SDG}$ in short), for some constant $c
\ge 1$, if every edge in $G$ is $\alpha$-stable and every $c\alpha$-stable of
$\mathrm{DT}(P)$ is in $G$.
  We show that if an edge is stable in the Euclidean Delaunay triangulation of
$P$, then it is also a stable edge, though for a different value of $\alpha$,
in the Delaunay triangulation of $P$ under any convex distance function that is
sufficiently close to the Euclidean norm, and vice-versa. In particular, a
$6\alpha$-stable edge in $\mathrm{DT}(P)$ is $\alpha$-stable in the Delaunay
triangulation under the distance function induced by a regular $k$-gon for $k
\ge 2\pi/\alpha$, and vice-versa. Exploiting this relationship and the analysis
in~\cite{polydel}, we present a linear-size kinetic data structure (KDS) for
maintaining an $(8\alpha,\alpha)$-$\mathrm{SDG}$ as the points of $P$ move. If
the points move along algebraic trajectories of bounded degree, the KDS
processes nearly quadratic events during the motion, each of which can
processed in $O(\log n)$ time. Finally, we show that a number of useful
properties of $\mathrm{DT}(P)$ are retained by $\mathrm{SDG}$ of $P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06852</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06852</id><created>2015-04-26</created><updated>2015-05-04</updated><authors><author><keyname>Fischer</keyname><forenames>Philipp</forenames></author><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Ilg</keyname><forenames>Eddy</forenames></author><author><keyname>H&#xe4;usser</keyname><forenames>Philip</forenames></author><author><keyname>Haz&#x131;rba&#x15f;</keyname><forenames>Caner</forenames></author><author><keyname>Golkov</keyname><forenames>Vladimir</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>FlowNet: Learning Optical Flow with Convolutional Networks</title><categories>cs.CV cs.LG</categories><comments>Added supplementary material</comments><acm-class>I.2.6; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) have recently been very successful in a
variety of computer vision tasks, especially on those linked to recognition.
Optical flow estimation has not been among the tasks where CNNs were
successful. In this paper we construct appropriate CNNs which are capable of
solving the optical flow estimation problem as a supervised learning task. We
propose and compare two architectures: a generic architecture and another one
including a layer that correlates feature vectors at different image locations.
  Since existing ground truth data sets are not sufficiently large to train a
CNN, we generate a synthetic Flying Chairs dataset. We show that networks
trained on this unrealistic data still generalize very well to existing
datasets such as Sintel and KITTI, achieving competitive accuracy at frame
rates of 5 to 10 fps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06855</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06855</id><created>2015-04-26</created><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Memetic Multi-Objective Particle Swarm Optimization-Based Energy-Aware
  Virtual Network Embedding</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1504.06846</comments><journal-ref>IJACSA Vol. 6, No. 4, 2015</journal-ref><doi>10.14569/IJACSA.2015.060405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud infrastructure, accommodating multiple virtual networks on a single
physical network reduces power consumed by physical resources and minimizes
cost of operating cloud data centers. However, mapping multiple virtual network
resources to physical network components, called virtual network embedding
(VNE), is known to be NP-hard. With considering energy efficiency, the problem
becomes more complicated. In this paper, we model energy-aware virtual network
embedding, devise metrics for evaluating performance of energy aware virtual
network-embedding algorithms, and propose an energy aware virtual
network-embedding algorithm based on multi-objective particle swarm
optimization augmented with local search to speed up convergence of the
proposed algorithm and improve solutions quality. Performance of the proposed
algorithm is evaluated and compared with existing algorithms using extensive
simulations, which show that the proposed algorithm improves virtual network
embedding by increasing revenue and decreasing energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06859</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06859</id><created>2015-04-26</created><authors><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author><author><keyname>Bazargani</keyname><forenames>Mosab</forenames></author></authors><title>When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown in the past that a multistart hillclimbing strategy
compares favourably to a standard genetic algorithm with respect to solving
instances of the multimodal problem generator. We extend that work and verify
if the utilization of diversity preservation techniques in the genetic
algorithm changes the outcome of the comparison. We do so under two scenarios:
(1) when the goal is to find the global optimum, (2) when the goal is to find
all optima.
  A mathematical analysis is performed for the multistart hillclimbing
algorithm and a through empirical study is conducted for solving instances of
the multimodal problem generator with increasing number of optima, both with
the hillclimbing strategy as well as with genetic algorithms with niching.
Although niching improves the performance of the genetic algorithm, it is still
inferior to the multistart hillclimbing strategy on this class of problems.
  An idealized niching strategy is also presented and it is argued that its
performance should be close to a lower bound of what any evolutionary algorithm
can do on this class of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06861</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06861</id><created>2015-04-26</created><updated>2015-07-14</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Puliga</keyname><forenames>Michelangelo</forenames></author><author><keyname>Smailovi&#x107;</keyname><forenames>Jasmina</forenames></author><author><keyname>Mozeti&#x10d;</keyname><forenames>Igor</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author></authors><title>Twitter-based analysis of the dynamics of collective attention to
  political parties</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>16 pages, 7 figures, 3 tables. Published in PLoS ONE</comments><journal-ref>PLoS ONE 10(7): e0131184 (2015)</journal-ref><doi>10.1371/journal.pone.0131184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale data from social media have a significant potential to describe
complex phenomena in real world and to anticipate collective behaviors such as
information spreading and social trends. One specific case of study is
represented by the collective attention to the action of political parties. Not
surprisingly, researchers and stakeholders tried to correlate parties' presence
on social media with their performances in elections. Despite the many efforts,
results are still inconclusive since this kind of data is often very noisy and
significant signals could be covered by (largely unknown) statistical
fluctuations. In this paper we consider the number of tweets (tweet volume) of
a party as a proxy of collective attention to the party, identify the dynamics
of the volume, and show that this quantity has some information on the
elections outcome. We find that the distribution of the tweet volume for each
party follows a log-normal distribution with a positive autocorrelation of the
volume over short terms, which indicates the volume has large fluctuations of
the log-normal distribution yet with a short-term tendency. Furthermore, by
measuring the ratio of two consecutive daily tweet volumes, we find that the
evolution of the daily volume of a party can be described by means of a
geometric Brownian motion (i.e., the logarithm of the volume moves randomly
with a trend). Finally, we determine the optimal period of averaging tweet
volume for reducing fluctuations and extracting short-term tendencies. We
conclude that the tweet volume is a good indicator of parties' success in the
elections when considered over an optimal time window. Our study identifies the
statistical nature of collective attention to political issues and sheds light
on how to model the dynamics of collective attention in social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06864</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06864</id><created>2015-04-26</created><authors><author><keyname>Najgebauer</keyname><forenames>Patryk</forenames></author><author><keyname>Rygal</keyname><forenames>Janusz</forenames></author><author><keyname>Nowak</keyname><forenames>Tomasz</forenames></author><author><keyname>Romanowski</keyname><forenames>Jakub</forenames></author><author><keyname>Rutkowski</keyname><forenames>Leszek</forenames></author><author><keyname>Voloshynovskiy</keyname><forenames>Sviatoslav</forenames></author><author><keyname>Scherer</keyname><forenames>Rafal</forenames></author></authors><title>Fast Dictionary Matching for Content-based Image Retrieval</title><categories>cs.CV</categories><comments>Accepted for the 14th International Conference on Artificial
  Intelligence and Soft Computing, ICAISC, June 14-18, 2015, Zakopane, Poland,
  http://www.icaisc.eu/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for searching for common sets of descriptors
between collections of images. The presented method operates on local interest
keypoints, which are generated using the SURF algorithm. The use of a
dictionary of descriptors allowed achieving good performance of the
content-based image retrieval. The method can be used to initially determine a
set of similar pairs of keypoints between images. For this purpose, we use a
certain level of tolerance between values of descriptors, as values of feature
descriptors are almost never equal but similar between different images. After
that, the method compares the structure of rotation and location of interest
points in one image with the point structure in other images. Thus, we were
able to find similar areas in images and determine the level of similarity
between them, even when images contain different scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06867</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06867</id><created>2015-04-26</created><authors><author><keyname>Grycuk</keyname><forenames>Rafal</forenames></author><author><keyname>Gabryel</keyname><forenames>Marcin</forenames></author><author><keyname>Scherer</keyname><forenames>Rafal</forenames></author><author><keyname>Voloshynovskiy</keyname><forenames>Sviatoslav</forenames></author></authors><title>Multi-layer Architecture For Storing Visual Data Based on WCF and
  Microsoft SQL Server Database</title><categories>cs.DB cs.IR</categories><comments>Accepted for the 14th International Conference on Artificial
  Intelligence and Soft Computing, ICAISC, June 14-18, 2015, Zakopane, Poland</comments><doi>10.1007/978-3-319-19324-3_64</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we present a novel architecture for storing visual data.
Effective storing, browsing and searching collections of images is one of the
most important challenges of computer science. The design of architecture for
storing such data requires a set of tools and frameworks such as SQL database
management systems and service-oriented frameworks. The proposed solution is
based on a multi-layer architecture, which allows to replace any component
without recompilation of other components. The approach contains five
components, i.e. Model, Base Engine, Concrete Engine, CBIR service and
Presentation. They were based on two well-known design patterns: Dependency
Injection and Inverse of Control. For experimental purposes we implemented the
SURF local interest point detector as a feature extractor and $K$-means
clustering as indexer. The presented architecture is intended for content-based
retrieval systems simulation purposes as well as for real-world CBIR tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06868</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06868</id><created>2015-04-26</created><authors><author><keyname>Cormack</keyname><forenames>Gordon V.</forenames></author><author><keyname>Grossman</keyname><forenames>Maura R.</forenames></author></authors><title>Autonomy and Reliability of Continuous Active Learning for
  Technology-Assisted Review</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We enhance the autonomy of the continuous active learning method shown by
Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted
review, in which documents from a collection are retrieved and reviewed, using
relevance feedback, until substantially all of the relevant documents have been
reviewed. Autonomy is enhanced through the elimination of topic-specific and
dataset-specific tuning parameters, so that the sole input required by the user
is, at the outset, a short query, topic description, or single relevant
document; and, throughout the review, ongoing relevance assessments of the
retrieved documents. We show that our enhancements consistently yield superior
results to Cormack and Grossman's version of continuous active learning, and
other methods, not only on average, but on the vast majority of topics from
four separate sets of tasks: the legal datasets examined by Cormack and
Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and
the construction of the TREC 2002 filtering test collection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06876</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06876</id><created>2015-04-26</created><updated>2016-02-14</updated><authors><author><keyname>Zakablukov</keyname><forenames>Dmitry V.</forenames></author></authors><title>On Asymptotic Gate Complexity and Depth of Reversible Circuits Without
  Additional Memory</title><categories>cs.ET</categories><comments>In English, 18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible computation is one of the most promising emerging technologies of
the future. The usage of reversible circuits in computing devices can lead to a
significantly lower power consumption. In this paper we study reversible logic
circuits consisting of NOT, CNOT and 2-CNOT gates. We introduce a set $F(n,q)$
of all transformations $\mathbb Z_2^n \to \mathbb Z_2^n$ that can be
implemented by reversible circuits with $(n+q)$ inputs. We define the Shannon
gate complexity function $L(n,q)$ and the depth function $D(n,q)$ as functions
of $n$ and the number of additional inputs $q$. First, we prove general lower
bounds for functions $L(n,q)$ and $D(n,q)$. Second, we introduce a new group
theory based synthesis algorithm, which can produce a circuit $\mathfrak S$
without additional inputs and with the gate complexity $L(\mathfrak S) \leq 3n
2^{n+4}(1+o(1)) \mathop / \log_2 n$. Using these bounds, we state that almost
every reversible circuit with no additional inputs, consisting of NOT, CNOT and
2-CNOT gates, implements a transformation from $F(n,0)$ with the gate
complexity $L(n,0) \asymp n 2^n \mathop / \log_2 n$ and with the depth $D(n,0)
\geq 2^n(1-o(1)) \mathop / (3\log_2 n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06877</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06877</id><created>2015-04-26</created><authors><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>Bayesian kernel-based system identification with quantized output data</title><categories>cs.SY stat.ML</categories><comments>Submitted to IFAC SysId 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel method for linear system identification
with quantized output data. We model the impulse response as a zero-mean
Gaussian process whose covariance (kernel) is given by the recently proposed
stable spline kernel, which encodes information on regularity and exponential
stability. This serves as a starting point to cast our system identification
problem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC)
methods to provide an estimate of the system. In particular, we show how to
design a Gibbs sampler which quickly converges to the target distribution.
Numerical simulations show a substantial improvement in the accuracy of the
estimates over state-of-the-art kernel-based methods when employed in
identification of systems with quantized data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06884</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06884</id><created>2015-04-26</created><authors><author><keyname>Ghanem</keyname><forenames>Samah A. M.</forenames></author></authors><title>Multiuser I-MMSE</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.0446</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize the fundamental relation between the derivative
of the mutual information and the minimum mean squared error (MMSE) to
multiuser setups. We prove that the derivative of the mutual information with
respect to the signal to noise ratio (SNR) is equal to the MMSE plus a
covariance induced due to the interference, quantified by a term with respect
to the cross correlation of the multiuser input estimates, the channels and the
precoding matrices. We also derive new relations for the gradient of the
conditional and non-conditional mutual information with respect to the MMSE.
Capitalizing on the new fundamental relations, we derive closed form
expressions of the mutual information for the multiuser channels, particularly
the two user multiple access Gaussian channel driven by binary phase shift
keying (BPSK) to illustrate and shed light on methods to derive similar
expressions for higher level constellations. We capitalize on the new unveiled
relation to derive the multiuser MMSE and mutual information in the low-SNR
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06890</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06890</id><created>2015-04-26</created><authors><author><keyname>Cardenas</keyname><forenames>Hector A.</forenames></author><author><keyname>Holtz</keyname><forenames>Chester</forenames></author><author><keyname>Janczak</keyname><forenames>Maria</forenames></author><author><keyname>Meyers</keyname><forenames>Philip</forenames></author><author><keyname>Potrepka</keyname><forenames>Nathaniel S.</forenames></author></authors><title>A Refutation of the Clique-Based P=NP Proofs of LaPlante and
  Tamta-Pande-Dhami</title><categories>cs.CC</categories><comments>14 pages, 11 figures arXiv:1403.1178v1 [cs.DS] arXiv:1503.04794v1
  [cs.DS]</comments><msc-class>68Q15</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we critique two papers, &quot;A Polynomial-Time Solution to the
Clique Problem&quot; by Tamta, Pande, and Dhami, and &quot;A Polynomial-Time Algorithm
For Solving Clique Problems&quot; by LaPlante. We summarize and analyze both papers,
noting that the algorithms presented in both papers are flawed. We conclude
that neither author has successfully established that P = NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06893</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06893</id><created>2015-04-26</created><authors><author><keyname>Erturk</keyname><forenames>Emre</forenames></author></authors><title>Two Trends in Mobile Security: Financial Motives and Transitioning from
  Static to Dynamic Analysis</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to analyze the behavior and intent of recent types
of privacy invasive Android adware. There are two recent trends in this area:
more financial motives instead of ego motives, and the development of more
dynamic analysis tools. This paper starts with a review of Android mobile
operating system security, and also addresses the pros and cons of open source
operating system security. Static analysis of malware provides high quality
results and leads to a good understanding as shown in this paper. However, as
malware grows in number and complexity, there have been recent efforts to
automate the detection mechanisms and many of the static tasks. As Android's
market share is rapidly growing around the world. Android security will be a
crucial area of research for IT security professionals and their academic
counterparts. The upside of the current situation is that malware is being
quickly exposed, thanks to open source software development tools. This
cooperation is important in curbing the widespread theft of personal
information with monetary value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06897</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06897</id><created>2015-04-26</created><authors><author><keyname>Bao</keyname><forenames>Chengqiang</forenames></author><author><keyname>He</keyname><forenames>Liangtian</forenames></author><author><keyname>Wang</keyname><forenames>Yilun</forenames></author></authors><title>Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse
  Coding for Image Classification</title><categories>cs.CV cs.LG</categories><msc-class>68T45</msc-class><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently sparse coding have been highly successful in image classification
mainly due to its capability of incorporating the sparsity of image
representation. In this paper, we propose an improved sparse coding model based
on linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform
(SIFT ) descriptors. The novelty is the simultaneous non-convex and
non-negative characters added to the sparse coding model. Our numerical
experiments show that the improved approach using non-convex and non-negative
sparse coding is superior than the original ScSPM[1] on several typical
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06917</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06917</id><created>2015-04-26</created><authors><author><keyname>Gill</keyname><forenames>Rajan</forenames></author><author><keyname>Kuli&#x107;</keyname><forenames>Dana</forenames></author><author><keyname>Nielsen</keyname><forenames>Christopher</forenames></author></authors><title>Spline Path Following for Redundant Mechanical Systems</title><categories>cs.RO cs.SY math.OC</categories><comments>Submitted to IEEE TRO (under review)</comments><msc-class>70Q05</msc-class><journal-ref>Robotics, IEEE Transactions on (Volume:31 , Issue: 6 ) 02 December
  2015</journal-ref><doi>10.1109/TRO.2015.2489502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path following controllers make the output of a control system approach and
traverse a pre-specified path with no apriori time parametrization. In this
paper we present a method for path following control design applicable to
framed curves generated by splines in the workspace of kinematically redundant
mechanical systems. The class of admissible paths includes self-intersecting
curves. Kinematic redundancies are resolved by designing controllers that solve
a suitably defined constrained quadratic optimization problem. By employing
partial feedback linearization, the proposed path following controllers have a
clear physical meaning. The approach is experimentally verified on a
4-degree-of-freedom (4-DOF) manipulator with a combination of revolute and
linear actuated links and significant model uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06919</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06919</id><created>2015-04-26</created><updated>2015-07-02</updated><authors><author><keyname>Eisenbrand</keyname><forenames>Friedrich</forenames></author><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Pinchasi</keyname><forenames>Rom</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>Node-balancing by edge-increments</title><categories>cs.DM</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose you are given a graph $G=(V,E)$ with a weight assignment
$w:V\rightarrow\mathbb{Z}$ and that your objective is to modify $w$ using legal
steps such that all vertices will have the same weight, where in each legal
step you are allowed to choose an edge and increment the weights of its end
points by $1$.
  In this paper we study several variants of this problem for graphs and
hypergraphs. On the combinatorial side we show connections with fundamental
results from matching theory such as Hall's Theorem and Tutte's Theorem. On the
algorithmic side we study the computational complexity of associated decision
problems.
  Our main results are a characterization of the graphs for which any initial
assignment can be balanced by edge-increments and a strongly polynomial-time
algorithm that computes a balancing sequence of increments if one exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06920</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06920</id><created>2015-04-26</created><authors><author><keyname>Kharche</keyname><forenames>Swapnil</forenames></author><author><keyname>patil</keyname><forenames>Jagdish</forenames></author><author><keyname>Gohad</keyname><forenames>Kanchan</forenames></author><author><keyname>Ambetkar</keyname><forenames>Bharti</forenames></author></authors><title>Preventing SQL Injection attack using pattern matching algorithm</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SQL injection attacks, a class of injection flaw in which specially crafted
input strings leads to illegal queries to databases, are one of the topmost
threats to web applications. A Number of research prototypes and commercial
products that maintain the queries structure in web applications have been
developed. But these techniques either fail to address the full scope of the
problem or have limitations. Based on our observation that the injected string
in a SQL injection attack is interpreted differently on different
databases.Injection attack is a method that can inject any kind of malicious
string or anomaly string on the original string. Pattern matching is a
technique that can be used to identify or detect any anomaly packet from a
sequential action. Most of the pattern based techniques are used static
analysis and patterns are generated from the attacked statements. In this
paper, we proposed a detection and prevention technique for preventing SQL
Injection Attack using AhoCorasick pattern matching algorithm. In this paper,
we proposed an overview of the architecture. In the initial stage evaluation,
we consider some sample of standard attack patterns and it shows that the
proposed algorithm is works well against the SQL Injection Attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06921</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06921</id><created>2015-04-26</created><authors><author><keyname>Ng</keyname><forenames>Hooi Sin</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author><author><keyname>Liang</keyname><forenames>Kim Meng</forenames></author><author><keyname>Mokayed</keyname><forenames>Hamam</forenames></author><author><keyname>Hon</keyname><forenames>Hock Woon</forenames></author></authors><title>Detection and Recognition of Malaysian Special License Plate Based On
  SIFT Features</title><categories>cs.CV</categories><comments>seven pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated car license plate recognition systems are developed and applied for
purpose of facilitating the surveillance, law enforcement, access control and
intelligent transportation monitoring with least human intervention. In this
paper, an algorithm based on SIFT feature points clustering and matching is
proposed to address the issue of recognizing Malaysian special plates. These
special plates do not follow the format of standard car plates as they may
contain italic, cursive, connected and small letters. The algorithm is tested
with 150 Malaysian special plate images under different environment and the
promising experimental results demonstrate that the proposed algorithm is
relatively robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06922</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06922</id><created>2015-04-26</created><authors><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author></authors><title>Simple Derivation of the Lifetime and the Distribution of Faces for a
  Binary Subdivision Model</title><categories>physics.soc-ph cs.GR math-ph math.MP</categories><comments>2 figures</comments><journal-ref>IEICE Trans. Fundamentals, Vol.E98-A, No.8, pp.1841-1844, (2015)</journal-ref><doi>10.1587/transfun.E98.A.1841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The iterative random subdivision of rectangles is used as a generation model
of networks in physics, computer science, and urban planning. However, these
researches were independent. We consider some relations in them, and derive
fundamental properties for the average lifetime depending on birth-time and the
balanced distribution of rectangle faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06924</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06924</id><created>2015-04-27</created><updated>2015-04-27</updated><authors><author><keyname>Agaskar</keyname><forenames>Ameya</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>Optimal Detection of Random Walks on Graphs: Performance Analysis via
  Statistical Physics</title><categories>cs.IT math.IT</categories><comments>38 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of detecting a random walk on a graph from a sequence of
noisy measurements at every node. There are two hypotheses: either every
observation is just meaningless zero-mean Gaussian noise, or at each time step
exactly one node has an elevated mean, with its location following a random
walk on the graph over time. We want to exploit knowledge of the graph
structure and random walk parameters (specified by a Markov chain transition
matrix) to detect a possibly very weak signal. The optimal detector is easily
derived, and we focus on the harder problem of characterizing its performance
through the (type-II) error exponent: the decay rate of the miss probability
under a false alarm constraint. The expression for the error exponent resembles
the free energy of a spin glass in statistical physics, and we borrow
techniques from that field to develop a lower bound. Our fully rigorous
analysis uses large deviations theory to show that the lower bound exhibits a
phase transition: strong performance is only guaranteed when the
signal-to-noise ratio exceeds twice the entropy rate of the random walk. Monte
Carlo simulations show that the lower bound fully captures the behavior of the
true exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06936</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06936</id><created>2015-04-27</created><authors><author><keyname>Metke-Jimenez</keyname><forenames>Alejandro</forenames></author><author><keyname>Karimi</keyname><forenames>Sarvnaz</forenames></author></authors><title>Concept Extraction to Identify Adverse Drug Reactions in Medical Forums:
  A Comparison of Algorithms</title><categories>cs.AI cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media is becoming an increasingly important source of information to
complement traditional pharmacovigilance methods. In order to identify signals
of potential adverse drug reactions, it is necessary to first identify medical
concepts in the social media text. Most of the existing studies use
dictionary-based methods which are not evaluated independently from the overall
signal detection task.
  We compare different approaches to automatically identify and normalise
medical concepts in consumer reviews in medical forums. Specifically, we
implement several dictionary-based methods popular in the relevant literature,
as well as a method we suggest based on a state-of-the-art machine learning
method for entity recognition. MetaMap, a popular biomedical concept extraction
tool, is used as a baseline. Our evaluations were performed in a controlled
setting on a common corpus which is a collection of medical forum posts
annotated with concepts and linked to controlled vocabularies such as MedDRA
and SNOMED CT.
  To our knowledge, our study is the first to systematically examine the effect
of popular concept extraction methods in the area of signal detection for
adverse reactions. We show that the choice of algorithm or controlled
vocabulary has a significant impact on concept extraction, which will impact
the overall signal detection process. We also show that our proposed machine
learning approach significantly outperforms all the other methods in
identification of both adverse reactions and drugs, even when trained with a
relatively small set of annotated text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06937</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06937</id><created>2015-04-27</created><updated>2015-10-19</updated><authors><author><keyname>Wu</keyname><forenames>Huasen</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Jiang</keyname><forenames>Chong</forenames></author></authors><title>Algorithms with Logarithmic or Sublinear Regret for Constrained
  Contextual Bandits</title><categories>cs.LG stat.ML</categories><comments>36 pages, 4 figures; accepted by the 29th Annual Conference on Neural
  Information Processing Systems (NIPS), Montr\'eal, Canada, Dec. 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We study contextual bandits with budget and time constraints, referred to as
constrained contextual bandits.The time and budget constraints significantly
complicate the exploration and exploitation tradeoff because they introduce
complex coupling among contexts over time.Such coupling effects make it
difficult to obtain oracle solutions that assume known statistics of bandits.
To gain insight, we first study unit-cost systems with known context
distribution. When the expected rewards are known, we develop an approximation
of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves
near-optimality and only requires the ordering of expected rewards. With these
highly desirable features, we then combine ALP with the upper-confidence-bound
(UCB) method in the general case where the expected rewards are unknown {\it a
priori}. We show that the proposed UCB-ALP algorithm achieves logarithmic
regret except for certain boundary cases. Further, we design algorithms and
obtain similar regret analysis results for more general systems with unknown
context distribution and heterogeneous costs. To the best of our knowledge,
this is the first work that shows how to achieve logarithmic regret in
constrained contextual bandits. Moreover, this work also sheds light on the
study of computationally efficient algorithms for general constrained
contextual bandits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06952</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06952</id><created>2015-04-27</created><updated>2016-01-31</updated><authors><author><keyname>F&#xe9;raud</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Allesiardo</keyname><forenames>Robin</forenames></author><author><keyname>Urvoy</keyname><forenames>Tanguy</forenames></author><author><keyname>Cl&#xe9;rot</keyname><forenames>Fabrice</forenames></author></authors><title>Random Forest for the Contextual Bandit Problem - extended version</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address the contextual bandit problem, we propose an online random forest
algorithm. The analysis of the proposed algorithm is based on the sample
complexity needed to find the optimal decision stump. Then, the decision stumps
are assembled in a random collection of decision trees, Bandit Forest. We show
that the proposed algorithm is optimal up to logarithmic factors. The
dependence of the sample complexity upon the number of contextual variables is
logarithmic. The computational cost of the proposed algorithm with respect to
the time horizon is linear. These analytical results allow the proposed
algorithm to be efficient in real applications, where the number of events to
process is huge, and where we expect that some contextual variables, chosen
from a large set, have potentially non- linear dependencies with the rewards.
In the experiments done to illustrate the theoretical analysis, Bandit Forest
obtain promising results in comparison with state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06954</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06954</id><created>2015-04-27</created><updated>2016-01-28</updated><authors><author><keyname>Nishimoto</keyname><forenames>Takaaki</forenames></author><author><keyname>Tomohiro</keyname><forenames>I</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Dynamic index, LZ factorization, and LCE queries in compressed space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the following results: (1) We propose a new
\emph{dynamic compressed index} of $O(w)$ space, that supports searching for a
pattern $P$ in the current text in $O(|P| f(M,w) + \log w \log |P| \log^* M
(\log N + \log |P| \log^* M) + \mathit{occ} \log N)$ time and
insertion/deletion of a substring of length $y$ in $O((y+ \log N\log^* M)\log w
\log N \log^* M)$ time, where $N$ is the length of the current text, $M$ is the
maximum length of the dynamic text, $z$ is the size of the Lempel-Ziv77 (LZ77)
factorization of the current text, $f(a,b) = O(\min \{ \frac{\log\log a
\log\log b}{\log\log\log a}, \sqrt{\frac{\log b}{\log\log b}} \})$ and $w = O(z
\log N \log^*M)$. (2) We propose a new space-efficient LZ77 factorization
algorithm for a given text of length $N$, which runs in $O(N f(N,w') + z \log
w' \log^3 N (\log^* N)^2)$ time with $O(w')$ working space, where $w' =O(z \log
N \log^* N)$. (3) We propose a data structure of $O(w)$ space which supports
longest common extension (LCE) queries on the text in $O(\log N + \log \ell
\log^* N)$ time, where $\ell$ is the output LCE length. On top of the above
contributions, we show several applications of our data structures which
improve previous best known results on grammar-compressed string processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06957</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06957</id><created>2015-04-27</created><authors><author><keyname>Liao</keyname><forenames>Yun</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Full-duplex MAC Protocol Design and Analysis</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of in-band full-duplex (FD) communications revives in recent years
owing to the significant progress in the self-interference cancellation and
hardware design techniques, offering the potential to double spectral
efficiency. The adaptations in upper layers are highly demanded in the design
of FD communication systems. In this letter, we propose a novel medium access
control (MAC) using FD techniques that allows transmitters to monitor the
channel usage while transmitting, and backoff as soon as collision happens.
Analytical saturation throughput of the FD-MAC protocol is derived with the
consideration of imperfect sensing brought by residual self- interference (RSI)
in the PHY layer. Both analytical and simulation results indicate that the
normalized saturation throughput of the proposed FD-MAC can significantly
outperforms conventional CSMA/CA under various network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06961</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06961</id><created>2015-04-27</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>van Hoek</keyname><forenames>Wilko</forenames></author><author><keyname>Weber</keyname><forenames>Alina</forenames></author><author><keyname>Kern</keyname><forenames>Dagmar</forenames></author></authors><title>WHOSE - A Tool for Whole-Session Analysis in IIR</title><categories>cs.IR</categories><comments>In Advances in Information Retrieval: 37th European Conference on IR
  Research, ECIR 2015, Vienna, Austria, March 29 - April 2, 2015. Proceedings,
  Lecture Notes in Computer Science 9022, 172-183</comments><doi>10.1007/978-3-319-16354-3_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges in Interactive Information Retrieval (IIR)
evaluation is the development and application of re-usable tools that allow
researchers to analyze search behavior of real users in different environments
and different domains, but with comparable results. Furthermore, IIR recently
focuses more on the analysis of whole sessions, which includes all user
interactions that are carried out within a session but also across several
sessions by the same user. Some frameworks have already been proposed for the
evaluation of controlled experiments in IIR, but yet no framework is available
for interactive evaluation of search behavior from real-world information
retrieval (IR) systems with real users. In this paper we present a framework
for whole-session evaluation that can also utilize these uncontrolled data
sets. The logging component can easily be integrated into real-world IR systems
for generating and analyzing new log data. Furthermore, due to a supplementary
mapping it is also possible to analyze existing log data. For every IR system
different actions and filters can be defined. This allows system operators and
researchers to use the framework for the analysis of user search behavior in
their IR systems and to compare it with others. Using a graphical user
interface they have the possibility to interactively explore the data set from
a broad overview down to individual sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06963</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06963</id><created>2015-04-27</created><updated>2015-06-27</updated><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author><author><keyname>M&#xe9;sz&#xe1;ros</keyname><forenames>Szabolcs</forenames></author></authors><title>Generalized solution for the Herman Protocol Conjecture</title><categories>cs.DS cs.DC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have a cycle of $N$ nodes and there is a token on an odd number of nodes.
At each step, each token independently moves to its clockwise neighbor or stays
at its position with probability $\frac{1}{2}$. If two tokens arrive to the
same node, then we remove both of them. The process ends when only one token
remains. The question is that for a fixed $N$, which is the initial
configuration that maximizes the expected number of steps $E(T)$. The Herman
Protocol Conjecture says that the $3$-token configuration with distances
$\lfloor\frac{N}{3}\rfloor$ and $\lceil\frac{N}{3}\rceil$ maximizes $E(T)$. We
present a proof of this conjecture not only for $E(T)$ but also for
$E\big(f(T)\big)$ for some function $f:\mathbb{N}\rightarrow\mathbb{R}^{+}$
which method applies for different generalizations of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06966</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06966</id><created>2015-04-27</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Wegener</keyname><forenames>Dennis</forenames></author><author><keyname>Schomisch</keyname><forenames>Siegfried</forenames></author></authors><title>Making sense of Open Data Statistics with Information from Wikipedia</title><categories>cs.HC cs.CY</categories><comments>In Availability, reliability, and security in information systems and
  HCI : IFIP 8.4, 8.9, TC 5 International Cross-Domain Conference, CD-ARES
  2013, Regensburg, Germany, September 2-6, 2013 ; proceedings, edited by
  Alfredo Cuzzocrea, Christian Kittl, Dimitris E. Simos, Edgar Weippl, and Lida
  Xu, Lecture Notes in Computer Science 8127, 329-344</comments><doi>10.1007/978-3-642-40511-2_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, more and more open data statistics are published by governments,
statistical offices and organizations like the United Nations, The World Bank
or Eurostat. This data is freely available and can be consumed by end users in
interactive visualizations. However, additional information is needed to enable
laymen to interpret these statistics in order to make sense of the raw data. In
this paper, we present an approach to combine open data statistics with
historical events. In a user interface we have integrated interactive
visualizations of open data statistics with a timeline of thematically
appropriate historical events from Wikipedia. This can help users to explore
statistical data in several views and to get related events for certain trends
in the timeline. Events include links to Wikipedia articles, where details can
be found and the search process can be continued. We have conducted a user
study to evaluate if users can use the interface intuitively, if relations
between trends in statistics and historical events can be found and if users
like this approach for their exploration process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06975</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06975</id><created>2015-04-27</created><updated>2016-02-19</updated><authors><author><keyname>Bashir</keyname><forenames>Noman</forenames></author><author><keyname>Sharani</keyname><forenames>Zohaib</forenames></author><author><keyname>Qayyum</keyname><forenames>Khushboo</forenames></author><author><keyname>Syed</keyname><forenames>Affan A.</forenames></author></authors><title>Aashiyana: Design and Evaluation of a Smart Demand-Response System for
  Highly-stressed Grids</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper targets the unexplored problem of demand response within the
context of power-grids that are allowed to regularly enforce blackouts as a
mean to balance supply with demand:highly-stressed grids. Currently these
utilities use as a cyclic and binary (power/no-power) schedule over consumer
groups leading to significant wastage of capacity and long hours of no-power.
We present here a novel building DLC system, Aashiyana, that can enforce
several user-defined low-power states. We evaluate distributed and centralized
load-shedding schemes using Aashiyana that can, compared to current
load-shedding strategy, reduce the number of homes with no power by 80% for
minor change in the fraction of homes with full-power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06979</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06979</id><created>2015-04-27</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author><author><keyname>Zhong</keyname><forenames>Mingxian</forenames></author></authors><title>Obstructions for three-coloring graphs without induced paths on six
  vertices</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there are 24 4-critical $P_6$-free graphs, and give the
complete list. We remark that, if $H$ is connected and not a subgraph of $P_6$,
there are infinitely many 4-critical $H$-free graphs. Our result answers
questions of Golovach et al. and Seymour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06982</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06982</id><created>2015-04-27</created><authors><author><keyname>Kokkala</keyname><forenames>Janne I.</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author></authors><title>Further Results on the Classification of MDS Codes</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $q$-ary maximum distance separable (MDS) code $C$ with length $n$,
dimension $k$ over an alphabet $\mathcal{A}$ of size $q$ is a set of $q^k$
codewords that are elements of $\mathcal{A}^n$, such that the Hamming distance
between two distinct codewords in $C$ is at least $n-k+1$. Sets of mutually
orthogonal Latin squares of orders $q\leq 9$, corresponding to two-dimensional
\mbox{$q$-}ary MDS codes, and $q$-ary one-error-correcting MDS codes for $q\leq
8$ have been classified in earlier studies. These results are used here to
complete the classification of all $7$-ary and $8$-ary MDS codes with $d\geq 3$
using a computer search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06983</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06983</id><created>2015-04-27</created><authors><author><keyname>Hadjam</keyname><forenames>Fatima</forenames></author><author><keyname>Moraga</keyname><forenames>Claudio</forenames></author></authors><title>A symbolic calculus for a class of quantum computing circuits</title><categories>cs.ET</categories><comments>2 pages, 6 figures</comments><msc-class>81-08</msc-class><acm-class>B.2.2; B.6.3; I.1.4</acm-class><journal-ref>Electronics Letters 51(9): 682-683, 2015</journal-ref><doi>10.1049/el.2014.3623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a symbolic calculus to evaluate the output signals at
the target line(s) of quantum computing subcircuits using controlled negations
and controlled-Q gates, where Q represents the k-th root of [0 1; 1 0], the
unitary matrix of NOT, and k is a power of two. The controlling signals are
GF(2) expressions possibly including Boolean expressions. The method does not
require operating with complex-valued matrices. The method may be used to
verify the functionality and to check for possible minimization of a given
quantum computing circuit using target lines. The method does not apply for a
whole circuit if there are interactions among target lines. In this case the
method applies for the independent subcircuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06986</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06986</id><created>2015-04-27</created><updated>2015-08-17</updated><authors><author><keyname>Linker</keyname><forenames>Sven</forenames><affiliation>Carl von Ossietzky Universit&#xe4;t Oldenburg</affiliation></author><author><keyname>Hilscher</keyname><forenames>Martin</forenames><affiliation>Carl von Ossietzky Universit&#xe4;t Oldenburg</affiliation></author></authors><title>Proof Theory of a Multi-Lane Spatial Logic</title><categories>cs.LO</categories><comments>This paper is the extended and slightly revised version of our
  publication in the 10th International Colloquium on Theoretical Aspects of
  Computing (ICTAC) in 2013</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:4) 2015</journal-ref><doi>10.2168/LMCS-11(3:4)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Multi-lane Spatial Logic MLSL, introduced in previous work for
proving the safety (collision freedom) of traffic maneuvers on a multi-lane
highway, by length measurement and dynamic modalities. We investigate the proof
theory of this extension, called EMLSL. To this end, we prove the
undecidability of EMLSL but nevertheless present a sound proof system which
allows for reasoning about the safety of traffic situations. We illustrate the
latter by giving a formal proof for the reservation lemma we could only prove
informally before. Furthermore we prove a basic theorem showing that the length
measurement is independent from the number of lanes on the highway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06993</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06993</id><created>2015-04-27</created><authors><author><keyname>Dong</keyname><forenames>Chao</forenames></author><author><keyname>Deng</keyname><forenames>Yubin</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Compression Artifacts Reduction by a Deep Convolutional Network</title><categories>cs.CV</categories><comments>9 pages, 12 figures, conference</comments><acm-class>I.4.5; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossy compression introduces complex compression artifacts, particularly the
blocking artifacts, ringing effects and blurring. Existing algorithms either
focus on removing blocking artifacts and produce blurred output, or restores
sharpened images that are accompanied with ringing effects. Inspired by the
deep convolutional networks (DCN) on super-resolution, we formulate a compact
and efficient network for seamless attenuation of different compression
artifacts. We also demonstrate that a deeper model can be effectively trained
with the features learned in a shallow network. Following a similar &quot;easy to
hard&quot; idea, we systematically investigate several practical transfer settings
and show the effectiveness of transfer learning in low-level vision problems.
Our method shows superior performance than the state-of-the-arts both on the
benchmark datasets and the real-world use case (i.e. Twitter). In addition, we
show that our method can be applied as pre-processing to facilitate other
low-level vision routines when they take compressed images as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.06998</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.06998</id><created>2015-04-27</created><authors><author><keyname>Alaggan</keyname><forenames>Mohammad</forenames></author><author><keyname>Gambs</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Kermarrec</keyname><forenames>Anne-Marie</forenames></author></authors><title>Heterogeneous Differential Privacy</title><categories>cs.CR</categories><comments>27 pages, 3 figures, presented at the first workshop on theory and
  practice of differential privacy (TPDP 2015) at London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive collection of personal data by personalization systems has
rendered the preservation of privacy of individuals more and more difficult.
Most of the proposed approaches to preserve privacy in personalization systems
usually address this issue uniformly across users, thus ignoring the fact that
users have different privacy attitudes and expectations (even among their own
personal data). In this paper, we propose to account for this non-uniformity of
privacy expectations by introducing the concept of heterogeneous differential
privacy. This notion captures both the variation of privacy expectations among
users as well as across different pieces of information related to the same
user. We also describe an explicit mechanism achieving heterogeneous
differential privacy, which is a modification of the Laplacian mechanism by
Dwork, McSherry, Nissim, and Smith. In a nutshell, this mechanism achieves
heterogeneous differential privacy by manipulating the sensitivity of the
function using a linear transformation on the input domain. Finally, we
evaluate on real datasets the impact of the proposed mechanism with respect to
a semantic clustering task. The results of our experiments demonstrate that
heterogeneous differential privacy can account for different privacy attitudes
while sustaining a good level of utility as measured by the recall for the
semantic clustering task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07000</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07000</id><created>2015-04-27</created><authors><author><keyname>Gkalelis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Mezaris</keyname><forenames>Vasileios</forenames></author></authors><title>Accelerated nonlinear discriminant analysis</title><categories>cs.LG</categories><comments>4 pages, report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel nonlinear discriminant analysis is proposed.
Experimental results show that the new method provides state of the art
performance when combined with LSVM in terms of training time and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07003</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07003</id><created>2015-04-27</created><updated>2015-11-04</updated><authors><author><keyname>Marsden</keyname><forenames>Daniel</forenames><affiliation>University of Oxford</affiliation></author></authors><title>A Graph Theoretic Perspective on CPM(Rel)</title><categories>cs.LO math.CT</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 273-284</journal-ref><doi>10.4204/EPTCS.195.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed states are of interest in quantum mechanics for modelling partial
information. More recently categorical approaches to linguistics have also
exploited the idea of mixed states to describe ambiguity and hyponym / hypernym
relationships. In both these application areas the category Rel of sets and
binary relations is often used as an alternative model. Selinger's CPM
construction provides the setting for mixed states in Hilbert space based
categorical quantum mechanics. By analogy, applying the CPM construction to Rel
is seen as introducing mixing into a relational setting. We investigate the
category CPM(Rel) of completely positive maps in Rel. We show that the states
of an object in CPM(Rel) are in bijective correspondence with certain families
of graphs. Via map-state duality this then allows us provide a graph theoretic
characterization of the morphisms in CPM(Rel). By identifying an appropriate
composition operation on graphs, we then show that CPM(Rel) is isomorphic to a
category of sets and graphs between them. This isomorphism then leads to a
graph based description of the complete join semilattice enriched dagger
compact structure of CPM(Rel). These results allow us to reason about CPM(Rel)
entirely in terms of graphs. We exploit these techniques in several examples.
We give a closed form expression for the number of states of a finite set in
CPM(Rel). The pure states are characterized in graph theoretic terms. We also
demonstrate the possibly surprising phenomenon of a pure state that can be
given as a mixture of two mixed states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07004</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07004</id><created>2015-04-27</created><authors><author><keyname>Chatterjee</keyname><forenames>Moitreya</forenames></author><author><keyname>Leuski</keyname><forenames>Anton</forenames></author></authors><title>An Active Learning Based Approach For Effective Video Annotation And
  Retrieval</title><categories>cs.MM cs.IR cs.LG</categories><comments>5 pages, 3 figures, Compressed version published at ACM ICMR 2015</comments><acm-class>H.3.3; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional multimedia annotation/retrieval systems such as Normalized
Continuous Relevance Model (NormCRM) [16] require a fully labeled training data
for a good performance. Active Learning, by determining an order for labeling
the training data, allows for a good performance even before the training data
is fully annotated. In this work we propose an active learning algorithm, which
combines a novel measure of sample uncertainty with a novel clustering-based
approach for determining sample density and diversity and integrate it with
NormCRM. The clusters are also iteratively refined to ensure both feature and
label-level agreement among samples. We show that our approach outperforms
multiple baselines both on a recent, open character animation dataset and on
the popular TRECVID corpus at both the tasks of annotation and text-based
retrieval of videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07009</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07009</id><created>2015-04-27</created><authors><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Efficient Interference Management Policies for Femtocell Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managing interference in a network of macrocells underlaid with femtocells
presents an important, yet challenging problem. A majority of spatial
(frequency/time) reuse based approaches partition the users based on coloring
the interference graph, which is shown to be suboptimal. Some spatial time
reuse based approaches schedule the maximal independent sets (MISs) in a
cyclic, (weighted) round-robin fashion, which is inefficient for
delay-sensitive applications. Our proposed policies schedule the MISs in a
non-cyclic fashion, which aim to optimize any given network performance
criterion for delay-sensitive applications while fulfilling minimum throughput
requirements of the users. Importantly, we do not take the interference graph
as given as in existing works; we propose an optimal construction of the
interference graph. We prove that under certain conditions, the proposed policy
achieves the optimal network performance. For large networks, we propose a
low-complexity algorithm for computing the proposed policy. We show that the
policy computed achieves a constant competitive ratio (with respect to the
optimal network performance), which is independent of the network size, under
wide range of deployment scenarios. The policy can be implemented in a
decentralized manner by the users. Compared to the existing policies, our
proposed policies can achieve improvement of up to 130 % in large-scale
deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07011</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07011</id><created>2015-04-27</created><updated>2015-12-12</updated><authors><author><keyname>Daminelli</keyname><forenames>Simone</forenames></author><author><keyname>Thomas</keyname><forenames>Josephine Maria</forenames></author><author><keyname>Dur&#xe1;n</keyname><forenames>Claudio</forenames></author><author><keyname>Cannistraci</keyname><forenames>Carlo Vittorio</forenames></author></authors><title>Common neighbours and the local-community-paradigm for link prediction
  in bipartite networks</title><categories>cs.SI nlin.AO physics.soc-ph</categories><journal-ref>2015, New Journal of Physics, 17, 113037</journal-ref><doi>10.1088/1367-2630/17/11/113037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bipartite networks are powerful descriptions of complex systems characterized
by two different classes of nodes and connections allowed only across but not
within the two classes. Surprisingly, current complex network theory presents a
theoretical bottle-neck: a general framework for local-based link prediction
directly in the bipartite domain is missing. Here, we overcome this theoretical
obstacle and present a formal definition of common neighbour index (CN) and
local-community-paradigm (LCP) for bipartite networks. As a consequence, we are
able to introduce the first node-neighbourhood-based and LCP-based models for
topological link prediction that utilize the bipartite domain. We performed
link prediction evaluations in several networks of different size and of
disparate origin, including technological, social and biological systems. Our
models significantly improve topological prediction in many bipartite networks
because they exploit local physical driving-forces that participate in the
formation and organization of many real-world bipartite networks. Furthermore,
we present a local-based formalism that allows to intuitively implement
neighbourhood-based link prediction entirely in the bipartite domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07012</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07012</id><created>2015-04-27</created><authors><author><keyname>Erturk</keyname><forenames>Emre</forenames></author><author><keyname>Fail</keyname><forenames>Derwyn</forenames></author></authors><title>Information Technology in New Zealand: Review of Emerging Social Trends,
  Current Issues, and Policies</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the general state of information technology in New
Zealand society, current issues, and policies. It is a qualitative study that
reviews recent scholarly articles, periodicals, and surveys in order to create
an understanding of some of the information technology issues and trends in New
Zealand. After reviewing previous research, it assesses the potential existence
and nature of a 'digital divide' in New Zealand society whilst also evaluating
possible strategic responses to the issue. New Zealand society has rapidly
accepted emerging online trends as well as achieving an overall high level of
Internet provision nationally. Through government policy and education, this
small island nation has remained at the forefront of information technology and
can be considered somewhat of an e-democracy. However, despite these positives,
there is a risk of low-income communities being left behind as New Zealand
society becomes increasingly dependent on IT in the workplace and in
governmental administration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07018</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07018</id><created>2015-04-27</created><authors><author><keyname>Agarwal</keyname><forenames>Vandit</forenames></author><author><keyname>Kushal</keyname><forenames>Mandhani</forenames></author><author><keyname>Kumar</keyname><forenames>Dr. Preetham</forenames></author></authors><title>An Improvised Frequent Pattern Tree Based Association Rule Mining
  Technique with Mining Frequent Item Sets Algorithm and a Modified Header
  Table</title><categories>cs.DB</categories><comments>15 pages, 8 tables, 7 figures, journal paper</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP), March 2015, Volume 5, Number 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays world there is a wide availability of huge amount of data and thus
there is a need for turning this data into useful information which is referred
to as knowledge. This demand for knowledge discovery process has led to the
development of many algorithms used to determine the association rules. One of
the major problems faced by these algorithms is generation of candidate sets.
The FP Tree algorithm is one of the most preferred algorithms for association
rule mining because it gives association rules without generating candidate
sets. But in the process of doing so, it generates many CP trees which
decreases its efficiency. In this research paper, an improvised FP tree
algorithm with a modified header table, along with a spare table and the MFI
algorithm for association rule mining is proposed. This algorithm generates
frequent item sets without using candidate sets and CP trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07019</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07019</id><created>2015-04-27</created><authors><author><keyname>Kamma</keyname><forenames>Lior</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Metric Decompositions of Path-Separable Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prominent tool in many problems involving metric spaces is a notion of
randomized low-diameter decomposition. Loosely speaking, $\beta$-decomposition
refers to a probability distribution over partitions of the metric into sets of
low diameter, such that nearby points (parameterized by $\beta&gt;0$) are likely
to be &quot;clustered&quot; together. Applying this notion to the shortest-path metric in
edge-weighted graphs, it is known that $n$-vertex graphs admit an $O(\ln
n)$-padded decomposition (Bartal, 1996), and that excluded-minor graphs admit
$O(1)$-padded decomposition (Klein, Plotkin and Rao 1993, Fakcharoenphol and
Talwar 2003, Abraham et al. 2014).
  We design decompositions to the family of $p$-path-separable graphs, which
was defined by Abraham and Gavoille (2006). and refers to graphs that admit
vertex-separators consisting of at most $p$ shortest paths in the graph.
  Our main result is that every $p$-path-separable $n$-vertex graph admits an
$O(\ln (p \ln n))$-decomposition, which refines the $O(\ln n)$ bound for
general graphs, and provides new bounds for families like bounded-treewidth
graphs. Technically, our clustering process differs from previous ones by
working in (the shortest-path metric of) carefully chosen subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07020</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07020</id><created>2015-04-27</created><authors><author><keyname>Gabbay</keyname><forenames>D. M.</forenames></author></authors><title>Theory of Semi-Instantiation in Abstract Argumentation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study instantiated abstract argumentation frames of the form $(S,R,I)$,
where $(S,R)$ is an abstract argumentation frame and where the arguments $x$ of
$S$ are instantiated by $I(x)$ as well formed formulas of a well known logic,
for example as Boolean formulas or as predicate logic formulas or as modal
logic formulas. We use the method of conceptual analysis to derive the
properties of our proposed system. We seek to define the notion of complete
extensions for such systems and provide algorithms for finding such extensions.
We further develop a theory of instantiation in the abstract, using the
framework of Boolean attack formations and of conjunctive and disjunctive
attacks. We discuss applications and compare critically with the existing
related literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07021</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07021</id><created>2015-04-27</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>On-Board Vision Processing For Small UAVs: Time to Rethink Strategy</title><categories>cs.CV cs.RO</categories><comments>2009 NASA/ESA Conference on Adaptive Hardware and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultimate research goal for unmanned aerial vehicles (UAVs) is to
facilitate autonomy of operation. Research in the last decade has highlighted
the potential of vision sensing in this regard. Although vital for
accomplishment of missions assigned to any type of unmanned aerial vehicles,
vision sensing is more critical for small aerial vehicles due to lack of high
precision inertial sensors. In addition, uncertainty of GPS signal in indoor
and urban environments calls for more reliance on vision sensing for such small
vehicles. With off-line processing does not offer an attractive option in terms
of autonomy, these vehicles have been challenging platforms to implement vision
processing onboard due to their strict payload capacity and power budget. The
strict constraints drive the need for new vision processing architectures for
small unmanned aerial vehicles. Recent research has shown encouraging results
with FPGA based hardware architectures. This paper reviews the bottle necks
involved in implementing vision processing on-board, advocates the potential of
hardware based solutions to tackle strict constraints of small unmanned aerial
vehicles and finally analyzes feasibility of ASICs, Structured ASICs and FPGAs
for use on future systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07028</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07028</id><created>2015-04-27</created><authors><author><keyname>Condessa</keyname><forenames>Filipe</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jose</forenames></author><author><keyname>Kovacevic</keyname><forenames>Jelena</forenames></author></authors><title>SegSALSA-STR: A convex formulation to supervised hyperspectral image
  segmentation using hidden fields and structure tensor regularization</title><categories>cs.CV</categories><comments>This paper was submitted to IEEE WHISPERS 2015: 7th Workshop on
  Hyperspectral Image and Signal Processing: Evolution on Remote Sensing. 5
  pages, 1 figure</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a supervised hyperspectral image segmentation algorithm based on a
convex formulation of a marginal maximum a posteriori segmentation with hidden
fields and structure tensor regularization: Segmentation via the Constraint
Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization
(SegSALSA-STR). This formulation avoids the generally discrete nature of
segmentation problems and the inherent NP-hardness of the integer optimization
associated.
  We extend the Segmentation via the Constraint Split Augmented Lagrangian
Shrinkage (SegSALSA) algorithm by generalizing the vectorial total variation
prior using a structure tensor prior constructed from a patch-based Jacobian.
The resulting algorithm is convex, time-efficient and highly parallelizable.
This shows the potential of combining hidden fields with convex optimization
through the inclusion of different regularizers. The SegSALSA-STR algorithm is
validated in the segmentation of real hyperspectral images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07029</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07029</id><created>2015-04-27</created><updated>2015-10-13</updated><authors><author><keyname>Novotny</keyname><forenames>David</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author></authors><title>Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object
  Detection</title><categories>cs.CV</categories><comments>Accepted to ICCV15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel efficient method for extraction of object proposals is introduced.
Its &quot;objectness&quot; function exploits deep spatial pyramid features, a novel
fast-to-compute HoG-based edge statistic and the EdgeBoxes score. The
efficiency is achieved by the use of spatial bins in a novel combination with
sparsity-inducing group normalized SVM. State-of-the-art recall performance is
achieved on Pascal VOC07, significantly outperforming methods with comparable
speed. Interestingly, when only 100 proposals per image are considered the
method attains 78% recall on VOC07. The method improves mAP of the RCNN
state-of-the-art class-specific detector, increasing it by 10 points when only
50 proposals are used in each image. The system trained on twenty classes
performs well on the two hundred class ILSVRC2013 set confirming generalization
capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07038</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07038</id><created>2015-04-27</created><authors><author><keyname>Pertin</keyname><forenames>Dimitri</forenames></author><author><keyname>F&#xe9;ron</keyname><forenames>Didier</forenames></author><author><keyname>Van Kempen</keyname><forenames>Alexandre</forenames></author><author><keyname>Parrein</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Performance evaluation of the Mojette erasure code for fault-tolerant
  distributed hot data storage</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packet erasure codes are today a real alternative to replication in fault
tolerant distributed storage systems. In this paper, we propose the Mojette
erasure code based on the Mojette transform, a formerly tomographic tool. The
performance of coding and decoding are compared to the Reed-Solomon code
implementations of the two open-source reference libraries namely ISA-L and
Jerasure 2.0. Results clearly show better performances for our discrete
geometric code compared to the classical algebraic approaches. A gain factor up
to $2$ is measured in comparison with the ISA-L Intel . Those very good
performances allow to deploy Mojette erasure code for hot data distributed
storage and I/O intensive applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07041</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07041</id><created>2015-04-27</created><updated>2015-04-30</updated><authors><author><keyname>Caltais</keyname><forenames>Georgiana</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>On the Verification of SCOOP Programs</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.7509</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the development of a toolbox for the verification
of programs in the context of SCOOP -- an elegant concurrency model, recently
formalized based on Rewriting Logic (RL) and Maude. SCOOP is implemented in
Eiffel and its applicability is demonstrated also from a practical perspective,
in the area of robotics programming. Our contribution consists in devising and
integrating an alias analyzer and a Coffman deadlock detector under the roof of
the same RL-based semantic framework of SCOOP. This enables using the Maude
rewriting engine and its LTL model-checker &quot;for free&quot;, in order to perform the
analyses of interest. We discuss the limitations of our approach for
model-checking deadlocks and provide solutions to the state explosion problem.
The latter is mainly caused by the size of the SCOOP formalization which
incorporates all the aspects of a real concurrency model. On the aliasing side,
we propose an extension of a previously introduced alias calculus based on
program expressions, to the setting of unbounded program executions such as
infinite loops and recursive calls. Moreover, we devise a corresponding
executable specification easily implementable on top of the SCOOP
formalization. An important property of our extension is that, in
non-concurrent settings, the corresponding alias expressions can be
over-approximated in terms of a notion of regular expressions. This further
enables us to derive an algorithm that always stops and provides a sound
over-approximation of the &quot;may aliasing&quot; information, where soundness stands
for the lack of false negatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07056</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07056</id><created>2015-04-27</created><updated>2016-01-27</updated><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>An Almost-Tight Distributed Algorithm for Computing Single-Source
  Shortest Paths</title><categories>cs.DC cs.DS</categories><comments>Under submission. Abstract shortened to respect the arXiv limit of
  1920 characters</comments><acm-class>C.2.4; F.2.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic $(1+o(1))$-approximation
$O(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source
shortest paths problem on distributed weighted networks (the CONGEST model);
here $n$ is the number of nodes in the network and $D$ is its (hop) diameter.
This is the first non-trivial deterministic algorithm for this problem. It also
improves (i) the running time of the randomized $(1+o(1))$-approximation
$\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a
factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log
\epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\tilde
O(n^{1/2+\epsilon}+D)$-time algorithm [STOC 2013] within the same running time.
Our running time matches the known time lower bound of $\Omega(n^{1/2}/\log n +
D)$ [Das Sarma et al. STOC 2011] modulo some lower-order terms, thus
essentially settling the status of this problem which was raised at least a
decade ago [Elkin, SIGACT News 2004]. It also implies a
$(2+o(1))$-approximation $O(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for
approximating a network's weighted diameter which almost matches the lower
bound by Holzer et al. [PODC 2012]. In achieving this result, we develop two
techniques which might be of independent interest and useful in other settings:
(i) a deterministic process that replaces the &quot;hitting set argument&quot; commonly
used for shortest paths computation in various settings, and (ii) a simple,
deterministic, construction of an $(n^{o(1)}, o(1))$-hop set of size
$O(n^{1+o(1)})$. We combine these techniques with many distributed algorithmic
techniques, some of which from problems that are not directly related to
shortest paths, e.g. ruling sets [Goldberg et al. STOC 1987], source detection
[Lenzen, Peleg PODC 2013], and partial distance estimation [Lenzen, Patt-Shamir
PODC 2015].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07057</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07057</id><created>2015-04-27</created><authors><author><keyname>Toscani</keyname><forenames>Giuseppe</forenames></author></authors><title>The fractional Fisher information and the central limit theorem for
  stable laws</title><categories>cs.IT math-ph math.IT math.MP math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new information-theoretic approach to the central limit theorem for stable
laws is presented. The main novelty is the concept of relative fractional
Fisher information, which shares most of the properties of the classical one,
included Blachman-Stam type inequalities. These inequalities relate the
fractional Fisher information of the sum of $n$ independent random variables to
the information contained in sums over subsets containing $n-1$ of the random
variables. As a consequence, a simple proof of the monotonicity of the relative
fractional Fisher information in central limit theorems for stable law is
obtained, together with an explicit decay rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07060</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07060</id><created>2015-04-27</created><updated>2015-05-04</updated><authors><author><keyname>Fan</keyname><forenames>Jun Wei</forenames></author></authors><title>Denoise in the pseudopolar grid Fourier space using exact inverse
  pseudopolar Fourier transform</title><categories>physics.data-an cond-mat.mes-hall cs.IT math.IT</categories><comments>4 pages, 7 figures, 1 important equation Eq.(3)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper I show a matrix method to calculate the exact inverse
pseudopolar grid Fourier transform, and use this transform to do noise removals
in the k space of pseudopolar grids. I apply the Gaussian filter to this
pseudopolar grid and find the advantages of the noise removals are very
excellent by using pseudopolar grid, and finally I show the Cartesian grid
denoise for comparisons. The results present the signal to noise ratio and the
variance are much better when doing noise removals in the pseudopolar grid than
the Cartesian grid. The noise removals of pseudopolar grid or Cartesian grid
are both in the k space, and all these noises are added in the real space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07066</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07066</id><created>2015-04-27</created><authors><author><keyname>M&#xe4;cker</keyname><forenames>Alexander</forenames></author><author><keyname>Malatyali</keyname><forenames>Manuel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author><author><keyname>Riechers</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Non-Preemptive Scheduling on Machines with Setup Times</title><categories>cs.DS</categories><comments>A conference version of this paper has been accepted for publication
  in the proceedings of the 14th Algorithms and Data Structures Symposium
  (WADS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem in which n jobs that are classified into k types are to
be scheduled on m identical machines without preemption. A machine requires a
proper setup taking s time units before processing jobs of a given type. The
objective is to minimize the makespan of the resulting schedule. We design and
analyze an approximation algorithm that runs in time polynomial in n, m and k
and computes a solution with an approximation factor that can be made
arbitrarily close to 3/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07067</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07067</id><created>2015-04-27</created><updated>2015-10-24</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Rolinek</keyname><forenames>Michal</forenames></author><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author></authors><title>Effectiveness of Structural Restrictions for Hybrid CSPs</title><categories>cs.CC cs.DM</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Satisfaction Problem (CSP) is a fundamental algorithmic problem
that appears in many areas of Computer Science. It can be equivalently stated
as computing a homomorphism $\mbox{$\bR \rightarrow \bGamma$}$ between two
relational structures, e.g.\ between two directed graphs. Analyzing its
complexity has been a prominent research direction, especially for {\em fixed
template CSPs} in which the right side $\bGamma$ is fixed and the left side
$\bR$ is unconstrained.
  Far fewer results are known for the {\em hybrid} setting that restricts both
sides simultaneously. It assumes that $\bR$ belongs to a certain class of
relational structures (called a {\em structural restriction} in this paper). We
study which structural restrictions are {\em effective}, i.e.\ there exists a
fixed template $\bGamma$ (from a certain class of languages) for which the
problem is tractable when $\bR$ is restricted, and NP-hard otherwise. We
provide a characterization for structural restrictions that are {\em closed
under inverse homomorphisms}. The criterion is based on the {\em chromatic
number} of a relational structure defined in this paper; it generalizes the
standard chromatic number of a graph.
  As our main tool, we use the algebraic machinery developed for fixed template
CSPs. To apply it to our case, we introduce a new construction called a &quot;lifted
language.&quot; We also give a characterization for structural restrictions
corresponding to minor-closed families of graphs, extend results to certain
Valued CSPs (namely conservative valued languages), and state implications for
CSPs with ordered variables, (valued) CSPs on structures with large girth, and
for the maximum weight independent set problem on some restricted families of
graphs including graphs with large girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07070</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07070</id><created>2015-04-27</created><authors><author><keyname>Wu</keyname><forenames>Weiyi</forenames></author><author><keyname>Zhai</keyname><forenames>Ennan</forenames></author><author><keyname>Jackowitz</keyname><forenames>Daniel</forenames></author><author><keyname>Wolinsky</keyname><forenames>David Isaac</forenames></author><author><keyname>Gu</keyname><forenames>Liang</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Warding off timing attacks in Deterland</title><categories>cs.OS</categories><comments>14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive parallelism and resource sharing embodying today's cloud business
model not only exacerbate the security challenge of timing channels, but also
undermine the viability of defenses based on resource partitioning. This paper
proposes hypervisor-enforced timing mitigation to control timing channels in
cloud environments. This approach closes &quot;reference clocks&quot; internal to the
cloud by imposing a deterministic view of time on guest code, and uses timing
mitigators to pace I/O and rate-limit potential information leakage to external
observers. Our prototype hypervisor implementation is the first system that can
mitigate timing-channel leakage across full-scale existing operating systems
such as Linux and applications written in arbitrary languages. Mitigation
incurs a varying performance cost, depending on workload and tunable
leakage-limiting parameters, but this cost may be justified for
security-critical cloud applications and data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07071</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07071</id><created>2015-04-27</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Wegener</keyname><forenames>Dennis</forenames></author><author><keyname>Schomisch</keyname><forenames>Siegfried</forenames></author></authors><title>Exploring semantically-related concepts from Wikipedia: the case of SeRE</title><categories>cs.CL cs.IR</categories><comments>In Classification &amp; visualization : interfaces to knowledge ;
  proceedings of the International UDC Seminar 24 - 25 October 2013, The Hague,
  The Netherlands, edited by Aida Slavic, Almila Akdag Salah, and Sylvie
  Davies, 153-165. W\&quot;urzburg: Ergon-Verl</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present our web application SeRE designed to explore
semantically related concepts. Wikipedia and DBpedia are rich data sources to
extract related entities for a given topic, like in- and out-links, broader and
narrower terms, categorisation information etc. We use the Wikipedia full text
body to compute the semantic relatedness for extracted terms, which results in
a list of entities that are most relevant for a topic. For any given query, the
user interface of SeRE visualizes these related concepts, ordered by semantic
relatedness; with snippets from Wikipedia articles that explain the connection
between those two entities. In a user study we examine how SeRE can be used to
find important entities and their relationships for a given topic and to answer
the question of how the classification system can be used for filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07073</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07073</id><created>2015-04-27</created><updated>2015-05-18</updated><authors><author><keyname>Buchwald</keyname><forenames>Sebastian</forenames></author><author><keyname>Mohr</keyname><forenames>Manuel</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Optimal Shuffle Code with Permutation Instructions</title><categories>cs.DS cs.PL</categories><comments>20 pages, 5 figures, full version of a paper accepted at WADS'15.
  Minor update: fixed typos, corrected comma placement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During compilation of a program, register allocation is the task of mapping
program variables to machine registers. During register allocation, the
compiler may introduce shuffle code, consisting of copy and swap operations,
that transfers data between the registers. Three common sources of shuffle code
are conflicting register mappings at joins in the control flow of the program,
e.g, due to if-statements or loops; the calling convention for procedures,
which often dictates that input arguments or results must be placed in certain
registers; and machine instructions that only allow a subset of registers to
occur as operands. Recently, Mohr et al. proposed to speed up shuffle code with
special hardware instructions that arbitrarily permute the contents of up to
five registers and gave a heuristic for computing such shuffle codes. In this
paper, we give an efficient algorithm for generating optimal shuffle code in
the setting of Mohr et al. An interesting special case occurs when no register
has to be transferred to more than one destination, i.e., it suffices to
permute the contents of the registers. This case is equivalent to factoring a
permutation into a minimal product of permutations, each of which permutes up
to five elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07075</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07075</id><created>2015-04-27</created><updated>2015-09-29</updated><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author></authors><title>Random coding exponents galore via decoupling</title><categories>quant-ph cs.IT math.IT</categories><comments>The changes are confined to Sec. 10. Added an exponentially decaying
  term to (152) ((149) in previous version) to make it precise - this makes the
  trace of one state to be one, this adds 1 more error term and 3 more
  equations, made the multiplying factor in the errors to be 20, and in few
  places the system labels were 'F' instead of 'G'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A missing piece in quantum information theory, with very few exceptions, has
been to provide the random coding exponents for quantum information-processing
protocols. We remedy the situation by providing these exponents for a variety
of protocols including those at the top of the family tree of protocols. Our
line of attack is to provide an exponential bound on the decoupling error for a
restricted class of completely positive maps where a key term in the exponent
is in terms of a R\'enyi \alpha-information-theoretic quantity for any \alpha
$\in$ (1,2]. Among the protocols covered are fully quantum Slepian-Wolf,
quantum state merging, quantum state redistribution, quantum/classical
communication across channels with side information at the transmitter with or
without entanglement assistance, and quantum communication across broadcast
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07082</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07082</id><created>2015-04-27</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>Pilar</keyname><forenames>Bharathi</forenames></author></authors><title>Shape Representation and Classification through Pattern Spectrum and
  Local Binary Pattern - A Decision Level Fusion Approach</title><categories>cs.CV</categories><comments>Fifth International Conference on Signals and Image Processing
  (ICSIP) 2014</comments><doi>10.1109/ICSIP.2014.41</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a decision level fused local Morphological Pattern
Spectrum(PS) and Local Binary Pattern (LBP) approach for an efficient shape
representation and classification. This method makes use of Earth Movers
Distance(EMD) as the measure in feature matching and shape retrieval process.
The proposed approach has three major phases : Feature Extraction, Construction
of hybrid spectrum knowledge base and Classification. In the first phase,
feature extraction of the shape is done using pattern spectrum and local binary
pattern method. In the second phase, the histograms of both pattern spectrum
and local binary pattern are fused and stored in the knowledge base. In the
third phase, the comparison and matching of the features, which are represented
in the form of histograms, is done using Earth Movers Distance(EMD) as metric.
The top-n shapes are retrieved for each query shape. The accuracy is tested by
means of standard Bulls eye score method. The experiments are conducted on
publicly available shape datasets like Kimia-99, Kimia-216 and MPEG-7. The
comparative study is also provided with the well known approaches to exhibit
the retrieval accuracy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07091</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07091</id><created>2015-04-27</created><updated>2015-07-03</updated><authors><author><keyname>Bergamini</keyname><forenames>Elisabetta</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Fully-dynamic Approximation of Betweenness Centrality</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Betweenness is a well-known centrality measure that ranks the nodes of a
network according to their participation in shortest paths. Since an exact
computation is prohibitive in large networks, several approximation algorithms
have been proposed. Besides that, recent years have seen the publication of
dynamic algorithms for efficient recomputation of betweenness in evolving
networks. In previous work we proposed the first semi-dynamic algorithms that
recompute an approximation of betweenness in connected graphs after batches of
edge insertions.
  In this paper we propose the first fully-dynamic approximation algorithms
(for weighted and unweighted undirected graphs that need not to be connected)
with a provable guarantee on the maximum approximation error. The transfer to
fully-dynamic and disconnected graphs implies additional algorithmic problems
that could be of independent interest. In particular, we propose a new upper
bound on the vertex diameter for weighted undirected graphs. For both weighted
and unweighted graphs, we also propose the first fully-dynamic algorithms that
keep track of such upper bound. In addition, we extend our former algorithm for
semi-dynamic BFS to batches of both edge insertions and deletions.
  Using approximation, our algorithms are the first to make in-memory
computation of betweenness in fully-dynamic networks with millions of edges
feasible. Our experiments show that they can achieve substantial speedups
compared to recomputation, up to several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07094</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07094</id><created>2015-04-27</created><authors><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Tang</keyname><forenames>Yu-Hang</forenames></author><author><keyname>Li</keyname><forenames>Xuejin</forenames></author><author><keyname>Karniadakis</keyname><forenames>George Em</forenames></author></authors><title>Mesoscale modeling of phase transition dynamics of thermoresponsive
  polymers</title><categories>physics.chem-ph cond-mat.mtrl-sci cs.CE physics.comp-ph</categories><comments>Manuscript submitted to Chemical Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a non-isothermal mesoscopic model for investigation of the phase
transition dynamics of thermoresponsive polymers. Since this model conserves
energy in the simulations, it is able to correctly capture not only the
transient behavior of polymer precipitation from solvent, but also the energy
variation associated with the phase transition process. Simulations provide
dynamic details of the thermally induced phase transition and confirm two
different mechanisms dominating the phase transition dynamics. A shift of
endothermic peak with concentration is observed and the underlying mechanism is
explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07098</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07098</id><created>2015-04-27</created><authors><author><keyname>Burton</keyname><forenames>Craig</forenames></author><author><keyname>Culnane</keyname><forenames>Chris</forenames></author><author><keyname>Schneider</keyname><forenames>Steve</forenames></author></authors><title>Secure and Verifiable Electronic Voting in Practice: the use of vVote in
  the Victorian State Election</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The November 2014 Australian State of Victoria election was the first
statutory political election worldwide at State level which deployed an
end-to-end verifiable electronic voting system in polling places. This was the
first time blind voters have been able to cast a fully secret ballot in a
verifiable way, and the first time a verifiable voting system has been used to
collect remote votes in a political election. The code is open source, and the
output from the election is verifiable. The system took 1121 votes from these
particular groups, an increase on 2010 and with fewer polling places.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07101</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07101</id><created>2015-04-27</created><updated>2016-01-16</updated><authors><author><keyname>Crimaldi</keyname><forenames>Irene</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Morrison</keyname><forenames>Greg</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>Homophily and Triadic Closure in Evolving Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>32 pages, 8 figures, 4 tables, submitted. Exposition of the work
  improved. Title upadated to : Modeling Networks with a Growing
  Feature-Structure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new network model accounting for multidimensional assortativity.
Each node is characterized by a number of features and the probability of a
link between two nodes depends on common features. We do not fix a priori the
total number of possible features. The bipartite network of the nodes and the
features evolves according to a stochastic dynamics that depends on three
parameters that respectively regulate the preferential attachment in the
transmission of the features to the nodes, the number of new features per node,
and the power-law behavior of the total number of observed features. Our model
also takes into account a mechanism of triadic closure. We provide theoretical
results and statistical estimators for the parameters of the model. We validate
our approach by means of simulations and an empirical analysis of a network of
scientific collaborations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07107</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07107</id><created>2015-04-27</created><updated>2015-06-20</updated><authors><author><keyname>Hu</keyname><forenames>Wenbo</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Xu</keyname><forenames>Minjie</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Fast Sampling for Bayesian Max-Margin Models</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian max-margin models have shown great superiority in various machine
learning tasks with a likelihood regularization, while the probabilistic Monte
Carlo sampling for these models still remains challenging, especially for
large-scale settings. In analogy to the data augmentation technique to tackle
with non-smoothness of the hinge loss, we present a stochastic subgradient MCMC
method which is easy to implement and computationally efficient. We investigate
the variants that use adaptive stepsizes and thermostats to improve mixing
speeds for Bayesian linear SVM. Furthermore, we design a stochastic subgradient
HMC within Gibbs method and a doubly stochastic HMC algorithm for mixture of
SVMs, a popular extension of linear classifiers. Experimental results on a wide
range of problems demonstrate the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07116</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07116</id><created>2015-04-27</created><updated>2015-07-03</updated><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Delouille</keyname><forenames>Veronique</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Meta learning of bounds on the Bayes classifier error</title><categories>cs.LG astro-ph.SR cs.CV cs.IT math.IT</categories><comments>6 pages, 3 figures, to appear in proceedings of 2015 IEEE Signal
  Processing and SP Education Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meta learning uses information from base learners (e.g. classifiers or
estimators) as well as information about the learning problem to improve upon
the performance of a single base learner. For example, the Bayes error rate of
a given feature space, if known, can be used to aid in choosing a classifier,
as well as in feature selection and model selection for the base classifiers
and the meta classifier. Recent work in the field of f-divergence functional
estimation has led to the development of simple and rapidly converging
estimators that can be used to estimate various bounds on the Bayes error. We
estimate multiple bounds on the Bayes error using an estimator that applies
meta learning to slowly converging plug-in estimators to obtain the parametric
convergence rate. We compare the estimated bounds empirically on simulated data
and then estimate the tighter bounds on features extracted from an image patch
analysis of sunspot continuum and magnetogram images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07127</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07127</id><created>2015-04-27</created><authors><author><keyname>Gasieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Jurdzinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Martin</keyname><forenames>Russell</forenames></author><author><keyname>Stachowiak</keyname><forenames>Grzegorz</forenames></author></authors><title>Deterministic Symmetry Breaking in Ring Networks</title><categories>cs.DC</categories><comments>Conference version accepted to ICDCS 2015</comments><msc-class>68W15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a distributed coordination mechanism for uniform agents located on a
circle. The agents perform their actions in synchronised rounds. At the
beginning of each round an agent chooses the direction of its movement from
clockwise, anticlockwise, or idle, and moves at unit speed during this round.
Agents are not allowed to overpass, i.e., when an agent collides with another
it instantly starts moving with the same speed in the opposite direction
(without exchanging any information with the other agent). However, at the end
of each round each agent has access to limited information regarding its
trajectory of movement during this round.
  We assume that $n$ mobile agents are initially located on a circle unit
circumference at arbitrary but distinct positions unknown to other agents. The
agents are equipped with unique identifiers from a fixed range. The {\em
location discovery} task to be performed by each agent is to determine the
initial position of every other agent.
  Our main result states that, if the only available information about movement
in a round is limited to %information about distance between the initial and
the final position, then there is a superlinear lower bound on time needed to
solve the location discovery problem. Interestingly, this result corresponds to
a combinatorial symmetry breaking problem, which might be of independent
interest. If, on the other hand, an agent has access to the distance to its
first collision with another agent in a round, we design an asymptotically
efficient and close to optimal solution for the location discovery problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07129</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07129</id><created>2015-04-27</created><authors><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Klimm</keyname><forenames>Max</forenames></author><author><keyname>L&#xfc;bbecke</keyname><forenames>Elisabeth</forenames></author></authors><title>Scheduling Bidirectional Traffic on a Path</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of scheduling bidirectional traffic along a
path composed of multiple segments. The main feature of the problem is that
jobs traveling in the same direction can be scheduled in quick succession on a
segment, while jobs in opposing directions cannot cross a segment at the same
time. We show that this tradeoff makes the problem significantly harder than
the related flow shop problem, by proving that it is NP-hard even for identical
jobs. We complement this result with a PTAS for a single segment and
non-identical jobs. If we allow some pairs of jobs traveling in different
directions to cross a segment concurrently, the problem becomes APX-hard even
on a single segment and with identical jobs. We give polynomial algorithms for
the setting with restricted compatibilities between jobs on a single and any
constant number of segments, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07135</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07135</id><created>2015-04-27</created><updated>2015-07-08</updated><authors><author><keyname>Alemzadeh</keyname><forenames>Homa</forenames></author><author><keyname>Chen</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Andrew</forenames></author><author><keyname>Kalbarczyk</keyname><forenames>Zbigniew</forenames></author><author><keyname>Raman</keyname><forenames>Jaishankar</forenames></author><author><keyname>Leveson</keyname><forenames>Nancy</forenames></author><author><keyname>Iyer</keyname><forenames>Ravishankar K.</forenames></author></authors><title>Systems-theoretic Safety Assessment of Robotic Telesurgical Systems</title><categories>cs.RO cs.CR cs.SE</categories><comments>Revise based on reviewers feedback. To appear in the the
  International Conference on Computer Safety, Reliability, and Security
  (SAFECOMP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic telesurgical systems are one of the most complex medical
cyber-physical systems on the market, and have been used in over 1.75 million
procedures during the last decade. Despite significant improvements in design
of robotic surgical systems through the years, there have been ongoing
occurrences of safety incidents during procedures that negatively impact
patients. This paper presents an approach for systems-theoretic safety
assessment of robotic telesurgical systems using software-implemented
fault-injection. We used a systemstheoretic hazard analysis technique (STPA) to
identify the potential safety hazard scenarios and their contributing causes in
RAVEN II robot, an open-source robotic surgical platform. We integrated the
robot control software with a softwareimplemented fault-injection engine which
measures the resilience of the system to the identified safety hazard scenarios
by automatically inserting faults into different parts of the robot control
software. Representative hazard scenarios from real robotic surgery incidents
reported to the U.S. Food and Drug Administration (FDA) MAUDE database were
used to demonstrate the feasibility of the proposed approach for safety-based
design of robotic telesurgical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07149</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07149</id><created>2015-04-27</created><updated>2015-07-02</updated><authors><author><keyname>Witt</keyname><forenames>Sascha</forenames></author></authors><title>Trip-Based Public Transit Routing</title><categories>cs.DS</categories><comments>Minor corrections, no substantial changes. To be presented at ESA
  2015</comments><acm-class>G.2.2; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing all Pareto-optimal journeys in a public
transit network regarding the two criteria of arrival time and number of
transfers taken. We take a novel approach, focusing on trips and transfers
between them, allowing fine-grained modeling. Our experiments on the
metropolitan network of London show that the algorithm computes full 24-hour
profiles in 70 ms after a preprocessing phase of 30 s, allowing fast queries in
dynamic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07154</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07154</id><created>2015-04-27</created><authors><author><keyname>Kapetanovic</keyname><forenames>Dzevdan</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author></authors><title>Physical Layer Security for Massive MIMO: An Overview on Passive
  Eavesdropping and Active Attacks</title><categories>cs.IT math.IT</categories><comments>5 figures, to appear in IEEE Communications Magazine 2015, special
  issue on Wireless Physical Layer Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses opportunities and challenges of physical layer
security integration in massive multiple-input multiple-output (MaMIMO)
systems. Specifically, we first show that MaMIMO itself is robust against
passive eavesdropping attacks. We then review a pilot contamination scheme
which actively attacks the channel estimation process. This pilot contamination
attack is not only dramatically reducing the achievable secrecy capacity but is
also difficult to detect. We proceed by reviewing some methods from literature
that detect active attacks on MaMIMO. The last part of the paper surveys the
open research problems that we believe are the most important to address in the
future and give a few promising directions of research to solve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07159</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07159</id><created>2015-04-27</created><authors><author><keyname>Fan</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Zheng</keyname><forenames>Kang</forenames></author><author><keyname>Lin</keyname><forenames>Yuewei</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author></authors><title>Combining Local Appearance and Holistic View: Dual-Source Deep Neural
  Networks for Human Pose Estimation</title><categories>cs.CV</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new learning-based method for estimating 2D human pose from a
single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN).
Recently, many methods have been developed to estimate human pose by using pose
priors that are estimated from physiologically inspired graphical models or
learned from a holistic perspective. In this paper, we propose to integrate
both the local (body) part appearance and the holistic view of each local part
for more accurate human pose estimation. Specifically, the proposed DS-CNN
takes a set of image patches (category-independent object proposals for
training and multi-scale sliding windows for testing) as the input and then
learns the appearance of each local part by considering their holistic views in
the full body. Using DS-CNN, we achieve both joint detection, which determines
whether an image patch contains a body joint, and joint localization, which
finds the exact location of the joint in the image patch. Finally, we develop
an algorithm to combine these joint detection/localization results from all the
image patches for estimating the human pose. The experimental results show the
effectiveness of the proposed method by comparing to the state-of-the-art
human-pose estimation methods based on pose priors that are estimated from
physiologically inspired graphical models or learned from a holistic
perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07168</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07168</id><created>2015-04-27</created><authors><author><keyname>Angelopoulos</keyname><forenames>Spyros</forenames></author></authors><title>Further Connections Between Contract-Scheduling and Ray-Searching
  Problems</title><categories>cs.AI</categories><comments>Full version of conference paper, to appear in Proceedings of IJCAI
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses two classes of different, yet interrelated optimization
problems. The first class of problems involves a robot that must locate a
hidden target in an environment that consists of a set of concurrent rays. The
second class pertains to the design of interruptible algorithms by means of a
schedule of contract algorithms. We study several variants of these families of
problems, such as searching and scheduling with probabilistic considerations,
redundancy and fault-tolerance issues, randomized strategies, and trade-offs
between performance and preemptions. For many of these problems we present the
first known results that apply to multi-ray and multi-problem domains. Our
objective is to demonstrate that several well-motivated settings can be
addressed using the same underlying approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07182</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07182</id><created>2015-04-27</created><authors><author><keyname>Wu</keyname><forenames>Ji</forenames></author><author><keyname>Li</keyname><forenames>Miao</forenames></author><author><keyname>Lee</keyname><forenames>Chin-Hui</forenames></author></authors><title>A Probabilistic Framework for Representing Dialog Systems and
  Entropy-Based Dialog Management through Dynamic Stochastic State Evolution</title><categories>cs.AI</categories><comments>10 pages, 6 figures, 6 tables,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a probabilistic framework for goal-driven spoken
dialog systems. A new dynamic stochastic state (DS-state) is then defined to
characterize the goal set of a dialog state at different stages of the dialog
process. Furthermore, an entropy minimization dialog management(EMDM) strategy
is also proposed to combine with the DS-states to facilitate a robust and
efficient solution in reaching a user's goals. A Song-On-Demand task, with a
total of 38117 songs and 12 attributes corresponding to each song, is used to
test the performance of the proposed approach. In an ideal simulation, assuming
no errors, the EMDM strategy is the most efficient goal-seeking method among
all tested approaches, returning the correct song within 3.3 dialog turns on
average. Furthermore, in a practical scenario, with top five candidates to
handle the unavoidable automatic speech recognition (ASR) and natural language
understanding (NLU) errors, the results show that only 61.7\% of the dialog
goals can be successfully obtained in 6.23 dialog turns on average when random
questions are asked by the system, whereas if the proposed DS-states are
updated with the top 5 candidates from the SLU output using the proposed EMDM
strategy executed at every DS-state, then a 86.7\% dialog success rate can be
accomplished effectively within 5.17 dialog turns on average. We also
demonstrate that entropy-based DM strategies are more efficient than
non-entropy based DM. Moreover, using the goal set distributions in EMDM, the
results are better than those without them, such as in sate-of-the-art database
summary DM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07184</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07184</id><created>2015-04-27</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Architectural Adequacy and Evolutionary Adequacy as Characteristics of a
  Candidate Informational Money</title><categories>cs.CY</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For money-like informational commodities the notions of architectural
adequacy and evolutionary adequacy are proposed as the first two stages of a
moneyness maturity hierarchy. Then three classes of informational commodities
are distinguished: exclusively informational commodities, strictly
informational commodities, and ownable informational commodities. For each
class money-like instances of that commodity class, as well as monies of that
class may exist.
  With the help of these classifications and making use of previous assessments
of Bitcoin, it is argued that at this stage Bitcoin is unlikely ever to evolve
into a money. Assessing the evolutionary adequacy of Bitcoin is perceived in
terms of a search through its design hull for superior design alternatives.
  An extensive comparison is made between the search for superior design
alternatives to Bitcoin and the search for design alternatives to a specific
and unconventional view on the definition of fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07192</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07192</id><created>2015-04-27</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Location-aware sign-on and key exchange using attribute-based encryption
  and Bluetooth beacons</title><categories>cs.CR</categories><comments>Communications and Network Security (CNS), 2013 IEEE Conference on.
  arXiv admin note: text overlap with arXiv:1410.0983</comments><doi>10.1109/CNS.2013.6682750</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a mobile sign-on scheme, which utilizes Bluetooth Low
Energy beacons for location awareness and Attribute-Based Encryption for
expressive, broadcast-style key exchange. Bluetooth Low Energy beacons
broadcast encrypted messages with encoded access policies. Within range of the
beacons, a user with appropriate attributes is able to decrypt the broadcast
message and obtain parameters that allow the user to perform a short or
simplified login. The effect is a &quot;traveling&quot; sign-on that accompanies the user
throughout different locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07193</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07193</id><created>2015-04-27</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Secure Zones: An Attribute-Based Encryption advisory system for safe
  firearms</title><categories>cs.CR</categories><comments>Communications and Network Security (CNS), 2013 IEEE Conference on.
  arXiv admin note: substantial text overlap with arXiv:1411.1733</comments><doi>10.1109/CNS.2013.6682746</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an application of the highly expressive Attribute-Based
Encryption to implement Secure Zones for firearms. Within these zones,
radio-transmitted local policies based on attributes of the user and the
firearm are received by embedded hardware in the firearms, which then advises
the user about safe operations. The Secure Zones utilize Attribute-Based
Encryption to encode the policies and user attributes, and providing privacy
and security through it cryptography. We describe a holistic approach to
evolving the firearm to a cyber-physical system to aid in augmenting safety. We
introduce a conceptual model for a firearm equipped with sensors and a
context-aware software agent. Based on the information from the sensors, the
agent can access the context and inform the user of potential unsafe
operations. To support Secure Zones and the cyber-physical firearm model, we
propose a Key Infrastructure Scheme for key generation, distribution, and
management, and a Context-Aware Software Agent Framework for Firearms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07206</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07206</id><created>2015-04-27</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Muthu Kumar</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author><author><keyname>Tan</keyname><forenames>Bernard C. Y.</forenames></author><author><keyname>Ragupathi</keyname><forenames>Kiruthika</forenames></author></authors><title>Learning Instructor Intervention from MOOC Forums: Early Results and
  Issues</title><categories>cs.CY</categories><comments>To appear in proceedings of Education Data Mining 2015, Madrid, Spain</comments><acm-class>H.3.3; K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With large student enrollment, MOOC instructors face the unique challenge in
deciding when to intervene in forum discussions with their limited bandwidth.
We study this problem of instructor intervention. Using a large sample of forum
data culled from 61 courses, we design a binary classifier to predict whether
an instructor should intervene in a discussion thread or not. By incorporating
novel information about a forum's type into the classification process, we
improve significantly over the previous state-of-the-art.
  We show how difficult this decision problem is in the real world by
validating against indicative human judgment, and empirically show the
problem's sensitivity to instructors' intervention preferences. We conclude
this paper with our take on the future research issues in intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07218</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07218</id><created>2015-04-27</created><updated>2015-05-28</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons</title><categories>cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>accepted to International Conference on Machine Learning (ICML), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the preference-based top-$K$ rank aggregation problem.
Suppose that a collection of items is repeatedly compared in pairs, and one
wishes to recover a consistent ordering that emphasizes the top-$K$ ranked
items, based on partially revealed preferences. We focus on the
Bradley-Terry-Luce (BTL) model that postulates a set of latent preference
scores underlying all items, where the odds of paired comparisons depend only
on the relative scores of the items involved.
  We characterize the minimax limits on identifiability of top-$K$ ranked
items, in the presence of random and non-adaptive sampling. Our results
highlight a separation measure that quantifies the gap of preference scores
between the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimum
sample complexity required for reliable top-$K$ ranking scales inversely with
the separation measure irrespective of other preference distribution metrics.
To approach this minimax limit, we propose a nearly linear-time ranking scheme,
called \emph{Spectral MLE}, that returns the indices of the top-$K$ items in
accordance to a careful score estimate. In a nutshell, Spectral MLE starts with
an initial score estimate with minimal squared loss (obtained via a spectral
method), and then successively refines each component with the assistance of
coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item
identification under minimal sample complexity. The practical applicability of
Spectral MLE is further corroborated by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07225</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07225</id><created>2015-04-27</created><updated>2015-10-12</updated><authors><author><keyname>Chandar</keyname><forenames>Sarath</forenames></author><author><keyname>Khapra</keyname><forenames>Mitesh M.</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author></authors><title>Correlational Neural Networks</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>27 pages. To Appear in Neural Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common Representation Learning (CRL), wherein different descriptions (or
views) of the data are embedded in a common subspace, is receiving a lot of
attention recently. Two popular paradigms here are Canonical Correlation
Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA
based approaches learn a joint representation by maximizing correlation of the
views when projected to the common subspace. AE based methods learn a common
representation by minimizing the error of reconstructing the two views. Each of
these approaches has its own advantages and disadvantages. For example, while
CCA based approaches outperform AE based approaches for the task of transfer
learning, they are not as scalable as the latter. In this work we propose an AE
based approach called Correlational Neural Network (CorrNet), that explicitly
maximizes correlation among the views when projected to the common subspace.
Through a series of experiments, we demonstrate that the proposed CorrNet is
better than the above mentioned approaches with respect to its ability to learn
correlated common representations. Further, we employ CorrNet for several cross
language tasks and show that the representations learned using CorrNet perform
better than the ones learned using other state of the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07227</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07227</id><created>2015-04-27</created><authors><author><keyname>Karzand</keyname><forenames>Mina</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Communication Strategies for Low-Latency Trading</title><categories>cs.IT math.IT q-fin.TR</categories><comments>Will appear in IEEE International Symposium on Information Theory
  (ISIT), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility of latency arbitrage in financial markets has led to the
deployment of high-speed communication links between distant financial centers.
These links are noisy and so there is a need for coding. In this paper, we
develop a gametheoretic model of trading behavior where two traders compete to
capture latency arbitrage opportunities using binary signalling. Different
coding schemes are strategies that trade off between reliability and latency.
When one trader has a better channel, the second trader should not compete.
With statistically identical channels, we find there are two different regimes
of channel noise for which: there is a unique Nash equilibrium yielding ties;
and there are two Nash equilibria with different winners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07235</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07235</id><created>2015-04-27</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Sign Stable Random Projections for Large-Scale Learning</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of &quot;sign $\alpha$-stable random projections&quot; (where
$0&lt;\alpha\leq 2$) for building basic data processing tools in the context of
large-scale machine learning applications (e.g., classification, regression,
clustering, and near-neighbor search). After the processing by sign stable
random projections, the inner products of the processed data approximate
various types of nonlinear kernels depending on the value of $\alpha$. Thus,
this approach provides an effective strategy for approximating nonlinear
learning algorithms essentially at the cost of linear learning. When $\alpha
=2$, it is known that the corresponding nonlinear kernel is the arc-cosine
kernel. When $\alpha=1$, the procedure approximates the arc-cos-$\chi^2$ kernel
(under certain condition). When $\alpha\rightarrow0+$, it corresponds to the
resemblance kernel.
  From practitioners' perspective, the method of sign $\alpha$-stable random
projections is ready to be tested for large-scale learning applications, where
$\alpha$ can be simply viewed as a tuning parameter. What is missing in the
literature is an extensive empirical study to show the effectiveness of sign
stable random projections, especially for $\alpha\neq 2$ or 1. The paper
supplies such a study on a wide variety of classification datasets. In
particular, we compare shoulder-by-shoulder sign stable random projections with
the recently proposed &quot;0-bit consistent weighted sampling (CWS)&quot; (Li 2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07251</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07251</id><created>2015-04-27</created><updated>2015-09-23</updated><authors><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Universal recovery map for approximate Markov chains</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v3: 31 pages, 1 figure, application to topological order of quantum
  systems added (Section 3). v2: 29 pages, relation to [Wilde,
  arXiv:1505.04661] clarified (Remark 2.5)</comments><journal-ref>Proceedings of the Royal Society A, vol. 472, no. 2186, 2016</journal-ref><doi>10.1098/rspa.2015.0623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central question in quantum information theory is to determine how well
lost information can be reconstructed. Crucially, the corresponding recovery
operation should perform well without knowing the information to be
reconstructed. In this work, we show that the quantum conditional mutual
information measures the performance of such recovery operations. More
precisely, we prove that the conditional mutual information $I(A:C|B)$ of a
tripartite quantum state $\rho_{ABC}$ can be bounded from below by its distance
to the closest recovered state $\mathcal{R}_{B \to BC}(\rho_{AB})$, where the
$C$-part is reconstructed from the $B$-part only and the recovery map
$\mathcal{R}_{B \to BC}$ merely depends on $\rho_{BC}$. One particular
application of this result implies the equivalence between two different
approaches to define topological order in quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07259</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07259</id><created>2015-04-27</created><authors><author><keyname>Benninghoff</keyname><forenames>Heike</forenames></author><author><keyname>Garcke</keyname><forenames>Harald</forenames></author></authors><title>Image Segmentation and Restoration Using Parametric Contours With Free
  Endpoints</title><categories>cs.CV math.AP math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel approach for active contours with free
endpoints. A scheme is presented for image segmentation and restoration based
on a discrete version of the Mumford-Shah functional where the contours can be
both closed and open curves. Additional to a flow of the curves in normal
direction, evolution laws for the tangential flow of the endpoints are derived.
Using a parametric approach to describe the evolving contours together with an
edge-preserving denoising, we obtain a fast method for image segmentation and
restoration. The analytical and numerical schemes are presented followed by
numerical experiments with artificial test images and with a real medical
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07269</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07269</id><created>2015-04-27</created><authors><author><keyname>Reddy</keyname><forenames>N. Dinesh</forenames></author><author><keyname>Singhal</keyname><forenames>Prateek</forenames></author><author><keyname>Chari</keyname><forenames>Visesh</forenames></author><author><keyname>Krishna</keyname><forenames>K. Madhava</forenames></author></authors><title>Dynamic Body VSLAM with Semantic Constraints</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image based reconstruction of urban environments is a challenging problem
that deals with optimization of large number of variables, and has several
sources of errors like the presence of dynamic objects. Since most large scale
approaches make the assumption of observing static scenes, dynamic objects are
relegated to the noise modeling section of such systems. This is an approach of
convenience since the RANSAC based framework used to compute most multiview
geometric quantities for static scenes naturally confine dynamic objects to the
class of outlier measurements. However, reconstructing dynamic objects along
with the static environment helps us get a complete picture of an urban
environment. Such understanding can then be used for important robotic tasks
like path planning for autonomous navigation, obstacle tracking and avoidance,
and other areas. In this paper, we propose a system for robust SLAM that works
in both static and dynamic environments. To overcome the challenge of dynamic
objects in the scene, we propose a new model to incorporate semantic
constraints into the reconstruction algorithm. While some of these constraints
are based on multi-layered dense CRFs trained over appearance as well as motion
cues, other proposed constraints can be expressed as additional terms in the
bundle adjustment optimization process that does iterative refinement of 3D
structure and camera / object motion trajectories. We show results on the
challenging KITTI urban dataset for accuracy of motion segmentation and
reconstruction of the trajectory and shape of moving objects relative to ground
truth. We are able to show average relative error reduction by a significant
amount for moving object trajectory reconstruction relative to state-of-the-art
methods like VISO 2, as well as standard bundle adjustment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07272</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07272</id><created>2015-04-27</created><authors><author><keyname>Kot&#x142;owski</keyname><forenames>Wojciech</forenames></author><author><keyname>Dembczy&#x144;ski</keyname><forenames>Krzysztof</forenames></author></authors><title>Surrogate regret bounds for generalized classification performance
  metrics</title><categories>cs.LG</categories><comments>17 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider optimization of generalized performance metrics for binary
classification by means of surrogate loss. We focus on a class of metrics,
which are linear-fractional functions of the false positive and false negative
rates (examples of which include $F_{\beta}$-measure, Jaccard similarity
coefficient, AM measure, and many others). Our analysis concerns the following
two-step procedure. First, a real-valued function $f$ is learned by minimizing
a surrogate loss for binary classification on the training sample. It is
assumed that the surrogate loss is a strongly proper composite loss function
(examples of which include logistic loss, squared-error loss, exponential loss,
etc.). Then, given $f$, a threshold $\hat{\theta}$ is tuned on a separate
validation sample, by direct optimization of the target performance measure. We
show that the regret of the resulting classifier (obtained from thresholding
$f$ on $\hat{\theta}$) measured with respect to the target metric is
upperbounded by the regret of $f$ measured with respect to the surrogate loss.
Our finding is further analyzed in a computational study on both synthetic and
real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07278</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07278</id><created>2015-04-27</created><authors><author><keyname>Arora</keyname><forenames>Vipul</forenames></author><author><keyname>Behera</keyname><forenames>Laxmidhar</forenames></author><author><keyname>Yadav</keyname><forenames>Ajay Pratap</forenames></author></authors><title>Optimal Convergence Rate in Feed Forward Neural Networks using HJB
  Equation</title><categories>cs.NE</categories><comments>9 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A control theoretic approach is presented in this paper for both batch and
instantaneous updates of weights in feed-forward neural networks. The popular
Hamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimal
weight update law. The remarkable contribution in this paper is that closed
form solutions for both optimal cost and weight update can be achieved for any
feed-forward network using HJB equation in a simple yet elegant manner. The
proposed approach has been compared with some of the existing best performing
learning algorithms. It is found as expected that the proposed approach is
faster in convergence in terms of computational time. Some of the benchmark
test data such as 8-bit parity, breast cancer and credit approval, as well as
2D Gabor function have been used to validate our claims. The paper also
discusses issues related to global optimization. The limitations of popular
deterministic weight update laws are critiqued and the possibility of global
optimization using HJB formulation is discussed. It is hoped that the proposed
algorithm will bring in a lot of interest in researchers working in developing
fast learning algorithms and global optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07281</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07281</id><created>2015-04-27</created><updated>2015-04-29</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>The DIR Net: A Distributed System for Detection, Isolation, and Recovery</title><categories>cs.DC</categories><comments>This is a revision of Technical Report ESAT/ACCA/1998/1, Katholieke
  Universiteit Leuven, 1998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes the DIR net, a distributed environment which is part
of the EFTOS fault tolerance framework. The DIR net is a system consisting of
two components, called DIR Manager (or, shortly, the manager) and DIR Backup
Agent (shortly, the backup). One manager and a set of backups is located in the
system to be `guarded', one component per node. At this point the DIR net
weaves a web which substantially does two things: 1) makes itself tolerant to a
number of possible faults, and 2) gathers information pertaining the run of the
user application. As soon as an error occurs within the DIR net, the system
executes built-in recovery actions that allow itself to continue processing
despite a number of hardware/software faults, possibly doing a graceful
degradation of its features; when an error occurs in the user application, the
DIR net, by means of custom- and user-defined detection tools, is informed of
such events and runs one or more recovery strategies, both built-in and coded
by the user using an ancillary compile-time tool, the rl translator. Such tools
translates the user-defined strategies into a binary `R-code', i.e., a
pseudo-code interpreted by a special component of the DIR net, the Recovery
Interpreter, rint (in a sense, rint is a r-code virtual machine.) This document
describes the generic component of the DIR net, a function which can behave
either as manager or as backup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07283</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07283</id><created>2015-04-27</created><authors><author><keyname>Sandholm</keyname><forenames>Thomas</forenames></author><author><keyname>Ward</keyname><forenames>Julie</forenames></author><author><keyname>Balestrieri</keyname><forenames>Filippo</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>QoS-Based Pricing and Scheduling of Batch Jobs in OpenStack Clouds</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current Cloud infrastructure services (IaaS) market employs a
resource-based selling model: customers rent nodes from the provider and pay
per-node per-unit-time. This selling model places the burden upon customers to
predict their job resource requirements and durations. Inaccurate prediction by
customers can result in over-provisioning of resources, or under-provisioning
and poor job performance. Thanks to improved resource virtualization and
multi-tenant performance isolation, as well as common frameworks for batch
jobs, such as MapReduce, Cloud providers can predict job completion times more
accurately. We offer a new definition of QoS-levels in terms of job completion
times and we present a new QoS-based selling mechanism for batch jobs in a
multi-tenant OpenStack cluster. Our experiments show that the QoS-based
solution yields up to 40% improvement over the revenue of more standard selling
mechanisms based on a fixed per-node price across various demand and supply
conditions in a 240-VCPU OpenStack cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07284</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07284</id><created>2015-04-27</created><authors><author><keyname>Bansal</keyname><forenames>Aayush</forenames></author><author><keyname>Shrivastava</keyname><forenames>Abhinav</forenames></author><author><keyname>Doersch</keyname><forenames>Carl</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Mid-level Elements for Object Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on the success of recent discriminative mid-level elements, we
propose a surprisingly simple approach for object detection which performs
comparable to the current state-of-the-art approaches on PASCAL VOC comp-3
detection challenge (no external data). Through extensive experiments and
ablation analysis, we show how our approach effectively improves upon the
HOG-based pipelines by adding an intermediate mid-level representation for the
task of object detection. This representation is easily interpretable and
allows us to visualize what our object detector &quot;sees&quot;. We also discuss the
insights our approach shares with CNN-based methods, such as sharing
representation between categories helps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07295</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07295</id><created>2015-04-27</created><updated>2015-07-24</updated><authors><author><keyname>Taddy</keyname><forenames>Matt</forenames></author></authors><title>Document Classification by Inversion of Distributed Language
  Representations</title><categories>cs.CL cs.IR stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been many recent advances in the structure and measurement of
distributed language models: those that map from words to a vector-space that
is rich in information about word choice and composition. This vector-space is
the distributed language representation. The goal of this note is to point out
that any distributed representation can be turned into a classifier through
inversion via Bayes rule. The approach is simple and modular, in that it will
work with any language representation whose training can be formulated as
optimizing a probability model. In our application to 2 million sentences from
Yelp reviews, we also find that it performs as well as or better than complex
purpose-built algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07298</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07298</id><created>2015-04-27</created><updated>2015-06-27</updated><authors><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>Adaptive Computation of the Swap-Insert Correction Distance</title><categories>cs.DS</categories><comments>16 pages, no figures, long version of the extended abstract accepted
  to SPIRE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Swap-Insert Correction distance from a string $S$ of length $n$ to
another string $L$ of length $m\geq n$ on the alphabet $[1..d]$ is the minimum
number of insertions, and swaps of pairs of adjacent symbols, converting $S$
into $L$. Contrarily to other correction distances, computing it is NP-Hard in
the size $d$ of the alphabet. We describe an algorithm computing this distance
in time within $O(d^2 nm g^{d-1})$, where there are $n_\alpha$ occurrences of
$\alpha$ in $S$, $m_\alpha$ occurrences of $\alpha$ in $L$, and where
$g=\max_{\alpha\in[1..d]} \min\{n_\alpha,m_\alpha-n_\alpha\}$ measures the
difficulty of the instance. The difficulty $g$ is bounded by above by various
terms, such as the length of the shortest string $S$, and by the maximum number
of occurrences of a single character in $S$. Those results illustrate how, in
many cases, the correction distance between two strings can be easier to
compute than in the worst case scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07300</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07300</id><created>2015-04-27</created><updated>2015-04-29</updated><authors><author><keyname>Nazari</keyname><forenames>Sam</forenames></author></authors><title>The Unknown Input Observer and its Advantages with Examples</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This brief memo reviews the theory of Unknown Input Observers (UIO) for state
estimation in systems subject to disturbance inputs that are not known a
priori. One main advantage of the UIO is that the observer structure naturally
decouples the plant inputs from the state estimation process. This advantage is
highlighted in three example dynamic systems commonly found in control theory.
It is shown that the estimation error for all three systems asymptotically
approaches zero by application of the UIO under the influence of dynamic
disturbances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07302</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07302</id><created>2015-04-27</created><updated>2015-07-31</updated><authors><author><keyname>Sun</keyname><forenames>Yuyin</forenames></author><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Fox</keyname><forenames>Dieter</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Building Hierarchies of Concepts via Crowdsourcing</title><categories>cs.AI</categories><comments>12 pages, 8 pages of main paper, 4 pages of appendix, IJCAI2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchies of concepts are useful in many applications from navigation to
organization of objects. Usually, a hierarchy is created in a centralized
manner by employing a group of domain experts, a time-consuming and expensive
process. The experts often design one single hierarchy to best explain the
semantic relationships among the concepts, and ignore the natural uncertainty
that may exist in the process. In this paper, we propose a crowdsourcing system
to build a hierarchy and furthermore capture the underlying uncertainty. Our
system maintains a distribution over possible hierarchies and actively selects
questions to ask using an information gain criterion. We evaluate our
methodology on simulated data and on a set of real world application domains.
Experimental results show that our system is robust to noise, efficient in
picking questions, cost-effective and builds high quality hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07308</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07308</id><created>2015-04-27</created><authors><author><keyname>Chen</keyname><forenames>Niangjun</forenames></author><author><keyname>Ren</keyname><forenames>Xiaoqi</forenames></author><author><keyname>Ren</keyname><forenames>Shaolei</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Greening Multi-Tenant Data Center Demand Response</title><categories>cs.GT</categories><comments>34 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data centers have emerged as promising resources for demand response,
particularly for emergency demand response (EDR), which saves the power grid
from incurring blackouts during emergency situations. However, currently, data
centers typically participate in EDR by turning on backup (diesel) generators,
which is both expensive and environmentally unfriendly. In this paper, we focus
on &quot;greening&quot; demand response in multi-tenant data centers, i.e., colocation
data centers, by designing a pricing mechanism through which the data center
operator can efficiently extract load reductions from tenants during emergency
periods to fulfill energy reduction requirement for EDR. In particular, we
propose a pricing mechanism for both mandatory and voluntary EDR programs,
ColoEDR, that is based on parameterized supply function bidding and provides
provably near-optimal efficiency guarantees, both when tenants are price-taking
and when they are price-anticipating. In addition to analytic results, we
extend the literature on supply function mechanism design, and evaluate ColoEDR
using trace-based simulation studies. These validate the efficiency analysis
and conclude that the pricing mechanism is both beneficial to the environment
and to the data center operator (by decreasing the need for backup diesel
generation), while also aiding tenants (by providing payments for load
reductions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07313</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07313</id><created>2015-04-27</created><authors><author><keyname>Aranki</keyname><forenames>Daniel</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Private Disclosure of Information in Health Tele-monitoring</title><categories>cs.CR cs.AI cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework, called Private Disclosure of Information (PDI),
which is aimed to prevent an adversary from inferring certain sensitive
information about subjects using the data that they disclosed during
communication with an intended recipient. We show cases where it is possible to
achieve perfect privacy regardless of the adversary's auxiliary knowledge while
preserving full utility of the information to the intended recipient and
provide sufficient conditions for such cases. We also demonstrate the
applicability of PDI on a real-world data set that simulates a health
tele-monitoring scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07324</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07324</id><created>2015-04-27</created><authors><author><keyname>Li</keyname><forenames>Piji</forenames></author><author><keyname>Bing</keyname><forenames>Lidong</forenames></author><author><keyname>Lam</keyname><forenames>Wai</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Liao</keyname><forenames>Yi</forenames></author></authors><title>Reader-Aware Multi-Document Summarization via Sparse Coding</title><categories>cs.CL cs.AI</categories><comments>7 pages, 2 figures, accepted as a full paper at IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new MDS paradigm called reader-aware multi-document
summarization (RA-MDS). Specifically, a set of reader comments associated with
the news reports are also collected. The generated summaries from the reports
for the event should be salient according to not only the reports but also the
reader comments. To tackle this RA-MDS problem, we propose a
sparse-coding-based method that is able to calculate the salience of the text
units by jointly considering news reports and reader comments. Another
reader-aware characteristic of our framework is to improve linguistic quality
via entity rewriting. The rewriting consideration is jointly assessed together
with other summarization requirements under a unified optimization model. To
support the generation of compressive summaries via optimization, we explore a
finer syntactic unit, namely, noun/verb phrase. In this work, we also generate
a data set for conducting RA-MDS. Extensive experiments on this data set and
some classical data sets demonstrate the effectiveness of our proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07325</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07325</id><created>2015-04-27</created><authors><author><keyname>Sandholm</keyname><forenames>Thomas</forenames></author><author><keyname>Lee</keyname><forenames>Dongman</forenames></author></authors><title>Notes on Cloud computing principles</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter provides a review of fundamental distributed systems and economic
Cloud computing principles. These principles are frequently deployed in their
respective fields, but their inter-dependencies are often neglected. Given that
Cloud Computing first and foremost is a new business model, a new model to sell
computational resources, the understanding of these concepts is facilitated by
treating them in unison. Here, we review some of the most important concepts
and how they relate to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07327</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07327</id><created>2015-04-27</created><authors><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author><author><keyname>Pouladi</keyname><forenames>Farhad</forenames></author><author><keyname>Talebi</keyname><forenames>Siamak</forenames></author></authors><title>Toward Smart Power Grids: Communication Network Design for Power Grids
  Synchronization</title><categories>cs.NE</categories><comments>This paper has been presented at the 27th International Power System
  Conference in 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In smart power grids, keeping the synchronicity of generators and the
corresponding controls is of great importance. To do so, a simple model is
employed in terms of swing equation to represent the interactions among
dynamics of generators and feedback control. In case of having a communication
network available, the control can be done based on the transmitted
measurements by the communication network. The stability of system is denoted
by the largest eigenvalue of the weighted sum of the Laplacian matrices of the
communication infrastructure and power network. In this work, we use graph
theory to model the communication network as a graph problem. Then, Ant Colony
System (ACS) is employed for optimum design of above graph for synchronization
of power grids. Performance evaluation of the proposed method for the 39-bus
New England power system versus methods such as exhaustive search and Rayleigh
quotient approximation indicates feasibility and effectiveness of our method
for even large scale smart power grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07329</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07329</id><created>2015-04-27</created><authors><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author><author><keyname>Nezamabadi-pour</keyname><forenames>Hossein</forenames></author><author><keyname>Saryazdi</keyname><forenames>Saeid</forenames></author><author><keyname>Farrahi-Moghaddam</keyname><forenames>Fereydoun</forenames></author></authors><title>Combined A*-Ants Algorithm: A New Multi-Parameter Vehicle Navigation
  Scheme</title><categories>cs.NE</categories><comments>This paper has been presented at the 16th Iranian Conference on
  Electrical Engineering in 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a multi-parameter A*(A- star)-ants based algorithm is proposed
in order to find the best optimized multi-parameter path between two desired
points in regions. This algorithm recognizes paths, according to user desired
parameters using electronic maps. The proposed algorithm is a combination of A*
and ants algorithm in which the proposed A* algorithm is the prologue to the
suggested ant based algorithm .In fact, this A* algorithm invigorates some
paths pheromones in ants algorithm. As one of implementations of this method,
this algorithm was applied on a part of Kerman city, Iran as a multi-parameter
vehicle navigator. It finds the best optimized multi-parameter direction
between two desired junctions based on city traveler parameters. Comparison
results between the proposed method and ants algorithm demonstrates efficiency
and lower cost function results of the proposed method versus ants algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07339</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07339</id><created>2015-04-27</created><updated>2015-09-24</updated><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Yan</keyname><forenames>Junjie</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Convolutional Channel Features</title><categories>cs.CV</categories><comments>9 pages, 5 figures, 6 tables; ICCV 2015 camera-ready version</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep learning methods are powerful tools but often suffer from expensive
computation and limited flexibility. An alternative is to combine light-weight
models with deep representations. As successful cases exist in several visual
problems, a unified framework is absent. In this paper, we revisit two widely
used approaches in computer vision, namely filtered channel features and
Convolutional Neural Networks (CNN), and absorb merits from both by proposing
an integrated method called Convolutional Channel Features (CCF). CCF transfers
low-level features from pre-trained CNN models to feed the boosting forest
model. With the combination of CNN features and boosting forest, CCF benefits
from the richer capacity in feature representation compared with channel
features, as well as lower cost in computation and storage compared with
end-to-end CNN methods. We show that CCF serves as a good way of tailoring
pre-trained CNN models to diverse tasks without fine-tuning the whole network
to each task by achieving state-of-the-art performances in pedestrian
detection, face detection, edge detection and object proposal generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07342</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07342</id><created>2015-04-28</created><authors><author><keyname>Liu</keyname><forenames>Xinyun</forenames></author><author><keyname>Zhu</keyname><forenames>Jiandong</forenames></author></authors><title>On Potential Equations of Finite Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, some new criteria for detecting whether a finite game is
potential are proposed by solving potential equations. The verification
equations with the minimal number for checking a potential game are obtained
for the first time. Some connections between the potential equations and the
existing characterizations of potential games are established. It is revealed
that a finite game is potential if and only if its every bi-matrix sub-game is
potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07350</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07350</id><created>2015-04-28</created><authors><author><keyname>Ruocco</keyname><forenames>Massimiliano</forenames></author><author><keyname>Ramampiaro</keyname><forenames>Heri</forenames></author></authors><title>Geo-Temporal Distribution of Tag Terms for Event-Related Image Retrieval</title><categories>cs.IR</categories><journal-ref>Information Processing &amp; Management Journal (IPM), 51(1), pp.
  92-110. 2015</journal-ref><doi>10.1016/j.ipm.2014.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Media sharing applications, such as Flickr and Panoramio, contain a large
amount of pictures related to real life events. For this reason, the
development of effective methods to retrieve these pictures is important, but
still a challenging task. Recognizing this importance, and to improve the
retrieval effectiveness of tag-based event retrieval systems, we propose a new
method to extract a set of geographical tag features from raw geo-spatial
profiles of user tags. The main idea is to use these features to select the
best expansion terms in a machine learning-based query expansion approach.
Specifically, we apply rigorous statistical exploratory analysis of spatial
point patterns to extract the geo-spatial features. We use the features both to
summarize the spatial characteristics of the spatial distribution of a single
term, and to determine the similarity between the spatial profiles of two terms
-- i.e., term-to-term spatial similarity. To further improve our approach, we
investigate the effect of combining our geo-spatial features with temporal
features on choosing the expansion terms. To evaluate our method, we perform
several experiments, including well-known feature analyses. Such analyses show
how much our proposed geo-spatial features contribute to improve the overall
retrieval performance. The results from our experiments demonstrate the
effectiveness and viability of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07361</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07361</id><created>2015-04-28</created><authors><author><keyname>Dehghani</keyname><forenames>Nazanin</forenames></author><author><keyname>Asadpour</keyname><forenames>Masoud</forenames></author></authors><title>Graph-based Method for Summarized Storyline Generation in Twitter</title><categories>cs.SI cs.IR</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter has become a leading source of real-time world-wide information and a
great medium for exploring emerging events, breaking news and general topics
which most matter to a broad audience. On the other hand, the explosive rate of
incoming information in Twitter leads users to experience information overload.
Whereas, a significant fraction of tweets are about news events, summarizing
the storyline of events can be helpful for users to easily access to the
relevant and key information hidden among tweets and thereby draw high level
conclusions. Storytelling is the task of providing chronological summaries of
significant sub-events development and sketching the relationship between
sub-events. In this paper, we propose a novel framework to generate a
summarized storyline of news events from social point of view. Utilizing the
concepts in graph-theory, we identify sub-events, summarize the evolution of
sub-events and generate a coherent storyline of them. Our approach models a
storyline as a directed tree of social salient sub-events evolving over time.
To overcome the enormous number of redundant tweets, we keep distilled
information in super-tweets. Experiments performed on a large scale data set
from tweets sent during the Iranian Presidential Election (#IranElection) and
the results demonstrate the efficiency and effectiveness of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07365</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07365</id><created>2015-04-28</created><authors><author><keyname>Schreck</keyname><forenames>Jan</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Compressive Rate Estimation with Applications to Device-to-Device
  Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework that we call compressive rate estimation. We assume
that the composite channel gain matrix (i.e. the matrix of all channel gains
between all network nodes) is compressible which means it can be approximated
by a sparse or low rank representation. We develop and study a novel sensing
and reconstruction protocol for the estimation of achievable rates. We develop
a sensing protocol that exploits the superposition principle of the wireless
channel and enables the receiving nodes to obtain non-adaptive random
measurements of columns of the composite channel matrix. The random
measurements are fed back to a central controller that decodes the composite
channel gain matrix (or parts of it) and estimates individual user rates. We
analyze the rate loss for a linear and a non-linear decoder and find the
scaling laws according to the number of non-adaptive measurements. In
particular if we consider a system with $N$ nodes and assume that each column
of the composite channel matrix is $k$ sparse, our findings can be summarized
as follows. For a certain class of non-linear decoders we show that if the
number of pilot signals $M$ scales like $M \sim k \log(N/k)$, then the rate
loss compared to perfect channel state information remains bounded. For a
certain class of linear decoders we show that the rate loss compared to perfect
channel state information scales like $1/\sqrt{M}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07372</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07372</id><created>2015-04-28</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Time-Frequency Trade-offs for Audio Source Separation with Binary Masks</title><categories>cs.SD</categories><msc-class>65Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The short-time Fourier transform (STFT) provides the foundation of
binary-mask based audio source separation approaches. In computing a
spectrogram, the STFT window size parameterizes the trade-off between time and
frequency resolution. However, it is not yet known how this parameter affects
the operation of the binary mask in terms of separation quality for real-world
signals such as speech or music. Here, we demonstrate that the trade-off
between time and frequency in the STFT, used to perform ideal binary mask
separation, depends upon the types of source that are to be separated. In
particular, we demonstrate that different window sizes are optimal for
separating different combinations of speech and musical signals. Our findings
have broad implications for machine audition and machine learning in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07377</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07377</id><created>2015-04-28</created><authors><author><keyname>Leone</keyname><forenames>Pierre</forenames></author><author><keyname>Samarasinghe</keyname><forenames>Kasun</forenames></author></authors><title>Succint greedy routing without metric on planar triangulations</title><categories>cs.NI cs.CG</categories><comments>12 pages</comments><msc-class>05C10</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic routing is an appealing routing strategy that uses the location
information of the nodes to route the data. This technique uses only local
information of the communication graph topology and does not require
computational effort to build routing table or equivalent data structures. A
particularly effi?cient implementation of this paradigm is greedy routing,
where along the data path the nodes forward the data to a neighboring node that
is closer to the destination. The decreasing distance to the destination
implies the success of the routing scheme. A related problem is to consider an
abstract graph and decide whether there exists an embedding of the graph in a
metric space, called a greedy embedding, such that greedy routing guarantees
the delivery of the data. In the present paper, we use a metric-free de?nition
of greedy path and we show that greedy routing is successful on planar
triangulations without considering the existence of greedy embedding. Our
algorithm rely entirely on the combinatorial description of the graph structure
and the coordinate system requires O(log(n)) bits where n is the number of
nodes in the graph. Previous works on greedy routing make use of the embedding
to route the data. In particular, in our framework, it is known that there
exists an embedding of planar triangulations such that greedy routing
guarantees the delivery of data. The result presented in this article leads to
the question whether the success of (any) greedy routing strategy is always
coupled with the existence of a greedy embedding?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07379</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07379</id><created>2015-04-28</created><authors><author><keyname>Brandes</keyname><forenames>Ulrik</forenames></author><author><keyname>Hamann</keyname><forenames>Michael</forenames></author><author><keyname>Strasser</keyname><forenames>Ben</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Fast Quasi-Threshold Editing</title><categories>cs.DS cs.SI physics.soc-ph</categories><comments>26 pages, 4 figures, submitted to ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Quasi-Threshold Mover (QTM), an algorithm to solve the
quasi-threshold (also called trivially perfect) graph editing problem with edge
insertion and deletion. Given a graph it computes a quasi-threshold graph which
is close in terms of edit count. This edit problem is NP-hard. We present an
extensive experimental study, in which we show that QTM is the first algorithm
that is able to scale to large real-world graphs in practice. As a side result
we further present a simple linear-time algorithm for the quasi-threshold
recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07384</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07384</id><created>2015-04-28</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Pavlogiannis</keyname><forenames>Andreas</forenames></author></authors><title>Faster Algorithms for Quantitative Verification in Constant Treewidth
  Graphs</title><categories>cs.DS</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the core algorithmic problems related to verification of systems
with respect to three classical quantitative properties, namely, the
mean-payoff property, the ratio property, and the minimum initial credit for
energy property. The algorithmic problem given a graph and a quantitative
property asks to compute the optimal value (the infimum value over all traces)
from every node of the graph. We consider graphs with constant treewidth, and
it is well-known that the control-flow graphs of most programs have constant
treewidth. Let $n$ denote the number of nodes of a graph, $m$ the number of
edges (for constant treewidth graphs $m=O(n)$) and $W$ the largest absolute
value of the weights. Our main theoretical results are as follows. First, for
constant treewidth graphs we present an algorithm that approximates the
mean-payoff value within a multiplicative factor of $\epsilon$ in time $O(n
\cdot \log (n/\epsilon))$ and linear space, as compared to the classical
algorithms that require quadratic time. Second, for the ratio property we
present an algorithm that for constant treewidth graphs works in time $O(n
\cdot \log (|a\cdot b|))=O(n\cdot\log (n\cdot W))$, when the output is
$\frac{a}{b}$, as compared to the previously best known algorithm with running
time $O(n^2 \cdot \log (n\cdot W))$. Third, for the minimum initial credit
problem we show that (i) for general graphs the problem can be solved in
$O(n^2\cdot m)$ time and the associated decision problem can be solved in
$O(n\cdot m)$ time, improving the previous known $O(n^3\cdot m\cdot \log
(n\cdot W))$ and $O(n^2 \cdot m)$ bounds, respectively; and (ii) for constant
treewidth graphs we present an algorithm that requires $O(n\cdot \log n)$ time,
improving the previous known $O(n^4 \cdot \log (n \cdot W))$ bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07385</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07385</id><created>2015-04-28</created><authors><author><keyname>Mamei</keyname><forenames>Marco</forenames></author><author><keyname>Colonna</keyname><forenames>Massimo</forenames></author></authors><title>Estimating Attendance From Cellular Network Data</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a methodology to estimate the number of attendees to events
happening in the city from cellular network data. In this work we used
anonymized Call Detail Records (CDRs) comprising data on where and when users
access the cellular network. Our approach is based on two key ideas: (1) we
identify the network cells associated to the event location. (2) We verify the
attendance of each user, as a measure of whether (s)he generates CDRs during
the event, but not during other times. We evaluate our approach to estimate the
number of attendees to a number of events ranging from football matches in
stadiums to concerts and festivals in open squares. Comparing our results with
the best groundtruth data available, our estimates provide a median error of
less than 15% of the actual number of attendees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07389</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07389</id><created>2015-04-28</created><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>De Smet</keyname><forenames>Frank</forenames></author><author><keyname>Gillard</keyname><forenames>Pieter</forenames></author><author><keyname>Mathieu</keyname><forenames>Chantal</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Building Classifiers to Predict the Start of Glucose-Lowering
  Pharmacotherapy Using Belgian Health Expenditure Data</title><categories>stat.ML cs.IR</categories><comments>23 pages, 5 figures, submitted to JMLR special issue on Learning from
  Electronic Health Data</comments><acm-class>I.5.4; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early diagnosis is important for type 2 diabetes (T2D) to improve patient
prognosis, prevent complications and reduce long-term treatment costs. We
present a novel risk profiling approach based exclusively on health expenditure
data that is available to Belgian mutual health insurers. We used expenditure
data related to drug purchases and medical provisions to construct models that
predict whether a patient will start glucose-lowering pharmacotherapy in the
coming years, based on that patient's recent medical expenditure history. The
design and implementation of the modeling strategy are discussed in detail and
several learning methods are benchmarked for our application. Our best
performing model obtains between 74.9% and 76.8% area under the ROC curve,
which is comparable to state-of-the-art risk prediction approaches for T2D
based on questionnaires. In contrast to other methods, our approach can be
implemented on a population-wide scale at virtually no extra operational cost.
Possibly, our approach can be further improved by additional information about
some risk factors of T2D that is unavailable in health expenditure data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07395</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07395</id><created>2015-04-28</created><authors><author><keyname>Ha</keyname><forenames>Thanh-Le</forenames></author><author><keyname>Niehues</keyname><forenames>Jan</forenames></author><author><keyname>Waibel</keyname><forenames>Alex</forenames></author></authors><title>Lexical Translation Model Using a Deep Neural Network Architecture</title><categories>cs.CL cs.LG cs.NE</categories><journal-ref>Proceedings of the 11th International Workshop on Spoken Language
  Translation (IWSLT 2014), page 223-229, Lake Tahoe - US, December 4th and
  5th, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we combine the advantages of a model using global source
sentence contexts, the Discriminative Word Lexicon, and neural networks. By
using deep neural networks instead of the linear maximum entropy model in the
Discriminative Word Lexicon models, we are able to leverage dependencies
between different source words due to the non-linearity. Furthermore, the
models for different target words can share parameters and therefore data
sparsity problems are effectively reduced.
  By using this approach in a state-of-the-art translation system, we can
improve the performance by up to 0.5 BLEU points for three different language
pairs on the TED translation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07406</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07406</id><created>2015-04-28</created><authors><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author><author><keyname>Loptev</keyname><forenames>Alexander</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>On Maximal Unbordered Factors</title><categories>cs.DS</categories><comments>Accepted to the 26th Annual Symposium on Combinatorial Pattern
  Matching (CPM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a string $S$ of length $n$, its maximal unbordered factor is the
longest factor which does not have a border. In this work we investigate the
relationship between $n$ and the length of the maximal unbordered factor of
$S$. We prove that for the alphabet of size $\sigma \ge 5$ the expected length
of the maximal unbordered factor of a string of length~$n$ is at least $0.99 n$
(for sufficiently large values of $n$). As an application of this result, we
propose a new algorithm for computing the maximal unbordered factor of a
string.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07416</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07416</id><created>2015-04-28</created><authors><author><keyname>Filimonov</keyname><forenames>A. V.</forenames></author><author><keyname>Osipov</keyname><forenames>A. V.</forenames></author><author><keyname>Klimov</keyname><forenames>A. B.</forenames></author></authors><title>Application of neural networks to identify trolls in social networks</title><categories>cs.SI</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we developed and tested a new algorithm of detecting in social
networks users (so-called trolls) who behave in an insulting and provocative
way towards other users. In order to detect trolls it is proposed to unite
users in groups where all the members have a similar way of communicating.
Defining the number of group and distributing the users into these groups is
carried out automatically due to application of neural networks of special type
- Kohonens self-organized maps. As for users characteristics according to which
the distribution into groups might be done we suggest using such data as the
number of comments, the average comment length and indicators determining the
emotional state of the user (the frequency of encountering certain characters
in comments).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07426</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07426</id><created>2015-04-28</created><authors><author><keyname>Anjum</keyname><forenames>Muhammad Ali Raza</forenames></author></authors><title>A New Approach to Linear Estimation Problem in Multi-user Massive MIMO
  Systems</title><categories>cs.SY cs.IT math.IT</categories><journal-ref>TELKOMNIKA Indonesian Journal of Electrical Engineering 13, no. 2
  (2015)</journal-ref><doi>10.11591/telkomnika.v13i2.7003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach for solving linear estimation problem in multi-user massive
MIMO systems is proposed. In this approach, the difficulty of matrix inversion
is attributed to the incomplete definition of the dot product. The general
definition of dot product implies that the columns of channel matrix are always
orthogonal whereas, in practice, they may be not. If the latter information can
be incorporated into dot product, then the unknowns can be directly computed
from projections without inverting the channel matrix. By doing so, the
proposed method is able to achieve an exact solution with a 25% reduction in
computational complexity as compared to the QR method. Proposed method is
stable, offers an extra flexibility of computing any single unknown, and can be
implemented in just twelve lines of code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07442</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07442</id><created>2015-04-28</created><authors><author><keyname>Velez</keyname><forenames>Gorka</forenames></author><author><keyname>Otaegui</keyname><forenames>Oihana</forenames></author></authors><title>Embedded Platforms for Computer Vision-based Advanced Driver Assistance
  Systems: a Survey</title><categories>cs.CV</categories><comments>10 pages. To be published in ITS World Congress 2015</comments><acm-class>I.4.9; I.2.10; D.4.7; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Vision, either alone or combined with other technologies such as
radar or Lidar, is one of the key technologies used in Advanced Driver
Assistance Systems (ADAS). Its role understanding and analysing the driving
scene is of great importance as it can be noted by the number of ADAS
applications that use this technology. However, porting a vision algorithm to
an embedded automotive system is still very challenging, as there must be a
trade-off between several design requisites. Furthermore, there is not a
standard implementation platform, so different alternatives have been proposed
by both the scientific community and the industry. This paper aims to review
the requisites and the different embedded implementation platforms that can be
used for Computer Vision-based ADAS, with a critical analysis and an outlook to
future trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07443</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07443</id><created>2015-04-28</created><authors><author><keyname>Baget</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Bienvenu</keyname><forenames>Meghyn</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Rocher</keyname><forenames>Swan</forenames></author></authors><title>Combining Existential Rules and Transitivity: Next Steps</title><categories>cs.AI</categories><comments>This is an extended version, completed with full proofs, of an
  article appearing in IJCAI'15</comments><msc-class>68T30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider existential rules (aka Datalog+) as a formalism for specifying
ontologies. In recent years, many classes of existential rules have been
exhibited for which conjunctive query (CQ) entailment is decidable. However,
most of these classes cannot express transitivity of binary relations, a
frequently used modelling construct. In this paper, we address the issue of
whether transitivity can be safely combined with decidable classes of
existential rules.
  First, we prove that transitivity is incompatible with one of the simplest
decidable classes, namely aGRD (acyclic graph of rule dependencies), which
clarifies the landscape of `finite expansion sets' of rules.
  Second, we show that transitivity can be safely added to linear rules (a
subclass of guarded rules, which generalizes the description logic DL-Lite-R)
in the case of atomic CQs, and also for general CQs if we place a minor
syntactic restriction on the rule set. This is shown by means of a novel query
rewriting algorithm that is specially tailored to handle transitivity rules.
  Third, for the identified decidable cases, we pinpoint the combined and data
complexities of query entailment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07459</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07459</id><created>2015-04-28</created><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Guille</keyname><forenames>Adrien</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author></authors><title>CommentWatcher: An Open Source Web-based platform for analyzing
  discussions on web forums</title><categories>cs.CL cs.SI</categories><acm-class>H.3.5; I.2.7; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CommentWatcher, an open source tool aimed at analyzing discussions
on web forums. Constructed as a web platform, CommentWatcher features automatic
mass fetching of user posts from forum on multiple sites, extracting topics,
visualizing the topics as an expression cloud and exploring their temporal
evolution. The underlying social network of users is simultaneously constructed
using the citation relations between users and visualized as a graph structure.
Our platform addresses the issues of the diversity and dynamics of structures
of webpages hosting the forums by implementing a parser architecture that is
independent of the HTML structure of webpages. This allows easy on-the-fly
adding of new websites. Two types of users are targeted: end users who seek to
study the discussed topics and their temporal evolution, and researchers in
need of establishing a forum benchmark dataset and comparing the performances
of analysis tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07460</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07460</id><created>2015-04-28</created><authors><author><keyname>Kolesnikov</keyname><forenames>Alexander</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Identifying Reliable Annotations for Large Scale Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Challenging computer vision tasks, in particular semantic image segmentation,
require large training sets of annotated images. While obtaining the actual
images is often unproblematic, creating the necessary annotation is a tedious
and costly process. Therefore, one often has to work with unreliable annotation
sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic
techniques. In this work, we present a Gaussian process (GP) based technique
for simultaneously identifying which images of a training set have unreliable
annotation and learning a segmentation model in which the negative effect of
these images is suppressed. Alternatively, the model can also just be used to
identify the most reliably annotated images from the training set, which can
then be used for training any other segmentation method. By relying on &quot;deep
features&quot; in combination with a linear covariance function, our GP can be
learned and its hyperparameter determined efficiently using only matrix
operations and gradient-based optimization. This makes our method scalable even
to large datasets with several million training instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07469</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07469</id><created>2015-04-28</created><updated>2015-11-24</updated><authors><author><keyname>Poleg</keyname><forenames>Yair</forenames></author><author><keyname>Ephrat</keyname><forenames>Ariel</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author><author><keyname>Arora</keyname><forenames>Chetan</forenames></author></authors><title>Compact CNN for Indexing Egocentric Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While egocentric video is becoming increasingly popular, browsing it is very
difficult. In this paper we present a compact 3D Convolutional Neural Network
(CNN) architecture for long-term activity recognition in egocentric videos.
Recognizing long-term activities enables us to temporally segment (index) long
and unstructured egocentric videos. Existing methods for this task are based on
hand tuned features derived from visible objects, location of hands, as well as
optical flow.
  Given a sparse optical flow volume as input, our CNN classifies the camera
wearer's activity. We obtain classification accuracy of 89%, which outperforms
the current state-of-the-art by 19%. Additional evaluation is performed on an
extended egocentric video dataset, classifying twice the amount of categories
than current state-of-the-art. Furthermore, our CNN is able to recognize
whether a video is egocentric or not with 99.2% accuracy, up by 24% from
current state-of-the-art. To better understand what the network actually
learns, we propose a novel visualization of CNN kernels as flow fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07479</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07479</id><created>2015-04-28</created><updated>2015-04-29</updated><authors><author><keyname>Davis</keyname><forenames>Philip M.</forenames></author><author><keyname>Cochran</keyname><forenames>Angela</forenames></author></authors><title>Cited Half-Life of the Journal Literature</title><categories>cs.DL</categories><comments>Table 1 is replaced to fix a sorting error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing 13,455 journals listed in the Journal Citation Report (Thomson
Reuters) from 1997 through 2013, we report that the mean cited half-life of the
scholarly literature is 6.5 years and growing at a rate of 0.13 years per
annum. Focusing on a subset of journals (N=4,937) for which we have a
continuous series of half-life observations, 209 of 229 (91%) subject
categories experienced increasing cited half-lives. Contrary to the overall
trend, engineering and chemistry journals experienced declining cited
half-lives. Last, as journals attracted more citations, a larger proportion of
them were directed toward older papers. The trend to cite older papers is not
fully explained by technology (digital publishing, search and retrieval, etc.),
but may be the result of a structural shift to fund incremental and applied
research over fundamental science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07481</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07481</id><created>2015-04-28</created><authors><author><keyname>Caldwell</keyname><forenames>Blake</forenames></author></authors><title>Improving Block-level Efficiency with scsi-mq</title><categories>cs.OS cs.DC</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current generation solid-state storage devices are exposing a new bottlenecks
in the SCSI and block layers of the Linux kernel, where IO throughput is
limited by lock contention, inefficient interrupt handling, and poor memory
locality. To address these limitations, the Linux kernel block layer underwent
a major rewrite with the blk-mq project to move from a single request queue to
a multi-queue model. The Linux SCSI subsystem rework to make use of this new
model, known as scsi-mq, has been merged into the Linux kernel and work is
underway for dm-multipath support in the upcoming Linux 4.0 kernel. These
pieces were necessary to make use of the multi-queue block layer in a Lustre
parallel filesystem with high availability requirements. We undertook adding
support of the 3.18 kernel to Lustre with scsi-mq and dm-multipath patches to
evaluate the potential of these efficiency improvements. In this paper we
evaluate the block-level performance of scsi-mq with backing storage hardware
representative of a HPC-targerted Lustre filesystem. Our findings show that
SCSI write request latency is reduced by as much as 13.6%. Additionally, when
profiling the CPU usage of our prototype Lustre filesystem, we found that CPU
idle time increased by a factor of 7 with Linux 3.18 and blk-mq as compared to
a standard 2.6.32 Linux kernel. Our findings demonstrate increased efficiency
of the multi-queue block layer even with disk-based caching storage arrays used
in existing parallel filesystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07482</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07482</id><created>2015-04-28</created><updated>2015-10-16</updated><authors><author><keyname>Haunschild</keyname><forenames>Robin</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Networks of reader and country status: An analysis of Mendeley reader
  statistics</title><categories>cs.DL cs.SI</categories><comments>26 pages, 6 figures (also web-based startable), and 2 tables</comments><journal-ref>PeerJ CompSci, 32 (2015)</journal-ref><doi>10.7717/peerj-cs.32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of papers published in journals indexed by the Web of Science core
collection is steadily increasing. In recent years, nearly two million new
papers were published each year; somewhat more than one million papers when
primary research papers are considered only (articles and reviews are the
document types where primary research is usually reported or reviewed).
However, who reads these papers? More precisely, which groups of researchers
from which (self-assigned) scientific disciplines and countries are reading
these papers? Is it possible to visualize readership patterns for certain
countries, scientific disciplines, or academic status groups? One popular
method to answer these questions is a network analysis. In this study, we
analyze Mendeley readership data of a set of 1,133,224 articles and 64,960
reviews with publication year 2012 to generate three different kinds of
networks: (1) The network based on disciplinary affiliations of Mendeley
readers contains four groups: (i) biology, (ii) social science and humanities
(including relevant computer science), (iii) bio-medical sciences, and (iv)
natural science and engineering. In all four groups, the category with the
addition &quot;miscellaneous&quot; prevails. (2) The network of co-readers in terms of
professional status shows that a common interest in papers is mainly shared
among PhD students, Master's students, and postdocs. (3) The country network
focusses on global readership patterns: a group of 53 nations is identified as
core to the scientific enterprise, including Russia and China as well as two
thirds of the OECD (Organisation for Economic Co-operation and Development)
countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07487</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07487</id><created>2015-04-28</created><updated>2015-11-24</updated><authors><author><keyname>Kim</keyname><forenames>Hanjin</forenames></author><author><keyname>Lee</keyname><forenames>Hoon</forenames></author><author><keyname>Ahn</keyname><forenames>Minki</forenames></author><author><keyname>Kong</keyname><forenames>Han-Bae</forenames></author><author><keyname>Lee</keyname><forenames>Inkyu</forenames></author></authors><title>Joint Subcarrier and Power Allocation Methods in Wireless Powered
  Communication Network for OFDM systems</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate wireless powered communication network for OFDM
systems, where a hybrid access point (H-AP) broadcasts energy signals to users
in the downlink, and the users transmit information signals to the H-AP in the
uplink based on an orthogonal frequency division multiple access scheme. We
consider a fullduplex H-AP which simultaneously transmits energy signals and
receives information signals. In this scenario, we address a joint subcarrier
scheduling and power allocation problem to maximize the sum-rate under two
cases: perfect self-interference cancelation (SIC) where the H-AP fully
eliminates its self interference (SI) and imperfect SIC where the residual SI
exist. In general, the problems for both cases are non-convex due to the
subcarrier scheduling, and thus it requires an exhaustive search method, which
is prohibitively complicated to obtain the globally optimal solution. In order
to reduce the complexity, for the perfect SIC scenario, we jointly optimize
subcarrier scheduling and power allocation by applying the Lagrange duality
method. Next, for the imperfect SIC case, the problem is more complicated due
to the SI at the H-AP. To solve the problem, we propose an iterative algorithm
based on the projected gradient method. Simulation results show that the
proposed algorithm for the case of perfect SIC exhibits only negligible
sum-rate performance loss compared to the optimal algorithm, and the proposed
iterative algorithm for the imperfect SIC case offers a significant performance
gain over conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07488</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07488</id><created>2015-04-28</created><authors><author><keyname>Bakhtiary</keyname><forenames>Amir H.</forenames><affiliation>Universitat Oberta de Catalunya</affiliation></author><author><keyname>Lapedriza</keyname><forenames>Agata</forenames><affiliation>Universitat Oberta de Catalunya</affiliation></author><author><keyname>Masip</keyname><forenames>David</forenames><affiliation>Universitat Oberta de Catalunya</affiliation></author></authors><title>Speeding Up Neural Networks for Large Scale Classification using WTA
  Hashing</title><categories>cs.CV</categories><comments>9 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose to use the Winner Takes All hashing technique to
speed up forward propagation and backward propagation in fully connected layers
in convolutional neural networks. The proposed technique reduces significantly
the computational complexity, which in turn, allows us to train layers with a
large number of kernels with out the associated time penalty.
  As a consequence we are able to train convolutional neural network on a very
large number of output classes with only a small increase in the computational
cost. To show the effectiveness of the technique we train a new output layer on
a pretrained network using both the regular multiplicative approach and our
proposed hashing methodology. Our results showed no drop in performance and
demonstrate, with our implementation, a 7 fold speed up during the training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07494</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07494</id><created>2015-04-28</created><authors><author><keyname>Little</keyname><forenames>John B.</forenames></author></authors><title>Toric Codes and Finite Geometries</title><categories>cs.IT math.AG math.IT</categories><comments>11 pages</comments><msc-class>Primary 94B27, Secondary 51C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how the theory of affine geometries over the ring ${\mathbb
Z}/\langle q - 1\rangle$ can be used to understand the properties of toric and
generalized toric codes over ${\mathbb F}_q$. The minimum distance of these
codes is strongly tied to the collections of lines in the finite geometry that
contain subsets of the exponent vectors of the monomials that are evaluated to
produce the standard generator matrix for the code. We argue that this
connection is, in fact, even more direct than the connection with the lattice
geometry of those exponent vectors considered as elements of ${\mathbb Z}^2$ or
${\mathbb R}^2$. This point of view should be useful both as a way to visualize
properties of these codes and as a guide to heuristic searches for good codes
constructed in this fashion. In particular, we will use these ideas to see a
reason why these constructions have been so successful over the field ${\mathbb
F}_8$, but less successful in other cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07495</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07495</id><created>2015-04-28</created><authors><author><keyname>Haija</keyname><forenames>Ahmad Abu Al</forenames></author><author><keyname>Zhong</keyname><forenames>Peng</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Decode-Forward Transmission for the Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to IEEE Transactions on Communications
  for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose composite decode-forward (DF) schemes for the two-way relay
channel in both the full- and half-duplex modes by combining coherent relaying,
independent relaying and partial relaying strategies. For the full-duplex mode,
the relay partially decodes each user's information in each block and forwards
this partial information coherently with the source user to the destination
user in the next block as in block Markov coding. In addition, the relay
independently broadcasts a binning index of both users' decoded information
parts in the next block as in independent network coding. Each technique has a
different impact on the relay power usage and the rate region. We further
analyze in detail the independent partial DF scheme and derive in closed-form
link regimes when this scheme achieves a strictly larger rate region than just
time-sharing between its constituent techniques, direct transmission and
independent DF relaying, and when it reduces to a simpler scheme. For the
half-duplex mode, we propose a 6-phase time-division scheme that incorporates
all considered relaying techniques and uses joint decoding simultaneously over
all receiving phases. Numerical results show significant rate gains over
existing DF schemes, obtained by performing link adaptation of the composite
scheme based on the identified link regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07499</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07499</id><created>2015-04-28</created><authors><author><keyname>Lemaitre</keyname><forenames>Sophie</forenames></author><author><keyname>Salnikov</keyname><forenames>Vladimir</forenames></author><author><keyname>Choi</keyname><forenames>Daniel</forenames></author><author><keyname>Karamian</keyname><forenames>Philippe</forenames></author></authors><title>Computation of thermal properties via 3D homogenization of multiphase
  materials using FFT-based accelerated scheme</title><categories>cs.CE physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the thermal effective behaviour for 3D multiphase
composite material consisting of three isotropic phases which are the matrix,
the inclusions and the coating media. For this purpose we use an accelerated
FFT-based scheme initially proposed in Eyre and Milton (1999) to evaluate the
thermal conductivity tensor. Matrix and spherical inclusions media are polymers
with similar properties whereas the coating medium is metallic hence better
conducting. Thus, the contrast between the coating and the others media is very
large. For our study, we use RVEs (Representative volume elements) generated by
RSA (Random Sequential Adsorption) method developed in our previous works,
then, we compute effective thermal properties using an FFT-based homogenization
technique validated by comparison with the direct finite elements method. We
study the thermal behaviour of the 3D-multiphase composite material and we show
what features should be taken into account to make the computational approach
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07504</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07504</id><created>2015-04-28</created><updated>2015-05-04</updated><authors><author><keyname>Autili</keyname><forenames>Marco</forenames></author><author><keyname>Inverardi</keyname><forenames>Paola</forenames></author><author><keyname>Tivoli</keyname><forenames>Massimo</forenames></author><author><keyname>Garlan</keyname><forenames>David</forenames></author></authors><title>Synthesis of correct adaptors for protocol enhancement in
  component-based systems</title><categories>cs.SE</categories><comments>8 pages in Proceedings of Specification and Verification of
  Component-Based Systems (SAVCBS'04) at 12th ACM SIGSOFT Symposium on the
  Foundations of Software Engineering (FSE'12), October 31-November 1, 2004,
  Newport Beach, California, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptation of software components is an important issue in Component Based
Software Engineering (CBSE). Building a system from reusable or
Commercial-Off-The-Shelf (COTS) components introduces a set of problems, mainly
related to compatibility and communication aspects. On one hand, components may
have incompatible interaction behavior. This might require to restrict the
system's behavior to a subset of safe behaviors. On the other hand, it might be
necessary to enhance the current communication protocol. This might require to
augment the system's behavior to introduce more sophisticated interactions
among components. We address these problems by enhancing our architectural
approach which allows for detection and recovery of incompatible interactions
by synthesizing a suitable coordinator. Taking into account the specification
of the system to be assembled and the specification of the protocol
enhancements, our tool (called SYNTHESIS) automatically derives, in a
compositional way, the glue code for the set of components. The synthesized
glue code implements a software coordinator which avoids incompatible
interactions and provides a protocol-enhanced version of the composed system.
By using an assume-guarantee technique, we are able to check, in a
compositional way, if the protocol enhancement is consistent with respect to
the restrictions applied to assure the specified safe behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07510</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07510</id><created>2015-04-28</created><authors><author><keyname>Yu</keyname><forenames>H.</forenames></author><author><keyname>Yang</keyname><forenames>C.</forenames></author></authors><title>An Improved DFT-based Channel Estimation for Mobile Communication
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel estimation is one of the most important parts in current mobile
communication systems. Among the huge contributions in channel estimation
studies, the discrete Fourier transform (DFT)-based channel estimation has
attracted lots of interests since it can not only be easily implemented but
also have acceptable performance in practical systems. In this paper, we
propose an improved DFT-based channel estimation scheme that tries to clean the
reference signals in the time domain before being used for interpolation using
the estimated noise variance from reference signals on multiple orthogonal
frequency division multiplexing (OFDM) symbols via the property of DFT. The
proposed channel estimation scheme does not need any priori channel
information. We validate the proposed channel estimation scheme in various
channel models via simulations and comparison with conventional channel
estimation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07513</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07513</id><created>2015-04-28</created><updated>2015-04-29</updated><authors><author><keyname>Bittner</keyname><forenames>Benjamin</forenames></author><author><keyname>Bozzano</keyname><forenames>Marco</forenames></author><author><keyname>Cavada</keyname><forenames>Roberto</forenames></author><author><keyname>Cimatti</keyname><forenames>Alessandro</forenames></author><author><keyname>Gario</keyname><forenames>Marco</forenames></author><author><keyname>Griggio</keyname><forenames>Alberto</forenames></author><author><keyname>Mattarei</keyname><forenames>Cristian</forenames></author><author><keyname>Micheli</keyname><forenames>Andrea</forenames></author><author><keyname>Zampedri</keyname><forenames>Gianni</forenames></author></authors><title>The xSAP Safety Analysis Platform</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the xSAP safety analysis platform. xSAP provides several
model-based safety analysis features for finite- and infinite-state synchronous
transition systems. In particular, it supports library-based definition of
fault modes, an automatic model extension facility, generation of safety
analysis artifacts such as Dynamic Fault Trees (DFTs) and Failure Mode and
Effects Analysis (FMEA) tables. Moreover, it supports probabilistic evaluation
of Fault Trees, failure propagation analysis using Timed Failure Propagation
Graphs (TFPGs), and Common Cause Analysis (CCA). xSAP has been used in several
industrial projects as verification back-end, and is currently being evaluated
in a joint R&amp;D Project involving FBK and The Boeing Company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07518</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07518</id><created>2015-04-28</created><authors><author><keyname>Rahn</keyname><forenames>Mona</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Guido</forenames></author></authors><title>Efficient Equilibria in Polymatrix Coordination Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider polymatrix coordination games with individual preferences where
every player corresponds to a node in a graph who plays with each neighbor a
separate bimatrix game with non-negative symmetric payoffs. In this paper, we
study $\alpha$-approximate $k$-equilibria of these games, i.e., outcomes where
no group of at most $k$ players can deviate such that each member increases his
payoff by at least a factor $\alpha$. We prove that for $\alpha \ge 2$ these
games have the finite coalitional improvement property (and thus
$\alpha$-approximate $k$-equilibria exist), while for $\alpha &lt; 2$ this
property does not hold. Further, we derive an almost tight bound of
$2\alpha(n-1)/(k-1)$ on the price of anarchy, where $n$ is the number of
players; in particular, it scales from unbounded for pure Nash equilibria ($k =
1)$ to $2\alpha$ for strong equilibria ($k = n$). We also settle the complexity
of several problems related to the verification and existence of these
equilibria. Finally, we investigate natural means to reduce the inefficiency of
Nash equilibria. Most promisingly, we show that by fixing the strategies of $k$
players the price of anarchy can be reduced to $n/k$ (and this bound is tight).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07526</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07526</id><created>2015-04-28</created><authors><author><keyname>Bajovi&#x107;</keyname><forenames>Dragana</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author><author><keyname>Xavier</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>Distributed inference over directed networks: Performance limits and
  optimal design</title><categories>cs.IT math.IT</categories><comments>35 pages, submitted to Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find large deviations rates for consensus-based distributed inference for
directed networks. When the topology is deterministic, we establish the large
deviations principle and find exactly the corresponding rate function, equal at
all nodes. We show that the dependence of the rate function on the stochastic
weight matrix associated with the network is fully captured by its left
eigenvector corresponding to the unit eigenvalue. Further, when the sensors'
observations are Gaussian, the rate function admits a closed form expression.
Motivated by these observations, we formulate the optimal network design
problem of finding the left eigenvector which achieves the highest value of the
rate function, for a given target accuracy. This eigenvector therefore
minimizes the time that the inference algorithm needs to reach the desired
accuracy. For Gaussian observations, we show that the network design problem
can be formulated as a semidefinite (convex) program, and hence can be solved
efficiently. When observations are identically distributed across agents, the
system exhibits an interesting property: the graph of the rate function always
lies between the graphs of the rate function of an isolated node and the rate
function of a fusion center that has access to all observations. We prove that
this fundamental property holds even when the topology and the associated
system matrices change randomly over time, with arbitrary distribution. Due to
generality of its assumptions, the latter result requires more subtle
techniques than the standard large deviations tools, contributing to the
general theory of large deviations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07544</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07544</id><created>2015-04-28</created><authors><author><keyname>Yang</keyname><forenames>Heecheol</forenames></author><author><keyname>Shin</keyname><forenames>Wonjae</forenames></author><author><keyname>Lee</keyname><forenames>Jungwoo</forenames></author></authors><title>Grouping Based Blind Interference Alignment for $K$-user MISO
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, to appear in IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a blind interference alignment (BIA) through staggered antenna
switching scheme with no ideal channel assumption. Contrary to the ideal
assumption that channels remain constant during BIA symbol extension period,
when the coherence time of the channel is relatively short, channel
coefficients may change during a given symbol extension period. To perform BIA
perfectly with realistic channel assumption, we propose a grouping based
supersymbol structure for $K$-user interference channels which can adjust a
supersymbol length to given coherence time. It is proved that the supersymbol
length could be reduced significantly by an appropriate grouping. Furthermore,
it is also shown that the grouping based supersymbol achieves higher degrees of
freedom than the conventional method with given coherence time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07545</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07545</id><created>2015-04-28</created><authors><author><keyname>Fujishige</keyname><forenames>Satoru</forenames></author><author><keyname>Goemans</keyname><forenames>Michel X.</forenames></author><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Matroids are Immune to Braess Paradox</title><categories>cs.GT cs.DM math.CO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous Braess paradox describes the following phenomenon: It might happen
that the improvement of resources, like building a new street within a
congested network, may in fact lead to larger costs for the players in an
equilibrium. In this paper we consider general nonatomic congestion games and
give a characterization of the maximal combinatorial property of strategy
spaces for which Braess paradox does not occur. In a nutshell, bases of
matroids are exactly this maximal structure. We prove our characterization by
two novel sensitivity results for convex separable optimization problems over
polymatroid base polyhedra which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07547</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07547</id><created>2015-04-28</created><authors><author><keyname>Shin</keyname><forenames>Wonjae</forenames></author><author><keyname>Han</keyname><forenames>Yonghee</forenames></author><author><keyname>Lee</keyname><forenames>Jungwoo</forenames></author><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Retrospective Interference Alignment for Two-Cell Uplink MIMO Cellular
  Networks with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, to appear in IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new retrospective interference alignment for
two-cell multiple-input multiple-output (MIMO) interfering multiple access
channels (IMAC) with the delayed channel state information at the transmitters
(CSIT). It is shown that having delayed CSIT can strictly increase the sum-DoF
compared to the case of no CSIT. The key idea is to align multiple interfering
signals from adjacent cells onto a small dimensional subspace over time by
fully exploiting the previously received signals as side information with
outdated CSIT in a distributed manner. Remarkably, we show that the
retrospective interference alignment can achieve the optimal sum-DoF in the
context of two-cell two-user scenario by providing a new outer bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07550</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07550</id><created>2015-04-28</created><updated>2015-09-24</updated><authors><author><keyname>Belharbi</keyname><forenames>Soufiane</forenames></author><author><keyname>Chatelain</keyname><forenames>Clement</forenames></author><author><keyname>Herault</keyname><forenames>Romain</forenames></author><author><keyname>Adam</keyname><forenames>Sebastien</forenames></author></authors><title>Facial landmark detection using structured output deep neural networks</title><categories>cs.LG stat.ML</categories><comments>Submitted to Pattern Recognition Letters (PRL), old title
  &quot;Input/Output Deep Architecture for Structured Output Problems&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial landmark detection is an important step for many perception tasks such
as face recognition and facial analysis. Regression-based methods have shown a
large success. In particular, deep neural networks (DNN) has demonstrated a
strong capability to model the high non-linearity between the face image and
the face shape. In this paper, we tackle this task as a structured output
problem, where we exploit the strong dependencies that lie between the outputs.
Beside learning a regression mapping function from the input to the output, we
learn, in an unsupervised way, the inter-dependencies between the outputs. For
this, we propose a generic regression framework for structured output problems.
Our framework allows a successful incorporation of learning the output
structure into DNN using the pre-training trick. We apply our method on a
facial landmark detection task, where the output is strongly structured. We
evaluate our DNN, named Input/Output Deep Architecture (IODA), on two public
challenging datasets: LFPW and HELEN. We show that IODA outperforms traditional
deep architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07553</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07553</id><created>2015-04-28</created><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Differentially Private Release and Learning of Threshold Functions</title><categories>cs.CR cs.LG</categories><comments>43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new upper and lower bounds on the sample complexity of $(\epsilon,
\delta)$ differentially private algorithms for releasing approximate answers to
threshold functions. A threshold function $c_x$ over a totally ordered domain
$X$ evaluates to $c_x(y) = 1$ if $y \le x$, and evaluates to $0$ otherwise. We
give the first nontrivial lower bound for releasing thresholds with
$(\epsilon,\delta)$ differential privacy, showing that the task is impossible
over an infinite domain $X$, and moreover requires sample complexity $n \ge
\Omega(\log^*|X|)$, which grows with the size of the domain. Inspired by the
techniques used to prove this lower bound, we give an algorithm for releasing
thresholds with $n \le 2^{(1+ o(1))\log^*|X|}$ samples. This improves the
previous best upper bound of $8^{(1 + o(1))\log^*|X|}$ (Beimel et al., RANDOM
'13).
  Our sample complexity upper and lower bounds also apply to the tasks of
learning distributions with respect to Kolmogorov distance and of properly PAC
learning thresholds with differential privacy. The lower bound gives the first
separation between the sample complexity of properly learning a concept class
with $(\epsilon,\delta)$ differential privacy and learning without privacy. For
properly learning thresholds in $\ell$ dimensions, this lower bound extends to
$n \ge \Omega(\ell \cdot \log^*|X|)$.
  To obtain our results, we give reductions in both directions from releasing
and properly learning thresholds and the simpler interior point problem. Given
a database $D$ of elements from $X$, the interior point problem asks for an
element between the smallest and largest elements in $D$. We introduce new
recursive constructions for bounding the sample complexity of the interior
point problem, as well as further reductions and techniques for proving
impossibility results for other basic problems in differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07558</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07558</id><created>2015-04-28</created><authors><author><keyname>Autili</keyname><forenames>Marco</forenames></author><author><keyname>Cortellessa</keyname><forenames>Vittorio</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Paolo</forenames></author><author><keyname>Inverardi</keyname><forenames>Paola</forenames></author></authors><title>On the adaptation of context-aware services</title><categories>cs.SE</categories><comments>Proceedings of the International Workshop on Service Oriented
  Computing: a look at the Inside (SOC@Inside'07) at the International
  Conference on Service Oriented Computing (ICSOC'07), Vienna, Austria, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous networking empowered by Beyond 3G networking makes it possible for
mobile users to access networked software services across heterogeneous
infrastructures by resource-constrained devices. Heterogeneity and device
limitedness creates serious problems for the development and deployment of
mobile services that are able to run properly on the execution context and are
able to ensures that users experience the &quot;best&quot; Quality of Service possible
according to their needs and specific contexts of use. To face these problems
the concept of adaptable service is increasingly emerging in the software
community. In this paper we describe how CHAMELEON, a declarative framework for
tailoring adaptable services, is used within the IST PLASTIC project whose goal
is the rapid and easy development/deployment of self-adapting services for B3G
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07563</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07563</id><created>2015-04-25</created><authors><author><keyname>Olafare</keyname><forenames>Olayinka</forenames></author><author><keyname>Parhizkar</keyname><forenames>Hani</forenames></author><author><keyname>Vem</keyname><forenames>Silas</forenames></author></authors><title>A New Secure Mobile Cloud Architecture</title><categories>cs.DC</categories><comments>15 Pages, 8 Figures, 9 Tables, A New Secure Mobile Cloud Architecture
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand and use of mobile phones, PDAs and smart phones are constantly on
the rise as such, manufacturers of these devices are improving the technology
and usability of these devices constantly. Due to the handy shape and size
these devices come in, their processing capabilities and functionalities, they
are preferred by many over the conventional desktop or laptop computers. Mobile
devices are being used today to perform most tasks that a desktop or laptop
computer could be used for. On this premise, mobile devices are also used to
connect to the resources of cloud computing hence, mobile cloud computing
(MCC). The seemingly ubiquitous and pervasive nature of most mobile devices has
made it acceptable and adequate to match the ubiquitous and pervasive nature of
cloud computing. Mobile cloud computing is said to have increased the
challenges known to cloud computing due to the security loop holes that most
mobile devices have.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07566</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07566</id><created>2015-04-28</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Designing Wireless Broadband Access for Energy Efficiency: Are Small
  Cells the Only Answer?</title><categories>cs.IT cs.NI math.IT</categories><comments>Published at Small Cell and 5G Networks (SmallNets) Workshop, IEEE
  International Conference on Communications (ICC), 6 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main usage of cellular networks has changed from voice to data traffic,
mostly requested by static users. In this paper, we analyze how a cellular
network should be designed to provide such wireless broadband access with
maximal energy efficiency (EE). Using stochastic geometry and a detailed power
consumption model, we optimize the density of access points (APs), number of
antennas and users per AP, and transmission power for maximal EE. Small cells
are of course a key technology in this direction, but the analysis shows that
the EE improvement of a small-cell network saturates quickly with the AP
density and then &quot;massive MIMO&quot; techniques can further improve the EE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07570</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07570</id><created>2015-04-28</created><updated>2015-05-07</updated><authors><author><keyname>Pezeshkpour</keyname><forenames>Pouya</forenames></author></authors><title>An Optimal Linear Coding for Index Coding Problem</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimal linear coding solution for index coding problem is established.
Instead of network coding approach by focus on graph theoric and algebraic
methods a linear coding program for solving both unicast and groupcast index
coding problem is presented. The coding is proved to be the optimal solution
from the linear perspective and can be easily utilize for any number of
messages. The importance of this work is lying mostly on the usage of the
presented coding in the groupcast index coding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07571</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07571</id><created>2015-04-28</created><authors><author><keyname>Okandan</keyname><forenames>Murat</forenames></author></authors><title>Can Machines Truly Think</title><categories>cs.AI cs.NE</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Can machines truly think? This question and its answer have many implications
that depend, in large part, on any number of assumptions underlying how the
issue has been addressed or considered previously. A crucial question, and one
that is almost taken for granted, is the starting point for this discussion:
Can &quot;thought&quot; be achieved or emulated by algorithmic procedures?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07575</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07575</id><created>2015-04-28</created><authors><author><keyname>Johns</keyname><forenames>Edward</forenames></author><author><keyname>Mac Aodha</keyname><forenames>Oisin</forenames></author><author><keyname>Brostow</keyname><forenames>Gabriel J.</forenames></author></authors><title>Becoming the Expert - Interactive Multi-Class Machine Teaching</title><categories>cs.CV cs.LG stat.ML</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to machines, humans are extremely good at classifying images into
categories, especially when they possess prior knowledge of the categories at
hand. If this prior information is not available, supervision in the form of
teaching images is required. To learn categories more quickly, people should
see important and representative images first, followed by less important
images later - or not at all. However, image-importance is individual-specific,
i.e. a teaching image is important to a student if it changes their overall
ability to discriminate between classes. Further, students keep learning, so
while image-importance depends on their current knowledge, it also varies with
time.
  In this work we propose an Interactive Machine Teaching algorithm that
enables a computer to teach challenging visual concepts to a human. Our
adaptive algorithm chooses, online, which labeled images from a teaching set
should be shown to the student as they learn. We show that a teaching strategy
that probabilistically models the student's ability and progress, based on
their correct and incorrect answers, produces better 'experts'. We present
results using real human participants across several varied and challenging
real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07578</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07578</id><created>2015-04-28</created><authors><author><keyname>Aloisio</keyname><forenames>Alessandro</forenames></author><author><keyname>Autili</keyname><forenames>Marco</forenames></author><author><keyname>D'Angelo</keyname><forenames>Alfredo</forenames></author><author><keyname>Viidanoja</keyname><forenames>Antti</forenames></author><author><keyname>Leguay</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Ginzler</keyname><forenames>Tobias</forenames></author><author><keyname>Lampe</keyname><forenames>Thorsten</forenames></author><author><keyname>Spagnolo</keyname><forenames>Luca</forenames></author><author><keyname>Wolthusen</keyname><forenames>Stephen</forenames></author><author><keyname>Flizikowski</keyname><forenames>Adam</forenames></author><author><keyname>Sliwa</keyname><forenames>Joanna</forenames></author></authors><title>TACTICS: TACTICal Service Oriented Architecture</title><categories>cs.SE</categories><comments>10 pages in 3rd International Conference in Software Engineering for
  Defence Applications (SEDA 2014), September 22-23, 2014, Rome, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing complexity and heterogeneity of contemporary Command,
Control, Communications, Computers, &amp; Intelligence systems at all levels within
military organizations, the adoption of the Service Oriented Architectures
(SOA) principles and concepts is becoming essential. SOA provides flexibility
and interoperability of services enabling the realization of efficient and
modular information infrastructure for command and control systems. However,
within a tactical domain, the presence of potentially highly mobile actors
equipped with constrained communications media (i.e., unreliable radio networks
with limited bandwidth) limits the applicability of traditional SOA
technologies. The TACTICS project aims at the definition and experimental
demonstration of a Tactical Services Infrastructure enabling tactical radio
networks (without any modifications of the radio part of those networks) to
participate in SOA infrastructures and provide, as well as consume, services to
and from the strategic domain independently of the user's location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07586</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07586</id><created>2015-04-28</created><authors><author><keyname>Naghnaeian</keyname><forenames>Mohammad</forenames></author><author><keyname>Hirzallah</keyname><forenames>Nabil</forenames></author><author><keyname>Voulgaris</keyname><forenames>Petros G.</forenames></author></authors><title>Dual Rate Control for Security in Cyber-physical Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider malicious attacks on actuators and sensors of a feedback system
which can be modeled as additive, possibly unbounded, disturbances at the
digital (cyber) part of the feedback loop. We precisely characterize the role
of the unstable poles and zeros of the system in the ability to detect stealthy
attacks in the context of the sampled data implementation of the controller in
feedback with the continuous (physical) plant. We show that, if there is a
single sensor that is guaranteed to be secure and the plant is observable from
that sensor, then there exist a class of multirate sampled data controllers
that ensure that all attacks remain detectable. These dual rate controllers are
sampling the output faster than the zero order hold rate that operates on the
control input and as such, they can even provide better nominal performance
than single rate, at the price of higher sampling of the continuous output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07590</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07590</id><created>2015-04-28</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author><author><keyname>Jayagopi</keyname><forenames>Dinesh Babu</forenames></author></authors><title>A Robust Lane Detection and Departure Warning System</title><categories>cs.CV</categories><comments>The Intelligent Vehicles Symposium (IV2015). arXiv admin note: text
  overlap with arXiv:1503.06648</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we have developed a robust lane detection and departure warning
technique. Our system is based on single camera sensor. For lane detection a
modified Inverse Perspective Mapping using only a few extrinsic camera
parameters and illuminant Invariant techniques is used. Lane markings are
represented using a combination of 2nd and 4th order steerable filters, robust
to shadowing. Effect of shadowing and extra sun light are removed using Lab
color space, and illuminant invariant representation. Lanes are assumed to be
cubic curves and fitted using robust RANSAC. This method can reliably detect
lanes of the road and its boundary. This method has been experimented in Indian
road conditions under different challenging situations and the result obtained
were very good. For lane departure angle an optical flow based method were
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07591</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07591</id><created>2015-04-28</created><authors><author><keyname>He</keyname><forenames>Qijun</forenames></author><author><keyname>Macauley</keyname><forenames>Matthew</forenames></author></authors><title>Stratification and enumeration of Boolean functions by canalizing depth</title><categories>cs.DM math.CO physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean network models have gained popularity in computational systems
biology over the last dozen years. Many of these networks use canalizing
Boolean functions, which has led to increased interest in the study of these
functions. The canalizing depth of a function describes how many canalizing
variables can be recursively picked off, until a non-canalizing function
remains. In this paper, we show how every Boolean function has a unique
algebraic form involving extended monomial layers and a well-defined core
polynomial. This generalizes recent work on the algebraic structure of nested
canalizing functions, and it yields a stratification of all Boolean functions
by their canalizing depth. As a result, we obtain closed formulas for the
number of n-variable Boolean functions with depth k, which simultaneously
generalizes enumeration formulas for canalizing, and nested canalizing
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07595</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07595</id><created>2015-04-28</created><authors><author><keyname>Br&#xe6;ndeland</keyname><forenames>Asbj&#xf8;rn</forenames></author></authors><title>nCk sequences and their difference sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nCk sequence is a sequence of n-bit numbers with k bits set. Given such a
sequence C, the difference sequence D of C is subject to certain regularities
that make it possible to generate D in 2|C| time, and, hence, to generate C in
3|C| time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07597</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07597</id><created>2015-04-27</created><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author></authors><title>Duplicate Detection with Efficient Language Models for Automatic
  Bibliographic Heterogeneous Data Integration</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method to detect duplicates used to merge different
bibliographic record corpora with the help of lexical and social information.
As we show, a trivial key is not available to delete useless documents. Merging
heteregeneous document databases to get a maximum of information can be of
interest. In our case we try to build a document corpus about the TOR molecule
so as to extract relationships with other gene components from PubMed and
WebOfScience document databases. Our approach makes key fingerprints based on
n-grams. We made two documents gold standards using this corpus to make an
evaluation. Comparison with other well-known methods in deduplication gives
best scores of recall (95\%) and precision (100\%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07614</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07614</id><created>2015-04-28</created><authors><author><keyname>Wang</keyname><forenames>Tong</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>Doshi-Velez</keyname><forenames>Finale</forenames></author><author><keyname>Liu</keyname><forenames>Yimin</forenames></author><author><keyname>Klampfl</keyname><forenames>Erica</forenames></author><author><keyname>MacNeille</keyname><forenames>Perry</forenames></author></authors><title>Or's of And's for Interpretable Classification, with Application to
  Context-Aware Recommender Systems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a machine learning algorithm for building classifiers that are
comprised of a small number of disjunctions of conjunctions (or's of and's). An
example of a classifier of this form is as follows: If X satisfies (x1 = 'blue'
AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '&lt;15') OR (x1 = 'yellow'), then we
predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal
and a conjunction of literals is called a pattern. Models of this form have the
advantage of being interpretable to human experts, since they produce a set of
conditions that concisely describe a specific class. We present two
probabilistic models for forming a pattern set, one with a Beta-Binomial prior,
and the other with Poisson priors. In both cases, there are prior parameters
that the user can set to encourage the model to have a desired size and shape,
to conform with a domain-specific definition of interpretability. We provide
two scalable MAP inference approaches: a pattern level search, which involves
association rule mining, and a literal level search. We show stronger priors
reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict
user behavior with respect to in-vehicle context-aware personalized recommender
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07615</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07615</id><created>2015-04-28</created><updated>2015-04-29</updated><authors><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Varan</keyname><forenames>Burak</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Throughput Maximization for Two-way Relay Channels with Energy
  Harvesting Nodes: The Impact of Relaying Strategies</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Communications,
  April 19, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the two-way relay channel with energy harvesting
nodes. In particular, we find transmission policies that maximize the
sum-throughput for two-way relay channels when the relay does not employ a data
buffer. The relay can perform decode-and-forward, compress-and-forward,
compute-and-forward or amplify-and-forward relaying. Furthermore, we consider
throughput improvement by dynamically choosing relaying strategies, resulting
in hybrid relaying strategies. We show that an iterative generalized
directional water-filling algorithm solves the offline throughput maximization
problem, with the achievable sum-rate from an individual or hybrid relaying
scheme. In addition to the optimum offline policy, we obtain the optimum online
policy via dynamic programming. We provide numerical results for each relaying
scheme to support the analytic findings, pointing out to the advantage of
adapting the instantaneous relaying strategy to the available harvested energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07621</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07621</id><created>2015-04-28</created><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Pietrzak</keyname><forenames>Krzysztof</forenames></author></authors><title>Condensed Unpredictability</title><categories>cs.CR</categories><comments>This is the full version of the paper published at ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of deriving a key with high HILL entropy from an
unpredictable source. Previous to this work, the only known way to transform
unpredictability into a key that was $\eps$ indistinguishable from having
min-entropy was via pseudorandomness, for example by Goldreich-Levin (GL)
hardcore bits. This approach has the inherent limitation that from a source
with $k$ bits of unpredictability entropy one can derive a key of length (and
thus HILL entropy) at most $k-2\log(1/\epsilon)$ bits. In many settings, e.g.
when dealing with biometric data, such a $2\log(1/\epsilon)$ bit entropy loss
in not an option. Our main technical contribution is a theorem that states that
in the high entropy regime, unpredictability implies HILL entropy. The loss in
circuit size in this argument is exponential in the entropy gap $d$. To
overcome the above restriction, we investigate if it's possible to first
&quot;condense&quot; unpredictability entropy and make the entropy gap small. We show
that any source with $k$ bits of unpredictability can be condensed into a
source of length $k$ with $k-3$ bits of unpredictability entropy. Our condenser
simply &quot;abuses&quot; the GL construction and derives a $k$ bit key from a source
with $k$ bits of unpredicatibily. The original GL theorem implies nothing when
extracting that many bits, but we show that in this regime, GL still behaves
like a &quot;condenser&quot; for unpredictability. This result comes with two caveats (1)
the loss in circuit size is exponential in $k$ and (2) we require that the
source we start with has \emph{no} HILL entropy (equivalently, one can
efficiently check if a guess is correct). We leave it as an intriguing open
problem to overcome these restrictions or to prove they're inherent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07626</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07626</id><created>2015-04-28</created><updated>2015-05-13</updated><authors><author><keyname>Br&#xe6;ndeland</keyname><forenames>Asbj&#xf8;rn</forenames></author></authors><title>Split-by-edges trees</title><categories>cs.DS cs.DM math.CO</categories><comments>The definition of 'ordered SBE-tree' has been added. This corrects an
  omission in the previous versions but does not change anything essential.
  Some changes have been made to accommodate the addition, and others have been
  made to correct minor errors and improve wordings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A split-by-edges tree of a graph G on n vertices is a binary tree T where the
root = V(G), every leaf is an independent set in G, and for every other node N
in T with children L and R there is a pair of vertices {u, v} in N such that L
= N - v, R = N - u, and uv is an edge in G. It follows from the definition that
every maximal independent set in G is a leaf in T, and the maximum independent
sets of G are the leaves closest to the root of T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07643</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07643</id><created>2015-04-28</created><authors><author><keyname>Ibrahim</keyname><forenames>Mazlinda</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author><author><keyname>Brito-Loeza</keyname><forenames>Carlos</forenames></author></authors><title>A novel variational model for image registration using Gaussian
  curvature</title><categories>math.NA cs.CV</categories><comments>23 pages, 5 figures. Key words: Image registration, Non-parametric
  image registration, Regularisation, Gaussian curvature, surface mapping</comments><msc-class>65F10, 68U10, 62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration is one important task in many image processing
applications. It aims to align two or more images so that useful information
can be extracted through comparison, combination or superposition. This is
achieved by constructing an optimal trans- formation which ensures that the
template image becomes similar to a given reference image. Although many models
exist, designing a model capable of modelling large and smooth deformation
field continues to pose a challenge. This paper proposes a novel variational
model for image registration using the Gaussian curvature as a regulariser. The
model is motivated by the surface restoration work in geometric processing
[Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. An
effective numerical solver is provided for the model using an augmented
Lagrangian method. Numerical experiments can show that the new model
outperforms three competing models based on, respectively, a linear curvature
[Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the mean
curvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp.
89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage,
(2009), pp. 61-72] in terms of robustness and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07647</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07647</id><created>2015-04-28</created><updated>2015-10-13</updated><authors><author><keyname>Geelen</keyname><forenames>Jim</forenames></author><author><keyname>Kapadia</keyname><forenames>Rohan</forenames></author></authors><title>Computing girth and cogirth in perturbed graphic matroids</title><categories>math.CO cs.DM</categories><comments>24 pages, no figures</comments><msc-class>05B35 (Primary), 94B05, 90C27 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give polynomial-time randomized algorithms for computing the girth and the
cogirth of binary matroids that are low-rank perturbations of graphic matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07648</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07648</id><created>2015-04-28</created><authors><author><keyname>Cheraghchi</keyname><forenames>Mahdi</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author></authors><title>Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard
  Transform</title><categories>cs.IT cs.CC cs.LG math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every fixed constant $\alpha &gt; 0$, we design an algorithm for computing
the $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \in
\mathbb{R}^N$ in time $k^{1+\alpha} (\log N)^{O(1)}$. Specifically, the
algorithm is given query access to $x$ and computes a $k$-sparse $\tilde{x} \in
\mathbb{R}^N$ satisfying $\|\tilde{x} - \hat{x}\|_1 \leq c \|\hat{x} -
H_k(\hat{x})\|_1$, for an absolute constant $c &gt; 0$, where $\hat{x}$ is the
transform of $x$ and $H_k(\hat{x})$ is its best $k$-sparse approximation. Our
algorithm is fully deterministic and only uses non-adaptive queries to $x$
(i.e., all queries are determined and performed in parallel when the algorithm
starts).
  An important technical tool that we use is a construction of nearly optimal
and linear lossless condensers which is a careful instantiation of the GUV
condenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a
deterministic and non-adaptive $\ell_1/\ell_1$ compressed sensing scheme based
on general lossless condensers that is equipped with a fast reconstruction
algorithm running in time $k^{1+\alpha} (\log N)^{O(1)}$ (for the GUV-based
condenser) and is of independent interest. Our scheme significantly simplifies
and improves an earlier expander-based construction due to Berinde, Gilbert,
Indyk, Karloff, Strauss (Allerton 2008).
  Our methods use linear lossless condensers in a black box fashion; therefore,
any future improvement on explicit constructions of such condensers would
immediately translate to improved parameters in our framework (potentially
leading to $k (\log N)^{O(1)}$ reconstruction time with a reduced exponent in
the poly-logarithmic factor, and eliminating the extra parameter $\alpha$).
  Finally, by allowing the algorithm to use randomness, while still using
non-adaptive queries, the running time of the algorithm can be improved to
$\tilde{O}(k \log^3 N)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07661</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07661</id><created>2015-04-28</created><updated>2015-04-30</updated><authors><author><keyname>Chowdhury</keyname><forenames>Iffat</forenames></author><author><keyname>Gibson</keyname><forenames>Matt</forenames></author></authors><title>A Characterization of Consistent Digital Line Segments in Two Dimensions</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our concern is the digitalization of line segments in two dimensions as
considered by Chun et al.[Discrete Comput. Geom., 2009] and Christ et
al.[Discrete Comput. Geom., 2012]. The key property that differentiates the
research of Chun et al. and Christ et al. from other research in digital line
segment construction is that the intersection of any two segments must be
connected. Such a system of segments is called a consistent digital line
segments system (CDS). Chun et al. give a construction for all segments in
higher dimensions that share a common endpoint (called consistent digital rays
(CDR)) that has asymptotically optimal Hausdorff distance, and Christ et al.
give a complete CDS in two dimensions with optimal Hausdorff distance. Christ
et al. also give a characterization of CDRs in two dimensions, and they leave
open the question on how to characterize CDSes in two dimensions. In this
paper, we answer the most important open question regarding CDSes in two
dimensions by giving the characterization asked for by Christ et al. We obtain
the characterization by giving a set of necessary and sufficient conditions
that a CDS must satisfy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07662</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07662</id><created>2015-04-28</created><authors><author><keyname>Yankov</keyname><forenames>Dragomir</forenames></author><author><keyname>Berkhin</keyname><forenames>Pavel</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Evaluation of Explore-Exploit Policies in Multi-result Ranking Systems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of using Explore-Exploit techniques to improve
precision in multi-result ranking systems such as web search, query
autocompletion and news recommendation. Adopting an exploration policy directly
online, without understanding its impact on the production system, may have
unwanted consequences - the system may sustain large losses, create user
dissatisfaction, or collect exploration data which does not help improve
ranking quality. An offline framework is thus necessary to let us decide what
policy and how we should apply in a production environment to ensure positive
outcome. Here, we describe such an offline framework.
  Using the framework, we study a popular exploration policy - Thompson
sampling. We show that there are different ways of implementing it in
multi-result ranking systems, each having different semantic interpretation and
leading to different results in terms of sustained click-through-rate (CTR)
loss and expected model improvement. In particular, we demonstrate that
Thompson sampling can act as an online learner optimizing CTR, which in some
cases can lead to an interesting outcome: lift in CTR during exploration. The
observation is important for production systems as it suggests that one can get
both valuable exploration data to improve ranking performance on the long run,
and at the same time increase CTR while exploration lasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07665</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07665</id><created>2015-04-28</created><authors><author><keyname>Narojczyk</keyname><forenames>Jakub W.</forenames></author><author><keyname>Piglowski</keyname><forenames>P. M.</forenames></author><author><keyname>Wojciechowski</keyname><forenames>K. W.</forenames></author><author><keyname>Tretiakov</keyname><forenames>K. V.</forenames></author></authors><title>Elastic properties of mono- and polydisperse two-dimensional crystals of
  hard--core repulsive Yukawa particles</title><categories>physics.comp-ph cond-mat.soft cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo simulations of mono-- and polydisperse two--dimensional crystals
are reported. The particles in the studied system, interacting through
hard--core repulsive Yukawa potential, form a solid phase of hexagonal lattice.
The elastic properties of crystalline Yukawa systems are determined in the
$NpT$ ensemble with variable shape of the periodic box. Effects of the Debye
screening length ($\kappa^{-1}$), contact value of the potential ($\epsilon$),
and the size polydispersity of particles on elastic properties of the system
are studied. The simulations show that the polydispersity of particles strongly
influences the elastic properties of the studied system, especially on the
shear modulus. It is also found that the elastic moduli increase with density
and their growth rate depends on the screening length. Shorter screening length
leads to faster increase of elastic moduli with density and decrease of the
Poisson's ratio. In contrast to its three-dimensional version, the studied
system is non-auxetic, i.e. shows positive Poisson's ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07672</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07672</id><created>2015-04-28</created><updated>2016-02-05</updated><authors><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>A Semidefinite Programming Method for Integer Convex Quadratic
  Minimization</title><categories>math.OC cs.DS</categories><comments>32 pages, 5 figures; submitted to SIAM Journal on Optimization
  (SIOPT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the NP-hard problem of minimizing a convex quadratic function
over the integer lattice ${\bf Z}^n$. We present a simple semidefinite
programming (SDP) relaxation for obtaining a nontrivial lower bound on the
optimal value of the problem. By interpreting the solution to the SDP
relaxation probabilistically, we obtain a randomized algorithm for finding good
suboptimal solutions, and thus an upper bound on the optimal value. The
effectiveness of the method is shown for numerical problem instances of various
sizes. Finally, we introduce several extensions to the idea, including how to
adapt the method to (quadratically) constrained integer problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07676</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07676</id><created>2015-04-28</created><authors><author><keyname>Wyner</keyname><forenames>Abraham J.</forenames></author><author><keyname>Olson</keyname><forenames>Matthew</forenames></author><author><keyname>Bleich</keyname><forenames>Justin</forenames></author><author><keyname>Mease</keyname><forenames>David</forenames></author></authors><title>Explaining the Success of AdaBoost and Random Forests as Interpolating
  Classifiers</title><categories>stat.ML cs.LG stat.ME</categories><comments>40 pages, 11 figures, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a large literature explaining why AdaBoost is a successful
classifier. The literature on AdaBoost focuses on classifier margins and
boosting's interpretation as the optimization of an exponential likelihood
function. These existing explanations, however, have been pointed out to be
incomplete. A random forest is another popular ensemble method for which there
is substantially less explanation in the literature. We introduce a novel
perspective on AdaBoost and random forests that proposes that the two
algorithms work for similar reasons. While both classifiers achieve similar
predictive accuracy, random forests cannot be conceived as a direct
optimization procedure. Rather, random forests is a self-averaging,
interpolating algorithm which creates what we denote as a &quot;spikey-smooth&quot;
classifier, and we view AdaBoost in the same light. We conjecture that both
AdaBoost and random forests succeed because of this mechanism. We provide a
number of examples and some theoretical justification to support this
explanation. In the process, we question the conventional wisdom that suggests
that boosting algorithms for classification require regularization or early
stopping and should be limited to low complexity classes of learners, such as
decision stumps. We conclude that boosting should be used like random forests:
with large decision trees and without direct regularization or early stopping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07678</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07678</id><created>2015-04-28</created><authors><author><keyname>Huang</keyname><forenames>Hongzhao</forenames></author><author><keyname>Heck</keyname><forenames>Larry</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author></authors><title>Leveraging Deep Neural Networks and Knowledge Graphs for Entity
  Disambiguation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity Disambiguation aims to link mentions of ambiguous entities to a
knowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for
this task based on the assumption that information from the same semantic
context tends to belong to the same topic. This paper presents a novel deep
semantic relatedness model (DSRM) based on deep neural networks (DNN) and
semantic knowledge graphs (KGs) to measure entity semantic relatedness for
topical coherence modeling. The DSRM is directly trained on large-scale KGs and
it maps heterogeneous types of knowledge of an entity from KGs to numerical
feature vectors in a latent space such that the distance between two
semantically-related entities is minimized. Compared with the state-of-the-art
relatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains
19.4% and 24.5% reductions in entity disambiguation errors on two publicly
available datasets respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07680</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07680</id><created>2015-04-28</created><updated>2015-09-11</updated><authors><author><keyname>Dunfield</keyname><forenames>Joshua</forenames></author></authors><title>Elaborating Evaluation-Order Polymorphism</title><categories>cs.PL</categories><comments>13 pages + appendix; in International Conference on Functional
  Programming (ICFP) 2015. arXiv version 3 makes corrections to Figure 5 and
  Section 2.4, preserving page breaks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify programming languages according to evaluation order: each
language fixes one evaluation order as the default, making it transparent to
program in that evaluation order, and troublesome to program in the other.
  This paper develops a type system that is impartial with respect to
evaluation order. Evaluation order is implicit in terms, and explicit in types,
with by-value and by-name versions of type connectives. A form of intersection
type quantifies over evaluation orders, describing code that is agnostic over
(that is, polymorphic in) evaluation order. By allowing such generic code,
programs can express the by-value and by-name versions of a computation without
code duplication.
  We also formulate a type system that only has by-value connectives, plus a
type that generalizes the difference between by-value and by-name connectives:
it is either a suspension (by name) or a &quot;no-op&quot; (by value). We show a
straightforward encoding of the impartial type system into the more economical
one. Then we define an elaboration from the economical language to a
call-by-value semantics, and prove that elaborating a well-typed source
program, where evaluation order is implicit, produces a well-typed target
program where evaluation order is explicit. We also prove a simulation between
evaluation of the target program and reductions (either by-value or by-name) in
the source program.
  Finally, we prove that typing, elaboration, and evaluation are faithful to
the type annotations given in the source program: if the programmer only writes
by-value types, no by-name reductions can occur at run time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07682</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07682</id><created>2015-04-28</created><updated>2015-09-27</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Ross</keyname><forenames>Nathan</forenames></author></authors><title>Shotgun assembly of labeled graphs</title><categories>math.PR cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing graphs or labeled graphs from
neighborhoods of a given radius r. Special instances of this problem include
DNA shotgun assembly, neural network reconstruction, and assembling random
jigsaw puzzles. We provide some necessary and some sufficient conditions for
correct recovery both in combinatorial terms and for some generative models
including random labelings of lattices, Erdos-Renyi random graphs, and the
random jigsaw puzzle model. Many open problems and conjectures are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07685</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07685</id><created>2015-04-28</created><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Knauer</keyname><forenames>Christian</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author><author><keyname>Wenk</keyname><forenames>Carola</forenames></author></authors><title>Fr\'echet Distance for Curves, Revisited</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\eps}{{\varepsilon}} \newcommand{\SetX}{\mathsf{X}}
\newcommand{\VorX}[1]{\mathcal{V} \pth{#1}} \newcommand{\Polygon}{\mathsf{P}}
\newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}$ We revisit the problem of
computing Fr\'echet distance between polygonal curves under $L_1$, $L_2$, and
$L_\infty$ norms, focusing on discrete Fr\'echet distance, where only distance
between vertices is considered. We develop efficient algorithms for two natural
classes of curves. In particular, given two polygonal curves of $n$ vertices
each, a $\eps$-approximation of their discrete Fr\'echet distance can be
computed in roughly $O(n\kappa^3\log n/\eps^3)$ time in three dimensions, if
one of the curves is \emph{$\kappa$-bounded}. Previously, only a
$\kappa$-approximation algorithm was known. If both curves are the so-called
\emph{\backbone~curves}, which are widely used to model protein backbones in
molecular biology, we can $\eps$-approximate their Fr\'echet distance in near
linear time in two dimensions, and in roughly $O(n^{4/3}\log nm)$ time in three
dimensions. In the second part, we propose a pseudo--output-sensitive algorithm
for computing Fr\'echet distance exactly. The complexity of the algorithm is a
function of a quantity we call the \emph{\bwnumber{}}, which is quadratic in
the worst case, but tends to be much smaller in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07687</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07687</id><created>2015-04-28</created><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Public projects, Boolean functions and the borders of Border's theorem</title><categories>cs.GT cs.CC</categories><comments>Accepted to ACM EC 2015</comments><msc-class>68Q17, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Border's theorem gives an intuitive linear characterization of the feasible
interim allocation rules of a Bayesian single-item environment, and it has
several applications in economic and algorithmic mechanism design. All known
generalizations of Border's theorem either restrict attention to relatively
simple settings, or resort to approximation. This paper identifies a
complexity-theoretic barrier that indicates, assuming standard complexity class
separations, that Border's theorem cannot be extended significantly beyond the
state-of-the-art. We also identify a surprisingly tight connection between
Myerson's optimal auction theory, when applied to public project settings, and
some fundamental results in the analysis of Boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07697</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07697</id><created>2015-04-28</created><authors><author><keyname>Narayanan</keyname><forenames>Anand Kumar</forenames></author></authors><title>Polynomial Factorization over Finite Fields By Computing Euler-Poincare
  Characteristics of Drinfeld Modules</title><categories>cs.CC cs.DM cs.DS math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and rigorously analyze two randomized algorithms to factor
univariate polynomials over finite fields using rank $2$ Drinfeld modules. The
first algorithm estimates the degree of an irreducible factor of a polynomial
from Euler-Poincare characteristics of random Drinfeld modules. Knowledge of a
factor degree allows one to rapidly extract all factors of that degree. As a
consequence, the problem of factoring polynomials over finite fields in time
nearly linear in the degree is reduced to finding Euler-Poincare
characteristics of random Drinfeld modules with high probability. Notably, the
worst case complexity of polynomial factorization over finite fields is reduced
to the average case complexity of a problem concerning Drinfeld modules. The
second algorithm is a random Drinfeld module analogue of Berlekamp's algorithm.
During the course of its analysis, we prove a new bound on degree distributions
in factorization patterns of polynomials over finite fields in certain short
intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07702</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07702</id><created>2015-04-28</created><updated>2015-11-25</updated><authors><author><keyname>Balkan</keyname><forenames>Ayca</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Controller Synthesis for Mode-Target Games</title><categories>math.OC cs.GT cs.LO cs.SY</categories><comments>Corrected a typo in equation (4.2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-Physical Systems (CPS) are notoriously difficult to verify due to the
intricate interactions between the cyber and the physical components. To
address this difficulty, several researchers have argued that the synthesis
paradigm is better suited to ensure the correct operation of CPS than the
verification paradigm. The key insight of synthesis is that design should be
constrained so that resulting systems are easily verified and, ideally,
synthesis algorithms should directly provide a proof of correctness.
  In this paper we present a Linear Temporal Logic fragment inspired by
specifications that frequently occur in control applications where we have a
set of modes and corresponding targets to be reached for each mode. The
synthesis problem for this fragment is formulated as a mode-target game and we
show that these games can be solved in polynomial time by providing two
embeddings of mode-target games into Generalized Reactivity(1) (GR(1)) games.
While solving GR(1) games requires $O(mnN^2)$ symbolic steps when we have m
assumptions, n guarantees, and a game graph with N states, mode-target games
can be solved in $O(nN^2)$ symbolic steps when we have n modes and a game graph
with N states. These embeddings, however, do not make full use of the
specificity of mode-target games. For this reason we investigate in this paper
a solution to mode-target games that does not rely on GR(1) embeddings. The
resulting algorithm has the same worst case time complexity and we illustrate
through experimental results the extent to which it improves upon the
algorithms obtained via GR(1) embeddings. In doing so, we highlight the
commonalities between mode-target games and GR(1) games while providing
additional insight into the solution of GR(1) games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07704</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07704</id><created>2015-04-28</created><authors><author><keyname>Heorhiadi</keyname><forenames>Victor</forenames></author><author><keyname>Reiter</keyname><forenames>Michael K.</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author></authors><title>Accelerating the Development of Software-Defined Network Optimization
  Applications Using SOL</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-defined networking (SDN) can enable diverse network management
applications such as traffic engineering, service chaining, network function
outsourcing, and topology reconfiguration. Realizing the benefits of SDN for
these applications, however, entails addressing complex network optimizations
that are central to these problems. Unfortunately, such optimization problems
require significant manual effort and expertise to express and non-trivial
computation and/or carefully crafted heuristics to solve. Our vision is to
simplify the deployment of SDN applications using general high-level
abstractions for capturing optimization requirements from which we can
efficiently generate optimal solutions. To this end, we present SOL, a
framework that demonstrates that it is indeed possible to simultaneously
achieve generality and efficiency. The insight underlying SOL is that SDN
applications can be recast within a unifying path-based optimization
abstraction, from which it efficiently generates near-optimal solutions, and
device configurations to implement those solutions. We illustrate the
generality of SOL by prototyping diverse and new applications. We show that SOL
simplifies the development of SDN-based network optimization applications and
provides comparable or better scalability than custom optimization solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07713</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07713</id><created>2015-04-28</created><authors><author><keyname>Halimi</keyname><forenames>Oualid El</forenames></author><author><keyname>Patel</keyname><forenames>Trith</forenames></author><author><keyname>Kiyani</keyname><forenames>Zohaib S.</forenames></author><author><keyname>Kumar</keyname><forenames>Naresh.</forenames></author><author><keyname>Singh</keyname><forenames>Ankit</forenames></author></authors><title>Comparative Stability of Cloned and Non-cloned Code: A Replication Study</title><categories>cs.SE</categories><comments>10 pages, 11 Tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code cloning is an important software engineering aspect. It is a common
software reuse principle that consists of duplicating source code within a
program or across different systems owned or maintained by the same entity.
There are several contradictory claims concerning the impact of cloning on
software stability and maintenance effort. Some papers state that cloning is
desired since it speeds up the development process and helps stakeholders meet
the tight schedule and deliver on time. Other papers argue that code clone
leads to code bloat and causes increase software maintenance costs due to
copied defects and dead code. In this paper, we are replicating a previous
study done on cloning by the original author. We are repeating his work using
the same methods and metrics but with different subjects and experimenters. The
paper we are addressing evaluates the impact of code cloning on code stability
using three different stability-measuring methods. Our team will apply the same
stability measurement techniques on a different software system developed in C
programming language to determine generalizability, assure that the results are
reliable, validate their outcomes, and to inspire new search by combining
previous findings from related studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07720</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07720</id><created>2015-04-29</created><authors><author><keyname>Krichene</keyname><forenames>Walid</forenames></author></authors><title>Dual Averaging on Compactly-Supported Distributions And Application to
  No-Regret Learning on a Continuum</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an online learning problem on a continuum. A decision maker is
given a compact feasible set $S$, and is faced with the following sequential
problem: at iteration~$t$, the decision maker chooses a distribution $x^{(t)}
\in \Delta(S)$, then a loss function $\ell^{(t)} : S \to \mathbb{R}_+$ is
revealed, and the decision maker incurs expected loss $\langle \ell^{(t)},
x^{(t)} \rangle = \mathbb{E}_{s \sim x^{(t)}} \ell^{(t)}(s)$. We view the
problem as an online convex optimization problem on the space $\Delta(S)$ of
Lebesgue-continnuous distributions on $S$. We prove a general regret bound for
the Dual Averaging method on $L^2(S)$, then prove that dual averaging with
$\omega$-potentials (a class of strongly convex regularizers) achieves
sublinear regret when $S$ is uniformly fat (a condition weaker than convexity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07728</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07728</id><created>2015-04-29</created><authors><author><keyname>Dong</keyname><forenames>Jie</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Hanlen</keyname><forenames>Leif</forenames></author></authors><title>Socially Optimal Coexistence of Wireless Body Area Networks Enabled by a
  Non-Cooperative Game</title><categories>cs.NI</categories><comments>11 figures, 16 pages, 2 tables, ACM Transactions on Sensor Network</comments><acm-class>C.2.1; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we enable the coexistence of multiple wireless body area
networks (BANs) using a finite repeated non-cooperative game for transmit power
control. With no coordination amongst these personal sensor networks, the
proposed game maximizes each network's packet delivery ratio (PDR) at low
transmit power. In this context we provide a novel utility function, which
gives reduced benefit to players with higher transmission power, and a
subsequent reduction in radio interference to other coexisting BANs.
Considering the purpose of inter-BAN interference mitigation, PDR is expressed
as a compressed exponential function of inverse
signal-to-interference-and-noise ratio (SINR), so it is essentially a function
of transmit powers of all coexisting BANs. It is shown that a unique Nash
Equilibrium (NE) exists, and hence there is a subgame-perfect equilibrium,
considering best-response at each stage independent of history. In addition,
the NE is proven to be the socially optimal solution across all action
profiles. Realistic and extensive on- and inter-body channel models are
employed. Results confirm the effectiveness of the proposed scheme in better
interference management, greater reliability and reduced transmit power, when
compared with other schemes that can be applied in BANs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07737</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07737</id><created>2015-04-29</created><updated>2015-05-08</updated><authors><author><keyname>Liu</keyname><forenames>Wanwei</forenames></author><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>A Simple Probabilistic Extension of Modal Mu-calculus</title><categories>cs.LO</categories><acm-class>F.4.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic systems are an important theme in AI domain. As the
specification language, the logic PCTL is now the default logic for reasoning
about probabilistic properties. In this paper, we present a natural and
succinct probabilistic extension of mu-calculus, another prominent logic in the
concurrency theory. We study the relationship with PCTL. Interestingly, the
expressiveness is highly orthogonal with PCTL. The proposed logic captures some
useful properties which cannot be expressed in PCTL. We investigate the model
checking and satisfiability problem, and show that the model checking problem
is in UP and co-UP, and the satisfiability checking can be decided via reducing
into solving parity games. This is in contrast to PCTL as well, whose
satisfiability checking is still an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07738</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07738</id><created>2015-04-29</created><authors><author><keyname>Du</keyname><forenames>Liping</forenames></author><author><keyname>Laghate</keyname><forenames>Mihir</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hao</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Improved Eigenvalue-based Spectrum Sensing via Sensor Signal Overlapping</title><categories>cs.IT cs.NI math.IT</categories><comments>conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eigenvalue-based detectors are considered as an important method of spectrum
sensing since they do not require the information about the primary user (PU)
signal. In this paper we propose a method to improve the performance of the
eigenvalue-based detector. The proposed method introduces a new test statistic
based on combinatorial matrix with components which are overlapping subgroups
extracted from the array of received signals. As a result, its covariance
matrix has a larger maximum eigenvalue and trace value than the one without
overlapping. Simulation results show that our proposed method can further
improve the detection performance of the optimal eigenvalue-based detector. The
paper also shows the effect of different overlapping methods on the receiver
operating characteristic curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07751</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07751</id><created>2015-04-29</created><updated>2015-05-12</updated><authors><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Dai</keyname><forenames>Xuchu</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>NOMA: An Information Theoretic Perspective</title><categories>cs.IT math.IT</categories><comments>One typo in Eq. (3) in v1 has been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the performance of non-orthogonal multiple access (NOMA) is
investigated from an information theoretic perspective. The relationships among
the capacity region of broadcast channels and two rate regions achieved by NOMA
and time-division multiple access (TDMA) are illustrated first. Then, the
performance of NOMA is evaluated by considering TDMA as the benchmark, where
both the sum rate and the individual user rates are used as the criteria. In a
wireless downlink scenario with user pairing, the developed analytical results
show that NOMA can outperform TDMA not only for the sum rate but also for each
user's individual rate, particularly when the difference between the users'
channels is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07753</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07753</id><created>2015-04-29</created><authors><author><keyname>Sloan</keyname><forenames>Robert H.</forenames></author><author><keyname>Stasi</keyname><forenames>Despina</forenames></author><author><keyname>Turan</keyname><forenames>Gyorgy</forenames></author></authors><title>Hydras: Directed Hypergraphs and Horn Formulas</title><categories>cs.DM cs.LO math.CO</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new graph parameter, the hydra number, arising from the
minimization problem for Horn formulas in propositional logic. The hydra number
of a graph $G=(V,E)$ is the minimal number of hyperarcs of the form
$u,v\rightarrow w$ required in a directed hypergraph $H=(V,F)$, such that for
every pair $(u, v)$, the set of vertices reachable in $H$ from $\{u, v\}$ is
the entire vertex set $V$ if $(u, v) \in E$, and it is $\{u, v\}$ otherwise.
Here reachability is defined by forward chaining, a standard marking algorithm.
  Various bounds are given for the hydra number. We show that the hydra number
of a graph can be upper bounded by the number of edges plus the path cover
number of the line graph of a spanning subgraph, which is a sharp bound in
several cases. On the other hand, we construct single-headed graphs for which
that bound is off by a constant factor. Furthermore, we characterize trees with
low hydra number, and give a lower bound for the hydra number of trees based on
the number of vertices that are leaves in the tree obtained from $T$ by
deleting its leaves. This bound is sharp for some families of trees. We give
bounds for the hydra number of complete binary trees and also discuss a related
minimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07758</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07758</id><created>2015-04-29</created><authors><author><keyname>Debattista</keyname><forenames>Jeremy</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Luzzu Quality Metric Language -- A DSL for Linked Data Quality
  Assessment</title><categories>cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1412.3750</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The steadily growing number of linked open datasets brought about a number of
reservations amongst data consumers with regard to the datasets' quality.
Quality assessment requires significant effort and consideration, including the
definition of data quality metrics and a process to assess datasets based on
these definitions. Luzzu is a quality assessment framework for linked data that
allows domain-specific metrics to be plugged in. LQML offers notations,
abstractions and expressive power, focusing on the representation of quality
metrics. It provides expressive power for defining sophisticated quality
metrics. Its integration with Luzzu enables their efficient processing and
execution and thus the comprehensive assessment of extremely large datasets in
a streaming way. We also describe a novel ontology that enables the reuse,
sharing and querying of such definitions. Finally, we evaluate the proposed DSL
against the cognitive dimensions of notation framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07766</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07766</id><created>2015-04-29</created><authors><author><keyname>Del Corso</keyname><forenames>Gianna M.</forenames></author><author><keyname>Romani</keyname><forenames>Francesco</forenames></author></authors><title>A multi-class approach for ranking graph nodes: models and experiments
  with incomplete data</title><categories>math.NA cs.IR physics.soc-ph</categories><msc-class>65F15</msc-class><acm-class>G.2.2; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After the phenomenal success of the PageRank algorithm, many researchers have
extended the PageRank approach to ranking graphs with richer structures beside
the simple linkage structure. In some scenarios we have to deal with
multi-parameters data where each node has additional features and there are
relationships between such features.
  This paper stems from the need of a systematic approach when dealing with
multi-parameter data. We propose models and ranking algorithms which can be
used with little adjustments for a large variety of networks (bibliographic
data, patent data, twitter and social data, healthcare data). In this paper we
focus on several aspects which have not been addressed in the literature: (1)
we propose different models for ranking multi-parameters data and a class of
numerical algorithms for efficiently computing the ranking score of such
models, (2) by analyzing the stability and convergence properties of the
numerical schemes we tune a fast and stable technique for the ranking problem,
(3) we consider the issue of the robustness of our models when data are
incomplete. The comparison of the rank on the incomplete data with the rank on
the full structure shows that our models compute consistent rankings whose
correlation is up to 60% when just 10% of the links of the attributes are
maintained suggesting the suitability of our model also when the data are
incomplete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07786</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07786</id><created>2015-04-29</created><updated>2015-10-03</updated><authors><author><keyname>Liu</keyname><forenames>Yunsong</forenames></author><author><keyname>Zhan</keyname><forenames>Zhifang</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Di</forenames></author><author><keyname>Chen</keyname><forenames>Zhong</forenames></author><author><keyname>Qu</keyname><forenames>Xiaobo</forenames></author></authors><title>Projected Iterative Soft-thresholding Algorithm for Tight Frames in
  Compressed Sensing Magnetic Resonance Imaging</title><categories>physics.med-ph cs.CV math.OC</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing has shown great potentials in accelerating magnetic
resonance imaging. Fast image reconstruction and high image quality are two
main issues faced by this new technology. It has been shown that, redundant
image representations, e.g. tight frames, can significantly improve the image
quality. But how to efficiently solve the reconstruction problem with these
redundant representation systems is still challenging. This paper attempts to
address the problem of applying iterative soft-thresholding algorithm (ISTA) to
tight frames based magnetic resonance image reconstruction. By introducing the
canonical dual frame to construct the orthogonal projection operator on the
range of the analysis sparsity operator, we propose a projected iterative
soft-thresholding algorithm (pISTA) and further accelerate it by incorporating
the strategy proposed by Beck and Teboulle in 2009. We theoretically prove that
pISTA converges to the minimum of a function with a balanced tight frame
sparsity. Experimental results demonstrate that the proposed algorithm achieves
better reconstruction than the widely used synthesis sparse model and the
accelerated pISTA converges faster or comparable to the state-of-art smoothing
FISTA. One major advantage of pISTA is that only one extra parameter, the step
size, is introduced and the numerical solution is stable to it in terms of
image reconstruction errors, thus allowing easily setting in many fast magnetic
resonance imaging applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07791</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07791</id><created>2015-04-29</created><updated>2015-04-30</updated><authors><author><keyname>Kang</keyname><forenames>Yangyang</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author></authors><title>On the Global Convergence of Majorization Minimization Algorithms for
  Nonconvex Optimization Problems</title><categories>cs.NA math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we study the global convergence of majorization minimization
(MM) algorithms for solving nonconvex regularized optimization problems. MM
algorithms have received great attention in machine learning. However, when
applied to nonconvex optimization problems, the convergence of MM algorithms is
a challenging issue. We introduce theory of the Kurdyka- Lojasiewicz inequality
to address this issue. In particular, we show that many nonconvex problems
enjoy the Kurdyka- Lojasiewicz property and establish the global convergence
result of the corresponding MM procedure. We also extend our result to a well
known method that called CCCP (concave-convex procedure).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07795</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07795</id><created>2015-04-29</created><authors><author><keyname>Itani</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Schiller</keyname><forenames>Ulf D.</forenames></author><author><keyname>Schmieschek</keyname><forenames>Sebastian</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Chandrashekar</keyname><forenames>Hoskote</forenames></author><author><keyname>Robertson</keyname><forenames>Fergus</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author><author><keyname>Groen</keyname><forenames>Derek</forenames></author></authors><title>An automated multiscale ensemble simulation approach for vascular blood
  flow</title><categories>cs.DC cs.CE</categories><comments>Journal of Computational Science (in press), 10 pages, 6 figures, 2
  tables</comments><doi>10.1016/j.jocs.2015.04.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cerebrovascular diseases such as brain aneurysms are a primary cause of adult
disability. The flow dynamics in brain arteries, both during periods of rest
and increased activity, are known to be a major factor in the risk of aneurysm
formation and rupture. The precise relation is however still an open field of
investigation. We present an automated ensemble simulation method for modelling
cerebrovascular blood flow under a range of flow regimes. By automatically
constructing and performing an ensemble of multiscale simulations, where we
unidirectionally couple a 1D solver with a 3D lattice-Boltzmann code, we are
able to model the blood flow in a patient artery over a range of flow regimes.
We apply the method to a model of a middle cerebral artery, and find that this
approach helps us to fine-tune our modelling techniques, and opens up new ways
to investigate cerebrovascular flow properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07825</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07825</id><created>2015-04-29</created><authors><author><keyname>Swamy</keyname><forenames>Peruru Subrahmanya</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Jagannathan</keyname><forenames>Krishna</forenames></author></authors><title>Spatial CSMA: A Distributed Scheduling Algorithm for the SIR Model with
  Time-varying Channels</title><categories>cs.NI</categories><comments>This work has been presented at National Conference on Communication,
  2015, held at IIT Bombay, Mumbai, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that adaptive CSMA algorithms can achieve throughput
optimality. However, these adaptive CSMA algorithms assume a rather simplistic
model for the wireless medium. Specifically, the interference is typically
modelled by a conflict graph, and the channels are assumed to be static. In
this work, we propose a distributed and adaptive CSMA algorithm under a more
realistic signal-to-interference ratio (SIR) based interference model, with
time-varying channels. We prove that our algorithm is throughput optimal under
this generalized model. Further, we augment our proposed algorithm by using a
parallel update technique. Numerical results show that our algorithm
outperforms the conflict graph based algorithms, in terms of supportable
throughput and the rate of convergence to steady-state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07828</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07828</id><created>2015-04-29</created><authors><author><keyname>Kapanowski</keyname><forenames>A.</forenames></author><author><keyname>Ga&#x142;uszka</keyname><forenames>&#x141;.</forenames></author></authors><title>Weighted graph algorithms with Python</title><categories>cs.DS</categories><comments>26 pages, no figures</comments><journal-ref>The Python Papers 11, 3 (2016)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Python implementation of selected weighted graph algorithms is presented. The
minimal graph interface is defined together with several classes implementing
this interface. Graph nodes can be any hashable Python objects. Directed edges
are instances of the Edge class. Graphs are instances of the Graph class. It is
based on the adjacency-list representation, but with fast lookup of nodes and
neighbors (dict-of-dict structure). Other implementations of this class are
also possible.
  In this work, many algorithms are implemented using a unified approach. There
are separate classes and modules devoted to different algorithms. Three
algorithms for finding a minimum spanning tree are implemented: the Boruvka's
algorithm, the Prim's algorithm (three implementations), and the Kruskal's
algorithm. Three algorithms for solving the single-source shortest path problem
are implemented: the dag shortest path algorithm, the Bellman-Ford algorithm,
and the Dijkstra's algorithm (two implementations). Two algorithms for solving
all-pairs shortest path problem are implemented: the Floyd-Warshall algorithm
and the Johnson's algorithm.
  All algorithms were tested by means of the unittest module, the Python unit
testing framework. Additional computer experiments were done in order to
compare real and theoretical computational complexity. The source code is
available from the public GitHub repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07829</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07829</id><created>2015-04-29</created><authors><author><keyname>Rebagliati</keyname><forenames>Sara</forenames></author><author><keyname>Sasso</keyname><forenames>Emanuela</forenames></author><author><keyname>Soraggi</keyname><forenames>Samuele</forenames></author></authors><title>Market forecasting using Hidden Markov Models</title><categories>stat.ML cs.LG</categories><comments>22 pages</comments><msc-class>91B84</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working on the daily closing prices and logreturns, in this paper we deal
with the use of Hidden Markov Models (HMMs) to forecast the price of the
EUR/USD Futures. The aim of our work is to understand how the HMMs describe
different financial time series depending on their structure. Subsequently, we
analyse the forecasting methods exposed in the previous literature, putting on
evidence their pros and cons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07830</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07830</id><created>2015-04-29</created><updated>2015-06-07</updated><authors><author><keyname>Hirai</keyname><forenames>Hiroshi</forenames></author><author><keyname>Iwamasa</keyname><forenames>Yuni</forenames></author></authors><title>On k-Submodular Relaxation</title><categories>math.OC cs.CC cs.DS</categories><comments>10 pages, corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $k$-submodular functions, introduced by Huber and Kolmogorov, are functions
defined on $\{0, 1, 2, \dots, k\}^n$ satisfying certain submodular-type
inequalities. $k$-submodular functions typically arise as relaxations of
NP-hard problems, and the relaxations by $k$-submodular functions play key
roles in design of efficient, approximation, or FPT algorithms. Motivated by
this, we consider the following problem: Given a function $f : \{1, 2, \dots,
k\}^n \rightarrow \mathbb{R} \cup \{\infty\}$, determine whether $f$ is
extended to a $k$-submodular function $g : \{0, 1, 2, \dots, k\}^n \rightarrow
\mathbb{R} \cup \{\infty\}$, where $g$ is called a $k$-submodular relaxation of
$f$.
  We give a polymorphic characterization of those functions which admit a
$k$-submodular relaxation, and also give a combinatorial $O((k^n)^2)$ time
algorithm to find a $k$-submodular relaxation or establish that a
$k$-submodular relaxation does not exist. Our algorithm has interesting
properties: (1) if the input function is integer-valued, then our algorithm
outputs a half-integral relaxation, and (2) if the input function is binary,
then our algorithm outputs the unique optimal relaxation. We present
applications of our algorithm to valued constraint satisfaction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07831</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07831</id><created>2015-04-29</created><authors><author><keyname>Yao</keyname><forenames>Ting</forenames></author><author><keyname>Shi</keyname><forenames>Minjia</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Skew Cyclic codes over $\F_q+u\F_q+v\F_q+uv\F_q$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study skew cyclic codes over the ring
$R=\F_q+u\F_q+v\F_q+uv\F_q$, where $u^{2}=u,v^{2}=v,uv=vu$, $q=p^{m}$ and $p$
is an odd prime. We investigate the structural properties of skew cyclic codes
over $R$ through a decomposition theorem. Furthermore, we give a formula for
the number of skew cyclic codes of length $n$ over $R.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07834</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07834</id><created>2015-04-29</created><authors><author><keyname>Bosman</keyname><forenames>Thomas</forenames></author></authors><title>A Solution Merging Heuristic for the Steiner Problem in Graphs Using
  Tree Decompositions</title><categories>cs.DS</categories><journal-ref>SEA 2015, LNCS 9125, pp. 391-402, 2015</journal-ref><doi>10.1007/978-3-319-20086-6_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed parameter tractable algorithms for bounded treewidth are known to exist
for a wide class of graph optimization problems. While most research in this
area has been focused on exact algorithms, it is hard to find decompositions of
treewidth sufficiently small to make these al- gorithms fast enough for
practical use. Consequently, tree decomposition based algorithms have limited
applicability to large scale optimization. However, by first reducing the input
graph so that a small width tree decomposition can be found, we can harness the
power of tree decomposi- tion based techniques in a heuristic algorithm, usable
on graphs of much larger treewidth than would be tractable to solve exactly. We
propose a solution merging heuristic to the Steiner Tree Problem that applies
this idea. Standard local search heuristics provide a natural way to generate
subgraphs with lower treewidth than the original instance, and subse- quently
we extract an improved solution by solving the instance induced by this
subgraph. As such the fixed parameter tractable algorithm be- comes an e?cient
tool for our solution merging heuristic. For a large class of sparse benchmark
instances the algorithm is able to find small width tree decompositions on the
union of generated solutions. Subsequently it can often improve on the
generated solutions fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07838</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07838</id><created>2015-04-29</created><authors><author><keyname>Bene&#x161;</keyname><forenames>Nikola</forenames></author><author><keyname>Bezd&#x11b;k</keyname><forenames>Peter</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Srba</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Language Emptiness of Continuous-Time Parametric Timed Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric timed automata extend the standard timed automata with the
possibility to use parameters in the clock guards. In general, if the
parameters are real-valued, the problem of language emptiness of such automata
is undecidable even for various restricted subclasses. We thus focus on the
case where parameters are assumed to be integer-valued, while the time still
remains continuous. On the one hand, we show that the problem remains
undecidable for parametric timed automata with three clocks and one parameter.
On the other hand, for the case with arbitrary many clocks where only one of
these clocks is compared with (an arbitrary number of) parameters, we show that
the parametric language emptiness is decidable. The undecidability result
tightens the bounds of a previous result which assumed six parameters, while
the decidability result extends the existing approaches that deal with
discrete-time semantics only. To the best of our knowledge, this is the first
positive result in the case of continuous-time and unbounded integer
parameters, except for the rather simple case of single-clock automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07843</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07843</id><created>2015-04-29</created><authors><author><keyname>Youn</keyname><forenames>Hyejin</forenames></author><author><keyname>Sutton</keyname><forenames>Logan</forenames></author><author><keyname>Smith</keyname><forenames>Eric</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Wilkins</keyname><forenames>Jon F.</forenames></author><author><keyname>Maddieson</keyname><forenames>Ian</forenames></author><author><keyname>Croft</keyname><forenames>William</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Tanmoy</forenames></author></authors><title>On the universal structure of human lexical semantics</title><categories>physics.soc-ph cs.CL</categories><comments>Press embargo in place until publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How universal is human conceptual structure? The way concepts are organized
in the human brain may reflect distinct features of cultural, historical, and
environmental background in addition to properties universal to human
cognition. Semantics, or meaning expressed through language, provides direct
access to the underlying conceptual structure, but meaning is notoriously
difficult to measure, let alone parameterize. Here we provide an empirical
measure of semantic proximity between concepts using cross-linguistic
dictionaries. Across languages carefully selected from a phylogenetically and
geographically stratified sample of genera, translations of words reveal cases
where a particular language uses a single polysemous word to express concepts
represented by distinct words in another. We use the frequency of polysemies
linking two concepts as a measure of their semantic proximity, and represent
the pattern of such linkages by a weighted network. This network is highly
uneven and fragmented: certain concepts are far more prone to polysemy than
others, and there emerge naturally interpretable clusters loosely connected to
each other. Statistical analysis shows such structural properties are
consistent across different language groups, largely independent of geography,
environment, and literacy. It is therefore possible to conclude the conceptual
structure connecting basic vocabulary studied is primarily due to universal
features of human cognition and language use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07844</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07844</id><created>2015-04-29</created><updated>2015-06-25</updated><authors><author><keyname>Gladisch</keyname><forenames>Stefan</forenames></author><author><keyname>Kister</keyname><forenames>Ulrike</forenames></author><author><keyname>Tominski</keyname><forenames>Christian</forenames></author><author><keyname>Dachselt</keyname><forenames>Raimund</forenames></author><author><keyname>Schumann</keyname><forenames>Heidrun</forenames></author></authors><title>Mapping Tasks to Interactions for Graph Exploration and Graph Editing on
  Interactive Surfaces</title><categories>cs.HC</categories><comments>21 pages, minor corrections (typos etc.)</comments><msc-class>-</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph exploration and editing are still mostly considered independently and
systems to work with are not designed for todays interactive surfaces like
smartphones, tablets or tabletops. When developing a system for those modern
devices that supports both graph exploration and graph editing, it is necessary
to 1) identify what basic tasks need to be supported, 2) what interactions can
be used, and 3) how to map these tasks and interactions. This technical report
provides a list of basic interaction tasks for graph exploration and editing as
a result of an extensive system review. Moreover, different interaction
modalities of interactive surfaces are reviewed according to their interaction
vocabulary and further degrees of freedom that can be used to make interactions
distinguishable are discussed. Beyond the scope of graph exploration and
editing, we provide an approach for finding and evaluating a mapping from tasks
to interactions, that is generally applicable. Thus, this work acts as a
guideline for developing a system for graph exploration and editing that is
specifically designed for interactive surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07846</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07846</id><created>2015-04-29</created><updated>2015-05-05</updated><authors><author><keyname>Ahuja</keyname><forenames>Nitin</forenames></author><author><keyname>Bender</keyname><forenames>Matthias</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>Wagner</keyname><forenames>Andreas</forenames></author></authors><title>Incorporating Road Networks into Territory Design</title><categories>math.OC cs.DS cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of basic areas, the territory design problem asks to create a
predefined number of territories, each containing at least one basic area, such
that an objective function is optimized. Desired properties of territories
often include a reasonable balance, compact form, contiguity and small average
journey times which are usually encoded in the objective function or formulated
as constraints. We address the territory design problem by developing graph
theoretic models that also consider the underlying road network. The derived
graph models enable us to tackle the territory design problem by modifying
graph partitioning algorithms and mixed integer programming formulations so
that the objective of the planning problem is taken into account. We test and
compare the algorithms on several real world instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07851</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07851</id><created>2015-04-29</created><updated>2015-05-11</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Cording</keyname><forenames>Patrick Hagge</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author><author><keyname>Skjoldjensen</keyname><forenames>Frederik Rye</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author><author><keyname>Vind</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>Dynamic Relative Compression, Dynamic Partial Sums, and Substring
  Concatenation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a static reference string $R$ and a source string $S$, a relative
compression of $S$ with respect to $R$ is an encoding of $S$ as a sequence of
references to substrings of $R$. Relative compression schemes are a classic
model of compression and have recently proved very successful for compressing
highly-repetitive massive data set such as genomes and web-data. We initiate
the study of relative compression in a dynamic setting where the compressed
source string $S$ is subject to edit operations. The goal is to maintain the
compressed representation compactly, while supporting edits and allowing
efficient random access to the (uncompressed) source string. We present new
data structures, that achieve optimal time for updates and queries while using
space linear in the size of the optimal relative compression, for nearly all
combination of parameters. We also present solution for restricted or extended
sets of updates. To achieve these results, we revisit the dynamic partial sums
problem and the substring concatenation problem. We present new optimal or near
optimal bounds for these problems. Plugging in our new results we also
immediately obtain new bounds for the string indexing for patterns with
wildcards problem and the dynamic text and static pattern matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07857</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07857</id><created>2015-04-29</created><updated>2015-05-01</updated><authors><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Pastor</keyname><forenames>Peter</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author><author><keyname>Billard</keyname><forenames>Aude</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Probabilistic Depth Image Registration incorporating Nonvisual
  Information</title><categories>cs.RO cs.CV</categories><doi>10.1109/ICRA.2012.6225179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive a probabilistic registration algorithm for object
modeling and tracking. In many robotics applications, such as manipulation
tasks, nonvisual information about the movement of the object is available,
which we will combine with the visual information. Furthermore we do not only
consider observations of the object, but we also take space into account which
has been observed to not be part of the object. Furthermore we are computing a
posterior distribution over the relative alignment and not a point estimate as
typically done in for example Iterative Closest Point (ICP). To our knowledge
no existing algorithm meets these three conditions and we thus derive a novel
registration algorithm in a Bayesian framework. Experimental results suggest
that the proposed methods perform favorably in comparison to PCL
implementations of feature mapping and ICP, especially if nonvisual information
is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07858</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07858</id><created>2015-04-29</created><authors><author><keyname>Guo</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Zixuan</forenames></author><author><keyname>Li</keyname><forenames>Ming</forenames></author><author><keyname>Aghajan</keyname><forenames>Hamid</forenames></author></authors><title>Intelligent Health Recommendation System for Computer Users</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time people spend in front of computers has been increasing steadily due
to the role computers play in modern society. Individuals who sit in front of
computers for an extended period of time, specifically with improper postures
may incur various health issues. In this work, individuals' behaviors in front
of computers are studied using web cameras. By means of non-rigid face tracking
system, data are analyzed to determine the 3D head pose, blink rate and yawn
frequency of computer users. When combining these visual cues, a system of
intelligent personal assistants for computer users is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07860</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07860</id><created>2015-04-29</created><authors><author><keyname>Shi</keyname><forenames>Minjia</forenames></author><author><keyname>Yao</keyname><forenames>Ting</forenames></author><author><keyname>Alahmadi</keyname><forenames>Adel</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Skew cyclic codes over
  $\mathbb{F}_{q}+v\mathbb{F}_{q}+v^{2}\mathbb{F}_{q}$</title><categories>cs.IT math.IT</categories><comments>This manuscript has been accepted by a magazine</comments><doi>10.1587/transfun.E98.A.1845</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study skew cyclic codes over ring
$R=\mathbb{F}_{q}+v\mathbb{F}_{q}+v^{2}\mathbb{F}_{q}$, where $q=p^{m}$, $p$ is
an odd prime and $v^{3}=v$. We describe generator polynomials of skew cyclic
codes over this ring and investigate the structural properties of skew cyclic
codes over $R$ by a decomposition theorem. We also describe the generator
polynomials of the duals of skew cyclic codes. Moreover, the idempotent
generators of skew cyclic codes over $\mathbb{F}_{q}$ and $R$ are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07862</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07862</id><created>2015-04-29</created><authors><author><keyname>Trtik</keyname><forenames>Marek</forenames></author></authors><title>Anonymous On-line Communication Between Program Analyses</title><categories>cs.PL</categories><comments>Technical report / Specification, 50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a light-weight client-server model of communication between
existing implementations of different program analyses. The communication is
on-line and anonymous which means that all analyses simultaneously analyse the
same program and an analysis does not know what other analyses participate in
the communication. The anonymity and model's strong emphasis on independence of
analyses allow to preserve almost everything in existing implementations. An
analysis only has to add an implementation of a proposed communication
protocol, determine places in its code where information from others would
help, and then check whether there is no communication scenario, which would
corrupt its result. We demonstrate functionality and effectiveness of the
proposed communication model in a detailed case study with three analyses: two
abstract interpreters and the classic symbolic execution. Results of the
evaluation on SV-COMP benchmarks show impressive improvements in computed
invariants and increased counts of successfully analysed benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07863</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07863</id><created>2015-04-29</created><updated>2015-10-08</updated><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Using the WOWA operator in robust discrete optimization problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a class of discrete optimization problems with uncertain costs
is discussed. The uncertainty is modeled by introducing a scenario set
containing a finite number of cost scenarios. A probability distribution in the
scenario set is available. In order to choose a solution the weighted OWA
criterion (WOWA) is applied. This criterion allows decision makers to take into
account both probabilities for scenarios and the degree of pessimism/ optimism.
In this paper the complexity of the considered class of discrete optimization
problems is described and some exact and approximation algorithms for solving
it are proposed. An application to a selection problem, together with results
of computational tests are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07865</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07865</id><created>2015-04-29</created><authors><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author><author><keyname>Agrawal</keyname><forenames>Surbhi</forenames></author><author><keyname>R</keyname><forenames>Manikandan.</forenames></author><author><keyname>Bora</keyname><forenames>Kakoli</forenames></author><author><keyname>Routh</keyname><forenames>Swati</forenames></author><author><keyname>Narasimhamurthy</keyname><forenames>Anand</forenames></author></authors><title>ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for
  Data Analytics in Astronomy</title><categories>cs.CE astro-ph.IM cs.LG</categories><comments>Habitability Catalog (HabCat), Supernova classification, data
  analysis, Astroinformatics, Machine learning, ASTROMLS toolkit, Na\&quot;ive
  Bayes, SVD, PCA, Random Forest, SVM, Decision Tree, LDA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astroinformatics is a new impact area in the world of astronomy, occasionally
called the final frontier, where several astrophysicists, statisticians and
computer scientists work together to tackle various data intensive astronomical
problems. Exponential growth in the data volume and increased complexity of the
data augments difficult questions to the existing challenges. Classical
problems in Astronomy are compounded by accumulation of astronomical volume of
complex data, rendering the task of classification and interpretation
incredibly laborious. The presence of noise in the data makes analysis and
interpretation even more arduous. Machine learning algorithms and data analytic
techniques provide the right platform for the challenges posed by these
problems. A diverse range of open problem like star-galaxy separation,
detection and classification of exoplanets, classification of supernovae is
discussed. The focus of the paper is the applicability and efficacy of various
machine learning algorithms like K Nearest Neighbor (KNN), random forest (RF),
decision tree (DT), Support Vector Machine (SVM), Na\&quot;ive Bayes and Linear
Discriminant Analysis (LDA) in analysis and inference of the decision theoretic
problems in Astronomy. The machine learning algorithms, integrated into
ASTROMLSKIT, a toolkit developed in the course of the work, have been used to
analyze HabCat data and supernovae data. Accuracy has been found to be
appreciably good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07874</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07874</id><created>2015-04-29</created><authors><author><keyname>Roldan-Carlos</keyname><forenames>Jennifer</forenames></author><author><keyname>Lux</keyname><forenames>Mathias</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Pia</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Nektarios</forenames></author></authors><title>Visual Information Retrieval in Endoscopic Video Archives</title><categories>cs.IR cs.CV cs.MM</categories><comments>Paper accepted at the IEEE/ACM 13th International Workshop on
  Content-Based Multimedia Indexing (CBMI) in Prague (Czech Republic) between
  10 and 12 June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In endoscopic procedures, surgeons work with live video streams from the
inside of their subjects. A main source for documentation of procedures are
still frames from the video, identified and taken during the surgery. However,
with growing demands and technical means, the streams are saved to storage
servers and the surgeons need to retrieve parts of the videos on demand. In
this submission we present a demo application allowing for video retrieval
based on visual features and late fusion, which allows surgeons to re-find
shots taken during the procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07877</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07877</id><created>2015-04-29</created><updated>2015-06-23</updated><authors><author><keyname>Kemmar</keyname><forenames>Amina</forenames></author><author><keyname>Loudni</keyname><forenames>Samir</forenames></author><author><keyname>Lebbah</keyname><forenames>Yahia</forenames></author><author><keyname>Boizumault</keyname><forenames>Patrice</forenames></author><author><keyname>Charnois</keyname><forenames>Thierry</forenames></author></authors><title>Prefix-Projection Global Constraint for Sequential Pattern Mining</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential pattern mining under constraints is a challenging data mining
task. Many efficient ad hoc methods have been developed for mining sequential
patterns, but they are all suffering from a lack of genericity. Recent works
have investigated Constraint Programming (CP) methods, but they are not still
effective because of their encoding. In this paper, we propose a global
constraint based on the projected databases principle which remedies to this
drawback. Experiments show that our approach clearly outperforms CP approaches
and competes well with ad hoc methods on large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07880</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07880</id><created>2015-04-29</created><authors><author><keyname>Parmentier</keyname><forenames>Axel</forenames></author></authors><title>Algorithms for Non-Linear and Stochastic Resource Constrained Shortest
  Paths</title><categories>cs.DS cs.DM</categories><msc-class>90B99</msc-class><acm-class>G.2.2; F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource constrained shortest path problems are usually solved by label
algorithms, which consist in a smart enumeration of the non-dominated paths.
Recent improvements of these algorithms rely on the use of bounds on path
resources to discard partial solutions. The quality of the bounds determines
the performance of the algorithm. The main contribution of this paper is to
introduce a standard procedure to generate bounds on paths resources in a
general setting which covers most resource constrained shortest path problems,
among which stochastic versions.
  In that purpose, we introduce a generalization of the resource constrained
shortest path problem where the resources are taken in a monoid. The resource
of a path is the monoid sum of the resources of its arcs. The problem consists
in finding a path whose resource minimizes a non-decreasing cost function of
the path resource among the paths that respect a given constraint. Label
algorithms are generalized to this framework. We use lattice theory to provide
a polynomial procedure to find good quality bounds. The efficiency of the
approach is proved through an extensive numerical study in the case of the
Stochastic Resource Constrained Shortest Path problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07883</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07883</id><created>2015-04-29</created><updated>2015-09-22</updated><authors><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>An Optimal Algorithm for Tiling the Plane with a Translated Polyomino</title><categories>cs.CG</categories><comments>In proceedings of ISAAC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a $O(n)$-time algorithm for determining whether translations of a
polyomino with $n$ edges can tile the plane. The algorithm is also a
$O(n)$-time algorithm for enumerating all such tilings that are also regular,
and we prove that at most $\Theta(n)$ such tilings exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07889</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07889</id><created>2015-04-29</created><updated>2015-09-29</updated><authors><author><keyname>Lin</keyname><forenames>Tsung-Yu</forenames></author><author><keyname>RoyChowdhury</keyname><forenames>Aruni</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author></authors><title>Bilinear CNN Models for Fine-grained Visual Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose bilinear models, a recognition architecture that consists of two
feature extractors whose outputs are multiplied using outer product at each
location of the image and pooled to obtain an image descriptor. This
architecture can model local pairwise feature interactions in a translationally
invariant manner which is particularly useful for fine-grained categorization.
It also generalizes various orderless texture descriptors such as the Fisher
vector, VLAD and O2P. We present experiments with bilinear models where the
feature extractors are based on convolutional neural networks. The bilinear
form simplifies gradient computation and allows end-to-end training of both
networks using image labels only. Using networks initialized from the ImageNet
dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the
CUB-200-2011 dataset requiring only category labels at training time. We
present experiments and visualizations that analyze the effects of fine-tuning
and the choice two networks on the speed and accuracy of the models. Results
show that the architecture compares favorably to the existing state of the art
on a number of fine-grained datasets while being substantially simpler and
easier to train. Moreover, our most accurate model is fairly efficient running
at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete
system will be made available at http://vis-www.cs.umass.edu/bcnn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07890</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07890</id><created>2015-04-29</created><authors><author><keyname>Frank</keyname><forenames>Alvaro</forenames></author><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames></author></authors><title>Large-scale linear regression: Development of high-performance routines</title><categories>cs.CE cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistics, series of ordinary least squares problems (OLS) are used to
study the linear correlation among sets of variables of interest; in many
studies, the number of such variables is at least in the millions, and the
corresponding datasets occupy terabytes of disk space. As the availability of
large-scale datasets increases regularly, so does the challenge in dealing with
them. Indeed, traditional solvers---which rely on the use of black-box&quot;
routines optimized for one single OLS---are highly inefficient and fail to
provide a viable solution for big-data analyses. As a case study, in this paper
we consider a linear regression consisting of two-dimensional grids of related
OLS problems that arise in the context of genome-wide association analyses, and
give a careful walkthrough for the development of {\sc ols-grid}, a
high-performance routine for shared-memory architectures; analogous steps are
relevant for tailoring OLS solvers to other applications. In particular, we
first illustrate the design of efficient algorithms that exploit the structure
of the OLS problems and eliminate redundant computations; then, we show how to
effectively deal with datasets that do not fit in main memory; finally, we
discuss how to cast the computation in terms of efficient kernels and how to
achieve scalability. Importantly, each design decision along the way is
justified by simple performance models. {\sc ols-grid} enables the solution of
$10^{11}$ correlated OLS problems operating on terabytes of data in a matter of
hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07893</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07893</id><created>2015-04-29</created><updated>2015-08-17</updated><authors><author><keyname>Wehmuth</keyname><forenames>Klaus</forenames><affiliation>National Laboratory for Scientific Computing</affiliation></author><author><keyname>Fleury</keyname><forenames>&#xc9;ric</forenames><affiliation>LIP - Ecole Normale Sup&#xe9;rieure de Lyon</affiliation></author><author><keyname>Ziviani</keyname><forenames>Artur</forenames><affiliation>National Laboratory for Scientific Computing</affiliation></author></authors><title>MultiAspect Graphs: Algebraic representation and algorithms</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the algebraic representation and basic algorithms of MultiAspect
Graphs~(MAGs), a structure capable of representing multilayer and time-varying
networks while also having the property of being isomorphic to a directed
graph. In particular, we show that, as a consequence of the properties
associated with the MAG structure, a MAG can be represented in matrix form.
Moreover, we also show that any possible MAG function (algorithm) can be
obtained from this matrix-based representation. This is an important
theoretical result since it paves the way for adapting well-known graph
algorithms for application in MAGs. We present a set of basic MAG algorithms,
constructed from well-known graph algorithms, such as degree computing, Breadth
First Search (BFS), and Depth First Search (DFS). These algorithms adapted to
the MAG context can be used as primitives for building other more sophisticated
MAG algorithms. Therefore, such examples can be seen as guidelines on how to
properly derive MAG algorithms from basic algorithms on directed graph. We also
make available python implementations of all the algorithms presented in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07901</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07901</id><created>2015-04-29</created><authors><author><keyname>Ben-Hamadou</keyname><forenames>Achraf</forenames></author><author><keyname>Soussen</keyname><forenames>Charles</forenames></author><author><keyname>Blondel</keyname><forenames>Walter</forenames></author><author><keyname>Daul</keyname><forenames>Christian</forenames></author><author><keyname>Wolf</keyname><forenames>Didier</forenames></author></authors><title>Comparative study of image registration techniques for bladder
  video-endoscopy</title><categories>cs.CV</categories><comments>7 pages, 5 figures</comments><journal-ref>Novel Optical Instrumentation for Biomedical Applications, 737118
  (10 July 2009)</journal-ref><doi>10.1117/12.831772</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bladder cancer is widely spread in the world. Many adequate diagnosis
techniques exist. Video-endoscopy remains the standard clinical procedure for
visual exploration of the bladder internal surface. However, video-endoscopy
presents the limit that the imaged area for each image is about nearly 1cm2.
And, lesions are, typically, spread over several images. The aim of this
contribution is to assess the performance of two mosaicing algorithms leading
to the construction of panoramic maps (one unique image) of bladder walls. The
quantitative comparison study is performed on a set of real endoscopic exam
data and on simulated data relative to bladder phantom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07907</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07907</id><created>2015-04-29</created><updated>2015-04-30</updated><authors><author><keyname>Nguyen</keyname><forenames>Quynh</forenames><affiliation>Max Planck Institute for Informatics</affiliation><affiliation>Saarland University</affiliation></author><author><keyname>Gautier</keyname><forenames>Antoine</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Hein</keyname><forenames>Matthias</forenames><affiliation>Saarland University</affiliation></author></authors><title>A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching</title><categories>cs.CV</categories><comments>CVPR 2015 (Long version - All proofs included)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of correspondences between two images resp. point sets is a
core problem in computer vision. One way to formulate the problem is graph
matching leading to the quadratic assignment problem which is NP-hard. Several
so called second order methods have been proposed to solve this problem. In
recent years hypergraph matching leading to a third order problem became
popular as it allows for better integration of geometric information. For most
of these third order algorithms no theoretical guarantees are known. In this
paper we propose a general framework for tensor block coordinate ascent methods
for hypergraph matching. We propose two algorithms which both come along with
the guarantee of monotonic ascent in the matching score on the set of discrete
assignment matrices. In the experiments we show that our new algorithms
outperform previous work both in terms of achieving better matching scores and
matching accuracy. This holds in particular for very challenging settings where
one has a high number of outliers and other forms of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07908</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07908</id><created>2015-04-29</created><authors><author><keyname>Burak</keyname><forenames>Maciej Rafal</forenames></author></authors><title>Inhomogeneous CTMC Model of a Call Center with Balking and Abandonment</title><categories>cs.PF</categories><comments>submitted to Studia Informatica (http://studiainformatica.polsl.pl/).
  arXiv admin note: substantial text overlap with arXiv:1410.0804</comments><journal-ref>Studia Informatica, Vol 36, No 2 (2015):23-34, jun 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a nonstationary multiserver queuing model with
abandonment and balking for inbound call centers. We present a continuous time
Markov chain (CTMC) model which captures the important characteristics of an
inbound call center and obtain a numerical solution for its transient state
probabilities using uniformization method with steady-state detection.
Keywords: call center, transient, Markov processes, numerical methods,
uniformization, abandonment, balking
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07912</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07912</id><created>2015-04-29</created><authors><author><keyname>Raskhodnikova</keyname><forenames>Sofya</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>Efficient Lipschitz Extensions for High-Dimensional Graph Statistics and
  Node Private Degree Distributions</title><categories>cs.CR cs.DS</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lipschitz extensions were recently proposed as a tool for designing node
differentially private algorithms. However, efficiently computable Lipschitz
extensions were known only for 1-dimensional functions (that is, functions that
output a single real value). In this paper, we study efficiently computable
Lipschitz extensions for multi-dimensional (that is, vector-valued) functions
on graphs. We show that, unlike for 1-dimensional functions, Lipschitz
extensions of higher-dimensional functions on graphs do not always exist, even
with a non-unit stretch. We design Lipschitz extensions with small stretch for
the sorted degree list and for the degree distribution of a graph. Crucially,
our extensions are efficiently computable.
  We also develop new tools for employing Lipschitz extensions in the design of
differentially private algorithms. Specifically, we generalize the exponential
mechanism, a widely used tool in data privacy. The exponential mechanism is
given a collection of score functions that map datasets to real values. It
attempts to return the name of the function with nearly minimum value on the
data set. Our generalized exponential mechanism provides better accuracy when
the sensitivity of an optimal score function is much smaller than the maximum
sensitivity of score functions.
  We use our Lipschitz extension and the generalized exponential mechanism to
design a node-differentially private algorithm for releasing an approximation
to the degree distribution of a graph. Our algorithm is much more accurate than
algorithms from previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07918</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07918</id><created>2015-04-29</created><authors><author><keyname>Condessa</keyname><forenames>Filipe</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jose</forenames></author><author><keyname>Kovacevic</keyname><forenames>Jelena</forenames></author></authors><title>Robust hyperspectral image classification with rejection fields</title><categories>cs.CV</categories><comments>This paper was submitted to IEEE WHISPERS 2015: 7th Workshop on
  Hyperspectral Image and Signal Processing: Evolution on Remote Sensing. 5
  pages, 1 figure, 2 tables</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel method for robust hyperspectral image
classification using context and rejection. Hyperspectral image classification
is generally an ill-posed image problem where pixels may belong to unknown
classes, and obtaining representative and complete training sets is costly.
Furthermore, the need for high classification accuracies is frequently greater
than the need to classify the entire image.
  We approach this problem with a robust classification method that combines
classification with context with classification with rejection. A rejection
field that will guide the rejection is derived from the classification with
contextual information obtained by using the SegSALSA algorithm. We validate
our method in real hyperspectral data and show that the performance gains
obtained from the rejection fields are equivalent to an increase the dimension
of the training sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07933</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07933</id><created>2015-04-29</created><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Lemieux</keyname><forenames>Yves</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>A Decentralized Approach to Software-Defined Networks (SDNs)</title><categories>cs.NI</categories><comments>26 pages, 5 tables, and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Redistribution of the intelligence and management in the software defined
networks (SDNs) is a potential approach to address the bottlenecks of
scalability and integrity of these networks. We propose to revisit the routing
concept based on the notion of regions. Using basic and consistent definition
of regions, a region-based packet routing called SmartRegion Routing is
presented. The flexibility of regions in terms of naming and addressing is then
leveraged in the form of a region stack among other features placed in the
associated packet header. In this way, most of complexity and dynamicity of a
network is absorbed, and therefore highly fast and simplified routing at the
inter-region level along with semi-autonomous intra-region routing will be
feasible. In addition, multipath planning can be naturally realized at both
inter and intra levels. A basic form of SmartRegion routing mechanism is
provided. Simplicity, scalability, and manageability of the proposed approach
would also bring future potentials to reduce energy consumption and
environmental footprint associated to the SDNs. Finally, various applications,
such as enabling seamless broadband access, providing beyond IP addressing
mechanisms, and also address-equivalent naming mechanisms, are considered and
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07940</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07940</id><created>2015-04-29</created><authors><author><keyname>Yershov</keyname><forenames>Dmitry</forenames></author><author><keyname>Otte</keyname><forenames>Michael</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Planning for Optimal Feedback Control in the Volume of Free Space</title><categories>cs.RO</categories><comments>ICRA'15, Workshop on Optimal Robot Motion Planning, full paper. Draft
  for IJRR submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimal feedback planning among obstacles in d-dimensional
configuration spaces is considered. We present a sampling-based, asymptotically
optimal feedback planning method. Our method combines an incremental
construction of the Delaunay triangulation, volumetric collision-detection
module, and a modified Fast Marching Method to compute a converging sequence of
feedback functions. The convergence and asymptotic runtime are proven
theoretically and investigated during numerical experiments, in which the
proposed method is compared with the state-of-the-art asymptotically optimal
path planners. The results show that our method is competitive with the
previous algorithms. Unlike the shortest trajectory computed by many path
planning algorithms, the resulting feedback functions can be used directly for
robot navigation in our case. Finally, we present a straightforward extension
of our method that handles dynamic environments where obstacles can appear,
disappear, or move.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07941</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07941</id><created>2015-04-29</created><updated>2015-06-05</updated><authors><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Trimpe</keyname><forenames>Sebastian</forenames></author><author><keyname>Kappler</keyname><forenames>Daniel</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>A New Perspective and Extension of the Gaussian Filter</title><categories>cs.RO</categories><comments>Will appear in Robotics: Science and Systems (R:SS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gaussian Filter (GF) is one of the most widely used filtering algorithms;
instances are the Extended Kalman Filter, the Unscented Kalman Filter and the
Divided Difference Filter. GFs represent the belief of the current state by a
Gaussian with the mean being an affine function of the measurement. We show
that this representation can be too restrictive to accurately capture the
dependences in systems with nonlinear observation models, and we investigate
how the GF can be generalized to alleviate this problem. To this end, we view
the GF from a variational-inference perspective. We analyse how restrictions on
the form of the belief can be relaxed while maintaining simplicity and
efficiency. This analysis provides a basis for generalizations of the GF. We
propose one such generalization which coincides with a GF using a virtual
measurement, obtained by applying a nonlinear function to the actual
measurement. Numerical experiments show that the proposed Feature Gaussian
Filter (FGF) can have a substantial performance advantage over the standard GF
for systems with nonlinear observation models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07947</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07947</id><created>2015-04-29</created><updated>2016-03-08</updated><authors><author><keyname>Hou</keyname><forenames>Le</forenames></author><author><keyname>Samaras</keyname><forenames>Dimitris</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin M.</forenames></author><author><keyname>Gao</keyname><forenames>Yi</forenames></author><author><keyname>Davis</keyname><forenames>James E.</forenames></author><author><keyname>Saltz</keyname><forenames>Joel H.</forenames></author></authors><title>Patch-based Convolutional Neural Network for Whole Slide Tissue Image
  Classification</title><categories>cs.CV</categories><acm-class>J.3; I.4; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNN) are state-of-the-art models for many
image classification tasks. However, to recognize cancer subtypes
automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images
(WSI) is currently computationally impossible. The differentiation of cancer
subtypes is based on cellular-level visual features observed on image patch
scale. Therefore, we argue that in this situation, training a patch-level
classifier on image patches will perform better than or similar to an
image-level classifier. The challenge becomes how to intelligently combine
patch-level classification results and model the fact that not all patches will
be discriminative. We propose to train a decision fusion model to aggregate
patch-level predictions given by patch-level CNNs, which to the best of our
knowledge has not been shown before. Furthermore, we formulate a novel
Expectation-Maximization (EM) based method that automatically locates
discriminative patches robustly by utilizing the spatial relationships of
patches. We apply our method to the classification of glioma and non-small-cell
lung carcinoma cases into subtypes. The classification accuracy of our method
is similar to the inter-observer agreement between pathologists. Although it is
impossible to train CNNs on WSIs, we experimentally demonstrate using a
comparable non-cancer dataset of smaller images that a patch-based CNN can
outperform an image-based CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07948</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07948</id><created>2015-04-29</created><updated>2015-05-01</updated><authors><author><keyname>Garrison</keyname><forenames>William C.</forenames><suffix>III</suffix></author><author><keyname>Lee</keyname><forenames>Adam J.</forenames></author></authors><title>Decomposing, Comparing, and Synthesizing Access Control Expressiveness
  Simulations (Extended Version)</title><categories>cs.CR</categories><comments>24-page extended version of &quot;Decomposing, Comparing, and Synthesizing
  Access Control Expressiveness Simulations&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access control is fundamental to computer security, and has thus been the
subject of extensive formal study. In particular, *relative expressiveness
analysis* techniques have used formal mappings called *simulations* to explore
whether one access control system is capable of emulating another, thereby
comparing the expressive power of these systems. Unfortunately, the notions of
expressiveness simulation that have been explored vary widely, which makes it
difficult to compare results in the literature, and even leads to apparent
contradictions between results. Furthermore, some notions of expressiveness
simulation make use of non-determinism, and thus cannot be used to define
mappings between access control systems that are useful in practical scenarios.
In this work, we define the minimum set of properties for an *implementable*
access control simulation; i.e., a deterministic &quot;recipe&quot; for using one system
in place of another. We then define a wide range of properties spread across
several dimensions that can be enforced on top of this minimum definition.
These properties define a taxonomy that can be used to separate and compare
existing notions of access control simulation, many of which were previously
incomparable. We position existing notions of simulation within our properties
lattice by formally proving each simulation's equivalence to a corresponding
set of properties. Lastly, we take steps towards bridging the gap between
theory and practice by exploring the systems implications of points within our
properties lattice. This shows that relative expressive analysis is more than
just a theoretical tool, and can also guide the choice of the most suitable
access control system for a specific application or scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07958</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07958</id><created>2015-04-29</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Exploring Integral Image Word Length Reduction Techniques for SURF
  Detector</title><categories>cs.CV</categories><comments>ICCEE 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speeded Up Robust Features (SURF) is a state of the art computer vision
algorithm that relies on integral image representation for performing fast
detection and description of image features that are scale and rotation
invariant. Integral image representation, however, has major draw back of large
binary word length that leads to substantial increase in memory size. When
designing a dedicated hardware to achieve real-time performance for the SURF
algorithm, it is imperative to consider the adverse effects of integral image
on memory size, bus width and computational resources. With the objective of
minimizing hardware resources, this paper presents a novel implementation
concept of a reduced word length integral image based SURF detector. It
evaluates two existing word length reduction techniques for the particular case
of SURF detector and extends one of these to achieve more reduction in word
length. This paper also introduces a novel method to achieve integral image
word length reduction for SURF detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07959</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07959</id><created>2015-04-29</created><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Sublinear-Time Decremental Algorithms for Single-Source Reachability and
  Shortest Paths on Directed Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dynamic algorithms for maintaining Single-Source Reachability
(SSR) and approximate Single-Source Shortest Paths (SSSP) on $n$-node $m$-edge
directed graphs under edge deletions (decremental algorithms). The previous
fastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach
[JACM 1981]; it has $ O(1) $ query time and $ O (mn) $ total update time (i.e.,
linear amortized update time if all edges are deleted). This algorithm serves
as a building block for several other dynamic algorithms. The question whether
its total update time can be improved is a major, long standing, open problem.
  In this paper, we answer this question affirmatively. We obtain a randomized
algorithm with an expected total update time of $ O(\min (m^{7/6} n^{2/3 +
o(1)}, m^{3/4} n^{5/4 + o(1)}) ) = O (m n^{9/10 + o(1)}) $ for SSR and
$(1+\epsilon)$-approximate SSSP if the edge weights are integers from $ 1 $ to
$ W \leq 2^{\log^c{n}} $ and $ \epsilon \geq 1 / \log^c{n} $ for some constant
$ c $. We also extend our algorithm to achieve roughly the same running time
for Strongly Connected Components (SCC), improving the algorithm of Roditty and
Zwick [FOCS 2002]. Our algorithm is most efficient for sparse and dense graphs.
When $ m = \Theta(n) $ its running time is $ O (n^{1 + 5/6 + o(1)}) $ and when
$ m = \Theta(n^2) $ its running time is $ O (n^{2 + 3/4 + o(1)}) $. For SSR we
also obtain an algorithm that is faster for dense graphs and has a total update
time of $ O ( m^{2/3} n^{4/3 + o(1)} + m^{3/7} n^{12/7 + o(1)}) $ which is $ O
(n^{2 + 2/3}) $ when $ m = \Theta(n^2) $. All our algorithms have constant
query time in the worst case and are correct with high probability against an
oblivious adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07962</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07962</id><created>2015-04-29</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Hardware based Scale- and Rotation-Invariant Feature Extraction: A
  Retrospective Analysis and Future Directions</title><categories>cs.CV</categories><comments>ICCEE 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Vision techniques represent a class of algorithms that are highly
computation and data intensive in nature. Generally, performance of these
algorithms in terms of execution speed on desktop computers is far from
real-time. Since real-time performance is desirable in many applications,
special-purpose hardware is required in most cases to achieve this goal. Scale-
and rotation-invariant local feature extraction is a low level computer vision
task with very high computational complexity. The state-of-the-art algorithms
that currently exist in this domain, like SIFT and SURF, suffer from slow
execution speeds and at best can only achieve rates of 2-3 Hz on modern desktop
computers. Hardware-based scale- and rotation-invariant local feature
extraction is an emerging trend enabling real-time performance for these
computationally complex algorithms. This paper takes a retrospective look at
the advances made so far in this field, discusses the hardware design
strategies employed and results achieved, identifies current research gaps and
suggests future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07967</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07967</id><created>2015-04-29</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Kanwal</keyname><forenames>Nadia</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Improved repeatability measures for evaluating performance of feature
  detectors</title><categories>cs.CV cs.PF</categories><journal-ref>Electronics Letters 8th July 2010 Vol. 46 No. 14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most frequently employed measure for performance characterisation of
local feature detectors is repeatability, but it has been observed that this
does not necessarily mirror actual performance. Presented are improved
repeatability formulations which correlate much better with the true
performance of feature detectors. Comparative results for several
state-of-the-art feature detectors are presented using these measures; it is
found that Hessian-based detectors are generally superior at identifying
features when images are subject to various geometric and photometric
transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07968</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07968</id><created>2015-04-29</created><authors><author><keyname>Sandouk</keyname><forenames>Ubai</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author></authors><title>Learning Contextualized Music Semantics from Tags via a Siamese Network</title><categories>cs.LG</categories><comments>26 pages. Technical Report 2015-02-06. School of Computer Science.
  The University of Manchester</comments><report-no>2015-02-06</report-no><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic annotation of music with tags is a promising methodology for the
acquisition of semantics that facilitates music information retrieval and
understanding. One of the biggest challenges for this methodology is modeling
concept semantics in context. Moreover, the out of vocabulary (OOV) problem
exacerbates its difficulty and has yet to be addressed so far. In this paper,
we propose a novel Siamese network to fight off the challenge. By means of tag
features and a probabilistic topic model, our Siamese network captures
contextualized music semantics from tags via unsupervised learning, which leads
to a contextualized music semantic space and a potential solution to the OOV.
We have conducted simulations on two public tag collections, CAL500 and
MagTag5K, and compared our approach to a number of the state-of-the-art
methods. Comparative results suggest that our approach outperforms the
state-of-the-art methods in terms of semantic priming measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07974</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07974</id><created>2015-04-15</created><updated>2015-08-04</updated><authors><author><keyname>Li</keyname><forenames>Quan-Lin</forenames></author></authors><title>Nonlinear Markov Processes in Big Networks</title><categories>cs.SY</categories><comments>25 pages</comments><msc-class>60J22, 90B18</msc-class><acm-class>C.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big networks express various large-scale networks in many practical areas
such as computer networks, internet of things, cloud computation, manufacturing
systems, transportation networks, and healthcare systems. This paper analyzes
such big networks, and applies the mean-field theory and the nonlinear Markov
processes to set up a broad class of nonlinear continuous-time block-structured
Markov processes, which can be applied to deal with many practical stochastic
systems. Firstly, a nonlinear Markov process is derived from a large number of
interacting big networks with symmetric interactions, each of which is
described as a continuous-time block-structured Markov process. Secondly, some
effective algorithms are given for computing the fixed points of the nonlinear
Markov process by means of the UL-type RG-factorization. Finally, the Birkhoff
center, the Lyapunov functions and the relative entropy are used to analyze
stability or metastability of the big network, and several interesting open
problems are proposed with detailed interpretation. We believe that the results
given in this paper can be useful and effective in the study of big networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07976</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07976</id><created>2015-04-29</created><authors><author><keyname>Erlebach</keyname><forenames>Thomas</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Kammer</keyname><forenames>Frank</forenames></author></authors><title>On Temporal Graph Exploration</title><categories>cs.DS</categories><comments>This is an extended version of an ICALP 2015 paper</comments><msc-class>05C85</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A temporal graph is a graph in which the edge set can change from step to
step. The temporal graph exploration problem TEXP is the problem of computing a
foremost exploration schedule for a temporal graph, i.e., a temporal walk that
starts at a given start node, visits all nodes of the graph, and has the
smallest arrival time. We consider only temporal graphs that are connected at
each step. For such temporal graphs with $n$ nodes, we show that it is NP-hard
to approximate TEXP with ratio $O(n^{1-\epsilon})$ for any $\epsilon&gt;0$. We
also provide an explicit construction of temporal graphs that require
$\Theta(n^2)$ steps to be explored. We then consider TEXP under the assumption
that the underlying graph (i.e. the graph that contains all edges that are
present in the temporal graph in at least one step) belongs to a specific class
of graphs. Among other results, we show that temporal graphs can be explored in
$O(n^{1.5} k^2 \log n)$ steps if the underlying graph has treewidth $k$ and in
$O(n \log^3 n)$ steps if the underlying graph is a $2\times n$ grid. Finally,
we show that sparse temporal graphs with regularly present edges can always be
explored in $O(n)$ steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07980</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07980</id><created>2015-04-29</created><authors><author><keyname>Stauffer</keyname><forenames>Alexandre</forenames></author></authors><title>A Lyapunov function for Glauber dynamics on lattice triangulations</title><categories>math.PR cs.DM math-ph math.CO math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study random triangulations of the integer points $[0,n]^2
\cap\mathbb{Z}^2$, where each triangulation $\sigma$ has probability measure
$\lambda^{|\sigma|}$ with $|\sigma|$ denoting the sum of the length of the
edges in $\sigma$. Such triangulations are called \emph{lattice
triangulations}. We construct a height function on lattice triangulations and
prove that, in the whole subcritical regime $\lambda&lt;1$, the function behaves
as a \emph{Lyapunov function} with respect to Glauber dynamics; that is, the
function is a supermartingale. We show the applicability of the above result by
establishing several features of lattice triangulations, such as tightness of
local measures, exponential tail of edge lengths, crossings of small triangles,
and decay of correlations in thin rectangles. These are the first results on
lattice triangulations that are valid in the whole subcritical regime
$\lambda&lt;1$. In a very recent work with Caputo, Martinelli and Sinclair, we
apply this Lyapunov function to establish tight bounds on the mixing time of
Glauber dynamics in thin rectangles that hold for all $\lambda&lt;1$. The Lyapunov
function result here holds in great generality; it holds for triangulations of
general lattice polygons (instead of the $[0,n]^2$ square) and also in the
presence of arbitrary constraint edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07981</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07981</id><created>2015-04-29</created><updated>2016-01-31</updated><authors><author><keyname>Zuev</keyname><forenames>Konstantin</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Hamiltonian Dynamics of Preferential Attachment</title><categories>physics.soc-ph cs.SI math-ph math.MP</categories><comments>16 pages, 5 figures</comments><journal-ref>J. Phys. A: Math. Theor. (2016), 49, 105001</journal-ref><doi>10.1088/1751-8113/49/10/105001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction and control of network dynamics are grand-challenge problems in
network science. The lack of understanding of fundamental laws driving the
dynamics of networks is among the reasons why many practical problems of great
significance remain unsolved for decades. Here we study the dynamics of
networks evolving according to preferential attachment, known to approximate
well the large-scale growth dynamics of a variety of real networks. We show
that this dynamics is Hamiltonian, thus casting the study of complex networks
dynamics to the powerful canonical formalism, in which the time evolution of a
dynamical system is described by Hamilton's equations. We derive the explicit
form of the Hamiltonian that governs network growth in preferential attachment.
This Hamiltonian turns out to be nearly identical to graph energy in the
configuration model, which shows that the ensemble of random graphs generated
by preferential attachment is nearly identical to the ensemble of random graphs
with scale-free degree distributions. In other words, preferential attachment
generates nothing but random graphs with power-law degree distribution. The
extension of the developed canonical formalism for network analysis to richer
geometric network models with non-degenerate groups of symmetries may
eventually lead to a system of equations describing network dynamics at small
scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.07999</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.07999</id><created>2015-04-29</created><updated>2015-09-23</updated><authors><author><keyname>Bremner</keyname><forenames>Michael J.</forenames></author><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author><author><keyname>Shepherd</keyname><forenames>Dan J.</forenames></author></authors><title>Average-case complexity versus approximate simulation of commuting
  quantum computations</title><categories>quant-ph cs.CC</categories><comments>This version is arguably easier to read than v1. Trust us, we argued
  about it. 4+1+5 pages, RevTex 4.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the class of commuting quantum computations known as IQP
(Instantaneous Quantum Polynomial time) to strengthen the conjecture that
quantum computers are hard to simulate classically. We show that, if either of
two plausible average-case hardness conjectures holds, then IQP computations
are hard to simulate classically up to constant additive error. One conjecture
relates to the hardness of estimating the complex-temperature partition
function for random instances of the Ising model; the other concerns
approximating the number of zeroes of random low-degree polynomials. We observe
that both conjectures can be shown to be valid in the setting of worst-case
complexity. We arrive at these conjectures by deriving spin-based
generalisations of the Boson Sampling problem that avoid the so-called
permanent anticoncentration conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08008</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08008</id><created>2015-04-29</created><authors><author><keyname>Fox</keyname><forenames>Kyle</forenames></author><author><keyname>Klein</keyname><forenames>Philip N.</forenames></author><author><keyname>Mozes</keyname><forenames>Shay</forenames></author></authors><title>A Polynomial-time Bicriteria Approximation Scheme for Planar Bisection</title><categories>cs.DS</categories><comments>To appear in STOC 2015</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph with edge costs and node weights, the minimum
bisection problem asks for a partition of the nodes into two parts of equal
weight such that the sum of edge costs between the parts is minimized. We give
a polynomial time bicriteria approximation scheme for bisection on planar
graphs.
  Specifically, let $W$ be the total weight of all nodes in a planar graph $G$.
For any constant $\varepsilon &gt; 0$, our algorithm outputs a bipartition of the
nodes such that each part weighs at most $W/2 + \varepsilon$ and the total cost
of edges crossing the partition is at most $(1+\varepsilon)$ times the total
cost of the optimal bisection. The previously best known approximation for
planar minimum bisection, even with unit node weights, was $O(\log n)$. Our
algorithm actually solves a more general problem where the input may include a
target weight for the smaller side of the bipartition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08011</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08011</id><created>2015-04-29</created><updated>2015-12-17</updated><authors><author><keyname>Horan</keyname><forenames>Victoria</forenames></author><author><keyname>Adachi</keyname><forenames>Steve</forenames></author><author><keyname>Bak</keyname><forenames>Stanley</forenames></author></authors><title>A Comparison of Approaches for Solving Hard Graph-Theoretic Problems</title><categories>cs.DS cs.DM math.CO quant-ph</categories><comments>23 pages, 13 figures; revised/reformatted: same main results but
  includes additional references and run times</comments><msc-class>68R10 (Primary), 05C69, 68Q12, 68Q25, 68Q60 (Secondary)</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In order to formulate mathematical conjectures likely to be true, a number of
base cases must be determined. However, many combinatorial problems are NP-hard
and the computational complexity makes this research approach difficult using a
standard brute force approach on a typical computer. One sample problem
explored is that of finding a minimum identifying code. To work around the
computational issues, a variety of methods are explored and consist of a
parallel computing approach using Matlab, a quantum annealing approach using
the D-Wave computer, and lastly using satisfiability modulo theory (SMT) and
corresponding SMT solvers. Each of these methods requires the problem to be
formulated in a unique manner. In this paper, we address the challenges of
computing solutions to this NP-hard problem with respect to each of these
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08013</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08013</id><created>2015-04-29</created><authors><author><keyname>Patten</keyname><forenames>Daniel R.</forenames></author><author><keyname>Blair</keyname><forenames>Howard A.</forenames></author><author><keyname>Jakel</keyname><forenames>David W.</forenames></author><author><keyname>Irwin</keyname><forenames>Robert J.</forenames></author></authors><title>Differential Calculus on Cayley Graphs</title><categories>cs.DM math.GN</categories><comments>12 pages, 4 figures. First author was supported in part by an NRC
  Fellowship. Second Author was supported in part by AFRL Contract No.
  F8713-13-2-0116GI</comments><msc-class>Primary: 54A20, Secondary: 39A12</msc-class><acm-class>G.2.2; G.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conservatively extend classical elementary differential calculus to the
Cartesian closed category of convergence spaces. By specializing results about
the convergence space representation of directed graphs, we use Cayley graphs
to obtain a differential calculus on groups, from which we then extract a
Boolean differential calculus, in which both linearity and the product rule,
also called the Leibniz identity, are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08021</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08021</id><created>2015-04-29</created><authors><author><keyname>Sundar</keyname><forenames>Harshavardhan</forenames></author><author><keyname>Sreenivas</keyname><forenames>Thippur V.</forenames></author></authors><title>Who Spoke What? A Latent Variable Framework for the Joint Decoding of
  Multiple Speakers and their Keywords</title><categories>cs.SD cs.LG</categories><comments>6 pages, 2 figures Submitted to : IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a latent variable (LV) framework to identify all
the speakers and their keywords given a multi-speaker mixture signal. We
introduce two separate LVs to denote active speakers and the keywords uttered.
The dependency of a spoken keyword on the speaker is modeled through a
conditional probability mass function. The distribution of the mixture signal
is expressed in terms of the LV mass functions and speaker-specific-keyword
models. The proposed framework admits stochastic models, representing the
probability density function of the observation vectors given that a particular
speaker uttered a specific keyword, as speaker-specific-keyword models. The LV
mass functions are estimated in a Maximum Likelihood framework using the
Expectation Maximization (EM) algorithm. The active speakers and their keywords
are detected as modes of the joint distribution of the two LVs. In mixture
signals, containing two speakers uttering the keywords simultaneously, the
proposed framework achieves an accuracy of 82% for detecting both the speakers
and their respective keywords, using Student's-t mixture models as
speaker-specific-keyword models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08022</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08022</id><created>2015-04-29</created><authors><author><keyname>Guo</keyname><forenames>Hongyu</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaodan</forenames></author><author><keyname>Min</keyname><forenames>Martin Renqiang</forenames></author></authors><title>A Deep Learning Model for Structured Outputs with High-order Interaction</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world applications are associated with structured data, where not
only input but also output has interplay. However, typical classification and
regression models often lack the ability of simultaneously exploring high-order
interaction within input and that within output. In this paper, we present a
deep learning model aiming to generate a powerful nonlinear functional mapping
from structured input to structured output. More specifically, we propose to
integrate high-order hidden units, guided discriminative pretraining, and
high-order auto-encoders for this purpose. We evaluate the model with three
datasets, and obtain state-of-the-art performances among competitive methods.
Our current work focuses on structured output regression, which is a less
explored area, although the model can be extended to handle structured label
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08023</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08023</id><created>2015-04-29</created><authors><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Pirsiavash</keyname><forenames>Hamed</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Anticipating the future by watching unlabeled video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many computer vision applications, machines will need to reason beyond the
present, and predict the future. This task is challenging because it requires
leveraging extensive commonsense knowledge of the world that is difficult to
write down. We believe that a promising resource for efficiently obtaining this
knowledge is through the massive amounts of readily available unlabeled video.
In this paper, we present a large scale framework that capitalizes on temporal
structure in unlabeled video to learn to anticipate both actions and objects in
the future. The key idea behind our approach is that we can train deep networks
to predict the visual representation of images in the future. We experimentally
validate this idea on two challenging &quot;in the wild&quot; video datasets, and our
results suggest that learning with unlabeled videos significantly helps
forecast actions and anticipate objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08024</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08024</id><created>2015-04-29</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Gupta</keyname><forenames>Shalmoli</forenames></author><author><keyname>Quanrud</keyname><forenames>Kent</forenames></author></authors><title>Streaming Algorithms for Submodular Function Maximization</title><categories>cs.DS</categories><comments>29 pages, 7 figures, extended abstract to appear in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing a nonnegative submodular set function
$f:2^{\mathcal{N}} \rightarrow \mathbb{R}^+$ subject to a $p$-matchoid
constraint in the single-pass streaming setting. Previous work in this context
has considered streaming algorithms for modular functions and monotone
submodular functions. The main result is for submodular functions that are {\em
non-monotone}. We describe deterministic and randomized algorithms that obtain
a $\Omega(\frac{1}{p})$-approximation using $O(k \log k)$-space, where $k$ is
an upper bound on the cardinality of the desired set. The model assumes value
oracle access to $f$ and membership oracles for the matroids defining the
$p$-matchoid constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08025</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08025</id><created>2015-04-29</created><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author><author><keyname>Kingma</keyname><forenames>Diederik P.</forenames></author></authors><title>Technical Note on Equivalence Between Recurrent Neural Network Time
  Series Models and Variational Bayesian Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observe that the standard log likelihood training objective for a
Recurrent Neural Network (RNN) model of time series data is equivalent to a
variational Bayesian training objective, given the proper choice of generative
and inference models. This perspective may motivate extensions to both RNNs and
variational Bayesian models. We propose one such extension, where multiple
particles are used for the hidden state of an RNN, allowing a natural
representation of uncertainty or multimodality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08027</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08027</id><created>2015-04-29</created><authors><author><keyname>Manda</keyname><forenames>Prashanti</forenames></author><author><keyname>McCarthy</keyname><forenames>Fiona</forenames></author><author><keyname>Nanduri</keyname><forenames>Bindu</forenames></author><author><keyname>Bridges</keyname><forenames>Susan M.</forenames></author></authors><title>Information Theoretic Interestingness Measures for Cross-Ontology Data
  Mining in the Mouse Anatomy Ontology and the Gene Ontology</title><categories>cs.AI cs.CE q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community annotation of biological entities with concepts from multiple
bio-ontologies has created large and growing repositories of ontology-based
annotation data with embedded implicit relationships among orthogonal
ontologies. Development of efficient data mining methods and metrics to mine
and assess the quality of the mined relationships has not kept pace with the
growth of annotation data. In this study, we present a data mining method that
uses ontology-guided generalization to discover relationships across ontologies
along with a new interestingness metric based on information theory. We apply
our data mining algorithm and interestingness measures to gene expression
datasets from the Gene Expression Database at the Mouse Genomics Institute as a
preliminary proof of concept to mine relationships between developmental stages
in the mouse anatomy ontology and Gene Ontology concepts (biological process,
molecular function and cellular component). In addition, we present a
comparison of our interestingness metric to four existing metrics.
Ontology-based annotation datasets provide a valuable resource for discovery of
relationships across ontologies. The use of efficient data mining methods and
appropriate interestingness metrics enables the identification of high quality
relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08033</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08033</id><created>2015-04-29</created><authors><author><keyname>Johnson</keyname><forenames>James Ian</forenames></author></authors><title>Automating Abstract Interpretation of Abstract Machines</title><categories>cs.PL</categories><comments>This dissertation has been accepted by the thesis committee</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static program analysis is a valuable tool for any programming language that
people write programs in. The prevalence of scripting languages in the world
suggests programming language interpreters are relatively easy to write. Users
of these languages lament their inability to analyze their code, therefore
programming language analyzers are not easy to write. This thesis investigates
a systematic method of creating abstract interpreters from traditional
interpreters, called Abstracting Abstract Machines.
  Abstract interpreters are difficult to develop due to technical, theoretical,
and pragmatic problems. Technical problems include engineering data structures
and algorithms. I show that modest and simple changes to the mathematical
presentation of abstract machines result in 1000 times better running time -
just seconds for moderately sized programs.
  In the theoretical realm, abstraction can make correctness difficult to
ascertain. I provide proof techniques for proving the correctness of regular,
pushdown, and stack-inspecting pushdown models of abstract computation by
leaving computational power to an external factor: allocation. Even if we don't
trust the proof, we can run models concretely against test suites to better
trust them.
  In the pragmatic realm, I show that the systematic process of abstracting
abstract machines is automatable. I develop a meta-language for expressing
abstract machines similar to other semantics engineering languages. The
language's special feature is that it provides an interface to abstract
allocation. The semantics guarantees that if allocation is finite, then the
semantics is a sound and computable approximation of the concrete semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08035</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08035</id><created>2015-04-29</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>The ELAPS Framework: Experimental Linear Algebra Performance Studies</title><categories>cs.PF cs.MS cs.NA</categories><comments>Submitted to SC15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal use of computing resources requires extensive coding, tuning and
benchmarking. To boost developer productivity in these time consuming tasks, we
introduce the Experimental Linear Algebra Performance Studies framework
(ELAPS), a multi-platform open source environment for fast yet powerful
performance experimentation with dense linear algebra kernels, algorithms, and
libraries. ELAPS allows users to construct experiments to investigate how
performance and efficiency vary depending on factors such as caching,
algorithmic parameters, problem size, and parallelism. Experiments are designed
either through Python scripts or a specialized GUI, and run on the whole
spectrum of architectures, ranging from laptops to clusters, accelerators, and
supercomputers. The resulting experiment reports provide various metrics and
statistics that can be analyzed both numerically and visually. We demonstrate
the use of ELAPS in four concrete application scenarios and in as many
computing environments, illustrating its practical value in supporting critical
performance decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08039</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08039</id><created>2015-04-29</created><authors><author><keyname>Vekris</keyname><forenames>Panagiotis</forenames></author><author><keyname>Cosman</keyname><forenames>Benjamin</forenames></author><author><keyname>Jhala</keyname><forenames>Ranjit</forenames></author></authors><title>Trust, but Verify: Two-Phase Typing for Dynamic Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge when statically typing so-called dynamic languages is the
ubiquity of value-based overloading, where a given function can dynamically
reflect upon and behave according to the types of its arguments. Thus, to
establish basic types, the analysis must reason precisely about values, but in
the presence of higher-order functions and polymorphism, this reasoning itself
can require basic types. In this paper we address this chicken-and-egg problem
by introducing the framework of two-phased typing. The first &quot;trust&quot; phase
performs classical, i.e. flow-, path- and value-insensitive type checking to
assign basic types to various program expressions. When the check inevitably
runs into &quot;errors&quot; due to value-insensitivity, it wraps problematic expressions
with DEAD-casts, which explicate the trust obligations that must be discharged
by the second phase. The second phase uses refinement typing, a flow- and
path-sensitive analysis, that decorates the first phase's types with logical
predicates to track value relationships and thereby verify the casts and
establish other correctness properties for dynamically typed languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08043</identifier>
 <datestamp>2015-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08043</id><created>2015-04-29</created><authors><author><keyname>Mac Aonghusa</keyname><forenames>P&#xf3;l</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Don't let Google know I'm lonely!</title><categories>cs.CR cs.SI</categories><comments>26 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From buying books to finding the perfect partner, we share our most intimate
wants and needs with our favourite online systems. But how far should we accept
promises of privacy in the face of personal profiling? In particular we ask how
can we improve detection of sensitive topic profiling by online systems? We
propose a definition of privacy disclosure we call
{\epsilon}-indistinguishability from which we construct scalable, practical
tools to assess an adversaries learning potential. We demonstrate our results
using openly available resources, detecting a learning rate in excess of 98%
for a range of sensitive topics during our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08050</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08050</id><created>2015-04-29</created><authors><author><keyname>Song</keyname><forenames>Shuangyong</forenames></author><author><keyname>Meng</keyname><forenames>Yao</forenames></author></authors><title>Detecting Concept-level Emotion Cause in Microblogging</title><categories>cs.CL cs.AI</categories><comments>2 pages, 2 figures, to appear on WWW 2015</comments><msc-class>68P20</msc-class><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead
of the mere word-level models, to discover causes of microblogging users'
diversified emotions on specific hot event. A modified topic-supervised biterm
topic model is utilized in CECM to detect emotion topics' in event-related
tweets, and then context-sensitive topical PageRank is utilized to detect
meaningful multiword expressions as emotion causes. Experimental results on a
dataset from Sina Weibo, one of the largest microblogging websites in China,
show CECM can better detect emotion causes than baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08052</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08052</id><created>2015-04-29</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Omar S.</forenames></author><author><keyname>Aguilar</keyname><forenames>Ra&#xfa;l A.</forenames></author><author><keyname>Uc&#xe1;n</keyname><forenames>Juan P.</forenames></author></authors><title>Efectividad de t\'ecnicas de prueba de software aplicadas por sujetos
  novicios de pregrado</title><categories>cs.SE</categories><comments>Encuentro Nacional de Ciencias de la Computaci\'on (ENC'2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this work is to examine possible effects of using
freshman student subjects in software engineering experiments. Particularly in
this work we report the effectiveness measured as percentage of observed and
observable defects of two software testing techniques: Black-box and white-box.
  Regarding observed defects, both techniques show an effectiveness around of
4%. With respect of observable defects by test cases, black-box testing is
slightly more effective (21%) than white-box testing (16%), although this
difference is not significant. We observe a considerable lack of technical
skills of subjects for applying both software testing techniques. Due to
observed findings, we suggest to employ students with more technical skills for
carrying out software engineering experiments.
  -----
  El objetivo de este trabajo se centra en investigar los efectos que conlleva
realizar experimentos en ingenier\'ia de software (IS) empleando como sujetos
experimentales a estudiantes de pregrado cursando su primer a\~no de estudios
de la carrera en ingenier\'ia de software. De manera particular en este trabajo
se investiga la efectividad medida en porcentaje de defectos observados y
observables de las t\'ecnicas de prueba de software funcional (caja negra) y
estructural (caja blanca).
  Con respecto a los defectos observados por los sujetos, ambas t\'ecnicas
obtuvieron una efectividad del 4%. Con respecto a los defectos observables por
los casos de prueba, la t\'ecnica funcional es ligeramente superior (21%) que
la t\'ecnica estructural (16%), aunque esta diferencia no es significativa. Se
observa un nivel de inexperiencia considerable en los sujetos para aplicar las
t\'ecnicas. Dado los hallazgos encontrados, se sugiere emplear sujetos de
pregrado con un nivel mayor de experiencia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08053</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08053</id><created>2015-04-29</created><updated>2015-05-28</updated><authors><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Monte Carlo Motion Planning for Robot Trajectory Optimization Under
  Uncertainty</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a novel approach, named MCMP (Monte Carlo Motion
Planning), to the problem of motion planning under uncertainty, i.e., to the
problem of computing a low-cost path that fulfills probabilistic collision
avoidance constraints. MCMP estimates the collision probability (CP) of a given
path by sampling via Monte Carlo the execution of a reference tracking
controller (in this paper we consider LQG). The key algorithmic contribution of
this paper is the design of statistical variance-reduction techniques, namely
control variates and importance sampling, to make such a sampling procedure
amenable to real-time implementation. MCMP applies this CP estimation procedure
to motion planning by iteratively (i) computing an (approximately) optimal path
for the deterministic version of the problem (here, using the FMT* algorithm),
(ii) computing the CP of this path, and (iii) inflating or deflating the
obstacles by a common factor depending on whether the CP is higher or lower
than a target value. The advantages of MCMP are threefold: (i) asymptotic
correctness of CP estimation, as opposed to most current approximations, which,
as shown in this paper, can be off by large multiples and hinder the
computation of feasible plans; (ii) speed and parallelizability, and (iii)
generality, i.e., the approach is applicable to virtually any planning problem
provided that a path tracking controller and a notion of distance to obstacles
in the configuration space are available. Numerical results illustrate the
correctness (in terms of feasibility), efficiency (in terms of path cost), and
computational speed of MCMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08068</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08068</id><created>2015-04-29</created><authors><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Optimum Wirelessly Powered Relaying</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper maximizes the achievable throughput of a relay-assisted wirelessly
powered communications system, where an energy constrained source, helped by an
energy constrained relay and both powered by a dedicated power beacon (PB),
communicates with a destination. Considering the time splitting approach, the
source and relay first harvest energy from the PB, which is equipped with
multiple antennas, and then transmits information to the destination. Simple
closed-form expressions are derived for the optimal PB energy beamforming
vector and time split for energy harvesting and information transmission.
Numerical results and simulations demonstrate the superior performance compared
with some intuitive benchmark beamforming scheme. Also, it is found that
placing the relay at the middle of the source-destination path is no longer
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08070</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08070</id><created>2015-04-29</created><updated>2015-04-30</updated><authors><author><keyname>Falahatgar</keyname><forenames>Moein</forenames></author><author><keyname>Jafarpour</keyname><forenames>Ashkan</forenames></author><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Pichapati</keyname><forenames>Venkatadheeraj</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Universal Compression of Power-Law Distributions</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  English words and the outputs of many other natural processes are well-known
to follow a Zipf distribution. Yet this thoroughly-established property has
never been shown to help compress or predict these important processes. We show
that the expected redundancy of Zipf distributions of order $\alpha&gt;1$ is
roughly the $1/\alpha$ power of the expected redundancy of unrestricted
distributions. Hence for these orders, Zipf distributions can be better
compressed and predicted than was previously known. Unlike the expected case,
we show that worst-case redundancy is roughly the same for Zipf and for
unrestricted distributions. Hence Zipf distributions have significantly
different worst-case and expected redundancies, making them the first natural
distribution class shown to have such a difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08076</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08076</id><created>2015-04-30</created><updated>2015-04-30</updated><authors><author><keyname>Zhang</keyname><forenames>Haijun</forenames></author><author><keyname>Jiang</keyname><forenames>Chunxiao</forenames></author><author><keyname>Cheng</keyname><forenames>Julian</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Cooperative Interference Mitigation and Handover Management for
  Heterogeneous Cloud Small Cell Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in IEEE Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous small cell network has attracted much attention to satisfy
users' explosive data traffic requirements. Heterogeneous cloud small cell
network (HCSNet), which combines cloud computing and heterogeneous small cell
network, will likely play an important role in 5G mobile communication
networks. However, with massive deployment of small cells, co-channel
interference and handover management are two important problems in HCSNet,
especially for cell edge users. In this article, we examine the problems of
cooperative interference mitigation and handover management in HCSNet. A
network architecture is described to combine cloud radio access network with
small cells. An effective coordinated multi-point (CoMP) clustering scheme
using affinity propagation is adopted to mitigate cell edge users'
interference. A low complexity handover management scheme is presented, and its
signaling procedure is analyzed in HCSNet. Numerical results show that the
proposed network architecture, CoMP clustering scheme and handover management
scheme can significantly increase the capacity of HCSNet while maintaining
users' quality of service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08083</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08083</id><created>2015-04-30</created><updated>2015-09-27</updated><authors><author><keyname>Girshick</keyname><forenames>Ross</forenames></author></authors><title>Fast R-CNN</title><categories>cs.CV</categories><comments>To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Fast Region-based Convolutional Network method (Fast
R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently
classify object proposals using deep convolutional networks. Compared to
previous work, Fast R-CNN employs several innovations to improve training and
testing speed while also increasing detection accuracy. Fast R-CNN trains the
very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and
achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains
VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is
implemented in Python and C++ (using Caffe) and is available under the
open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08090</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08090</id><created>2015-04-30</created><authors><author><keyname>Moghadam</keyname><forenames>Mohammad R. Vedady</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Multiuser Wireless Power Transfer via Magnetic Resonant Coupling:
  Performance Analysis, Charging Control, and Power Region Characterization</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Signal Processing. arXiv admin
  note: text overlap with arXiv:1502.02385</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonant coupling (MRC) is an efficient method for realizing the
near-field wireless power transfer (WPT). Although the MRC enabled WPT
(MRC-WPT) with a single pair of transmitter and receiver has been thoroughly
studied in the literature, there is limited work on the general setup with
multiple transmitters and/or receivers. In this paper, we consider a
point-to-multipoint MRC-WPT system with one transmitter delivering wireless
power to a set of distributed receivers. We aim to introduce new applications
of signal processing and optimization techniques to the performance
characterization and optimization in multiuser WPT via MRC. We first derive
closed-form expressions for the power drawn from the energy source at the
transmitter and that delivered to the load at each receiver. We identify a
&quot;near-far&quot; fairness issue in multiuser power transmission due to receivers'
distance-dependent mutual inductance with the transmitter. To tackle this
problem, we propose a centralized charging control algorithm to jointly
optimize the receivers' load resistance to minimize the total transmitter power
drawn while meeting the given power requirement of each load. For ease of
practical implementation, we also devise a distributed algorithm for the
receivers to adjust their load resistance independently in an iterative manner.
Last, we characterize the power region that constitutes all the achievable
power-tuples of the loads with their adjustable resistance. In particular, we
compare the power regions without versus with the time sharing of users' power
transmission, where it is shown that time sharing yields a larger power region
in general. Extensive simulation results are provided to validate our analysis
and corroborate our study for the multiuser MRC-WPT system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08096</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08096</id><created>2015-04-30</created><authors><author><keyname>Chatouh</keyname><forenames>K.</forenames></author><author><keyname>Guenda</keyname><forenames>K.</forenames></author><author><keyname>Gulliver</keyname><forenames>T. A.</forenames></author><author><keyname>Noui</keyname><forenames>L.</forenames></author></authors><title>On Some Classes of $\mathbb{Z}_{2}\mathbb{Z}_{4}-$Linear Codes and their
  Covering Radius</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1411.1822 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we define $\mathbb{Z}_{2}\mathbb{Z}_{4}-$Simplex and MacDonald
Codes of type $\alpha $ and $\beta $ and we give the covering radius of these
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08097</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08097</id><created>2015-04-30</created><authors><author><keyname>Melakheso</keyname><forenames>A.</forenames></author><author><keyname>Guenda</keyname><forenames>K.</forenames></author></authors><title>The Dual and the Gray Image of Codes over
  $\mathbb{F}_{q}+v\mathbb{F}_{q}+v^{2}\mathbb{F}_{q}$</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we study the linear codes over the commutative ring
$R=\F_{q}+v\F_{q}+v^{2}\F_{q}$ and their Gray images, where $v^{3}=v$. We
define the Lee weight of the elements of $R$, we give a Gray map from $R^{n}$
to $\F^{3n}_{q}$ and we give the relation between the dual and the Gray image
of a code. This allows us to investigate the structure and properties of
self-dual cyclic, formally self-dual and the Gray image of formally self-dual
codes over $R$. Further, we give several constructions of formally self-dual
codes over $R
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08100</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08100</id><created>2015-04-30</created><authors><author><keyname>Keil</keyname><forenames>Matthias</forenames></author><author><keyname>Guria</keyname><forenames>Sankha Narayan</forenames></author><author><keyname>Schlegel</keyname><forenames>Andreas</forenames></author><author><keyname>Geffken</keyname><forenames>Manuel</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>Transparent Object Proxies for JavaScript</title><categories>cs.PL</categories><comments>Technical Report</comments><acm-class>D.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Proxies are the swiss army knives of object adaptation. They introduce a
level of indirection to intercept select operations on a target object and
divert them as method calls to a handler. Proxies have many uses like
implementing access control, enforcing contracts, virtualizing resources.
  One important question in the design of a proxy API is whether a proxy object
should inherit the identity of its target. Apparently proxies should have their
own identity for security-related applications whereas other applications, in
particular contract systems, require transparent proxies that compare equal to
their target objects.
  We examine the issue with transparency in various use cases for proxies,
discuss different approaches to obtain transparency, and propose two designs
that require modest modifications in the JavaScript engine and cannot be
bypassed by the programmer.
  We implement our designs in the SpiderMonkey JavaScript interpreter and
bytecode compiler. Our evaluation shows that these modifications of have no
statistically significant impact on the benchmark performance of the JavaScript
engine. Furthermore, we demonstrate that contract systems based on wrappers
require transparent proxies to avoid interference with program execution in
realistic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08102</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08102</id><created>2015-04-30</created><authors><author><keyname>van Miltenburg</keyname><forenames>Emiel</forenames></author></authors><title>Detecting and ordering adjectival scalemates</title><categories>cs.CL</categories><comments>Paper presented at MAPLEX 2015, February 9-10, Yamagata, Japan
  (http://lang.cs.tut.ac.jp/maplex2015/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a pattern-based method that can be used to infer
adjectival scales, such as &lt;lukewarm, warm, hot&gt;, from a corpus. Specifically,
the proposed method uses lexical patterns to automatically identify and order
pairs of scalemates, followed by a filtering phase in which unrelated pairs are
discarded. For the filtering phase, several different similarity measures are
implemented and compared. The model presented in this paper is evaluated using
the current standard, along with a novel evaluation set, and shown to be at
least as good as the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08108</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08108</id><created>2015-04-30</created><updated>2015-06-04</updated><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author><author><keyname>Santoso</keyname><forenames>Ario</forenames></author></authors><title>Verification of Generalized Inconsistency-Aware Knowledge and Action
  Bases (Extended Version)</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge and Action Bases (KABs) have been put forward as a semantically
rich representation of a domain, using a DL KB to account for its static
aspects, and actions to evolve its extensional part over time, possibly
introducing new objects. Recently, KABs have been extended to manage
inconsistency, with ad-hoc verification techniques geared towards specific
semantics. This work provides a twofold contribution along this line of
research. On the one hand, we enrich KABs with a high-level, compact action
language inspired by Golog, obtaining so called Golog-KABs (GKABs). On the
other hand, we introduce a parametric execution semantics for GKABs, so as to
elegantly accomodate a plethora of inconsistency-aware semantics based on the
notion of repair. We then provide several reductions for the verification of
sophisticated first-order temporal properties over inconsistency-aware GKABs,
and show that it can be addressed using known techniques, developed for
standard KABs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08110</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08110</id><created>2015-04-30</created><authors><author><keyname>Keil</keyname><forenames>Matthias</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>TreatJS: Higher-Order Contracts for JavaScript</title><categories>cs.PL</categories><comments>Technical Report</comments><acm-class>D.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  TreatJS is a language embedded, higher-order contract system for JavaScript
which enforces contracts by run-time monitoring. Beyond providing the standard
abstractions for building higher-order contracts (base, function, and object
contracts), TreatJS's novel contributions are its guarantee of non-interfering
contract execution, its systematic approach to blame assignment, its support
for contracts in the style of union and intersection types, and its notion of a
parameterized contract scope, which is the building block for composable
run-time generated contracts that generalize dependent function contracts.
  TreatJS is implemented as a library so that all aspects of a contract can be
specified using the full JavaScript language. The library relies on JavaScript
proxies to guarantee full interposition for contracts. It further exploits
JavaScript's reflective features to run contracts in a sandbox environment,
which guarantees that the execution of contract code does not modify the
application state. No source code transformation or change in the JavaScript
run-time system is required.
  The impact of contracts on execution speed is evaluated using the Google
Octane benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08117</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08117</id><created>2015-04-30</created><updated>2015-06-02</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Lin</keyname><forenames>Guangming</forenames></author></authors><title>Average Convergence Rate of Evolutionary Algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In evolutionary optimization, it is important to understand how fast
evolutionary algorithms converge to the optimum per generation, or their
convergence rate. This paper proposes a new measure of the convergence rate,
called average convergence rate. It is a normalised geometric mean of the
reduction ratio of the fitness difference per generation. The calculation of
the average convergence rate is very simple and it is applicable for most
evolutionary algorithms on both continuous and discrete optimization. A
theoretical study of the average convergence rate is conducted for discrete
optimization. Lower bounds on the average convergence rate are derived. The
limit of the average convergence rate is analysed and then the asymptotic
average convergence rate is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08120</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08120</id><created>2015-04-30</created><authors><author><keyname>Kolay</keyname><forenames>Sudeshna</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author></authors><title>Parameterized Algorithms for Deletion to (r,l)-graphs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fixed integers $r,\ell \geq 0$, a graph $G$ is called an {\em
$(r,\ell)$-graph} if the vertex set $V(G)$ can be partitioned into $r$
independent sets and $\ell$ cliques. This brings us to the following natural
parameterized questions: {\sc Vertex $(r,\ell)$-Partization} and {\sc Edge
$(r,\ell)$-Partization}. An input to these problems consist of a graph $G$ and
a positive integer $k$ and the objective is to decide whether there exists a
set $S\subseteq V(G)$ ($S\subseteq E(G)$) such that the deletion of $S$ from
$G$ results in an $(r,\ell)$-graph. These problems generalize well studied
problems such as {\sc Odd Cycle Transversal}, {\sc Edge Odd Cycle Transversal},
{\sc Split Vertex Deletion} and {\sc Split Edge Deletion}. We do not hope to
get parameterized algorithms for either {\sc Vertex $(r,\ell)$-Partization} or
{\sc Edge $(r,\ell)$-Partization} when either of $r$ or $\ell$ is at least $3$
as the recognition problem itself is NP-complete. This leaves the case of
$r,\ell \in \{1,2\}$. We almost complete the parameterized complexity dichotomy
for these problems. Only the parameterized complexity of {\sc Edge
$(2,2)$-Partization} remains open. We also give an approximation algorithm and
a Turing kernelization for {\sc Vertex $(r,\ell)$-Partization}. We use an
interesting finite forbidden induced graph characterization, for a class of
graphs known as $(r,\ell)$-split graphs, properly containing the class of
$(r,\ell)$-graphs. This approach to obtain approximation algorithms could be of
an independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08128</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08128</id><created>2015-04-30</created><authors><author><keyname>Flaut</keyname><forenames>Cristina</forenames></author></authors><title>Some connections between binary block codes and Hilbert algebras</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will study some connections between Hilbert al- gebras and
binary block-codes.With these codes, we can eassy obtain orders which determine
suplimentary properties on these algebras. We will try to emphasize how, using
binary block-codes, we can provide examples of classes of Hilbert algebras with
some properties, in our case, classes of semisimple Hilbert algebras and
classes of local Hilbert algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08142</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08142</id><created>2015-04-30</created><updated>2015-05-06</updated><authors><author><keyname>Shi</keyname><forenames>Qiquan</forenames></author><author><keyname>Lu</keyname><forenames>Haiping</forenames></author></authors><title>Semi-Orthogonal Multilinear PCA with Relaxed Start</title><categories>stat.ML cs.CV cs.LG</categories><comments>8 pages, 2 figures, to appear in Proceedings of the 24th
  International Joint Conference on Artificial Intelligence (IJCAI 2015)</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is an unsupervised method for learning
low-dimensional features with orthogonal projections. Multilinear PCA methods
extend PCA to deal with multidimensional data (tensors) directly via
tensor-to-tensor projection or tensor-to-vector projection (TVP). However,
under the TVP setting, it is difficult to develop an effective multilinear PCA
method with the orthogonality constraint. This paper tackles this problem by
proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA
learns low-dimensional features directly from tensors via TVP by imposing the
orthogonality constraint in only one mode. This formulation results in more
captured variance and more learned features than full orthogonality. For better
generalization, we further introduce a relaxed start (RS) strategy to get
SO-MPCA-RS by fixing the starting projection vectors, which increases the bias
and reduces the variance of the learning model. Experiments on both face (2D)
and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing
algorithms on the whole, and the relaxed start strategy is also effective for
other TVP-based PCA methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08145</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08145</id><created>2015-04-30</created><authors><author><keyname>Sousa-Rodrigues</keyname><forenames>David</forenames></author><author><keyname>de Sampayo</keyname><forenames>Mafalda Teixeira</forenames></author><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio Rodrigues</forenames></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Antunes</keyname><forenames>Carlos Henggeler</forenames></author></authors><title>Online survey for collective clustering of computer generated
  architectural floor plans</title><categories>cs.HC</categories><comments>Extended abstract accepted for ICTPI'15 conference, June 17-19,
  Milton Keynes, United Kingdom - http://www.ictpi15.info/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study is to understand what are the collective actions of
architecture practitioners when grouping floor plan designs. The understanding
of how professionals and students solve this complex problem may help to
develop specific programmes for the teaching of architecture. In addition, the
findings of this study can help in the development of query mechanisms for
database retrieval of floor plans and the implementation of clustering
mechanisms to aggregate floor plans resulting from generative design methods.
The study aims to capture how practitioners define similarity between floor
plans from a pool of available designs. A hybrid evolutionary strategy is used,
which takes into account the building's functional program to generate
alternative floor plan designs. The first step of this methodology consisted in
an online survey to gather information on how the respondents would perform a
clustering task. Online surveys have been used in several applications and are
a method of data collection that conveys several advantages. When properly
developed and implemented, a survey portrays the characteristics of large
groups of respondents on a specific topic and allows assessing its
representation. Several types of surveys are available; e.g. questionnaire and
interview formats, phone survey, and online surveys, which can be coupled with
inference engines that act and direct the survey according to respondents'
answers. In the present study, the survey was posed as an online exercise in
which respondents had to perform a pre-defined task, which makes it similar to
running an experiment in an online environment. The experiment aimed to
understand the perception and criteria of the target population to perform the
clustering task by comparing the results with the respondents' answers to a
questionnaire presented at the end of the exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08146</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08146</id><created>2015-04-30</created><authors><author><keyname>B&#xe9;rczi</keyname><forenames>Krist&#xf3;f</forenames></author><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Vizer</keyname><forenames>M&#xe1;t&#xe9;</forenames></author></authors><title>Regular graphs are antimagic</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove - with a slight modification of an argument of Cranston
et al. \cite{cranston} - that $k$-regular graphs are antimagic for $k\ge 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08150</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08150</id><created>2015-04-30</created><authors><author><keyname>Li</keyname><forenames>Quan-Lin</forenames></author><author><keyname>Yang</keyname><forenames>Feifei</forenames></author><author><keyname>Li</keyname><forenames>Na</forenames></author></authors><title>Reward Processes and Performance Simulation in Supermarket Models with
  Different Servers</title><categories>cs.PF math.PR</categories><comments>35 pages, 4 figures</comments><msc-class>90B22, 90B18, 60J28</msc-class><acm-class>C.2.1; C.2.6; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supermarket models with different servers become a key in modeling resource
management of stochastic networks, such as, computer networks, manufacturing
systems and transportation networks. While these different servers always make
analysis of such a supermarket model more interesting, difficult and
challenging. This paper provides a new novel method for analyzing the
supermarket model with different servers through a multi-dimensional
continuous-time Markov reward processes. Firstly, the utility functions are
constructed for expressing a routine selection mechanism that depends on queue
lengths, on service rates, and on some probabilities of individual preference.
Then applying the continuous-time Markov reward processes, some segmented
stochastic integrals of the random reward function are established by means of
an event-driven technique. Based on this, the mean of the random reward
function in a finite time period is effectively computed by means of the state
jump points of the Markov reward process, and also the mean of the discounted
random reward function in an infinite time period can be calculated through the
same event-driven technique. Finally, some simulation experiments are given to
indicate how the expected queue length of each server depends on the main
parameters of this supermarket model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08153</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08153</id><created>2015-04-30</created><updated>2016-02-01</updated><authors><author><keyname>Benzi</keyname><forenames>Kirell</forenames></author><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Principal Patterns on Graphs: Discovering Coherent Structures in
  Datasets</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are now ubiquitous in almost every field of research. Recently, new
research areas devoted to the analysis of graphs and data associated to their
vertices have emerged. Focusing on dynamical processes, we propose a fast,
robust and scalable framework for retrieving and analyzing recurring patterns
of activity on graphs. Our method relies on a novel type of multilayer graph
that encodes the spreading or propagation of events between successive time
steps. We demonstrate the versatility of our method by applying it on three
different real-world examples. Firstly, we study how rumor spreads on a social
network. Secondly, we reveal congestion patterns of pedestrians in a train
station. Finally, we show how patterns of audio playlists can be used in a
recommender system. In each example, relevant information previously hidden in
the data is extracted in a very efficient manner, emphasizing the scalability
of our method. With a parallel implementation scaling linearly with the size of
the dataset, our framework easily handles millions of nodes on a single
commodity server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08167</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08167</id><created>2015-04-30</created><updated>2015-12-02</updated><authors><author><keyname>Avner</keyname><forenames>Orly</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Multi-user lax communications: a multi-armed bandit approach</title><categories>cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by cognitive radio networks, we consider a setting where multiple
users share several channels modeled as a multi-user multi-armed bandit (MAB)
problem. The characteristics of each channel are unknown and are different for
each user. Each user can choose between the channels, but her success depends
on the particular channel chosen as well as on the selections of other users:
if two users select the same channel their messages collide and none of them
manages to send any data. Our setting is fully distributed, so there is no
central control. As in many communication systems, the users cannot set up a
direct communication protocol, so information exchange must be limited to a
minimum. We develop an algorithm for learning a stable configuration for the
multi-user MAB problem. We further offer both convergence guarantees and
experiments inspired by real communication networks, including comparison to
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08168</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08168</id><created>2015-04-30</created><updated>2015-05-04</updated><authors><author><keyname>&#x17d;egklitz</keyname><forenames>Jan</forenames></author><author><keyname>Po&#x161;&#xed;k</keyname><forenames>Petr</forenames></author></authors><title>Model Selection and Overfitting in Genetic Programming: Empirical Study
  [Extended Version]</title><categories>cs.NE cs.LG</categories><comments>8 pages, 12 figures, full paper for GECCO 2015 (accepted as poster,
  this is the original paper submitted to the conference); added subtitle and
  removed copyright text at the first page, fixed some typography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Programming has been very successful in solving a large area of
problems but its use as a machine learning algorithm has been limited so far.
One of the reasons is the problem of overfitting which cannot be solved or
suppresed as easily as in more traditional approaches. Another problem, closely
related to overfitting, is the selection of the final model from the
population.
  In this article we present our research that addresses both problems:
overfitting and model selection. We compare several ways of dealing with
ovefitting, based on Random Sampling Technique (RST) and on using a validation
set, all with an emphasis on model selection. We subject each approach to a
thorough testing on artificial and real--world datasets and compare them with
the standard approach, which uses the full training data, as a baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08175</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08175</id><created>2015-04-30</created><authors><author><keyname>Vinagre</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Jorge</keyname><forenames>Al&#xed;pio M&#xe1;rio</forenames></author><author><keyname>Gama</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Evaluation of recommender systems in streaming environments</title><categories>cs.IR</categories><comments>Workshop on 'Recommender Systems Evaluation: Dimensions and Design'
  (REDD 2014), held in conjunction with RecSys 2014. October 10, 2014, Silicon
  Valley, United States</comments><doi>10.13140/2.1.4381.5367</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Evaluation of recommender systems is typically done with finite datasets.
This means that conventional evaluation methodologies are only applicable in
offline experiments, where data and models are stationary. However, in real
world systems, user feedback is continuously generated, at unpredictable rates.
Given this setting, one important issue is how to evaluate algorithms in such a
streaming data environment. In this paper we propose a prequential evaluation
protocol for recommender systems, suitable for streaming data environments, but
also applicable in stationary settings. Using this protocol we are able to
monitor the evolution of algorithms' accuracy over time. Furthermore, we are
able to perform reliable comparative assessments of algorithms by computing
significance tests over a sliding window. We argue that besides being suitable
for streaming data, prequential evaluation allows the detection of phenomena
that would otherwise remain unnoticed in the evaluation of both offline and
online recommender systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08177</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08177</id><created>2015-04-30</created><updated>2015-05-29</updated><authors><author><keyname>Banerjee</keyname><forenames>Pradeep Kr.</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Nirmal B.</forenames></author></authors><title>Noise Sensitivity of Teager-Kaiser Energy Operators and Their Ratios</title><categories>cs.SD math.ST physics.data-an stat.TH</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Teager-Kaiser energy operator (TKO) belongs to a class of autocorrelators
and their linear combination that can track the instantaneous energy of a
nonstationary sinusoidal signal source. TKO-based monocomponent AM-FM
demodulation algorithms work under the basic assumption that the operator
outputs are always positive. In the absence of noise, this is assured for pure
sinusoidal inputs and the instantaneous property is also guaranteed. Noise
invalidates both of these, particularly under small signal conditions.
Post-detection filtering and thresholding are of use to reestablish these at
the cost of some time to acquire. Key questions are: (a) how many samples must
one use and (b) how much noise power at the detector input can one tolerate.
Results of study of the role of delay and the limits imposed by additive
Gaussian noise are presented along with the computation of the cumulants and
probability density functions of the individual quadratic forms and their
ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08183</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08183</id><created>2015-04-30</created><authors><author><keyname>Kutuzov</keyname><forenames>Andrey</forenames></author><author><keyname>Andreev</keyname><forenames>Igor</forenames></author></authors><title>Texts in, meaning out: neural language models in semantic similarity
  task for Russian</title><categories>cs.CL</categories><comments>Proceedings of the Dialog 2015 Conference. Moscow, Russia</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Distributed vector representations for natural language vocabulary get a lot
of attention in contemporary computational linguistics. This paper summarizes
the experience of applying neural network language models to the task of
calculating semantic similarity for Russian. The experiments were performed in
the course of Russian Semantic Similarity Evaluation track, where our models
took from the 2nd to the 5th position, depending on the task.
  We introduce the tools and corpora used, comment on the nature of the shared
task and describe the achieved results. It was found out that Continuous
Skip-gram and Continuous Bag-of-words models, previously successfully applied
to English material, can be used for semantic modeling of Russian as well.
Moreover, we show that texts in Russian National Corpus (RNC) provide an
excellent training material for such models, outperforming other, much larger
corpora. It is especially true for semantic relatedness tasks (although
stacking models trained on larger corpora on top of RNC models improves
performance even more).
  High-quality semantic vectors learned in such a way can be used in a variety
of linguistic tasks and promise an exciting field for further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08190</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08190</id><created>2015-04-30</created><authors><author><keyname>Risuleo</keyname><forenames>Riccardo Sven</forenames></author><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>A new kernel-based approach for overparameterized Hammerstein system
  identification</title><categories>cs.SY stat.ML</categories><comments>7 pages, submitted to IEEE Conference on Decision and Control 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new identification scheme for Hammerstein systems,
which are dynamic systems consisting of a static nonlinearity and a linear
time-invariant dynamic system in cascade. We assume that the nonlinear function
can be described as a linear combination of $p$ basis functions. We reconstruct
the $p$ coefficients of the nonlinearity together with the first $n$ samples of
the impulse response of the linear system by estimating an $np$-dimensional
overparameterized vector, which contains all the combinations of the unknown
variables. To avoid high variance in these estimates, we adopt a regularized
kernel-based approach and, in particular, we introduce a new kernel tailored
for Hammerstein system identification. We show that the resulting scheme
provides an estimate of the overparameterized vector that can be uniquely
decomposed as the combination of an impulse response and $p$ coefficients of
the static nonlinearity. We also show, through several numerical experiments,
that the proposed method compares very favorably with two standard methods for
Hammerstein system identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08193</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08193</id><created>2015-04-30</created><updated>2015-07-22</updated><authors><author><keyname>Gerencs&#xe9;r</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author></authors><title>Push sum with transmission failures</title><categories>cs.DC math.PR</categories><msc-class>90B18, 68M10, 68W15, 93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The push-sum algorithm allows distributed computing of the average on a
directed graph, and is particularly relevant when one is restricted to one-way
and/or asynchronous communications. We investigate its behavior in the presence
of unreliable communication channels where messages can be lost. We show that
convergence still holds, and analyze the error of the final common value we get
for the essential case of two nodes, both theoretically and numerically. We
compare this error performance with that of the standard consensus algorithm.
For the multi-node case, we deduce fundamental properties that implicitly
describe the distribution of the final value obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08196</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08196</id><created>2015-04-30</created><authors><author><keyname>Risuleo</keyname><forenames>Riccardo Sven</forenames></author><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>On the estimation of initial conditions in kernel-based system
  identification</title><categories>cs.SY stat.ML</categories><comments>8 pages, submitted to IEEE Conference on Decision and Control 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in system identification have brought attention to
regularized kernel-based methods, where, adopting the recently introduced
stable spline kernel, prior information on the unknown process is enforced.
This reduces the variance of the estimates and thus makes kernel-based methods
particularly attractive when few input-output data samples are available. In
such cases however, the influence of the system initial conditions may have a
significant impact on the output dynamics. In this paper, we specifically
address this point. We propose three methods that deal with the estimation of
initial conditions using different types of information. The methods consist in
various mixed maximum likelihood--a posteriori estimators which estimate the
initial conditions and tune the hyperparameters characterizing the stable
spline kernel. To solve the related optimization problems, we resort to the
expectation-maximization method, showing that the solutions can be attained by
iterating among simple update steps. Numerical experiments show the advantages,
in terms of accuracy in reconstructing the system impulse response, of the
proposed strategies, compared to other kernel-based schemes not accounting for
the effect initial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08200</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08200</id><created>2015-04-30</created><updated>2015-11-23</updated><authors><author><keyname>Tekin</keyname><forenames>Bugra</forenames></author><author><keyname>Sun</keyname><forenames>Xiaolu</forenames></author><author><keyname>Wang</keyname><forenames>Xinchao</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Predicting People's 3D Poses from Short Sequences</title><categories>cs.CV</categories><comments>superseded by arXiv:1511.06692</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient approach to exploiting motion information from
consecutive frames of a video sequence to recover the 3D pose of people.
Instead of computing candidate poses in individual frames and then linking
them, as is often done, we regress directly from a spatio-temporal block of
frames to a 3D pose in the central one. We will demonstrate that this approach
allows us to effectively overcome ambiguities and to improve upon the
state-of-the-art on challenging sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08211</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08211</id><created>2015-04-30</created><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Multidimensional beyond worst-case and almost-sure problems for
  mean-payoff objectives</title><categories>cs.GT cs.SY</categories><comments>Technical report a paper accepted to LICS'15</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The beyond worst-case threshold problem (BWC), recently introduced by
Bruy\`ere et al., asks given a quantitative game graph for the synthesis of a
strategy that i) enforces some minimal level of performance against any
adversary, and ii) achieves a good expectation against a stochastic model of
the adversary. They solved the BWC problem for finite-memory strategies and
unidimensional mean-payoff objectives and they showed membership of the problem
in NP$\cap$coNP. They also noted that infinite-memory strategies are more
powerful than finite-memory ones, but the respective threshold problem was left
open. We extend these results in several directions. First, we consider
multidimensional mean-payoff objectives. Second, we study both finite-memory
and infinite-memory strategies. We show that the multidimensional BWC problem
is coNP-complete in both cases. Third, in the special case when the worst-case
objective is unidimensional (but the expectation objective is still
multidimensional) we show that the complexity decreases to NP$\cap$coNP. This
solves the infinite-memory threshold problem left open by Bruy\`ere et al., and
this complexity cannot be improved without improving the currently known
complexity of classical mean-payoff games. Finally, we introduce a natural
relaxation of the BWC problem, the beyond almost-sure threshold problem (BAS),
which asks for the synthesis of a strategy that ensures some minimal level of
performance with probability one and a good expectation against the stochastic
model of the adversary. We show that the multidimensional BAS threshold problem
is solvable in P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08212</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08212</id><created>2015-04-30</created><authors><author><keyname>Fendji</keyname><forenames>Jean Louis Ebongue Kedieng</forenames></author><author><keyname>Thron</keyname><forenames>Christopher</forenames></author><author><keyname>Nlong</keyname><forenames>Jean Michel</forenames></author></authors><title>A Metropolis Approach for Mesh Router Nodes placement in Rural Wireless
  Mesh Networks</title><categories>cs.NI</categories><comments>14 pages</comments><journal-ref>Journal of Computers vol. 10, no. 2, pp. 101-114, 2015</journal-ref><doi>10.17706/jcp.10.2.101-114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless mesh networks appear as an appealing solution to reduce the digital
divide between rural and urban regions. However the placement of router nodes
is still a critical issue when planning this type of network, especially in
rural regions where we usually observe low density and sparse population. In
this paper, we firstly provide a network model tied to rural regions by
considering the area to cover as decomposed into a set of elementary areas
which can be required or optional in terms of coverage and where a node can be
placed or not. Afterwards, we try to determine an optimal number and positions
of mesh router nodes while maximizing the coverage of areas of interest,
minimizing the coverage of optional areas and ensuring connectivity of all mesh
router nodes. For that we propose a particularized algorithm based on
Metropolis approach to ensure an optimal coverage and connectivity with an
optimal number of routers. The proposed algorithm is evaluated on different
region instances. We obtained a required coverage between 94% and 97% and a
coverage percentage of optional areas less than 16% with an optimal number of
routers nr_max2 =1.3*nr_min , (nr_min being the minimum number of router which
is the ratio between the total area requiring coverage and the area which can
be covered by a router).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08213</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08213</id><created>2015-04-30</created><authors><author><keyname>Fendji</keyname><forenames>Jean Louis Ebongue Kedieng</forenames></author><author><keyname>Nlong</keyname><forenames>Jean Michel</forenames></author></authors><title>Rural Wireless Mesh Network: A Design Methodology</title><categories>cs.NI</categories><comments>9 pages, 2 figures, 3 tables</comments><journal-ref>International Journal of Communications, Network and System
  Sciences, 8, 1-9</journal-ref><doi>10.4236/ijcns.2015.81001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Mesh Network is presented as an appealing solution for bridging the
digital divide between developed and under-developed regions. But the planning
and deployment of these networks are not just a technical matter, since the
success depends on many other factors tied to the related region. Although we
observe some deployments, to ensure usefulness and sustainability, there is
still a need of concrete design process model and proper network planning
approach for rural regions, especially in Sub-Saharan Africa. This paper
presents a design methodology to provide network connectivity from a landline
node in a rural region at very low cost. We propose a methodology composed of
ten steps, starting by a deep analysis of the region in order to identify
relevant constraints and useful applications to sustain local activities and
communication. Approach for planning the physical architecture of the network
is based on an indoor-outdoor deployment for reducing the overall cost of the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08215</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08215</id><created>2015-04-30</created><authors><author><keyname>Rasmus</keyname><forenames>Antti</forenames></author><author><keyname>Valpola</keyname><forenames>Harri</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author></authors><title>Lateral Connections in Denoising Autoencoders Support Supervised
  Learning</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how a deep denoising autoencoder with lateral connections can be used
as an auxiliary unsupervised learning task to support supervised learning. The
proposed model is trained to minimize simultaneously the sum of supervised and
unsupervised cost functions by back-propagation, avoiding the need for
layer-wise pretraining. It improves the state of the art significantly in the
permutation-invariant MNIST classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08218</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08218</id><created>2015-04-30</created><authors><author><keyname>Minhas</keyname><forenames>Shahryar</forenames></author><author><keyname>Hoff</keyname><forenames>Peter D.</forenames></author><author><keyname>Ward</keyname><forenames>Michael D.</forenames></author></authors><title>Relax, Tensors Are Here: Dependencies in International Processes</title><categories>stat.AP cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous models of international conflict have suffered two shortfalls. They
tended not to embody dynamic changes, focusing rather on static slices of
behavior over time. These models have also been empirically evaluated in ways
that assumed the independence of each country, when in reality they are
searching for the interdependence among all countries. We illustrate a solution
to these two hurdles and evaluate this new, dynamic, network based approach to
the dependencies among the ebb and flow of daily international interactions
using a newly developed, and openly available, database of events among
nations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08219</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08219</id><created>2015-04-30</created><authors><author><keyname>Mac Aodha</keyname><forenames>Oisin</forenames></author><author><keyname>Campbell</keyname><forenames>Neill D. F.</forenames></author><author><keyname>Kautz</keyname><forenames>Jan</forenames></author><author><keyname>Brostow</keyname><forenames>Gabriel J.</forenames></author></authors><title>Hierarchical Subquery Evaluation for Active Learning on a Graph</title><categories>cs.CV cs.LG stat.ML</categories><comments>CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To train good supervised and semi-supervised object classifiers, it is
critical that we not waste the time of the human experts who are providing the
training labels. Existing active learning strategies can have uneven
performance, being efficient on some datasets but wasteful on others, or
inconsistent just between runs on the same dataset. We propose perplexity based
graph construction and a new hierarchical subquery evaluation algorithm to
combat this variability, and to release the potential of Expected Error
Reduction.
  Under some specific circumstances, Expected Error Reduction has been one of
the strongest-performing informativeness criteria for active learning. Until
now, it has also been prohibitively costly to compute for sizeable datasets. We
demonstrate our highly practical algorithm, comparing it to other active
learning measures on classification datasets that vary in sparsity,
dimensionality, and size. Our algorithm is consistent over multiple runs and
achieves high accuracy, while querying the human expert for labels at a
frequency that matches their desired time budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08231</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08231</id><created>2015-04-30</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Required Number of Antennas in a Point-to-Point Large-but-Finite
  MIMO System: Outage-Limited Scenario</title><categories>cs.IT math.IT</categories><comments>Under review in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of the point-to-point
multiple-input-multiple-output (MIMO) systems in the presence of a large but
finite numbers of antennas at the transmitters and/or receivers. Considering
the cases with and without hybrid automatic repeat request (HARQ) feedback, we
determine the minimum numbers of the transmit/receive antennas which are
required to satisfy different outage probability constraints. Our results are
obtained for different fading conditions and the effect of the power amplifiers
efficiency on the performance of the MIMO-HARQ systems is analyzed. Moreover,
we derive closed-form expressions for the asymptotic performance of the
MIMO-HARQ systems when the number of antennas increases. Our analytical and
numerical results show that different outage requirements can be satisfied with
relatively few transmit/receive antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08235</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08235</id><created>2015-04-30</created><authors><author><keyname>Fafianie</keyname><forenames>Stefan</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>A shortcut to (sun)flowers: Kernels in logarithmic space or linear time</title><categories>cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate whether kernelization results can be obtained if we restrict
kernelization algorithms to run in logarithmic space. This restriction for
kernelization is motivated by the question of what results are attainable for
preprocessing via simple and/or local reduction rules. We find kernelizations
for d-Hitting Set(k), d-Set Packing(k), Edge Dominating Set(k) and a number of
hitting and packing problems in graphs, each running in logspace. Additionally,
we return to the question of linear-time kernelization. For d-Hitting Set(k) a
linear-time kernelization was given by van Bevern [Algorithmica (2014)]. We
give a simpler procedure and save a large constant factor in the size bound.
Furthermore, we show that we can obtain a linear-time kernel for d-Set
Packing(k) as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08241</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08241</id><created>2015-04-30</created><authors><author><keyname>Ra&#xdf;</keyname><forenames>Alexander</forenames></author><author><keyname>Schmitt</keyname><forenames>Manuel</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>Explanation of Stagnation at Points that are not Local Optima in
  Particle Swarm Optimization by Potential Analysis</title><categories>cs.AI</categories><comments>Full version of poster on Genetic and Evolutionary Computation
  Conference (GECCO) 15</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle Swarm Optimization (PSO) is a nature-inspired meta-heuristic for
solving continuous optimization problems. In the literature, the potential of
the particles of swarm has been used to show that slightly modified PSO
guarantees convergence to local optima. Here we show that under specific
circumstances the unmodified PSO, even with swarm parameters known (from the
literature) to be good, almost surely does not yield convergence to a local
optimum is provided. This undesirable phenomenon is called stagnation. For this
purpose, the particles' potential in each dimension is analyzed mathematically.
Additionally, some reasonable assumptions on the behavior if the particles'
potential are made. Depending on the objective function and, interestingly, the
number of particles, the potential in some dimensions may decrease much faster
than in other dimensions. Therefore, these dimensions lose relevance, i.e., the
contribution of their entries to the decisions about attractor updates becomes
insignificant and, with positive probability, they never regain relevance. If
Brownian Motion is assumed to be an approximation of the time-dependent drop of
potential, practical, i.e., large values for this probability are calculated.
Finally, on chosen multidimensional polynomials of degree two, experiments are
provided showing that the required circumstances occur quite frequently.
Furthermore, experiments are provided showing that even when the very simple
sphere function is processed the described stagnation phenomenon occurs.
Consequently, unmodified PSO does not converge to any local optimum of the
chosen functions for tested parameter settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08245</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08245</id><created>2015-04-30</created><updated>2016-02-22</updated><authors><author><keyname>Koch</keyname><forenames>Tobias</forenames></author></authors><title>The Shannon Lower Bound is Asymptotically Tight</title><categories>cs.IT math.IT</categories><comments>13 pages, no figures. Replaced with version that has been submitted
  to IEEE Transactions on Information Theory. Lemma 1 has been generalized</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shannon lower bound is one of the few lower bounds on the rate-distortion
function that holds for a large class of sources. In this paper, it is
demonstrated that its gap to the rate-distortion function vanishes as the
allowed distortion tends to zero for all sources having a finite differential
entropy and whose integer part is finite. Conversely, it is demonstrated that
if the integer part of the source has an infinite entropy, then its
rate-distortion function is infinite for every finite distortion. Consequently,
the Shannon lower bound provides an asymptotically tight bound on the
rate-distortion function if, and only if, the integer part of the source has a
finite entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08247</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08247</id><created>2015-04-30</created><updated>2015-08-10</updated><authors><author><keyname>Feinerman</keyname><forenames>Ofer</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author></authors><title>Clock Synchronization and Distributed Estimation in Highly Dynamic
  Networks: An Information Theoretic Approach</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the External Clock Synchronization problem in dynamic sensor
networks. Initially, sensors obtain inaccurate estimations of an external time
reference and subsequently collaborate in order to synchronize their internal
clocks with the external time. For simplicity, we adopt the drift-free
assumption, where internal clocks are assumed to tick at the same pace. Hence,
the problem is reduced to an estimation problem, in which the sensors need to
estimate the initial external time. This work is further relevant to the
problem of collective approximation of environmental values by biological
groups.
  Unlike most works on clock synchronization that assume static networks, this
paper focuses on an extreme case of highly dynamic networks. Specifically, we
assume a non-adaptive scheduler adversary that dictates in advance an
arbitrary, yet independent, meeting pattern. Such meeting patterns fit, for
example, with short-time scenarios in highly dynamic settings, where each
sensor interacts with only few other arbitrary sensors.
  We propose an extremely simple clock synchronization algorithm that is based
on weighted averages, and prove that its performance on any given independent
meeting pattern is highly competitive with that of the best possible algorithm,
which operates without any resource or computational restrictions, and knows
the meeting pattern in advance. In particular, when all distributions involved
are Gaussian, the performances of our scheme coincide with the optimal
performances. Our proofs rely on an extensive use of the concept of Fisher
information. We use the Cramer-Rao bound and our definition of a Fisher Channel
Capacity to quantify information flows and to obtain lower bounds on collective
performance. This opens the door for further rigorous quantifications of
information flows within collaborative sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08248</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08248</id><created>2015-04-30</created><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Frugal Bribery in Voting</title><categories>cs.AI cs.MA</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bribery in elections is an important problem in computational social choice
theory. However, bribery with money is often illegal in elections. Motivated by
this, we introduce the notion of frugal bribery and formulate two new pertinent
computational problems which we call Frugal-bribery and Frugal- $bribery to
capture bribery without money in elections. In the proposed model, the briber
is frugal in nature and this is captured by her inability to bribe votes of a
certain kind, namely, non-vulnerable votes. In the Frugal-bribery problem, the
goal is to make a certain candidate win the election by changing only
vulnerable votes. In the Frugal-{dollar}bribery problem, the vulnerable votes
have prices and the goal is to make a certain candidate win the election by
changing only vulnerable votes, subject to a budget constraint of the briber.
We further formulate two natural variants of the Frugal-{dollar}bribery problem
namely Uniform-frugal-{dollar}bribery and Nonuniform-frugal-{dollar}bribery
where the prices of the vulnerable votes are, respectively, all the same or
different.
  We study the computational complexity of the above problems for unweighted
and weighted elections for several commonly used voting rules. We observe that,
even if we have only a small number of candidates, the problems are intractable
for all voting rules studied here for weighted elections, with the sole
exception of the Frugal-bribery problem for the plurality voting rule. In
contrast, we have polynomial time algorithms for the Frugal-bribery problem for
plurality, veto, k-approval, k-veto, and plurality with runoff voting rules for
unweighted elections. However, the Frugal-{dollar}bribery problem is
intractable for all the voting rules studied here barring the plurality and the
veto voting rules for unweighted elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08250</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08250</id><created>2015-04-30</created><updated>2015-07-07</updated><authors><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Rubin</keyname><forenames>Natan</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Beyond the Richter-Thomassen Conjecture</title><categories>math.CO cs.CG</categories><msc-class>05C10, 05C35, 05D99, 52C30, 52C45, 52C10</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If two closed Jordan curves in the plane have precisely one point in common,
then it is called a {\em touching point}. All other intersection points are
called {\em crossing points}. The main result of this paper is a Crossing Lemma
for closed curves: In any family of $n$ pairwise intersecting simple closed
curves in the plane, no three of which pass through the same point, the number
of crossing points exceeds the number of touching points by a factor of at
least $\Omega((\log\log n)^{1/8})$.
  As a corollary, we prove the following long-standing conjecture of Richter
and Thomassen: The total number of intersection points between any $n$ pairwise
intersecting simple closed curves in the plane, no three of which pass through
the same point, is at least $(1-o(1))n^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08251</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08251</id><created>2015-04-30</created><authors><author><keyname>Cornelissen</keyname><forenames>Kamiel</forenames></author><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author></authors><title>Smoothed Analysis of the Minimum-Mean Cycle Canceling Algorithm and the
  Network Simplex Algorithm</title><categories>cs.DS</categories><comments>Extended abstract to appear in the proceedings of COCOON 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum-cost flow (MCF) problem is a fundamental optimization problem
with many applications and seems to be well understood. Over the last half
century many algorithms have been developed to solve the MCF problem and these
algorithms have varying worst-case bounds on their running time. However, these
worst-case bounds are not always a good indication of the algorithms'
performance in practice. The Network Simplex (NS) algorithm needs an
exponential number of iterations for some instances, but it is considered the
best algorithm in practice and performs best in experimental studies. On the
other hand, the Minimum-Mean Cycle Canceling (MMCC) algorithm is strongly
polynomial, but performs badly in experimental studies.
  To explain these differences in performance in practice we apply the
framework of smoothed analysis. We show an upper bound of
$O(mn^2\log(n)\log(\phi))$ for the number of iterations of the MMCC algorithm.
Here $n$ is the number of nodes, $m$ is the number of edges, and $\phi$ is a
parameter limiting the degree to which the edge costs are perturbed. We also
show a lower bound of $\Omega(m\log(\phi))$ for the number of iterations of the
MMCC algorithm, which can be strengthened to $\Omega(mn)$ when
$\phi=\Theta(n^2)$. For the number of iterations of the NS algorithm we show a
smoothed lower bound of $\Omega(m \cdot \min \{ n, \phi \} \cdot \phi)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08256</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08256</id><created>2015-04-30</created><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Manipulation is Harder with Incomplete Votes</title><categories>cs.AI cs.MA</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Coalitional Manipulation (CM) problem has been studied extensively in the
literature for many voting rules. The CM problem, however, has been studied
only in the complete information setting, that is, when the manipulators know
the votes of the non-manipulators. A more realistic scenario is an incomplete
information setting where the manipulators do not know the exact votes of the
non- manipulators but may have some partial knowledge of the votes. In this
paper, we study a setting where the manipulators know a partial order for each
voter that is consistent with the vote of that voter. In this setting, we
introduce and study two natural computational problems - (1) Weak Manipulation
(WM) problem where the manipulators wish to vote in a way that makes their
preferred candidate win in at least one extension of the partial votes of the
non-manipulators; (2) Strong Manipulation (SM) problem where the manipulators
wish to vote in a way that makes their preferred candidate win in all possible
extensions of the partial votes of the non-manipulators. We study the
computational complexity of the WM and the SM problems for commonly used voting
rules such as plurality, veto, k-approval, k-veto, maximin, Copeland, and
Bucklin. Our key finding is that, barring a few exceptions, manipulation
becomes a significantly harder problem in the setting of incomplete votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08259</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08259</id><created>2015-04-30</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Otop</keyname><forenames>Jan</forenames></author></authors><title>Edit Distance for Pushdown Automata</title><categories>cs.FL</categories><comments>The full version of a paper accepted to ICALP 2015 with the same
  title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The edit distance between two words $w_1, w_2$ is the minimal number of word
operations (letter insertions, deletions, and substitutions) necessary to
transform $w_1$ to $w_2$. The edit distance generalizes to languages ${\cal
L}_1, {\cal L}_2$, where the edit distance is the minimal number $k$ such that
for every word from ${\cal L}_1$ there exists a word in ${\cal L}_2$ with edit
distance at most $k$. We study the edit distance computation problem between
pushdown automata and their subclasses. The problem of computing edit distance
to a pushdown automaton is undecidable, and in practice, the interesting
question is to compute the edit distance from a pushdown automaton (the
implementation, a standard model for programs with recursion) to a regular
language (the specification). In this work, we present a complete picture of
decidability and complexity for deciding whether, for a given threshold $k$,
the edit distance from a pushdown automaton to a finite automaton is at most
$k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08262</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08262</id><created>2015-04-30</created><authors><author><keyname>Yakovets</keyname><forenames>Nikolay</forenames></author><author><keyname>Godfrey</keyname><forenames>Parke</forenames></author><author><keyname>Gryz</keyname><forenames>Jarek</forenames></author></authors><title>Towards Query Optimization for SPARQL Property Paths</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extension of SPARQL in version 1.1 with property paths offers a type of
regular path query for RDF graph databases. Such queries are difficult to
optimize and evaluate efficiently, however. We have embarked on a project,
Waveguide, to build a cost-based optimizer for SPARQL queries with property
paths. Waveguide builds a query plan - a waveguide plan (WGP) - which guides
the query evaluation. There are numerous choices in the construction of a plan,
and a number of optimization methods, meaning the space of plans for a query
can be quite large. Execution costs of plans for the same query can vary by
orders of magnitude. We illustrate the types of optimizations this approach
affords and the performance gains that can be obtained. A WGP's costs can be
estimated, which opens the way to cost-based optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08265</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08265</id><created>2015-04-30</created><authors><author><keyname>Kantor</keyname><forenames>Erez</forenames></author><author><keyname>Kutten</keyname><forenames>Shay</forenames></author></authors><title>Optimal competitiveness for the Rectilinear Steiner Arborescence problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present optimal online algorithms for two related known problems involving
Steiner Arborescence, improving both the lower and the upper bounds. One of
them is the well studied continuous problem of the {\em Rectilinear Steiner
Arborescence} ($RSA$). We improve the lower bound and the upper bound on the
competitive ratio for $RSA$ from $O(\log N)$ and $\Omega(\sqrt{\log N})$ to
$\Theta(\frac{\log N}{\log \log N})$, where $N$ is the number of Steiner
points. This separates the competitive ratios of $RSA$ and the Symetric-$RSA$,
two problems for which the bounds of Berman and Coulston is STOC 1997 were
identical. The second problem is one of the Multimedia Content Distribution
problems presented by Papadimitriou et al. in several papers and Charikar et
al. SODA 1998. It can be viewed as the discrete counterparts (or a network
counterpart) of $RSA$. For this second problem we present tight bounds also in
terms of the network size, in addition to presenting tight bounds in terms of
the number of Steiner points (the latter are similar to those we derived for
$RSA$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08274</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08274</id><created>2015-04-30</created><authors><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Cellular-Broadcast Service Convergence through Caching for CoMP Cloud
  RANs</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE PIMRC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular and Broadcast services have been traditionally treated independently
due to the different market requirements, thus resulting in different business
models and orthogonal frequency allocations. However, with the advent of cheap
memory and smart caching, this traditional paradigm can converge into a single
system which can provide both services in an efficient manner. This paper
focuses on multimedia delivery through an integrated network, including both a
cellular (also known as unicast or broadband) and a broadcast last mile
operating over shared spectrum. The subscribers of the network are equipped
with a cache which can effectively create zero perceived latency for multimedia
delivery, assuming that the content has been proactively and intelligently
cached. The main objective of this work is to establish analytically the
optimal content popularity threshold, based on a intuitive cost function. In
other words, the aim is to derive which content should be broadcasted and which
content should be unicasted. To facilitate this, Cooperative Multi- Point
(CoMP) joint processing algorithms are employed for the uni and broad-cast PHY
transmissions. To practically implement this, the integrated network controller
is assumed to have access to traffic statistics in terms of content popularity.
Simulation results are provided to assess the gain in terms of total spectral
efficiency. A conventional system, where the two networks operate
independently, is used as benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08289</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08289</id><created>2015-04-30</created><updated>2015-12-05</updated><authors><author><keyname>Simon</keyname><forenames>Marcel</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author></authors><title>Neural Activation Constellations: Unsupervised Part Model Discovery with
  Convolutional Networks</title><categories>cs.CV</categories><comments>Published at IEEE International Conference on Computer Vision (ICCV)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part models of object categories are essential for challenging recognition
tasks, where differences in categories are subtle and only reflected in
appearances of small parts of the object. We present an approach that is able
to learn part models in a completely unsupervised manner, without part
annotations and even without given bounding boxes during learning. The key idea
is to find constellations of neural activation patterns computed using
convolutional neural networks. In our experiments, we outperform existing
approaches for fine-grained recognition on the CUB200-2011, NA birds, Oxford
PETS, and Oxford Flowers dataset in case no part or bounding box annotations
are available and achieve state-of-the-art performance for the Stanford Dog
dataset. We also show the benefits of neural constellation models as a data
augmentation technique for fine-tuning. Furthermore, our paper unites the areas
of generic and fine-grained classification, since our approach is suitable for
both scenarios. The source code of our method is available online at
http://www.inf-cv.uni-jena.de/part_discovery
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08291</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08291</id><created>2015-04-30</created><updated>2016-01-11</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex M.</forenames></author></authors><title>Deep Neural Networks with Random Gaussian Weights: A Universal
  Classification Strategy?</title><categories>cs.NE cs.LG stat.ML</categories><comments>14 pages, 13 figures</comments><msc-class>62M45</msc-class><acm-class>I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three important properties of a classification machinery are: (i) the system
preserves the core information of the input data; (ii) the training examples
convey information about unseen data; and (iii) the system is able to treat
differently points from different classes. In this work we show that these
fundamental properties are satisfied by the architecture of deep neural
networks. We formally prove that these networks with random Gaussian weights
perform a distance-preserving embedding of the data, with a special treatment
for in-class and out-of-class data. Similar points at the input of the network
are likely to have a similar output. The theoretical analysis of deep networks
here presented exploits tools used in the compressed sensing and dictionary
learning literature, thereby making a formal connection between these important
topics. The derived results allow drawing conclusions on the metric learning
properties of the network and their relation to its structure, as well as
providing bounds on the required size of the training set such that the
training examples would represent faithfully the unseen data. The results are
validated with state-of-the-art trained networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08308</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08308</id><created>2015-04-30</created><updated>2015-05-06</updated><authors><author><keyname>Zeppelzauer</keyname><forenames>Matthias</forenames></author><author><keyname>Seidl</keyname><forenames>Markus</forenames></author></authors><title>Efficient Image-Space Extraction and Representation of 3D Surface
  Topography</title><categories>cs.CV</categories><comments>Initial version of the paper accepted at the IEEE ICIP Conference
  2015</comments><acm-class>I.4; I.4.3; I.4.7; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surface topography refers to the geometric micro-structure of a surface and
defines its tactile characteristics (typically in the sub-millimeter range).
High-resolution 3D scanning techniques developed recently enable the 3D
reconstruction of surfaces including their surface topography. In his paper, we
present an efficient image-space technique for the extraction of surface
topography from high-resolution 3D reconstructions. Additionally, we filter
noise and enhance topographic attributes to obtain an improved representation
for subsequent topography classification. Comprehensive experiments show that
the our representation captures well topographic attributes and significantly
improves classification performance compared to alternative 2D and 3D
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08309</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08309</id><created>2015-04-30</created><authors><author><keyname>Botin&#x10d;an</keyname><forenames>Matko</forenames></author><author><keyname>Dodds</keyname><forenames>Mike</forenames></author><author><keyname>Magill</keyname><forenames>Stephen</forenames></author></authors><title>Refining Existential Properties in Separation Logic Analyses</title><categories>cs.LO cs.PL</categories><acm-class>F.3.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In separation logic program analyses, tractability is generally achieved by
restricting invariants to a finite abstract domain. As this domain cannot vary,
loss of information can cause failure even when verification is possible in the
underlying logic. In this paper, we propose a CEGAR-like method for detecting
spurious failures and avoiding them by refining the abstract domain. Our
approach is geared towards discovering existential properties, e.g. &quot;list
contains value x&quot;. To diagnose failures, we use abduction, a technique for
inferring command preconditions. Our method works backwards from an error,
identifying necessary information lost by abstraction, and refining the forward
analysis to avoid the error. We define domains for several classes of
existential properties, and show their effectiveness on case studies adapted
from Redis, Azureus and FreeRTOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08316</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08316</id><created>2015-04-30</created><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author></authors><title>Concentration of the number of solutions of random planted CSPs and
  Goldreich's one-way candidates</title><categories>math.PR cs.CC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the logarithm of the number of solutions of a random
planted $k$-SAT formula concentrates around a deterministic $n$-independent
threshold. Specifically, if $F^*_{k}(\alpha,n)$ is a random $k$-SAT formula on
$n$ variables, with clause density $\alpha$ and with a uniformly drawn planted
solution, there exists a function $\phi_k(\cdot)$ such that, besides for some
$\alpha$ in a set of Lesbegue measure zero, we have $ \frac{1}{n}\log
Z(F^*_{k}(\alpha,n)) \to \phi_k(\alpha)$ in probability, where $Z(F)$ is the
number of solutions of the formula $F$. This settles a problem left open in
Abbe-Montanari RANDOM 2013, where the concentration is obtained only for the
expected logarithm over the clause distribution. The result is also extended to
a more general class of random planted CSPs; in particular, it is shown that
the number of pre-images for the Goldreich one-way function model concentrates
for some choices of the predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08321</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08321</id><created>2015-04-30</created><updated>2015-05-19</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Ponse</keyname><forenames>Alban</forenames></author></authors><title>Evaluation trees for proposition algebra</title><categories>cs.LO</categories><comments>36 pages, 1 table</comments><msc-class>03B05, 03B70</msc-class><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proposition algebra is based on Hoare's conditional connective, which is a
ternary connective comparable to if-then-else and used in the setting of
propositional logic. Conditional statements are provided with a simple
semantics that is based on evaluation trees and that characterizes so-called
free valuation congruence: two conditional statements are free valuation
congruent if, and only if, they have equal evaluation trees. Free valuation
congruence is axiomatized by the four basic equational axioms of proposition
algebra that define the conditional connective. Valuation congruences that
identify more conditional statements than free valuation congruence are
repetition-proof, contractive, memorizing, and static valuation congruence.
Each of these valuation congruences is characterized using a transformation on
evaluation trees: two conditional statements are C-valuation congruent if, and
only if, their C-transformed evaluation trees are equal. These transformations
are simple and natural, and only for static valuation congruence a slightly
more complex transformation is used. Also, each of these valuation congruences
is axiomatized in proposition algebra. A spin-off of our approach is &quot;basic
form semantics for proposition algebra&quot;: for each valuation congruence C
considered, two conditional statements are C-valuation congruent if, and only
if, they have equal C-basic forms, where C-basic forms are obtained by a
syntactic transformation of conditional statements, which is a form of
normalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08333</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08333</id><created>2015-04-30</created><updated>2015-07-25</updated><authors><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Li</keyname><forenames>James</forenames></author></authors><title>Revenue-Maximizing Mechanism Design for Quasi-Proportional Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quasi-proportional auctions, each bidder receives a fraction of the
allocation equal to the weight of their bid divided by the sum of weights of
all bids, where each bid's weight is determined by a weight function. We study
the relationship between the weight function, bidders' private values, number
of bidders, and the seller's revenue in equilibrium. It has been shown that if
one bidder has a much higher private value than the others, then a nearly flat
weight function maximizes revenue. Essentially, threatening the bidder who has
the highest valuation with having to share the allocation maximizes the
revenue. We show that as bidder private values approach parity, steeper weight
functions maximize revenue by making the quasi-proportional auction more like a
winner-take-all auction. We also show that steeper weight functions maximize
revenue as the number of bidders increases. For flatter weight functions, there
is known to be a unique pure-strategy Nash equilibrium. We show that a
pure-strategy Nash equilibrium also exists for steeper weight functions, and we
give lower bounds for bids at an equilibrium. For a special case that includes
the two-bidder auction, we show that the pure-strategy Nash equilibrium is
unique, and we show how to compute the revenue at equilibrium. We also show
that selecting a weight function based on private value ratios and number of
bidders is necessary for a quasi-proportional auction to produce more revenue
than a second-price auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08339</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08339</id><created>2015-04-30</created><authors><author><keyname>Braberman</keyname><forenames>Victor</forenames></author><author><keyname>D'Ippolito</keyname><forenames>Nicolas</forenames></author><author><keyname>Kramer</keyname><forenames>Jeff</forenames></author><author><keyname>Sykes</keyname><forenames>Daniel</forenames></author><author><keyname>Uchitel</keyname><forenames>Sebastian</forenames></author></authors><title>MORPH: A Reference Architecture for Configuration and Behaviour
  Self-Adaptation</title><categories>cs.SE</categories><doi>10.1145/2804337.2804339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An architectural approach to self-adaptive systems involves runtime change of
system configuration (i.e., the system's components, their bindings and
operational parameters) and behaviour update (i.e., component orchestration).
Thus, dynamic reconfiguration and discrete event control theory are at the
heart of architectural adaptation. Although controlling configuration and
behaviour at runtime has been discussed and applied to architectural
adaptation, architectures for self-adaptive systems often compound these two
aspects reducing the potential for adaptability. In this paper we propose a
reference architecture that allows for coordinated yet transparent and
independent adaptation of system configuration and behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08342</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08342</id><created>2015-04-30</created><updated>2016-03-08</updated><authors><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author><author><keyname>Gildea</keyname><forenames>Daniel</forenames></author></authors><title>Parsing Linear Context-Free Rewriting Systems with Fast Matrix
  Multiplication</title><categories>cs.CL cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a matrix multiplication recognition algorithm for a subset of
binary linear context-free rewriting systems (LCFRS) with running time
$O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m
\times m$ matrix multiplication and $d$ is the &quot;contact rank&quot; of the LCFRS --
the maximal number of combination and non-combination points that appear in the
grammar rules. We also show that this algorithm can be used as a subroutine to
get a recognition algorithm for general binary LCFRS with running time
$O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than
$2.38$. Our result provides another proof for the best known result for parsing
mildly context sensitive formalisms such as combinatory categorial grammars,
head grammars, linear indexed grammars, and tree adjoining grammars, which can
be parsed in time $O(n^{4.76})$. It also shows that inversion transduction
grammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS
subsumes many other formalisms and types of grammars, for some of which we also
improve the asymptotic complexity of parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08352</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08352</id><created>2015-04-30</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Ko</keyname><forenames>Young Kun</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>ETH Hardness for Densest-$k$-Subgraph with Perfect Completeness</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, assuming the (deterministic) Exponential Time Hypothesis,
distinguishing between a graph with an induced $k$-clique and a graph in which
all k-subgraphs have density at most $1-\epsilon$, requires $n^{\tilde
\Omega(log n)}$ time. Our result essentially matches the quasi-polynomial
algorithms of Feige and Seltser [FS97] and Barman [Bar15] for this problem, and
is the first one to rule out an additive PTAS for Densest $k$-Subgraph. We
further strengthen this result by showing that our lower bound continues to
hold when, in the soundness case, even subgraphs smaller by a near-polynomial
factor ($k' = k 2^{-\tilde \Omega (log n)}$) are assumed to be at most
($1-\epsilon$)-dense.
  Our reduction is inspired by recent applications of the &quot;birthday repetition&quot;
technique [AIM14,BKW15]. Our analysis relies on information theoretical
machinery and is similar in spirit to analyzing a parallel repetition of
two-prover games in which the provers may choose to answer some challenges
multiple times, while completely ignoring other challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08360</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08360</id><created>2015-04-30</created><updated>2015-05-16</updated><authors><author><keyname>Gupta</keyname><forenames>Manoj</forenames></author></authors><title>Simple and Faster algorithm for Reachability in a Decremental Directed
  Graph</title><categories>cs.DS</categories><comments>This paper is withdrawn by the author due to a crucial error in Lemma
  3.4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of maintaining source sink
reachability($st$-Reachability), single source reachability(SSR) and strongly
connected component(SCC) in an edge decremental directed graph. In particular,
we design a randomized algorithm that maintains with high probability:
  1) $st$-Reachability in $\tilde{O}(mn^{4/5})$ total update time. 2)
$st$-Reachability in a total update time of $\tilde{O}(n^{8/3})$ in a dense
graph. 3) SSR in a total update time of $\tilde{O}(m n^{9/10})$. 4) SCC in a
total update time of $\tilde{O}(m n^{9/10})$. For all the above problems, we
improve upon the previous best algorithm (by Henzinger et. al. (STOC 2014)).
  Our main focus is maintaining $st$-Reachability in an edge decremental
directed graph (other problems can be reduced to $st$-Reachability). The
classical algorithm of Even and Shiloach (JACM 81) solved this problem in
$O(1)$ query time and $O(mn)$ total update time. Recently, Henzinger,
Krinninger and Nanongkai (STOC 2014) designed a randomized algorithm which
achieves an update time of $\tilde{O}(m n^{0.98})$ and broke the long-standing
$O(mn)$ bound of Even and Shiloach. However, they designed four algorithms $A_i
(1\le i \le 4)$ such that for graphs having total number of edges between $m_i$
and $m_{i+1}$ ($m_{i+1} &gt; m_i$), $A_i$ outperforms other three algorithms. That
is, one of the four algorithms may be faster for a particular density range of
edges, but it may be too slow asymptotically for the other ranges. Our main
contribution is that we design a {\it single} algorithm which works for all
types of graphs. Not only is our algorithm faster, it is much simpler than the
algorithm designed by Henzinger et.al. (STOC 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08361</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08361</id><created>2015-04-30</created><updated>2016-02-07</updated><authors><author><keyname>Chen</keyname><forenames>Jing</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>McCauley</keyname><forenames>Samuel</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>Singh</keyname><forenames>Shikha</forenames><affiliation>Stony Brook University</affiliation></author></authors><title>Rational Proofs with Multiple Provers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive proofs model a world where a verifier delegates computation to an
untrustworthy prover, verifying the prover's claims before accepting them.
Rational proofs, introduced by Azar and Micali (STOC 2012), are an interactive
proof model in which the prover is rational rather than untrustworthy---he may
lie, but only to increase his payment (received from the verifier). This allows
the verifier to leverage the greed of the prover to obtain better protocols:
while rational proofs are no more powerful than interactive proofs, the
protocols are simpler and more efficient. Azar and Micali posed as an open
problem whether multiple provers are more powerful than one for rational
proofs.
  We provide a model that extends rational proofs to allow multiple provers. In
this model, a verifier can crosscheck the answers received by asking several
provers. The verifier can pay the provers according to the quality of their
work, incentivizing them to provide correct information.
  We analyze rational proofs with multiple provers from a complexity-theoretic
point of view. We fully characterize this model by giving tight upper and lower
bounds on its power. On the way, we resolve Azar and Micali's open problem in
the affirmative, showing that multiple rational provers are strictly more
powerful than one (under standard complexity-theoretic assumptions). We further
show that the full power of rational proofs with multiple provers can be
achieved using only two provers and five rounds of interaction. Finally, we
consider more demanding models where the verifier wants the provers' payment to
decrease significantly when they are lying, and fully characterize the power of
the model when the payment gap must be noticeable (i.e., at least 1/p where p
is a polynomial).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08362</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08362</id><created>2015-04-30</created><updated>2016-01-07</updated><authors><author><keyname>Figurnov</keyname><forenames>Michael</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>PerforatedCNNs: Acceleration through Elimination of Redundant
  Convolutions</title><categories>cs.CV</categories><comments>Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach to reduce the computational cost of evaluation of
convolutional neural networks, a factor that has hindered their deployment in
low-power devices such as mobile phones. Inspired by the loop perforation
technique from source code optimization, we speed up the bottleneck
convolutional layers by skipping their evaluation in some of the spatial
positions. We propose and analyze several strategies of choosing these
positions. Our method allows to reduce the evaluation time of modern
convolutional neural networks by 50% with a small decrease in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08363</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08363</id><created>2015-04-30</created><updated>2015-11-23</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>On the Structure, Covering, and Learning of Poisson Multinomial
  Distributions</title><categories>cs.DS cs.LG math.PR math.ST stat.TH</categories><comments>49 pages, extended abstract appeared in FOCS 2015</comments><doi>10.1109/FOCS.2015.77</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the
sum of $n$ independent random vectors supported on the set ${\cal
B}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We prove
a structural characterization of these distributions, showing that, for all
$\varepsilon &gt;0$, any $(n, k)$-Poisson multinomial random vector is
$\varepsilon$-close, in total variation distance, to the sum of a discretized
multidimensional Gaussian and an independent $(\text{poly}(k/\varepsilon),
k)$-Poisson multinomial random vector. Our structural characterization extends
the multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to
all approximation requirements $\varepsilon$. In particular, it overcomes
factors depending on $\log n$ and, importantly, the minimum eigenvalue of the
PMD's covariance matrix from the distance to a multidimensional Gaussian random
variable.
  We use our structural characterization to obtain an $\varepsilon$-cover, in
total variation distance, of the set of all $(n, k)$-PMDs, significantly
improving the cover size of Daskalakis and Papadimitriou, and obtaining the
same qualitative dependence of the cover size on $n$ and $\varepsilon$ as the
$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure
to show that $(n,k)$-PMDs can be learned to within $\varepsilon$ in total
variation distance from $\tilde{O}_k(1/\varepsilon^2)$ samples, which is
near-optimal in terms of dependence on $\varepsilon$ and independent of $n$. In
particular, our result generalizes the single-dimensional result of Daskalakis,
Diakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08366</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08366</id><created>2015-04-30</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Reentrant phase transitions and defensive alliances in social dilemmas
  with informed strategies</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>6 two-column pages, 5 figures; accepted for publication in
  Europhysics Letters</comments><journal-ref>EPL 110 (2015) 38003</journal-ref><doi>10.1209/0295-5075/110/38003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing the strategy of an opponent in a competitive environment conveys
obvious evolutionary advantages. But this information is costly, and the
benefit of being informed may not necessarily offset the additional cost. Here
we introduce social dilemmas with informed strategies, and we show that this
gives rise to two cyclically dominant triplets that form defensive alliances.
The stability of these two alliances is determined by the rotation velocity of
the strategies within each triplet. A weaker strategy in a faster rotating
triplet can thus overcome an individually stronger competitor. Fascinating
spatial patterns favor the dominance of a single defensive alliance, but enable
also the stable coexistence of both defensive alliances in very narrow regions
of the parameter space. A continuous reentrant phase transition reveals before
unseen complexity behind the stability of strategic alliances in evolutionary
social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.08367</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.08367</id><created>2015-04-28</created><authors><author><keyname>Bera</keyname><forenames>Debasish</forenames></author><author><keyname>Pathak</keyname><forenames>Sant S.</forenames></author><author><keyname>Chakrabarty</keyname><forenames>Indrajit</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Another look in the Analysis of Cooperative Spectrum Sensing over
  Nakagami-$m$ Fading Channels</title><categories>cs.IT math.IT</categories><comments>29 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling and analysis of cooperative spectrum sensing is an important aspect
in cognitive radio systems. In this paper, the problem of energy detection (ED)
of an unknown signal over Nakagami-$m$ fading is revisited. Specifically, an
analytical expression for the local probability of detection is derived, while
using the approach of ED at the individual secondary user (SU), a new fusion
rule, based on the likelihood ratio test, is presented. The channels between
the primary user to SUs and SUs to fusion center are considered to be
independent Nakagami-$m$. The proposed fusion rule uses the channel statistics,
instead of the instantaneous channel state information, and is based on the
Neyman-Pearson criteria. Closed-form solutions for the system-level probability
of detection and probability of false alarm are also derived. Furthermore, a
closed-form expression for the optimal number of cooperative SUs, needed to
minimize the total error rate, is presented. The usefulness of factor graph and
sum-product-algorithm models for computing likelihoods, is also discussed to
highlight its advantage, in terms of computational cost. The performance of the
proposed schemes have been evaluated both by analysis and simulations. Results
show that the proposed rules perform well over a wide range of the
signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00001</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00001</id><created>2015-04-29</created><authors><author><keyname>Br&#xe6;ndeland</keyname><forenames>Asbj&#xf8;rn</forenames></author></authors><title>Rule based lexicographical permutation sequences</title><categories>cs.DS</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a permutation sequence built by means of sub permutations the transition
between successive permutations are subject to a set of n(n - 1)/2 rules that
group into n - 1 matrices with a high degree of regularity. By means of these
rules the sequence can be produced in O(3n!) time and O(n^3) space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00002</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00002</id><created>2015-04-29</created><authors><author><keyname>Di Franco</keyname><forenames>Anthony</forenames></author></authors><title>FIFTH system for general-purpose connectionist computation</title><categories>cs.AI</categories><comments>Submitted, COSYNE 2015 (extended abstract)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, work on formalizing connectionist computation in a way that is at
least Turing-complete has focused on recurrent architectures and developed
equivalences to Turing machines or similar super-Turing models, which are of
more theoretical than practical significance. We instead develop connectionist
computation within the framework of information propagation networks extended
with unbounded recursion, which is related to constraint logic programming and
is more declarative than the semantics typically used in practical programming,
but is still formally known to be Turing-complete. This approach yields
contributions to the theory and practice of both connectionist computation and
programming languages. Connectionist computations are carried out in a way that
lets them communicate with, and be understood and interrogated directly in
terms of the high-level semantics of a general-purpose programming language.
Meanwhile, difficult (unbounded-dimension, NP-hard) search problems in
programming that have previously been left to the programmer to solve in a
heuristic, domain-specific way are solved uniformly a priori in a way that
approximately achieves information-theoretic limits on performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00005</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00005</id><created>2015-04-29</created><authors><author><keyname>Bozorgi</keyname><forenames>Masoud</forenames></author><author><keyname>Nayak</keyname><forenames>Rohan</forenames></author><author><keyname>Zaffar</keyname><forenames>Arslan</forenames></author><author><keyname>Hoque</keyname><forenames>Mohammad Iftekharul</forenames></author><author><keyname>Ghouri</keyname><forenames>Saad Anwer</forenames></author><author><keyname>Singh</keyname><forenames>Harmeet</forenames></author><author><keyname>Kalshan</keyname><forenames>Parminder Singh</forenames></author></authors><title>A Case Study on Quality Attribute Measurement using MARF and GIPSY</title><categories>cs.SE</categories><comments>50 pages, 58 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This literature focuses on doing a comparative analysis between Modular Audio
Recognition Framework (MARF) and the General Intentional Programming System
(GIPSY) with the help of different software metrics. At first, we understand
the general principles, architecture and working of MARF and GIPSY by looking
at their frameworks and running them in the Eclipse environment. Then, we study
some of the important metrics including a few state of the art metrics and rank
them in terms of their usefulness and their influence on the different quality
attributes of a software. The quality attributes are viewed and computed with
the help of the Logiscope and McCabe IQ tools. These tools perform a
comprehensive analysis on the case studies and generate a quality report at the
factor level, criteria level and metrics level. In next step, we identify the
worst code at each of these levels, extract the worst code and provide
recommendations to improve the quality. We implement and test some of the
metrics which are ranked as the most useful metrics with a set of test cases in
JDeodorant. Finally, we perform an analysis on both MARF and GIPSY by doing a
fuzzy code scan using MARFCAT to find the list of weak and vulnerable classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00017</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00017</id><created>2015-04-30</created><authors><author><keyname>Hannan</keyname><forenames>Tyler</forenames></author><author><keyname>Holtz</keyname><forenames>Chester</forenames></author><author><keyname>Liao</keyname><forenames>Jonathan</forenames></author></authors><title>Comparative Analysis of Classic Garbage-Collection Algorithms for a
  Lisp-like Language</title><categories>cs.PL</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we demonstrate the effectiveness of Cheney's Copy Algorithm
for a Lisp-like system and experimentally show the infeasability of developing
an optimal garbage collector for general use. We summarize and compare several
garbage-collection algorithms including Cheney's Algorithm, the canonical Mark
and Sweep Algorithm, and Knuth's Classical Lisp 2 Algorithm. We implement and
analyze these three algorithms in the context of a custom MicroLisp
environment. We conclude and present the core considerations behind the
development of a garbage collector---specifically for Lisp---and make an
attempt to investigate these issues in depth. We also discuss experimental
results that imply the effectiveness of Cheney's algorithm over Mark-Sweep for
Lisp-like languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00019</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00019</id><created>2015-04-30</created><authors><author><keyname>Zolotov</keyname><forenames>Boris</forenames></author></authors><title>Another Solution to the Thue Problem of Non-Repeating Words</title><categories>math.CO cs.FL</categories><comments>27 pages, 8 figures</comments><msc-class>05E99</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work we consider morphisms that preserve well-known non-repeating
properties: squarefreeness, cubefreeness, overlap-freeness and weak
squarefreeness. Up to the present moment only the morphisms preserving three
out of four non-repeating properties have been known. The problem of the
existence of weakly squarefree morphisms was open.
  The essential result of this work is the positive solution to this problem.
An example of the morphism preserving all four properties is provided. Also, it
is proved that there are no morphisms with the same properties and a lower
rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00023</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00023</id><created>2015-04-30</created><authors><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Ichter</keyname><forenames>Brian</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Deterministic Sampling-Based Motion Planning: Optimality, Complexity,
  and Performance</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic sampling-based algorithms, such as the probabilistic roadmap
(PRM) and the rapidly-exploring random tree (RRT) algorithms, represent one of
the most successful approaches to robotic motion planning, due to their strong
theoretical properties (in terms of probabilistic completeness or even
asymptotic optimality) and remarkable practical performance. Such algorithms
are probabilistic in that they compute a path by connecting independently and
identically distributed random points in the configuration space. Their
randomization aspect, however, makes several tasks challenging, including
certification for safety-critical applications and use of offline computation
to improve real-time execution. Hence, an important open question is whether
similar (or better) theoretical guarantees and practical performance could be
obtained by considering deterministic, as opposed to random sampling sequences.
The objective of this paper is to provide a rigorous answer to this question.
Specifically, we first show that PRM, for a certain selection of tuning
parameters and deterministic low-dispersion sampling sequences, is
deterministically asymptotically optimal. Second, we characterize the
convergence rate, and we find that the factor of sub-optimality can be very
explicitly upper-bounded in terms of the l2-dispersion of the sampling sequence
and the connection radius of PRM. Third, we show that an asymptotically optimal
version of PRM exists with computational and space complexity arbitrarily close
to O(n) (the theoretical lower bound), where n is the number of points in the
sequence. This is in stark contrast to the O(n logn) complexity results for
existing asymptotically-optimal probabilistic planners. Finally, through
numerical experiments, we show that planning with deterministic low-dispersion
sampling generally provides superior performance in terms of path cost and
success rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00026</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00026</id><created>2015-04-30</created><authors><author><keyname>Xiao</keyname><forenames>Zhiqing</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Yunzhou</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author></authors><title>Distributed Multilevel Diversity Coding</title><categories>cs.IT math.IT</categories><comments>38 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed multilevel diversity coding, $K$ correlated sources (each with
$K$ components) are encoded in a distributed manner such that, given the
outputs from any $\alpha$ encoders, the decoder can reconstruct the first
$\alpha$ components of each of the corresponding $\alpha$ sources. For this
problem, the optimality of a multilayer Slepian-Wolf coding scheme based on
binning and superposition is established when $K\leq 3$. The same conclusion is
shown to hold for general $K$ under a certain symmetry condition, which
generalizes a celebrated result by Yeung and Zhang.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00036</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00036</id><created>2015-04-30</created><authors><author><keyname>Datta</keyname><forenames>Amit</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Zick</keyname><forenames>Yair</forenames></author></authors><title>Influence in Classification via Cooperative Game Theory</title><categories>cs.GT</categories><comments>accepted to IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dataset has been classified by some unknown classifier into two types of
points. What were the most important factors in determining the classification
outcome? In this work, we employ an axiomatic approach in order to uniquely
characterize an influence measure: a function that, given a set of classified
points, outputs a value for each feature corresponding to its influence in
determining the classification outcome. We show that our influence measure
takes on an intuitive form when the unknown classifier is linear. Finally, we
employ our influence measure in order to analyze the effects of user profiling
on Google's online display advertising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00039</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00039</id><created>2015-04-30</created><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Zick</keyname><forenames>Yair</forenames></author></authors><title>Learning Cooperative Games</title><categories>cs.GT</categories><comments>accepted to IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores a PAC (probably approximately correct) learning model in
cooperative games. Specifically, we are given $m$ random samples of coalitions
and their values, taken from some unknown cooperative game; can we predict the
values of unseen coalitions? We study the PAC learnability of several
well-known classes of cooperative games, such as network flow games, threshold
task games, and induced subgraph games. We also establish a novel connection
between PAC learnability and core stability: for games that are efficiently
learnable, it is possible to find payoff divisions that are likely to be stable
using a polynomial number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00040</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00040</id><created>2015-04-30</created><authors><author><keyname>Ragab</keyname><forenames>Mohammad Ehab</forenames></author></authors><title>Overlapping and Non-overlapping Camera Layouts for Robot Pose Estimation</title><categories>cs.CV</categories><comments>7 pages, 5 figures</comments><journal-ref>IJCSI - March 2015 Issue (Volume 12, Issue 2)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of overlapping and non-overlapping camera layouts in
estimating the ego-motion of a moving robot. To estimate the location and
orientation of the robot, we investigate using four cameras as non-overlapping
individuals, and as two stereo pairs. The pros and cons of the two approaches
are elucidated. The cameras work independently and can have larger field of
view in the non-overlapping layout. However, a scale factor ambiguity should be
dealt with. On the other hand, stereo systems provide more accuracy but require
establishing feature correspondence with more computational demand. For both
approaches, the extended Kalman filter is used as a real-time recursive
estimator. The approaches studied are verified with synthetic and real
experiments alike.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00043</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00043</id><created>2015-04-30</created><authors><author><keyname>Tian</keyname><forenames>Yun</forenames></author><author><keyname>Xu</keyname><forenames>Bojian</forenames></author><author><keyname>Ji</keyname><forenames>Yanqing</forenames></author><author><keyname>Scholer</keyname><forenames>Jesse</forenames></author></authors><title>CloudTree: A Library to Extend Cloud Services for Trees</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a library that enables on a cloud the creation and
management of tree data structures from a cloud client. As a proof of concept,
we implement a new cloud service CloudTree. With CloudTree, users are able to
organize big data into tree data structures of their choice that are physically
stored in a cloud. We use caching, prefetching, and aggregation techniques in
the design and implementation of CloudTree to enhance performance. We have
implemented the services of Binary Search Trees (BST) and Prefix Trees as
current members in CloudTree and have benchmarked their performance using the
Amazon Cloud. The idea and techniques in the design and implementation of a BST
and prefix tree is generic and thus can also be used for other types of trees
such as B-tree, and other link-based data structures such as linked lists and
graphs. Preliminary experimental results show that CloudTree is useful and
efficient for various big data applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00044</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00044</id><created>2015-04-30</created><authors><author><keyname>Staples</keyname><forenames>Patrick C.</forenames></author><author><keyname>Ogburn</keyname><forenames>Elizabeth L.</forenames></author><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames></author></authors><title>Incorporating Contact Network Structure in Cluster Randomized Trials</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>20 Pages, 6 Figures, and 2 Tables. Supplement contains 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whenever possible, the efficacy of a new treatment, such as a drug or
behavioral intervention, is investigated by randomly assigning some individuals
to a treatment condition and others to a control condition, and comparing the
outcomes between the two groups. Often, when the treatment aims to slow an
infectious disease, groups or clusters of individuals are assigned en masse to
each treatment arm. The structure of interactions within and between clusters
can reduce the power of the trial, i.e. the probability of correctly detecting
a real treatment effect. We investigate the relationships among power,
within-cluster structure, between-cluster mixing, and infectivity by simulating
an infectious process on a collection of clusters. We demonstrate that current
power calculations may be conservative for low levels of between-cluster
mixing, but failing to account for moderate or high amounts can result in
severely underpowered studies. Power also depends on within-cluster network
structure for certain kinds of infectious spreading. Infections that spread
opportunistically through very highly connected individuals have unpredictable
infectious breakouts, which makes it harder to distinguish between random
variation and real treatment effects. Our approach can be used before
conducting a trial to assess power using network information if it is
available, and we demonstrate how empirical data can inform the extent of
between-cluster mixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00055</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00055</id><created>2015-04-30</created><authors><author><keyname>Simkin</keyname><forenames>M. V.</forenames></author><author><keyname>Roychowdhury</keyname><forenames>V. P.</forenames></author></authors><title>Chess players' fame versus their merit</title><categories>physics.soc-ph cs.CY stat.AP</categories><comments>To appear in Applied Economics Letters</comments><journal-ref>Applied Economics Letters 22(18):1499-1504, 2015</journal-ref><doi>10.1080/13504851.2015.1042135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a pool of international chess title holders born between 1901
and 1943. Using Elo ratings we compute for every player his expected score in a
game with a randomly selected player from the pool. We use this figure as
player's merit. We measure players' fame as the number of Google hits. The
correlation between fame and merit is 0.38. At the same time the correlation
between the logarithm of fame and merit is 0.61. This suggests that fame grows
exponentially with merit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00058</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00058</id><created>2015-04-28</created><authors><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author><author><keyname>Li</keyname><forenames>GuoZhong</forenames></author><author><keyname>Wang</keyname><forenames>Xinyang</forenames></author><author><keyname>Xiong</keyname><forenames>Qin</forenames></author><author><keyname>Jiang</keyname><forenames>Yaqiu</forenames></author></authors><title>Transforming NP to P: An Approach to Solve NP Complete Problems</title><categories>cs.CC</categories><comments>10 pages</comments><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  NP complete problem is one of the most challenging issues. The question of
whether all problems in NP are also in P is generally considered one of the
most important open questions in mathematics and theoretical computer science
as it has far-reaching consequences to other problems in mathematics, computer
science, biology, philosophy and cryptography. There are intensive research on
proving `NP not equal to P' and `NP equals to P'. However, none of the `proved'
results is commonly accepted by the research community up to now. In this
paper, instead of proving either one, we aim to provide new perspective:
transforming two typical NP complete problems to exactly solvable P problems in
polynomial time. This approach helps to solve originally NP complete problems
with practical applications. It may shine light on solving other NP complete
problems in similar way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00061</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00061</id><created>2015-04-30</created><authors><author><keyname>Ramos</keyname><forenames>Marcus Vin&#xed;cius Midena</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author></authors><title>Context-Free Language Theory Formalization</title><categories>cs.FL cs.LO</categories><comments>52 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof assistants are software-based tools that are used in the mechanization
of proof construction and validation in mathematics and computer science, and
also in certified program development. Different tools are being increasingly
used in order to accelerate and simplify proof checking. Context-free language
theory is a well-established area of mathematics, relevant to computer science
foundations and technology. This proposal aims at formalizing parts of
context-free language theory in the Coq proof assistant. This report presents
the underlying theory and general characteristics of proof assistants,
including Coq itself, discusses its use in relevant formalization projects,
presents the current status of the implementation, addresses related projects
and the contributions of this work. The results obtained so far include the
formalization of closure properties for context-free grammars (under union,
concatenation and closure) and the formalization of grammar simplification.
Grammar simplification is a subject of high importance in computer language
processing technology as well as in formal language theory, and the
formalization refers to the fact that general context-free grammars generate
languages that can be also generated by simpler and equivalent context-free
grammars. Namely, useless symbol elimination, inaccessible symbol elimination,
unit rules elimination and empty rules elimination operations were described
and proven correct with respect to the preservation of the language generated
by the original grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00062</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00062</id><created>2015-04-30</created><authors><author><keyname>Appleton</keyname><forenames>Ben</forenames></author><author><keyname>O'Reilly</keyname><forenames>Michael</forenames></author></authors><title>Multi-probe consistent hashing</title><categories>cs.DS</categories><comments>15 pages, 3 figures</comments><acm-class>E.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a consistent hashing algorithm which performs multiple lookups
per key in a hash table of nodes. It requires no additional storage beyond the
hash table, and achieves a peak-to-average load ratio of 1 + epsilon with just
1 + 1/epsilon lookups per key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00066</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00066</id><created>2015-04-30</created><updated>2015-09-28</updated><authors><author><keyname>Tulsiani</keyname><forenames>Shubham</forenames></author><author><keyname>Carreira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Pose Induction for Novel Object Categories</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the task of predicting pose for objects of unannotated object
categories from a small seed set of annotated object classes. We present a
generalized classifier that can reliably induce pose given a single instance of
a novel category. In case of availability of a large collection of novel
instances, our approach then jointly reasons over all instances to improve the
initial estimates. We empirically validate the various components of our
algorithm and quantitatively show that our method produces reliable pose
estimates. We also show qualitative results on a diverse set of classes and
further demonstrate the applicability of our system for learning shape models
of novel object classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00073</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00073</id><created>2015-04-30</created><authors><author><keyname>Huynh</keyname><forenames>Lisa</forenames></author><author><keyname>Gingold</keyname><forenames>Yotam</forenames></author></authors><title>Bijective Deformations in $\mathbb{R}^n$ via Integral Curve Coordinates</title><categories>cs.GR</categories><msc-class>37E30</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Integral Curve Coordinates, which identify each point in a
bounded domain with a parameter along an integral curve of the gradient of a
function $f$ on that domain; suitable functions have exactly one critical
point, a maximum, in the domain, and the gradient of the function on the
boundary points inward. Because every integral curve intersects the boundary
exactly once, Integral Curve Coordinates provide a natural bijective mapping
from one domain to another given a bijection of the boundary. Our approach can
be applied to shapes in any dimension, provided that the boundary of the shape
(or cage) is topologically equivalent to an $n$-sphere. We present a simple
algorithm for generating a suitable function space for $f$ in any dimension. We
demonstrate our approach in 2D and describe a practical (simple and robust)
algorithm for tracing integral curves on a (piecewise-linear) triangulated
regular grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00074</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00074</id><created>2015-04-30</created><updated>2015-05-25</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Rithwik</keyname><forenames>Kollipara</forenames></author></authors><title>Image Denoising using Optimally Weighted Bilateral Filters: A Sure and
  Fast Approach</title><categories>cs.CV</categories><comments>To appear in the IEEE International Conference on Image Processing
  (ICIP 2015). Link to the Matlab code added in the revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bilateral filter is known to be quite effective in denoising images
corrupted with small dosages of additive Gaussian noise. The denoising
performance of the filter, however, is known to degrade quickly with the
increase in noise level. Several adaptations of the filter have been proposed
in the literature to address this shortcoming, but often at a substantial
computational overhead. In this paper, we report a simple pre-processing step
that can substantially improve the denoising performance of the bilateral
filter, at almost no additional cost. The modified filter is designed to be
robust at large noise levels, and often tends to perform poorly below a certain
noise threshold. To get the best of the original and the modified filter, we
propose to combine them in a weighted fashion, where the weights are chosen to
minimize (a surrogate of) the oracle mean-squared-error (MSE). The
optimally-weighted filter is thus guaranteed to perform better than either of
the component filters in terms of the MSE, at all noise levels. We also provide
a fast algorithm for the weighted filtering. Visual and quantitative denoising
results on standard test images are reported which demonstrate that the
improvement over the original filter is significant both visually and in terms
of PSNR. Moreover, the denoising performance of the optimally-weighted
bilateral filter is competitive with the computation-intensive non-local means
filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00075</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00075</id><created>2015-04-30</created><authors><author><keyname>Zheng</keyname><forenames>Shaoqiu</forenames></author><author><keyname>Li</keyname><forenames>Junzhi</forenames></author><author><keyname>Janecek</keyname><forenames>Andreas</forenames></author><author><keyname>Tan</keyname><forenames>Ying</forenames></author></authors><title>A Cooperative Framework for Fireworks Algorithm</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a cooperative framework for fireworks algorithm (CoFFWA).
A detailed analysis of existing fireworks algorithm (FWA) and its recently
developed variants has revealed that (i) the selection strategy lead to the
contribution of the firework with the best fitness (core firework) for the
optimization overwhelms the contributions of the rest of fireworks (non-core
fireworks) in the explosion operator, (ii) the Gaussian mutation operator is
not as effective as it is designed to be. To overcome these limitations, the
CoFFWA is proposed, which can greatly enhance the exploitation ability of
non-core fireworks by using independent selection operator and increase the
exploration capacity by crowdness-avoiding cooperative strategy among the
fireworks. Experimental results on the CEC2013 benchmark functions suggest that
CoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony,
differential evolution, the standard particle swarm optimization (SPSO) in 2007
and the most recent SPSO in 2011 in term of convergence performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00076</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00076</id><created>2015-04-30</created><authors><author><keyname>Mirahsan</keyname><forenames>Meisam</forenames></author><author><keyname>Schoenen</keyname><forenames>Rainer</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author></authors><title>HetHetNets: Heterogeneous Traffic Distribution in Heterogeneous Wireless
  Cellular Networks</title><categories>cs.NI</categories><comments>JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent approach in modeling and analysis of the supply and demand in
heterogeneous wireless cellular networks has been the use of two independent
Poisson point processes (PPPs) for the locations of base stations (BSs) and
user equipments (UEs). This popular approach has two major shortcomings. First,
although the PPP model may be a fitting one for the BS locations, it is less
adequate for the UE locations mainly due to the fact that the model is not
adjustable (tunable) to represent the severity of the heterogeneity
(non-uniformity) in the UE locations. Besides, the independence assumption
between the two PPPs does not capture the often-observed correlation between
the UE and BS locations.
  This paper presents a novel heterogeneous spatial traffic modeling which
allows statistical adjustment. Simple and non-parameterized, yet sufficiently
accurate, measures for capturing the traffic characteristics in space are
introduced. Only two statistical parameters related to the UE distribution,
namely, the coefficient of variation (the normalized second-moment), of an
appropriately defined inter-UE distance measure, and correlation coefficient
(the normalized cross-moment) between UE and BS locations, are adjusted to
control the degree of heterogeneity and the bias towards the BS locations,
respectively. This model is used in heterogeneous wireless cellular networks
(HetNets) to demonstrate the impact of heterogeneous and BS-correlated traffic
on the network performance. This network is called HetHetNet since it has two
types of heterogeneity: heterogeneity in the infrastructure (supply), and
heterogeneity in the spatial traffic distribution (demand).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00077</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00077</id><created>2015-04-30</created><updated>2015-05-25</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author></authors><title>Fast and Accurate Bilateral Filtering using Gauss-Polynomial
  Decomposition</title><categories>cs.CV</categories><comments>To appear in the IEEE International Conference on Image Processing
  (ICIP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bilateral filter is a versatile non-linear filter that has found diverse
applications in image processing, computer vision, computer graphics, and
computational photography. A widely-used form of the filter is the Gaussian
bilateral filter in which both the spatial and range kernels are Gaussian. A
direct implementation of this filter requires $O(\sigma^2)$ operations per
pixel, where $\sigma$ is the standard deviation of the spatial Gaussian. In
this paper, we propose an accurate approximation algorithm that can cut down
the computational complexity to $O(1)$ per pixel for any arbitrary $\sigma$
(constant-time implementation). This is based on the observation that the range
kernel operates via the translations of a fixed Gaussian over the range space,
and that these translated Gaussians can be accurately approximated using the
so-called Gauss-polynomials. The overall algorithm emerging from this
approximation involves a series of spatial Gaussian filtering, which can be
implemented in constant-time using separability and recursion. We present some
preliminary results to demonstrate that the proposed algorithm compares
favorably with some of the existing fast algorithms in terms of speed and
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00078</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00078</id><created>2015-04-30</created><authors><author><keyname>Chatzivasileiadis</keyname><forenames>Spyros</forenames></author><author><keyname>Bonvini</keyname><forenames>Marco</forenames></author><author><keyname>Matanza</keyname><forenames>Javier</forenames></author><author><keyname>Yin</keyname><forenames>Rongxin</forenames></author><author><keyname>Liu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Nouidui</keyname><forenames>Thierry</forenames></author><author><keyname>Kara</keyname><forenames>Emre C.</forenames></author><author><keyname>Parmar</keyname><forenames>Rajiv</forenames></author><author><keyname>Lorenzetti</keyname><forenames>David</forenames></author><author><keyname>Wetter</keyname><forenames>Michael</forenames></author><author><keyname>Kiliccote</keyname><forenames>Sila</forenames></author></authors><title>Cyber physical modeling of distributed resources for distribution system
  operations</title><categories>cs.CE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Co-simulation platforms are necessary to study the interactions of complex
systems integrated in future smart grids. The Virtual Grid Integration
Laboratory (VirGIL) is a modular co-simulation platform designed to study
interactions between demand response strategies, building comfort,
communication networks, and power system operation. This paper presents the
coupling of power systems, buildings, communications and control under a master
algorithm. There are two objectives. First, to use a modular architecture for
VirGIL, based on the Functional Mock-up Interface (FMI), where several
different modules can be added, exchanged, and tested. Second, to use a
commercial power system simulation platform, familiar to power system
operators, such as DIgSILENT Powerfactory. This will help reduce the barriers
to the industry for adopting such platforms, investigate and subsequently
deploy demand response strategies in their daily operation. VirGIL further
introduces the integration of the Quantized State System (QSS) methods for
simulation in this co-simulation platform. Results on how these systems
interact using a real network and consumption data are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00079</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00079</id><created>2015-04-30</created><updated>2015-05-09</updated><authors><author><keyname>Zhou</keyname><forenames>Zhuojie</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Leveraging History for Faster Sampling of Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Technical report for the VLDB 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to enable efficient analytics over such data has been an increasingly
important research problem. Given the sheer size of such social networks, many
existing studies resort to sampling techniques that draw random nodes from an
online social network through its restrictive web/API interface. Almost all of
them use the exact same underlying technique of random walk - a Markov Chain
Monte Carlo based method which iteratively transits from one node to its random
neighbor.
  Random walk fits naturally with this problem because, for most online social
networks, the only query we can issue through the interface is to retrieve the
neighbors of a given node (i.e., no access to the full graph topology). A
problem with random walks, however, is the &quot;burn-in&quot; period which requires a
large number of transitions/queries before the sampling distribution converges
to a stationary value that enables the drawing of samples in a statistically
valid manner.
  In this paper, we consider a novel problem of speeding up the fundamental
design of random walks (i.e., reducing the number of queries it requires)
without changing the stationary distribution it achieves - thereby enabling a
more efficient &quot;drop-in&quot; replacement for existing sampling-based analytics
techniques over online social networks. Our main idea is to leverage the
history of random walks to construct a higher-ordered Markov chain. We develop
two algorithms, Circulated Neighbors and Groupby Neighbors Random Walk (CNRW
and GNRW) and prove that, no matter what the social network topology is, CNRW
and GNRW offer better efficiency than baseline random walks while achieving the
same stationary distribution. We demonstrate through extensive experiments on
real-world social networks and synthetic graphs the superiority of our
techniques over the existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00080</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00080</id><created>2015-04-30</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Full-Duplex MIMO Relaying Powered by Wireless Energy Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted for IEEE International Workshop on Signal Processing
  Advances in Wireless Communications (SPAWC 2015), Invited paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a full-duplex decode-and-forward system, where the wirelessly
powered relay employs the time-switching protocol to receive power from the
source and then transmit information to the destination. It is assumed that the
relay node is equipped with two sets of antennas to enable full-duplex
communications. Three different interference mitigation schemes are studied,
namely, 1) optimal 2) zero-forcing and 3) maximum ratio combining/maximum ratio
transmission. We develop new outage probability expressions to investigate
delay-constrained transmission throughput of these schemes. Our analysis show
interesting performance comparisons of the considered precoding schemes for
different system and link parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00081</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00081</id><created>2015-04-30</created><authors><author><keyname>Huang</keyname><forenames>Lingxiao</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Shi</keyname><forenames>Qicai</forenames></author></authors><title>Approximation Algorithms for the Connected Sensor Cover Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimum connected sensor cover problem (\mincsc) and the
budgeted connected sensor cover (\bcsc) problem, both motivated by important
applications in wireless sensor networks. In both problems, we are given a set
of sensors and a set of target points in the Euclidean plane. In \mincsc, our
goal is to find a set of sensors of minimum cardinality, such that all target
points are covered, and all sensors can communicate with each other (i.e., the
communication graph is connected). We obtain a constant factor approximation
algorithm, assuming that the ratio between the sensor radius and communication
radius is bounded. In \bcsc\ problem, our goal is to choose a set of $B$
sensors, such that the number of targets covered by the chosen sensors is
maximized and the communication graph is connected. We also obtain a constant
approximation under the same assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00082</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00082</id><created>2015-04-30</created><updated>2016-01-29</updated><authors><author><keyname>Wang</keyname><forenames>Taotao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Frequency-Asynchronous Multiuser Joint Channel-Parameter Estimation, CFO
  Compensation and Channel Decoding</title><categories>cs.IT math.IT</categories><comments>This work is accepted for publication by IEEE TVT at Jan. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a channel-coded multiuser system operated with
orthogonal frequency-division multiplexing (OFDM) and interleaved division
multiple-access (IDMA). To realize the potential advantage of OFDM-IDMA, two
challenges must be addressed. The first challenge is the estimation of multiple
channel parameters. An issue is how to contain the estimation errors of the
channel parameters of the multiple users, considering that the overall
estimation errors may increase with the number of users because the estimations
of their channel parameters are intertwined with each other. The second
challenge is that the transmitters of the multiple users may be driven by
different RF oscillators. The associated frequency asynchrony may cause
multiple CFOs at the receiver. Compared with a single-user receiver where the
single CFO can be compensated away, a particular difficulty for a multiuser
receiver is that it is not possible to compensate for all the multiple CFOs
simultaneously. To tackle the two challenges, we put forth a framework to solve
the problems of multiuser channel-parameter estimation, CFO compensation, and
channel decoding jointly and iteratively. The framework employs the space
alternating generalized expectation-maximization (SAGE) algortihm to decompose
the multisuser problem into multiple single-user problems, and the
expectation-conditional maximization (ECM) algorithm to tackle each of the
single-user subproblems. Iterative executions of SAGE and ECM in the framework
allow the two aforementioned challenges to be tackled in an optimal manner.
Simulations and real experiments based on software-defined radio indicate that,
compared with other approaches, our approach can achieve significant
performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00090</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00090</id><created>2015-05-01</created><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Liew</keyname><forenames>Vincent</forenames></author><author><keyname>P&#x1ce;tra&#x15f;cu</keyname><forenames>Mihai</forenames></author></authors><title>Finding the Median (Obliviously) with Bounded Space</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any oblivious algorithm using space $S$ to find the median of a
list of $n$ integers from $\{1,...,2n\}$ requires time $\Omega(n \log\log_S
n)$. This bound also applies to the problem of determining whether the median
is odd or even. It is nearly optimal since Chan, following Munro and Raman, has
shown that there is a (randomized) selection algorithm using only $s$
registers, each of which can store an input value or $O(\log n)$-bit counter,
that makes only $O(\log\log_s n)$ passes over the input. The bound also implies
a size lower bound for read-once branching programs computing the low order bit
of the median and implies the analog of $P \ne NP \cap coNP$ for length $o(n
\log\log n)$ oblivious branching programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00092</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00092</id><created>2015-05-01</created><authors><author><keyname>Pelleg</keyname><forenames>Dan</forenames></author><author><keyname>Yom-Tov</keyname><forenames>Elad</forenames></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author></authors><title>On the Effect of Human-Computer Interfaces on Language Expression</title><categories>cs.HC cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language expression is known to be dependent on attributes intrinsic to the
author. To date, however, little attention has been devoted to the effect of
interfaces used to articulate language on its expression. Here we study a large
corpus of text written using different input devices and show that writers
unconsciously prefer different letters depending on the interplay between their
individual traits (e.g., hand laterality and injuries) and the layout of
keyboards. Our results show, for the first time, how the interplay between
technology and its users modifies language expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00107</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00107</id><created>2015-05-01</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Eshan</forenames></author><author><keyname>Goyal</keyname><forenames>Vipul</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Non-Malleable Extractors and Codes, with their Many Tampered Extensions</title><categories>cs.CR cs.CC</categories><comments>50 pages; see paper for full abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomness extractors and error correcting codes are fundamental objects in
computer science. Recently, there have been several natural generalizations of
these objects, in the context and study of tamper resilient cryptography. These
are seeded non-malleable extractors, introduced in [DW09]; seedless
non-malleable extractors, introduced in [CG14b]; and non-malleable codes,
introduced in [DPW10].
  However, explicit constructions of non-malleable extractors appear to be
hard, and the known constructions are far behind their non-tampered
counterparts.
  In this paper we make progress towards solving the above problems. Our
contributions are as follows.
  (1) We construct an explicit seeded non-malleable extractor for min-entropy
$k \geq \log^2 n$. This dramatically improves all previous results and gives a
simpler 2-round privacy amplification protocol with optimal entropy loss,
matching the best known result in [Li15b].
  (2) We construct the first explicit non-malleable two-source extractor for
min-entropy $k \geq n-n^{\Omega(1)}$, with output size $n^{\Omega(1)}$ and
error $2^{-n^{\Omega(1)}}$.
  (3) We initiate the study of two natural generalizations of seedless
non-malleable extractors and non-malleable codes, where the sources or the
codeword may be tampered many times. We construct the first explicit
non-malleable two-source extractor with tampering degree $t$ up to
$n^{\Omega(1)}$, which works for min-entropy $k \geq n-n^{\Omega(1)}$, with
output size $n^{\Omega(1)}$ and error $2^{-n^{\Omega(1)}}$. We show that we can
efficiently sample uniformly from any pre-image. By the connection in [CG14b],
we also obtain the first explicit non-malleable codes with tampering degree $t$
up to $n^{\Omega(1)}$, relative rate $n^{\Omega(1)}/n$, and error
$2^{-n^{\Omega(1)}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00110</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00110</id><created>2015-05-01</created><authors><author><keyname>Cai</keyname><forenames>Hongping</forenames></author><author><keyname>Wu</keyname><forenames>Qi</forenames></author><author><keyname>Corradi</keyname><forenames>Tadeo</forenames></author><author><keyname>Hall</keyname><forenames>Peter</forenames></author></authors><title>The Cross-Depiction Problem: Computer Vision Algorithms for Recognising
  Objects in Artwork and in Photographs</title><categories>cs.CV</categories><comments>12 pages, 6 figures</comments><msc-class>68745</msc-class><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cross-depiction problem is that of recognising visual objects regardless
of whether they are photographed, painted, drawn, etc. It is a potentially
significant yet under-researched problem. Emulating the remarkable human
ability to recognise objects in an astonishingly wide variety of depictive
forms is likely to advance both the foundations and the applications of
Computer Vision.
  In this paper we benchmark classification, domain adaptation, and deep
learning methods; demonstrating that none perform consistently well in the
cross-depiction problem. Given the current interest in deep learning, the fact
such methods exhibit the same behaviour as all but one other method: they show
a significant fall in performance over inhomogeneous databases compared to
their peak performance, which is always over data comprising photographs only.
Rather, we find the methods that have strong models of spatial relations
between parts tend to be more robust and therefore conclude that such
information is important in modelling object classes regardless of appearance
details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00111</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00111</id><created>2015-05-01</created><authors><author><keyname>Guo</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Daqing</forenames></author><author><keyname>Yu</keyname><forenames>Zhiwen</forenames></author><author><keyname>Chin</keyname><forenames>Alvin</forenames></author></authors><title>Mobile Crowd Sensing and Computing: When Participatory Sensing Meets
  Participatory Social Media</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of mobile sensing and mobile social networking
techniques, Mobile Crowd Sensing and Computing (MCSC), which leverages
heterogeneous crowdsourced data for large-scale sensing, has become a leading
paradigm. Built on top of the participatory sensing vision, MCSC has two
characterizing features: (1) it leverages heterogeneous crowdsourced data from
two data sources: participatory sensing and participatory social media; and (2)
it presents the fusion of human and machine intelligence (HMI) in both the
sensing and computing process. This paper characterizes the unique features and
challenges of MCSC. We further present early efforts on MCSC to demonstrate the
benefits of aggregating heterogeneous crowdsourced data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00113</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00113</id><created>2015-05-01</created><updated>2015-09-18</updated><authors><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author></authors><title>The quantum complexity of approximating the frequency moments</title><categories>quant-ph cs.DS</categories><comments>21 pages; v2: some typos and infelicities corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$'th frequency moment of a sequence of integers is defined as $F_k =
\sum_j n_j^k$, where $n_j$ is the number of times that $j$ occurs in the
sequence. Here we study the quantum complexity of approximately computing the
frequency moments in two settings. In the query complexity setting, we wish to
minimise the number of queries to the input used to approximate $F_k$ up to
relative error $\epsilon$. We give quantum algorithms which outperform the best
possible classical algorithms up to quadratically. In the multiple-pass
streaming setting, we see the elements of the input one at a time, and seek to
minimise the amount of storage space, or passes over the data, used to
approximate $F_k$. We describe quantum algorithms for $F_0$, $F_2$ and
$F_\infty$ in this model which outperform the best possible classical
algorithms almost quadratically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00114</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00114</id><created>2015-05-01</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Device-Relaying in Cellular D2D Networks: A Fairness Perspective</title><categories>cs.IT math.IT</categories><comments>CROWNCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-Device (D2D) communication is envisioned to be an integral
component of 5G networks, and a technique for meeting the demand for high data
rates. In a cellular network, D2D allows not only direct communication between
users, but also device relaying. In this paper, a simple instance of
device-relaying is investigated, and the impact of D2D on fairness among users
is studied. Namely, a cellular network consisting of two D2D-enabled users and
a base-station (BS) is considered. Thus, the users who want to establish
communication with the BS can act as relays for each other's signals. While
this problem is traditionally considered in the literature as a multiple-access
channel with cooperation in the uplink, and a broadcast channel with
cooperation in the downlink, we propose a different treatment of the problem as
a multi-way channel. A simple communication scheme is proposed, and is shown to
achieve significant gain in terms of fairness (measured by the symmetric rate
supported) in comparison to the aforementioned traditional treatment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00115</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00115</id><created>2015-05-01</created><authors><author><keyname>Baccini</keyname><forenames>Alberto</forenames></author><author><keyname>De Nicolao</keyname><forenames>Giuseppe</forenames></author></authors><title>Do they agree? Bibliometric evaluation vs informed peer review in the
  Italian research assessment exercise</title><categories>cs.DL physics.soc-ph stat.AP</categories><msc-class>62K99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the Italian research assessment exercise, the national agency ANVUR
performed an experiment to assess agreement between grades obtained through
informed peer review (IR) and bibliometrics. A sample was evaluated by using
both methods and concordance was analyzed by weighted Cohen's kappas. According
to ANVUR results indicated an overall &quot;more than adequate&quot; agreement which
&quot;fully justifies&quot; the choice of using jointly both techniques in the
assessment. However, according to available statistical guidelines for kappa
values, the degree of agreement has to be interpreted, for all research fields,
as poor or, in a few cases, as, at most, fair. The only notable exception is
Area 13 (economics and statistics) and its sub-areas, showing moderate
agreement. However, a statistical meta-analysis rejects the hypothesis that
kappas from Area 13 share the same distribution as those from the other areas.
In fact, a scrutiny of the experiment protocol adopted by the Area 13 panel
highlights substantial modifications with respect to protocols of all the other
areas, to the point that results for Area 13 have to be considered as fatally
flawed. The evidence of a poor to fair concordance supports the conclusion that
IR and bibliometrics do not produce similar results. As a consequence, final
results reached in the Italian research assessment possibly depend on the mix
of instruments used for evaluating research outputs. The conclusion reached by
ANVUR must be reversed: the available evidence does not justify at all the
joint use of both techniques within the same research assessment exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00118</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00118</id><created>2015-05-01</created><authors><author><keyname>Krajicek</keyname><forenames>Jan</forenames></author></authors><title>Expansions of pseudofinite structures and circuit and proof complexity</title><categories>math.LO cs.CC</categories><comments>Preliminary version May 2015</comments><msc-class>03F20, 03C99, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I shall describe a general model-theoretic task to construct expansions of
pseudofinite structures and discuss several examples of particular relevance to
computational complexity. Then I will present one specific situation where
finding a suitable expansion would imply that, assuming a one-way permutation
exists, the computational class NP is not closed under complementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00122</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00122</id><created>2015-05-01</created><updated>2015-09-30</updated><authors><author><keyname>Blythe</keyname><forenames>Richard A.</forenames></author></authors><title>Hierarchy of Scales in Language Dynamics</title><categories>physics.soc-ph cs.CL</categories><comments>Colloquium (short review paper) solicited by European Physical
  Journal B. 18 pages, 3 figures. accepted v2 contains more text, figures,
  references and coherence</comments><journal-ref>EPJB (2015) v88 295</journal-ref><doi>10.1140/epjb/e2015-60347-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods and insights from statistical physics are finding an increasing
variety of applications where one seeks to understand the emergent properties
of a complex interacting system. One such area concerns the dynamics of
language at a variety of levels of description, from the behaviour of
individual agents learning simple artificial languages from each other, up to
changes in the structure of languages shared by large groups of speakers over
historical timescales. In this Colloquium, we survey a hierarchy of scales at
which language and linguistic behaviour can be described, along with the main
progress in understanding that has been made at each of them---much of which
has come from the statistical physics community. We argue that future
developments may arise by linking the different levels of the hierarchy
together in a more coherent fashion, in particular where this allows more
effective use of rich empirical data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00130</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00130</id><created>2015-05-01</created><updated>2015-09-10</updated><authors><author><keyname>Abdi</keyname><forenames>Younes</forenames><affiliation>University of Jyvaskyla, Finland</affiliation></author><author><keyname>Ristaniemi</keyname><forenames>Tapani</forenames><affiliation>University of Jyvaskyla, Finland</affiliation></author></authors><title>Random Interruptions in Cooperation for Spectrum Sensing in Cognitive
  Radio Networks</title><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures, further discussions, analysis, and results have
  been included, main results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new cooperation structure for spectrum sensing in cognitive
radio networks is proposed which outperforms the existing commonly-used ones in
terms of energy efficiency. The efficiency is achieved in the proposed design
by introducing random interruptions in the cooperation process between the
sensing nodes and the fusion center, along with a compensation process at the
fusion center. Regarding the hypothesis testing problem concerned, first, the
proposed system behavior is thoroughly analyzed and its associated
likelihood-ratio test (LRT) is provided. Next, based on a general linear fusion
rule, statistics of the global test summary are derived and the sensing quality
is characterized in terms of the probability of false alarm and the probability
of detection. Then, optimization of the overall detection performance is
formulated according to the Neyman-Pearson criterion (NPC) and it is discussed
that the optimization required is indeed a decision-making process with
uncertainty which incurs prohibitive computational complexity. The NPC is then
modified to achieve a good affordable solution by using semidefinite
programming (SDP) techniques and it is shown that this new solution is nearly
optimal according to the deflection criterion. Finally, effectiveness of the
proposed architecture and its associated SDP are demonstrated by simulation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00136</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00136</id><created>2015-05-01</created><authors><author><keyname>Schiffer</keyname><forenames>Johannes</forenames></author><author><keyname>Zonetti</keyname><forenames>Daniele</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Stankovic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Sezi</keyname><forenames>Tevfik</forenames></author><author><keyname>Raisch</keyname><forenames>Joerg</forenames></author></authors><title>Modeling of microgrids - from fundamental physics to phasors and voltage
  sources</title><categories>cs.SY math.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microgrids are an increasingly popular class of electrical systems that
facilitate the integration of renewable distributed generation units. Their
analysis and controller design requires the development of advanced (typically
model-based) techniques naturally posing an interesting challenge to the
control community. Although there are widely accepted reduced order models to
describe the dynamic behavior of microgrids, they are typically presented
without details about the reduction procedure - hampering the understanding of
the physical phenomena behind them. The present paper aims to provide a
complete modular model derivation of a three-phase inverter-based microgrid.
Starting from fundamental physics, we present detailed dynamical models of the
main microgrid components and clearly state the underlying assumptions which
lead to the standard reduced model representation with inverters represented as
controllable voltage sources, as well as static network interconnections and
loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00138</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00138</id><created>2015-05-01</created><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author></authors><title>Compositional Distributional Semantics with Compact Closed Categories
  and Frobenius Algebras</title><categories>cs.CL cs.AI math.CT math.QA quant-ph</categories><comments>Ph.D. Dissertation, University of Oxford</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis contributes to ongoing research related to the categorical
compositional model for natural language of Coecke, Sadrzadeh and Clark in
three ways: Firstly, I propose a concrete instantiation of the abstract
framework based on Frobenius algebras (joint work with Sadrzadeh). The theory
improves shortcomings of previous proposals, extends the coverage of the
language, and is supported by experimental work that improves existing results.
The proposed framework describes a new class of compositional models that find
intuitive interpretations for a number of linguistic phenomena. Secondly, I
propose and evaluate in practice a new compositional methodology which
explicitly deals with the different levels of lexical ambiguity (joint work
with Pulman). A concrete algorithm is presented, based on the separation of
vector disambiguation from composition in an explicit prior step. Extensive
experimental work shows that the proposed methodology indeed results in more
accurate composite representations for the framework of Coecke et al. in
particular and every other class of compositional models in general. As a last
contribution, I formalize the explicit treatment of lexical ambiguity in the
context of the categorical framework by resorting to categorical quantum
mechanics (joint work with Coecke). In the proposed extension, the concept of a
distributional vector is replaced with that of a density matrix, which
compactly represents a probability distribution over the potential different
meanings of the specific word. Composition takes the form of quantum
measurements, leading to interesting analogies between quantum physics and
linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00144</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00144</id><created>2015-05-01</created><updated>2015-10-02</updated><authors><author><keyname>Chevalier</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Reachability of Consensus and Synchronizing Automata</title><categories>cs.DC cs.DM cs.SY</categories><comments>Update after review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of determining the existence of a sequence of
matrices driving a discrete-time consensus system to consensus. We transform
this problem into one of the existence of a product of the transition
(stochastic) matrices that has a positive column. We then generalize some
results from automata theory to sets of stochastic matrices. We obtain as a
main result a polynomial-time algorithm to decide the existence of a sequence
of matrices achieving consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00145</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00145</id><created>2015-05-01</created><authors><author><keyname>Cabezas</keyname><forenames>Ferran</forenames></author><author><keyname>Carlier</keyname><forenames>Axel</forenames></author><author><keyname>Salvador</keyname><forenames>Amaia</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author><author><keyname>Charvillat</keyname><forenames>Vincent</forenames></author></authors><title>Quality Control in Crowdsourced Object Segmentation</title><categories>cs.CV cs.HC</categories><comments>Paper accepted at the IEEE International Conference on Image
  Processing (ICIP) 2015. Quebec City, 27-30 September 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores processing techniques to deal with noisy data in
crowdsourced object segmentation tasks. We use the data collected with
&quot;Click'n'Cut&quot;, an online interactive segmentation tool, and we perform several
experiments towards improving the segmentation results. First, we introduce
different superpixel-based techniques to filter users' traces, and assess their
impact on the segmentation result. Second, we present different criteria to
detect and discard the traces from potential bad users, resulting in a
remarkable increase in performance. Finally, we show a novel superpixel-based
segmentation algorithm which does not require any prior filtering and is based
on weighting each user's contribution according to his/her level of expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00146</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00146</id><created>2015-05-01</created><authors><author><keyname>Xia</keyname><forenames>Yingce</forenames></author><author><keyname>Li</keyname><forenames>Haifang</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Yu</keyname><forenames>Nenghai</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Thompson Sampling for Budgeted Multi-armed Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson sampling is one of the earliest randomized algorithms for
multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to
Budgeted MAB, where there is random cost for pulling an arm and the total cost
is constrained by a budget. We start with the case of Bernoulli bandits, in
which the random rewards (costs) of an arm are independently sampled from a
Bernoulli distribution. To implement the Thompson sampling algorithm in this
case, at each round, we sample two numbers from the posterior distributions of
the reward and cost for each arm, obtain their ratio, select the arm with the
maximum ratio, and then update the posterior distributions. We prove that the
distribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$
denotes the budget. By introducing a Bernoulli trial, we further extend this
algorithm to the setting that the rewards (costs) are drawn from general
distributions, and prove that its regret bound remains almost the same. Our
simulation results demonstrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00147</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00147</id><created>2015-05-01</created><authors><author><keyname>Brodal</keyname><forenames>Gerth St&#xf8;lting</forenames></author><author><keyname>Nielsen</keyname><forenames>Jesper Sindahl</forenames></author><author><keyname>Truelsen</keyname><forenames>Jakob</forenames></author></authors><title>Strictly Implicit Priority Queues: On the Number of Moves and Worst-Case
  Time</title><categories>cs.DS</categories><comments>To appear at WADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary heap of Williams (1964) is a simple priority queue characterized
by only storing an array containing the elements and the number of elements $n$
- here denoted a strictly implicit priority queue. We introduce two new
strictly implicit priority queues. The first structure supports amortized
$O(1)$ time Insert and $O(\log n)$ time ExtractMin operations, where both
operations require amortized $O(1)$ element moves. No previous implicit heap
with $O(1)$ time Insert supports both operations with $O(1)$ moves. The second
structure supports worst-case $O(1)$ time Insert and $O(\log n)$ time (and
moves) ExtractMin operations. Previous results were either amortized or needed
$O(\log n)$ bits of additional state information between operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00149</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00149</id><created>2015-05-01</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author><author><keyname>Sammut</keyname><forenames>Paul</forenames></author><author><keyname>Willans</keyname><forenames>James</forenames></author></authors><title>Applied Metamodelling: A Foundation for Language Driven Development
  (Third Edition)</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern day system developers have some serious problems to contend with. The
systems they develop are becoming increasingly complex as customers demand
richer functionality delivered in ever shorter timescales. They have to manage
a huge diversity of implementation technologies, design techniques and
development processes: everything from scripting languages to web-services to
the latest 'silver bullet' design abstraction. To add to that, nothing stays
still: today's 'must have' technology rapidly becomes tomorrow's legacy problem
that must be managed along with everything else. How can these problems be
dealt with? In this book we propose that there is a common foundation to their
resolution: languages. Languages are the primary way in which system developers
communicate, design and implement systems. Languages provide abstractions that
can encapsulate complexity, embrace the diversity of technologies and design
abstractions, and unite modern and legacy systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00153</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00153</id><created>2015-05-01</created><authors><author><keyname>Alavi</keyname><forenames>S. M. M.</forenames></author><author><keyname>Mahdi</keyname><forenames>A.</forenames></author><author><keyname>Payne</keyname><forenames>S. J.</forenames></author><author><keyname>Howey</keyname><forenames>D. A.</forenames></author></authors><title>Structural identifiability of battery equivalent circuit models</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies structural identifiability of battery equivalent circuit
models. It is shown that the general Randles model structures, both in the
continuous and discrete time domains, are locally identifiable. With respect to
the battery electrochemical impedance spectroscopy, some conditions are added
to the model structures, making them globally identifiable. Finally, numerical
simulations are provided to demonstrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00157</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00157</id><created>2015-05-01</created><authors><author><keyname>Huang</keyname><forenames>Yang</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Wireless Information and Power Transfer for an Autonomous Multiple
  Antenna Relay System</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering a three-node multiple antenna relay system, this paper proposes a
two-phase amplify-and-forward (AF) relaying protocol, which enables the
autonomous relay to simultaneously harvest wireless power from the source
information signal and from an energy signal conveyed by the destination. We
first study this energy-flow-assisted (EFA) relaying in a single-input
single-output (SISO) relay system and aim at maximizing the rate. By
transforming the optimization problem into an equivalent convex form, a global
optimum can be found. We then extend the protocol to a multiple antenna relay
system. The relay processing matrix is optimized to maximize the rate. The
optimization problem can be efficiently solved by eigenvalue decomposition,
after linear algebra manipulation. It is observed that the benefits of the
energy flow are interestingly shown only in the multiple antenna case, and it
is revealed that the received information signal and the energy leakage at the
relay can be nearly separated by making use of the signal space, such that the
desired signal can be amplified with a larger coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00161</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00161</id><created>2015-05-01</created><authors><author><keyname>Bollegala</keyname><forenames>Danushka</forenames></author><author><keyname>Maehara</keyname><forenames>Takanori</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Embedding Semantic Relations into Word Representations</title><categories>cs.CL</categories><comments>International Joint Conferences in AI (IJCAI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning representations for semantic relations is important for various
tasks such as analogy detection, relational search, and relation
classification. Although there have been several proposals for learning
representations for individual words, learning word representations that
explicitly capture the semantic relations between words remains under
developed. We propose an unsupervised method for learning vector
representations for words such that the learnt representations are sensitive to
the semantic relations that exist between two words. First, we extract lexical
patterns from the co-occurrence contexts of two words in a corpus to represent
the semantic relations that exist between those two words. Second, we represent
a lexical pattern as the weighted sum of the representations of the words that
co-occur with that lexical pattern. Third, we train a binary classifier to
detect relationally similar vs. non-similar lexical pattern pairs. The proposed
method is unsupervised in the sense that the lexical pattern pairs we use as
train data are automatically sampled from a corpus, without requiring any
manual intervention. Our proposed method statistically significantly
outperforms the current state-of-the-art word representations on three
benchmark datasets for proportional analogy detection, demonstrating its
ability to accurately capture the semantic relations among words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00162</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00162</id><created>2015-05-01</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A Modification of the Halpern-Pearl Definition of Causality</title><categories>cs.AI</categories><comments>This is an extended version of a paper that will appear in IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original Halpern-Pearl definition of causality [Halpern and Pearl, 2001]
was updated in the journal version of the paper [Halpern and Pearl, 2005] to
deal with some problems pointed out by Hopkins and Pearl [2003]. Here the
definition is modified yet again, in a way that (a) leads to a simpler
definition, (b) handles the problems pointed out by Hopkins and Pearl, and many
others, (c) gives reasonable answers (that agree with those of the original and
updated definition) in the standard problematic examples of causality, and (d)
has lower complexity than either the original or updated definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00164</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00164</id><created>2015-05-01</created><updated>2015-12-30</updated><authors><author><keyname>Haunert</keyname><forenames>Jan-Henrik</forenames></author><author><keyname>Niedermann</keyname><forenames>Benjamin</forenames></author></authors><title>An Algorithmic Framework for Labeling Network Maps</title><categories>cs.CG cs.DS</categories><comments>Full version of COCOON 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing network maps automatically comprises two challenging steps, namely
laying out the map and placing non-overlapping labels. In this paper we tackle
the problem of labeling an already existing network map considering the
application of metro maps. We present a flexible and versatile labeling model.
Despite its simplicity, we prove that it is NP-complete to label a single line
of the network. For a restricted variant of that model, we then introduce an
efficient algorithm that optimally labels a single line with respect to a given
weighting function. Based on that algorithm, we present a general and
sophisticated workflow for multiple metro lines, which is experimentally
evaluated on real-world metro maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00168</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00168</id><created>2015-05-01</created><authors><author><keyname>Goyal</keyname><forenames>Manan Mohan</forenames></author><author><keyname>Agrawal</keyname><forenames>Neha</forenames></author><author><keyname>Sarma</keyname><forenames>Manoj Kumar</forenames></author><author><keyname>Kalita</keyname><forenames>Nayan Jyoti</forenames></author></authors><title>Comparison Clustering using Cosine and Fuzzy set based Similarity
  Measures of Text Documents</title><categories>cs.IR</categories><comments>4 pages, International Conference on Computing and Communication
  Systems 2015 (I3CS'15), ISBM: 978-1-4799-5857-01, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keeping in consideration the high demand for clustering, this paper focuses
on understanding and implementing K-means clustering using two different
similarity measures. We have tried to cluster the documents using two different
measures rather than clustering it with Euclidean distance. Also a comparison
is drawn based on accuracy of clustering between fuzzy and cosine similarity
measure. The start time and end time parameters for formation of clusters are
used in deciding optimum similarity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00171</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00171</id><created>2015-05-01</created><authors><author><keyname>Handa</keyname><forenames>Ankur</forenames></author><author><keyname>Patraucean</keyname><forenames>Viorica</forenames></author><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Stent</keyname><forenames>Simon</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We are interested in automatic scene understanding from geometric cues. To
this end, we aim to bring semantic segmentation in the loop of real-time
reconstruction. Our semantic segmentation is built on a deep autoencoder stack
trained exclusively on synthetic depth data generated from our novel 3D scene
library, SynthCam3D. Importantly, our network is able to segment real world
scenes without any noise modelling. We present encouraging preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00172</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00172</id><created>2015-05-01</created><authors><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Technical Analysis of Security Infrastructure in RFID Technology</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is a technical analysis of the security infrastructure in the field
of RFID technology. The paper briefly discusses the architecture of the RFID
technology. Then it analyses the various features and advantages RFID
technology has over the existing technologies like bar codes. This is followed
by a discussion of the various disadvantages and security drawbacks of RFID
technology that prevents its widespread adoption in the mainstream market. The
paper concludes with a brief analysis of some of the security measures that are
implemented within the RFID technology for securing up the whole
infrastructure. The main aim of the paper is to focus on the drawbacks of the
pre-existing security measures in RFID technology as well as to discuss the
direction in which further research has to be carried out without the
compromise on its unique features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00178</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00178</id><created>2015-05-01</created><authors><author><keyname>Duursma</keyname><forenames>Iwan M.</forenames></author></authors><title>Shortened regenerating codes</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For general exact repair regenerating codes, the optimal trade-offs between
storage size and repair bandwith remain undetermined. Various outer bounds and
partial results have been proposed. Using a simple chain rule argument we
identify nonnegative differences between the functional repair and the exact
repair outer bounds. One of the differences is then bounded from below by the
repair data of a shortened subcode. Our main result is a new outer bound for an
exact repair regenerating code in terms of its shortened subcodes. In general
the new outer bound is implicit and depends on the choice of shortened
subcodes. For the linear case we obtain explicit bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00181</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00181</id><created>2015-05-01</created><authors><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Molecular Communications with Longitudinal Carrier Waves: Baseband to
  Passband Modulation</title><categories>cs.ET</categories><comments>submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional molecular communications via diffusion (MCvD) systems have used
baseband modulation techniques by varying properties of molecular pulses such
as the amplitude, the frequency of the transversal wave of the pulse, and the
time delay between subsequent pulses. In this letter, we propose and implement
passband modulation with molecules that exhibit longitudinal carrier wave
properties. This is achieved through the oscillation of the transmitter.
Frequency division multiplexing is employed to allow different molecular
information streams to co-exist in the same space and time channel, creating an
effective bandwidth for MCvD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00184</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00184</id><created>2015-05-01</created><authors><author><keyname>Afshani</keyname><forenames>Peyman</forenames></author><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author><author><keyname>Chan</keyname><forenames>Timothy</forenames></author></authors><title>Instance Optimal Geometric Algorithms</title><categories>cs.CG cs.DS</categories><comments>28 pages in fullpage</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of an algorithm $A$ for computing 2-d or 3-d convex
hulls that is optimal for every point set in the following sense: for every
sequence $\sigma$ of $n$ points and for every algorithm $A'$ in a certain class
$\mathcal{A}$, the running time of $A$ on input $\sigma$ is at most a constant
factor times the maximum running time of $A'$ on the worst possible permutation
of $\sigma$ for $A'$. We establish a stronger property: for every sequence
$\sigma$ of points and every algorithm $A'$, the running time of $A$ on
$\sigma$ is at most a constant factor times the average running time of $A'$
over all permutations of $\sigma$. We call algorithms satisfying these
properties instance-optimal in the order-oblivious and random-order setting.
Such instance-optimal algorithms simultaneously subsume output-sensitive
algorithms and distribution-dependent average-case algorithms, and all
algorithms that do not take advantage of the order of the input or that assume
the input is given in a random order. The class $\mathcal{A}$ under
consideration consists of all algorithms in a decision tree model where the
tests involve only multilinear functions with a constant number of arguments.
To establish an instance-specific lower bound, we deviate from traditional
Ben-Or-style proofs and adopt a new adversary argument. For 2-d convex hulls,
we prove that a version of the well known algorithm by Kirkpatrick and Seidel
(1986) or Chan, Snoeyink, and Yap (1995) already attains this lower bound. For
3-d convex hulls, we propose a new algorithm. We further obtain
instance-optimal results for a few other standard problems in computational
geometry. Our framework also reveals connection to distribution-sensitive data
structures and yields new results as a byproduct, for example, on on-line
orthogonal range searching in 2-d and on-line halfspace range reporting in 2-d
and 3-d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00185</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00185</id><created>2015-04-30</created><authors><author><keyname>Solomon</keyname><forenames>Justin</forenames></author></authors><title>PDE Approaches to Graph Analysis</title><categories>cs.DM cs.SI math.CO</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys and discusses recent work adapting partial differential
equation (PDE) models to discrete structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00192</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00192</id><created>2015-04-30</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Mandal</keyname><forenames>Soham</forenames></author><author><keyname>Pratiher</keyname><forenames>Sawon</forenames></author><author><keyname>Barman</keyname><forenames>Ritwik</forenames></author><author><keyname>Venkatesh</keyname><forenames>M.</forenames></author><author><keyname>Ghosh</keyname><forenames>Nirmalya</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Application of S-Transform on Hyper kurtosis based Modified Duo
  Histogram Equalized DIC images for Pre-cancer Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our proposed hyper kurtosis based histogram equalized DIC images enhances the
contrast by preserving the brightness. The evolution and development of
precancerous activity among tissues are studied through S-transform (ST). The
significant variations of amplitude spectra can be observed due to increased
medium roughness from normal tissue were observed in time-frequency domain. The
randomness and inhomogeneity of the tissue structures among human normal and
different grades of DIC tissues is recognized by ST based timefrequency
analysis. This study offers a simpler and better way to recognize the
substantial changes among different stages of DIC tissues, which are reflected
by spatial information containing within the inhomogeneity structures of
different types of tissue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00193</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00193</id><created>2015-05-01</created><authors><author><keyname>Benninghoff</keyname><forenames>Heike</forenames></author><author><keyname>Garcke</keyname><forenames>Harald</forenames></author></authors><title>Segmentation and Restoration of Images on Surfaces by Parametric Active
  Contours with Topology Changes</title><categories>cs.CV math.AP math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a new method for segmentation and restoration of images on
two-dimensional surfaces is given. Active contour models for image segmentation
are extended to images on surfaces. The evolving curves on the surfaces are
mathematically described using a parametric approach. For image restoration, a
diffusion equation with Neumann boundary conditions is solved in a
postprocessing step in the individual regions. Numerical schemes are presented
which allow to efficiently compute segmentations and denoised versions of
images on surfaces. Also topology changes of the evolving curves are detected
and performed using a fast sub-routine. Finally, several experiments are
presented where the developed methods are applied on different artificial and
real images defined on different surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00199</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00199</id><created>2015-05-01</created><updated>2015-08-17</updated><authors><author><keyname>Parambath</keyname><forenames>Shameem A Puthiya</forenames></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames></author><author><keyname>Grandvalet</keyname><forenames>Yves</forenames></author></authors><title>Theory of Optimizing Pseudolinear Performance Measures: Application to
  F-measure</title><categories>cs.LG</categories><comments>Extended Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-linear performance measures are widely used for the evaluation of
learning algorithms. For example, $F$-measure is a commonly used performance
measure for classification problems in machine learning and information
retrieval community. We study the theoretical properties of a subset of
non-linear performance measures called pseudo-linear performance measures which
includes $F$-measure, \emph{Jaccard Index}, among many others. We establish
that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear
functions of the per-class false negatives and false positives for binary,
multiclass and multilabel classification. Based on this observation, we present
a general reduction of such performance measure optimization problem to
cost-sensitive classification problem with unknown costs. We then propose an
algorithm with provable guarantees to obtain an approximately optimal
classifier for the $F$-measure by solving a series of cost-sensitive
classification problems. The strength of our analysis is to be valid on any
dataset and any class of classifiers, extending the existing theoretical
results on pseudo-linear measures, which are asymptotic in nature. We also
establish the multi-objective nature of the $F$-score maximization problem by
linking the algorithm with the weighted-sum approach used in multi-objective
optimization. We present numerical experiments to illustrate the relative
importance of cost asymmetry and thresholding when learning linear classifiers
on various $F$-measure optimization tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00200</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00200</id><created>2015-05-01</created><updated>2015-07-13</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>An Effective Algorithmic Framework for Near Optimal Multi-Robot Path
  Planning</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a centralized algorithmic framework for solving multi-robot path
planning problems in general, two-dimensional, continuous environments while
minimizing globally the task completion time. The framework obtains high levels
of effectiveness through the composition of an optimal discretization of the
continuous environment and the subsequent fast, near-optimal resolution of the
resulting discrete planning problem. This principled approach achieves orders
of magnitudes better performance with respect to both speed and the supported
robot density. For a wide variety of environments, our method is shown to
compute globally near-optimal solutions for fifty robots in seconds with robots
packed close to each other. In the extreme, the method can consistently solve
problems with hundreds of robots that occupy over 30% of the free space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00212</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00212</id><created>2015-05-01</created><authors><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Nenov</keyname><forenames>Yavor</forenames></author><author><keyname>Piro</keyname><forenames>Robert</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Combining Rewriting and Incremental Materialisation Maintenance for
  Datalog Programs with Equality</title><categories>cs.DB cs.DS</categories><comments>All proofs contained in the appendix. 7 pages + 4 pages appendix. 7
  algorithms and one table with evaluation results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Materialisation precomputes all consequences of a set of facts and a datalog
program so that queries can be evaluated directly (i.e., independently from the
program). Rewriting optimises materialisation for datalog programs with
equality by replacing all equal constants with a single representative; and
incremental maintenance algorithms can efficiently update a materialisation for
small changes in the input facts. Both techniques are critical to practical
applicability of datalog systems; however, we are unaware of an approach that
combines rewriting and incremental maintenance. In this paper we present the
first such combination, and we show empirically that it can speed up updates by
several orders of magnitude compared to using either rewriting or incremental
maintenance in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00217</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00217</id><created>2015-05-01</created><authors><author><keyname>Molinero</keyname><forenames>Carlos</forenames></author><author><keyname>Arcaute</keyname><forenames>Elsa</forenames></author><author><keyname>Smith</keyname><forenames>Duncan</forenames></author><author><keyname>Batty</keyname><forenames>Michael</forenames></author></authors><title>The Fractured Nature of British Politics</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The outcome of the British General Election to be held in just over one
week's time is widely regarded as the most difficult in living memory to
predict. Current polls suggest that the two main parties are neck and neck but
that there will be a landslide to the Scottish Nationalist Party with that
party taking most of the constituencies in Scotland. The Liberal Democrats are
forecast to loose more than half their seats and the fringe parties of whom the
UK Independence Party is the biggest are simply unknown quantities. Much of
this volatility relates to long-standing and deeply rooted cultural and
nationalist attitudes that relate to geographical fault lines that have been
present for 500 years or more but occasionally reveal themselves, at times like
this. In this paper our purpose is to raise the notion that these fault lines
are critical to thinking about regionalism, nationalism and the hierarchy of
cities in Great Britain (excluding Northern Ireland). We use a percolation
method (Arcaute et al. 2015) to reveal them that treats Britain as a giant
cluster of related places each defined from the intersections of the road
network at a very fine spatial scale. We break this giant cluster into a
detailed hierarchy of sub-clusters by successively reducing a distance
threshold which first breaks off some of the Scottish Islands and then reveals
the very distinct nations and regions that make up Britain, all the way down to
the definition of the largest cities that appear when the threshold reaches
300m. We use these percolation clusters to apportion the 2010 voting pattern to
a new hierarchy of constituencies based on these clusters, and this gives us a
picture of how Britain might vote on purely geographical lines. We then examine
this voting pattern which provides us with some sense of how important the new
configuration of political parties might be to the election next week.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00218</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00218</id><created>2015-05-01</created><authors><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author><author><keyname>Isack</keyname><forenames>Hossam</forenames></author><author><keyname>Olsson</keyname><forenames>Carl</forenames></author><author><keyname>Ayed</keyname><forenames>Ismail Ben</forenames></author></authors><title>Volumetric Bias in Segmentation and Reconstruction: Secrets and
  Solutions</title><categories>cs.CV</categories><comments>9 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many standard optimization methods for segmentation and reconstruction
compute ML model estimates for appearance or geometry of segments, e.g.
Zhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012.
We observe that the standard likelihood term in these formulations corresponds
to a generalized probabilistic K-means energy. In learning it is well known
that this energy has a strong bias to clusters of equal size, which can be
expressed as a penalty for KL divergence from a uniform distribution of
cardinalities. However, this volumetric bias has been mostly ignored in
computer vision. We demonstrate significant artifacts in standard segmentation
and reconstruction methods due to this bias. Moreover, we propose binary and
multi-label optimization techniques that either (a) remove this bias or (b)
replace it by a KL divergence term for any given target volume distribution.
Our general ideas apply to many continuous or discrete energy formulations in
segmentation, stereo, and other reconstruction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00236</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00236</id><created>2015-05-01</created><authors><author><keyname>Kande</keyname><forenames>Chaitanya Krishna</forenames></author></authors><title>Discussion of various models related to cloud performance</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the various models related to cloud computing. Knowing
the metrics related to infrastructure is very critical to enhance the
performance of cloud services. Various metrics related to clouds such as
pageview response time, admission control and enforcing elasticity to cloud
infrastructure are very crucial in analyzing the characteristics of the cloud
to enhance the cloud performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00241</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00241</id><created>2015-05-01</created><authors><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Pastor</keyname><forenames>Peter</forenames></author><author><keyname>Kalakrishnan</keyname><forenames>Mrinal</forenames></author><author><keyname>Bohg</keyname><forenames>Jeannette</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Probabilistic Object Tracking using a Range Camera</title><categories>cs.RO</categories><doi>10.1109/IROS.2013.6696810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of tracking the 6-DoF pose of an object while it is
being manipulated by a human or a robot. We use a dynamic Bayesian network to
perform inference and compute a posterior distribution over the current object
pose. Depending on whether a robot or a human manipulates the object, we employ
a process model with or without knowledge of control inputs. Observations are
obtained from a range camera. As opposed to previous object tracking methods,
we explicitly model self-occlusions and occlusions from the environment, e.g,
the human or robotic hand. This leads to a strongly non-linear observation
model and additional dependencies in the Bayesian network. We employ a
Rao-Blackwellised particle filter to compute an estimate of the object pose at
every time step. In a set of experiments, we demonstrate the ability of our
method to accurately and robustly track the object pose in real-time while it
is being manipulated by a human or a robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00244</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00244</id><created>2015-05-01</created><authors><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author></authors><title>An Improved Private Mechanism for Small Databases</title><categories>cs.DS cs.CR</categories><comments>To appear in ICALP 2015, Track A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of answering a workload of linear queries $\mathcal{Q}$,
on a database of size at most $n = o(|\mathcal{Q}|)$ drawn from a universe
$\mathcal{U}$ under the constraint of (approximate) differential privacy.
Nikolov, Talwar, and Zhang~\cite{NTZ} proposed an efficient mechanism that, for
any given $\mathcal{Q}$ and $n$, answers the queries with average error that is
at most a factor polynomial in $\log |\mathcal{Q}|$ and $\log |\mathcal{U}|$
worse than the best possible. Here we improve on this guarantee and give a
mechanism whose competitiveness ratio is at most polynomial in $\log n$ and
$\log |\mathcal{U}|$, and has no dependence on $|\mathcal{Q}|$. Our mechanism
is based on the projection mechanism of Nikolov, Talwar, and Zhang, but in
place of an ad-hoc noise distribution, we use a distribution which is in a
sense optimal for the projection mechanism, and analyze it using convex duality
and the restricted invertibility principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00246</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00246</id><created>2015-05-01</created><updated>2015-05-20</updated><authors><author><keyname>Tegmark</keyname><forenames>Max</forenames><affiliation>MIT</affiliation></author></authors><title>Nuclear War from a Cosmic Perspective</title><categories>physics.soc-ph cs.CY</categories><comments>Typos corrected. Based on my talk at the symposium &quot;The Dynamics of
  Possible Nuclear Extinction&quot; held February 28-March 1 2015 at The New York
  Academy of Medicine, http://totalwebcasting.com/view/?id=hcf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I discuss the impact of computer progress on nuclear war policy, both by
enabling more accurate nuclear winter simulations and by affecting the
probability of war starting accidentally. I argue that from a cosmic
perspective, humanity's track record of risk mitigation is inexcusably
pathetic, jeopardizing the potential for life to flourish for billions of
years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00249</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00249</id><created>2015-05-01</created><authors><author><keyname>Zlateski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Seung</keyname><forenames>H. Sebastian</forenames></author></authors><title>Image Segmentation by Size-Dependent Single Linkage Clustering of a
  Watershed Basin Graph</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for hierarchical image segmentation that defines a
disaffinity graph on the image, over-segments it into watershed basins, defines
a new graph on the basins, and then merges basins with a modified,
size-dependent version of single linkage clustering. The quasilinear runtime of
the method makes it suitable for segmenting large images. We illustrate the
method on the challenging problem of segmenting 3D electron microscopic brain
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00251</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00251</id><created>2015-05-01</created><authors><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Bohg</keyname><forenames>Jeannette</forenames></author><author><keyname>Kappler</keyname><forenames>Daniel</forenames></author><author><keyname>Pfreundt</keyname><forenames>Claudia</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>The Coordinate Particle Filter - A novel Particle Filter for High
  Dimensional Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric filters, such as the Extended Kalman Filter and the Unscented
Kalman Filter, typically scale well with the dimensionality of the problem, but
they are known to fail if the posterior state distribution cannot be closely
approximated by a density of the assumed parametric form. For nonparametric
filters, such as the Particle Filter, the converse holds. Such methods are able
to approximate any posterior, but the computational requirements scale
exponentially with the number of dimensions of the state space. In this paper,
we present the Coordinate Particle Filter which alleviates this problem. We
propose to compute the particle weights recursively, dimension by dimension.
This allows us to explore one dimension at a time, and resample after each
dimension if necessary. Experimental results on simulated as well as real data
confirm that the proposed method has a substantial performance advantage over
the Particle Filter in high-dimensional systems where not all dimensions are
highly correlated. We demonstrate the benefits of the proposed method for the
problem of multi-object and robotic manipulator tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00253</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00253</id><created>2015-05-01</created><authors><author><keyname>Pittu</keyname><forenames>Ganesh Reddy</forenames></author></authors><title>Generating Primes Using Partitions</title><categories>cs.CR</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new technique of generating large prime numbers using a
smaller one by employing Goldbach partitions. Experiments are presented showing
how this method produces candidate prime numbers that are subsequently tested
using either Miller Rabin or AKS primality tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00256</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00256</id><created>2015-05-01</created><updated>2015-09-26</updated><authors><author><keyname>Chen</keyname><forenames>Chenyi</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Kornhauser</keyname><forenames>Alain</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>DeepDriving: Learning Affordance for Direct Perception in Autonomous
  Driving</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, there are two major paradigms for vision-based autonomous driving
systems: mediated perception approaches that parse an entire scene to make a
driving decision, and behavior reflex approaches that directly map an input
image to a driving action by a regressor. In this paper, we propose a third
paradigm: a direct perception approach to estimate the affordance for driving.
We propose to map an input image to a small number of key perception indicators
that directly relate to the affordance of a road/traffic state for driving. Our
representation provides a set of compact yet complete descriptions of the scene
to enable a simple controller to drive autonomously. Falling in between the two
extremes of mediated perception and behavior reflex, we argue that our direct
perception representation provides the right level of abstraction. To
demonstrate this, we train a deep Convolutional Neural Network using recording
from 12 hours of human driving in a video game and show that our model can work
well to drive a car in a very diverse set of virtual environments. We also
train a model for car distance estimation on the KITTI dataset. Results show
that our direct perception approach can generalize well to real driving images.
Source code and data are available on our project website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00267</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00267</id><created>2015-05-01</created><updated>2016-03-02</updated><authors><author><keyname>Zeng</keyname><forenames>Yanyan</forenames></author><author><keyname>Mills</keyname><forenames>K. Alex</forenames></author><author><keyname>Gokhale</keyname><forenames>Shreyas</forenames></author><author><keyname>Mittal</keyname><forenames>Neeraj</forenames></author><author><keyname>Venkatesan</keyname><forenames>S.</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>R.</forenames></author></authors><title>Robust Neighbor Discovery in Multi-Hop Multi-Channel Heterogeneous
  Wireless Networks</title><categories>cs.DC</categories><comments>54 pages, 5 figures, 1 table, 6 algorithms, accepted in Journal of
  Parallel and Distributed Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important first step when deploying a wireless ad hoc network is neighbor
discovery in which every node attempts to determine the set of nodes it can
communicate with in one wireless hop. In the recent years, cognitive radio (CR)
technology has gained attention as an attractive approach to alleviate spectrum
congestion. A cognitive radio transceiver can operate over a wide range of
frequencies, possibly scanning multiple frequency bands. A cognitive radio node
can opportunistically utilize unused wireless spectrum without interference
from other wireless devices in its vicinity. Due to spatial variations in
frequency usage and hardware variations in radio transceivers, different nodes
in the network may perceive different subsets of frequencies available to them
for communication. This heterogeneity in the available channel sets across the
network increases the complexity of solving the neighbor discovery problem in a
cognitive radio network. In this work, we design and analyze several randomized
algorithms for neighbor discovery in such a (heterogeneous) network under a
variety of assumptions (e.g. maximum node degree known or unknown) for both
synchronous and asynchronous systems under minimal knowledge. We also show that
our randomized algorithms are naturally suited to tolerate unreliable channels
and adversarial attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00273</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00273</id><created>2015-05-01</created><updated>2015-11-20</updated><authors><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Chouzenoux</keyname><forenames>Emilie</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steve</forenames></author></authors><title>A Survey of Stochastic Simulation and Optimization Methods in Signal
  Processing</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Journal of Selected Topics in Signal Processing
  special issue on Stochastic Simulation and Optimisation in Signal Processing,
  March 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern signal processing (SP) methods rely very heavily on probability and
statistics to solve challenging SP problems. SP methods are now expected to
deal with ever more complex models, requiring ever more sophisticated
computational inference techniques. This has driven the development of
statistical SP methods based on stochastic simulation and optimization.
Stochastic simulation and optimization algorithms are computationally intensive
tools for performing statistical inference in models that are analytically
intractable and beyond the scope of deterministic inference methods. They have
been recently successfully applied to many difficult problems involving complex
statistical models and sophisticated (often Bayesian) statistical inference
techniques. This survey paper offers an introduction to stochastic simulation
and optimization methods in signal and image processing. The paper addresses a
variety of high-dimensional Markov chain Monte Carlo (MCMC) methods as well as
deterministic surrogate methods, such as variational Bayes, the Bethe approach,
belief and expectation propagation and approximate message passing algorithms.
It also discusses a range of optimization methods that have been adopted to
solve stochastic problems, as well as stochastic methods for deterministic
optimization. Subsequently, areas of overlap between simulation and
optimization, in particular optimization-within-MCMC and MCMC-driven
optimization are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00274</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00274</id><created>2015-05-01</created><updated>2015-11-23</updated><authors><author><keyname>Liu</keyname><forenames>Miao</forenames></author><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Liao</keyname><forenames>Xuejun</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author></authors><title>Stick-Breaking Policy Learning in Dec-POMDPs</title><categories>cs.AI cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expectation maximization (EM) has recently been shown to be an efficient
algorithm for learning finite-state controllers (FSCs) in large decentralized
POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often
converge to maxima that are far from optimal. This paper considers a
variable-size FSC to represent the local policy of each agent. These
variable-size FSCs are constructed using a stick-breaking prior, leading to a
new framework called \emph{decentralized stick-breaking policy representation}
(Dec-SBPR). This approach learns the controller parameters with a variational
Bayesian algorithm without having to assume that the Dec-POMDP model is
available. The performance of Dec-SBPR is demonstrated on several benchmark
problems, showing that the algorithm scales to large problems while
outperforming other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00276</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00276</id><created>2015-05-01</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Cohen</keyname><forenames>Scott</forenames></author><author><keyname>Price</keyname><forenames>Brian</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Joint Object and Part Segmentation using Deep Learned Potentials</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmenting semantic objects from images and parsing them into their
respective semantic parts are fundamental steps towards detailed object
understanding in computer vision. In this paper, we propose a joint solution
that tackles semantic object and part segmentation simultaneously, in which
higher object-level context is provided to guide part segmentation, and more
detailed part-level localization is utilized to refine object segmentation.
Specifically, we first introduce the concept of semantic compositional parts
(SCP) in which similar semantic parts are grouped and shared among different
objects. A two-channel fully convolutional network (FCN) is then trained to
provide the SCP and object potentials at each pixel. At the same time, a
compact set of segments can also be obtained from the SCP predictions of the
network. Given the potentials and the generated segments, in order to explore
long-range context, we finally construct an efficient fully connected
conditional random field (FCRF) to jointly predict the final object and part
labels. Extensive evaluation on three different datasets shows that our
approach can mutually enhance the performance of object and part segmentation,
and outperforms the current state-of-the-art on both tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00277</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00277</id><created>2015-05-01</created><authors><author><keyname>Movshovitz-Attias</keyname><forenames>Dana</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author></authors><title>Grounded Discovery of Coordinate Term Relationships between Software
  Entities</title><categories>cs.CL cs.AI cs.LG cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for the detection of coordinate-term relationships
between entities from the software domain, that refer to Java classes. Usually,
relations are found by examining corpus statistics associated with text
entities. In some technical domains, however, we have access to additional
information about the real-world objects named by the entities, suggesting that
coupling information about the &quot;grounded&quot; entities with corpus statistics might
lead to improved methods for relation discovery. To this end, we develop a
similarity measure for Java classes using distributional information about how
they are used in software, which we combine with corpus statistics on the
distribution of contexts in which the classes appear in text. Using our
approach, cross-validation accuracy on this dataset can be improved
dramatically, from around 60% to 88%. Human labeling results show that our
classifier has an F1 score of 86% over the top 1000 predicted pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00278</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00278</id><created>2015-05-01</created><authors><author><keyname>Mattsson</keyname><forenames>Bj&#xf6;rn Persson</forenames></author><author><keyname>Vajda</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>&#x10c;ertick&#xfd;</keyname><forenames>Michal</forenames></author></authors><title>Automatic Observer Script for StarCraft: Brood War Bot Games (technical
  report)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short report describes an automated BWAPI-based script developed for
live streams of a StarCraft Brood War bot tournament, SSCAIT. The script
controls the in-game camera in order to follow the relevant events and improve
the viewer experience. We enumerate its novel features and provide a few
implementation notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00284</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00284</id><created>2015-05-01</created><updated>2015-12-14</updated><authors><author><keyname>Rosman</keyname><forenames>Benjamin</forenames></author><author><keyname>Hawasly</keyname><forenames>Majd</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author></authors><title>Bayesian Policy Reuse</title><categories>cs.AI</categories><comments>32 pages, submitted to the Machine Learning Journal</comments><msc-class>68T05</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long-lived autonomous agent should be able to respond online to novel
instances of tasks from a familiar domain. Acting online requires 'fast'
responses, in terms of rapid convergence, especially when the task instance has
a short duration, such as in applications involving interactions with humans.
These requirements can be problematic for many established methods for learning
to act. In domains where the agent knows that the task instance is drawn from a
family of related tasks, albeit without access to the label of any given
instance, it can choose to act through a process of policy reuse from a
library, rather than policy learning from scratch. In policy reuse, the agent
has prior knowledge of the class of tasks in the form of a library of policies
that were learnt from sample task instances during an offline training phase.
We formalise the problem of policy reuse, and present an algorithm for
efficiently responding to a novel task instance by reusing a policy from the
library of existing policies, where the choice is based on observed 'signals'
which correlate to policy performance. We achieve this by posing the problem as
a Bayesian choice problem with a corresponding notion of an optimal response,
but the computation of that response is in many cases intractable. Therefore,
to reduce the computation cost of the posterior, we follow a Bayesian
optimisation approach and define a set of policy selection functions, which
balance exploration in the policy library against exploitation of previously
tried policies, together with a model of expected performance of the policy
library on their corresponding task instances. We validate our method in
several simulated domains of interactive, short-duration episodic tasks,
showing rapid convergence in unknown task variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00289</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00289</id><created>2015-05-01</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R</forenames></author><author><keyname>Roma</keyname><forenames>Gerard</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural
  Network</title><categories>cs.SD</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio source separation is a difficult machine learning problem and
performance is measured by comparing extracted signals with the component
source signals. However, if separation is motivated by the ultimate goal of
re-mixing then complete separation is not necessary and hence separation
difficulty and separation quality are dependent on the nature of the re-mix.
Here, we use a convolutional deep neural network (DNN), trained to estimate
'ideal' binary masks for separating voice from music, to perform re-mixing of
the vocal balance by operating directly on the individual magnitude components
of the musical mixture spectrogram. Our results demonstrate that small changes
in vocal gain may be applied with very little distortion to the ultimate
re-mix. Our method may be useful for re-mixing existing mixes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00290</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00290</id><created>2015-05-01</created><updated>2015-06-30</updated><authors><author><keyname>Kyng</keyname><forenames>Rasmus</forenames></author><author><keyname>Rao</keyname><forenames>Anup</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author></authors><title>Algorithms for Lipschitz Learning on Graphs</title><categories>cs.LG cs.DS math.MG</categories><comments>Code used in this work is available at
  https://github.com/danspielman/YINSlex 30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop fast algorithms for solving regression problems on graphs where
one is given the value of a function at some vertices, and must find its
smoothest possible extension to all vertices. The extension we compute is the
absolutely minimal Lipschitz extension, and is the limit for large $p$ of
$p$-Laplacian regularization. We present an algorithm that computes a minimal
Lipschitz extension in expected linear time, and an algorithm that computes an
absolutely minimal Lipschitz extension in expected time $\widetilde{O} (m n)$.
The latter algorithm has variants that seem to run much faster in practice.
These extensions are particularly amenable to regularization: we can perform
$l_{0}$-regularization on the given values in polynomial time and
$l_{1}$-regularization on the initial function values and on graph edge weights
in time $\widetilde{O} (m^{3/2})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00294</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00294</id><created>2015-05-01</created><authors><author><keyname>Bhatt</keyname><forenames>Nirav</forenames></author><author><keyname>Ayyar</keyname><forenames>Arun</forenames></author></authors><title>Monotonous (Semi-)Nonnegative Matrix Factorization</title><categories>cs.LG stat.ML</categories><acm-class>I.2</acm-class><doi>10.1145/2732587.2732600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) factorizes a non-negative matrix into
product of two non-negative matrices, namely a signal matrix and a mixing
matrix. NMF suffers from the scale and ordering ambiguities. Often, the source
signals can be monotonous in nature. For example, in source separation problem,
the source signals can be monotonously increasing or decreasing while the
mixing matrix can have nonnegative entries. NMF methods may not be effective
for such cases as it suffers from the ordering ambiguity. This paper proposes
an approach to incorporate notion of monotonicity in NMF, labeled as monotonous
NMF. An algorithm based on alternating least-squares is proposed for recovering
monotonous signals from a data matrix. Further, the assumption on mixing matrix
is relaxed to extend monotonous NMF for data matrix with real numbers as
entries. The approach is illustrated using synthetic noisy data. The results
obtained by monotonous NMF are compared with standard NMF algorithms in the
literature, and it is shown that monotonous NMF estimates source signals well
in comparison to standard NMF algorithms when the underlying sources signals
are monotonous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00295</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00295</id><created>2015-05-01</created><updated>2015-12-17</updated><authors><author><keyname>Walker</keyname><forenames>Jacob</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>Dense Optical Flow Prediction from a Static Image</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a scene, what is going to move, and in what direction will it move?
Such a question could be considered a non-semantic form of action prediction.
In this work, we present a convolutional neural network (CNN) based approach
for motion prediction. Given a static image, this CNN predicts the future
motion of each and every pixel in the image in terms of optical flow. Our CNN
model leverages the data in tens of thousands of realistic videos to train our
model. Our method relies on absolutely no human labeling and is able to predict
motion based on the context of the scene. Because our CNN model makes no
assumptions about the underlying scene, it can predict future optical flow on a
diverse set of scenarios. We outperform all previous approaches by large
margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00296</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00296</id><created>2015-05-01</created><authors><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Du</keyname><forenames>Wenbin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Object-Scene Convolutional Neural Networks for Event Recognition in
  Images</title><categories>cs.CV</categories><comments>CVPR, ChaLearn Looking at People (LAP) challenge, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event recognition from still images is of great importance for image
understanding. However, compared with event recognition in videos, there are
much fewer research works on event recognition in images. This paper addresses
the issue of event recognition from images and proposes an effective method
with deep neural networks. Specifically, we design a new architecture, called
Object-Scene Convolutional Neural Network (OS-CNN). This architecture is
decomposed into object net and scene net, which extract useful information for
event understanding from the perspective of objects and scene context,
respectively. Meanwhile, we investigate different network architectures for
OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks
to the task of event recognition. Furthermore, we find that the deep and
very-deep networks are complementary to each other. Finally, based on the
proposed OS-CNN and comparative study of different network architectures, we
come up with a solution of five-stream CNN for the track of cultural event
recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method
obtains the performance of 85.5% and ranks the $1^{st}$ place in this
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00297</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00297</id><created>2015-05-01</created><authors><author><keyname>Beveridge</keyname><forenames>Andrew</forenames></author><author><keyname>Cai</keyname><forenames>Yiqing</forenames></author></authors><title>Two-Dimensional Pursuit-Evasion in a Compact Domain with Piecewise
  Analytic Boundary</title><categories>math.MG cs.GT math.OC</categories><comments>21 pages, 6 figures</comments><msc-class>91A24, 49N75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a pursuit-evasion game, a team of pursuers attempt to capture an evader.
The players alternate turns, move with equal speed, and have full information
about the state of the game. We consider the most restictive capture condition:
a pursuer must become colocated with the evader to win the game. We prove two
general results about pursuit-evasion games in topological spaces. First, we
show that one pursuer has a winning strategy in any CAT(0) space under this
restrictive capture criterion. This complements a result of Alexander, Bishop
and Ghrist, who provide a winning strategy for a game with positive capture
radius. Second, we consider the game played in a compact domain in Euclidean
two-space with piecewise analytic boundary and arbitrary Euler characteristic.
We show that three pursuers always have a winning strategy by extending recent
work of Bhadauria, Klein, Isler and Suri from polygonal environments to our
more general setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00299</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00299</id><created>2015-05-01</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Compressed Sensing Based Multi-User Millimeter Wave Systems: How Many
  Measurements Are Needed?</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) systems will likely employ directional beamforming
with large antenna arrays at both the transmitters and receivers. Acquiring
channel knowledge to design these beamformers, however, is challenging due to
the large antenna arrays and small signal-to-noise ratio before beamforming. In
this paper, we propose and evaluate a downlink system operation for multi-user
mmWave systems based on compressed sensing channel estimation and conjugate
analog beamforming. Adopting the achievable sum-rate as a performance metric,
we show how many compressed sensing measurements are needed to approach the
perfect channel knowledge performance. The results illustrate that the proposed
algorithm requires an order of magnitude less training overhead compared with
traditional lower-frequency solutions, while employing mmWave-suitable
hardware. They also show that the number of measurements need to be optimized
to handle the trade-off between the channel estimate quality and the training
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00303</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00303</id><created>2015-05-01</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Achievable Rates of Multi-User Millimeter Wave Systems with Hybrid
  Precoding</title><categories>cs.IT math.IT</categories><comments>to be presented in IEEE ICC 2015 - Workshop on 5G &amp; Beyond - Enabling
  Technologies and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) systems will likely employ large antenna arrays at
both the transmitters and receivers. A natural application of antenna arrays is
simultaneous transmission to multiple users, which requires multi-user
precoding at the transmitter. Hardware constraints, however, make it difficult
to apply conventional lower frequency MIMO precoding techniques at mmWave. This
paper proposes and analyzes a low complexity hybrid analog/digital beamforming
algorithm for downlink multi-user mmWave systems. Hybrid precoding involves a
combination of analog and digital processing that is motivated by the
requirement to reduce the power consumption of the complete radio frequency and
mixed signal hardware. The proposed algorithm configures hybrid precoders at
the transmitter and analog combiners at multiple receivers with a small
training and feedback overhead. For this algorithm, we derive a lower bound on
the achievable rate for the case of single-path channels, show its asymptotic
optimality at large numbers of antennas, and make useful insights for more
general cases. Simulation results show that the proposed algorithm offers
higher sum rates compared with analog-only beamforming, and approaches the
performance of the unconstrained digital precoding solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00308</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00308</id><created>2015-05-01</created><authors><author><keyname>Nimmagadda</keyname><forenames>Tejaswi</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Multi-Object Classification and Unsupervised Scene Understanding Using
  Deep Learning Features and Latent Tree Probabilistic Models</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has shown state-of-art classification performance on datasets
such as ImageNet, which contain a single object in each image. However,
multi-object classification is far more challenging. We present a unified
framework which leverages the strengths of multiple machine learning methods,
viz deep learning, probabilistic models and kernel methods to obtain
state-of-art performance on Microsoft COCO, consisting of non-iconic images. We
incorporate contextual information in natural images through a conditional
latent tree probabilistic model (CLTM), where the object co-occurrences are
conditioned on the extracted fc7 features from pre-trained Imagenet CNN as
input. We learn the CLTM tree structure using conditional pairwise
probabilities for object co-occurrences, estimated through kernel methods, and
we learn its node and edge potentials by training a new 3-layer neural network,
which takes fc7 features as input. Object classification is carried out via
inference on the learnt conditional tree model, and we obtain significant gain
in precision-recall and F-measures on MS-COCO, especially for difficult object
categories. Moreover, the latent variables in the CLTM capture scene
information: the images with top activations for a latent node have common
themes such as being a grasslands or a food scene, and on on. In addition, we
show that a simple k-means clustering of the inferred latent nodes alone
significantly improves scene classification performance on the MIT-Indoor
dataset, without the need for any retraining, and without using scene labels
during training. Thus, we present a unified framework for multi-object
classification and unsupervised scene understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00314</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00314</id><created>2015-05-02</created><authors><author><keyname>Narasimhan</keyname><forenames>Shankar</forenames></author><author><keyname>Bhatt</keyname><forenames>Nirav</forenames></author></authors><title>Deconstructing Principal Component Analysis Using a Data Reconciliation
  Perspective</title><categories>cs.LG cs.SY stat.ME</categories><acm-class>I.2</acm-class><journal-ref>Computers and Chemical Engineering 77 (2015) 74-84</journal-ref><doi>10.1016/j.compchemeng.2015.03.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data reconciliation (DR) and Principal Component Analysis (PCA) are two
popular data analysis techniques in process industries. Data reconciliation is
used to obtain accurate and consistent estimates of variables and parameters
from erroneous measurements. PCA is primarily used as a method for reducing the
dimensionality of high dimensional data and as a preprocessing technique for
denoising measurements. These techniques have been developed and deployed
independently of each other. The primary purpose of this article is to
elucidate the close relationship between these two seemingly disparate
techniques. This leads to a unified framework for applying PCA and DR. Further,
we show how the two techniques can be deployed together in a collaborative and
consistent manner to process data. The framework has been extended to deal with
partially measured systems and to incorporate partial knowledge available about
the process model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00315</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00315</id><created>2015-05-02</created><authors><author><keyname>Ramanathan</keyname><forenames>Vignesh</forenames></author><author><keyname>Tang</keyname><forenames>Kevin</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Learning Temporal Embeddings for Complex Video Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to learn temporal embeddings of video frames for
complex video analysis. Large quantities of unlabeled video data can be easily
obtained from the Internet. These videos possess the implicit weak label that
they are sequences of temporally and semantically coherent images. We leverage
this information to learn temporal embeddings for video frames by associating
frames with the temporal context that they appear in. To do this, we propose a
scheme for incorporating temporal context based on past and future frames in
videos, and compare this to other contextual representations. In addition, we
show how data augmentation using multi-resolution samples and hard negatives
helps to significantly improve the quality of the learned embeddings. We
evaluate various design decisions for learning temporal embeddings, and show
that our embeddings can improve performance for multiple video tasks such as
retrieval, classification, and temporal order recovery in unconstrained
Internet video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00322</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00322</id><created>2015-05-02</created><updated>2015-06-03</updated><authors><author><keyname>Curran</keyname><forenames>William</forenames></author><author><keyname>Brys</keyname><forenames>Tim</forenames></author><author><keyname>Taylor</keyname><forenames>Matthew</forenames></author><author><keyname>Smart</keyname><forenames>William</forenames></author></authors><title>Using PCA to Efficiently Represent State Spaces</title><categories>cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning algorithms need to deal with the exponential growth of
states and actions when exploring optimal control in high-dimensional spaces.
This is known as the curse of dimensionality. By projecting the agent's state
onto a low-dimensional manifold, we can represent the state space in a smaller
and more efficient representation. By using this representation during
learning, the agent can converge to a good policy much faster. We test this
approach in the Mario Benchmarking Domain. When using dimensionality reduction
in Mario, learning converges much faster to a good policy. But, there is a
critical convergence-performance trade-off. By projecting onto a
low-dimensional manifold, we are ignoring important data. In this paper, we
explore this trade-off of convergence and performance. We find that learning in
as few as 4 dimensions (instead of 9), we can improve performance past learning
in the full dimensional space at a faster convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00326</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00326</id><created>2015-05-02</created><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author></authors><title>Combining Existential Rules and Description Logics (Extended Version)</title><categories>cs.DB</categories><comments>32 pages. To appear in IJCAI 2015. Extended version including proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query answering under existential rules -- implications with existential
quantifiers in the head -- is known to be decidable when imposing restrictions
on the rule bodies such as frontier-guardedness [BLM10, BLMS11]. Query
answering is also decidable for description logics [Baa03], which further allow
disjunction and functionality constraints (assert that certain relations are
functions), however, they are focused on ER-type schemas, where relations have
arity two.
  This work investigates how to get the best of both worlds: having decidable
existential rules on arbitrary arity relations, while allowing rich description
logics, including functionality constraints, on arity-two relations. We first
show negative results on combining such decidable languages. Second, we
introduce an expressive set of existential rules (frontier-one rules with a
certain restriction) which can be combined with powerful constraints on
arity-two relations (e.g. GC 2, ALCQIb) while retaining decidable query
answering. Further, we provide conditions to add functionality constraints on
the higher-arity relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00330</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00330</id><created>2015-05-02</created><updated>2015-08-31</updated><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Linear Precoding of Data and Artificial Noise in Secure Massive MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>Revision submitted to IEEE Transactions on Wireless Communications,
  34 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider secure downlink transmission in a multi-cell
massive multiple-input multiple-output (MIMO) system where the numbers of base
station (BS) antennas, mobile terminals, and eavesdropper antennas are
asymptotically large. The channel state information of the eavesdropper is
assumed to be unavailable at the BS and hence, linear precoding of data and
artificial noise (AN) are employed for secrecy enhancement. Four different data
precoders (i.e., selfish zero-forcing (ZF)/regularized channel inversion (RCI)
and collaborative ZF/RCI precoders) and three different AN precoders (i.e.,
random, selfish/collaborative null-space based precoders) are investigated and
the corresponding achievable ergodic secrecy rates are analyzed. Our analysis
includes the effects of uplink channel estimation, pilot contamination,
multi-cell interference, and path-loss. Furthermore, to strike a balance
between complexity and performance, linear precoders that are based on matrix
polynomials are proposed for both data and AN precoding. The polynomial
coefficients of the data and AN precoders are optimized respectively for
minimization of the sum mean squared error of and the AN leakage to the mobile
terminals in the cell of interest using tools from free probability and random
matrix theory. Our analytical and simulation results provide interesting
insights for the design of secure multi-cell massive MIMO systems and reveal
that the proposed polynomial data and AN precoders closely approach the
performance of selfish RCI data and null-space based AN precoders,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00335</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00335</id><created>2015-05-02</created><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author></authors><title>Cracking a hierarchical chaotic image encryption algorithm based on
  permutation</title><categories>cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In year 2000, an efficient hierarchical chaotic image encryption (HCIE)
algorithm was proposed, which divides a plain-image of size $M\times N$ with
$T$ possible value levels into $K$ blocks of the same size and then operates
position permutation on two levels: intra-block and inter-block. As a typical
position permutation-only encryption algorithm, it has received intensive
attention. The present paper analyzes specific security performance of HCIE
against ciphertext-only attack and known/chosen-plaintext attack. It is found
that only $O(\lceil\log_T(M\cdot N/K) \rceil)$ known/chosen plain-images are
sufficient to achieve a good performance, and the computational complexity is
$O(M\cdot N\cdot \lceil\log_T(M\cdot N/K) \rceil)$, which effectively
demonstrates that hierarchical permutation-only image encryption algorithms are
less secure than normal (i.e., non-hierarchical) ones. Detailed experiment
results are given to verify the feasibility of the known-plaintext attack. In
addition, it is pointed out that the security of HCIE against ciphertext-only
attack was much overestimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00341</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00341</id><created>2015-05-02</created><updated>2016-02-11</updated><authors><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author></authors><title>Structure in Dichotomous Preferences</title><categories>cs.GT</categories><comments>A preliminary version appeared in the proceedings of IJCAI 2015, the
  International Joint Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many hard computational social choice problems are known to become tractable
when voters' preferences belong to a restricted domain, such as those of
single-peaked or single-crossing preferences. However, to date, all algorithmic
results of this type have been obtained for the setting where each voter's
preference list is a total order of candidates. The goal of this paper is to
extend this line of research to the setting where voters' preferences are
dichotomous, i.e., each voter approves a subset of candidates and disapproves
the remaining candidates. We propose several analogues of the notions of
single-peaked and single-crossing preferences for dichotomous profiles and
investigate the relationships among them. We then demonstrate that for some of
these notions the respective restricted domains admit efficient algorithms for
computationally hard approval-based multi-winner rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00343</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00343</id><created>2015-05-02</created><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Quick</keyname><forenames>David</forenames></author></authors><title>A first-order logic for string diagrams</title><categories>math.CT cs.LO</categories><comments>15 pages + appendix</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Equational reasoning with string diagrams provides an intuitive means of
proving equations between morphisms in a symmetric monoidal category. This can
be extended to proofs of infinite families of equations using a simple
graphical syntax called !-box notation. While this does greatly increase the
proving power of string diagrams, previous attempts to go beyond equational
reasoning have been largely ad hoc, owing to the lack of a suitable logical
framework for diagrammatic proofs involving !-boxes. In this paper, we extend
equational reasoning with !-boxes to a fully-fledged first order logic called
with conjunction, implication, and universal quantification over !-boxes. This
logic, called !L, is then rich enough to properly formalise an induction
principle for !-boxes. We then build a standard model for !L and give an
example proof of a theorem for non-commutative bialgebras using !L, which is
unobtainable by equational reasoning alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00344</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00344</id><created>2015-05-02</created><authors><author><keyname>Merrison-Hort</keyname><forenames>Robert</forenames></author></authors><title>Fireflies: New software for interactively exploring dynamical systems
  using GPU computing</title><categories>cs.MS cs.DC math.DS</categories><comments>31 pages, 8 figures, 4 supplementary videos</comments><doi>10.1142/S0218127415501813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In non-linear systems, where explicit analytic solutions usually can't be
found, visualisation is a powerful approach which can give insights into the
dynamical behaviour of models; it is also crucial for teaching this area of
mathematics. In this paper we present new software, Fireflies, which exploits
the power of graphical processing unit (GPU) computing to produce spectacular
interactive visualisations of arbitrary systems of ordinary differential
equations. In contrast to typical phase portraits, Fireflies draws the current
position of trajectories (projected onto 2D or 3D space) as single points of
light, which move as the system is simulated. Due to the massively parallel
nature of GPU hardware, Fireflies is able to simulate millions of trajectories
in parallel (even on standard desktop computer hardware), producing &quot;swarms&quot; of
particles that move around the screen in real-time according to the equations
of the system. Particles that move forwards in time reveal stable attractors
(e.g. fixed points and limit cycles), while the option of integrating another
group of trajectories backwards in time can reveal unstable objects
(repellers). Fireflies allows the user to change the parameters of the system
as it is running, in order to see the effect that they have on the dynamics and
to observe bifurcations. We demonstrate the capabilities of the software with
three examples: a two-dimensional &quot;mean field&quot; model of neuronal activity, the
classical Lorenz system, and a 15-dimensional model of three interacting
biologically realistic neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00346</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00346</id><created>2015-05-02</created><updated>2016-03-08</updated><authors><author><keyname>Abtahi</keyname><forenames>Azra</forenames></author><author><keyname>Modarres-Hashemi</keyname><forenames>M.</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author><author><keyname>Tabataba</keyname><forenames>Foroogh S.</forenames></author></authors><title>Power Allocation and Measurement Matrix Design for Block CS-Based
  Distributed MIMO Radars</title><categories>cs.IT math.IT</categories><comments>The paper is accepted in Elseveir Aerospace Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) radars offer higher resolution, better
target detection, and more accurate target parameter estimation. Due to the
sparsity of the targets in space-velocity domain, we can exploit Compressive
Sensing (CS) to improve the performance of MIMO radars when the sampling rate
is much less than the Nyquist rate. In distributed MIMO radars, block CS
methods can be used instead of classical CS ones for more performance
improvement, because the received signal in this group of MIMO radars is a
block sparse signal in a basis. In this paper, two new methods are proposed to
improve the performance of the block CS-based distributed MIMO radars. The
first one is a new method for optimal energy allocation to the transmitters,
and the other one is a new method for optimal design of the measurement matrix.
These methods are based on the minimization of an upper bound of the sensing
matrix block-coherence. Simulation results show an increase in the accuracy of
multiple targets parameters estimation for both proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00353</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00353</id><created>2015-05-02</created><authors><author><keyname>Yin</keyname><forenames>Xi</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Chen</keyname><forenames>Jin</forenames></author><author><keyname>Kramer</keyname><forenames>David M.</forenames></author></authors><title>Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence
  Plant Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework for fluorescence plant video
processing. Biologists are interested in the leaf level photosynthetic analysis
within a plant. A prerequisite for such analysis is to segment all leaves,
estimate their structures and track them over time. We treat this as a joint
multi-leaf segmentation, alignment, and tracking problem. First, leaf
segmentation and alignment are applied on the last frame of a plant video to
find a number of well-aligned leaf candidates. Second, leaf tracking is applied
on the remaining frames with leaf candidate transformation from the previous
frame. We form two optimization problems with shared terms in their objective
functions for leaf alignment and tracking respectively. Gradient descent is
used to solve the proposed optimization problems. A quantitative evaluation
framework is formulated to evaluate the performance of our algorithm with three
metrics. Two models are learned to predict the alignment accuracy and detect
tracking failure respectively. We also study the limitation of our proposed
alignment and tracking framework. Experimental results show the effectiveness,
efficiency, and robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00356</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00356</id><created>2015-05-02</created><authors><author><keyname>Batoul</keyname><forenames>Aicha</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>On Repeated-Root Constacyclic Codes of Length $2^amp^r$ over Finite
  Fields</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1406.1848 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we investigate the structure of repeated root constacyclic
codes of length $2^amp^r$ over $\mathbb{F}_{p^s}$ with $a\geq1$ and $(m,p)=1$.
  We characterize the codes in terms of their generator polynomials. This
provides simple conditions on the existence of self-dual negacyclic codes.
Further, we gave cases where the constacyclic codes are equivalent to cyclic
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00357</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00357</id><created>2015-05-02</created><updated>2015-10-03</updated><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Golin</keyname><forenames>Mordecai</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Optimal search trees with 2-way comparisons</title><categories>cs.DS</categories><comments>v4 is full version of extended abstract from ISAAC'15</comments><report-no>ISAAC 2015; LNCS 9472</report-no><msc-class>68P10, 68P30, 68W25, 94A45,</msc-class><acm-class>E.4; G.1.6; G.2.2; H.3.1; I.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1971, Knuth gave an $O(n^2)$-time algorithm for the classic problem of
finding an optimal binary search tree. Knuth's algorithm works only for search
trees based on 3-way comparisons, while most modern computers support only
2-way comparisons (e.g., $&lt;, \le, =, \ge$, and $&gt;$). Until this paper, the
problem of finding an optimal search tree using 2-way comparisons remained open
-- poly-time algorithms were known only for restricted variants. We solve the
general case, giving (i) an $O(n^4)$-time algorithm and (ii) an $O(n \log
n)$-time additive-3 approximation algorithm. Also, for finding optimal binary
split trees, we (iii) obtain a linear speedup and (iv) prove some previous work
incorrect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00359</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00359</id><created>2015-05-02</created><updated>2015-06-20</updated><authors><author><keyname>de Vries</keyname><forenames>Harm</forenames></author><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author></authors><title>Can deep learning help you find the perfect match?</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is he/she my type or not? The answer to this question depends on the personal
preferences of the one asking it. The individual process of obtaining a full
answer may generally be difficult and time consuming, but often an approximate
answer can be obtained simply by looking at a photo of the potential match.
Such approximate answers based on visual cues can be produced in a fraction of
a second, a phenomenon that has led to a series of recently successful dating
apps in which users rate others positively or negatively using primarily a
single photo. In this paper we explore using convolutional networks to create a
model of an individual's personal preferences based on rated photos. This
introduced task is difficult due to the large number of variations in profile
pictures and the noise in attractiveness labels. Toward this task we collect a
dataset comprised of $9364$ pictures and binary labels for each. We compare
performance of convolutional models trained in three ways: first directly on
the collected dataset, second with features transferred from a network trained
to predict gender, and third with features transferred from a network trained
on ImageNet. Our findings show that ImageNet features transfer best, producing
a model that attains $68.1\%$ accuracy on the test set and is moderately
successful at predicting matches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00370</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00370</id><created>2015-05-02</created><updated>2015-10-31</updated><authors><author><keyname>Drmac</keyname><forenames>Zlatko</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author></authors><title>A New Selection Operator for the Discrete Empirical Interpolation Method
  -- improved a priori error bound and extensions</title><categories>cs.NA math.DS math.NA</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new framework for constructing the Discrete Empirical
Interpolation Method DEIM projection operator. The interpolation node selection
procedure is formulated using the QR factorization with column pivoting, and it
enjoys a sharper error bound for the DEIM projection error. Furthermore, for a
subspace $\mathcal{U}$ given as the range of an orthonormal $U$, the DEIM
projection does not change if $U$ is replaced by $U \Omega$ with arbitrary
unitary matrix $\Omega$. In a large-scale setting, the new approach allows
modifications that use only randomly sampled rows of $U$, but with the
potential of producing good approximations with corresponding probabilistic
error bounds. Another salient feature of the new framework is that robust and
efficient software implementation is easily developed, based on readily
available high performance linear algebra packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00383</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00383</id><created>2015-05-02</created><authors><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yu</keyname><forenames>Xiangcheng</forenames></author></authors><title>Tracking Many Solution Paths of a Polynomial Homotopy on a Graphics
  Processing Unit</title><categories>cs.MS cs.NA math.AG math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial systems occur in many areas of science and engineering. Unlike
general nonlinear systems, the algebraic structure enables to compute all
solutions of a polynomial system. We describe our massive parallel
predictor-corrector algorithms to track many solution paths of a polynomial
homotopy. The data parallelism that provides the speedups stems from the
evaluation and differentiation of the monomials in the same polynomial system
at different data points, which are the points on the solution paths.
Polynomial homotopies that have tens of thousands of solution paths can keep a
sufficiently large amount of threads occupied. Our accelerated code combines
the reverse mode of algorithmic differentiation with double double and quad
double precision to compute more accurate results faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00384</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00384</id><created>2015-05-02</created><authors><author><keyname>Tushar</keyname><forenames>Abhinav</forenames></author></authors><title>Making Sense of Hidden Layer Information in Deep Networks by Learning
  Hierarchical Targets</title><categories>cs.NE cs.LG</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an architecture for deep neural networks with hidden
layer branches that learn targets of lower hierarchy than final layer targets.
The branches provide a channel for enforcing useful information in hidden layer
which helps in attaining better accuracy, both for the final layer and hidden
layers. The shared layers modify their weights using the gradients of all cost
functions higher than the branching layer. This model provides a flexible
inference system with many levels of targets which is modular and can be used
efficiently in situations requiring different levels of results according to
complexity. This paper applies the idea to a text classification task on 20
Newsgroups data set with two level of hierarchical targets and a comparison is
made with training without the use of hidden layer branches.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="76000" completeListSize="102538">1122234|77001</resumptionToken>
</ListRecords>
</OAI-PMH>
