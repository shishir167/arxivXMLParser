<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:51:17Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|22001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1194</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1194</id><created>2011-06-06</created><updated>2013-09-19</updated><authors><author><keyname>Anastassi</keyname><forenames>Angelos A.</forenames></author></authors><title>Constructing Runge-Kutta Methods with the Use of Artificial Neural
  Networks</title><categories>cs.NE math.NA</categories><comments>The final publication is available at link.springer.com</comments><msc-class>68T05, 65L06</msc-class><doi>10.1007/2Fs00521-013-1476-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A methodology that can generate the optimal coefficients of a numerical
method with the use of an artificial neural network is presented in this work.
The network can be designed to produce a finite difference algorithm that
solves a specific system of ordinary differential equations numerically. The
case we are examining here concerns an explicit two-stage Runge-Kutta method
for the numerical solution of the two-body problem. Following the
implementation of the network, the latter is trained to obtain the optimal
values for the coefficients of the Runge-Kutta method. The comparison of the
new method to others that are well known in the literature proves its
efficiency and demonstrates the capability of the network to provide efficient
algorithms for specific problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1199</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1199</id><created>2011-06-06</created><authors><author><keyname>Lee</keyname><forenames>Bowon</forenames></author><author><keyname>Goudeseune</keyname><forenames>Camille</forenames></author><author><keyname>Hasegawa-Johnson</keyname><forenames>Mark A.</forenames></author></authors><title>Open-loop multi-channel inversion of room impulse response</title><categories>cs.SD</categories><comments>30 pages, 24 figures</comments><msc-class>76Q05</msc-class><acm-class>H.5.5; G.2.3; I.5.4</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper considers methods for audio display in a CAVE-type virtual reality
theater, a 3 m cube with displays covering all six rigid faces. Headphones are
possible since the user's headgear continuously measures ear positions, but
loudspeakers are preferable since they enhance the sense of total immersion.
The proposed solution consists of open-loop acoustic point control. The
transfer function, a matrix of room frequency responses from the loudspeakers
to the ears of the user, is inverted using multi-channel inversion methods, to
create exactly the desired sound field at the user's ears. The inverse transfer
function is constructed from impulse responses simulated by the image source
method. This technique is validated by measuring a 2x2 matrix transfer
function, simulating a transfer function with the same geometry, and filtering
the measured transfer function through the inverse of the simulation. Since
accuracy of the image source method decreases with time, inversion performance
is improved by windowing the simulated response prior to inversion. Parameters
of the simulation and inversion are adjusted to minimize residual reverberant
energy; the best-case dereverberation ratio is 10 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1207</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1207</id><created>2011-06-06</created><updated>2012-12-17</updated><authors><author><keyname>Jiao</keyname><forenames>Zhuang</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Zhong</keyname><forenames>Yi-Sheng</forenames></author></authors><title>Stability Analysis of Linear Time-Invariant Distributed-Order Systems</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded-input bounded-output stability condition of linear time invariant
(LTI) distributed-order system over integral interval $(0,1)$ has been
established for the first time. Two cases about weighting function of the
distributed order are investigated, and sufficient and necessary conditions of
stability for these two types of distributed-order systems are derived. Based
on the complex integration analysis, time-domain responses of distributed-order
systems are also given by analytical method, and numerical examples are
presented to illustrate the proposed conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1211</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1211</id><created>2011-06-06</created><updated>2012-12-17</updated><authors><author><keyname>Jiao</keyname><forenames>Zhuang</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Zhong</keyname><forenames>Yi-Sheng</forenames></author></authors><title>Stability of fractional-order linear time-invariant system with
  noncommensurate orders</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded-input bounded-output stability conditions for fractional-order linear
time-invariant (LTI) system with multiple noncommensurate orders have been
established in this paper. The orders become noncommensurate orders when they
do not have a common divisor. Sufficient and necessary conditions of stability
for this kind of fractional-order LTI system with multiple noncommensurate
orders. Based on the numerical inverse Laplace transform technique, time-domain
responses for a fractional-order system with double noncommensurate orders are
presented to illustrate the obtained stability results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1216</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1216</id><created>2011-06-06</created><updated>2011-06-14</updated><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Tromer</keyname><forenames>Eran</forenames></author></authors><title>Using More Data to Speed-up Training Time</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In many recent applications, data is plentiful. By now, we have a rather
clear understanding of how more data can be used to improve the accuracy of
learning algorithms. Recently, there has been a growing interest in
understanding how more data can be leveraged to reduce the required training
runtime. In this paper, we study the runtime of learning as a function of the
number of available training examples, and underscore the main high-level
techniques. We provide some initial positive results showing that the runtime
can decrease exponentially while only requiring a polynomial growth of the
number of examples, and spell-out several interesting open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1220</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1220</id><created>2011-06-06</created><updated>2012-12-17</updated><authors><author><keyname>Jiao</keyname><forenames>Zhuang</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author></authors><title>Impulse response of a generalized fractional second order filter</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impulse response of a generalized fractional second order filter of the
form ${{({{s}^{2\alpha}}+a{{s}^{\alpha}}+b)}^{-\gamma}}$ is derived, where
$0&lt;\alpha \le 1$, $0&lt;\gamma &lt;2$. The asymptotic properties of the impulse
responses are obtained for two cases, and the two cases show the similar
properties for the changing of $\gamma$ values. It is shown that only when
${{({{s}^{2\alpha}}+a{{s}^{\alpha}}+b)}^{-1}}$ has the critical stability, the
generalized fractional second order filter
${{({{s}^{2\alpha}}+a{{s}^{\alpha}}+b)}^{-\gamma}}$ has different properties as
we change the value of $\gamma$. Finally, numerical examples to illustrate the
impulse response are provided to verify the proposed concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1224</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1224</id><created>2011-06-06</created><updated>2012-12-17</updated><authors><author><keyname>Jiao</keyname><forenames>Zhuang</forenames></author><author><keyname>Zhong</keyname><forenames>Yisheng</forenames></author></authors><title>Robust stability for fractional-order systems with structured and
  unstructured uncertainties</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issues of robust stability for two types of uncertain fractional-order
systems of order $\alpha \in (0,1)$ are dealt with in this paper. For the
polytope-type uncertainty case, a less conservative sufficient condition of
robust stability is given; for the norm-bounded uncertainty case, a sufficient
and necessary condition of robust stability is presented. Both of these
conditions can be checked by solving sets of linear matrix inequalities. Two
numerical examples are presented to confirm the proposed conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1226</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1226</id><created>2011-06-06</created><updated>2012-12-18</updated><authors><author><keyname>Jiao</keyname><forenames>Zhuang</forenames></author><author><keyname>Zhong</keyname><forenames>Yisheng</forenames></author></authors><title>Sufficient and Necessary Condition of Admissibility for Fractional-order
  Singular System</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  mathematical derivation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn. This paper focuses on the admissibility
condition for fractional-order singular system with order $\alpha \in (0,1)$.
The definitions of regularity, impulse-free and admissibility are given first,
then a sufficient and necessary condition of admissibility for fractional-order
singular system is established. A numerical example is included to illustrate
the proposed condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1228</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1228</id><created>2011-06-06</created><authors><author><keyname>Lustig</keyname><forenames>Yoad</forenames><affiliation>Rice University</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe</forenames><affiliation>Rice University</affiliation></author></authors><title>Synthesis from Recursive-Components Libraries</title><categories>cs.LO cs.SE</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>B.1.2;B.5.2</acm-class><journal-ref>EPTCS 54, 2011, pp. 1-16</journal-ref><doi>10.4204/EPTCS.54.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis is the automatic construction of a system from its specification.
In classical synthesis algorithms it is always assumed that the system is
&quot;constructed from scratch&quot; rather than composed from reusable components. This,
of course, rarely happens in real life. In real life, almost every non-trivial
commercial software system relies heavily on using libraries of reusable
components. Furthermore, other contexts, such as web-service orchestration, can
be modeled as synthesis of a system from a library of components.
  In 2009 we introduced LTL synthesis from libraries of reusable components.
Here, we extend the work and study synthesis from component libraries with
&quot;call and return&quot;' control flow structure. Such control-flow structure is very
common in software systems. We define the problem of Nested-Words Temporal
Logic (NWTL) synthesis from recursive component libraries, where NWTL is a
specification formalism, richer than LTL, that is suitable for &quot;call and
return&quot; computations. We solve the problem, providing a synthesis algorithm,
and show the problem is 2EXPTIME-complete, as standard synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1229</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1229</id><created>2011-06-06</created><authors><author><keyname>Appold</keyname><forenames>Christian</forenames></author></authors><title>Improving BDD Based Symbolic Model Checking with Isomorphism Exploiting
  Transition Relations</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 54, 2011, pp. 17-30</journal-ref><doi>10.4204/EPTCS.54.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic model checking by using BDDs has greatly improved the applicability
of model checking. Nevertheless, BDD based symbolic model checking can still be
very memory and time consuming. One main reason is the complex transition
relation of systems. Sometimes, it is even not possible to generate the
transition relation, due to its exhaustive memory requirements. To diminish
this problem, the use of partitioned transition relations has been proposed.
However, there are still systems which can not be verified at all. Furthermore,
if the granularity of the partitions is too fine, the time required for
verification may increase. In this paper we target the symbolic verification of
asynchronous concurrent systems. For such systems we present an approach which
uses similarities in the transition relation to get further memory reductions
and runtime improvements. By applying our approach, even the verification of
systems with an previously intractable transition relation becomes feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1230</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1230</id><created>2011-06-06</created><authors><author><keyname>Haar</keyname><forenames>Stefan</forenames><affiliation>INRIA and LSV, Ecole Normale Sup&#xe9;rieure de Cachan and CNRS</affiliation></author><author><keyname>Kern</keyname><forenames>Christian</forenames><affiliation>TU M&#xfc;nchen</affiliation></author><author><keyname>Schwoon</keyname><forenames>Stefan</forenames><affiliation>INRIA and LSV, Ecole Normale Sup&#xe9;rieure de Cachan and CNRS</affiliation></author></authors><title>Computing the Reveals Relation in Occurrence Nets</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 31-44</journal-ref><doi>10.4204/EPTCS.54.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Petri net unfoldings are a useful tool to tackle state-space explosion in
verification and related tasks. Moreover, their structure allows to access
directly the relations of causal precedence, concurrency, and conflict between
events. Here, we explore the data structure further, to determine the following
relation: event a is said to reveal event b iff the occurrence of a implies
that b inevitably occurs, too, be it before, after, or concurrently with a.
Knowledge of reveals facilitates in particular the analysis of partially
observable systems, in the context of diagnosis, testing or verification; it
can also be used to generate more concise representations of behaviours via
abstractions. The reveals relation was previously introduced in the context of
fault diagnosis, where it was shown that the reveals relation was decidable:
for a given pair a,b in the unfolding U of a safe Petri net N, a finite prefix
P of U is sufficient to decide whether or not a reveals b. In this paper, we
first considerably improve the bound on |P|. We then show that there exists an
efficient algorithm for computing the relation on a given prefix. We have
implemented the algorithm and report on experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1231</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1231</id><created>2011-06-06</created><authors><author><keyname>Buti</keyname><forenames>Federico</forenames><affiliation>University of Camerino</affiliation></author><author><keyname>De Donato</keyname><forenames>Massimo Callisto</forenames><affiliation>University of Camerino</affiliation></author><author><keyname>Corradini</keyname><forenames>Flavio</forenames><affiliation>University of Camerino</affiliation></author><author><keyname>Di Berardini</keyname><forenames>Maria Rita</forenames><affiliation>University of Camerino</affiliation></author><author><keyname>Vogler</keyname><forenames>Walter</forenames><affiliation>University of Augsburg</affiliation></author></authors><title>Automated Analysis of MUTEX Algorithms with FASE</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>D.2.2; D.2.4</acm-class><journal-ref>EPTCS 54, 2011, pp. 45-59</journal-ref><doi>10.4204/EPTCS.54.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the liveness of several MUTEX solutions by
representing them as processes in PAFAS s, a CCS-like process algebra with a
specific operator for modelling non-blocking reading behaviours. Verification
is carried out using the tool FASE, exploiting a correspondence between
violations of the liveness property and a special kind of cycles (called
catastrophic cycles) in some transition system. We also compare our approach
with others in the literature. The aim of this paper is twofold: on the one
hand, we want to demonstrate the applicability of FASE to some concrete,
meaningful examples; on the other hand, we want to study the impact of
introducing non-blocking behaviours in modelling concurrent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1232</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1232</id><created>2011-06-06</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames><affiliation>Institute of Science and Technology</affiliation></author><author><keyname>Fijalkow</keyname><forenames>Nathana&#xeb;l</forenames><affiliation>Institute of Science and Technology</affiliation></author></authors><title>A reduction from parity games to simple stochastic games</title><categories>cs.GT</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 74-86</journal-ref><doi>10.4204/EPTCS.54.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Games on graphs provide a natural model for reactive non-terminating systems.
In such games, the interaction of two players on an arena results in an
infinite path that describes a run of the system. Different settings are used
to model various open systems in computer science, as for instance turn-based
or concurrent moves, and deterministic or stochastic transitions. In this
paper, we are interested in turn-based games, and specifically in deterministic
parity games and stochastic reachability games (also known as simple stochastic
games). We present a simple, direct and efficient reduction from deterministic
parity games to simple stochastic games: it yields an arena whose size is
linear up to a logarithmic factor in size of the original arena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1233</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1233</id><created>2011-06-06</created><authors><author><keyname>Maubert</keyname><forenames>Bastien</forenames><affiliation>ENS Cachan - Bretagne</affiliation></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames><affiliation>IRISA</affiliation></author><author><keyname>Bozzelli</keyname><forenames>Laura</forenames><affiliation>UPM</affiliation></author></authors><title>Opacity Issues in Games with Imperfect Information</title><categories>cs.GT</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 87-101</journal-ref><doi>10.4204/EPTCS.54.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in depth the class of games with opacity condition, which are
two-player games with imperfect information in which one of the players only
has imperfect information, and where the winning condition relies on the
information he has along the play. Those games are relevant for security
aspects of computing systems: a play is opaque whenever the player who has
imperfect information never &quot;knows&quot; for sure that the current position is one
of the distinguished &quot;secret&quot; positions. We study the problems of deciding the
existence of a winning strategy for each player, and we call them the
opacity-violate problem and the opacity-guarantee problem. Focusing on the
player with perfect information is new in the field of games with
imperfect-information because when considering classical winning conditions it
amounts to solving the underlying perfect-information game. We establish the
EXPTIME-completeness of both above-mentioned problems, showing that our winning
condition brings a gap of complexity for the player with perfect information,
and we exhibit the relevant opacity-verify problem, which noticeably
generalizes approaches considered in the literature for opacity analysis in
discrete-event systems. In the case of blindfold games, this problem relates to
the two initial ones, yielding the determinacy of blindfold games with opacity
condition and the PSPACE-completeness of the three problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1234</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1234</id><created>2011-06-06</created><authors><author><keyname>Tatsuta</keyname><forenames>Makoto</forenames><affiliation>National Institute of Informatics</affiliation></author><author><keyname>Damiani</keyname><forenames>Ferruccio</forenames><affiliation>Universita di Torino</affiliation></author></authors><title>Type Inference for Bimorphic Recursion</title><categories>cs.LO cs.PL</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>F.3.3</acm-class><journal-ref>EPTCS 54, 2011, pp. 102-115</journal-ref><doi>10.4204/EPTCS.54.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes bimorphic recursion, which is restricted polymorphic
recursion such that every recursive call in the body of a function definition
has the same type. Bimorphic recursion allows us to assign two different types
to a recursively defined function: one is for its recursive calls and the other
is for its calls outside its definition. Bimorphic recursion in this paper can
be nested. This paper shows bimorphic recursion has principal types and
decidable type inference. Hence bimorphic recursion gives us flexible typing
for recursion with decidable type inference. This paper also shows that its
typability becomes undecidable because of nesting of recursions when one
removes the instantiation property from the bimorphic recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1235</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1235</id><created>2011-06-06</created><authors><author><keyname>Wu</keyname><forenames>Zhilin</forenames><affiliation>State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences</affiliation></author></authors><title>A Decidable Extension of Data Automata</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 116-130</journal-ref><doi>10.4204/EPTCS.54.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data automata on data words is a decidable model proposed by Boja\'nczyk et
al. in 2006. Class automata, introduced recently by Boja\'nczyk and Lasota, is
an extension of data automata which unifies different automata models on data
words. The nonemptiness of class automata is undecidable, since class automata
can simulate two-counter machines. In this paper, a decidable model called
class automata with priority class condition, which restricts class automata
but strictly extends data automata, is proposed. The decidability of this model
is obtained by establishing a correspondence with priority multicounter
automata. This correspondence also completes the picture of the links between
various class conditions of class automata and various models of counter
machines. Moreover, this model is applied to extend a decidability result of
Alur, Cern\'y and Weinstein on the algorithmic analysis of array-accessing
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1236</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1236</id><created>2011-06-06</created><authors><author><keyname>Gr&#xfc;ner</keyname><forenames>Sten</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Radmacher</keyname><forenames>Frank G.</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Thomas</keyname><forenames>Wolfgang</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Connectivity Games over Dynamic Networks</title><categories>cs.GT cs.CC cs.NI</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 131-145</journal-ref><doi>10.4204/EPTCS.54.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A game-theoretic model for the study of dynamic networks is analyzed. The
model is motivated by communication networks that are subject to failure of
nodes and where the restoration needs resources. The corresponding two-player
game is played between &quot;Destructor&quot; (who can delete nodes) and &quot;Constructor&quot;
(who can restore or even create nodes under certain conditions). We also
include the feature of information flow by allowing Constructor to change
labels of adjacent nodes. As objective for Constructor the network property to
be connected is considered, either as a safety condition or as a reachability
condition (in the latter case starting from a non-connected network). We show
under which conditions the solvability of the corresponding games for
Constructor is decidable, and in this case obtain upper and lower complexity
bounds, as well as algorithms derived from winning strategies. Due to the
asymmetry between the players, safety and reachability objectives are not dual
to each other and are treated separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1237</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1237</id><created>2011-06-06</created><authors><author><keyname>Zimmermann</keyname><forenames>Martin</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Optimal Bounds in Parametric LTL Games</title><categories>cs.GT cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 146-161</journal-ref><doi>10.4204/EPTCS.54.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider graph games of infinite duration with winning conditions in
parameterized linear temporal logic, where the temporal operators are equipped
with variables for time bounds. In model checking such specifications were
introduced as &quot;PLTL&quot; by Alur et al. and (in a different version called
&quot;PROMPT-LTL&quot;) by Kupferman et al..
  We present an algorithm to determine optimal variable valuations that allow a
player to win a game. Furthermore, we show how to determine whether a player
wins a game with respect to some, infinitely many, or all valuations. All our
algorithms run in doubly-exponential time; so, adding bounded temporal
operators does not increase the complexity compared to solving plain LTL games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1239</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1239</id><created>2011-06-06</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames><affiliation>UPM, Madrid, Spain</affiliation></author></authors><title>New results on pushdown module checking with imperfect information</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>F.0</acm-class><journal-ref>EPTCS 54, 2011, pp. 162-177</journal-ref><doi>10.4204/EPTCS.54.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking of open pushdown systems (OPD) w.r.t. standard branching
temporal logics (pushdown module checking or PMC) has been recently
investigated in the literature, both in the context of environments with
perfect and imperfect information about the system (in the last case, the
environment has only a partial view of the system's control states and stack
content). For standard CTL, PMC with imperfect information is known to be
undecidable. If the stack content is assumed to be visible, then the problem is
decidable and 2EXPTIME-complete (matching the complexity of PMC with perfect
information against CTL). The decidability status of PMC with imperfect
information against CTL restricted to the case where the depth of the stack
content is visible is open. In this paper, we show that with this restriction,
PMC with imperfect information against CTL remains undecidable. On the other
hand, we individuate an interesting subclass of OPDS with visible stack content
depth such that PMC with imperfect information against the existential fragment
of CTL is decidable and in 2EXPTIME. Moreover, we show that the program
complexity of PMC with imperfect information and visible stack content against
CTL is 2EXPTIME-complete (hence, exponentially harder than the program
complexity of PMC with perfect information, which is known to be
EXPTIME-complete).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1240</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1240</id><created>2011-06-06</created><authors><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames></author><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames></author></authors><title>Reactive Safety</title><categories>cs.FL cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 178-191</journal-ref><doi>10.4204/EPTCS.54.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distinction between safety and liveness properties is a fundamental
classification with immediate implications on the feasibility and complexity of
various monitoring, model checking, and synthesis problems. In this paper, we
revisit the notion of safety for reactive systems, i.e., for systems whose
behavior is characterized by the interplay of uncontrolled environment inputs
and controlled system outputs. We show that reactive safety is a strictly
larger class of properties than standard safety. We provide algorithms for
checking if a property, given as a temporal formula or as a word or tree
automaton, is a reactive safety property and for translating such properties
into safety automata. Based on this construction, the standard verification and
synthesis algorithms for safety properties immediately extend to the larger
class of reactive safety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1241</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1241</id><created>2011-06-06</created><authors><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>University of Verona</affiliation></author><author><keyname>Montanari</keyname><forenames>Angelo</forenames><affiliation>University of Udine</affiliation></author><author><keyname>Sala</keyname><forenames>Pietro</forenames><affiliation>University of Verona</affiliation></author><author><keyname>Sciavicco</keyname><forenames>Guido</forenames><affiliation>University of Murcia</affiliation></author></authors><title>An Optimal Decision Procedure for MPNL over the Integers</title><categories>cs.LO cs.CC</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 54, 2011, pp. 192-206</journal-ref><doi>10.4204/EPTCS.54.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interval temporal logics provide a natural framework for qualitative and
quantitative temporal reason- ing over interval structures, where the truth of
formulae is defined over intervals rather than points. In this paper, we study
the complexity of the satisfiability problem for Metric Propositional Neigh-
borhood Logic (MPNL). MPNL features two modalities to access intervals &quot;to the
left&quot; and &quot;to the right&quot; of the current one, respectively, plus an infinite set
of length constraints. MPNL, interpreted over the naturals, has been recently
shown to be decidable by a doubly exponential procedure. We improve such a
result by proving that MPNL is actually EXPSPACE-complete (even when length
constraints are encoded in binary), when interpreted over finite structures,
the naturals, and the in- tegers, by developing an EXPSPACE decision procedure
for MPNL over the integers, which can be easily tailored to finite linear
orders and the naturals (EXPSPACE-hardness was already known).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1242</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1242</id><created>2011-06-06</created><authors><author><keyname>Latte</keyname><forenames>Markus</forenames></author></authors><title>Separation of Test-Free Propositional Dynamic Logics over Context-Free
  Languages</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.1.2; F.1.1; F.4.3</acm-class><journal-ref>EPTCS 54, 2011, pp. 207-221</journal-ref><doi>10.4204/EPTCS.54.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a class L of languages let PDL[L] be an extension of Propositional
Dynamic Logic which allows programs to be in a language of L rather than just
to be regular. If L contains a non-regular language, PDL[L] can express
non-regular properties, in contrast to pure PDL.
  For regular, visibly pushdown and deterministic context-free languages, the
separation of the respective PDLs can be proven by automata-theoretic
techniques. However, these techniques introduce non-determinism on the automata
side. As non-determinism is also the difference between DCFL and CFL, these
techniques seem to be inappropriate to separate PDL[DCFL] from PDL[CFL].
Nevertheless, this separation is shown but for programs without test operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1243</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1243</id><created>2011-06-06</created><authors><author><keyname>Lenzi</keyname><forenames>Giacomo</forenames><affiliation>University of Salerno</affiliation></author></authors><title>On P-transitive graphs and applications</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 222-236</journal-ref><doi>10.4204/EPTCS.54.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of graphs which we call P-transitive graphs, lying
between transitive and 3-transitive graphs. First we show that the analogue of
de Jongh-Sambin Theorem is false for wellfounded P-transitive graphs; then we
show that the mu-calculus fixpoint hierarchy is infinite for P-transitive
graphs. Both results contrast with the case of transitive graphs. We give also
an undecidability result for an enriched mu-calculus on P-transitive graphs.
Finally, we consider a polynomial time reduction from the model checking
problem on arbitrary graphs to the model checking problem on P-transitive
graphs. All these results carry over to 3-transitive graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1244</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1244</id><created>2011-06-06</created><authors><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>University of Verona</affiliation></author><author><keyname>Capiluppi</keyname><forenames>Marta</forenames><affiliation>University of Verona</affiliation></author></authors><title>A Game-Theoretic approach to Fault Diagnosis of Hybrid Systems</title><categories>cs.FL cs.GT</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 237-249</journal-ref><doi>10.4204/EPTCS.54.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical systems can fail. For this reason the problem of identifying and
reacting to faults has received a large attention in the control and computer
science communities. In this paper we study the fault diagnosis problem for
hybrid systems from a game-theoretical point of view. A hybrid system is a
system mixing continuous and discrete behaviours that cannot be faithfully
modeled neither by using a formalism with continuous dynamics only nor by a
formalism including only discrete dynamics. We use the well known framework of
hybrid automata for modeling hybrid systems, and we define a Fault Diagnosis
Game on them, using two players: the environment and the diagnoser. The
environment controls the evolution of the system and chooses whether and when a
fault occurs. The diagnoser observes the external behaviour of the system and
announces whether a fault has occurred or not. Existence of a winning strategy
for the diagnoser implies that faults can be detected correctly, while
computing such a winning strategy corresponds to implement a diagnoser for the
system. We will show how to determine the existence of a winning strategy, and
how to compute it, for some decidable classes of hybrid automata like o-minimal
hybrid automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1245</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1245</id><created>2011-06-06</created><authors><author><keyname>Tveretina</keyname><forenames>Olga</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Funke</keyname><forenames>Daniel</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Deciding Reachability for 3-Dimensional Multi-Linear Systems</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 250-262</journal-ref><doi>10.4204/EPTCS.54.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of point-to-point reachability in
multi-linear systems. These systems consist of a partition of the Euclidean
space into a finite number of regions and a constant derivative assigned to
each region in the partition, which governs the dynamical behavior of the
system within it. The reachability problem for multi-linear systems has been
proven to be decidable for the two-dimensional case and undecidable for the
dimension three and higher.
  Multi-linear systems however exhibit certain properties that make them very
suitable for topological analysis. We prove that reachability can be decided
exactly in the 3-dimensional case when systems satisfy certain conditions. We
show with experiments that our approach can be orders of magnitude more
efficient than simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1246</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1246</id><created>2011-06-06</created><authors><author><keyname>Benerecetti</keyname><forenames>Massimo</forenames><affiliation>Universit&#xe0; di Napoli &quot;Federico II&quot;, Italy</affiliation></author><author><keyname>Faella</keyname><forenames>Marco</forenames><affiliation>Universit&#xe0; di Napoli &quot;Federico II&quot;, Italy</affiliation></author><author><keyname>Minopoli</keyname><forenames>Stefano</forenames><affiliation>Universit&#xe0; di Napoli &quot;Federico II&quot;, Italy</affiliation></author></authors><title>Towards Efficient Exact Synthesis for Linear Hybrid Systems</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2011, arXiv:1106.0814</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 54, 2011, pp. 263-277</journal-ref><doi>10.4204/EPTCS.54.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of automatically computing the controllable region of a
Linear Hybrid Automaton, with respect to a safety objective. We describe the
techniques that are needed to effectively and efficiently implement a
recently-proposed solution procedure, based on polyhedral abstractions of the
state space. Supporting experimental results are presented, based on an
implementation of the proposed techniques on top of the tool PHAVer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1250</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1250</id><created>2011-06-06</created><authors><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Huang</keyname><forenames>Cheng</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author><author><keyname>Li</keyname><forenames>Jin</forenames></author></authors><title>Optimal Repair of MDS Codes in Distributed Storage via Subspace
  Interference Alignment</title><categories>cs.IT math.IT</categories><comments>To be presented in part at ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that an (n,k) code can be used to store 'k' units of
information in 'n' unit-capacity disks of a distributed data storage system. If
the code used is maximum distance separable (MDS), then the system can tolerate
any (n-k) disk failures, since the original information can be recovered from
any k surviving disks. The focus of this paper is the design of a systematic
MDS code with the additional property that a single disk failure can be
repaired with minimum repair bandwidth, i.e., with the minimum possible amount
of data to be downloaded for recovery of the failed disk. Previously, a lower
bound of (n-1)/(n-k) units has been established by Dimakis et. al, on the
repair bandwidth for a single disk failure in an (n,k) MDS code . Recently, the
existence of asymptotic codes achieving this lower bound for arbitrary (n,k)
has been established by drawing connections to interference alignment. While
the existence of asymptotic constructions achieving this lower bound have been
shown, finite code constructions achieving this lower bound existed in previous
literature only for the special (high-redundancy) scenario where $k \leq
\max(n/2,3)$. The question of existence of finite codes for arbitrary values of
(n,k) achieving the lower bound on the repair bandwidth remained open. In this
paper, by using permutation coding sub-matrices, we provide the first known
finite MDS code which achieves the optimal repair bandwidth of (n-1)/(n-k) for
arbitrary (n,k), for recovery of a failed systematic disk. We also generalize
our permutation matrix based constructions by developing a novel framework for
repair-bandwidth-optimal MDS codes based on the idea of subspace interference
alignment - a concept previously introduced by Suh and Tse the context of
wireless cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1257</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1257</id><created>2011-06-07</created><authors><author><keyname>Zhuang</keyname><forenames>Yanyan</forenames></author><author><keyname>Pan</keyname><forenames>Jianping</forenames></author></authors><title>Random Distances Associated with Rhombuses</title><categories>math.GM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallelograms are one of the basic building blocks in two-dimensional
tiling. They have important applications in a wide variety of science and
engineering fields, such as wireless communication networks, urban
transportation, operations research, etc. Different from rectangles and
squares, the coordinates of a random point in parallelograms are no longer
independent. As a case study of parallelograms, the explicit probability
density functions of the random Euclidean distances associated with rhombuses
are given in this report, when both endpoints are randomly distributed in 1)
the same rhombus, 2) two parallel rhombuses sharing a side, and 3) two
rhombuses having a common diagonal, respectively. The accuracy of the distance
distribution functions is verified by simulation, and the correctness is
validated by a recursion and a probabilistic sum. The first two statistical
moments of the random distances, and the polynomial fit of the density
functions are also given in this report for practical uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1282</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1282</id><created>2011-06-07</created><authors><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author><author><keyname>Ng</keyname><forenames>Wai Yin</forenames></author></authors><title>Exploring Network Economics</title><categories>cs.NI</categories><comments>It is a position paper, about what we might teach in a Network
  Economics course, and the type of research we found useful. Therefore, it is
  not an extensive survey paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore what \emph{network economics} is all about,
focusing on the interesting topics brought about by the Internet. Our intent is
make this a brief survey, useful as an outline for a course on this topic, with
an extended list of references. We try to make it as intuitive and readable as
possible. We also deliberately try to be critical at times, and hope our
interpretation of the topic will lead to interests for further discussions by
those doing research in the same field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1286</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1286</id><created>2011-06-07</created><authors><author><keyname>Rajeswari</keyname><forenames>S.</forenames></author><author><keyname>Venkataramani</keyname><forenames>Y.</forenames></author></authors><title>Traffic Performance Analysis of Manet Routing Protocol</title><categories>cs.DC</categories><comments>12 pages,12 figures</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.2, No.3, May 2011</journal-ref><doi>10.5121/ijdps.2011.2306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary objective of this research work is to study and investigate the
performance measures of Gossip Routing protocol and Energy Efficient and
Reliable Adaptive Gossip routing protocols. We use TCP and CBR based traffic
models to analyze the performance of above mentioned protocols based on the
parameters of Packet Delivery Ratio, Average End-to-End Delay and Throughput.
We will investigate the effect of change in the simulation time and Number of
nodes for the MANET routing protocols. For Simulation, we have used ns-2
simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1287</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1287</id><created>2011-06-07</created><authors><author><keyname>Arunmozhi</keyname><forenames>S. A.</forenames></author><author><keyname>Venkataramani</keyname><forenames>Y.</forenames></author></authors><title>DDoS Attack and Defense Scheme in Wireless Ad hoc Networks</title><categories>cs.CR cs.DC</categories><comments>6 pages, 5 figures</comments><journal-ref>Arunmozhi S.A., Venkataramani Y.,DDoS Attack and Defense Scheme in
  Wireless Ad hoc Networks,International Journal of Network Security &amp; Its
  Applications (IJNSA), Vol.3, No.3, May 2011</journal-ref><doi>10.5121/ijnsa.2011.3312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless ad hoc networks are highly vulnerable to distributed denial of
service(DDoS) attacks because of its unique characteristics such as open
network architecture, shared wireless medium and stringent resource
constraints. These attacks throttle the tcp throughput heavily and reduce the
quality of service(QoS) to end systems gradually rather than refusing the
clients from the services completely. In this paper, we discussed the DDoS
attacks and proposed a defense scheme to improve the performance of the ad hoc
networks. Our proposed defense mechanism uses the medium access control (MAC)
layer information to detect the attackers. The status values from MAC layer
that can be used for detection are Frequency of receiving RTS/CTS packets,
Frequency of sensing a busy channel and the number of RTS/DATA retransmissions.
Once the attackers are identified, all the packets from those nodes will be
blocked. The network resources are made available to the legitimate users. We
perform the simulation with Network Simulator NS2 and we proved that our
proposed system improves the network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1311</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1311</id><created>2011-06-07</created><updated>2011-06-30</updated><authors><author><keyname>Nguyen</keyname><forenames>Phuong-Lan</forenames></author><author><keyname>Demoen</keyname><forenames>Bart</forenames></author></authors><title>Representation Sharing for Prolog</title><categories>cs.PL</categories><comments>37 pages, 11 figures, 3 tables; To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>D.3.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation sharing can reduce the memory footprint of a program by
sharing one representation between duplicate terms. The most common
implementation of representation sharing in functional programming systems is
known as hash-consing. In the context of Prolog, representation sharing has
been given little attention. Some current techniques that deal with
representation sharing are reviewed. The new contributions are: (1) an easy
implementation of {\em input sharing} for {\em findall/3}; (2) a description of
a {\em sharer} module that introduces representation sharing at runtime. Their
realization is shown in the context of the WAM as implemented by hProlog. Both
can be adapted to any WAM-like Prolog implementation. The sharer works
independently of the garbage collector, but it can be made to cooperate with
the garbage collector. Benchmark results show that the sharer has a cost
comparable to the heap garbage collector, that its effectiveness is highly
application dependent, and that its policy must be tuned to the collector. To
appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1319</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1319</id><created>2011-06-07</created><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Shahram</keyname><forenames>Morteza</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaosheng</forenames></author></authors><title>ShearLab: A Rational Design of a Digital Parabolic Scaling Algorithm</title><categories>math.NA cs.MS cs.NA</categories><comments>submitted to SIAM J. Multiscale Model. Simul</comments><msc-class>42C40, 41A05, 41A15, 42B05, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate problems are typically governed by anisotropic features such as
edges in images. A common bracket of most of the various directional
representation systems which have been proposed to deliver sparse
approximations of such features is the utilization of parabolic scaling. One
prominent example is the shearlet system. Our objective in this paper is
three-fold: We firstly develop a digital shearlet theory which is rationally
designed in the sense that it is the digitization of the existing shearlet
theory for continuous data. This implicates that shearlet theory provides a
unified treatment of both the continuum and digital realm. Secondly, we analyze
the utilization of pseudo-polar grids and the pseudo-polar Fourier transform
for digital implementations of parabolic scaling algorithms. We derive an
isometric pseudo-polar Fourier transform by careful weighting of the
pseudo-polar grid, allowing exploitation of its adjoint for the inverse
transform. This leads to a digital implementation of the shearlet transform; an
accompanying Matlab toolbox called ShearLab is provided. And, thirdly, we
introduce various quantitative measures for digital parabolic scaling
algorithms in general, allowing one to tune parameters and objectively improve
the implementation as well as compare different directional transform
implementations. The usefulness of such measures is exemplarily demonstrated
for the digital shearlet transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1325</identifier>
 <datestamp>2011-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1325</id><created>2011-06-07</created><updated>2011-08-05</updated><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Lemvig</keyname><forenames>Jakob</forenames></author><author><keyname>Lim</keyname><forenames>Wang-Q</forenames></author></authors><title>Shearlets and Optimally Sparse Approximations</title><categories>math.FA cs.IT cs.NA math.IT</categories><comments>in &quot;Shearlets: Multiscale Analysis for Multivariate Data&quot;,
  Birkh\&quot;auser-Springer</comments><msc-class>42C40, 42C15, 41A30, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate functions are typically governed by anisotropic features such as
edges in images or shock fronts in solutions of transport-dominated equations.
One major goal both for the purpose of compression as well as for an efficient
analysis is the provision of optimally sparse approximations of such functions.
Recently, cartoon-like images were introduced in 2D and 3D as a suitable model
class, and approximation properties were measured by considering the decay rate
of the $L^2$ error of the best $N$-term approximation. Shearlet systems are to
date the only representation system, which provide optimally sparse
approximations of this model class in 2D as well as 3D. Even more, in contrast
to all other directional representation systems, a theory for compactly
supported shearlet frames was derived which moreover also satisfy this
optimality benchmark. This chapter shall serve as an introduction to and a
survey about sparse approximations of cartoon-like images by band-limited and
also compactly supported shearlet frames as well as a reference for the
state-of-the-art of this research field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1347</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1347</id><created>2011-06-03</created><authors><author><keyname>Hedtke</keyname><forenames>Ivo</forenames></author></authors><title>Methods of Matrix Multiplication: An Overview of Several Methods and
  their Implementation</title><categories>cs.MS</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this overview article we present several methods for multiplying matrices
and the implementation of these methods in C. Also a little test program is
given to compare their running time and the numerical stability.
  The methods are: naive method, naive method working on arrays, naive method
with the \textsc{Kahan} trick, three methods with loop unrolling, winograd
method and the scaled variant, original \textsc{Strassen} method and the
\textsc{Strassen}-\textsc{Winograd} variant.
  Please note, that this is the FIRST version. The algorithms are not well
tested and the implementation is not optimized. If you like to join the
project, please contact me.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1351</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1351</id><created>2011-06-07</created><authors><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Kun-Yu</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Worst-Case SINR Constrained Robust Coordinated Beamforming for Multicell
  Wireless Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicell coordinated beamforming (MCBF) has been recognized as a promising
approach to enhancing the system throughput and spectrum efficiency of wireless
cellular systems. In contrast to the conventional single-cell beamforming (SBF)
design, MCBF jointly optimizes the beamforming vectors of cooperative base
stations (BSs) (via a central processing unit(CPU)) in order to mitigate the
intercell interference. While most of the existing designs assume that the CPU
has the perfect knowledge of the channel state information (CSI) of mobile
stations (MSs), this paper takes into account the inevitable CSI errors at the
CPU, and study the robust MCBF design problem. Specifically, we consider the
worst-case robust design formulation that minimizes the weighted sum
transmission power of BSs subject to worst-case
signal-to-interference-plus-noise ratio (SINR) constraints on MSs. The
associated optimization problem is challenging because it involves infinitely
many nonconvex SINR constraints. In this paper, we show that the worst-case
SINR constraints can be reformulated as linear matrix inequalities, and the
approximation method known as semidefinite relation can be used to efficiently
handle the worst-case robust MCBF problem. Simulation results show that the
proposed robustMCBF design can provide guaranteed SINR performance for the MSs
and outperforms the robust SBF design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1356</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1356</id><created>2011-06-04</created><updated>2013-03-27</updated><authors><author><keyname>Kogan</keyname><forenames>Simon</forenames></author><author><keyname>Frenkel</keyname><forenames>Zakharia</forenames></author><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Volkovich</keyname><forenames>Zeev</forenames></author></authors><title>Hierarchy of protein loop-lock structures: a new server for the
  decomposition of a protein structure into a set of closed loops</title><categories>physics.chem-ph cs.CE q-bio.QM</categories><comments>11 pages, 4 figures</comments><journal-ref>Computational Molecular Bioscience, Vol.3, No.1, March 2013</journal-ref><doi>10.4236/cmb.2013.31001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HoPLLS (Hierarchy of protein loop-lock structures)
(http://leah.haifa.ac.il/~skogan/Apache/mydata1/main.html) is a web server that
identifies closed loops - a structural basis for protein domain hierarchy. The
server is based on the loop-and-lock theory for structural organisation of
natural proteins. We describe this web server, the algorithms for the
decomposition of a 3D protein into loops and the results of scientific
investigations into a structural &quot;alphabet&quot; of loops and locks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1364</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1364</id><created>2011-06-07</created><updated>2011-06-16</updated><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Gaiser</keyname><forenames>Andreas</forenames></author></authors><title>Probabilistic Abstractions with Arbitrary Domains</title><categories>cs.LO cs.PL</categories><comments>This is a technical report that goes along with an article to appear
  in the Proceedings of the 18th International Static Analysis Symposium (SAS),
  2011</comments><acm-class>D.2.4; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work by Hermanns et al. and Kattenbelt et al. has extended
counterexample-guided abstraction refinement (CEGAR) to probabilistic programs.
These approaches are limited to predicate abstraction. We present a novel
technique, based on the abstract reachability tree recently introduced by
Gulavani et al., that can use arbitrary abstract domains and widening operators
(in the sense of abstract interpretation). We show how suitable widening
operators can deduce loop invariants diffcult to find for predicate
abstraction, and propose refinement techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1370</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1370</id><created>2011-06-07</created><authors><author><keyname>Chen</keyname><forenames>Jingchao</forenames></author></authors><title>Exploiting Dynamically Propositional Logic Structures in SAT</title><categories>cs.LO cs.DM</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The 32-bit hwb (hwb-n32 for short) problem is from equivalence checking that
arises in combining two circuits computing the hidden weighted bit function.
Since 2002, it remains still unsolvable in every SAT competition. This paper
focuses on solving problems such as hwb-n32. Generally speaking, modern solvers
can detect only XOR, AND, OR and ITE gates. Other non-clausal formulas
(propositional logic structures) cannot be detected. To solve the hwb-n32
problem, we extract dynamically some special propositional logic structures,
and then use a variant of DPLL-based solvers to solve the subproblem simplified
by the extracted structure information. Using the dynamic extraction technique,
we solved efficiently the hwb-n32 problem, even some of which were solved
within 3000 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1372</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1372</id><created>2011-06-07</created><authors><author><keyname>Chen</keyname><forenames>Jingchao</forenames></author></authors><title>Phase Selection Heuristics for Satisfiability Solvers</title><categories>cs.LO cs.DM</categories><comments>12 pages 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In general, a SAT Solver based on conflict-driven DPLL consists of variable
selection, phase selection, Boolean Constraint Propagation, conflict analysis,
clause learning and its database maintenance. Optimizing any part of these
components can enhance the performance of a solver. This paper focuses on
optimizing phase selection. Although the ACE (Approximation of the Combined
lookahead Evaluation) weight is applied to a lookahead SAT solver such as
March, so far, no conflict-driven SAT solver applies successfully the ACE
weight, since computing the ACE weight is time-consuming. Here we apply the ACE
weight to partial phase selection of conflict-driven SAT solvers. This can be
seen as an improvement of the heuristic proposed by Jeroslow-Wang (1990). We
incorporate the ACE heuristic and the existing phase selection heuristics in
the new solver MPhaseSAT, and select a phase heuristic in a way similar to
portfolio methods. Experimental results show that adding the ACE heuristic can
improve the conflict-driven solvers. Particularly on application instances,
MPhaseSAT with the ACE heuristic is significantly better than MPhaseSAT without
the ACE heuristic, and even can solve a few SAT instances that remain
unsolvable so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1379</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1379</id><created>2011-06-07</created><updated>2014-10-29</updated><authors><author><keyname>Feldman</keyname><forenames>Dan</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>A Unified Framework for Approximating and Clustering Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $F$ of $n$ positive functions over a ground set $X$, we consider
the problem of computing $x^*$ that minimizes the expression $\sum_{f\in
F}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where we
wish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from
a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$
corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$,
and we seek a shape $x$ that minimizes the sum of distances from each point in
$P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and
$f(x)$ is the distance from $p$ to its closest shape in $x$.
  Our main result is a unified framework for constructing {\em coresets} and
{\em approximate clustering} for such general sets of functions. To achieve our
results, we forge a link between the classic and well defined notion of
$\varepsilon$-approximations from the theory of PAC Learning and VC dimension,
to the relatively new (and not so consistent) paradigm of coresets, which are
some kind of &quot;compressed representation&quot; of the input set $F$. Using
traditional techniques, a coreset usually implies an LTAS (linear time
approximation scheme) for the corresponding optimization problem, which can be
computed in parallel, via one pass over the data, and using only
polylogarithmic space (i.e, in the streaming model).
  We show how to generalize the results of our framework for squared distances
(as in $k$-mean), distances to the $q$th power, and deterministic
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1401</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1401</id><created>2011-06-07</created><authors><author><keyname>Roozbehani</keyname><forenames>Mardavij</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy K</forenames></author></authors><title>Volatility of Power Grids under Real-Time Pricing</title><categories>cs.SY math.DS math.OC q-fin.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a framework for modeling and analysis of the dynamics of
supply, demand, and clearing prices in power system with real-time retail
pricing and information asymmetry. Real-time retail pricing is characterized by
passing on the real-time wholesale electricity prices to the end consumers, and
is shown to create a closed-loop feedback system between the physical layer and
the market layer of the power system. In the absence of a carefully designed
control law, such direct feedback between the two layers could increase
volatility and lower the system's robustness to uncertainty in demand and
generation. A new notion of generalized price-elasticity is introduced, and it
is shown that price volatility can be characterized in terms of the system's
maximal relative price elasticity, defined as the maximal ratio of the
generalized price-elasticity of consumers to that of the producers. As this
ratio increases, the system becomes more volatile, and eventually, unstable. As
new demand response technologies and distributed storage increase the
price-elasticity of demand, the architecture under examination is likely to
lead to increased volatility and possibly instability. This highlights the need
for assessing architecture systematically and in advance, in order to optimally
strike the trade-offs between volatility, economic efficiency, and system
reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1412</identifier>
 <datestamp>2011-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1412</id><created>2011-06-07</created><authors><author><keyname>Burq</keyname><forenames>Nicolas</forenames></author><author><keyname>Zworski</keyname><forenames>Maciej</forenames></author></authors><title>Control for Schroedinger operators on tori</title><categories>math.AP cs.SY math-ph math.MP math.OC</categories><comments>2 figures</comments><msc-class>35Q41, 35Q93, 49J20, 81Q93</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well known result of Jaffard states that an arbitrary region on a torus
controls, in the L2 sense, solutions of the free stationary and dynamical
Schroedinger equations. In this note we show that the same result is valid in
the presence of a smooth time-independent potential. The methods apply to
continuous potentials as well and we conjecture that the L2 control is valid
for any bounded time dependent potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1414</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1414</id><created>2011-06-07</created><authors><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Pusane</keyname><forenames>Ali E.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Exact Free Distance and Trapping Set Growth Rates for LDPC Convolutional
  Codes</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2011 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensembles of (J,K)-regular low-density parity-check convolutional (LDPCC)
codes are known to be asymptotically good, in the sense that the minimum free
distance grows linearly with the constraint length. In this paper, we use a
protograph-based analysis of terminated LDPCC codes to obtain an upper bound on
the free distance growth rate of ensembles of periodically time-varying LDPCC
codes. This bound is compared to a lower bound and evaluated numerically. It is
found that, for a sufficiently large period, the bounds coincide. This approach
is then extended to obtain bounds on the trapping set numbers, which define the
size of the smallest, non-empty trapping sets, for these asymptotically good,
periodically time-varying LDPCC code ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1424</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1424</id><created>2011-06-07</created><updated>2011-09-12</updated><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>&#x158;eh&#xe1;k</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Fixed-delay Events in Generalized Semi-Markov Processes Revisited</title><categories>cs.SY cs.PF math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study long run average behavior of generalized semi-Markov processes with
both fixed-delay events as well as variable-delay events. We show that allowing
two fixed-delay events and one variable-delay event may cause an unstable
behavior of a GSMP. In particular, we show that a frequency of a given state
may not be defined for almost all runs (or more generally, an invariant measure
may not exist). We use this observation to disprove several results from
literature. Next we study GSMP with at most one fixed-delay event combined with
an arbitrary number of variable-delay events. We prove that such a GSMP always
possesses an invariant measure which means that the frequencies of states are
always well defined and we provide algorithms for approximation of these
frequencies. Additionally, we show that the positive results remain valid even
if we allow an arbitrary number of reasonably restricted fixed-delay events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1445</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1445</id><created>2011-06-07</created><updated>2015-12-02</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>From Classical to Quantum Shannon Theory</title><categories>quant-ph cs.IT math.IT</categories><comments>v6: 768 pages, 301 exercises, 81 figures, available under a Creative
  Commons Attribution-NonCommercial-ShareAlike license (see
  http://creativecommons.org/licenses/by-nc-sa/3.0/), preprint for a second
  edition of the book</comments><doi>10.1017/CBO9781139525343</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The aim of this book is to develop &quot;from the ground up&quot; many of the major,
exciting, pre- and post-millenium developments in the general area of study
known as quantum Shannon theory. As such, we spend a significant amount of time
on quantum mechanics for quantum information theory (Part II), we give a
careful study of the important unit protocols of teleportation, super-dense
coding, and entanglement distribution (Part III), and we develop many of the
tools necessary for understanding information transmission or compression (Part
IV). Parts V and VI are the culmination of this book, where all of the tools
developed come into play for understanding many of the important results in
quantum Shannon theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1465</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1465</id><created>2011-06-07</created><updated>2012-01-04</updated><authors><author><keyname>Ayyer</keyname><forenames>Arvind</forenames></author></authors><title>Determinants and Perfect Matchings</title><categories>math.CO cs.DM math.RA math.RT</categories><comments>15 pages, terminology improved, exposition tightened, &quot;deranged
  matchings&quot; example removed</comments><msc-class>05B20, 05C10, 05C70, 15B57, 15A15</msc-class><journal-ref>Journal of Combinatorial Theory A 120 (2013) 304-314</journal-ref><doi>10.1016/j.jcta.2012.08.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a combinatorial interpretation of the determinant of a matrix as a
generating function over Brauer diagrams in two different but related ways. The
sign of a permutation associated to its number of inversions in the Leibniz
formula for the determinant is replaced by the number of crossings in the
Brauer diagram. This interpretation naturally explains why the determinant of
an even antisymmetric matrix is the square of a Pfaffian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1474</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1474</id><created>2011-06-07</created><updated>2012-02-28</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Simple Bounds for Recovering Low-complexity Models</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents a unified analysis of the recovery of simple objects from
random linear measurements. When the linear functionals are Gaussian, we show
that an s-sparse vector in R^n can be efficiently recovered from 2s log n
measurements with high probability and a rank r, n by n matrix can be
efficiently recovered from r(6n-5r) with high probability. For sparse vectors,
this is within an additive factor of the best known nonasymptotic bounds. For
low-rank matrices, this matches the best known bounds. We present a parallel
analysis for block sparse vectors obtaining similarly tight bounds. In the case
of sparse and block sparse signals, we additionally demonstrate that our bounds
are only slightly weakened when the measurement map is a random sign matrix.
Our results are based on analyzing a particular dual point which certifies
optimality conditions of the respective convex programming problem. Our
calculations rely only on standard large deviation inequalities and our
analysis is self-contained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1478</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1478</id><created>2011-06-07</created><authors><author><keyname>Rodr&#xed;guez</keyname><forenames>M. Andrea</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Caniupan</keyname><forenames>Monica</forenames></author></authors><title>Consistent Query Answering under Spatial Semantic Constraints</title><categories>cs.DB</categories><comments>Journal submission, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistent query answering is an inconsistency tolerant approach to obtaining
semantically correct answers from a database that may be inconsistent with
respect to its integrity constraints. In this work we formalize the notion of
consistent query answer for spatial databases and spatial semantic integrity
constraints. In order to do this, we first characterize conflicting spatial
data, and next, we define admissible instances that restore consistency while
staying close to the original instance. In this way we obtain a repair
semantics, which is used as an instrumental concept to define and possibly
derive consistent query answers. We then concentrate on a class of spatial
denial constraints and spatial queries for which there exists an efficient
strategy to compute consistent query answers. This study applies inconsistency
tolerance in spatial databases, rising research issues that shift the goal from
the consistency of a spatial database to the consistency of query answering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1500</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1500</id><created>2011-06-08</created><updated>2011-06-09</updated><authors><author><keyname>Loiseau</keyname><forenames>Mathieu</forenames><affiliation>LIDILEM, LSE</affiliation></author><author><keyname>Potolia</keyname><forenames>Anthippi</forenames><affiliation>PLIDAM</affiliation></author><author><keyname>Zourou</keyname><forenames>Katerina</forenames><affiliation>LIDILEM, LCMI</affiliation></author></authors><title>Communaut\'es Web 2.0 d'apprenants de langue avec parcours
  d'apprentissage : r\^oles, p\'edagogie et rapports au contenu</title><categories>cs.CY</categories><proxy>ccsd</proxy><journal-ref>EIAH 2011, Mons : Belgique (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the various types of online language learner collective types, we
analyze in this contribution Web 2.0 communities featuring an explicit
progression. We use three analysis angles (user roles, pedagogical progression
and content) in order to provides leads towards expressing the interrelations
between the implementation choices regarding the concepts linked to Web 2.0 and
the learning experience of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1510</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1510</id><created>2011-06-08</created><authors><author><keyname>Shkotin</keyname><forenames>Alex</forenames></author><author><keyname>Ryakhovsky</keyname><forenames>Vladimir</forenames></author><author><keyname>Kudryavtsev</keyname><forenames>Dmitry</forenames></author></authors><title>Towards OWL-based Knowledge Representation in Petrology</title><categories>cs.AI</categories><comments>10 pages. The paper has been accepted by OWLED2011 as a long
  presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our work on development of OWL-driven systems for formal
representation and reasoning about terminological knowledge and facts in
petrology. The long-term aim of our project is to provide solid foundations for
a large-scale integration of various kinds of knowledge, including basic terms,
rock classification algorithms, findings and reports. We describe three steps
we have taken towards that goal here. First, we develop a semi-automated
procedure for transforming a database of igneous rock samples to texts in a
controlled natural language (CNL), and then a collection of OWL ontologies.
Second, we create an OWL ontology of important petrology terms currently
described in natural language thesauri. We describe a prototype of a tool for
collecting definitions from domain experts. Third, we present an approach to
formalization of current industrial standards for classification of rock
samples, which requires linear equations in OWL 2. In conclusion, we discuss a
range of opportunities arising from the use of semantic technologies in
petrology and outline the future work in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1516</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1516</id><created>2011-06-08</created><authors><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Miorandi</keyname><forenames>Daniele</forenames></author><author><keyname>Chlamtac</keyname><forenames>Imrich</forenames></author></authors><title>Distributed Wake-Up Scheduling for Energy Saving in Wireless Networks</title><categories>cs.NI cs.DC</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A customary solution to reduce the energy consumption of wireless
communication devices is to periodically put the radio into low-power sleep
mode. A relevant problem is to schedule the wake-up of nodes in such a way as
to ensure proper coordination among devices, respecting delay constraints while
still saving energy. In this paper, we introduce a simple algebraic
characterization of the problem of periodic wake-up scheduling under both
energy consumption and delay constraints. We demonstrate that the general
problem of wake-up times coordination is equivalent to integer factorization
and discuss the implications on the design of efficient scheduling algorithms.
We then propose simple polynomial time heuristic algorithms that can be
implemented in a distributed fashion and present a message complexity of the
order of the number of links in the network. Numerical results are provided in
order to assess the performance of the proposed techniques when applied to
wireless sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1521</identifier>
 <datestamp>2011-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1521</id><created>2011-06-08</created><updated>2011-08-09</updated><authors><author><keyname>Jang</keyname><forenames>Min-Hee</forenames></author><author><keyname>Kim</keyname><forenames>Sang-Wook</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author><author><keyname>Park</keyname><forenames>Sunju</forenames></author></authors><title>A Linear-Time Approximation of the Earth Mover's Distance</title><categories>cs.IR</categories><comments>This paper has been withdrawn by the author due to some mistakes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color descriptors are one of the important features used in content-based
image retrieval. The Dominant Color Descriptor (DCD) represents a few
perceptually dominant colors in an image through color quantization. For image
retrieval based on DCD, the earth mover's distance and the optimal color
composition distance are proposed to measure the dissimilarity between two
images. Although providing good retrieval results, both methods are too
time-consuming to be used in a large image database. To solve the problem, we
propose a new distance function that calculates an approximate earth mover's
distance in linear time. To calculate the dissimilarity in linear time, the
proposed approach employs the space-filling curve for multidimensional color
space. To improve the accuracy, the proposed approach uses multiple curves and
adjusts the color positions. As a result, our approach achieves
order-of-magnitude time improvement but incurs small errors. We have performed
extensive experiments to show the effectiveness and efficiency of the proposed
approach. The results reveal that our approach achieves almost the same results
with the EMD in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1523</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1523</id><created>2011-06-08</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Schaible</keyname><forenames>Johann</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>A Novel Combined Term Suggestion Service for Domain-Specific Digital
  Libraries</title><categories>cs.DL cs.IR</categories><comments>To be published in Proceedings of Theories and Practice in Digital
  Libraries (TPDL), 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive query expansion can assist users during their query formulation
process. We conducted a user study with over 4,000 unique visitors and four
different design approaches for a search term suggestion service. As a basis
for our evaluation we have implemented services which use three different
vocabularies: (1) user search terms, (2) terms from a terminology service and
(3) thesaurus terms. Additionally, we have created a new combined service which
utilizes thesaurus term and terms from a domain-specific search term
re-commender. Our results show that the thesaurus-based method clearly is used
more often compared to the other single-method implementations. We interpret
this as a strong indicator that term suggestion mechanisms should be
domain-specific to be close to the user terminology. Our novel combined
approach which interconnects a thesaurus service with additional statistical
relations out-performed all other implementations. All our observations show
that domain-specific vocabulary can support the user in finding alternative
concepts and formulating queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1531</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1531</id><created>2011-06-08</created><updated>2011-06-09</updated><authors><author><keyname>Nystr&#xf6;m-Persson</keyname><forenames>Johan</forenames></author><author><keyname>Honiden</keyname><forenames>Shinichi</forenames></author></authors><title>Poplar: A Java Extension for Evolvable Component Integration</title><categories>cs.PL cs.SE</categories><comments>17 pages. Corrected typos</comments><acm-class>D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Java programming language contains many features that aid component-based
software development (CBSD), such as interfaces, visibility levels, and strong
support for encapsulation. However, component evolution often causes so-called
breaking changes, largely because of the rigidity of component interconnections
in the form of explicit method calls and field accesses. We present a Java
extension, Poplar, which we are currently developing. In Poplar,
inter-component dependencies are expressed using declarative queries; concrete
linking code, generated using a planning algorithm, replaces these at compile
time. Poplar includes a minimal specification language based on typestate-like
protocols and labels, and a lightweight effect system, which ensures the
absence of unwanted interference between hand-written code and generated code.
We give several examples of fully automatic component integration using Poplar,
and demonstrate its potential to simplify object-oriented software development
greatly through evolvable and statically checkable integration links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1570</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1570</id><created>2011-06-08</created><authors><author><keyname>ElSawy</keyname><forenames>Ismaail</forenames></author><author><keyname>Hosny</keyname><forenames>Hossam</forenames></author><author><keyname>Razek</keyname><forenames>Mohammed Abdel</forenames></author></authors><title>A Neural Network Model for Construction Projects Site Overhead Cost
  Estimating in Egypt</title><categories>cs.NE</categories><comments>11 pages,IJCSI</comments><journal-ref>International Journal of Computer Science Issues, Vol. 8, Issue 3,
  May 2011, 273-283</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating of the overhead costs of building construction projects is an
important task in the management of these projects. The quality of construction
management depends heavily on their accurate cost estimation. Construction
costs prediction is a very difficult and sophisticated task especially when
using manual calculation methods. This paper uses Artificial Neural Network
(ANN) approach to develop a parametric cost-estimating model for site overhead
cost in Egypt. Fifty-two actual real-life cases of building projects
constructed in Egypt during the seven year period 2002-2009 were used as
training materials. The neural network architecture is presented for the
estimation of the site overhead costs as a percentage from the total project
price.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1577</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1577</id><created>2011-06-08</created><authors><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>Market efficiency, anticipation and the formation of bubbles-crashes</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>22 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamical model is introduced for the formation of a bullish or bearish
trends driving an asset price in a given market. Initially, each agent decides
to buy or sell according to its personal opinion, which results from the
combination of its own private information, the public information and its own
analysis. It then adjusts such opinion through the market as it observes
sequentially the behavior of a group of random selection of other agents. Its
choice is then determined by a local majority rule including itself. Whenever
the selected group is at a tie, i.e., it is undecided on what to do, the choice
is determined by the local group belief with respect to the anticipated trend
at that time. These local adjustments create a dynamic that leads the market
price formation. In case of balanced anticipations the market is found to be
efficient in being successful to make the &quot;right price&quot; to emerge from the
sequential aggregation of all the local individual informations which all
together contain the fundamental value. However, when a leading optimistic
belief prevails, the same efficient market mechanisms are found to produce a
bullish dynamic even though most agents have bearish private informations. The
market yields then a wider and wider discrepancy between the fundamental value
and the market value, which in turn creates a speculative bubble. Nevertheless,
there exists a limit in the growing of the bubble where private opinions take
over again and at once invert the trend, originating a sudden bearish trend.
Moreover, in the case of a drastic shift in the collective expectations, a huge
drop in price levels may also occur extremely fast and puts the market out of
control, it is a market crash.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1583</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1583</id><created>2011-06-08</created><authors><author><keyname>Hussain</keyname><forenames>Walayat</forenames></author><author><keyname>Sohaib</keyname><forenames>Osama</forenames></author><author><keyname>Ali</keyname><forenames>Arif</forenames></author></authors><title>Improving Web Page Readability by Plain Language</title><categories>cs.HC cs.SE</categories><comments>pages 315- 319, http://www.IJCSI.org</comments><report-no>ISSN (Online): 1694-0814</report-no><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 1, May 2011, 315-319</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world anybody who wants to access any information the first choice
is to use the web because it is the only source to provide easy and instant
access to information. However web readers face many hurdles from web which
includes load of web pages, text size, finding related information, spelling
and grammar etc. However understanding of web pages written in English language
creates great problems for non native readers who have basic knowledge of
English. In this paper, we propose a plain language for a local language (Urdu)
using English alphabets for web pages in Pakistan. For this purpose we
developed two websites, one with a normal English fonts and other in a local
language text scheme using English alphabets. We also conducted a questionnaire
from 40 different users with a different level of English language fluency in
Pakistan to gain the evidence of the practicality of our approach. The result
shows that the proposed plain language text scheme using English alphabets
improved the reading comprehension for non native English speakers in Pakistan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1590</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1590</id><created>2011-06-08</created><authors><author><keyname>Vieira</keyname><forenames>Fabio R. J.</forenames></author><author><keyname>de Rezende</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author></authors><title>Scheduling links for heavy traffic on interfering routes in wireless
  mesh networks</title><categories>cs.NI</categories><journal-ref>Computer Networks 56 (2012), 1584-1598</journal-ref><doi>10.1016/j.comnet.2012.01.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider wireless mesh networks and the problem of scheduling the links of
a given set of routes under the assumption of a heavy-traffic pattern. We
assume some TDMA protocol provides a background of synchronized time slots and
seek to schedule the routes' links to maximize the number of packets that get
delivered to their destinations per time slot. Our approach is to construct an
undirected graph G and to heuristically obtain node multicolorings for G that
can be turned into efficient link schedules. In G each node represents a link
to be scheduled and the edges are set up to represent every possible
interference for any given set of interference assumptions. We present two
multicoloring-based heuristics and study their performance through extensive
simulations. One of the two heuristics is based on relaxing the notion of a
node multicoloring by dynamically exploiting the availability of communication
opportunities that would otherwise be wasted. We have found that, as a
consequence, its performance is significantly superior to the other's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1595</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1595</id><created>2011-06-08</created><authors><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Transmission with Energy Harvesting Nodes in Fading Wireless Channels:
  Optimal Policies</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in the JSAC special issue on Energy-Efficient Wireless
  Communications. Submitted October 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless systems comprised of rechargeable nodes have a significantly
prolonged lifetime and are sustainable. A distinct characteristic of these
systems is the fact that the nodes can harvest energy throughout the duration
in which communication takes place. As such, transmission policies of the nodes
need to adapt to these harvested energy arrivals. In this paper, we consider
optimization of point-to-point data transmission with an energy harvesting
transmitter which has a limited battery capacity, communicating in a wireless
fading channel. We consider two objectives: maximizing the throughput by a
deadline, and minimizing the transmission completion time of the communication
session. We optimize these objectives by controlling the time sequence of
transmit powers subject to energy storage capacity and causality constraints.
We, first, study optimal offline policies. We introduce a directional
water-filling algorithm which provides a simple and concise interpretation of
the necessary optimality conditions. We show the optimality of an adaptive
directional water-filling algorithm for the throughput maximization problem. We
solve the transmission completion time minimization problem by utilizing its
equivalence to its throughput maximization counterpart. Next, we consider
online policies. We use stochastic dynamic programming to solve for the optimal
online policy that maximizes the average number of bits delivered by a deadline
under stochastic fading and energy arrival processes with causal channel state
feedback. We also propose near-optimal policies with reduced complexity, and
numerically study their performances along with the performances of the offline
and online optimal policies under various different configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1622</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1622</id><created>2011-06-08</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Large-Scale Convex Minimization with a Low-Rank Constraint</title><categories>cs.LG stat.ML</categories><comments>ICML 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of minimizing a convex function over the space of
large matrices with low rank. While this optimization problem is hard in
general, we propose an efficient greedy algorithm and derive its formal
approximation guarantees. Each iteration of the algorithm involves
(approximately) finding the left and right singular vectors corresponding to
the largest singular value of a certain matrix, which can be calculated in
linear time. This leads to an algorithm which can scale to large matrices
arising in several applications such as matrix completion for collaborative
filtering and robust low rank matrix approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1631</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1631</id><created>2011-06-08</created><authors><author><keyname>Bashan</keyname><forenames>Amir</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>The combined effect of connectivity and dependency links on percolation
  of networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>11 pages, 3 figures</comments><doi>10.1007/s10955-011-0333-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Percolation theory is extensively studied in statistical physics and
mathematics with applications in diverse fields. However, the research is
focused on systems with only one type of links, connectivity links. We review a
recently developed mathematical framework for analyzing percolation properties
of realistic scenarios of networks having links of two types, connectivity and
dependency links. This formalism was applied to study
Erd$\ddot{o}$s-R$\acute{e}$nyi (ER) networks that include also dependency
links. For an ER network with average degree $k$ that is composed of dependency
clusters of size $s$, the fraction of nodes that belong to the giant component,
$P_\infty$, is given by $ P_\infty=p^{s-1}[1-\exp{(-kpP_\infty)}]^s $ where
$1-p$ is the initial fraction of randomly removed nodes. Here, we apply the
formalism to the study of random-regular (RR) networks and find a formula for
the size of the giant component in the percolation process:
$P_\infty=p^{s-1}(1-r^k)^s$ where $r$ is the solution of
$r=p^s(r^{k-1}-1)(1-r^k)+1$. These general results coincide, for $s=1$, with
the known equations for percolation in ER and RR networks respectively without
dependency links. In contrast to $s=1$, where the percolation transition is
second order, for $s&gt;1$ it is of first order. Comparing the percolation
behavior of ER and RR networks we find a remarkable difference regarding their
resilience. We show, analytically and numerically, that in ER networks with low
connectivity degree or large dependency clusters, removal of even a finite
number (zero fraction) of the network nodes will trigger a cascade of failures
that fragments the whole network. This result is in contrast to RR networks
where such cascades and full fragmentation can be triggered only by removal of
a finite fraction of nodes in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1634</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1634</id><created>2011-06-08</created><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author></authors><title>Repair Optimal Erasure Codes through Hadamard Designs</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>19 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed storage systems that employ erasure coding, the issue of
minimizing the total {\it communication} required to exactly rebuild a storage
node after a failure arises. This repair bandwidth depends on the structure of
the storage code and the repair strategies used to restore the lost data.
Designing high-rate maximum-distance separable (MDS) codes that achieve the
optimum repair communication has been a well-known open problem. In this work,
we use Hadamard matrices to construct the first explicit 2-parity MDS storage
code with optimal repair properties for all single node failures, including the
parities. Our construction relies on a novel method of achieving perfect
interference alignment over finite fields with a finite file size, or number of
extensions. We generalize this construction to design $m$-parity MDS codes that
achieve the optimum repair communication for single systematic node failures
and show that there is an interesting connection between our $m$-parity codes
and the systematic-repair optimal permutation-matrix based codes of Tamo {\it
et al.} \cite{Tamo} and Cadambe {\it et al.} \cite{PermCodes_ISIT, PermCodes}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1636</identifier>
 <datestamp>2011-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1636</id><created>2011-06-08</created><updated>2011-07-20</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>A Sequence of Relaxations Constraining Hidden Variable Models</title><categories>cs.AI cs.SI physics.soc-ph quant-ph stat.ML</categories><comments>UAI 2011 Best Paper Runner-Up; Proceedings of the 27th Conference on
  Uncertainty in Artificial Intelligence (UAI 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many widely studied graphical models with latent variables lead to nontrivial
constraints on the distribution of the observed variables. Inspired by the Bell
inequalities in quantum mechanics, we refer to any linear inequality whose
violation rules out some latent variable model as a &quot;hidden variable test&quot; for
that model. Our main contribution is to introduce a sequence of relaxations
which provides progressively tighter hidden variable tests. We demonstrate
applicability to mixtures of sequences of i.i.d. variables, Bell inequalities,
and homophily models in social networks. For the last, we demonstrate that our
method provides a test that is able to rule out latent homophily as the sole
explanation for correlations on a real social network that are known to be due
to influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1651</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1651</id><created>2011-06-08</created><authors><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Karystinos</keyname><forenames>George N.</forenames></author></authors><title>Sparse Principal Component of a Rank-deficient Matrix</title><categories>cs.IT cs.LG cs.SY math.IT math.OC</categories><comments>5 pages, 1 figure, to be presented at ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying the sparse principal component of a
rank-deficient matrix. We introduce auxiliary spherical variables and prove
that there exists a set of candidate index-sets (that is, sets of indices to
the nonzero elements of the vector argument) whose size is polynomially
bounded, in terms of rank, and contains the optimal index-set, i.e. the
index-set of the nonzero elements of the optimal solution. Finally, we develop
an algorithm that computes the optimal sparse principal component in polynomial
time for any sparsity degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1652</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1652</id><created>2011-06-08</created><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Distributed Storage Codes through Hadamard Designs</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>5 pages, 3 figures, to be presented at ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed storage systems that employ erasure coding, the issue of
minimizing the total {\it repair bandwidth} required to exactly regenerate a
storage node after a failure arises. This repair bandwidth depends on the
structure of the storage code and the repair strategies used to restore the
lost data. Minimizing it requires that undesired data during a repair align in
the smallest possible spaces, using the concept of interference alignment (IA).
Here, a points-on-a-lattice representation of the symbol extension IA of
Cadambe {\it et al.} provides cues to perfect IA instances which we combine
with fundamental properties of Hadamard matrices to construct a new storage
code with favorable repair properties. Specifically, we build an explicit
$(k+2,k)$ storage code over $\mathbb{GF}(3)$, whose single systematic node
failures can be repaired with bandwidth that matches exactly the theoretical
minimum. Moreover, the repair of single parity node failures generates at most
the same repair bandwidth as any systematic node failure. Our code can tolerate
any single node failure and any pair of failures that involves at most one
systematic failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1674</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1674</id><created>2011-06-08</created><authors><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>Moment based estimation of stochastic Kronecker graph parameters</title><categories>stat.ML cs.SI</categories><comments>22 pages, 4 figures</comments><msc-class>63P99, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Kronecker graphs supply a parsimonious model for large sparse real
world graphs. They can specify the distribution of a large random graph using
only three or four parameters. Those parameters have however proved difficult
to choose in specific applications. This article looks at method of moments
estimators that are computationally much simpler than maximum likelihood. The
estimators are fast and in our examples, they typically yield Kronecker
parameters with expected feature counts closer to a given graph than we get
from KronFit. The improvement was especially prominent for the number of
triangles in the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1684</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1684</id><created>2011-06-08</created><authors><author><keyname>Sen</keyname><forenames>Mehmet Umut</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Max-Margin Stacking and Sparse Regularization for Linear Classifier
  Combination and Selection</title><categories>cs.LG</categories><comments>8 pages, 3 figures, 6 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main principle of stacked generalization (or Stacking) is using a
second-level generalizer to combine the outputs of base classifiers in an
ensemble. In this paper, we investigate different combination types under the
stacking framework; namely weighted sum (WS), class-dependent weighted sum
(CWS) and linear stacked generalization (LSG). For learning the weights, we
propose using regularized empirical risk minimization with the hinge loss. In
addition, we propose using group sparsity for regularization to facilitate
classifier selection. We performed experiments using two different ensemble
setups with differing diversities on 8 real-world datasets. Results show the
power of regularized learning with the hinge loss function. Using sparse
regularization, we are able to reduce the number of selected classifiers of the
diverse ensemble without sacrificing accuracy. With the non-diverse ensembles,
we even gain accuracy on average by using sparse regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1697</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1697</id><created>2011-06-08</created><authors><author><keyname>Michel</keyname><forenames>Lo&#xef;c</forenames></author></authors><title>Model-free control of non-minimum phase systems and switched systems</title><categories>math.OC cs.SY</categories><comments>13 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This brief presents a simple derivation of the standard model-free control
for the non-minimum phase systems. The robustness of the proposed method is
studied in simulation considering the case of switched systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1703</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1703</id><created>2011-06-09</created><updated>2013-08-24</updated><authors><author><keyname>Liu</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author></authors><title>Structural Controllability of Switched Linear Systems</title><categories>cs.SY math.OC</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the structural controllability of a class of uncertain
switched linear systems, where the parameters of subsystems state matrices are
either unknown or zero. The structural controllability is a generalization of
the traditional controllability concept for dynamical systems, and purely based
on the interconnection relation between the state variables and inputs through
non-zero elements in the state matrices. In order to illustrate such a
relationship, two kinds of graphic representations of switched linear systems
are proposed, based on which graph theory based necessary and sufficient
characterizations of the structural controllability for switched linear systems
are presented. Finally, the paper concludes with discussions on the results and
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1716</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1716</id><created>2011-06-09</created><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author></authors><title>Predicting growth fluctuation in network economy</title><categories>cs.AI q-bio.GN</categories><comments>presented at the 5th International Workshop on Data Mining and
  Statistical Science, Osaka, March 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a method to predict the growth fluctuation of firms
interdependent in a network economy. The risk of downward growth fluctuation of
firms is calculated from the statistics on Japanese industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1731</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1731</id><created>2011-06-09</created><updated>2012-01-04</updated><authors><author><keyname>Iwamoto</keyname><forenames>Mitsugu</forenames></author><author><keyname>Ohta</keyname><forenames>Kazuo</forenames></author></authors><title>Security Notions for Information Theoretically Secure Encryptions</title><categories>cs.CR</categories><comments>6 pages, presented at ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with several security notions for information
theoretically secure encryptions defined by the variational (statistical)
distance. To ensure the perfect secrecy (PS), the mutual information is often
used to evaluate the statistical independence between a message and a
cryptogram. On the other hand, in order to recognize the information
theoretically secure encryptions and computationally secure ones
comprehensively, it is necessary to reconsider the notion of PS in terms of the
variational distance. However, based on the variational distance, three kinds
of definitions for PS are naturally introduced, but their relations are not
known. In this paper, we clarify that one of three definitions for PS with the
variational distance, which is a straightforward extension of Shannon's perfect
secrecy, is stronger than the others, and the weaker two definitions of PS are
essentially equivalent to the statistical versions of indistinguishability and
semantic security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1770</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1770</id><created>2011-06-09</created><updated>2011-10-04</updated><authors><author><keyname>Oksanen</keyname><forenames>Jan</forenames></author><author><keyname>Lund&#xe9;n</keyname><forenames>Jarmo</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Reinforcement learning based sensing policy optimization for energy
  efficient cognitive radio networks</title><categories>cs.LG</categories><comments>10 pages, 13 figures, Accepted to Neurocomputing special issue:
  Machine learning for signal processing, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a machine learning based collaborative multi-band
spectrum sensing policy for cognitive radios. The proposed sensing policy
guides secondary users to focus the search of unused radio spectrum to those
frequencies that persistently provide them high data rate. The proposed policy
is based on machine learning, which makes it adaptive with the temporally and
spatially varying radio spectrum. Furthermore, there is no need for dynamic
modeling of the primary activity since it is implicitly learned over time.
Energy efficiency is achieved by minimizing the number of assigned sensors per
each subband under a constraint on miss detection probability. It is important
to control the missed detections because they cause collisions with primary
transmissions and lead to retransmissions at both the primary and secondary
user. Simulations show that the proposed machine learning based sensing policy
improves the overall throughput of the secondary network and improves the
energy efficiency while controlling the miss detection probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1788</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1788</id><created>2011-06-09</created><updated>2015-09-25</updated><authors><author><keyname>Bendahmane</keyname><forenames>Mostafa</forenames></author><author><keyname>Chaves-Silva</keyname><forenames>Felipe Wallison</forenames></author></authors><title>Uniform Null Controllability for a Degenerating Reaction-Diffusion
  System Approximating a Simplified Cardiac Model</title><categories>math.OC cs.SY</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to the analysis of the uniform null controllability for
a family of nonlinear reaction-diffusion systems approximating a
parabolic-elliptic system which models the electrical activity of the heart.
The uniform, with respect to the degenerating parameter, null controllability
of the approximating system by means of a single control is shown. The proof is
based on the combination of Carleman estimates and weighted energy
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1791</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1791</id><created>2011-06-09</created><updated>2011-11-18</updated><authors><author><keyname>Baez</keyname><forenames>John C.</forenames></author><author><keyname>Fritz</keyname><forenames>Tobias</forenames></author><author><keyname>Leinster</keyname><forenames>Tom</forenames></author></authors><title>A Characterization of Entropy in Terms of Information Loss</title><categories>cs.IT math-ph math.IT math.MP quant-ph</categories><comments>11 pages LaTeX, minor revision</comments><msc-class>94A17, 62B10</msc-class><journal-ref>Entropy 2011, 13(11), 1945-1957</journal-ref><doi>10.3390/e13111945</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are numerous characterizations of Shannon entropy and Tsallis entropy
as measures of information obeying certain properties. Using work by Faddeev
and Furuichi, we derive a very simple characterization. Instead of focusing on
the entropy of a probability measure on a finite set, this characterization
focuses on the `information loss', or change in entropy, associated with a
measure-preserving function. Information loss is a special case of conditional
entropy: namely, it is the entropy of a random variable conditioned on some
function of that variable. We show that Shannon entropy gives the only concept
of information loss that is functorial, convex-linear and continuous. This
characterization naturally generalizes to Tsallis entropy as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1796</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1796</id><created>2011-06-09</created><authors><author><keyname>Drummond</keyname><forenames>C.</forenames></author></authors><title>Accelerating Reinforcement Learning by Composing Solutions of
  Automatically Identified Subtasks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  59-104, 2002</journal-ref><doi>10.1613/jair.904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a system that accelerates reinforcement learning by
using transfer from related tasks. Without such transfer, even if two tasks are
very similar at some abstract level, an extensive re-learning effort is
required. The system achieves much of its power by transferring parts of
previously learned solutions rather than a single complete solution. The system
exploits strong features in the multi-dimensional function produced by
reinforcement learning in solving a particular task. These features are stable
and easy to recognize early in the learning process. They generate a
partitioning of the state space and thus the function. The partition is
represented as a graph. This is used to index and compose functions stored in a
case base to form a close approximation to the solution of the new task.
Experiments demonstrate that function composition often produces more than an
order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1797</identifier>
 <datestamp>2011-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1797</id><created>2011-06-09</created><authors><author><keyname>Sato</keyname><forenames>T.</forenames></author><author><keyname>Kameya</keyname><forenames>Y.</forenames></author></authors><title>Parameter Learning of Logic Programs for Symbolic-Statistical Modeling</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 15, pages
  391-454, 2001</journal-ref><doi>10.1613/jair.912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a logical/mathematical framework for statistical parameter
learning of parameterized logic programs, i.e. definite clause programs
containing probabilistic facts with a parameterized distribution. It extends
the traditional least Herbrand model semantics in logic programming to
distribution semantics, possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs
including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM
algorithm, the graphical EM algorithm, that runs for a class of parameterized
logic programs representing sequential decision processes where each decision
is exclusive and independent. It runs on a new data structure called support
graphs describing the logical relationship between observations and their
explanations, and learns parameters by computing inside and outside probability
generalized for logic programs. The complexity analysis shows that when
combined with OLDT search for all explanations for observations, the graphical
EM algorithm, despite its generality, has the same time complexity as existing
EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside
algorithm for PCFGs, and the one for singly connected Bayesian networks that
have been developed independently in each research field. Learning experiments
with PCFGs using two corpora of moderate size indicate that the graphical EM
algorithm can significantly outperform the Inside-Outside algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1799</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1799</id><created>2011-06-09</created><authors><author><keyname>Meek</keyname><forenames>C.</forenames></author></authors><title>Finding a Path is Harder than Finding a Tree</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 15, pages
  383-389, 2001</journal-ref><doi>10.1613/jair.914</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I consider the problem of learning an optimal path graphical model from data
and show the problem to be NP-hard for the maximum likelihood and minimum
description length approaches and a Bayesian approach. This hardness result
holds despite the fact that the problem is a restriction of the polynomially
solvable problem of finding the optimal tree graphical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1800</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1800</id><created>2011-06-09</created><authors><author><keyname>Baget</keyname><forenames>J. F.</forenames></author><author><keyname>Mugnier</keyname><forenames>M. L.</forenames></author></authors><title>Extensions of Simple Conceptual Graphs: the Complexity of Rules and
  Constraints</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  425-465, 2002</journal-ref><doi>10.1613/jair.918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple conceptual graphs are considered as the kernel of most knowledge
representation formalisms built upon Sowa's model. Reasoning in this model can
be expressed by a graph homomorphism called projection, whose semantics is
usually given in terms of positive, conjunctive, existential FOL. We present
here a family of extensions of this model, based on rules and constraints,
keeping graph homomorphism as the basic operation. We focus on the formal
definitions of the different models obtained, including their operational
semantics and relationships with FOL, and we analyze the decidability and
complexity of the associated problems (consistency and deduction). As soon as
rules are involved in reasonings, these problems are not decidable, but we
exhibit a condition under which they fall in the polynomial hierarchy. These
results extend and complete the ones already published by the authors. Moreover
we systematically study the complexity of some particular cases obtained by
restricting the form of constraints and/or rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1802</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1802</id><created>2011-06-09</created><authors><author><keyname>Baader</keyname><forenames>F.</forenames></author><author><keyname>Lutz</keyname><forenames>C.</forenames></author><author><keyname>Sturm</keyname><forenames>H.</forenames></author><author><keyname>Wolter</keyname><forenames>F.</forenames></author></authors><title>Fusions of Description Logics and Abstract Description Systems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  1-58, 2002</journal-ref><doi>10.1613/jair.919</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fusions are a simple way of combining logics. For normal modal logics,
fusions have been investigated in detail. In particular, it is known that,
under certain conditions, decidability transfers from the component logics to
their fusion. Though description logics are closely related to modal logics,
they are not necessarily normal. In addition, ABox reasoning in description
logics is not covered by the results from modal logics. In this paper, we
extend the decidability transfer results from normal modal logics to a large
class of description logics. To cover different description logics in a uniform
way, we introduce abstract description systems, which can be seen as a common
generalization of description and modal logics, and show the transfer results
in this general setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1803</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1803</id><created>2011-06-09</created><authors><author><keyname>Blockeel</keyname><forenames>H.</forenames></author><author><keyname>Dehaspe</keyname><forenames>L.</forenames></author><author><keyname>Demoen</keyname><forenames>B.</forenames></author><author><keyname>Janssens</keyname><forenames>G.</forenames></author><author><keyname>Ramon</keyname><forenames>J.</forenames></author><author><keyname>Vandecasteele</keyname><forenames>H.</forenames></author></authors><title>Improving the Efficiency of Inductive Logic Programming Through the Use
  of Query Packs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  135-166, 2002</journal-ref><doi>10.1613/jair.924</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inductive logic programming, or relational learning, is a powerful paradigm
for machine learning or data mining. However, in order for ILP to become
practically useful, the efficiency of ILP systems must improve substantially.
To this end, the notion of a query pack is introduced: it structures sets of
similar queries. Furthermore, a mechanism is described for executing such query
packs. A complexity analysis shows that considerable efficiency improvements
can be achieved through the use of this query pack execution mechanism. This
claim is supported by empirical results obtained by incorporating support for
query pack execution in two existing learning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1804</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1804</id><created>2011-06-09</created><authors><author><keyname>Dahlman</keyname><forenames>E.</forenames></author><author><keyname>Howe</keyname><forenames>A. E.</forenames></author></authors><title>A Critical Assessment of Benchmark Comparison in Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  1-33, 2002</journal-ref><doi>10.1613/jair.935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent trends in planning research have led to empirical comparison becoming
commonplace. The field has started to settle into a methodology for such
comparisons, which for obvious practical reasons requires running a subset of
planners on a subset of problems. In this paper, we characterize the
methodology and examine eight implicit assumptions about the problems, planners
and metrics used in many of these comparisons. The problem assumptions are:
PR1) the performance of a general purpose planner should not be
penalized/biased if executed on a sampling of problems and domains, PR2) minor
syntactic differences in representation do not affect performance, and PR3)
problems should be solvable by STRIPS capable planners unless they require ADL.
The planner assumptions are: PL1) the latest version of a planner is the best
one to use, PL2) default parameter settings approximate good performance, and
PL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)
performance degrades similarly for each planner when run on degraded runtime
environments (e.g., machine platform) and M2) the number of plan steps
distinguishes performance. We find that most of these assumptions are not
supported empirically; in particular, that planners are affected differently by
these assumptions. We conclude with a call to the community to devote research
resources to improving the state of the practice and especially to enhancing
the available benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1809</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1809</id><created>2011-06-09</created><authors><author><keyname>Abdo</keyname><forenames>Hosam</forenames></author><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author><author><keyname>Gutman</keyname><forenames>Ivan</forenames></author></authors><title>On the Zagreb Indices Equality</title><categories>cs.DM</categories><comments>11 pages, 1 figure</comments><journal-ref>Discrete Applied Mathematics 160(1-2): 1-8 (2012)</journal-ref><doi>10.1016/j.dam.2011.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a simple graph $G$ with $n$ vertices and $m$ edges, the first Zagreb
index and the second Zagreb index are defined as $M_1(G)=\sum_{v\in V}d(v)^2 $
and $M_2(G)=\sum_{uv\in E}d(u)d(v)$. In \cite{VGFAD}, it was shown that if a
connected graph $G$ has maximal degree 4, then $G$ satisfies $M_1(G)/n =
M_2(G)/m$ (also known as the Zagreb indices equality) if and only if $G$ is
regular or biregular of class 1 (a biregular graph whose no two vertices of
same degree are adjacent). There, it was also shown that there exist infinitely
many connected graphs of maximal degree $\Delta= 5$ that are neither regular
nor biregular of class 1 which satisfy the Zagreb indices equality. Here, we
generalize that result by showing that there exist infinitely many connected
graphs of maximal degree $\Delta \geq 5$ that are neither regular nor biregular
graphs of class 1 which satisfy the Zagreb indices equality. We also consider
when the above equality holds when the degrees of vertices of a given graph are
in a prescribed interval of integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1811</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1811</id><created>2011-06-09</created><updated>2011-06-10</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Arnab</forenames></author><author><keyname>Teja</keyname><forenames>B. Palvali</forenames></author><author><keyname>Dutta</keyname><forenames>Sourav</forenames></author></authors><title>Caching Stars in the Sky: A Semantic Caching Approach to Accelerate
  Skyline Queries</title><categories>cs.DB</categories><comments>11 pages; will be published in DEXA 2011</comments><acm-class>H.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-criteria decision making has been made possible with the advent of
skyline queries. However, processing such queries for high dimensional datasets
remains a time consuming task. Real-time applications are thus infeasible,
especially for non-indexed skyline techniques where the datasets arrive online.
In this paper, we propose a caching mechanism that uses the semantics of
previous skyline queries to improve the processing time of a new query. In
addition to exact queries, utilizing such special semantics allow accelerating
related queries. We achieve this by generating partial result sets guaranteed
to be in the skyline sets. We also propose an index structure for efficient
organization of the cached queries. Experiments on synthetic and real datasets
show the effectiveness and scalability of our proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1813</identifier>
 <datestamp>2011-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1813</id><created>2011-06-09</created><authors><author><keyname>Chawla</keyname><forenames>N. V.</forenames></author><author><keyname>Bowyer</keyname><forenames>K. W.</forenames></author><author><keyname>Hall</keyname><forenames>L. O.</forenames></author><author><keyname>Kegelmeyer</keyname><forenames>W. P.</forenames></author></authors><title>SMOTE: Synthetic Minority Over-sampling Technique</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  321-357, 2002</journal-ref><doi>10.1613/jair.953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to the construction of classifiers from imbalanced datasets is
described. A dataset is imbalanced if the classification categories are not
approximately equally represented. Often real-world data sets are predominately
composed of &quot;normal&quot; examples with only a small percentage of &quot;abnormal&quot; or
&quot;interesting&quot; examples. It is also the case that the cost of misclassifying an
abnormal (interesting) example as a normal example is often much higher than
the cost of the reverse error. Under-sampling of the majority (normal) class
has been proposed as a good means of increasing the sensitivity of a classifier
to the minority class. This paper shows that a combination of our method of
over-sampling the minority (abnormal) class and under-sampling the majority
(normal) class can achieve better classifier performance (in ROC space) than
only under-sampling the majority class. This paper also shows that a
combination of our method of over-sampling the minority class and
under-sampling the majority class can achieve better classifier performance (in
ROC space) than varying the loss ratios in Ripper or class priors in Naive
Bayes. Our method of over-sampling the minority class involves creating
synthetic minority class examples. Experiments are performed using C4.5, Ripper
and a Naive Bayes classifier. The method is evaluated using the area under the
Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1814</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1814</id><created>2011-06-09</created><authors><author><keyname>Chan</keyname><forenames>H.</forenames></author><author><keyname>Darwiche</keyname><forenames>A.</forenames></author></authors><title>When do Numbers Really Matter?</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  265-287, 2002</journal-ref><doi>10.1613/jair.967</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common wisdom has it that small distinctions in the probabilities
(parameters) quantifying a belief network do not matter much for the results of
probabilistic queries. Yet, one can develop realistic scenarios under which
small variations in network parameters can lead to significant changes in
computed queries. A pending theoretical question is then to analytically
characterize parameter changes that do or do not matter. In this paper, we
study the sensitivity of probabilistic queries to changes in network parameters
and prove some tight bounds on the impact that such parameters can have on
queries. Our analytic results pinpoint some interesting situations under which
parameter changes do or do not matter. These results are important for
knowledge engineers as they help them identify influential network parameters.
They also help explain some of the previous experimental results and
observations with regards to network robustness against parameter changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1816</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1816</id><created>2011-06-09</created><authors><author><keyname>Kaminka</keyname><forenames>G. A.</forenames></author><author><keyname>Pynadath</keyname><forenames>D. V.</forenames></author><author><keyname>Tambe</keyname><forenames>M.</forenames></author></authors><title>Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  83-135, 2002</journal-ref><doi>10.1613/jair.970</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1817</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1817</id><created>2011-06-09</created><authors><author><keyname>Gorin</keyname><forenames>A.</forenames></author><author><keyname>Langkilde-Geary</keyname><forenames>I.</forenames></author><author><keyname>Walker</keyname><forenames>M. A.</forenames></author><author><keyname>Wright</keyname><forenames>J.</forenames></author><author><keyname>Hastie</keyname><forenames>H. Wright</forenames></author></authors><title>Automatically Training a Problematic Dialogue Predictor for a Spoken
  Dialogue System</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  293-319, 2002</journal-ref><doi>10.1613/jair.971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spoken dialogue systems promise efficient and natural access to a large
variety of information sources and services from any phone. However, current
spoken dialogue systems are deficient in their strategies for preventing,
identifying and repairing problems that arise in the conversation. This paper
reports results on automatically training a Problematic Dialogue Predictor to
predict problematic human-computer dialogues using a corpus of 4692 dialogues
collected with the 'How May I Help You' (SM) spoken dialogue system. The
Problematic Dialogue Predictor can be immediately applied to the system's
decision of whether to transfer the call to a human customer care agent, or be
used as a cue to the system's dialogue manager to modify its behavior to repair
problems, and even perhaps, to prevent them. We show that a Problematic
Dialogue Predictor using automatically-obtainable features from the first two
exchanges in the dialogue can predict problematic dialogues 13.2% more
accurately than the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1818</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1818</id><created>2011-06-09</created><authors><author><keyname>Nock</keyname><forenames>R.</forenames></author></authors><title>Inducing Interpretable Voting Classifiers without Trading Accuracy for
  Simplicity: Theoretical Results, Approximation Algorithms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  137-170, 2002</journal-ref><doi>10.1613/jair.986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in the study of voting classification algorithms have brought
empirical and theoretical results clearly showing the discrimination power of
ensemble classifiers. It has been previously argued that the search of this
classification power in the design of the algorithms has marginalized the need
to obtain interpretable classifiers. Therefore, the question of whether one
might have to dispense with interpretability in order to keep classification
strength is being raised in a growing number of machine learning or data mining
papers. The purpose of this paper is to study both theoretically and
empirically the problem. First, we provide numerous results giving insight into
the hardness of the simplicity-accuracy tradeoff for voting classifiers. Then
we provide an efficient &quot;top-down and prune&quot; induction heuristic, WIDC, mainly
derived from recent results on the weak learning and boosting frameworks. It is
to our knowledge the first attempt to build a voting classifier as a base
formula using the weak learning framework (the one which was previously highly
successful for decision tree induction), and not the strong learning framework
(as usual for such classifiers with boosting-like approaches). While it uses a
well-known induction scheme previously successful in other classes of concept
representations, thus making it easy to implement and compare, WIDC also relies
on recent or new results we give about particular cases of boosting known as
partition boosting and ranking loss boosting. Experimental results on
thirty-one domains, most of which readily available, tend to display the
ability of WIDC to produce small, accurate, and interpretable decision
committees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1819</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1819</id><created>2011-06-09</created><authors><author><keyname>Darwiche</keyname><forenames>A.</forenames></author><author><keyname>Marquis</keyname><forenames>P.</forenames></author></authors><title>A Knowledge Compilation Map</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  229-264, 2002</journal-ref><doi>10.1613/jair.989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a perspective on knowledge compilation which calls for analyzing
different compilation approaches according to two key dimensions: the
succinctness of the target compilation language, and the class of queries and
transformations that the language supports in polytime. We then provide a
knowledge compilation map, which analyzes a large number of existing target
compilation languages according to their succinctness and their polytime
transformations and queries. We argue that such analysis is necessary for
placing new compilation approaches within the context of existing ones. We also
go beyond classical, flat target compilation languages based on CNF and DNF,
and consider a richer, nested class based on directed acyclic graphs (such as
OBDDs), which we show to include a relatively large number of target
compilation languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1820</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1820</id><created>2011-06-09</created><authors><author><keyname>Barzilay</keyname><forenames>R.</forenames></author><author><keyname>Elhadad</keyname><forenames>N.</forenames></author></authors><title>Inferring Strategies for Sentence Ordering in Multidocument News
  Summarization</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  35-55, 2002</journal-ref><doi>10.1613/jair.991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of organizing information for multidocument summarization so that
the generated summary is coherent has received relatively little attention.
While sentence ordering for single document summarization can be determined
from the ordering of sentences in the input article, this is not the case for
multidocument summarization where summary sentences may be drawn from different
input articles. In this paper, we propose a methodology for studying the
properties of ordering information in the news genre and describe experiments
done on a corpus of multiple acceptable orderings we developed for the task.
Based on these experiments, we implemented a strategy for ordering information
that combines constraints from chronological order of events and topical
relatedness. Evaluation of our augmented algorithm shows a significant
improvement of the ordering over two baseline strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1821</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1821</id><created>2011-06-09</created><authors><author><keyname>Tumer</keyname><forenames>K.</forenames></author><author><keyname>Wolpert</keyname><forenames>D. H.</forenames></author></authors><title>Collective Intelligence, Data Routing and Braess' Paradox</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  359-387, 2002</journal-ref><doi>10.1613/jair.995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing the the utility functions of the
utility-maximizing agents in a multi-agent system so that they work
synergistically to maximize a global utility. The particular problem domain we
explore is the control of network routing by placing agents on all the routers
in the network. Conventional approaches to this task have the agents all use
the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many
cases, due to the side-effects of one agent's actions on another agent's
performance, having agents use ISPA's is suboptimal as far as global aggregate
cost is concerned, even when they are only used to route infinitesimally small
amounts of traffic. The utility functions of the individual agents are not
&quot;aligned&quot; with the global utility, intuitively speaking. As a particular
example of this we present an instance of Braess' paradox in which adding new
links to a network whose agents all use the ISPA results in a decrease in
overall throughput. We also demonstrate that load-balancing, in which the
agents' decisions are collectively made to optimize the global cost incurred by
all traffic currently being routed, is suboptimal as far as global cost
averaged across time is concerned. This is also due to 'side-effects', in this
case of current routing decision on future traffic. The mathematics of
Collective Intelligence (COIN) is concerned precisely with the issue of
avoiding such deleterious side-effects in multi-agent systems, both over time
and space. We present key concepts from that mathematics and use them to derive
an algorithm whose ideal version should have better performance than that of
having all agents use the ISPA, even in the infinitesimal limit. We present
experiments verifying this, and also showing that a machine-learning-based
version of this COIN algorithm in which costs are only imprecisely estimated
via empirical means (a version potentially applicable in the real world) also
outperforms the ISPA, despite having access to less information than does the
ISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1822</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1822</id><created>2011-06-09</created><authors><author><keyname>Guestrin</keyname><forenames>C.</forenames></author><author><keyname>Koller</keyname><forenames>D.</forenames></author><author><keyname>Parr</keyname><forenames>R.</forenames></author><author><keyname>Venkataraman</keyname><forenames>S.</forenames></author></authors><title>Efficient Solution Algorithms for Factored MDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  399-468, 2003</journal-ref><doi>10.1613/jair.1000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of planning under uncertainty in large
Markov Decision Processes (MDPs). Factored MDPs represent a complex state space
using state variables and the transition model using a dynamic Bayesian
network. This representation often allows an exponential reduction in the
representation size of structured MDPs, but the complexity of exact solution
algorithms for such MDPs can grow exponentially in the representation size. In
this paper, we present two approximate solution algorithms that exploit
structure in factored MDPs. Both use an approximate value function represented
as a linear combination of basis functions, where each basis function involves
only a small subset of the domain variables. A key contribution of this paper
is that it shows how the basic operations of both algorithms can be performed
efficiently in closed form, by exploiting both additive and context-specific
structure in a factored MDP. A central element of our algorithms is a novel
linear program decomposition technique, analogous to variable elimination in
Bayesian networks, which reduces an exponentially large LP to a provably
equivalent, polynomial-sized one. One algorithm uses approximate linear
programming, and the second approximate dynamic programming. Our dynamic
programming algorithm is novel in that it uses an approximation based on
max-norm, a technique that more directly minimizes the terms that appear in
error bounds for approximate MDP algorithms. We provide experimental results on
problems with over 10^40 states, demonstrating a promising indication of the
scalability of our approach, and compare our algorithm to an existing
state-of-the-art approach, showing, in some problems, exponential gains in
computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1842</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1842</id><created>2011-06-09</created><updated>2011-07-04</updated><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>Fixed points avoiding Abelian $k$-powers</title><categories>cs.FL math.CO</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of whether the fixed point of a morphism avoids
Abelian $k$-powers is decidable under rather general conditions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1845</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1845</id><created>2011-06-09</created><updated>2012-05-01</updated><authors><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Broadcast in Point-to-Point Networks using Local Linear Coding</title><categories>cs.DC</categories><acm-class>F.2.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of Byzantine Broadcast (BB) is to allow a set of fault-free nodes to
agree on information that a source node wants to broadcast to them, in the
presence of Byzantine faulty nodes. We consider design of efficient algorithms
for BB in {\em synchronous} point-to-point networks, where the rate of
transmission over each communication link is limited by its &quot;link capacity&quot;.
The throughput of a particular BB algorithm is defined as the average number of
bits that can be reliably broadcast to all fault-free nodes per unit time using
the algorithm without violating the link capacity constraints. The {\em
capacity} of BB in a given network is then defined as the supremum of all
achievable BB throughputs in the given network, over all possible BB
algorithms.
  We develop NAB -- a Network-Aware Byzantine broadcast algorithm -- for
arbitrary point-to-point networks consisting of $n$ nodes, wherein the number
of faulty nodes is at most $f$, $f&lt;n/3$, and the network connectivity is at
least $2f+1$. We also prove an upper bound on the capacity of Byzantine
broadcast, and conclude that NAB can achieve throughput at least 1/3 of the
capacity. When the network satisfies an additional condition, NAB can achieve
throughput at least 1/2 of the capacity.
  To the best of our knowledge, NAB is the first algorithm that can achieve a
constant fraction of capacity of Byzantine Broadcast (BB) in arbitrary
point-to-point networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1846</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1846</id><created>2011-06-09</created><authors><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures</title><categories>cs.DC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we investigate the multi-valued Byzantine consensus problem.
We introduce two algorithms: the first one achieves traditional validity
requirement for consensus, and the second one achieves a stronger &quot;q-validity&quot;
requirement. Both algorithms are more efficient than the ones introduces in our
recent PODC 2011 paper titled &quot;Error-Free Multi-Valued Consensus with Byzantine
Failures&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1850</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1850</id><created>2011-06-09</created><updated>2013-02-26</updated><authors><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Univ. Bordeaux, LaBRI</affiliation></author><author><keyname>Srivathsan</keyname><forenames>B</forenames><affiliation>Univ. Bordeaux, LaBRI</affiliation></author></authors><title>Coarse abstractions make Zeno behaviours difficult to detect</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4; F.1.3; F.1.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  27, 2013) lmcs:882</journal-ref><doi>10.2168/LMCS-9(1:6)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An infinite run of a timed automaton is Zeno if it spans only a finite amount
of time. Such runs are considered unfeasible and hence it is important to
detect them, or dually, find runs that are non-Zeno. Over the years important
improvements have been obtained in checking reachability properties for timed
automata. We show that some of these very efficient optimizations make testing
for Zeno runs costly. In particular we show NP-completeness for the
LU-extrapolation of Behrmann et al. We analyze the source of this complexity in
detail and give general conditions on extrapolation operators that guarantee a
(low) polynomial complexity of Zenoness checking. We propose a slight weakening
of the LU-extrapolation that satisfies these conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1852</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1852</id><created>2011-06-09</created><updated>2012-02-17</updated><authors><author><keyname>Lovejoy</keyname><forenames>Kristen</forenames></author><author><keyname>Waters</keyname><forenames>Richard</forenames></author><author><keyname>Saxton</keyname><forenames>Gregory D.</forenames></author></authors><title>Engaging Stakeholders through Twitter: How Nonprofit Organizations are
  Getting More Out of 140 Characters or Less</title><categories>cs.CY cs.HC</categories><comments>In press, Public Relations Review; 10 pages</comments><doi>10.1016/j.pubrev.2012.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  140 characters seems like too small a space for any meaningful information to
be exchanged, but Twitter users have found creative ways to get the most out of
each Tweet by using different communication tools. This paper looks into how 73
nonprofit organizations use Twitter to engage stakeholders not only through
their tweets, but also through other various communication methods.
Specifically, it looks into the organizations' utilization of tweet frequency,
following behavior, hyperlinks, hashtags, public messages, retweets, and
multimedia files. After analyzing 4,655 tweets, the study found that the
nation's largest nonprofits are not using Twitter to maximize stakeholder
involvement. Instead, they continue to use social media as a one-way
communication channel, as less than 20% of their total tweets demonstrate
conversations and roughly 16% demonstrate indirect connections to specific
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1853</identifier>
 <datestamp>2011-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1853</id><created>2011-06-09</created><updated>2011-08-22</updated><authors><author><keyname>Hsiao</keyname><forenames>Ching-an</forenames></author><author><keyname>Tian</keyname><forenames>Xinchun</forenames></author></authors><title>Intelligent decision: towards interpreting the Pe Algorithm</title><categories>cs.AI</categories><comments>23pages, 12 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human intelligence lies in the algorithm, the nature of algorithm lies in
the classification, and the classification is equal to outlier detection. A lot
of algorithms have been proposed to detect outliers, meanwhile a lot of
definitions. Unsatisfying point is that definitions seem vague, which makes the
solution an ad hoc one. We analyzed the nature of outliers, and give two clear
definitions. We then develop an efficient RDD algorithm, which converts outlier
problem to pattern and degree problem. Furthermore, a collapse mechanism was
introduced by IIR algorithm, which can be united seamlessly with the RDD
algorithm and serve for the final decision. Both algorithms are originated from
the study on general AI. The combined edition is named as Pe algorithm, which
is the basis of the intelligent decision. Here we introduce longest k-turn
subsequence problem and corresponding solution as an example to interpret the
function of Pe algorithm in detecting curve-type outliers. We also give a
comparison between IIR algorithm and Pe algorithm, where we can get a better
understanding at both algorithms. A short discussion about intelligence is
added to demonstrate the function of the Pe algorithm. Related experimental
results indicate its robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1862</identifier>
 <datestamp>2011-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1862</id><created>2011-06-09</created><authors><author><keyname>Carette</keyname><forenames>Jacques</forenames></author><author><keyname>Farmer</keyname><forenames>William M.</forenames></author><author><keyname>Jeremic</keyname><forenames>Filip</forenames></author><author><keyname>Maccio</keyname><forenames>Vincent</forenames></author><author><keyname>O'Connor</keyname><forenames>Russell</forenames></author><author><keyname>Tran</keyname><forenames>Quang M.</forenames></author></authors><title>The MathScheme Library: Some Preliminary Experiments</title><categories>cs.MS cs.SC cs.SE math.RA</categories><comments>Accepted as a work-in-progress paper at CICM 2011</comments><acm-class>D.2.1; D.3.3; F.4.1; I.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some of the experiments we have performed to best test our design
for a library for MathScheme, the mechanized mathematics software system we are
building. We wish for our library design to use and reflect, as much as
possible, the mathematical structure present in the objects which populate the
library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1875</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1875</id><created>2011-06-09</created><updated>2012-07-05</updated><authors><author><keyname>Padovani</keyname><forenames>Vincent</forenames><affiliation>PPS</affiliation></author></authors><title>Ticket Entailment is decidable</title><categories>cs.LO</categories><comments>Submitted on 06/09/2010 to Math. Struct. in Comp. Science. Accepted
  for publication on 12/19/2011. Last revision on 03/06/2012</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the decidability of Ticket Entailment. Raised by Anderson and Belnap
within the framework of relevance logic, this question is equivalent to the
question of the decidability of type inhabitation in simply-typed combinatory
logic with the partial basis BB'IW. We solve the equivalent problem of type
inhabitation for the restriction of simply-typed lambda-calculus to
hereditarily right-maximal terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1879</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1879</id><created>2011-06-09</created><updated>2012-04-05</updated><authors><author><keyname>Nomura</keyname><forenames>Ryo</forenames></author><author><keyname>Han</keyname><forenames>Te Sun</forenames></author></authors><title>Second-Order Resolvability, Intrinsic Randomness, and Fixed-Length
  Source Coding for Mixed Sources: Information Spectrum Approach</title><categories>cs.IT math.IT</categories><comments>Revised version; the title was changed, Section 8 and figures were
  added</comments><msc-class>94A15</msc-class><journal-ref>IEEE Transaction on Information Theory, vol.59, no.1, pp1-16, Jan.
  2013</journal-ref><doi>10.1109/TIT.2012.2215836</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The second-order achievable asymptotics in typical random number generation
problems such as resolvability, intrinsic randomness, fixed-length source
coding are considered. In these problems, several researchers have derived the
first-order and the second-order achievability rates for general sources using
the information spectrum methods. Although these formulas are general, their
computation are quite hard. Hence, an attempt to address explicit computation
problems of achievable rates is meaningful. In particular, for i.i.d. sources,
the second-order achievable rates have earlier been determined simply by using
the asymptotic normality. In this paper, we consider mixed sources of two
i.i.d. sources. The mixed source is a typical case of nonergodic sources and
whose self-information does not have the asymptotic normality. Nonetheless, we
can explicitly compute the second-order achievable rates for these sources on
the basis of two-peak asymptotic normality. In addition, extensions of our
results to more general mixed sources, such as a mixture of countably infinite
i.i.d. sources or Markovian sources, and a continuous mixture of i.i.d.
sources, are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1887</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1887</id><created>2011-06-09</created><updated>2012-05-01</updated><authors><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Learning the Dependence Graph of Time Series with Latent Factors</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of learning, from samples, the dependency
structure of a system of linear stochastic differential equations, when some of
the variables are latent. In particular, we observe the time evolution of some
variables, and never observe other variables; from this, we would like to find
the dependency structure between the observed variables - separating out the
spurious interactions caused by the (marginalizing out of the) latent
variables' time series. We develop a new method, based on convex optimization,
to do so in the case when the number of latent variables is smaller than the
number of observed ones. For the case when the dependency structure between the
observed variables is sparse, we theoretically establish a high-dimensional
scaling result for structure recovery. We verify our theoretical result with
both synthetic and real data (from the stock market).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1894</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1894</id><created>2011-06-09</created><authors><author><keyname>Basu</keyname><forenames>J.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>A.</forenames></author><author><keyname>Chakraborty</keyname><forenames>S.</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>T. K.</forenames></author></authors><title>A Comparative Study Between a Micromechanical Cantilever Resonator and
  MEMS-based Passives for Band-pass Filtering Application</title><categories>cs.OH</categories><comments>6 pages, 15 figures</comments><journal-ref>Proceedings of the 2011 IEEE TechSym Conference, Kharagpur, India,
  Jan. 2011, pp. 247-252</journal-ref><doi>10.1109/TECHSYM.2011.5783824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, significant growth has been observed in using MEMS
based passive components in the RF microelectronics domain, especially in
transceiver components. This is due to some excellent properties of the MEMS
devices like low loss, excellent isolation etc. in the microwave frequency
domain where the on-chip passives normally tend to become leakier and degrades
the transceiver performance. This paper presents a comparative analysis between
MEMS-resonator based and MEMS-passives based band-pass filter configurations
for RF applications, along with their design, simulation, fabrication and
characterization. The filters were designed to have a center frequency of 455
kHz, meant for use as the intermediate frequency (IF) filter in superheterodyne
receivers. The filter structures have been fabricated in PolyMUMPs process, a
three-polysilicon layer surface micromachining process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1900</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1900</id><created>2011-06-09</created><updated>2012-04-10</updated><authors><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author><author><keyname>Gaburov</keyname><forenames>Evghenii</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>A sparse octree gravitational N-body code that runs entirely on the GPU
  processor</title><categories>astro-ph.IM cs.DC</categories><comments>Accepted version. Published in Journal of Computational Physics. 35
  pages, 12 figures, single column</comments><journal-ref>Journal of Computational Physics. Volume 231, Issue 7, 1 April
  2012, Pages 2825-2839</journal-ref><doi>10.1016/j.jcp.2011.12.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present parallel algorithms for constructing and traversing sparse octrees
on graphics processing units (GPUs). The algorithms are based on parallel-scan
and sort methods. To test the performance and feasibility, we implemented them
in CUDA in the form of a gravitational tree-code which completely runs on the
GPU.(The code is publicly available at:
http://castle.strw.leidenuniv.nl/software.html) The tree construction and
traverse algorithms are portable to many-core devices which have support for
CUDA or OpenCL programming languages. The gravitational tree-code outperforms
tuned CPU code during the tree-construction and shows a performance improvement
of more than a factor 20 overall, resulting in a processing rate of more than
2.8 million particles per second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1910</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1910</id><created>2011-06-09</created><authors><author><keyname>Nezhad</keyname><forenames>Vahid Majid</forenames></author><author><keyname>Gader</keyname><forenames>Habib Motee</forenames></author><author><keyname>Efimov</keyname><forenames>Evgueni</forenames></author></authors><title>A Novel Hybrid Algorithm for Task Graph Scheduling</title><categories>cs.CC</categories><journal-ref>IJCSI, Vol 8, Issue 2, March 2011, p32-38</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important problems in multiprocessor systems is Task Graph
Scheduling. Task Graph Scheduling is an NP-Hard problem. Both learning automata
and genetic algorithms are search tools which are used for solving many NP-Hard
problems. In this paper a new hybrid method based on Genetic Algorithm and
Learning Automata is proposed. The proposed algorithm begins with an initial
population of randomly generated chromosomes and after some stages, each
chromosome maps to an automaton. Experimental results show that superiority of
the proposed algorithm over the current approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1919</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1919</id><created>2011-06-09</created><authors><author><keyname>Azad</keyname><forenames>Amar Prakash</forenames></author></authors><title>Sleep Mode Analysis via Workload Decomposition</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to establish a general approach for analyzing
queueing models with repeated inhomogeneous vacations. The server goes on for a
vacation if the inactivity prolongs more than the vacation trigger duration.
Once the system enters in vacation mode, it may continue for several
consecutive vacations. At the end of a vacation, the server goes on another
vacation, possibly with a different probability distribution; if during the
previous vacation there have been no arrivals. However the system enters in
vacation mode only if the inactivity is persisted beyond defined trigger
duration. In order to get an insight on the influence of parameters on the
performance, we choose to study a simple M/G/1 queue (Poisson arrivals and
general independent service times) which has the advantage of being tractable
analytically. The theoretical model is applied to the problem of power saving
for mobile devices in which the sleep durations of a device correspond to the
vacations of the server. Various system performance metrics such as the frame
response time and the economy of energy are derived. A constrained optimization
problem is formulated to maximize the economy of energy achieved in power save
mode, with constraints as QoS conditions to be met. An illustration of the
proposed methods is shown with a WiMAX system scenario to obtain design
parameters for better performance. Our analysis allows us not only to optimize
the system parameters for a given traffic intensity but also to propose
parameters that provide the best performance under worst case conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1925</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1925</id><created>2011-06-09</created><updated>2011-06-13</updated><authors><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author></authors><title>Ranking via Sinkhorn Propagation</title><categories>stat.ML cs.IR cs.LG</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is of increasing importance to develop learning methods for ranking. In
contrast to many learning objectives, however, the ranking problem presents
difficulties due to the fact that the space of permutations is not smooth. In
this paper, we examine the class of rank-linear objective functions, which
includes popular metrics such as precision and discounted cumulative gain. In
particular, we observe that expectations of these gains are completely
characterized by the marginals of the corresponding distribution over
permutation matrices. Thus, the expectations of rank-linear objectives can
always be described through locations in the Birkhoff polytope, i.e.,
doubly-stochastic matrices (DSMs). We propose a technique for learning
DSM-based ranking functions using an iterative projection operator known as
Sinkhorn normalization. Gradients of this operator can be computed via
backpropagation, resulting in an algorithm we call Sinkhorn propagation, or
SinkProp. This approach can be combined with a wide range of gradient-based
approaches to rank learning. We demonstrate the utility of SinkProp on several
information retrieval data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1933</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1933</id><created>2011-06-09</created><updated>2012-04-23</updated><authors><author><keyname>Bauso</keyname><forenames>Dario</forenames></author><author><keyname>Reddy</keyname><forenames>Puduru Viswanadha</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Lyapunov stochastic stability and control of robust dynamic coalitional
  games with transferable utilities</title><categories>cs.GT cs.LG cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper considers a dynamic game with transferable utilities (TU), where
the characteristic function is a continuous-time bounded mean ergodic process.
A central planner interacts continuously over time with the players by choosing
the instantaneous allocations subject to budget constraints. Before the game
starts, the central planner knows the nature of the process (bounded mean
ergodic), the bounded set from which the coalitions' values are sampled, and
the long run average coalitions' values. On the other hand, he has no knowledge
of the underlying probability function generating the coalitions' values. Our
goal is to find allocation rules that use a measure of the extra reward that a
coalition has received up to the current time by re-distributing the budget
among the players. The objective is two-fold: i) guaranteeing convergence of
the average allocations to the core (or a specific point in the core) of the
average game, ii) driving the coalitions' excesses to an a priori given cone.
The resulting allocation rules are robust as they guarantee the aforementioned
convergence properties despite the uncertain and time-varying nature of the
coaltions' values. We highlight three main contributions. First, we design an
allocation rule based on full observation of the extra reward so that the
average allocation approaches a specific point in the core of the average game,
while the coalitions' excesses converge to an a priori given direction. Second,
we design a new allocation rule based on partial observation on the extra
reward so that the average allocation converges to the core of the average
game, while the coalitions' excesses converge to an a priori given cone. And
third, we establish connections to approachability theory and attainability
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1940</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1940</id><created>2011-06-09</created><authors><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>The Degree Sequence of Random Apollonian Networks</title><categories>cs.SI math.PR physics.soc-ph</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the asymptotic behavior of the degree sequence of Random
Apollonian Networks \cite{maximal}. For previous weaker results see
\cite{comment,maximal}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1944</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1944</id><created>2011-06-09</created><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>Operating LDPC Codes with Zero Shaping Gap</title><categories>cs.IT math.IT</categories><comments>Sbmitted to ITW 2011, Paraty</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unequal transition probabilities between input and output symbols, input
power constraints, or input symbols of unequal durations can lead to
non-uniform capacity achieving input distributions for communication channels.
Using uniform input distributions reduces the achievable rate, which is called
the shaping gap. Gallager's idea for reliable communication with zero shaping
gap is to do encoding, matching, and jointly decoding and dematching. In this
work, a scheme is proposed that consists in matching, encoding, decoding, and
dematching. Only matching is channel specific whereas coding is not. Thus
off-the-shelf LDPC codes can be applied. Analytical formulas for shaping and
coding gap of the proposed scheme are derived and it is shown that the shaping
gap can be made zero. Numerical results show that the proposed scheme allows to
operate off-the-shelf LDPC codes with zero shaping gap and a coding gap that is
unchanged compared to uniform transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1953</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1953</id><created>2011-06-10</created><updated>2012-03-12</updated><authors><author><keyname>Trifina</keyname><forenames>Lucian</forenames></author><author><keyname>Tarniceriu</keyname><forenames>Daniela</forenames></author></authors><title>Analysis of cubic permutation polynomials for turbo codes</title><categories>cs.IT math.IT</categories><comments>accepted for publication to Wireless Personal Communications (19
  pages, 4 figures, 5 tables). The final publication is available at
  springerlink.com</comments><journal-ref>Wireless Personal Communications, vol. 69, no. 1, pp. 1-22, March
  2013</journal-ref><doi>10.1007/s11277-012-0557-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadratic permutation polynomials (QPPs) have been widely studied and used as
interleavers in turbo codes. However, less attention has been given to cubic
permutation polynomials (CPPs). This paper proves a theorem which states
sufficient and necessary conditions for a cubic permutation polynomial to be a
null permutation polynomial. The result is used to reduce the search complexity
of CPP interleavers for short lengths (multiples of 8, between 40 and 352), by
improving the distance spectrum over the set of polynomials with the largest
spreading factor. The comparison with QPP interleavers is made in terms of
search complexity and upper bounds of the bit error rate (BER) and frame error
rate (FER) for AWGN and for independent fading Rayleigh channels. Cubic
permutation polynomials leading to better performance than quadratic
permutation polynomials are found for some lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1957</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1957</id><created>2011-06-10</created><authors><author><keyname>Maier</keyname><forenames>Frederick</forenames></author></authors><title>Interdefinability of defeasible logic and logic programming under the
  well-founded semantics</title><categories>cs.AI cs.LO</categories><comments>36 pages; To appear in Theory and Practice of Logic Programming
  (TPLP)</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a method of translating theories of Nute's defeasible logic into
logic programs, and a corresponding translation in the opposite direction.
Under certain natural restrictions, the conclusions of defeasible theories
under the ambiguity propagating defeasible logic ADL correspond to those of the
well-founded semantics for normal logic programs, and so it turns out that the
two formalisms are closely related. Using the same translation of logic
programs into defeasible theories, the semantics for the ambiguity blocking
defeasible logic NDL can be seen as indirectly providing an ambiguity blocking
semantics for logic programs. We also provide antimonotone operators for both
ADL and NDL, each based on the Gelfond-Lifschitz (GL) operator for logic
programs. For defeasible theories without defeaters or priorities on rules, the
operator for ADL corresponds to the GL operator and so can be seen as partially
capturing the consequences according to ADL. Similarly, the operator for NDL
captures the consequences according to NDL, though in this case no restrictions
on theories apply. Both operators can be used to define stable model semantics
for defeasible theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1958</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1958</id><created>2011-06-10</created><authors><author><keyname>Jamall</keyname><forenames>Mohammad Shoaib</forenames></author></authors><title>A Brooks' Theorem for Triangle-Free Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a triangle-free graph with maximum degree \delta(G). We show that
the chromatic number \c{hi}(G) is less than 67(1 + o(1))\delta/ log \delta.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1969</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1969</id><created>2011-06-10</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>The Capacity Region of Multiway Relay Channels Over Finite Fields with
  Full Data Exchange</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory (Special Issue on
  Interference Networks), Vol. 57, No. 5, pp. 3016--3031 May 2011</journal-ref><doi>10.1109/TIT.2011.2120010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-way relay channel is a multicast network where L users exchange
data through a relay. In this paper, the capacity region of a class of
multi-way relay channels is derived, where the channel inputs and outputs take
values over finite fields. The cut-set upper bound to the capacity region is
derived and is shown to be achievable by our proposed functional-decode-forward
coding strategy. More specifically, for the general case where the users can
transmit at possibly different rates, functional-decode-forward, combined with
rate splitting and joint source-channel decoding, is proved to achieve the
capacity region; while for the case where all users transmit at a common rate,
rate splitting and joint source-channel decoding are not required to achieve
the capacity. That the capacity-achieving coding strategies do not utilize the
users' received signals in the users' encoding functions implies that feedback
does not increase the capacity region of this class of multi-way relay
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1973</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1973</id><created>2011-06-10</created><updated>2011-06-16</updated><authors><author><keyname>Mukkamala</keyname><forenames>Padmini</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author></authors><title>Drawing cubic graphs with the four basic slopes</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every cubic graph can be drawn in the plane with straight-line
edges using only the four basic slopes $\{0,\pi/4,\pi/2,3\pi/4\}$. We also
prove that four slopes have this property if and only if we can draw $K_4$ with
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1975</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1975</id><created>2011-06-10</created><updated>2011-07-01</updated><authors><author><keyname>Masmoudi</keyname><forenames>Khaled</forenames></author><author><keyname>Antonini</keyname><forenames>Marc</forenames></author><author><keyname>Kornprobst</keyname><forenames>Pierre</forenames></author></authors><title>Exact Reconstruction of the Rank Order Coding using Frames Theory</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to revisit rank order coding by proposing an original exact
decoding procedure for it. Rank order coding was proposed by Simon Thorpe et
al. who stated that the retina represents the visual stimulus by the order in
which its cells are activated. A classical rank order coder/decoder was then
designed on this basis [1]. Though, it appeared that the decoding procedure
employed yields reconstruction errors that limit the model Rate/Quality
performances when used as an image codec. The attempts made in the literature
to overcome this issue are time consuming and alter the coding procedure, or
are lacking mathematical support and feasibility for standard size images. Here
we solve this problem in an original fashion by using the frames theory, where
a frame of a vector space designates an extension for the notion of basis.
First, we prove that the analyzing filter bank considered is a frame, and then
we define the corresponding dual frame that is necessary for the exact image
reconstruction. Second, to deal with the problem of memory overhead, we design
a recursive out-of-core blockwise algorithm for the computation of this dual
frame. Our work provides a mathematical formalism for the retinal model under
study and defines a simple and exact reverse transform for it with up to 270 dB
of PSNR gain compared to [1]. Furthermore, the framework presented here can be
extended to several models of the visual cortical areas using redundant
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1978</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1978</id><created>2011-06-10</created><updated>2012-06-08</updated><authors><author><keyname>Zhang</keyname><forenames>Chenyi</forenames></author><author><keyname>Pang</keyname><forenames>Jun</forenames></author></authors><title>An Algorithm for Probabilistic Alternating Simulation</title><categories>cs.GT cs.LO</categories><comments>We've fixed a problem in the SOFSEM'12 conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In probabilistic game structures, probabilistic alternating simulation
(PA-simulation) relations preserve formulas defined in probabilistic
alternating-time temporal logic with respect to the behaviour of a subset of
players. We propose a partition based algorithm for computing the largest
PA-simulation, which is to our knowledge the first such algorithm that works in
polynomial time, by extending the generalised coarsest partition problem (GCPP)
in a game-based setting with mixed strategies. The algorithm has higher
complexities than those in the literature for non-probabilistic simulation and
probabilistic simulation without mixed actions, but slightly improves the
existing result for computing probabilistic simulation with respect to mixed
actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.1998</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.1998</id><created>2011-06-10</created><updated>2011-06-13</updated><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames></author><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>A Linear Time Natural Evolution Strategy for Non-Separable Functions</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES
(R1-NES), which uses a low rank approximation of the search distribution
covariance matrix. The algorithm allows computation of the natural gradient
with cost linear in the dimensionality of the parameter space, and excels in
solving high-dimensional non-separable problems, including the best result to
date on the Rosenbrock function (512 dimensions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2007</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2007</id><created>2011-06-10</created><updated>2013-10-22</updated><authors><author><keyname>Mathiesen</keyname><forenames>Joachim</forenames></author><author><keyname>Yde</keyname><forenames>Pernilly</forenames></author><author><keyname>Jensen</keyname><forenames>Mogens H.</forenames></author></authors><title>Modular networks of word correlations on Twitter</title><categories>physics.soc-ph cs.HC cs.SI</categories><comments>9 pages, 4 figures</comments><journal-ref>Scientific Reports 2, 814 (2012)</journal-ref><doi>10.1038/srep00814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks are important tools for analyzing the information flow in
many aspects of nature and human society. Using data from the microblogging
service Twitter, we study networks of correlations in the appearance of words
from three different categories, international brands, nouns and US major
cities. We create networks where the strength of links is determined by a
similarity measure based on the rate of coappearance of words. In comparison
with the null model, where words are assumed to be uncorrelated, the
heavy-tailed distribution of pair correlations is shown to be a consequence of
modules of words representing similar entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2013</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2013</id><created>2011-06-10</created><updated>2012-11-25</updated><authors><author><keyname>Bjelakovic</keyname><forenames>Igor</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Sommerfeld</keyname><forenames>Jochen</forenames></author></authors><title>Secrecy Results for Compound Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>25 pages, 1 figure. Accepted for publication in the journal &quot;Problems
  of Information Transmission&quot;. Some of the results were presented at the ITW
  2011 Paraty [arXiv:1103.0135] and published in the conference paper available
  at the IEEE Xplore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a lower bound on the secrecy capacity of the compound wiretap
channel with channel state information at the transmitter which matches the
general upper bound on the secrecy capacity of general compound wiretap
channels given by Liang et al. and thus establishing a full coding theorem in
this case. We achieve this with a stronger secrecy criterion and the maximum
error probability criterion, and with a decoder that is robust against the
effect of randomisation in the encoding. This relieves us from the need of
decoding the randomisation parameter which is in general not possible within
this model. Moreover we prove a lower bound on the secrecy capacity of the
compound wiretap channel without channel state information and derive a
multi-letter expression for the capacity in this communication scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2025</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2025</id><created>2011-06-10</created><updated>2013-03-14</updated><authors><author><keyname>Maleki</keyname><forenames>Sina</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Censored Truncated Sequential Spectrum Sensing for Cognitive Radio
  Networks</title><categories>cs.SY cs.IT math.IT stat.AP</categories><comments>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6464630&amp;isnumber=6464503</comments><journal-ref>IEEE Journal on Selected Areas in Communications, Vol.31, No.3,
  pp. 364, 378, March 2013</journal-ref><doi>10.1109/JSAC.2013.130304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable spectrum sensing is a key functionality of a cognitive radio
network. Cooperative spectrum sensing improves the detection reliability of a
cognitive radio system but also increases the system energy consumption which
is a critical factor particularly for low-power wireless technologies. A
censored truncated sequential spectrum sensing technique is considered as an
energy-saving approach. To design the underlying sensing parameters, the
maximum energy consumption per sensor is minimized subject to a lower bounded
global probability of detection and an upper bounded false alarm rate. This way
both the interference to the primary user due to miss detection and the network
throughput as a result of a low false alarm rate is controlled. We compare the
performance of the proposed scheme with a fixed sample size censoring scheme
under different scenarios. It is shown that as the sensing cost of the
cognitive radios increases, the energy efficiency of the censored truncated
sequential approach grows significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2050</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2050</id><created>2011-06-10</created><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Multi-User Privacy: The Gray-Wyner System and Generalized Common
  Information</title><categories>cs.IT math.IT</categories><comments>accepted for publication and presentation at ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of preserving privacy when a multivariate source is required to
be revealed partially to multiple users is modeled as a Gray-Wyner source
coding problem with K correlated sources at the encoder and K decoders in which
the kth decoder, k = 1, 2, ...,K, losslessly reconstructs the kth source via a
common link and a private link. The privacy requirement of keeping each decoder
oblivious of all sources other than the one intended for it is introduced via
an equivocation constraint at each decoder such that the total equivocation
summed over all decoders is E. The set of achievable rates-equivocation tuples
is completely characterized. Using this characterization, two different
definitions of common information are presented and are shown to be equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2055</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2055</id><created>2011-06-10</created><authors><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy K.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Channels That Die</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. on Information Theory, vol. 58, no. 9, pp. 5711-5724,
  September 2012</journal-ref><doi>10.1109/TIT.2012.2199078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the possibility of communication systems failing catastrophically, we
investigate limits to communicating over channels that fail at random times.
These channels are finite-state semi-Markov channels. We show that
communication with arbitrarily small probability of error is not possible.
Making use of results in finite blocklength channel coding, we determine
sequences of blocklengths that optimize transmission volume communicated at
fixed maximum message error probabilities. We provide a partial ordering of
communication channels. A dynamic programming formulation is used to show the
structural result that channel state feedback does not improve performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2057</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2057</id><created>2011-06-10</created><authors><author><keyname>Tandon</keyname><forenames>Ravi</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Fellow, IEEE</affiliation></author></authors><title>Discriminatory Lossy Source Coding: Side Information Privacy</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lossy source coding problem is studied in which a source encoder
communicates with two decoders, one with and one without correlated side
information with an additional constraint on the privacy of the side
information at the uninformed decoder. Two cases of this problem arise
depending on the availability of the side information at the encoder. The set
of all feasible rate-distortion-equivocation tuples are characterized for both
cases. The difference between the informed and uninformed cases and the
advantages of encoder side information for enhancing privacy are highlighted
for a binary symmetric source with erasure side information and Hamming
distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2065</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2065</id><created>2011-06-10</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Feige</keyname><forenames>Uriel</forenames></author><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>Oblivious Collaboration</title><categories>cs.DC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication is a crucial ingredient in every kind of collaborative work.
But what is the least possible amount of communication required for a given
task? We formalize this question by introducing a new framework for distributed
computation, called {\em oblivious protocols}.
  We investigate the power of this model by considering two concrete examples,
the {\em musical chairs} task $MC(n,m)$ and the well-known {\em Renaming}
problem. The $MC(n,m)$ game is played by $n$ players (processors) with $m$
chairs. Players can {\em occupy} chairs, and the game terminates as soon as
each player occupies a unique chair. Thus we say that player $P$ is {\em in
conflict} if some other player $Q$ is occupying the same chair, i.e.,
termination means there are no conflicts. By known results from distributed
computing, if $m \le 2n-2$, no strategy of the players can guarantee
termination. However, there is a protocol with $m = 2n-1$ chairs that always
terminates. Here we consider an oblivious protocol where in every time step the
only communication is this: an adversarial {\em scheduler} chooses an arbitrary
nonempty set of players, and for each of them provides only one bit of
information, specifying whether the player is currently in conflict or not. A
player notified not to be in conflict halts and never changes its chair,
whereas a player notified to be in conflict changes its chair according to its
deterministic program. Remarkably, even with this minimal communication
termination can be guaranteed with only $m=2n-1$ chairs. Likewise, we obtain an
oblivious protocol for the Renaming problem whose name-space is small as that
of the optimal nonoblivious distributed protocol.
  Other aspects suggest themselves, such as the efficiency (program length) of
our protocols. We make substantial progress here as well, though many
interesting questions remain open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2075</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2075</id><created>2011-06-10</created><authors><author><keyname>Huillet</keyname><forenames>Thierry</forenames><affiliation>LPTM</affiliation></author></authors><title>A Bose-Einstein Approach to the Random Partitioning of an Integer</title><categories>cond-mat.stat-mech cs.DM math.CO</categories><proxy>ccsd</proxy><doi>10.1088/1742-5468/2011/08/P08021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider N equally-spaced points on a circle of circumference N. Choose at
random n points out of $N$ on this circle and append clockwise an arc of
integral length k to each such point. The resulting random set is made of a
random number of connected components. Questions such as the evaluation of the
probability of random covering and parking configurations, number and length of
the gaps are addressed. They are the discrete versions of similar problems
raised in the continuum. For each value of k, asymptotic results are presented
when n,N both go to infinity according to two different regimes. This model may
equivalently be viewed as a random partitioning problem of N items into n
recipients. A grand-canonical balls in boxes approach is also supplied, giving
some insight into the multiplicities of the box filling amounts or spacings.
The latter model is a k-nearest neighbor random graph with N vertices and kn
edges. We shall also briefly consider the covering problem in the context of a
random graph model with N vertices and n (out-degree 1) edges whose endpoints
are no more bound to be neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2077</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2077</id><created>2011-06-10</created><authors><author><keyname>Quinsat</keyname><forenames>Yann</forenames><affiliation>LURPA</affiliation></author><author><keyname>Lavernhe</keyname><forenames>Sylvain</forenames><affiliation>LURPA</affiliation></author><author><keyname>Lartigue</keyname><forenames>Claire</forenames><affiliation>LURPA</affiliation></author></authors><title>Characterization of 3D surface topography in 5-axis milling</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Wear 271, 3-4 (2010) 590-595</journal-ref><doi>10.1016/j.wear.2010.05.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the context of 5-axis free-form machining, CAM software offers various
ways of tool-path generation, depending on the geometry of the surface to be
machined. Therefore, as the manufactured surface quality results from the
choice of the machining strategy and machining parameters, the prediction of
surface roughness in function of the machining conditions is an important issue
in 5-axis machining. The objective of this paper is to propose a simulation
model of material removal in 5-axis based on the N-buffer method and
integrating the Inverse Kinematics Transformation. The tooth track is linked
with the velocity giving the surface topography resulting from actual machining
conditions. The model is assessed thanks to a series of sweeping over planes
according to various tool axis orientations and cutting conditions. 3D surface
topography analyses are performed through the new areal surface roughness
parameters proposed by recent standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2104</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2104</id><created>2011-06-10</created><authors><author><keyname>Houston</keyname><forenames>Robin</forenames></author><author><keyname>White</keyname><forenames>Joseph</forenames></author><author><keyname>Amos</keyname><forenames>Martyn</forenames></author></authors><title>Zen Puzzle Garden is NP-complete</title><categories>cs.CC</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zen Puzzle Garden (ZPG) is a one-player puzzle game. In this paper, we prove
that deciding the solvability of ZPG is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2109</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2109</id><created>2011-06-10</created><authors><author><keyname>Nozaki</keyname><forenames>Takayuki</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Analysis of Error Floors of Non-Binary LDPC Codes over MBIOS Channel</title><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures, The material in this paper was presented in part
  at IEEE International Conference on Communications, submitted in IEICE
  transaction fundamentals</comments><doi>10.1587/transfun.E94.A.2144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the error floors of non-binary low-density
parity-check (LDPC) codes transmitted over the memoryless binary-input
output-symmetric (MBIOS) channels. We provide a necessary and sufficient
condition for successful decoding of zigzag cycle codes over the MBIOS channel
by the belief propagation decoder. We consider an expurgated ensemble of
non-binary LDPC codes by using the above necessary and sufficient condition,
and hence exhibit lower error floors. Finally, we show lower bounds of the
error floors for the expurgated LDPC code ensembles over the MBIOS channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2113</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2113</id><created>2011-05-27</created><authors><author><keyname>Liu</keyname><forenames>Caixing</forenames></author><author><keyname>Xie</keyname><forenames>Jierui</forenames></author><author><keyname>Hu</keyname><forenames>Yueming</forenames></author></authors><title>Using Hopfield to Solve Resource-Leveling Problem</title><categories>cs.NE</categories><journal-ref>Proceedings of the 11th joint international computer
  conference(JICC), November ,2005, pp:564-567</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the traditional permute matrix coming along with Hopfield is able to
describe many common problems, it seems to have limitation in solving more
complicated problem with more constrains, like resource leveling which is
actually a NP problem. This paper tries to find a better solution for it by
using neural network. In order to give the neural network description of
resource leveling problem, a new description method called Augmented permute
matrix is proposed by expending the ability of the traditional one. An Embedded
Hybrid Model combining Hopfield model and SA are put forward to improve the
optimization in essence in which Hopfield servers as State Generator for the
SA. The experiment results show that Augmented permute matrix is able to
completely and appropriately describe the application. The energy function and
hybrid model given in this study are also highly efficient in solving resource
leveling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2122</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2122</id><created>2011-06-10</created><authors><author><keyname>Praveen</keyname><forenames>M.</forenames></author><author><keyname>Lodaya</keyname><forenames>Kamal</forenames></author></authors><title>Parameterized complexity results for 1-safe Petri nets</title><categories>cs.LO cs.CC cs.DS cs.FL</categories><comments>Full version of the paper appearing in CONCUR 2011</comments><acm-class>F.1.1; F.2.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We associate a graph with a 1-safe Petri net and study the parameterized
complexity of various problems with parameters derived from the graph. With
treewidth as the parameter, we give W[1]-hardness results for many problems
about 1-safe Petri nets. As a corollary, this proves a conjecture of Downey et.
al. about the hardness of some graph pebbling problems. We consider the
parameter benefit depth (that is known to be helpful in getting better
algorithms for general Petri nets) and again give W[1]-hardness results for
various problems on 1-safe Petri nets. We also consider the stronger parameter
vertex cover number. Combining the well known automata-theoretic method and a
powerful fixed parameter tractability (FPT) result about Integer Linear
Programming, we give a FPT algorithm for model checking Monadic Second Order
(MSO) formulas on 1-safe Petri nets, with parameters vertex cover number and
the size of the formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2124</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2124</id><created>2011-06-10</created><authors><author><keyname>Wang</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Gao</keyname><forenames>Hao</forenames></author><author><keyname>Weir</keyname><forenames>Victor</forenames></author><author><keyname>Yu</keyname><forenames>Hengyong</forenames></author><author><keyname>Cong</keyname><forenames>Wenxiang</forenames></author><author><keyname>Xu</keyname><forenames>Xiaochen</forenames></author><author><keyname>Shen</keyname><forenames>Haiou</forenames></author><author><keyname>Bennett</keyname><forenames>James</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Vannier</keyname><forenames>Michael</forenames></author></authors><title>Omni-tomography/Multi-tomography -- Integrating Multiple Modalities for
  Simultaneous Imaging</title><categories>physics.med-ph cs.CV math.NA stat.AP</categories><comments>43 pages, 15 figures, 99 references, provisional patent applications
  filed by Virginia Tech</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current tomographic imaging systems need major improvements, especially when
multi-dimensional, multi-scale, multi-temporal and multi-parametric phenomena
are under investigation. Both preclinical and clinical imaging now depend on in
vivo tomography, often requiring separate evaluations by different imaging
modalities to define morphologic details, delineate interval changes due to
disease or interventions, and study physiological functions that have
interconnected aspects. Over the past decade, fusion of multimodality images
has emerged with two different approaches: post-hoc image registration and
combined acquisition on PET-CT, PET-MRI and other hybrid scanners. There are
intrinsic limitations for both the post-hoc image analysis and dual/triple
modality approaches defined by registration errors and physical constraints in
the acquisition chain. We envision that tomography will evolve beyond current
modality fusion and towards grand fusion, a large scale fusion of all or many
imaging modalities, which may be referred to as omni-tomography or
multi-tomography. Unlike modality fusion, grand fusion is here proposed for
truly simultaneous but often localized reconstruction in terms of all or many
relevant imaging mechanisms such as CT, MRI, PET, SPECT, US, optical, and
possibly more. In this paper, the technical basis for omni-tomography is
introduced and illustrated with a top-level design of a next generation
scanner, interior tomographic reconstructions of representative modalities, and
anticipated applications of omni-tomography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2126</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2126</id><created>2011-06-10</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Bar-Joseph</keyname><forenames>Ziv</forenames></author></authors><title>MIS on the fly</title><categories>cs.DC</categories><comments>have been submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are very good at optimizing solutions for specific problems.
Biological processes, on the other hand, have evolved to handle multiple
constrained distributed environments and so they are robust and adaptable.
Inspired by observations made in a biological system we have recently presented
a simple new randomized distributed MIS algorithm \cite{ZScience}. Here we
extend these results by removing a number of strong assumptions that we made,
making the algorithms more practical. Specifically we present an $O(\log^2 n)$
rounds synchronous randomized MIS algorithm which uses only 1 bit unary
messages (a beeping signal with collision detection), allows for asynchronous
wake up, does not assume any knowledge of the network topology, and assumes
only a loose bound on the network size. We also present an extension with no
collision detection in which the round complexity increases to $(\log^3 n)$.
Finally, we show that our algorithm is optimal under some restriction, by
presenting a tight lower bound of $\Omega(\log^2 n)$ on the number of rounds
required to construct a MIS for a restricted model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2134</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2134</id><created>2011-06-10</created><updated>2012-04-16</updated><authors><author><keyname>Nicosia</keyname><forenames>V.</forenames></author><author><keyname>Tang</keyname><forenames>J.</forenames></author><author><keyname>Musolesi</keyname><forenames>M.</forenames></author><author><keyname>Russo</keyname><forenames>G.</forenames></author><author><keyname>Mascolo</keyname><forenames>C.</forenames></author><author><keyname>Latora</keyname><forenames>V.</forenames></author></authors><title>Components in time-varying graphs</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>12 pages, 4 figures, 3 tables</comments><journal-ref>Chaos, 22, 023101 (2012)</journal-ref><doi>10.1063/1.3697996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real complex systems are inherently time-varying. Thanks to new communication
systems and novel technologies, it is today possible to produce and analyze
social and biological networks with detailed information on the time of
occurrence and duration of each link. However, standard graph metrics
introduced so far in complex network theory are mainly suited for static
graphs, i.e., graphs in which the links do not change over time, or graphs
built from time-varying systems by aggregating all the links as if they were
concurrent in time. In this paper, we extend the notion of connectedness, and
the definitions of node and graph components, to the case of time-varying
graphs, which are represented as time-ordered sequences of graphs defined over
a fixed set of nodes. We show that the problem of finding strongly connected
components in a time-varying graph can be mapped into the problem of
discovering the maximal-cliques in an opportunely constructed static graph,
which we name the affine graph. It is therefore an NP-complete problem. As a
practical example, we have performed a temporal component analysis of
time-varying graphs constructed from three data sets of human interactions. The
results show that taking time into account in the definition of graph
components allows to capture important features of real systems. In particular,
we observe a large variability in the size of node temporal in- and
out-components. This is due to intrinsic fluctuations in the activity patterns
of individuals, which cannot be detected by static graph analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2156</identifier>
 <datestamp>2011-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2156</id><created>2011-06-10</created><authors><author><keyname>Wism&#xfc;ller</keyname><forenames>Axel</forenames></author></authors><title>A Computational Framework for Nonlinear Dimensionality Reduction of
  Large Data Sets: The Exploratory Inspection Machine (XIM)</title><categories>cs.NE</categories><comments>20 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel computational framework for nonlinear
dimensionality reduction which is specifically suited to process large data
sets: the Exploratory Inspection Machine (XIM). XIM introduces a conceptual
cross-link between hitherto separate domains of machine learning, namely
topographic vector quantization and divergence-based neighbor embedding
approaches. There are three ways to conceptualize XIM, namely (i) as the
inversion of the Exploratory Observation Machine (XOM) and its variants, such
as Neighbor Embedding XOM (NE-XOM), (ii) as a powerful optimization scheme for
divergence-based neighbor embedding cost functions inspired by Stochastic
Neighbor Embedding (SNE) and its variants, such as t-distributed SNE (t-SNE),
and (iii) as an extension of topographic vector quantization methods, such as
the Self-Organizing Map (SOM). By preserving both global and local data
structure, XIM combines the virtues of classical and advanced recent embedding
methods. It permits direct visualization of large data collections without the
need for prior data reduction. Finally, XIM can contribute to many application
domains of data analysis and visualization important throughout the sciences
and engineering, such as pattern matching, constrained incremental learning,
data clustering, and the analysis of non-metric dissimilarity data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2176</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2176</id><created>2011-06-10</created><updated>2011-10-16</updated><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Barba</keyname><forenames>Lorena</forenames></author></authors><title>A Tuned and Scalable Fast Multipole Method as a Preeminent Algorithm for
  Exascale Systems</title><categories>cs.NA cs.MS</categories><msc-class>70F10</msc-class><acm-class>D.1.3; G.1.0; G.1.2</acm-class><doi>10.1177/1094342011429952</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the algorithms that are likely to play a major role in future exascale
computing, the fast multipole method (FMM) appears as a rising star. Our
previous recent work showed scaling of an FMM on GPU clusters, with problem
sizes in the order of billions of unknowns. That work led to an extremely
parallel FMM, scaling to thousands of GPUs or tens of thousands of CPUs. This
paper reports on a a campaign of performance tuning and scalability studies
using multi-core CPUs, on the Kraken supercomputer. All kernels in the FMM were
parallelized using OpenMP, and a test using 10^7 particles randomly distributed
in a cube showed 78% efficiency on 8 threads. Tuning of the
particle-to-particle kernel using SIMD instructions resulted in 4x speed-up of
the overall algorithm on single-core tests with 10^3 - 10^7 particles. Parallel
scalability was studied in both strong and weak scaling. The strong scaling
test used 10^8 particles and resulted in 93% parallel efficiency on 2048
processes for the non-SIMD code and 54% for the SIMD-optimized code (which was
still 2x faster). The weak scaling test used 10^6 particles per process, and
resulted in 72% efficiency on 32,768 processes, with the largest calculation
taking about 40 seconds to evaluate more than 32 billion unknowns. This work
builds up evidence for our view that FMM is poised to play a leading role in
exascale computing, and we end the paper with a discussion of the features that
make it a particularly favorable algorithm for the emerging heterogeneous and
massively parallel architectural landscape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2181</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2181</id><created>2011-06-10</created><updated>2013-06-21</updated><authors><author><keyname>Song</keyname><forenames>Lei</forenames><affiliation>IT University of Copenhagen, Denmark</affiliation></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames><affiliation>DTU Informatics, Technical University of Denmark</affiliation></author><author><keyname>Godskesen</keyname><forenames>Jens Chr.</forenames><affiliation>IT University of Copenhagen, Denmark</affiliation></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames><affiliation>DTU Compute, Technical University of Denmark</affiliation></author></authors><title>Bisimulations Meet PCTL Equivalences for Probabilistic Automata</title><categories>cs.LO</categories><comments>Long version of CONCUR'11 with the same title: added extension to
  simulations, countable states</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 21,
  2013) lmcs:1238</journal-ref><doi>10.2168/LMCS-9(2:7)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic automata (PAs) have been successfully applied in formal
verification of concurrent and stochastic systems. Efficient model checking
algorithms have been studied, where the most often used logics for expressing
properties are based on probabilistic computation tree logic (PCTL) and its
extension PCTL^*. Various behavioral equivalences are proposed, as a powerful
tool for abstraction and compositional minimization for PAs. Unfortunately, the
equivalences are well-known to be sound, but not complete with respect to the
logical equivalences induced by PCTL or PCTL*. The desire of a both sound and
complete behavioral equivalence has been pointed out by Segala in 1995, but
remains open throughout the years. In this paper we introduce novel notions of
strong bisimulation relations, which characterize PCTL and PCTL* exactly. We
extend weak bisimulations that characterize PCTL and PCTL* without next
operator, respectively. Further, we also extend the framework to simulation
preorders. Thus, our paper bridges the gap between logical and behavioral
equivalences and preorders in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2200</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2200</id><created>2011-06-11</created><authors><author><keyname>Zhuang</keyname><forenames>Yanyan</forenames></author><author><keyname>Pan</keyname><forenames>Jianping</forenames></author></authors><title>Random Distances Associated with Hexagons</title><categories>math.GM cs.NI</categories><comments>brief results announcement, based on http://arxiv.org/abs/1106.1257</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, the explicit probability density functions of the random
Euclidean distances associated with regular hexagons are given, when the two
endpoints of a link are randomly distributed in the same hexagon, and two
adjacent hexagons sharing a side, respectively. Simulation results show the
accuracy of the obtained closed-form distance distribution functions, which are
important in a wide variety of applied sciences and engineering fields. In
particular, hexagons are often used in wireless communication networks such as
the cellular systems. The correctness of these distance distribution functions
is validated by a recursion and a probabilistic sum. The first two statistical
moments of the random distances, and the polynomial fits of the density
functions are also given in this report for practical uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2207</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2207</id><created>2011-06-11</created><authors><author><keyname>Lyonnet</keyname><forenames>Barbara</forenames><affiliation>SYMME</affiliation></author><author><keyname>Pillet</keyname><forenames>Maurice</forenames><affiliation>SYMME</affiliation></author><author><keyname>Pralus</keyname><forenames>Magali</forenames><affiliation>SYMME</affiliation></author></authors><title>Optimisation de la taille de la s\'erie: illustration par un cas
  industriel de sous-traitance m\'ecanique</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Congr\`es International de G\'enie Industriel CIGI'09, Tarbes :
  France (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing costs of manufactured products is one of the key issues of
companies. Bar turning companies (mechanical subcontracting companies) are
faced with the following dilemma: use a pull strategy or use a push strategy.
Instinctively these companies produce more than demand required by customers.
This strategy allows them to respond to requests forecasts and reduce their
cost of changeover time. These companies make a bet on sales opportunities and
think to realize an additional profit. We have tried to find in this study to
provide elements to know the limits of this strategy. Our proposal focuses on
developing a model to support the decision taking into account the mix of
opportunities, economic constraints and mean constraints. This model features
the particular importance of high rates of ownership and the risk of not
selling.
  R\'eduire les co\^uts de revient des produits fabriqu\'es est une des
probl\'ematiques essentielles des entreprises d'aujourd'hui. Les entreprises de
d\'ecolletage (entreprises de sous-traitance m\'ecanique) sont confront\'ees au
dilemme suivant : produire juste la demande client ou produire plus.
Instinctivement ces entreprises, dont les temps de changement de s\'erie sont
\'elev\'es, cherchent \`a produire plus que la demande exig\'ee par le client.
Cette strat\'egie leur permet de r\'epondre \`a des demandes pr\'evisionnelles
et r\'eduire ainsi le co\^ut de revient des produits. Ces entreprises
r\'ealisent un pari sur les opportunit\'es de vente et pensent r\'ealiser un
gain suppl\'ementaire en r\'ealisant des stocks. Nous avons cherch\'e dans
cette \'etude \`a fournir des \'el\'ements de d\'ecision pour conna\^itre les
limites de cette r\`egle de gestion. Notre proposition porte sur le
d\'eveloppement d'un mod\`ele d'aide \`a la d\'ecision prenant en
consid\'eration le mixte entre opportunit\'es commerciales, contraintes
\'economiques et contraintes de moyen. Ce mod\`ele souligne l'importance
particuli\`ere du taux de possession et du risque de non vente.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2217</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2217</id><created>2011-06-11</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>An application of Grossone to the study of a family of tilings of the
  hyperbolic plane</title><categories>cs.DM</categories><comments>25 pages, 13 figures. The paper will appear in the journal Applied
  Mathematics and Computations, see below at DOI</comments><msc-class>68R01</msc-class><acm-class>F.2.1; G.1.m; G.2.0</acm-class><doi>10.1016/j.amc.2011.04.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look at the improvement of our knowledge on a family of
tilings of the hyperbolic plane which is brought in by the use of Sergeyev's
numeral system based on grossone. It appears that the information we can get by
using this new numeral system depends on the way we look at the tilings. The
ways are significantly different but they confirm some results which were
obtained in the traditional but constructive frame and allow us to obtain an
additional precision with respect to this information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2229</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2229</id><created>2011-06-11</created><authors><author><keyname>Contreras</keyname><forenames>Pedro</forenames></author><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>Fast, Linear Time Hierarchical Clustering using the Baire Metric</title><categories>stat.ML cs.IR stat.AP</categories><comments>27 pages, 6 tables, 10 figures</comments><msc-class>11Z05</msc-class><acm-class>H.3.3</acm-class><journal-ref>Journal of Classification, July 2012, Volume 29, Issue 2, pp
  118-143</journal-ref><doi>10.1007/s00357-012-9106-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Baire metric induces an ultrametric on a dataset and is of linear
computational complexity, contrasted with the standard quadratic time
agglomerative hierarchical clustering algorithm. In this work we evaluate
empirically this new approach to hierarchical clustering. We compare
hierarchical clustering based on the Baire metric with (i) agglomerative
hierarchical clustering, in terms of algorithm properties; (ii) generalized
ultrametrics, in terms of definition; and (iii) fast clustering through k-means
partititioning, in terms of quality of results. For the latter, we carry out an
in depth astronomical study. We apply the Baire distance to spectrometric and
photometric redshifts from the Sloan Digital Sky Survey using, in this work,
about half a million astronomical objects. We want to know how well the (more
costly to determine) spectrometric redshifts can predict the (more easily
obtained) photometric redshifts, i.e. we seek to regress the spectrometric on
the photometric redshifts, and we use clusterwise regression for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2233</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2233</id><created>2011-06-11</created><authors><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Nefedov</keyname><forenames>Nikolai</forenames></author></authors><title>Clustering with Multi-Layer Graphs: A Spectral Perspective</title><categories>cs.LG cs.CV cs.SI stat.ML</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 60, no. 11, pp.
  5820-5831, November 2012</journal-ref><doi>10.1109/TSP.2012.2212886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observational data usually comes with a multimodal nature, which means that
it can be naturally represented by a multi-layer graph whose layers share the
same set of vertices (users) with different edges (pairwise relationships). In
this paper, we address the problem of combining different layers of the
multi-layer graph for improved clustering of the vertices compared to using
layers independently. We propose two novel methods, which are based on joint
matrix factorization and graph regularization framework respectively, to
efficiently combine the spectrum of the multiple graph layers, namely the
eigenvectors of the graph Laplacian matrices. In each case, the resulting
combination, which we call a &quot;joint spectrum&quot; of multiple graphs, is used for
clustering the vertices. We evaluate our approaches by simulations with several
real world social network datasets. Results demonstrate the superior or
competitive performance of the proposed methods over state-of-the-art technique
and common baseline methods, such as co-regularization and summation of
information from individual graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2263</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2263</id><created>2011-06-11</created><authors><author><keyname>Antunes</keyname><forenames>David Miguel</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Gaspar</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>A Library for Implementing the Multiple Hypothesis Tracking Algorithm</title><categories>cs.DS cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multiple Hypothesis Tracking (MHT) algorithm is known to produce good
results in difficult multi-target tracking situations. However, its
implementation is not trivial, and is associated with a significant programming
effort, code size and long implementation time. We propose a library which
addresses these problems by providing a domain independent implementation of
the most complex MHT operations. We also address the problem of applying
clustering in domain independent manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2272</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2272</id><created>2011-06-11</created><authors><author><keyname>Xu</keyname><forenames>Wenyan</forenames></author><author><keyname>Liu</keyname><forenames>Sanyang</forenames></author></authors><title>Soundness and completeness of the cirquent calculus system CL6 for
  computability logic</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Computability logic is a formal theory of computability. The earlier article
&quot;Introduction to cirquent calculus and abstract resource semantics&quot; by
Japaridze proved soundness and completeness for the basic fragment CL5 of
computability logic. The present article extends that result to the more
expressive cirquent calculus system CL6, which is a conservative extension of
both CL5 and classical propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2275</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2275</id><created>2011-06-12</created><authors><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>Byzantine Fault Tolerance of Regenerating Codes</title><categories>cs.DC cs.CR</categories><comments>In The 11th IEEE International Conference on Peer-to-Peer Computing
  (P2P 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed a slew of coding techniques custom designed for
networked storage systems. Network coding inspired regenerating codes are the
most prolifically studied among these new age storage centric codes. A lot of
effort has been invested in understanding the fundamental achievable trade-offs
of storage and bandwidth usage to maintain redundancy in presence of different
models of failures, showcasing the efficacy of regenerating codes with respect
to traditional erasure coding techniques. For practical usability in open and
adversarial environments, as is typical in peer-to-peer systems, we need
however not only resilience against erasures, but also from (adversarial)
errors. In this paper, we study the resilience of generalized regenerating
codes (supporting multi-repairs, using collaboration among newcomers) in the
presence of two classes of Byzantine nodes, relatively benign selfish
(non-cooperating) nodes, as well as under more active, malicious polluting
nodes. We give upper bounds on the resilience capacity of regenerating codes,
and show that the advantages of collaborative repair can turn to be detrimental
in the presence of Byzantine nodes. We further exhibit that system mechanisms
can be combined with regenerating codes to mitigate the effect of rogue nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2289</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2289</id><created>2011-06-12</created><authors><author><keyname>Bouramoul</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Kholladi</keyname><forenames>Mohamed-Khireddine</forenames></author><author><keyname>Doan</keyname><forenames>Bich-Lien</forenames></author></authors><title>PRESY: A Context Based Query Reformulation Tool for Information
  Retrieval on the Web</title><categories>cs.IR</categories><comments>8 pages</comments><journal-ref>Journal of Computer Science (JCS) - ISSN: 15493636, Vol.6, No.4 :
  470-477, April 2010</journal-ref><doi>10.3844/jcssp.2010.470.477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problem Statement: The huge number of information on the web as well as the
growth of new inexperienced users creates new challenges for information
retrieval. It has become increasingly difficult for these users to find
relevant documents that satisfy their individual needs. Certainly the current
search engines (such as Google, Bing and Yahoo) offer an efficient way to
browse the web content. However, the result quality is highly based on uses
queries which need to be more precise to find relevant documents. This task
still complicated for the majority of inept users who cannot express their
needs with significant words in the query. For that reason, we believe that a
reformulation of the initial user's query can be a good alternative to improve
the information selectivity. This study proposes a novel approach and presents
a prototype system called PRESY (Profile-based REformulation SYstem) for
information retrieval on the web. Approach: It uses an incremental approach to
categorize users by constructing a contextual base. The latter is composed of
two types of context (static and dynamic) obtained using the users' profiles.
The architecture proposed was implemented using .Net environment to perform
queries reformulating tests. Results: The experiments gives at the end of this
article show that the precision of the returned content is effectively
improved. The tests were performed with the most popular searching engine (i.e.
Google, Bind and Yahoo) selected in particular for their high selectivity.
Among the given results, we found that query reformulation improve the first
three results by 10.7% and 11.7% of the next seven returned elements. So as we
can see the reformulation of users' initial queries improves the pertinence of
returned content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2290</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2290</id><created>2011-06-12</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>Using Grossone to count the number of elements of infinite sets and the
  connection with bijections</title><categories>cs.DM</categories><comments>13 pages, 2 figures, a slightly different variant is accepted by the
  journal &quot;p-Adic Numbers, Ultrametric Analysis and Applications&quot;</comments><msc-class>68R01</msc-class><acm-class>F.2.1; G.1.m; G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look at how to count the number of elements of a set within
the frame of Sergeyev's numeral system. We also look at the connection between
the number of elements of a set and the notion of bijection in this new
setting. We also show the difference between this new numeral system and the
results of the traditional naive set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2294</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2294</id><created>2011-06-12</created><authors><author><keyname>Liao</keyname><forenames>Kewen</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author></authors><title>Unconstrained and Constrained Fault-Tolerant Resource Allocation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, we study the Unconstrained Fault-Tolerant Resource Allocation (UFTRA)
problem (a.k.a. FTFA problem in \cite{shihongftfa}). In the problem, we are
given a set of sites equipped with an unconstrained number of facilities as
resources, and a set of clients with set $\mathcal{R}$ as corresponding
connection requirements, where every facility belonging to the same site has an
identical opening (operating) cost and every client-facility pair has a
connection cost. The objective is to allocate facilities from sites to satisfy
$\mathcal{R}$ at a minimum total cost. Next, we introduce the Constrained
Fault-Tolerant Resource Allocation (CFTRA) problem. It differs from UFTRA in
that the number of resources available at each site $i$ is limited by $R_{i}$.
Both problems are practical extensions of the classical Fault-Tolerant Facility
Location (FTFL) problem \cite{Jain00FTFL}. For instance, their solutions
provide optimal resource allocation (w.r.t. enterprises) and leasing (w.r.t.
clients) strategies for the contemporary cloud platforms.
  In this paper, we consider the metric version of the problems. For UFTRA with
uniform $\mathcal{R}$, we present a star-greedy algorithm. The algorithm
achieves the approximation ratio of 1.5186 after combining with the cost
scaling and greedy augmentation techniques similar to
\cite{Charikar051.7281.853,Mahdian021.52}, which significantly improves the
result of \cite{shihongftfa} using a phase-greedy algorithm. We also study the
capacitated extension of UFTRA and give a factor of 2.89. For CFTRA with
uniform $\mathcal{R}$, we slightly modify the algorithm to achieve
1.5186-approximation. For a more general version of CFTRA, we show that it is
reducible to FTFL using linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2301</identifier>
 <datestamp>2012-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2301</id><created>2011-06-12</created><updated>2012-08-07</updated><authors><author><keyname>Yakhontov</keyname><forenames>Sergey V.</forenames></author></authors><title>A simple algorithm for the evaluation of the hypergeometric series using
  quasi-linear time and linear space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple algorithm with quasi-linear time complexity and linear space
complexity for the evaluation of the hypergeometric series with rational
coefficients is constructed. It is shown that this algorithm is suitable in
practical informatics for constructive analogues of often used constants of
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2305</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2305</id><created>2011-06-12</created><updated>2012-07-16</updated><authors><author><keyname>Nguyen</keyname><forenames>Linh Anh</forenames></author></authors><title>Cut-Free ExpTime Tableaux for Checking Satisfiability of a Knowledge
  Base in the Description Logic SHI</title><categories>cs.LO</categories><comments>a long version of the paper &quot;Linh Anh Nguyen. A Cut-Free ExpTime
  Tableau Decision Procedure for the Description Logic SHI. In Proceedings of
  ICCCI'2011, LNAI 6922, pages 572-581, Springer-Verlag, 2011&quot;, 27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first cut-free ExpTime (optimal) tableau decision procedure for
checking satisfiability of a knowledge base in the description logic SHI, which
extends the description logic ALC with transitive roles, inverse roles and role
hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2312</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2312</id><created>2011-06-12</created><authors><author><keyname>Rathipriya</keyname><forenames>R.</forenames></author><author><keyname>Thangavel</keyname><forenames>Dr. K.</forenames></author><author><keyname>Bagyamani</keyname><forenames>J.</forenames></author></authors><title>Evolutionary Biclustering of Clickstream Data</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biclustering is a two way clustering approach involving simultaneous
clustering along two dimensions of the data matrix. Finding biclusters of web
objects (i.e. web users and web pages) is an emerging topic in the context of
web usage mining. It overcomes the problem associated with traditional
clustering methods by allowing automatic discovery of browsing pattern based on
a subset of attributes. A coherent bicluster of clickstream data is a local
browsing pattern such that users in bicluster exhibit correlated browsing
pattern through a subset of pages of a web site. This paper proposed a new
application of biclustering to web data using a combination of heuristics and
meta-heuristics such as K-means, Greedy Search Procedure and Genetic Algorithms
to identify the coherent browsing pattern. Experiment is conducted on the
benchmark clickstream msnbc dataset from UCI repository. Results demonstrate
the efficiency and beneficial outcome of the proposed method by correlating the
users and pages of a web site in high degree.This approach shows excellent
performance at finding high degree of overlapped coherent biclusters from web
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2320</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2320</id><created>2011-06-12</created><authors><author><keyname>Barreto</keyname><forenames>Raimundo</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas</forenames></author><author><keyname>Fischer</keyname><forenames>Bernd</forenames></author></authors><title>Verifying Embedded C Software with Timing Constraints using an Untimed
  Model Checker</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded systems are everywhere, from home appliances to critical systems
such as medical devices. They usually have associated timing constraints that
need to be verified for the implementation. Here, we use an untimed bounded
model checker to verify timing properties of embedded C programs. We propose an
approach to specify discrete time timing constraints using code annotations.
The annotated code is then automatically translated to code that manipulates
auxiliary timer variables and is thus suitable as input to conventional,
untimed software model checker such as ESBMC. Thus, we can check timing
constraints in the same way and at the same time as untimed system
requirements, and even allow for interaction between them. We applied the
proposed method in a case study, and verified timing constraints of a pulse
oximeter, a noninvasive medical device that measures the oxygen saturation of
arterial blood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2325</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2325</id><created>2011-06-12</created><authors><author><keyname>Hou</keyname><forenames>Shujie</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author><author><keyname>Chen</keyname><forenames>Zhe</forenames></author><author><keyname>Hu</keyname><forenames>Zhen</forenames></author></authors><title>SVM and Dimensionality Reduction in Cognitive Radio with Experimental
  Validation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a trend of applying machine learning algorithms to cognitive radio.
One fundamental open problem is to determine how and where these algorithms are
useful in a cognitive radio network. In radar and sensing signal processing,
the control of degrees of freedom (DOF)---or dimensionality---is the first
step, called pre-processing. In this paper, the combination of dimensionality
reduction with SVM is proposed apart from only applying SVM for classification
in cognitive radio. Measured Wi-Fi signals with high signal to noise ratio
(SNR) are employed to the experiments. The DOF of Wi-Fi signals is extracted by
dimensionality reduction techniques. Experimental results show that with
dimensionality reduction, the performance of classification is much better with
fewer features than that of without dimensionality reduction. The error rates
of classification with only one feature of the proposed algorithm can match the
error rates of 13 features of the original data. The proposed method will be
further tested in our cognitive radio network testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2327</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2327</id><created>2011-06-12</created><authors><author><keyname>Mudunuru</keyname><forenames>M. K.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>A framework for coupled deformation-diffusion analysis with application
  to degradation/healing</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the formulation and numerical implementation of a fully
coupled continuum model for deformation-diffusion in linearized elastic solids.
The mathematical model takes into account the effect of the deformation on the
diffusion process, and the affect of the transport of an inert chemical species
on the deformation of the solid. We then present a robust computational
framework for solving the proposed mathematical model, which consists of
coupled non-linear partial differential equations. It should be noted that many
popular numerical formulations may produce unphysical negative values for the
concentration, particularly, when the diffusion process is anisotropic. The
violation of the non-negative constraint by these numerical formulations is not
mere numerical noise. In the proposed computational framework we employ a novel
numerical formulation that will ensure that the concentration of the diffusant
be always non-negative, which is one of the main contributions of this paper.
Representative numerical examples are presented to show the robustness,
convergence, and performance of the proposed computational framework. Another
contribution of this paper is to systematically study the affect of transport
of the diffusant on the deformation of the solid and vice-versa, and their
implication in modeling degradation/healing of materials. We show that the
coupled response is both qualitatively and quantitatively different from the
uncoupled response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2351</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2351</id><created>2011-06-12</created><authors><author><keyname>Ilic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Ilic</keyname><forenames>Andreja</forenames></author></authors><title>On vertex covers and matching number of trapezoid graphs</title><categories>cs.DS</categories><comments>9 pages, 1 figure, 4 algorithms</comments><msc-class>05C85, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intersection graph of a collection of trapezoids with corner points lying
on two parallel lines is called a trapezoid graph. Using binary indexed tree
data structure, we improve algorithms for calculating the size and the number
of minimum vertex covers (or independent sets), as well as the total number of
vertex covers, and reduce the time complexity from $O (n^2)$ to $O (n \log n)$,
where $n$ is the number of trapezoids. Furthermore, we present the family of
counterexamples for recently proposed algorithm with time complexity $O (n^2)$
for calculating the maximum cardinality matching in trapezoid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2352</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2352</id><created>2011-06-12</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author><author><keyname>State</keyname><forenames>Luminita</forenames></author></authors><title>Cognitive Binary Logic - The Natural Unified Formal Theory of
  Propositional Binary Logic</title><categories>cs.LO math.LO</categories><msc-class>03B70, 97E30, 03F03</msc-class><acm-class>F.4.1</acm-class><journal-ref>Recent Advances in Computational Intelligence, Proc. 4th
  International Conference on Computational Intelligence, pp. 135-142, ISSN:
  1790-5117, ISBN 978-960-474-179-3, WSEAS Press, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal theory which describes propositional binary
logic as a semantically closed formal language, and allows for syntactically
and semantically well-formed formulae, formal proofs (demonstrability in
Hilbertian acception), deduction (Gentzen's view of demonstrability),
CNF-ization, and deconstruction to be expressed and tested in the same
(computational) formal language, using the same data structure. It is also
shown here that Cognitive Binary Logic is a self-described theory in which the
Liar Paradox is deconstructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2354</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2354</id><created>2011-06-12</created><authors><author><keyname>Jim&#xe9;nez-P&#xe9;rez</keyname><forenames>Hugo</forenames></author><author><keyname>de Medrano</keyname><forenames>Santiago L&#xf3;pez</forenames></author></authors><title>On Alfred Gray's Elliptical Catenoid</title><categories>math.DG cs.CG</categories><comments>6 pages, 5 figures, Proceedings of the International Congress on
  Differential Geometry 2000, in Honour of Alfred Gray</comments><msc-class>53A10, 33E05</msc-class><journal-ref>Contemporary Mathematics 288, AMS, pp 348-352, 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a parameterization of Alfred Gray's Elliptical Catenoid and
Elliptical Hellicoid using Jacobi's elliptic functions. This parameterization
avoids some problems present in the original depiction of these surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2357</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2357</id><created>2011-06-12</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author><author><keyname>Balas</keyname><forenames>Valentina E.</forenames></author></authors><title>Comparing Haar-Hilbert and Log-Gabor Based Iris Encoders on Bath Iris
  Image Database</title><categories>cs.CV</categories><comments>6 pages, 4 figures, latest version: http://fmi.spiruharet.ro/bodorin/</comments><msc-class>68T10</msc-class><acm-class>I.5</acm-class><journal-ref>Proc. 4th International Workshop on Soft Computing Applications,
  pp. 191-196, IEEE Press, July 2010</journal-ref><doi>10.1109/SOFA.2010.5565599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This papers introduces a new family of iris encoders which use 2-dimensional
Haar Wavelet Transform for noise attenuation, and Hilbert Transform to encode
the iris texture. In order to prove the usefulness of the newly proposed iris
encoding approach, the recognition results obtained by using these new encoders
are compared to those obtained using the classical Log- Gabor iris encoder.
Twelve tests involving single/multienrollment and conducted on Bath Iris Image
Database are presented here. One of these tests achieves an Equal Error Rate
comparable to the lowest value reported so far for this database. New Matlab
tools for iris image processing are also released together with this paper: a
second version of the Circular Fuzzy Iris Segmentator (CFIS2), a fast Log-Gabor
encoder and two Haar-Hilbert based encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2363</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2363</id><created>2011-06-12</created><updated>2014-03-24</updated><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Random design analysis of ridge regression</title><categories>math.ST cs.AI cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work gives a simultaneous analysis of both the ordinary least squares
estimator and the ridge regression estimator in the random design setting under
mild assumptions on the covariate/response distributions. In particular, the
analysis provides sharp results on the ``out-of-sample'' prediction error, as
opposed to the ``in-sample'' (fixed design) error. The analysis also reveals
the effect of errors in the estimated covariance structure, as well as the
effect of modeling errors, neither of which effects are present in the fixed
design setting. The proofs of the main results are based on a simple
decomposition lemma combined with concentration inequalities for random vectors
and matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2369</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2369</id><created>2011-06-12</created><authors><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Efficient Optimal Learning for Contextual Bandits</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of learning in an online setting where the learner
repeatedly observes features, selects among a set of actions, and receives
reward for the action taken. We provide the first efficient algorithm with an
optimal regret. Our algorithm uses a cost sensitive classification learner as
an oracle and has a running time $\mathrm{polylog}(N)$, where $N$ is the number
of classification rules among which the oracle might choose. This is
exponentially faster than all previous algorithms that achieve optimal regret
in this setting. Our formulation also enables us to create an algorithm with
regret that is additive rather than multiplicative in feedback delay as in all
previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2378</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2378</id><created>2011-06-12</created><authors><author><keyname>Iwasaki</keyname><forenames>Atsushi</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author><author><keyname>Salek</keyname><forenames>Mahyar</forenames></author><author><keyname>Yokoo</keyname><forenames>Makoto</forenames></author></authors><title>False-name-proof Mechanisms for Hiring a Team</title><categories>cs.GT cs.CC</categories><comments>23 pages, 3 figures, This paper is an extented version of
  &quot;False-name-proof Mechanisms for Hiring a Team&quot; in the Proceedings of the
  Workshop on Internet and Network Economics, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of hiring a team of selfish agents to perform a task.
Each agent is assumed to own one or more elements of a set system, and the
auctioneer is trying to purchase a feasible solution by conducting an auction.
Our goal is to design auctions that are truthful and false-name-proof, meaning
that it is in the agents' best interest to reveal ownership of all elements
(which may not be known to the auctioneer a priori) as well as their true
incurred costs.
  We first propose and analyze a false-name-proof mechanism for the special
case where each agent owns only one element in reality, but may pretend that
this element is in fact a set of multiple elements. We prove that its frugality
ratio is bounded by $2^n$, which, up to constants, matches a lower bound of
$\Omega(2^n)$ for all false-name-proof mechanisms in this scenario. We then
propose a second mechanism for the general case in which agents may own
multiple elements. It requires the auctioneer to choose a reserve cost a
priori, and thus does not always purchase a solution. In return, it is
false-name-proof even when agents own multiple elements. We experimentally
evaluate the payment (as well as social surplus) of the second mechanism
through simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2380</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2380</id><created>2011-06-12</created><updated>2011-07-12</updated><authors><author><keyname>Gonzalez-Horta</keyname><forenames>Francisco A.</forenames></author><author><keyname>Enriquez-Caldera</keyname><forenames>Rogerio A.</forenames></author><author><keyname>Ramirez-Cortes</keyname><forenames>Juan M.</forenames></author><author><keyname>Martinez-Carballido</keyname><forenames>Jorge</forenames></author><author><keyname>Buenfil-Alpuche</keyname><forenames>Eldamira</forenames></author></authors><title>Mathematical Model for the Optimal Utilization Percentile in M/M/1
  Systems: A Contribution about Knees in Performance Curves</title><categories>cs.PF</categories><comments>7 pages, 5 figures, 1 table, accepted for publication at The Third
  International Conference on Adaptive and Self-Adaptive Systems and
  Applications (ADAPTIVE 2011), September 25-30, 2011 - Rome, Italy. ISBN:
  978-1-61208-011-6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance curves of queuing systems can be analyzed by separating them into
three regions: the flat region, the knee region, and the exponential region.
Practical considerations, usually locate the knee region between 70-90% of the
theoretical maximum utilization. However, there is not a clear agreement about
where the boundaries between regions are, and where exactly the utilization
knee is located. An open debate about knees in performance curves was
undertaken at least 20 years ago. This historical debate is mainly divided
between those who claim that a knee in the curve is not a well-defined term in
mathematics, or it is a subjective and not really meaningful concept, and those
who define knees mathematically and consider their relevance and application.
In this paper, we present a mathematical model and analysis for identifying the
three mentioned regions on performance curves for M/M/1 systems; specifically,
we found the knees, or optimal utilization percentiles, at the vertices of the
hyperbolas that relate response time as a function of utilization. Using these
results, we argue that an adaptive and optimal queuing system could be deployed
by keeping load and throughput within the knee region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2402</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2402</id><created>2011-06-13</created><authors><author><keyname>Savitha</keyname><forenames>K.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>DR. C.</forenames></author></authors><title>Network Selection Using TOPSIS in Vertical Handover Decision Schemes for
  Heterogeneous Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Handover&quot; is one of the techniques used to achieve the service continuity in
Fourth generation wireless networks (FGWNs). Seamless continuity is the main
goal in fourth generation Wireless networks (FGWNs), when a mobile terminal
(MT) is in overlapping area for service continuity Handover mechanism are
mainly used While moving in the heterogeneous wireless networks continual
connection is the main challenge. Vertical handover is used as a technique to
minimize the processing delay in heterogeneous wireless networks this paper,
Vertical handover decision schemes are compared and Technique of order
preference by similarity to ideal solution (TOPSIS) in a distributed manner.
TOPSIS is used to choose the best network from the available Visitor networks
(VTs) for the continuous connection by the mobile terminal. In our work we
mainly concentrated to the handover decision Phase and to reduce the processing
delay in the period of handover
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2404</identifier>
 <datestamp>2012-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2404</id><created>2011-06-13</created><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Some Results on the Information Loss in Dynamical Systems</title><categories>cs.IT math.IT nlin.SI</categories><comments>6 pages, 2 figures, submitted to a conference</comments><report-no>(c) IEEE 2011</report-no><journal-ref>Proc. IEEE Int. Sym. Wireless Communication Systems, 2011, pp. 794
  - 798</journal-ref><doi>10.1109/ISWCS.2011.6125271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the information loss in (nonlinear) dynamical
input-output systems and provide some general results. In particular, we
present an upper bound on the information loss rate, defined as the
(non-negative) difference between the entropy rates of the jointly stationary
stochastic processes at the input and output of the system.
  We further introduce a family of systems with vanishing information loss
rate. It is shown that not only linear filters belong to that family, but -
under certain circumstances - also finite-precision implementations of the
latter, which typically consist of nonlinear elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2407</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2407</id><created>2011-06-13</created><updated>2011-06-28</updated><authors><author><keyname>Blanco</keyname><forenames>V.</forenames></author><author><keyname>Ben-Ali</keyname><forenames>S. El-Haj</forenames></author><author><keyname>Puerto</keyname><forenames>J.</forenames></author></authors><title>A Semidefinite Programming approach for minimizing ordered weighted
  averages of rational functions</title><categories>math.OC cs.NA</categories><comments>27 pages, 1 figure, 7 tables</comments><msc-class>90B85, 90C22, 65K05, 12Y05, 46N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of minimizing the ordered weighted average
(or ordered median) function of finitely many rational functions over compact
semi-algebraic sets. Ordered weighted averages of rational functions are not,
in general, neither rational functions nor the supremum of rational functions
so that current results available for the minimization of rational functions
cannot be applied to handle these problems. We prove that the problem can be
transformed into a new problem embedded in a higher dimension space where it
admits a convenient representation. This reformulation admits a hierarchy of
SDP relaxations that approximates, up to any degree of accuracy, the optimal
value of those problems. We apply this general framework to a broad family of
continuous location problems showing that some difficult problems (convex and
non-convex) that up to date could only be solved on the plane and with
Euclidean distance, can be reasonably solved with different $\ell_p$-norms and
in any finite dimension space. We illustrate this methodology with some
extensive computational results on location problems in the plane and the
3-dimension space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2414</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2414</id><created>2011-06-13</created><authors><author><keyname>Kehagias</keyname><forenames>Athanasios</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author></authors><title>Some remarks on cops and drunk robbers</title><categories>cs.DM cs.GT cs.RO math.CO math.PR</categories><msc-class>68R10 (Primary), 05C85 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cops and robbers game has been extensively studied under the assumption
of optimal play by both the cops and the robbers. In this paper we study the
problem in which cops are chasing a drunk robber (that is, a robber who
performs a random walk) on a graph. Our main goal is to characterize the &quot;cost
of drunkenness.&quot; Specif?ically, we study the ratio of expected capture times
for the optimal version and the drunk robber one. We also examine the
algorithmic side of the problem; that is, how to compute near-optimal search
schedules for the cops. Finally, we present a preliminary investigation of the
invisible robber game and point out diff?erences between this game and graph
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2417</identifier>
 <datestamp>2011-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2417</id><created>2011-06-13</created><updated>2011-08-03</updated><authors><author><keyname>Giotsas</keyname><forenames>Vasileios</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author></authors><title>Inferring Internet AS Relationships Based on BGP Routing Policies</title><categories>cs.NI</categories><comments>8 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The type of business relationships between the Internet autonomous systems
(AS) determines the BGP inter-domain routing. Previous works on inferring AS
relationships relied on the connectivity information between ASes. In this
paper we infer AS relationships by analysing the routing polices of ASes
encoded in the BGP attributes Communities and the Locpref. We accumulate BGP
data from RouteViews, RIPE RIS and the public Route Servers in August 2010 and
February 2011. Based on the routing policies extracted from data of the two BGP
attributes, we obtain AS relationships for 39% links in our data, which include
all links among the Tier-1 ASes and most links between Tier-1 and Tier-2 ASes.
We also reveal a number of special AS relationships, namely the hybrid
relationship, the partial-transit relationship, the indirect peering
relationship and the backup links. These special relationships are relevant to
a better understanding of the Internet routing. Our work provides a profound
methodological progress for inferring the AS relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2428</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2428</id><created>2011-06-13</created><updated>2011-11-29</updated><authors><author><keyname>Danielsen</keyname><forenames>Lars Eirik</forenames></author></authors><title>On the classification of Hermitian self-dual additive codes over GF(9)</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>12 pages, 6 figures</comments><journal-ref>IEEE Trans. Inform. Theory 58(8), pp. 5500-5511, 2012</journal-ref><doi>10.1109/TIT.2012.2196255</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive codes over GF(9) that are self-dual with respect to the Hermitian
trace inner product have a natural application in quantum information theory,
where they correspond to ternary quantum error-correcting codes. However, these
codes have so far received far less interest from coding theorists than
self-dual additive codes over GF(4), which correspond to binary quantum codes.
Self-dual additive codes over GF(9) have been classified up to length 8, and in
this paper we extend the complete classification to codes of length 9 and 10.
The classification is obtained by using a new algorithm that combines two graph
representations of self-dual additive codes. The search space is first reduced
by the fact that every code can be mapped to a weighted graph, and a different
graph is then introduced that transforms the problem of code equivalence into a
problem of graph isomorphism. By an extension technique, we are able to
classify all optimal codes of length 11 and 12. There are 56,005,876
(11,3^11,5) codes and 6493 (12,3^12,6) codes. We also find the smallest codes
with trivial automorphism group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2429</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2429</id><created>2011-06-13</created><updated>2013-09-11</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Efficient Transductive Online Learning via Randomized Rounding</title><categories>cs.LG stat.ML</categories><comments>To appear in a Festschrift in honor of V.N. Vapnik. Preliminary
  version presented in NIPS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most traditional online learning algorithms are based on variants of mirror
descent or follow-the-leader. In this paper, we present an online algorithm
based on a completely different approach, tailored for transductive settings,
which combines &quot;random playout&quot; and randomized rounding of loss subgradients.
As an application of our approach, we present the first computationally
efficient online algorithm for collaborative filtering with trace-norm
constrained matrices. As a second application, we solve an open question
linking batch learning and transductive online learning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2436</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2436</id><created>2011-06-13</created><updated>2011-10-25</updated><authors><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>From Bandits to Experts: On the Value of Side-Observations</title><categories>cs.LG stat.ML</categories><comments>Presented at the NIPS 2011 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an adversarial online learning setting where a decision maker can
choose an action in every stage of the game. In addition to observing the
reward of the chosen action, the decision maker gets side observations on the
reward he would have obtained had he chosen some of the other actions. The
observation structure is encoded as a graph, where node i is linked to node j
if sampling i provides information on the reward of j. This setting naturally
interpolates between the well-known &quot;experts&quot; setting, where the decision maker
can view all rewards, and the multi-armed bandits setting, where the decision
maker can only view the reward of the chosen action. We develop practical
algorithms with provable regret guarantees, which depend on non-trivial
graph-theoretic properties of the information feedback structure. We also
provide partially-matching lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2440</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2440</id><created>2011-06-13</created><updated>2012-04-05</updated><authors><author><keyname>Lichter</keyname><forenames>Shaun</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Friesz</keyname><forenames>Terry</forenames></author></authors><title>A Game Theoretic Perspective on Network Topologies</title><categories>math.OC cs.GT</categories><comments>25 pages, this revised version has been resubmitted to Computational
  and Mathematical Organization Theory after being reviewed</comments><msc-class>91A06, 91A40, 91A43, 91A80</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  As an alternative view to the graph formation models in the statistical
physics community, we introduce graph formation models using \textit{network
formation} through selfish competition as an approach to modeling graphs with
particular topologies. We further investigate a specific application of our
results to collaborative oligopolies. We extend the results of Goyal and Joshi
(S. Goyal and S. Joshi. Networks of collaboration in oligopoly. Games and
Economic behavior, 43(1):57-85, 2003), who first considered the problem of
collaboration networks of oligopolies and showed that under certain linear
assumptions network collaboration produced a stable complete graph through
selfish competition. We show with nonlinear cost functions and player payoff
alteration that stable collaboration graphs with an arbitrary degree sequence
can result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2441</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2441</id><created>2011-06-13</created><authors><author><keyname>Suzuki</keyname><forenames>Kazuhiro</forenames></author></authors><title>An f-chromatic spanning forest of edge-colored complete bipartite graphs</title><categories>math.CO cs.DM</categories><comments>8 pages, 3 figures</comments><msc-class>05C05, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2001, Brualdi and Hollingsworth proved that an edge-colored balanced
complete bipartite graph Kn,n with a color set C = {1,2,3,..., 2n-1} has a
heterochromatic spanning tree if the number of edges colored with colors in R
is more than |R|^2 /4 for any non-empty subset R \subseteq C, where a
heterochromatic spanning tree is a spanning tree whose edges have distinct
colors, namely, any color appears at most once. In 2010, Suzuki generalized
heterochromatic graphs to f-chromatic graphs, where any color c appears at most
f(c). Moreover, he presented a necessary and sufficient condition for graphs to
have an f-chromatic spanning forest with exactly w components. In this paper,
using this necessary and sufficient condition, we generalize the
Brualdi-Hollingsworth theorem above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2464</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2464</id><created>2011-06-13</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>On the Sum Capacity of K-user Cascade Gaussian Z-Interference Channel</title><categories>cs.IT math.IT</categories><comments>Accepted to ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $K$-user cascade Gaussian Z-interference channel is a subclass of the
general $K$-user Gaussian interference channel, where each user, except the
first one, experiences interference only from the previous user. Under simple
Han-Kobayashi schemes assuming Gaussian inputs and no time sharing, it is shown
that the maximum sum rate is achieved by each user transmitting either common
or private signals. For K=3, channel conditions under which the achieved sum
rate is either equal to or within 0.5 bits to the sum capacity are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2473</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2473</id><created>2011-06-13</created><authors><author><keyname>Velden</keyname><forenames>Theresa</forenames></author><author><keyname>Haque</keyname><forenames>Asif-ul</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author></authors><title>Resolving Author Name Homonymy to Improve Resolution of Structures in
  Co-author Networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>Accepted for JCDL 2011. Groundtruth data set attached and described
  in README file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate how author name homonymy distorts clustered large-scale
co-author networks, and present a simple, effective, scalable and generalizable
algorithm to ameliorate such distortions. We evaluate the performance of the
algorithm to improve the resolution of mesoscopic network structures. To this
end, we establish the ground truth for a sample of author names that is
statistically representative of different types of nodes in the co-author
network, distinguished by their role for the connectivity of the network. We
finally observe that this distinction of node roles based on the mesoscopic
structure of the network, in combination with a quantification of author name
commonality, suggests a new approach to assess network distortion by homonymy
and to analyze the reduction of distortion in the network after disambiguation,
without requiring ground truth sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2481</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2481</id><created>2011-06-13</created><updated>2015-04-28</updated><authors><author><keyname>Lin</keyname><forenames>Tianrong</forenames></author></authors><title>Another approach to the equivalence of measure-many one-way quantum
  finite automata and its application</title><categories>cs.CC cs.FL</categories><comments>V 10: Corollary 3 is deleted, since it is folk. (V 9: Revised in
  terms of the referees's comments) All comments, especially the linguistic
  comments, are welcome</comments><journal-ref>J. Comput. Syst. Sci. 81 (2015) 1413</journal-ref><doi>10.1016/j.jcss.2015.02.002</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we present a much simpler, direct and elegant approach to the
equivalence problem of {\it measure many one-way quantum finite automata}
(MM-1QFAs). The approach is essentially generalized from the work of Carlyle
[J. Math. Anal. Appl. 7 (1963) 167-175]. Namely, we reduce the equivalence
problem of MM-1QFAs to that of two (initial) vectors.
  As an application of the approach, we utilize it to address the equivalence
problem of {\it Enhanced one-way quantum finite automata} (E-1QFAs) introduced
by Nayak [Proceedings of the 40th Annual IEEE Symposium on Foundations of
Computer Science, 1999, pp.~369-376]. We prove that two E-1QFAs $\mathcal{A}_1$
and $\mathcal{A}_2$ over $\Sigma$ are equivalence if and only if they are
$n_1^2+n_2^2-1$-equivalent where $n_1$ and $n_2$ are the numbers of states in
$\mathcal{A}_1$ and $\mathcal{A}_2$, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2489</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2489</id><created>2011-06-13</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames><affiliation>Department of Computer Science, University of Toronto, Canada</affiliation></author></authors><title>Eliciting Forecasts from Self-interested Experts: Scoring Rules for
  Decision Makers</title><categories>cs.GT cs.AI cs.MA cs.SI</categories><comments>11 pages 4 figures pdflatex See
  http://www.cs.toronto.edu/~cebly/papers.html</comments><acm-class>I.2.11; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scoring rules for eliciting expert predictions of random variables are
usually developed assuming that experts derive utility only from the quality of
their predictions (e.g., score awarded by the rule, or payoff in a prediction
market). We study a more realistic setting in which (a) the principal is a
decision maker and will take a decision based on the expert's prediction; and
(b) the expert has an inherent interest in the decision. For example, in a
corporate decision market, the expert may derive different levels of utility
from the actions taken by her manager. As a consequence the expert will usually
have an incentive to misreport her forecast to influence the choice of the
decision maker if typical scoring rules are used. We develop a general model
for this setting and introduce the concept of a compensation rule. When
combined with the expert's inherent utility for decisions, a compensation rule
induces a net scoring rule that behaves like a normal scoring rule. Assuming
full knowledge of expert utility, we provide a complete characterization of all
(strictly) proper compensation rules. We then analyze the situation where the
expert's utility function is not fully known to the decision maker. We show
bounds on: (a) expert incentive to misreport; (b) the degree to which an expert
will misreport; and (c) decision maker loss in utility due to such uncertainty.
These bounds depend in natural ways on the degree of uncertainty, the local
degree of convexity of net scoring function, and natural properties of the
decision maker's utility function. They also suggest optimization procedures
for the design of compensation rules. Finally, we briefly discuss the use of
compensation rules as market scoring rules for self-interested experts in a
prediction market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2503</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2503</id><created>2011-06-13</created><updated>2013-06-23</updated><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author></authors><title>A Large-Scale Community Structure Analysis In Facebook</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>30 pages, 13 Figures - Published on: EPJ Data Science, 1:9, 2012 -
  open access at: http://www.epjdatascience.com/content/1/1/9</comments><msc-class>91D30, 05C82, 68R10, 90B10, 90C35</msc-class><acm-class>H.2.8; D.2.8</acm-class><journal-ref>EPJ Data Science, 1:9, 2012</journal-ref><doi>10.1140/epjds9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding social dynamics that govern human phenomena, such as
communications and social relationships is a major problem in current
computational social sciences. In particular, given the unprecedented success
of online social networks (OSNs), in this paper we are concerned with the
analysis of aggregation patterns and social dynamics occurring among users of
the largest OSN as the date: Facebook. In detail, we discuss the mesoscopic
features of the community structure of this network, considering the
perspective of the communities, which has not yet been studied on such a large
scale. To this purpose, we acquired a sample of this network containing
millions of users and their social relationships; then, we unveiled the
communities representing the aggregation units among which users gather and
interact; finally, we analyzed the statistical features of such a network of
communities, discovering and characterizing some specific organization patterns
followed by individuals interacting in online social networks, that emerge
considering different sampling techniques and clustering methodologies. This
study provides some clues of the tendency of individuals to establish social
interactions in online social networks that eventually contribute to building a
well-connected social structure, and opens space for further social studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2522</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2522</id><created>2011-06-13</created><authors><author><keyname>Ekrem</keyname><forenames>Ersen</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Degrees of Freedom Region of the Gaussian MIMO Broadcast Channel with
  Common and Private Messages</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Gaussian multiple-input multiple-output (MIMO) broadcast
channel with common and private messages. We obtain the degrees of freedom
(DoF) region of this channel. We first show that a parallel Gaussian broadcast
channel with unmatched sub-channels can be constructed from any given Gaussian
MIMO broadcast channel by using the generalized singular value decomposition
(GSVD) and a relaxation on the power constraint for the channel input, in a way
that the capacity region of the constructed parallel channel provides an outer
bound for the capacity region of the original channel. The capacity region of
the parallel Gaussian broadcast channel with unmatched sub-channels is known,
using which we obtain an explicit outer bound for the DoF region of the
Gaussian MIMO broadcast channel. We finally show that this outer bound for the
DoF region can be attained both by the achievable scheme that uses a classical
Gaussian coding for the common message and dirty-paper coding (DPC) for the
private messages, as well as by a variation of the zero-forcing (ZF) scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2530</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2530</id><created>2011-06-13</created><authors><author><keyname>Golovkins</keyname><forenames>Marats</forenames></author><author><keyname>Kravtsev</keyname><forenames>Maksim</forenames></author><author><keyname>Kravcevs</keyname><forenames>Vasilijs</forenames></author></authors><title>Quantum Finite Automata and Probabilistic Reversible Automata: R-trivial
  Idempotent Languages</title><categories>cs.FL quant-ph</categories><comments>30 pages, 3 figures</comments><journal-ref>MFCS 2011, Lecture Notes in Computer Science, Vol. 6907, pp.
  351-363, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the recognition of R-trivial idempotent (R1) languages by various
models of &quot;decide-and-halt&quot; quantum finite automata (QFA) and probabilistic
reversible automata (DH-PRA). We introduce bistochastic QFA (MM-BQFA), a model
which generalizes both Nayak's enhanced QFA and DH-PRA. We apply tools from
algebraic automata theory and systems of linear inequalities to give a complete
characterization of R1 languages recognized by all these models. We also find
that &quot;forbidden constructions&quot; known so far do not include all of the languages
that cannot be recognized by measure-many QFA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2533</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2533</id><created>2011-06-13</created><authors><author><keyname>Adler</keyname><forenames>Isolde</forenames></author><author><keyname>Farley</keyname><forenames>Arthur M.</forenames></author><author><keyname>Proskurowski</keyname><forenames>Andrzej</forenames></author></authors><title>Obstructions for linear rankwidth at most 1</title><categories>cs.DM math.CO</categories><comments>16 pages, 10 figures</comments><msc-class>05C83</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a characterization of graphs of linear rankwidth at most 1 by
minimal excluded vertex-minors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2568</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2568</id><created>2011-06-13</created><authors><author><keyname>Bao</keyname><forenames>Yungang</forenames></author><author><keyname>Zhang</keyname><forenames>Jinyong</forenames></author><author><keyname>Zhu</keyname><forenames>Yan</forenames></author><author><keyname>Tang</keyname><forenames>Dan</forenames></author><author><keyname>Ruan</keyname><forenames>Yuan</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Fan</keyname><forenames>Jianping</forenames></author></authors><title>HMTT: A Hybrid Hardware/Software Tracing System for Bridging Memory
  Trace's Semantic Gap</title><categories>cs.AR cs.PF</categories><comments>16 papges, an extension version of ACM SIGMETRICS'08 paper</comments><report-no>CAS-ICT-TECH-REPORT-20090327</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory trace analysis is an important technology for architecture research,
system software (i.e., OS, compiler) optimization, and application performance
improvements. Hardware-snooping is an effective and efficient approach to
monitor and collect memory traces. Compared with software-based approaches,
memory traces collected by hardware-based approaches are usually lack of
semantic information, such as process/function/loop identifiers, virtual
address and I/O access. In this paper we propose a hybrid hardware/software
mechanism which is able to collect memory reference trace as well as semantic
information. Based on this mechanism, we designed and implemented a prototype
system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping
mechanism to snoop on memory bus and a software-controlled tracing mechanism to
inject semantic information into normal memory trace. To the best of our
knowledge, the HMTT system is the first hardware tracing system capable of
correlating memory trace with high-level events. Comprehensive validations and
evaluations show that the HMTT system has both hardware's (e.g., no distortion
or pollution) and software's advantages (e.g., flexibility and more
information).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2573</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2573</id><created>2011-06-13</created><updated>2012-04-21</updated><authors><author><keyname>Cowan</keyname><forenames>Noah J.</forenames></author><author><keyname>Chastain</keyname><forenames>Erick J.</forenames></author><author><keyname>Vilhena</keyname><forenames>Daril A.</forenames></author><author><keyname>Freudenberg</keyname><forenames>James S.</forenames></author><author><keyname>Bergstrom</keyname><forenames>Carl T.</forenames></author></authors><title>Nodal dynamics, not degree distributions, determine the structural
  controllability of complex networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>1 Figures, 6 pages</comments><doi>10.1371/journal.pone.0038398</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural controllability has been proposed as an analytical framework for
making predictions regarding the control of complex networks across myriad
disciplines in the physical and life sciences (Liu et al.,
Nature:473(7346):167-173, 2011). Although the integration of control theory and
network analysis is important, we argue that the application of the structural
controllability framework to most if not all real-world networks leads to the
conclusion that a single control input, applied to the power dominating set
(PDS), is all that is needed for structural controllability. This result is
consistent with the well-known fact that controllability and its dual
observability are generic properties of systems. We argue that more important
than issues of structural controllability are the questions of whether a system
is almost uncontrollable, whether it is almost unobservable, and whether it
possesses almost pole-zero cancellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2575</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2575</id><created>2011-06-13</created><authors><author><keyname>Tobin-Hochstadt</keyname><forenames>Sam</forenames></author><author><keyname>Felleisen</keyname><forenames>Matthias</forenames></author></authors><title>The Design and Implementation of Typed Scheme: From Scripts to Programs</title><categories>cs.PL</categories><comments>Accepted for publication in Higher Order and Symbolic Computation.
  This is a revised and extended version of a paper presented at the 35th ACM
  SIGPLAN-SIGACT Symposium on Principles of Programming Languages, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When scripts in untyped languages grow into large programs, maintaining them
becomes difficult. A lack of explicit type annotations in typical scripting
languages forces programmers to must (re)discover critical pieces of design
information every time they wish to change a program. This analysis step both
slows down the maintenance process and may even introduce mistakes due to the
violation of undiscovered invariants.
  This paper presents Typed Scheme, an explicitly typed extension of PLT
Scheme, an untyped scripting language. Its type system is based on the novel
notion of occurrence typing, which we formalize and mechanically prove sound.
The implementation of Typed Scheme additionally borrows elements from a range
of approaches, including recursive types, true unions and subtyping, plus
polymorphism combined with a modicum of local inference.
  The formulation of occurrence typing naturally leads to a simple and
expressive version of predicates to describe refinement types. A Typed Scheme
program can use these refinement types to keep track of arbitrary classes of
values via the type system. Further, we show how the Typed Scheme type system,
in conjunction with simple recursive types, is able to encode refinements of
existing datatypes, thus expressing both proposed variations of refinement
types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2578</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2578</id><created>2011-06-13</created><authors><author><keyname>Tobin-Hochstadt</keyname><forenames>Sam</forenames></author></authors><title>Extensible Pattern Matching in an Extensible Language</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern matching is a widely used technique in functional languages,
especially those in the ML and Haskell traditions, where it is at the core of
the semantics. In languages in the Lisp tradition, in contrast, pattern
matching it typically provided by libraries built with macros. We present
match, a sophisticated pattern matcher for Racket, implemented as language
extension. using macros. The system supports novel and widely-useful
pattern-matching forms, and is itself extensible. The extensibility of match is
implemented via a general technique for creating extensible language
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2581</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2581</id><created>2011-06-13</created><authors><author><keyname>Leong</keyname><forenames>Derek</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author></authors><title>Distributed Storage Allocations for Optimal Delay</title><categories>cs.IT math.IT</categories><comments>Extended version of an IEEE ISIT 2011 paper. 10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the problem of creating an encoded distributed storage
representation of a data object for a network of mobile storage nodes so as to
achieve the optimal recovery delay. A source node creates a single data object
and disseminates an encoded representation of it to other nodes for storage,
subject to a given total storage budget. A data collector node subsequently
attempts to recover the original data object by contacting other nodes and
accessing the data stored in them. By using an appropriate code, successful
recovery is achieved when the total amount of data accessed is at least the
size of the original data object. The goal is to find an allocation of the
given budget over the nodes that optimizes the recovery delay incurred by the
data collector; two objectives are considered: (i) maximization of the
probability of successful recovery by a given deadline, and (ii) minimization
of the expected recovery delay. We solve the problem completely for the second
objective in the case of symmetric allocations (in which all nonempty nodes
store the same amount of data), and show that the optimal symmetric allocation
for the two objectives can be quite different. A simple data dissemination and
storage protocol for a mobile delay-tolerant network is evaluated under various
scenarios via simulations. Our results show that the choice of storage
allocation can have a significant impact on the recovery delay performance, and
that coding may or may not be beneficial depending on the circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2587</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2587</id><created>2011-06-13</created><updated>2011-12-08</updated><authors><author><keyname>Hoobin</keyname><forenames>Christopher</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Zobel</keyname><forenames>Justin</forenames></author></authors><title>Relative Lempel-Ziv Factorization for Efficient Storage and Retrieval of
  Web Collections</title><categories>cs.DS cs.DB cs.IR</categories><comments>VLDB2012</comments><report-no>vol5no3/p265_christopherhoobin_vldb2012</report-no><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  265-273 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression techniques that support fast random access are a core component
of any information system. Current state-of-the-art methods group documents
into fixed-sized blocks and compress each block with a general-purpose adaptive
algorithm such as GZIP. Random access to a specific document then requires
decompression of a block. The choice of block size is critical: it trades
between compression effectiveness and document retrieval times. In this paper
we present a scalable compression method for large document collections that
allows fast random access. We build a representative sample of the collection
and use it as a dictionary in a LZ77-like encoding of the rest of the
collection, relative to the dictionary. We demonstrate on large collections,
that using a dictionary as small as 0.1% of the collection size, our algorithm
is dramatically faster than previous methods, and in general gives much better
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2593</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2593</id><created>2011-06-13</created><authors><author><keyname>Mazonka</keyname><forenames>Oleg</forenames></author><author><keyname>Kolodin</keyname><forenames>Alex</forenames></author></authors><title>A Simple Multi-Processor Computer Based on Subleq</title><categories>cs.DC cs.AR cs.PL</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subleq (Subtract and Branch on result Less than or Equal to zero) is both an
instruction set and a programming language for One Instruction Set Computer
(OISC). We describe a hardware implementation of an array of 28 one-instruction
Subleq processors on a low-cost FPGA board. Our test results demonstrate that
computational power of our Subleq OISC multi-processor is comparable to that of
CPU of a modern personal computer. Additionally, we provide implementation
details of our complier from a C-style language to Subleq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2601</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2601</id><created>2011-06-13</created><authors><author><keyname>Dhillon</keyname><forenames>Vikram</forenames></author></authors><title>Knowledge Dispersion Index for Measuring Intellectual Capital</title><categories>cs.SI q-fin.GN</categories><comments>Submitted to Innocentive</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we propose a novel index to quantify and measure the flow of
information on macro and micro scales. We discuss the implications of this
index for knowledge management fields and also as intellectual capital that can
thus be utilized by entrepreneurs. We explore different function and human
oriented metrics that can be used at micro-scales to process the flow of
information. We present a table of about 23 metrics, such as change in IT
inventory and percentage of employees with advanced degrees, that can be used
at micro scales to wholly quantify knowledge dispersion as intellectual
capital. At macro scales we split the economy in an industrial and consumer
sector where the flow of information in each determines how fast an economy is
going to grow and how overall an economy will perform given the aggregate
demand. Lastly, we propose a model for knowledge dispersion based on graph
theory and show how corrections in the flow become self-evident. Through the
principals of flow conservation and capacity constrains we also speculate how
this flow might seeks some equilibrium and exhibit self-correction codes. This
proposed model allows us to account for perturbations in form of local noise,
evolution of networks, provide robustness against local damage from lower
nodes, and help determine the underlying classification into network
super-families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2603</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2603</id><created>2011-06-14</created><authors><author><keyname>Ye</keyname><forenames>Chengxi</forenames></author><author><keyname>Ma</keyname><forenames>Zhanshan Sam</forenames></author><author><keyname>Cannon</keyname><forenames>Charles H.</forenames></author><author><keyname>Pop</keyname><forenames>Mihai</forenames></author><author><keyname>Yu</keyname><forenames>Douglas W.</forenames></author></authors><title>SparseAssembler: de novo Assembly with the Sparse de Bruijn Graph</title><categories>cs.DS q-bio.GN</categories><comments>Corresponding author: Douglas W. Yu, dougwyu@gmail.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  de Bruijn graph-based algorithms are one of the two most widely used
approaches for de novo genome assembly. A major limitation of this approach is
the large computational memory space requirement to construct the de Bruijn
graph, which scales with k-mer length and total diversity (N) of unique k-mers
in the genome expressed in base pairs or roughly (2k+8)N bits. This limitation
is particularly important with large-scale genome analysis and for sequencing
centers that simultaneously process multiple genomes. We present a sparse de
Bruijn graph structure, based on which we developed SparseAssembler that
greatly reduces memory space requirements. The structure also allows us to
introduce a novel method for the removal of substitution errors introduced
during sequencing. The sparse de Bruijn graph structure skips g intermediate
k-mers, therefore reducing the theoretical memory space requirement to
~(2k/g+8)N. We have found that a practical value of g=16 consumes approximately
10% of the memory required by standard de Bruijn graph-based algorithms but
yields comparable results. A high error rate could potentially derail the
SparseAssembler. Therefore, we developed a sparse de Bruijn graph-based
denoising algorithm that can remove more than 99% of substitution errors from
datasets with a \leq 2% error rate. Given that substitution error rates for the
current generation of sequencers is lower than 1%, our denoising procedure is
sufficiently effective to safeguard the performance of our algorithm. Finally,
we also introduce a novel Dijkstra-like breadth-first search algorithm for the
sparse de Bruijn graph structure to circumvent residual errors and resolve
polymorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2610</identifier>
 <datestamp>2011-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2610</id><created>2011-06-14</created><updated>2011-08-24</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Pathlength scaling in graphs with incomplete navigational information</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 5 figures</comments><journal-ref>Physica A 390, 3996 (2011)</journal-ref><doi>10.1016/j.physa.2011.06.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph-navigability problem concerns how one can find as short paths as
possible between a pair of vertices, given an incomplete picture of a graph. We
study the navigability of graphs where the vertices are tagged by a number
(between 1 and the total number of vertices) in a way to aid navigation. This
information is too little to ensure errorfree navigation but enough, as we will
show, for the agents to do significantly better than a random walk. In our
setup, given a graph, we first assign information to the vertices that agents
can utilize for their navigation. To evaluate the navigation, we calculate the
average distance traveled over random pairs of source and target and different
graph realizations. We show that this type of embedding can be made quite
efficiently; the more information is embedded, the more efficient it gets. We
also investigate the embedded navigational information in a standard graph
layout algorithm and find that although this information does not make
algorithms as efficient as the above-mentioned schemes, it is significantly
helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2619</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2619</id><created>2011-06-14</created><updated>2011-06-17</updated><authors><author><keyname>Dubey</keyname><forenames>Chandan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>Approximating the Closest Vector Problem Using an Approximate Shortest
  Vector Oracle</title><categories>cs.DS cs.CC cs.CR</categories><comments>10 pages, Published in APPROX 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time Turing reduction from the
$\gamma^2\sqrt{n}$-approximate closest vector problem on a lattice of dimension
$n$ to a $\gamma$-approximate oracle for the shortest vector problem. This is
an improvement over a reduction by Kannan, which achieved $\gamma^2n^{3/2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2637</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2637</id><created>2011-06-14</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Gonnord</keyname><forenames>Laure</forenames><affiliation>LIFL</affiliation></author></authors><title>Using Bounded Model Checking to Focus Fixpoint Iterations</title><categories>cs.PL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two classical sources of imprecision in static analysis by abstract
interpretation are widening and merge operations. Merge operations can be done
away by distinguishing paths, as in trace partitioning, at the expense of
enumerating an exponential number of paths. In this article, we describe how to
avoid such systematic exploration by focusing on a single path at a time,
designated by SMT-solving. Our method combines well with acceleration
techniques, thus doing away with widenings as well in some cases. We illustrate
it over the well-known domain of convex polyhedra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2647</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2647</id><created>2011-06-14</created><updated>2013-08-17</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>From Causal Models To Counterfactual Structures</title><categories>cs.AI</categories><comments>A preliminary version of this paper appears in the Proceedings of the
  Twelfth International Conference on Principles of Knowledge Representation
  and Reasoning (KR 2010), 2010.}</comments><acm-class>I.2.4</acm-class><journal-ref>Review of Symbolic Logic 6:2, 2013, pp. 305--322</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Galles and Pearl claimed that &quot;for recursive models, the causal model
framework does not add any restrictions to counterfactuals, beyond those
imposed by Lewis's [possible-worlds] framework.&quot; This claim is examined
carefully, with the goal of clarifying the exact relationship between causal
models and Lewis's framework. Recursive models are shown to correspond
precisely to a subclass of (possible-world) counterfactual structures. On the
other hand, a slight generalization of recursive models, models where all
equations have unique solutions, is shown to be incomparable in expressive
power to counterfactual structures, despite the fact that the Galles and Pearl
arguments should apply to them as well. The problem with the Galles and Pearl
argument is identified: an axiom that they viewed as irrelevant, because it
involved disjunction (which was not in their language), is not irrelevant at
all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2649</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2649</id><created>2011-06-14</created><updated>2012-11-19</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Viewpoint: Journals for Certification, Conferences for Rapid
  Dissemination</title><categories>cs.DL</categories><comments>To appear, Communications of the ACM</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The publication culture in Computer Science is different from that of all
other disciplines. Whereas other disciplines focus on journal publication, the
standard practice in CS has been to publish in a conference and then
(sometimes) publish a journal version of the conference paper. We discuss the
role of journal publication in CS.
  Indeed, it is through publication in selective, leading conferences that the
quality of CS research is typically assessed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2650</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2650</id><created>2011-06-14</created><authors><author><keyname>Rose</keyname><forenames>Luca</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>On the Nash Equilibria in Decentralized Parallel Interference Channels</title><categories>cs.GT</categories><comments>6 pages, 4 figures, presented in ICCC Kyoto 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the 2-dimensional decentralized parallel interference channel
(IC) with 2 transmitter-receiver pairs is modelled as a non-cooperative static
game. Each transmitter is assumed to be a fully rational entity with complete
information on the game, aiming to maximize its own individual spectral
efficiency by tuning its own power allocation (PA) vector. Two scenarios are
analysed. First, we consider that transmitters can split their transmit power
between both dimensions (PA game). Second, we consider that each transmitter is
limited to use only one dimension (channel selection CS game). In the first
scenario, the game might have either one or three NE in pure strategies (PS).
However, two or infinitely many NE in PS might also be observed with zero
probability. In the second scenario, there always exists either one or two NE
in PS. We show that in both games there always exists a non-zero probability of
observing more than one NE. More interestingly, using Monte-Carlo simulations,
we show that the highest and lowest network spectral efficiency at any of the
NE in the CS game are always higher than the ones in the PA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2652</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2652</id><created>2011-06-14</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Hitchcock</keyname><forenames>Christopher</forenames></author></authors><title>Actual causation and the art of modeling</title><categories>cs.AI</categories><comments>In &lt;em&gt;Heuristics, Probability and Causality: A Tribute to Judea
  Pearl&lt;/em&gt; (editors, R. Dechter, H. Geffner, and J. Y. Halpern), College
  Publications, 2010, pp. 383-406</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We look more carefully at the modeling of causality using structural
equations. It is clear that the structural equations can have a major impact on
the conclusions we draw about causality. In particular, the choice of variables
and their values can also have a significant impact on causality. These choices
are, to some extent, subjective. We consider what counts as an appropriate
choice. More generally, we consider what makes a model an appropriate model,
especially if we want to take defaults into account, as was argued is necessary
in recent work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2657</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2657</id><created>2011-06-14</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>I Don't Want to Think About it Now:Decision Theory With Costly
  Computation</title><categories>cs.GT</categories><comments>In Conference on Knowledge Representation and Reasoning (KR '10)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation plays a major role in decision making. Even if an agent is
willing to ascribe a probability to all states and a utility to all outcomes,
and maximize expected utility, doing so might present serious computational
problems. Moreover, computing the outcome of a given act might be difficult. In
a companion paper we develop a framework for game theory with costly
computation, where the objects of choice are Turing machines. Here we apply
that framework to decision theory. We show how well-known phenomena like
first-impression-matters biases (i.e., people tend to put more weight on
evidence they hear early on), belief polarization (two people with different
prior beliefs, hearing the same evidence, can end up with diametrically opposed
conclusions), and the status quo bias (people are much more likely to stick
with what they already have) can be easily captured in that framework. Finally,
we use the framework to define some new notions: value of computational
information (a computational variant of value of information) and and
computational value of conversation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2662</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2662</id><created>2011-06-14</created><authors><author><keyname>Rose</keyname><forenames>Luca</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Learning Equilibria with Partial Information in Decentralized Wireless
  Networks</title><categories>cs.LG cs.AI cs.GT cs.MA</categories><comments>16 pages, 5 figures, 1 table. To appear in IEEE Communication
  Magazine, special Issue on Game Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a survey of several important equilibrium concepts for
decentralized networks is presented. The term decentralized is used here to
refer to scenarios where decisions (e.g., choosing a power allocation policy)
are taken autonomously by devices interacting with each other (e.g., through
mutual interference). The iterative long-term interaction is characterized by
stable points of the wireless network called equilibria. The interest in these
equilibria stems from the relevance of network stability and the fact that they
can be achieved by letting radio devices to repeatedly interact over time. To
achieve these equilibria, several learning techniques, namely, the best
response dynamics, fictitious play, smoothed fictitious play, reinforcement
learning algorithms, and regret matching, are discussed in terms of information
requirements and convergence properties. Most of the notions introduced here,
for both equilibria and learning schemes, are illustrated by a simple case
study, namely, an interference channel with two transmitter-receiver pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2670</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2670</id><created>2011-06-14</created><authors><author><keyname>Perrot</keyname><forenames>K&#xe9;vin</forenames></author><author><keyname>R&#xe9;mila</keyname><forenames>Eric</forenames></author></authors><title>Transduction on Kadanoff Sand Pile Model Avalanches, Application to Wave
  Pattern Emergence</title><categories>cs.DM</categories><comments>14 pages</comments><journal-ref>MFCS 2011, ARCoSS LNCS Springer-Verlag, volume 6907, pages 508-520</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sand pile models are dynamical systems describing the evolution from $N$
stacked grains to a stable configuration. It uses local rules to depict grain
moves and iterate it until reaching a fixed configuration from which no rule
can be applied. The main interest of sand piles relies in their {\em Self
Organized Criticality} (SOC), the property that a small perturbation | adding
some sand grains | on a fixed configuration has uncontrolled consequences on
the system, involving an arbitrary number of grain fall. Physicists L. Kadanoff
{\em et al} inspire KSPM, a model presenting a sharp SOC behavior, extending
the well known {\em Sand Pile Model}. In KSPM($D$), we start from a pile of $N$
stacked grains and apply the rule: $D-1$ grains can fall from column $i$ onto
the $D-1$ adjacent columns to the right if the difference of height between
columns $i$ and $i+1$ is greater or equal to $D$. This paper develops a formal
background for the study of KSPM fixed points. This background, resumed in a
finite state word transducer, is used to provide a plain formula for fixed
points of KSPM(3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2673</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2673</id><created>2011-06-14</created><authors><author><keyname>Dolev</keyname><forenames>Danny</forenames></author><author><keyname>Feitelson</keyname><forenames>Dror G.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Kupferman</keyname><forenames>Raz</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author></authors><title>No justified complaints: On fair sharing of multiple resources</title><categories>cs.DC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fair allocation has been studied intensively in both economics and computer
science, and fair sharing of resources has aroused renewed interest with the
advent of virtualization and cloud computing. Prior work has typically focused
on mechanisms for fair sharing of a single resource. We provide a new
definition for the simultaneous fair allocation of multiple
continuously-divisible resources. Roughly speaking, we define fairness as the
situation where every user either gets all the resources he wishes for, or else
gets at least his entitlement on some bottleneck resource, and therefore cannot
complain about not getting more. This definition has the same desirable
properties as the recently suggested dominant resource fairness, and also
handles the case of multiple bottlenecks. We then prove that a fair allocation
according to this definition is guaranteed to exist for any combination of user
requests and entitlements (where a user's relative use of the different
resources is fixed). The proof, which uses tools from the theory of ordinary
differential equations, is constructive and provides a method to compute the
allocations numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2677</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2677</id><created>2011-06-14</created><authors><author><keyname>McLaughlin</keyname><forenames>Mark Anthony</forenames></author></authors><title>A Framework for Enabling Distributed Applications on the Internet</title><categories>cs.DC cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The last five years have seen the rapid rise in popularity of what we term
internet distributed applications (IDAs). These are internet applications with
which many users interact simultaneously. IDAs range from P2P file-sharing
applications, to collaborative distributed computing projects, to massively
multiplayer online games (MMOGs). Currently, there is no framework that
combines IDAs collectively within a single context. We provide a basis for such
a framework here.
  In considering IDAs collectively, we found that there was no generic
description that had been applied to them as a group. We have therefore put
forward such a description here. In our description, IDAs are functionality
separated into three logic layers, which are designed and built individually.
Each layer is represented by functionality on the software client running on
each participating computer, which together comprise the overall IDA.
  The core contribution of this work is a framework, called the Internet
Distributed Application Framework (IDAF), which outlines how IDAs can be
designed, built and run. The IDAF outlines a set of constraints that each
implementing software system must abide by. To verify the IDAF, we have built a
system prototype implementation called the Internet Distributed Application
System (IDAS). The IDAS includes an implementation of the IDAF layer model,
which specifies IDAs are built. The IDAS also includes a generic software
client that is capable of simultaneously running and managing arbitrary IDAs.
We provide sample IDAs and demonstrations to verify both that the IDAF is
implementable and that the IDAS is a workable usable system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2684</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2684</id><created>2011-06-14</created><authors><author><keyname>Heus</keyname><forenames>Pascal</forenames></author><author><keyname>Gomez</keyname><forenames>Richard</forenames></author></authors><title>QIS-XML: An Extensible Markup Language for Quantum Information Science</title><categories>cs.SE cs.ET cs.PL quant-ph</categories><comments>83 pages, 58 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This Master thesis examines issues of interoperability and integration
between the Classic Information Science (CIS) and Quantum Information Science
(QIS). It provides a short introduction to the Extensible Markup Language (XML)
and proceeds to describe the development steps that have lead to a prototype
XML specification for quantum computing (QIS-XML). QIS-XML is a proposed
framework, based on the widely used standard (XML) to describe, visualize,
exchange and process quantum gates and quantum circuits. It also provides a
potential approach to a generic programming language for quantum computers
through the concept of XML driven compilers. Examples are provided for the
description of commonly used quantum gates and circuits, accompanied with tools
to visualize them in standard web browsers. An algorithmic example is also
presented, performing a simple addition operation with quantum circuits and
running the program on a quantum computer simulator. Overall, this initial
effort demonstrates how XML technologies could be at the core of the
architecture for describing and programming quantum computers. By leveraging a
widely accepted standard, QIS-XML also builds a bridge between classic and
quantum IT, which could foster the acceptance of QIS by the ICT community and
facilitate the understanding of quantum technology by IT experts. This would
support the consolidation of Classic Information Science and Quantum
Information Science into a Complete Information Science, a challenge that could
be referred to as the &quot;Information Science Grand Unification Challenge&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2686</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2686</id><created>2011-06-14</created><authors><author><keyname>Srivastava</keyname><forenames>Praveen Ranjan</forenames></author><author><keyname>Baby</keyname><forenames>Km</forenames></author></authors><title>Automated Software Testing Using Metahurestic Technique Based on An Ant
  Colony Optimization</title><categories>cs.SE</categories><comments>Electronic System Design (ISED), 2010 International Symposium on
  Issue Date: 20-22 Dec. 2010 On page(s): 235 - 240 Location: Bhubaneswar Print
  ISBN: 978-1-4244-8979-4 INSPEC Accession Number: 11835766 Digital Object
  Identifier: 10.1109/ISED.2010.52</comments><doi>10.1109/ISED.2010.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is an important and valuable part of the software
development life cycle. Due to time, cost and other circumstances, exhaustive
testing is not feasible that's why there is a need to automate the software
testing process. Testing effectiveness can be achieved by the State Transition
Testing (STT) which is commonly used in real time, embedded and web-based type
of software systems. Aim of the current paper is to present an algorithm by
applying an ant colony optimization technique, for generation of optimal and
minimal test sequences for behavior specification of software. Present paper
approach generates test sequence in order to obtain the complete software
coverage. This paper also discusses the comparison between two metaheuristic
techniques (Genetic Algorithm and Ant Colony optimization) for transition based
testing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2692</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2692</id><created>2011-06-14</created><authors><author><keyname>Aravantinos</keyname><forenames>Vincent</forenames></author><author><keyname>Peltier</keyname><forenames>Nicolas</forenames></author></authors><title>Generating Schemata of Resolution Proofs</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two distinct algorithms are presented to extract (schemata of) resolution
proofs from closed tableaux for propositional schemata. The first one handles
the most efficient version of the tableau calculus but generates very complex
derivations (denoted by rather elaborate rewrite systems). The second one has
the advantage that much simpler systems can be obtained, however the considered
proof procedure is less efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2694</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2694</id><created>2011-06-14</created><authors><author><keyname>Argyriou</keyname><forenames>Evmorfia N.</forenames></author><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Symvonis</keyname><forenames>Antonios</forenames></author></authors><title>Geometric Simultaneous RAC Drawings of Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce and study &quot;geometric simultaneous RAC drawing
problems&quot;, i.e., a combination of problems on geometric RAC drawings and
geometric simultaneous graph drawings. To the best of our knowledge, this is
the first time where such a combination is attempted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2695</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2695</id><created>2011-06-14</created><authors><author><keyname>Chau</keyname><forenames>Duc Phu</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bremond</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Thonnat</keyname><forenames>Monique</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Corvee</keyname><forenames>Etienne</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Robust Mobile Object Tracking Based on Multiple Feature Similarity and
  Trajectory Filtering</title><categories>cs.CV</categories><proxy>ccsd</proxy><journal-ref>The International Conference on Computer Vision Theory and
  Applications (VISAPP) (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new algorithm to track mobile objects in different
scene conditions. The main idea of the proposed tracker includes estimation,
multi-features similarity measures and trajectory filtering. A feature set
(distance, area, shape ratio, color histogram) is defined for each tracked
object to search for the best matching object. Its best matching object and its
state estimated by the Kalman filter are combined to update position and size
of the tracked object. However, the mobile object trajectories are usually
fragmented because of occlusions and misdetections. Therefore, we also propose
a trajectory filtering, named global tracker, aims at removing the noisy
trajectories and fusing the fragmented trajectories belonging to a same mobile
object. The method has been tested with five videos of different scene
conditions. Three of them are provided by the ETISEO benchmarking project
(http://www-sop.inria.fr/orion/ETISEO) in which the proposed tracker
performance has been compared with other seven tracking algorithms. The
advantages of our approach over the existing state of the art ones are: (i) no
prior knowledge information is required (e.g. no calibration and no contextual
models are needed), (ii) the tracker is more reliable by combining multiple
feature similarities, (iii) the tracker can perform in different scene
conditions: single/several mobile objects, weak/strong illumination,
indoor/outdoor scenes, (iv) a trajectory filtering is defined and applied to
improve the tracker performance, (v) the tracker performance outperforms many
algorithms of the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2696</identifier>
 <datestamp>2011-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2696</id><created>2011-06-14</created><authors><author><keyname>Schaffer</keyname><forenames>Peter</forenames></author><author><keyname>Aouada</keyname><forenames>Djamila</forenames></author><author><keyname>Nagaraja</keyname><forenames>Shishir</forenames></author></authors><title>Who clicks there!: Anonymizing the photographer in a camera saturated
  society</title><categories>cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, social media has played an increasingly important role in
reporting world events. The publication of crowd-sourced photographs and videos
in near real-time is one of the reasons behind the high impact. However, the
use of a camera can draw the photographer into a situation of conflict.
Examples include the use of cameras by regulators collecting evidence of Mafia
operations; citizens collecting evidence of corruption at a public service
outlet; and political dissidents protesting at public rallies. In all these
cases, the published images contain fairly unambiguous clues about the location
of the photographer (scene viewpoint information). In the presence of adversary
operated cameras, it can be easy to identify the photographer by also combining
leaked information from the photographs themselves. We call this the camera
location detection attack. We propose and review defense techniques against
such attacks. Defenses such as image obfuscation techniques do not protect
camera-location information; current anonymous publication technologies do not
help either. However, the use of view synthesis algorithms could be a promising
step in the direction of providing probabilistic privacy guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2720</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2720</id><created>2011-06-14</created><authors><author><keyname>Ballico</keyname><forenames>Edoardo</forenames></author><author><keyname>Elia</keyname><forenames>Michele</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>Complexity of multivariate polynomial evaluation</title><categories>math.AC cs.CR cs.DS</categories><msc-class>12Y05, 94B27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method to evaluate multivariate polynomials over a finite field
and discuss its multiplicative complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2729</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2729</id><created>2011-06-14</created><updated>2014-04-23</updated><authors><author><keyname>Karaman</keyname><forenames>Svebor</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Benois-Pineau</keyname><forenames>Jenny</forenames><affiliation>LaBRI</affiliation></author><author><keyname>M&#xe9;gret</keyname><forenames>R&#xe9;mi</forenames><affiliation>IMS</affiliation></author></authors><title>Nested Graph Words for Object Recognition</title><categories>cs.MM cs.CV</categories><comments>Preliminary version of accepted paper &quot;Multi-Layer Local Graph Words
  for Object Recognition&quot;. Leaving only final version to avoid confusion</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new, scalable approach for the task of object
based image search or object recognition. Despite the very large literature
existing on the scalability issues in CBIR in the sense of retrieval
approaches, the scalability of media and scalability of features remain an
issue. In our work we tackle the problem of scalability and structural
organization of features. The proposed features are nested local graphs built
upon sets of SURF feature points with Delaunay triangulation. A
Bag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth
to a Bag-of-Graph-Words representation. The nested nature of the descriptors
consists in scaling from trivial Delaunay graphs - isolated feature points - by
increasing the number of nodes layer by layer up to graphs with maximal number
of nodes. For each layer of graphs its proper visual dictionary is built. The
experiments conducted on the SIVAL data set reveal that the graph features at
different layers exhibit complementary performances on the same content. The
nested approach, the combination of all existing layers, yields significant
improvement of the object recognition performance compared to single level
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2756</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2756</id><created>2011-06-14</created><authors><author><keyname>Ernvall-Hyt&#xf6;nen</keyname><forenames>Anne-Maria</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>On the Eavesdropper's Correct Decision in Gaussian and Fading Wiretap
  Channels Using Lattice Codes</title><categories>cs.CR</categories><comments>5 pages. Submitted to ITW 2011</comments><journal-ref>Proceedings of 2011 IEEE Inf. Theory Workshop ITW 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the probability of Eve the Eavesdropper's correct decision is
considered both in the Gaussian and Rayleigh fading wiretap channels when using
lattice codes for the transmission.
  First, it is proved that the secrecy function determining Eve's performance
attains its maximum at y=1 on all known extremal even unimodular lattices. This
is a special case of a conjecture by Belfiore and Sol\'e. Further, a very
simple method to verify or disprove the conjecture on any given unimodular
lattice is given.
  Second, preliminary analysis on the behavior of Eve's probability of correct
decision in the fast fading wiretap channel is provided. More specifically, we
compute the truncated inverse norm power sum factors in Eve's probability
expression. The analysis reveals a performance-secrecy-complexity tradeoff:
relaxing on the legitimate user's performance can significantly increase the
security of transmission. The confusion experienced by the eavesdropper may be
further increased by using skewed lattices, but at the cost of increased
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2766</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2766</id><created>2011-06-14</created><authors><author><keyname>Nogueira</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Pinho</keyname><forenames>Lu&#xed;s Miguel</forenames></author></authors><title>Supporting Parallelism in Server-based Multiprocessor Systems</title><categories>cs.DC cs.OS</categories><comments>WiP Session of the 31st IEEE Real-Time Systems Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing an efficient server-based real-time scheduling solution that
supports dynamic task-level parallelism is now relevant to even the desktop and
embedded domains and no longer only to the high performance computing market
niche. This paper proposes a novel approach that combines the constant
bandwidth server abstraction with a work-stealing load balancing scheme which,
while ensuring isolation among tasks, enables a task to be executed on more
than one processor at a given time instant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2769</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2769</id><created>2011-06-14</created><updated>2011-08-23</updated><authors><author><keyname>Iljazovic</keyname><forenames>Zvonko</forenames><affiliation>University of Zagreb, Croatia</affiliation></author></authors><title>Co-c.e. spheres and cells in computable metric spaces</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><acm-class>math.LO</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (August 25,
  2011) lmcs:885</journal-ref><doi>10.2168/LMCS-7(3:5)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate conditions under which a co-computably enumerable set in a
computable metric space is computable. Using higher-dimensional chains and
spherical chains we prove that in each computable metric space which is locally
computable each co-computably enumerable sphere is computable and each co-c.e.
cell with co-c.e. boundary sphere is computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2772</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2772</id><created>2011-06-14</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>Dominating Induced Matchings for P7-Free Graphs in Linear Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a finite undirected graph with edge set $E$. An edge set $E'
\subseteq E$ is an {\em induced matching} in $G$ if the pairwise distance of
the edges of $E'$ in $G$ is at least two; $E'$ is {\em dominating} in $G$ if
every edge $e \in E \setminus E'$ intersects some edge in $E'$. The
\emph{Dominating Induced Matching Problem} (\emph{DIM}, for short) asks for the
existence of an induced matching $E'$ which is also dominating in $G$; this
problem is also known as the \emph{Efficient Edge Domination} Problem.
  The DIM problem is related to parallel resource allocation problems, encoding
theory and network routing. It is \NP-complete even for very restricted graph
classes such as planar bipartite graphs with maximum degree three. However, its
complexity was open for $P_k$-free graphs for any $k \ge 5$; $P_k$ denotes a
chordless path with $k$ vertices and $k-1$ edges. We show in this paper that
the weighted DIM problem is solvable in linear time for $P_7$-free graphs in a
robust way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2773</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2773</id><created>2011-06-14</created><authors><author><keyname>Stockbridge</keyname><forenames>Richard H.</forenames></author><author><keyname>Zhu</keyname><forenames>Chao</forenames></author></authors><title>On Optimal Harvesting in Stochastic Environments: Optimal Policies in a
  Relaxed Model</title><categories>math.OC cs.SY q-bio.PE</categories><comments>Key Words: Singular stochastic control, linear programming, relaxed
  control</comments><msc-class>93E20, 60J60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the objective of optimally harvesting a single species in
a stochastic environment. This problem has previously been analyzed in Alvarez
(2000) using dynamic programming techniques and, due to the natural payoff
structure of the price rate function (the price decreases as the population
increases), no optimal harvesting policy exists. This paper establishes a
relaxed formulation of the harvesting model in such a manner that existence of
an optimal relaxed harvesting policy can not only be proven but also
identified. The analysis embeds the harvesting problem in an
infinite-dimensional linear program over a space of occupation measures in
which the initial position enters as a parameter and then analyzes an auxiliary
problem having fewer constraints. In this manner upper bounds are determined
for the optimal value (with the given initial position); these bounds depend on
the relation of the initial population size to a specific target size. The more
interesting case occurs when the initial population exceeds this target size; a
new argument is required to obtain a sharp upper bound. Though the initial
population size only enters as a parameter, the value is determined in a
closed-form functional expression of this parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2774</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2774</id><created>2011-06-14</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Orthogonal Matching Pursuit with Replacement</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of compressed sensing where the goal
is to recover almost all the sparse vectors using a small number of fixed
linear measurements. For this problem, we propose a novel partial
hard-thresholding operator that leads to a general family of iterative
algorithms. While one extreme of the family yields well known hard thresholding
algorithms like ITI (Iterative Thresholding with Inversion) and HTP (Hard
Thresholding Pursuit), the other end of the spectrum leads to a novel algorithm
that we call Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like
the classic greedy algorithm OMP, adds exactly one coordinate to the support at
each iteration, based on the correlation with the current residual. However,
unlike OMP, OMPR also removes one coordinate from the support. This simple
change allows us to prove that OMPR has the best known guarantees for sparse
recovery in terms of the Restricted Isometry Property (a condition on the
measurement matrix). In contrast, OMP is known to have very weak performance
guarantees under RIP. Given its simple structure, we are able to extend OMPR
using locality sensitive hashing to get OMPR-Hash, the first provably
sub-linear (in dimensionality) algorithm for sparse recovery. Our proof
techniques are novel and flexible enough to also permit the tightest known
analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit.
We provide experimental results on large problems providing recovery for
vectors of size up to million dimensions. We demonstrate that for large-scale
problems our proposed methods are more robust and faster than existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2781</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2781</id><created>2011-06-14</created><updated>2014-11-20</updated><authors><author><keyname>Feng</keyname><forenames>Runhuan</forenames></author><author><keyname>Volkmer</keyname><forenames>Hans</forenames></author><author><keyname>Zhang</keyname><forenames>Shuaiqi</forenames></author><author><keyname>Zhu</keyname><forenames>Chao</forenames></author></authors><title>Optimal Dividend Payments for the Piecewise-Deterministic Poisson Risk
  Model</title><categories>math.OC cs.SY math.PR q-fin.RM</categories><comments>Key Words: Piecewise-deterministic compound Poisson model, optimal
  stochastic control, HJB equation, quasi-variational inequality, threshold
  strategy, barrier strategy</comments><msc-class>93E20, 60J75</msc-class><journal-ref>Scandinavian Actuarial Journal, 2013, 1--31</journal-ref><doi>10.1080/03461238.2013.846277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimal dividend payment problem in
piecewise-deterministic compound Poisson risk models. The objective is to
maximize the expected discounted dividend payout up to the time of ruin. We
provide a comparative study in this general framework of both restricted and
unrestricted payment schemes, which were only previously treated separately in
certain special cases of risk models in the literature. In the case of
restricted payment scheme, the value function is shown to be a classical
solution of the corresponding HJB equation, which in turn leads to an optimal
restricted payment policy known as the threshold strategy. In the case of
unrestricted payment scheme, by solving the associated integro-differential
quasi-variational inequality, we obtain the value function as well as an
optimal unrestricted dividend payment scheme known as the barrier strategy.
When claim sizes are exponentially distributed, we provide easily verifiable
conditions under which the threshold and barrier strategies are optimal
restricted and unrestricted dividend payment policies, respectively. The main
results are illustrated with several examples, including a new example
concerning regressive growth rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2788</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2788</id><created>2011-06-14</created><authors><author><keyname>Cho</keyname><forenames>Yoon-Sik</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Co-evolution of Selection and Influence in Social Networks</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>In Proc. of the Twenty-Fifth Conference on Artificial Intelligence
  (AAAI-11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many networks are complex dynamical systems, where both attributes of nodes
and topology of the network (link structure) can change with time. We propose a
model of co-evolving networks where both node at- tributes and network
structure evolve under mutual influence. Specifically, we consider a mixed
membership stochastic blockmodel, where the probability of observing a link
between two nodes depends on their current membership vectors, while those
membership vectors themselves evolve in the presence of a link between the
nodes. Thus, the network is shaped by the interaction of stochastic processes
describing the nodes, while the processes themselves are influenced by the
changing network structure. We derive an efficient variational inference
procedure for our model, and validate the model on both synthetic and
real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2792</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2792</id><created>2011-06-14</created><authors><author><keyname>Li</keyname><forenames>Shizheng</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Algebraic codes for Slepian-Wolf code design</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted by ISIT 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Practical constructions of lossless distributed source codes (for the
Slepian-Wolf problem) have been the subject of much investigation in the past
decade. In particular, near-capacity achieving code designs based on LDPC codes
have been presented for the case of two binary sources, with a binary-symmetric
correlation. However, constructing practical codes for the case of non-binary
sources with arbitrary correlation remains by and large open. From a practical
perspective it is also interesting to consider coding schemes whose performance
remains robust to uncertainties in the joint distribution of the sources.
  In this work we propose the usage of Reed-Solomon (RS) codes for the
asymmetric version of this problem. We show that algebraic soft-decision
decoding of RS codes can be used effectively under certain correlation
structures. In addition, RS codes offer natural rate adaptivity and performance
that remains constant across a family of correlation structures with the same
conditional entropy. The performance of RS codes is compared with dedicated and
rate adaptive multistage LDPC codes (Varodayan et al. '06), where each LDPC
code is used to compress the individual bit planes. Our simulations show that
in classical Slepian-Wolf scenario, RS codes outperform both dedicated and
rate-adaptive LDPC codes under $q$-ary symmetric correlation, and are better
than rate-adaptive LDPC codes in the case of sparse correlation models, where
the conditional distribution of the sources has only a few dominant entries. In
a feedback scenario, the performance of RS codes is comparable with both
designs of LDPC codes. Our simulations also demonstrate that the performance of
RS codes in the presence of inaccuracies in the joint distribution of the
sources is much better as compared to multistage LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2794</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2794</id><created>2011-06-02</created><authors><author><keyname>p</keyname><forenames>Reshma.</forenames></author></authors><title>Power Management during Scan Based Sequential Circuit Testing</title><categories>cs.CE</categories><comments>ACIJ 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that not every scan cell contributes equally to the power
consumption during scan based test. The transitions at some scan cells cause
more toggles at the internal signal lines of a circuit than the transitions at
other scan cells. Hence the transitions at these scan cells have a larger
impact on the power consumption during test application. These scan cells are
called power sensitive scan cells.A verilog based approach is proposed to
identify a set of power sensitive scan cells. Additional hardware is added to
freeze the outputs of power sensitive scan cells during scan shifting in order
to reduce the shift power consumption.when multiple scan chain is incorporated
along with freezing the power sensitive scan cell,over all power during testing
can be reduced to a larger extend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2819</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2819</id><created>2011-06-14</created><updated>2012-02-07</updated><authors><author><keyname>Karout</keyname><forenames>Johnny</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Szczerba</keyname><forenames>Krzysztof</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author></authors><title>Optimizing Constellations for Single-Subcarrier Intensity-Modulated
  Optical Systems</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory, June 2011</comments><journal-ref>IEEE Transactions on Information Theory, vol. 58, no. 7, pp.
  4645-4659, July 2012</journal-ref><doi>10.1109/TIT.2012.2193374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We optimize modulation formats for the additive white Gaussian noise channel
with nonnegative input, also known as the intensity-modulated direct-detection
channel, with and without confining them to a lattice structure. Our
optimization criteria are the average electrical, average optical, and peak
power. The nonnegative constraint on the input to the channel is translated
into a conical constraint in signal space, and modulation formats are designed
by sphere packing inside this cone. Some dense packings are found, which yield
more power-efficient modulation formats than previously known. For example, at
a spectral efficiency of 1.5 bit/s/Hz, the modulation format optimized for
average electrical power has a 2.55 dB average electrical power gain over the
best known format to achieve a symbol error rate of 10^-6. The corresponding
gains for formats optimized for average and peak optical power are 1.35 and
1.72 dB, respectively. Using modulation formats optimized for peak power in
average-power limited systems results in a smaller power penalty than when
using formats optimized for average power in peak-power limited systems. We
also evaluate the modulation formats in terms of their mutual information to
predict their performance in the presence of capacity-achieving error-
correcting codes, and finally show numerically and analytically that the
optimal modulation formats for reliable transmission in the wideband regime
have only one nonzero point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2844</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2844</id><created>2011-06-14</created><updated>2012-06-19</updated><authors><author><keyname>Gurvits</keyname><forenames>Leonid</forenames></author></authors><title>Unleashing the power of Schrijver's permanental inequality with the help
  of the Bethe Approximation</title><categories>math.CO cs.CC cs.IT math-ph math.IT math.MP</categories><comments>30 pages, more typos are fixed, more remarks are added, importantly a
  concrete counter-example to [Lu,Mohr,Szekely] positive correlation conjecture
  is presented</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Let $A \in \Omega_n$ be doubly-stochastic $n \times n$ matrix. Alexander
Schrijver proved in 1998 the following remarkable inequality per(\widetilde{A})
\geq \prod_{1 \leq i,j \leq n} (1- A(i,j)); \widetilde{A}(i,j) =:
A(i,j)(1-A(i,j)), 1 \leq i,j \leq n.
  We use the above Shrijver's inequality to prove the following lower bound:
  \frac{per(A)}{F(A)} \geq 1; F(A) =: \prod_{1 \leq i,j \leq n} (1- A(i,j))^{1-
A(i,j)}.
  We use this new lower bound to prove S.Friedland's Asymptotic Lower Matching
Conjecture(LAMC) on monomer-dimer problem.
  We use some ideas of our proof of (LAMC) to disprove [Lu,Mohr,Szekely]
positive correlation conjecture.
  We present explicit doubly-stochastic $n \times n$ matrices $A$ with the
ratio $\frac{per(A)}{F(A)} = \sqrt{2}^{n}$; conjecture that
  \max_{A \in \Omega_n}\frac{per(A)}{F(A)} \approx (\sqrt{2})^{n} and give some
examples supporting the conjecture.
  If true, the conjecture (and other ones stated in the paper) would imply a
deterministic poly-time algorithm to approximate the permanent of $n \times n$
nonnegative matrices within the relative factor $(\sqrt{2})^{n}$. The best
current such factor is $e^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2872</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2872</id><created>2011-06-15</created><updated>2011-10-11</updated><authors><author><keyname>Deng</keyname><forenames>Yuxin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author></authors><title>Program Equivalence in Linear Contexts</title><categories>cs.PL cs.LO</categories><comments>Technical report, Laboratory for Computer Science, ISCAS</comments><report-no>ISCAS-SKLCS-11-04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program equivalence in linear contexts, where programs are used or executed
exactly once, is an important issue in programming languages. However, existing
techniques like those based on bisimulations and logical relations only target
at contextual equivalence in the usual (non-linear) functional languages, and
fail in capturing non-trivial equivalent programs in linear contexts,
particularly when non-determinism is present. We propose the notion of linear
contextual equivalence to formally characterize such program equivalence, as
well as a novel and general approach to studying it in higher-order languages,
based on labeled transition systems specifically designed for functional
languages. We show that linear contextual equivalence indeed coincides with
trace equivalence - it is sound and complete. We illustrate our technique in
both deterministic (a linear version of PCF) and non-deterministic (linear PCF
in Moggi's framework) functional languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2877</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2877</id><created>2011-06-15</created><authors><author><keyname>Sottile</keyname><forenames>Frank</forenames></author><author><keyname>Zhu</keyname><forenames>Chungang</forenames></author></authors><title>Injectivity of 2D Toric B\'{e}zier Patches</title><categories>cs.GR math.AG</categories><comments>4 pages, extended abstract, to be publised in Proceedings of
  CAD/Graphis 2011</comments><report-no>Mittag-Leffler-2011spring</report-no><msc-class>65D17, 14M25</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rational B\'{e}zier functions are widely used as mapping functions in surface
reparameterization, finite element analysis, image warping and morphing. The
injectivity (one-to-one property) of a mapping function is typically necessary
for these applications. Toric B\'{e}zier patches are generalizations of
classical patches (triangular, tensor product) which are defined on the convex
hull of a set of integer lattice points. We give a geometric condition on the
control points that we show is equivalent to the injectivity of every 2D toric
B\'{e}zier patch with those control points for all possible choices of weights.
This condition refines that of Craciun, et al., which only implied injectivity
on the interior of a patch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2882</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2882</id><created>2011-06-15</created><authors><author><keyname>Soklakov</keyname><forenames>Andrei N.</forenames></author></authors><title>Learning, investments and derivatives</title><categories>q-fin.GN cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent crisis and the following flight to simplicity put most derivative
businesses around the world under considerable pressure. We argue that the
traditional modeling techniques must be extended to include product design. We
propose a quantitative framework for creating products which meet the challenge
of being optimal from the investors point of view while remaining relatively
simple and transparent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2886</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2886</id><created>2011-06-15</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>The Finite Field Multi-Way Relay Channel with Correlated Sources: The
  Three-User Case</title><categories>cs.IT math.IT</categories><comments>to be presented at ISIT 2011</comments><journal-ref>Proceedings of the 2011 IEEE International Symposium on
  Information Theory (ISIT 2011), Saint Petersburg, Russia, pp. 2238-2242, July
  31-Aug. 5, 2011</journal-ref><doi>10.1109/ISIT.2011.6033958</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three-user finite field multi-way relay channel with correlated sources
is considered. The three users generate possibly correlated messages, and each
user is to transmit its message to the two other users reliably in the Shannon
sense. As there is no direct link among the users, communication is carried out
via a relay, and the link from the users to the relay and those from the relay
to the users are finite field adder channels with additive noise of arbitrary
distribution. The problem is to determine the set of all possible achievable
rates, defined as channel uses per source symbol for reliable communication.
For two classes of source/channel combinations, the solution is obtained using
Slepian-Wolf source coding combined with functional-decode-forward channel
coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2888</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2888</id><created>2011-06-15</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>On Achievable Rate Regions of the Asymmetric AWGN Two-Way Relay Channel</title><categories>cs.IT math.IT</categories><comments>to be presented at ISIT 2011</comments><journal-ref>Proceedings of the 2011 IEEE International Symposium on
  Information Theory (ISIT 2011), Saint Petersburg, Russia, pp. 84-88, July
  31-Aug. 5, 2011</journal-ref><doi>10.1109/ISIT.2011.6034256</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the additive white Gaussian noise two-way relay
channel, where two users exchange messages through a relay. Asymmetrical
channels are considered where the users can transmit data at different rates
and at different power levels. We modify and improve existing coding schemes to
obtain three new achievable rate regions. Comparing four downlink-optimal
coding schemes, we show that the scheme that gives the best sum-rate
performance is (i) complete-decode-forward, when both users transmit at low
signal-to-noise ratio (SNR); (ii) functional-decode-forward with nested lattice
codes, when both users transmit at high SNR; (iii) functional-decode-forward
with rate splitting and time-division multiplexing, when one user transmits at
low SNR and another user at medium--high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2902</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2902</id><created>2011-06-15</created><authors><author><keyname>O&#x15b;wi&#x119;cimka</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kwapie&#x144;</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Celi&#x144;ska</keyname><forenames>Iwona</forenames></author><author><keyname>Dro&#x17c;d&#x17c;</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Rak</keyname><forenames>Rafa&#x142;</forenames></author></authors><title>Computational approach to multifractal music</title><categories>physics.data-an cs.SD physics.comp-ph physics.soc-ph</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we perform a fractal analysis of 160 pieces of music belonging
to six different genres. We show that the majority of the pieces reveal
characteristics that allow us to classify them as physical processes called the
1/f (pink) noise. However, this is not true for classical music represented
here by Frederic Chopin's works and for some jazz pieces that are much more
correlated than the pink noise. We also perform a multifractal (MFDFA) analysis
of these music pieces. We show that all the pieces reveal multifractal
properties. The richest multifractal structures are observed for pop and rock
music. Also the viariably of multifractal features is best visible for popular
music genres. This can suggest that, from the multifractal perspective,
classical and jazz music is much more uniform than pieces of the most popular
genres of music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2927</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2927</id><created>2011-06-15</created><updated>2011-12-13</updated><authors><author><keyname>Kavehei</keyname><forenames>Omid</forenames></author><author><keyname>Al-Sarawi</keyname><forenames>Said</forenames></author><author><keyname>Cho</keyname><forenames>Kyoung-Rok</forenames></author><author><keyname>Eshraghian</keyname><forenames>Kamran</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>An Analytical Approach for Memristive Nanoarchitectures</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>12 pages, 10 figures, 4 tables</comments><doi>10.1109/TNANO.2011.2174802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As conventional memory technologies are challenged by their technological
physical limits, emerging technologies driven by novel materials are becoming
an attractive option for future memory architectures. Among these technologies,
Resistive Memories (ReRAM) created new possibilities because of their
nano-features and unique $I$-$V$ characteristics. One particular problem that
limits the maximum array size is interference from neighboring cells due to
sneak-path currents. A possible device level solution to address this issue is
to implement a memory array using complementary resistive switches (CRS).
Although the storage mechanism for a CRS is fundamentally different from what
has been reported for memristors (low and high resistances), a CRS is simply
formed by two series bipolar memristors with opposing polarities. In this paper
our intention is to introduce modeling principles that have been previously
verified through measurements and extend the simulation principles based on
memristors to CRS devices and hence provide an analytical approach to the
design of a CRS array. The presented approach creates the necessary design
methodology platform that will assist designers in implementation of CRS
devices in future systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2944</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2944</id><created>2011-06-15</created><updated>2013-06-10</updated><authors><author><keyname>Lenz</keyname><forenames>Matthias</forenames></author></authors><title>Matroids and log-concavity</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><msc-class>Primary: 05A20, 05B35, Secondary: 05E45, 05C31</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that f-vectors of matroid complexes of realisable matroids are
log-concave. This was conjectured by Mason in 1972. Our proof uses the recent
result by Huh and Katz who showed that the coefficients of the characteristic
polynomial of a realisable matroid form a log-concave sequence. We also discuss
the relationship between log-concavity of f-vectors and h-vectors of matroids.
In the last section we explain the connection between zonotopal algebra and
f-vectors and characteristic polynomials of matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2946</identifier>
 <datestamp>2011-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2946</id><created>2011-06-15</created><updated>2011-08-14</updated><authors><author><keyname>Gorla</keyname><forenames>Jagadeesh</forenames></author><author><keyname>Robertson</keyname><forenames>Stephen</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A Unified Relevance Retrieval Model by Eliteness Hypothesis</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an Eliteness Hypothesis for information retrieval is proposed,
where we define two generative processes to create information items and
queries. By assuming the deterministic relationships between the eliteness of
terms and relevance, we obtain a new theoretical retrieval framework. The
resulting ranking function is a unified one as it is capable of using available
relevance information on both the document and the query, which is otherwise
unachievable by existing retrieval models. Our preliminary experiment on a
simple ranking function has demonstrated the potential of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2992</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2992</id><created>2011-06-15</created><authors><author><keyname>van Tol</keyname><forenames>Michiel W.</forenames></author></authors><title>A Characterization of the SPARC T3-4 System</title><categories>cs.PF cs.DC cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report covers a set of experiments on the 64-core SPARC T3-4
system, comparing it to two similar AMD and Intel systems. Key characteristics
as maximum integer and floating point arithmetic throughput are measured as
well as memory throughput, showing the scalability of the SPARC T3-4 system.
The performance of POSIX threads primitives is characterized and compared in
detail, such as thread creation and mutex synchronization. Scalability tests
with a fine grained multithreaded runtime are performed, showing problems with
atomic CAS operations on such physically highly parallel systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2993</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2993</id><created>2011-06-15</created><updated>2011-09-22</updated><authors><author><keyname>Cenzer</keyname><forenames>Douglas</forenames><affiliation>University of Florida</affiliation></author><author><keyname>Brodhead</keyname><forenames>Paul</forenames><affiliation>Indian River State College</affiliation></author><author><keyname>Toska</keyname><forenames>Ferit</forenames><affiliation>University of Florida</affiliation></author><author><keyname>Wyman</keyname><forenames>Sebastian</forenames><affiliation>University of Florida</affiliation></author></authors><title>Algorithmic Randomness and Capacity of Closed Sets</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  23, 2011) lmcs:1020</journal-ref><doi>10.2168/LMCS-7(3:16)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connection between measure, capacity and algorithmic
randomness for the space of closed sets. For any computable measure m, a
computable capacity T may be defined by letting T(Q) be the measure of the
family of closed sets K which have nonempty intersection with Q. We prove an
effective version of Choquet's capacity theorem by showing that every
computable capacity may be obtained from a computable measure in this way. We
establish conditions on the measure m that characterize when the capacity of an
m-random closed set equals zero. This includes new results in classical
probability theory as well as results for algorithmic randomness. For certain
computable measures, we construct effectively closed sets with positive
capacity and with Lebesgue measure zero. We show that for computable measures,
a real q is upper semi-computable if and only if there is an effectively closed
set with capacity q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.2994</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.2994</id><created>2011-06-15</created><authors><author><keyname>Abdallah</keyname><forenames>Saeed</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Ioannis N.</forenames></author></authors><title>Widely Linear vs. Conventional Subspace-Based Estimation of SIMO
  Flat-Fading Channels: Mean-Squared Error Analysis</title><categories>cs.IT math.IT stat.OT</categories><comments>20 pages, 7 figures</comments><doi>10.1109/TSP.2011.2177261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the mean-squared error (MSE) performance of widely linear (WL) and
conventional subspace-based channel estimation for single-input multiple-output
(SIMO) flat-fading channels employing binary phase-shift-keying (BPSK)
modulation when the covariance matrix is estimated using a finite number of
samples. The conventional estimator suffers from a phase ambiguity that reduces
to a sign ambiguity for the WL estimator. We derive closed-form expressions for
the MSE of the two estimators under four different ambiguity resolution
scenarios. The first scenario is optimal resolution, which minimizes the
Euclidean distance between the channel estimate and the actual channel. The
second scenario assumes that a randomly chosen coefficient of the actual
channel is known and the third assumes that the one with the largest magnitude
is known. The fourth scenario is the more realistic case where pilot symbols
are used to resolve the ambiguities. Our work demonstrates that there is a
strong relationship between the accuracy of ambiguity resolution and the
relative performance of WL and conventional subspace-based estimators, and
shows that the less information available about the actual channel for
ambiguity resolution, or the lower the accuracy of this information, the higher
the performance gap in favor of the WL estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3037</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3037</id><created>2011-06-15</created><authors><author><keyname>Ilic</keyname><forenames>Aleksandar</forenames></author></authors><title>Efficient algorithm for the vertex connectivity of trapezoid graphs</title><categories>cs.DS math.CO</categories><comments>12 pages, 2 figures</comments><msc-class>05C85, 68R10, 05C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intersection graph of a collection of trapezoids with corner points lying
on two parallel lines is called a trapezoid graph. These graphs and their
generalizations were applied in various fields, including modeling channel
routing problems in VLSI design and identifying the optimal chain of
non-overlapping fragments in bioinformatics. Using modified binary indexed tree
data structure, we design an algorithm for calculating the vertex connectivity
of trapezoid graph $G$ with time complexity $O (n \log n)$, where $n$ is the
number of trapezoids. Furthermore, we establish sufficient and necessary
condition for a trapezoid graph $G$ to be bipartite and characterize trees that
can be represented as trapezoid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3054</identifier>
 <datestamp>2012-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3054</id><created>2011-06-15</created><updated>2012-05-08</updated><authors><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>The Complexity of Mean-Payoff Automaton Expression</title><categories>cs.LO cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1006.1492</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Quantitative languages are extension of boolean languages that assign to
each word a real number. Mean-payoff automata are finite automata with
numerical weights on transitions that assign to each infinite path the long-run
average of the transition weights. The class of \emph{mean-payoff automaton
expressions}, introduced in [1], is a class of quantitative languages, which is
robust: it is closed under the four pointwise operations of max, min, sum and
numerical complement.&quot;[1] In this paper we improve the computational complexity
for solving the classical decision problems for mean-payoff automaton
expressions: while the previously best known upper bound was 4EXPTIME, and no
lower bound was known, we give an optimal PSPACE complete bound. As a
consequence we also obtain a conceptually simple algorithm to solve the
classical decision problems for mean-payoff automaton expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3059</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3059</id><created>2011-06-15</created><updated>2013-12-09</updated><authors><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author></authors><title>Algorithmic Complexity for Short Binary Strings Applied to Psychology: A
  Primer</title><categories>cs.CC</categories><comments>To appear in Behavior Research Methods</comments><msc-class>91E45</msc-class><doi>10.3758/s13428-013-0416-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since human randomness production has been studied and widely used to assess
executive functions (especially inhibition), many measures have been suggested
to assess the degree to which a sequence is random-like. However, each of them
focuses on one feature of randomness, leading authors to have to use multiple
measures. Here we describe and advocate for the use of the accepted universal
measure for randomness based on algorithmic complexity, by means of a novel
previously presented technique using the the definition of algorithmic
probability. A re-analysis of the classical Radio Zenith data in the light of
the proposed measure and methodology is provided as a study case of an
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3077</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3077</id><created>2011-06-15</created><authors><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Chameleons in imagined conversations: A new approach to understanding
  coordination of linguistic style in dialogs</title><categories>cs.CL physics.soc-ph</categories><comments>data available at http://www.cs.cornell.edu/~cristian/movies</comments><acm-class>I.2.7; J.4</acm-class><journal-ref>Proceedings of the ACL workshop on Cognitive Modeling and
  Computational Linguistics, pp 76-87, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversational participants tend to immediately and unconsciously adapt to
each other's language styles: a speaker will even adjust the number of articles
and other function words in their next utterance in response to the number in
their partner's immediately preceding utterance. This striking level of
coordination is thought to have arisen as a way to achieve social goals, such
as gaining approval or emphasizing difference in status. But has the adaptation
mechanism become so deeply embedded in the language-generation process as to
become a reflex? We argue that fictional dialogs offer a way to study this
question, since authors create the conversations but don't receive the social
benefits (rather, the imagined characters do). Indeed, we find significant
coordination across many families of function words in our large movie-script
corpus. We also report suggestive preliminary findings on the effects of gender
and other features; e.g., surprisingly, for articles, on average, characters
adapt more to females than to males.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3094</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3094</id><created>2011-06-15</created><updated>2011-07-13</updated><authors><author><keyname>Cuenda</keyname><forenames>Sara</forenames></author><author><keyname>Crespo</keyname><forenames>Juan A.</forenames></author></authors><title>Simple rules govern finite-size effects in scale-free networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Published in Europhysics Letters</comments><journal-ref>EPL 95 (2011) 38002</journal-ref><doi>10.1209/0295-5075/95/38002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an intuitive though general explanation of the finite-size effect in
scale-free networks in terms of the degree distribution of the starting
network. This result clarifies the relevance of the starting network in the
final degree distribution. We use two different approaches: the deterministic
mean-field approximation used by Barab\'asi and Albert (but taking into account
the nodes of the starting network), and the probability distribution of the
degree of each node, which considers the stochastic process. Numerical
simulations show that the accuracy of the predictions of the mean-field
approximation depend on the contribution of the dispersion in the final
distribution. The results in terms of the probability distribution of the
degree of each node are very accurate when compared to numerical simulations.
The analysis of the standard deviation of the degree distribution allows us to
assess the influence of the starting core when fitting the model to real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3126</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3126</id><created>2011-06-15</created><authors><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Testing List H-Homomorphisms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H$ be an undirected graph. In the List $H$-Homomorphism Problem, given
an undirected graph $G$ with a list constraint $L(v) \subseteq V(H)$ for each
variable $v \in V(G)$, the objective is to find a list $H$-homomorphism $f:V(G)
\to V(H)$, that is, $f(v) \in L(v)$ for every $v \in V(G)$ and $(f(u),f(v)) \in
E(H)$ whenever $(u,v) \in E(G)$.
  We consider the following problem: given a map $f:V(G) \to V(H)$ as an oracle
access, the objective is to decide with high probability whether $f$ is a list
$H$-homomorphism or \textit{far} from any list $H$-homomorphisms. The
efficiency of an algorithm is measured by the number of accesses to $f$.
  In this paper, we classify graphs $H$ with respect to the query complexity
for testing list $H$-homomorphisms and show the following trichotomy holds: (i)
List $H$-homomorphisms are testable with a constant number of queries if and
only if $H$ is a reflexive complete graph or an irreflexive complete bipartite
graph. (ii) List $H$-homomorphisms are testable with a sublinear number of
queries if and only if $H$ is a bi-arc graph. (iii) Testing list
$H$-homomorphisms requires a linear number of queries if $H$ is not a bi-arc
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3134</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3134</id><created>2011-06-15</created><authors><author><keyname>Karimadini</keyname><forenames>Mohammad</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author></authors><title>Communicate only when necessary: Cooperative tasking for multi-agent
  systems</title><categories>cs.MA cs.SY math.OC</categories><comments>36 Pages, 4 figures</comments><report-no>Technical Report: NUS-ACT-11-003-Ver.1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New advances in large scale distributed systems have amazingly offered
complex functionalities through parallelism of simple and rudimentary
components. The key issue in cooperative control of multi-agent systems is the
synthesis of local control and interaction rules among the agents such that the
entire controlled system achieves a desired global behavior. For this purpose,
three fundamental problems have to be addressed: (1) task decomposition for
top-down design, such that the fulfillment of local tasks guarantees the
satisfaction of the global task, by the team; (2) fault-tolerant top-down
design, such that the global task remains decomposable and achievable, in spite
of some failures, and (3) design of interactions among agents to make an
undecomposable task decomposable and achievable in a top-down framework. The
first two problems have been addressed in our previous works, by identifying
necessary and sufficient conditions for task automaton decomposition, and
fault-tolerant task decomposability. This paper deals with the third problem
and proposes a procedure to redistribute the events among agents in order to
enforce decomposability of an undecomposable task automaton. The
decomposability conditions are used to identify the root causes of
undecomposability which are found to be due to over-communications that have to
be deleted, while respecting the fault-tolerant decomposability conditions; or
because of the lack of communications that require new sharing of events, while
considering new violations of decomposability conditions. This result provides
a sufficient condition to make any undecomposable deterministic task automaton
decomposable in order to facilitate cooperative tasking. Illustrative examples
are presented to show the concept of task automaton decomposabilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3153</identifier>
 <datestamp>2012-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3153</id><created>2011-06-16</created><updated>2012-02-14</updated><authors><author><keyname>Takahashi</keyname><forenames>Hayato</forenames></author></authors><title>Algorithmic analogies to kamae-Weiss theorem on normal numbers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study subsequences of random numbers. In Kamae (1973),
selection functions that depend only on coordinates are studied, and their
necessary and sufficient condition for the selected sequences to be normal
numbers is given. In van Lambalgen (1987), an algorithmic analogy to the
theorem is conjectured in terms of algorithmic randomness and Kolmogorov
complexity. In this paper, we show different algorithmic analogies to the
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3161</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3161</id><created>2011-06-16</created><updated>2011-11-22</updated><authors><author><keyname>Downey</keyname><forenames>Rodney G.</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Confronting Intractability via Parameters</title><categories>cs.CC</categories><comments>Accepted for publication in Computer Science Review [with some
  additional corrections]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One approach to confronting computational hardness is to try to understand
the contribution of various parameters to the running time of algorithms and
the complexity of computational tasks. Almost no computational tasks in real
life are specified by their size alone. It is not hard to imagine that some
parameters contribute more intractability than others and it seems reasonable
to develop a theory of computational complexity which seeks to exploit this
fact. Such a theory should be able to address the needs of practicioners in
algorithmics. The last twenty years have seen the development of such a theory.
This theory has a large number of successes in terms of a rich collection of
algorithmic techniques both practical and theoretical, and a fine-grained
intractability theory. Whilst the theory has been widely used in a number of
areas of applications including computational biology, linguistics, VLSI
design, learning theory and many others, knowledge of the area is highly
varied. We hope that this article will show both the basic theory and point at
the wide array of techniques available. Naturally the treatment is condensed,
and the reader who wants more should go to the texts, Downey and Fellows, Flum
and Grohe, Niedermeier, and the upcoming undergraduate text Downey and Fellows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3172</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3172</id><created>2011-06-16</created><authors><author><keyname>Baraglia</keyname><forenames>Ranieri</forenames></author><author><keyname>Dazzi</keyname><forenames>Patrizio</forenames></author><author><keyname>Mordacchini</keyname><forenames>Matteo</forenames></author><author><keyname>Ricci</keyname><forenames>Laura</forenames></author><author><keyname>Alessi</keyname><forenames>Luca</forenames></author></authors><title>On Democracy in Peer-to-Peer systems</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information flow inside a P2P network is highly dependent on the network
structure. In order to ease the diffusion of relevant data toward interested
peers, many P2P protocols gather similar nodes by putting them in direct
contact. With this approach the similarity between nodes is computed in a
point-to-point fashion: each peer individually identifies the nodes that share
similar interests with it. This leads to the creation of a sort of &quot;private&quot;
communities, limited to each peer neighbors list. This &quot;private&quot; knowledge do
not allow to identify the features needed to discover and characterize the
correlations that collect similar peers in broader groups. In order to let
these correlations to emerge, the collective knowledge of peers must be
exploited. One common problem to overcome in order to avoid the &quot;private&quot;
vision of the network, is related to how distributively determine the
representation of a community and how nodes may decide to belong to it. We
propose to use a gossip-like approach in order to let peers elect and identify
leaders of interest communities. Once leaders are elected, their profiles are
used as community representatives. Peers decide to adhere to a community or
another by choosing the most similar representative they know about.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3176</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3176</id><created>2011-06-16</created><authors><author><keyname>Kerbrat</keyname><forenames>Olivier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Mognol</keyname><forenames>Pascal</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Hasco&#xeb;t</keyname><forenames>Jean-Yves</forenames><affiliation>IRCCyN</affiliation></author></authors><title>A new DFM approach to combine machining and additive manufacturing</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Computers in Industry (2011) XXX-XXX</journal-ref><doi>10.1016/j.compind.2011.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design For Manufacturing (DFM) approaches aim to integrate manufacturability
aspects during the design stage. Most of DFM approaches usually consider only
one manufacturing process, but products competitiveness may be improved by
designing hybrid modular products, in which products are seen as 3-D puzzles
with modules realized aside by the best manufacturing process and further
gathered. A new DFM system is created in order to give quantitative information
during the product design stage of which modules will benefit in being machined
and which ones will advantageously be realized by an additive process (such as
Selective Laser Sintering or laser deposition). A methodology for a
manufacturability evaluation in case of a subtractive or an additive
manufacturing process is developed and implemented in a CAD software. Tests are
carried out on industrial products from automotive industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3184</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3184</id><created>2011-06-16</created><authors><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>The restricted isometry property for time-frequency structured random
  matrices</title><categories>cs.IT math.CA math.IT math.PR</categories><comments>25 pages</comments><msc-class>60B20, 42C40, 94A12</msc-class><journal-ref>Probab. Theory Related Fields, Vol. 156, num. 3--4, pp. 707-737,
  Aug. 2013</journal-ref><doi>10.1007/s00440-012-0441-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the restricted isometry property for finite dimensional Gabor
systems, that is, for families of time--frequency shifts of a randomly chosen
window function. We show that the $s$-th order restricted isometry constant of
the associated $n \times n^2$ Gabor synthesis matrix is small provided $s \leq
c \, n^{2/3} / \log^2 n$. This improves on previous estimates that exhibit
quadratic scaling of $n$ in $s$. Our proof develops bounds for a corresponding
chaos process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3242</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3242</id><created>2011-06-16</created><updated>2011-07-01</updated><authors><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Misra</keyname><forenames>Vishal</forenames></author></authors><title>The Public Option: a Non-regulatory Alternative to Network Neutrality</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network neutrality and the role of regulation on the Internet have been
heavily debated in recent times. Amongst the various definitions of network
neutrality, we focus on the one which prohibits paid prioritization of content
and we present an analytical treatment of the topic. We develop a model of the
Internet ecosystem in terms of three primary players: consumers, ISPs and
content providers. Our analysis looks at this issue from the point of view of
the consumer, and we describe the desired state of the system as one which
maximizes consumer surplus. By analyzing different scenarios of monopoly and
competition, we obtain different conclusions on the desirability of regulation.
We also introduce the notion of a Public Option ISP, an ISP that carries
traffic in a network neutral manner. Our major findings are (i) in a
monopolistic scenario, network neutral regulations benefit consumers; however,
the introduction of a Public Option ISP is even better for consumers, as it
aligns the interests of the monopolistic ISP with the consumer surplus and (ii)
in an oligopolistic situation, the presence of a Public Option ISP is again
preferable to network neutral regulations, although the presence of competing
price-discriminating ISPs provides the most desirable situation for the
consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3273</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3273</id><created>2011-06-16</created><updated>2012-05-07</updated><authors><author><keyname>Nutz</keyname><forenames>Marcel</forenames></author></authors><title>A Quasi-Sure Approach to the Control of Non-Markovian Stochastic
  Differential Equations</title><categories>math.PR cs.SY math.OC q-fin.RM</categories><comments>27 pages</comments><msc-class>93E20, 49L20, 60H10, 60G44, 91B30</msc-class><journal-ref>Electronic Journal of Probability, Vol. 17, No. 23, pp. 1-23, 2012</journal-ref><doi>10.1214/EJP.v17-1892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study stochastic differential equations (SDEs) whose drift and diffusion
coefficients are path-dependent and controlled. We construct a value process on
the canonical path space, considered simultaneously under a family of singular
measures, rather than the usual family of processes indexed by the controls.
This value process is characterized by a second order backward SDE, which can
be seen as a non-Markovian analogue of the Hamilton-Jacobi-Bellman partial
differential equation. Moreover, our value process yields a generalization of
the G-expectation to the context of SDEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3276</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3276</id><created>2011-06-16</created><authors><author><keyname>Kong</keyname><forenames>Lingchen</forenames></author><author><keyname>Tun&#xe7;el</keyname><forenames>Levent</forenames></author><author><keyname>Xiu</keyname><forenames>Naihua</forenames></author></authors><title>Sufficient Conditions for Low-rank Matrix Recovery, Translated from
  Sparse Signal Recovery</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The low-rank matrix recovery (LMR) is a rank minimization problem subject to
linear equality constraints, and it arises in many fields such as signal and
image processing, statistics, computer vision, system identification and
control. This class of optimization problems is $\N\P$-hard and a popular
approach replaces the rank function with the nuclear norm of the matrix
variable. In this paper, we extend the concept of $s$-goodness for a sensing
matrix in sparse signal recovery (proposed by Juditsky and Nemirovski [Math
Program, 2011]) to linear transformations in LMR. Then, we give
characterizations of $s$-goodness in the context of LMR. Using the two
characteristic $s$-goodness constants, ${\gamma}_s$ and $\hat{\gamma}_s$, of a
linear transformation, not only do we derive necessary and sufficient
conditions for a linear transformation to be $s$-good, but also provide
sufficient conditions for exact and stable $s$-rank matrix recovery via the
nuclear norm minimization under mild assumptions. Moreover, we give computable
upper bounds for one of the $s$-goodness characteristics which leads to
verifiable sufficient conditions for exact low-rank matrix recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3279</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3279</id><created>2011-06-16</created><updated>2012-07-19</updated><authors><author><keyname>Gu&#xe9;ant</keyname><forenames>Olivier</forenames></author><author><keyname>Lehalle</keyname><forenames>Charles-Albert</forenames></author><author><keyname>Tapia</keyname><forenames>Joaquin Fernandez</forenames></author></authors><title>Optimal Portfolio Liquidation with Limit Orders</title><categories>q-fin.TR cs.SY math.OC</categories><comments>Submitted, in revision</comments><doi>10.1137/110850475</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the optimal scheduling of the liquidation of a portfolio
using a new angle. Instead of focusing only on the scheduling aspect like
Almgren and Chriss, or only on the liquidity-consuming orders like Obizhaeva
and Wang, we link the optimal trade-schedule to the price of the limit orders
that have to be sent to the limit order book to optimally liquidate a
portfolio. Most practitioners address these two issues separately: they compute
an optimal trading curve and they then send orders to the markets to try to
follow it. The results obtained here solve simultaneously the two problems. As
in a previous paper that solved the &quot;intra-day market making problem&quot;, the
interactions of limit orders with the market are modeled via a Poisson process
pegged to a diffusive &quot;fair price&quot; and a Hamilton-Jacobi-Bellman equation is
used to solve the problem involving both non-execution risk and price risk.
Backtests are carried out to exemplify the use of our results, both on long
periods of time (for the entire liquidation process) and on slices of 5 minutes
(to follow a given trading curve).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3286</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3286</id><created>2011-06-16</created><authors><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>ReProCS: A Missing Link between Recursive Robust PCA and Recursive
  Sparse Recovery in Large but Correlated Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the recursive robust principal components' analysis (PCA)
problem. Here, &quot;robust&quot; refers to robustness to both independent and correlated
sparse outliers, although we focus on the latter. A key application where this
problem occurs is in video surveillance where the goal is to separate a slowly
changing background from moving foreground objects on-the-fly. The background
sequence is well modeled as lying in a low dimensional subspace, that can
gradually change over time, while the moving foreground objects constitute the
correlated sparse outliers. In this and many other applications, the foreground
is an outlier for PCA but is actually the &quot;signal of interest&quot; for the
application; where as the background is the corruption or noise. Thus our
problem can also be interpreted as one of recursively recovering a time
sequence of sparse signals in the presence of large but spatially correlated
noise.
  This work has two key contributions. First, we provide a new way of looking
at this problem and show how a key part of our solution strategy involves
solving a noisy compressive sensing (CS) problem. Second, we show how we can
utilize the correlation of the outliers to our advantage in order to even deal
with very large support sized outliers. The main idea is as follows. The
correlation model applied to the previous support estimate helps predict the
current support. This prediction serves as &quot;partial support knowledge&quot; for
solving the modified-CS problem instead of CS. The support estimate of the
modified-CS reconstruction is, in turn, used to update the correlation model
parameters using a Kalman filter (or any adaptive filter). We call the
resulting approach &quot;support-predicted modified-CS&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3305</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3305</id><created>2011-06-16</created><authors><author><keyname>Graham</keyname><forenames>Matthew J.</forenames></author></authors><title>The Art of Data Science</title><categories>astro-ph.IM cs.DL</categories><comments>12 pages, invited talk at Astrostatistics and Data Mining in Large
  Astronomical Databases workshop, La Palma, Spain, 30 May - 3 June 2011, to
  appear in Springer Series on Astrostatistics</comments><doi>10.1007/978-1-4614-3323-1_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To flourish in the new data-intensive environment of 21st century science, we
need to evolve new skills. These can be expressed in terms of the systemized
framework that formed the basis of mediaeval education - the trivium (logic,
grammar, and rhetoric) and quadrivium (arithmetic, geometry, music, and
astronomy). However, rather than focusing on number, data is the new keystone.
We need to understand what rules it obeys, how it is symbolized and
communicated and what its relationship to physical space and time is. In this
paper, we will review this understanding in terms of the technologies and
processes that it requires. We contend that, at least, an appreciation of all
these aspects is crucial to enable us to extract scientific information and
knowledge from the data sets which threaten to engulf and overwhelm us.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3314</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3314</id><created>2011-06-16</created><updated>2011-09-29</updated><authors><author><keyname>Gitlin</keyname><forenames>Roman</forenames></author></authors><title>Reducing Interpolation on Multi-Grid to Quantizing Grid's Data-Base as a
  Recursion</title><categories>cs.DC cs.CG cs.PL</categories><comments>75 pages, 2 code snippets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his article &quot;Powerlist: A Structure for Parallel Recursion&quot; Jayadev Misra
wrote:
  &quot;Many data parallel algorithms Fast Fourier Transform, Batcher's sorting
schemes and prefix sum -exhibit recursive structure. We propose a data
structure, powerlist, that permits succinct descriptions of such algorithms,
highlighting the roles of both parallelism and recursion. Simple algebraic
properties of this data structure can be exploited to derive properties of
these algorithms and establish equivalence of different algorithms that solve
the same problem.&quot;
  The quote above illustrates a widely shared assumption about recursion
implementations: either they are done in purely structural terms or they cannot
be done at all.
  Multi-dimensional interpolation on a grid is one of hosts of semi-recursive
schemes that, while often referred to as recursive and routinely described in
vaguely recursive terms, cannot be implemented as a recursion in their
structural entirety.
  This article describes a computer-implemented scheme for isolating the
recursive core of interpolation on a multi-grid, an arrangement that both stems
from and provides a structural framework to a number of multi-dimensional
interpolation optimization techniques that, once implemented, provide gains in
multi-dimensional interpolation speed that, compared to some known benchmarks,
measure in multiple orders of magnitude.
  Categories and Subject Descriptors: Multi-dimensional Programming; Concurrent
Programming; Recursion
  General terms: Parallel Processing, Prioritized Processing, Interpolation,
Recursion, Multi-Cube
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3325</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3325</id><created>2011-06-14</created><authors><author><keyname>Wilkerson</keyname><forenames>Daniel Shawcross</forenames></author><author><keyname>Goldsmith</keyname><forenames>Simon Fredrick Vicente</forenames></author><author><keyname>Barrett</keyname><forenames>Ryan</forenames></author><author><keyname>Armbrust</keyname><forenames>Erick</forenames></author><author><keyname>Johnson</keyname><forenames>Robert</forenames></author><author><keyname>Fuller</keyname><forenames>Alfred</forenames></author></authors><title>Distributed Transactions for Google App Engine: Optimistic Distributed
  Transactions built upon Local Multi-Version Concurrency Control</title><categories>cs.DC cs.DB cs.DS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massively scalable web applications encounter a fundamental tension in
computing between &quot;performance&quot; and &quot;correctness&quot;: performance is often
addressed by using a large and therefore distributed machine where programs are
multi-threaded and interruptible, whereas correctness requires data invariants
to be maintained with certainty. A solution to this problem is &quot;transactions&quot;
[Gray-Reuter].
  Some distributed systems such as Google App Engine
[http://code.google.com/appengine/docs/] provide transaction semantics but only
for functions that access one of a set of predefined local regions of the
database: a &quot;Local Transaction&quot; (LT)
[http://code.google.com/appengine/docs/python/datastore/transactions.html]. To
address this problem we give a &quot;Distributed Transaction&quot; (DT) algorithm which
provides transaction semantics for functions that operate on any set of objects
distributed across the machine. Our algorithm is in an &quot;optimistic&quot;
[http://en.wikipedia.org/wiki/Optimistic_concurrency_control] style. We assume
Sequential [Time-]Consistency
[http://en.wikipedia.org/wiki/Sequential_consistency] for Local Transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3326</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3326</id><created>2011-06-16</created><authors><author><keyname>Andolfatto</keyname><forenames>Lo&#xef;c</forenames><affiliation>LURPA</affiliation></author><author><keyname>Mayer</keyname><forenames>Ren&#xe9;</forenames><affiliation>LRFV</affiliation></author><author><keyname>Lavernhe</keyname><forenames>Sylvain</forenames><affiliation>LURPA</affiliation></author></authors><title>Adaptive Monte Carlo applied to uncertainty estimation in a five axis
  machine tool link errors identification</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>International Journal of Machine Tools and Manufacture 51, 7-8
  (2011) July-August 2011, Pages 618-627</journal-ref><doi>10.1016/j.ijmachtools.2011.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge of a machine tool axis to axis location errors allows compensation
and correcting actions to be taken to enhance its volumetric accuracy. Several
procedures exist, involving either lengthy individual test for each geometric
error or faster single tests to identify all errors at once. This study focuses
on the closed kinematic Cartesian chain method which uses a single setup test
to identify the eight link errors of a five axis machine tool. The
identification is based on volumetric error measurements for different poses
with a non-contact measuring instrument called CapBall, developed in house. In
order to evaluate the uncertainty on each identified error, a multi-output
Monte Carlo approach is implemented. Uncertainty sources in the measurement and
identification chain - such as sensors output, machine drift and frame
transformation uncertainties - can be included in the model and propagated to
the identified errors. The estimated uncertainties are finally compared to
experimental results to assess the method. It shows that the effect of the
drift, a disturbance, must be simulated as a function of time the Monte Carlo
approach. The machine drift is found to be an important uncertainty in sources
for the machine tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3348</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3348</id><created>2011-06-16</created><updated>2011-11-01</updated><authors><author><keyname>M&#xe9;ndez-D&#xed;az</keyname><forenames>Isabel</forenames></author><author><keyname>Nasini</keyname><forenames>Graciela</forenames></author><author><keyname>Severin</keyname><forenames>Daniel</forenames></author></authors><title>A polyhedral approach for the Equitable Coloring Problem</title><categories>cs.DM</categories><msc-class>90C57, 05C15</msc-class><journal-ref>Discrete Applied Mathematics. Volume 164, Part 2 (2014), p.
  413--426</journal-ref><doi>10.1016/j.dam.2012.11.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the polytope associated with a 0,1-integer programming
formulation for the Equitable Coloring Problem. We find several families of
valid inequalities and derive sufficient conditions in order to be
facet-defining inequalities. We also present computational evidence that shows
the efficacy of these inequalities used in a cutting-plane algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3349</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3349</id><created>2011-06-16</created><authors><author><keyname>M&#xe9;ndez-D&#xed;az</keyname><forenames>Isabel</forenames></author><author><keyname>Nasini</keyname><forenames>Graciela</forenames></author><author><keyname>Severin</keyname><forenames>Daniel</forenames></author></authors><title>Polyhedral results for the Equitable Coloring Problem</title><categories>cs.DM</categories><msc-class>90C27, 05C15</msc-class><journal-ref>Electronic Notes in Discrete Mathematics. Volume 37 (2011), p.
  159--164</journal-ref><doi>10.1016/j.endm.2011.05.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the polytope associated with a 0/1 integer programming
formulation for the Equitable Coloring Problem. We find several families of
valid inequalities and derive sufficient conditions in order to be
facet-defining inequalities. We also present computational evidence of the
effectiveness of including these inequalities as cuts in a Branch &amp; Cut
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3355</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3355</id><created>2011-06-16</created><updated>2012-03-05</updated><authors><author><keyname>Martin</keyname><forenames>Ryan</forenames></author><author><keyname>Tilak</keyname><forenames>Omkar</forenames></author></authors><title>On epsilon-optimality of the pursuit learning algorithm</title><categories>cs.LG</categories><journal-ref>Journal of Applied Probability, 49(3), 795-805, 2012</journal-ref><doi>10.1239/jap/1346955334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimator algorithms in learning automata are useful tools for adaptive,
real-time optimization in computer science and engineering applications. This
paper investigates theoretical convergence properties for a special case of
estimator algorithms: the pursuit learning algorithm. In this note, we identify
and fill a gap in existing proofs of probabilistic convergence for pursuit
learning. It is tradition to take the pursuit learning tuning parameter to be
fixed in practical applications, but our proof sheds light on the importance of
a vanishing sequence of tuning parameters in a theoretical convergence
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3361</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3361</id><created>2011-06-16</created><authors><author><keyname>Kursa</keyname><forenames>Miron B.</forenames></author><author><keyname>Komsta</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Rudnicki</keyname><forenames>Witold R.</forenames></author></authors><title>Random forest models of the retention constants in the thin layer
  chromatography</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current study we examine an application of the machine learning
methods to model the retention constants in the thin layer chromatography
(TLC). This problem can be described with hundreds or even thousands of
descriptors relevant to various molecular properties, most of them redundant
and not relevant for the retention constant prediction. Hence we employed
feature selection to significantly reduce the number of attributes.
Additionally we have tested application of the bagging procedure to the feature
selection. The random forest regression models were built using selected
variables. The resulting models have better correlation with the experimental
data than the reference models obtained with linear regression. The
cross-validation confirms robustness of the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3369</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3369</id><created>2011-06-16</created><authors><author><keyname>Kumar</keyname><forenames>Bimal Aklesh</forenames></author></authors><title>Evaluation of Fiji National University Campus Information Systems</title><categories>cs.HC</categories><comments>vol. 2 no. 3 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fiji National University (FNU) has been encountering many difficulties with
its current campus administrative systems. These difficulties include
accessibility, scalability, performance, flexibility and integration. In order
to address these difficulties, we developed a thin client web based campus
information system. The newly designed system allows the students, academic and
administration staff of the university to handle their day to day affairs with
the university online. In this paper we describe three types of evaluation
carried out to determine the suitability of newly developed system for FNU
environment. User interface evaluation was carried out to assess user interface
on a set of usability principles, usability evaluation to see the ease at which
users can use the system and finally performance evaluation to verify and
validate user response time required to complete various tasks. The result of
each of these evaluations were analysed and the system was rectified as part of
iterative design process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3373</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3373</id><created>2011-06-16</created><updated>2013-03-09</updated><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Perturbation Analysis of Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>29 pages</comments><journal-ref>IEEE Transactions on Signal Processing, 61(2): 398-410, 2013</journal-ref><doi>10.1109/TSP.2012.2222377</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal Matching Pursuit (OMP) is a canonical greedy pursuit algorithm for
sparse approximation. Previous studies of OMP have mainly considered the exact
recovery of a sparse signal $\bm x$ through $\bm \Phi$ and $\bm y=\bm \Phi \bm
x$, where $\bm \Phi$ is a matrix with more columns than rows. In this paper,
based on Restricted Isometry Property (RIP), the performance of OMP is analyzed
under general perturbations, which means both $\bm y$ and $\bm \Phi$ are
perturbed. Though exact recovery of an almost sparse signal $\bm x$ is no
longer feasible, the main contribution reveals that the exact recovery of the
locations of $k$ largest magnitude entries of $\bm x$ can be guaranteed under
reasonable conditions. The error between $\bm x$ and solution of OMP is also
estimated. It is also demonstrated that the sufficient condition is rather
tight by constructing an example. When $\bm x$ is strong-decaying, it is proved
that the sufficient conditions can be relaxed, and the locations can even be
recovered in the order of the entries' magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3381</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3381</id><created>2011-06-17</created><authors><author><keyname>Cui</keyname><forenames>Hongfei</forenames></author><author><keyname>Sun</keyname><forenames>Jianqiang</forenames></author><author><keyname>Ding</keyname><forenames>Yiming</forenames></author></authors><title>The rates of convergence for generalized entropy of the normalized sums
  of IID random variables</title><categories>cs.IT math.IT math.PR</categories><comments>7 pages</comments><msc-class>94A17, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the generalized differential entropy of normalized sums of
independent and identically distributed (IID) continuous random variables. We
prove that the R\'{e}nyi entropy and Tsallis entropy of order $\alpha\
(\alpha&gt;0)$ of the normalized sum of IID continuous random variables with
bounded moments are convergent to the corresponding R\'{e}nyi entropy and
Tsallis entropy of the Gaussian limit, and obtain sharp rates of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3395</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3395</id><created>2011-06-17</created><authors><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LITIS</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>Decoding finger movements from ECoG signals using switching linear
  models</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges of ECoG-based Brain-Machine Interfaces is the
movement prediction of a human subject. Several methods exist to predict an arm
2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is
to predict individual finger movements (5-D trajectory). The difficulty lies in
the fact that there is no simple relation between ECoG signals and finger
movement. We propose in this paper to decode finger flexions using switching
models. This method permits to simplify the system as it is now described as an
ensemble of linear models depending on an internal state. We show that an
interesting accuracy prediction can be obtained by such a model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3396</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3396</id><created>2011-06-17</created><authors><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LITIS</affiliation></author><author><keyname>Labb&#xe9;</keyname><forenames>Benjamin</forenames><affiliation>LITIS</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>Large margin filtering for signal sequence labeling</title><categories>cs.LG</categories><comments>IEEE International Conference on Acoustics Speech and Signal
  Processing (ICASSP), 2010, Dallas : United States (2010)</comments><proxy>ccsd</proxy><doi>10.1109/ICASSP.2010.5495281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal Sequence Labeling consists in predicting a sequence of labels given an
observed sequence of samples. A naive way is to filter the signal in order to
reduce the noise and to apply a classification algorithm on the filtered
samples. We propose in this paper to jointly learn the filter with the
classifier leading to a large margin filtering for classification. This method
allows to learn the optimal cutoff frequency and phase of the filter that may
be different from zero. Two methods are proposed and tested on a toy dataset
and on a real life BCI dataset from BCI Competition III.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3397</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3397</id><created>2011-06-17</created><authors><author><keyname>Niaf</keyname><forenames>Emilie</forenames><affiliation>CREATIS</affiliation></author><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LITIS</affiliation></author><author><keyname>Lartizien</keyname><forenames>Carole</forenames><affiliation>CREATIS</affiliation></author><author><keyname>Canu</keyname><forenames>St&#xe9;phane</forenames><affiliation>LITIS</affiliation></author></authors><title>Handling uncertainties in SVM classification</title><categories>cs.LG</categories><comments>IEEE Workshop on Statistical Signal Processing, Nice: France (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the pattern classification problem arising when
available target data include some uncertainty information. Target data
considered here is either qualitative (a class label) or quantitative (an
estimation of the posterior probability). Our main contribution is a SVM
inspired formulation of this problem allowing to take into account class label
through a hinge loss as well as probability estimates using epsilon-insensitive
cost function together with a minimum norm (maximum margin) objective. This
formulation shows a dual form leading to a quadratic problem and allows the use
of a representer theorem and associated kernel. The solution provided can be
used for both decision and posterior probability estimation. Based on empirical
evidence our method outperforms regular SVM in terms of probability predictions
and classification performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3402</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3402</id><created>2011-06-17</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>The Capacity Region of the Linear Shift Deterministic Y-Channel</title><categories>cs.IT math.IT</categories><comments>to appear in ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear shift deterministic Y-channel is studied. That is, we have three
users and one relay, where each user wishes to broadcast one message to each
other user via the relay, resulting in a multi-way relaying setup. The cut-set
bounds for this setup are shown to be not sufficient to characterize its
capacity region. New upper bounds are derived, which when combined with the
cut-set bounds provide an outer bound on the capacity region. It is shown that
this outer bound is achievable, and as a result, the capacity region of the
linear shift deterministic Y-channel is characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3409</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3409</id><created>2011-06-17</created><updated>2012-01-17</updated><authors><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Collings</keyname><forenames>Ian B.</forenames></author></authors><title>System Identification in Wireless Relay Networks via Gaussian Process</title><categories>cs.IT math.IT stat.AP</categories><comments>28 pages, 9 figures, one table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a flexible stochastic model for a class of cooperative wireless
relay networks, in which the relay processing functionality is not known at the
destination. In addressing this problem we develop efficient algorithms to
perform relay identification in a wireless relay network. We first construct a
statistical model based on a representation of the system using Gaussian
Processes in a non-standard manner due to the way we treat the imperfect
channel state information. We then formulate the estimation problem to perform
system identification, taking into account complexity and computational
efficiency. Next we develop a set of three algorithms to solve the
identification problem each of decreasing complexity, trading-off the
estimation bias for computational efficiency. The joint optimisation problem is
tackled via a Bayesian framework using the Iterated Conditioning on the Modes
methodology. We develop a lower bound and several sub-optimal computationally
efficient solutions to the identification problem, for comparison. We
illustrate the estimation performance of our methodology for a range of widely
used relay functionalities. The relative total error attained by our algorithm
when compared to the lower bound is found to be at worst 9% for low SNR values
under all functions considered. The effect of the relay functional estimation
error is also studied via BER simulations and is shown to be less than 2dB
worse than the lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3445</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3445</id><created>2011-06-17</created><updated>2011-08-23</updated><authors><author><keyname>Lakin</keyname><forenames>Matthew R.</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Constraint solving in non-permutative nominal abstract syntax</title><categories>cs.PL</categories><proxy>LMCS</proxy><acm-class>D.3.1, D.3.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (August 25,
  2011) lmcs:995</journal-ref><doi>10.2168/LMCS-7(3:6)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nominal abstract syntax is a popular first-order technique for encoding, and
reasoning about, abstract syntax involving binders. Many of its applications
involve constraint solving. The most commonly used constraint solving algorithm
over nominal abstract syntax is the Urban-Pitts-Gabbay nominal unification
algorithm, which is well-behaved, has a well-developed theory and is applicable
in many cases. However, certain problems require a constraint solver which
respects the equivariance property of nominal logic, such as Cheney's
equivariant unification algorithm. This is more powerful but is more
complicated and computationally hard. In this paper we present a novel
algorithm for solving constraints over a simple variant of nominal abstract
syntax which we call non-permutative. This constraint problem has similar
complexity to equivariant unification but without many of the additional
complications of the equivariant unification term language. We prove our
algorithm correct, paying particular attention to issues of termination, and
present an explicit translation of name-name equivariant unification problems
into non-permutative constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3448</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3448</id><created>2011-06-17</created><updated>2013-02-13</updated><authors><author><keyname>Krebbers</keyname><forenames>Robbert</forenames></author><author><keyname>Spitters</keyname><forenames>Bas</forenames></author></authors><title>Type classes for efficient exact real arithmetic in Coq</title><categories>cs.LO math.NA</categories><comments>arXiv admin note: text overlap with arXiv:1105.2751</comments><proxy>Logical Methods In Computer Science</proxy><acm-class>D.2.4; F.4.1; G.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  14, 2013) lmcs:958</journal-ref><doi>10.2168/LMCS-9(1:01)2013</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Floating point operations are fast, but require continuous effort on the part
of the user in order to ensure that the results are correct. This burden can be
shifted away from the user by providing a library of exact analysis in which
the computer handles the error estimates. Previously, we [Krebbers/Spitters
2011] provided a fast implementation of the exact real numbers in the Coq proof
assistant. Our implementation improved on an earlier implementation by O'Connor
by using type classes to describe an abstract specification of the underlying
dense set from which the real numbers are built. In particular, we used dyadic
rationals built from Coq's machine integers to obtain a 100 times speed up of
the basic operations already. This article is a substantially expanded version
of [Krebbers/Spitters 2011] in which the implementation is extended in the
various ways. First, we implement and verify the sine and cosine function.
Secondly, we create an additional implementation of the dense set based on
Coq's fast rational numbers. Thirdly, we extend the hierarchy to capture order
on undecidable structures, while it was limited to decidable structures before.
This hierarchy, based on type classes, allows us to share theory on the
naturals, integers, rationals, dyadics, and reals in a convenient way. Finally,
we obtain another dramatic speed-up by avoiding evaluation of termination
proofs at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3456</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3456</id><created>2011-06-17</created><authors><author><keyname>Reddy</keyname><forenames>P. Venkata Subba</forenames></author><author><keyname>Iyer</keyname><forenames>K. Viswanathan</forenames></author></authors><title>Conditional and Unique Coloring of Graphs</title><categories>cs.DM math.CO</categories><comments>Under review in International Journal of Computer Mathematics</comments><msc-class>68R10, 05C15</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For integers $k, r &gt; 0$, a conditional $(k,r)$-coloring of a graph $G$ is a
proper $k$-coloring of the vertices of $G$ such that every vertex $v$ of degree
$d(v)$ in $G$ is adjacent to at least $\min\{r, d(v)\}$ differently colored
vertices. Given $r$, the smallest integer $k$ for which $G$ has a conditional
$(k,r)$-coloring is called the $r$th order conditional chromatic number
$\chi_r(G)$ of $G$. We give results (exact values or bounds for $\chi_r(G)$,
depending on $r$) related to the conditional coloring of some graphs. We
introduce \emph{unique conditional colorability} and give some related results.
(Keywords. cartesian product of graphs; conditional chromatic number; gear
graph; join of graphs.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3457</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3457</id><created>2011-06-17</created><authors><author><keyname>Charalambidis</keyname><forenames>A.</forenames></author><author><keyname>Handjopoulos</keyname><forenames>K.</forenames></author><author><keyname>Rondogiannis</keyname><forenames>P.</forenames></author><author><keyname>Wadge</keyname><forenames>W. W.</forenames></author></authors><title>Extensional Higher-Order Logic Programming</title><categories>cs.PL cs.AI cs.LO</categories><comments>45 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a purely extensional semantics for higher-order logic programming.
In this semantics program predicates denote sets of ordered tuples, and two
predicates are equal iff they are equal as sets. Moreover, every program has a
unique minimum Herbrand model which is the greatest lower bound of all Herbrand
models of the program and the least fixed-point of an immediate consequence
operator. We also propose an SLD-resolution proof procedure which is proven
sound and complete with respect to the minimum model semantics. In other words,
we provide a purely extensional theoretical framework for higher-order logic
programming which generalizes the familiar theory of classical (first-order)
logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3464</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3464</id><created>2011-06-17</created><authors><author><keyname>Bhowmik</keyname><forenames>Mrinal Kanti</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>Polar Fusion Technique Analysis for Evaluating the Performances of Image
  Fusion of Thermal and Visual Images for Human Face Recognition</title><categories>cs.CV</categories><comments>Proceedings of IEEE Workshop on Computational Intelligence in
  Biometrics and Identity Management (IEEE CIBIM 2011), Paris, France, April 11
  - 15, 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a comparative study of two different methods, which are
based on fusion and polar transformation of visual and thermal images. Here,
investigation is done to handle the challenges of face recognition, which
include pose variations, changes in facial expression, partial occlusions,
variations in illumination, rotation through different angles, change in scale
etc. To overcome these obstacles we have implemented and thoroughly examined
two different fusion techniques through rigorous experimentation. In the first
method log-polar transformation is applied to the fused images obtained after
fusion of visual and thermal images whereas in second method fusion is applied
on log-polar transformed individual visual and thermal images. After this step,
which is thus obtained in one form or another, Principal Component Analysis
(PCA) is applied to reduce dimension of the fused images. Log-polar transformed
images are capable of handling complicacies introduced by scaling and rotation.
The main objective of employing fusion is to produce a fused image that
provides more detailed and reliable information, which is capable to overcome
the drawbacks present in the individual visual and thermal face images.
Finally, those reduced fused images are classified using a multilayer
perceptron neural network. The database used for the experiments conducted here
is Object Tracking and Classification Beyond Visible Spectrum (OTCBVS) database
benchmark thermal and visual face images. The second method has shown better
performance, which is 95.71% (maximum) and on an average 93.81% as correct
recognition rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3466</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3466</id><created>2011-06-17</created><authors><author><keyname>Bhowmik</keyname><forenames>Mrinal Kanti</forenames></author><author><keyname>Majumdar</keyname><forenames>Gautam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>Next Level of Data Fusion for Human Face Recognition</title><categories>cs.CV</categories><comments>Keywords: Thermal Image, Visual Image, Fused Image, Data Fusion,
  Wavelet decomposition, Decision Fusion, Classification</comments><journal-ref>Proceedings of International Conference on Mathematical Modeling
  and Applications to Industrial Problems (MMIP' 11), National Institute of
  Technology, Calicut, India, March 28-31, 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper demonstrates two different fusion techniques at two different
levels of a human face recognition process. The first one is called data fusion
at lower level and the second one is the decision fusion towards the end of the
recognition process. At first a data fusion is applied on visual and
corresponding thermal images to generate fused image. Data fusion is
implemented in the wavelet domain after decomposing the images through
Daubechies wavelet coefficients (db2). During the data fusion maximum of
approximate and other three details coefficients are merged together. After
that Principle Component Analysis (PCA) is applied over the fused coefficients
and finally two different artificial neural networks namely Multilayer
Perceptron(MLP) and Radial Basis Function(RBF) networks have been used
separately to classify the images. After that, for decision fusion based
decisions from both the classifiers are combined together using Bayesian
formulation. For experiments, IRIS thermal/visible Face Database has been used.
Experimental results show that the performance of multiple classifier system
along with decision fusion works well over the single classifier system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3467</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3467</id><created>2011-06-17</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>High Performance Human Face Recognition using Independent High Intensity
  Gabor Wavelet Responses: A Statistical Approach</title><categories>cs.CV</categories><comments>Keywords: Feature extraction; Gabor Wavelets; independent
  high-intensity feature (IHIF); Independent Component Analysis (ICA);
  Specificity; Sensitivity; Cosine Similarity Measure; E-ISSN: 2044-6004</comments><journal-ref>International Journal of Computer Science &amp; Emerging Technologies
  pp 178-187, Volume 2, Issue 1, February 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a technique by which high-intensity feature vectors
extracted from the Gabor wavelet transformation of frontal face images, is
combined together with Independent Component Analysis (ICA) for enhanced face
recognition. Firstly, the high-intensity feature vectors are automatically
extracted using the local characteristics of each individual face from the
Gabor transformed images. Then ICA is applied on these locally extracted
high-intensity feature vectors of the facial images to obtain the independent
high intensity feature (IHIF) vectors. These IHIF forms the basis of the work.
Finally, the image classification is done using these IHIF vectors, which are
considered as representatives of the images. The importance behind implementing
ICA along with the high-intensity features of Gabor wavelet transformation is
twofold. On the one hand, selecting peaks of the Gabor transformed face images
exhibit strong characteristics of spatial locality, scale, and orientation
selectivity. Thus these images produce salient local features that are most
suitable for face recognition. On the other hand, as the ICA employs locally
salient features from the high informative facial parts, it reduces redundancy
and represents independent features explicitly. These independent features are
most useful for subsequent facial discrimination and associative recall. The
efficiency of IHIF method is demonstrated by the experiment on frontal facial
images dataset, selected from the FERET, FRAV2D, and the ORL database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3478</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3478</id><created>2011-06-15</created><authors><author><keyname>Breitner</keyname><forenames>Joachim</forenames></author></authors><title>Conditional Elimination through Code Duplication</title><categories>cs.PL</categories><comments>11 pages, 5 figures</comments><acm-class>D.3.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose an optimizing transformation which reduces program runtime at the
expense of program size by eliminating conditional jumps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3488</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3488</id><created>2011-06-17</created><authors><author><keyname>Derrick</keyname><forenames>John</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Boiten</keyname><forenames>Eerke</forenames><affiliation>University of Kent</affiliation></author><author><keyname>Reeves</keyname><forenames>Steve</forenames><affiliation>University of Waikato</affiliation></author></authors><title>Proceedings 15th International Refinement Workshop</title><categories>cs.SE cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011</journal-ref><doi>10.4204/EPTCS.55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refinement is one of the cornerstones of a formal approach to software
engineering: the process of developing a more detailed design or implementation
from an abstract specification through a sequence of mathematically-based steps
that maintain correctness with respect to the original specification.
  The aim of this BCS FACS Refinement Workshop, is to bring together people who
are interested in the development of more concrete designs or executable
programs from abstract specifications using formal notations, tool support for
formal software development, and practical experience with formal refinement
methodologies.
  The purpose of the workshop is to provide a forum for the exchange of ideas,
and discussion of common ground and key differences.
  This 15th workshop continued a 20 year tradition in refinement workshops run
under the auspices of the British Computer Society (BCS) FACS special interest
group. After the first seven editions had been held in the UK, in 1998 it was
combined with the Australasian Refinement Workshop to form the International
Refinement Workshop, hosted at The Australian National University. Six more
editions have followed in a variety of locations, all with electronic published
proceedings and associated journal special issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3490</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3490</id><created>2011-06-17</created><updated>2012-10-31</updated><authors><author><keyname>Fang</keyname><forenames>Wenjie</forenames></author></authors><title>New Computational Result on Harmonious Trees</title><categories>cs.DM math.CO</categories><comments>5 pages, source code of the improved version used in yoyo@home is
  available at
  http://rechenkraft.net/yoyo/download/download/tmp/hat/HarmoniousB.cpp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graham and Sloane proposed in 1980 a conjecture stating that every tree has a
harmonious labelling, a graph labelling closely related to additive base. Very
limited results on this conjecture are known. In this paper, we proposed a
computational approach to this conjecture by checking trees with limited size.
With a hybrid algorithm, we are able to show that every tree with at most 31
nodes is harmonious, extending the best previous result in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3498</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3498</id><created>2011-06-17</created><authors><author><keyname>Bailleux</keyname><forenames>Olivier</forenames></author></authors><title>On the expressive power of unit resolution</title><categories>cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This preliminary report addresses the expressive power of unit resolution
regarding input data encoded with partial truth assignments of propositional
variables. A characterization of the functions that are computable in this way,
which we propose to call propagatable functions, is given. By establishing that
propagatable functions can also be computed using monotone circuits, we show
that there exist polynomial time complexity propagable functions requiring an
exponential amount of clauses to be computed using unit resolution. These
results shed new light on studying CNF encodings of NP-complete problems in
order to solve them using propositional satisfiability algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3508</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3508</id><created>2011-06-17</created><authors><author><keyname>Blaustein</keyname><forenames>Barbara</forenames><affiliation>MITRE</affiliation></author><author><keyname>Chapman</keyname><forenames>Adriane</forenames><affiliation>MITRE</affiliation></author><author><keyname>Seligman</keyname><forenames>Len</forenames><affiliation>MITRE</affiliation></author><author><keyname>Allen</keyname><forenames>M. David</forenames><affiliation>MITRE</affiliation></author><author><keyname>Rosenthal</keyname><forenames>Arnon</forenames><affiliation>MITRE</affiliation></author></authors><title>Surrogate Parenthood: Protected and Informative Graphs</title><categories>cs.SI physics.soc-ph</categories><comments>VLDB2011</comments><proxy>uroehm</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications, including provenance and some analyses of social networks,
require path-based queries over graph-structured data. When these graphs
contain sensitive information, paths may be broken, resulting in uninformative
query results. This paper presents innovative techniques that give users more
informative graph query results; the techniques leverage a common industry
practice of providing what we call surrogates: alternate, less sensitive
versions of nodes and edges releasable to a broader community. We describe
techniques for interposing surrogate nodes and edges to protect sensitive graph
components, while maximizing graph connectivity and giving users as much
information as possible. In this work, we formalize the problem of creating a
protected account G' of a graph G. We provide a utility measure to compare the
informativeness of alternate protected accounts and an opacity measure for
protected accounts, which indicates the likelihood that an attacker can
recreate the topology of the original graph from the protected account. We
provide an algorithm to create a maximally useful protected account of a
sensitive graph, and show through evaluation with the PLUS prototype that using
surrogates and protected accounts adds value for the user, with no significant
impact on the time required to generate results for graph queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3517</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3517</id><created>2011-06-17</created><authors><author><keyname>R.</keyname><forenames>Shashi Kumar D.</forenames></author><author><keyname>Raja</keyname><forenames>K. B.</forenames></author><author><keyname>Chhootaray</keyname><forenames>R. K.</forenames></author><author><keyname>Pattanaik</keyname><forenames>Sabyasachi</forenames></author></authors><title>DWT Based Fingerprint Recognition using Non Minutiae Features</title><categories>cs.CV</categories><comments>9 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 2, March 2011, 257-265</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forensic applications like criminal investigations, terrorist identification
and National security issues require a strong fingerprint data base and
efficient identification system. In this paper we propose DWT based Fingerprint
Recognition using Non Minutiae (DWTFR) algorithm. Fingerprint image is
decomposed into multi resolution sub bands of LL, LH, HL and HH by applying 3
level DWT. The Dominant local orientation angle {\theta} and Coherence are
computed on LL band only. The Centre Area Features and Edge Parameters are
determined on each DWT level by considering all four sub bands. The comparison
of test fingerprint with database fingerprint is decided based on the Euclidean
Distance of all the features. It is observed that the values of FAR, FRR and
TSR are improved compared to the existing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3527</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3527</id><created>2011-06-17</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Soleimanfallah</keyname><forenames>Arezou</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Parameterized Complexity Results for General Factors in Bipartite Graphs
  with an Application to Constraint Programming</title><categories>cs.DS cs.DM</categories><comments>Full version of a paper that appeared in preliminary form in the
  proceedings of IPEC'10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-hard general factor problem asks, given a graph and for each vertex a
list of integers, whether the graph has a spanning subgraph where each vertex
has a degree that belongs to its assigned list. The problem remains NP-hard
even if the given graph is bipartite with partition U+V, and each vertex in U
is assigned the list {1}; this subproblem appears in the context of constraint
programming as the consistency problem for the extended global cardinality
constraint. We show that this subproblem is fixed-parameter tractable when
parameterized by the size of the second partite set V. More generally, we show
that the general factor problem for bipartite graphs, parameterized by |V|, is
fixed-parameter tractable as long as all vertices in U are assigned lists of
length 1, but becomes W[1]-hard if vertices in U are assigned lists of length
at most 2. We establish fixed-parameter tractability by reducing the problem
instance to a bounded number of acyclic instances, each of which can be solved
in polynomial time by dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3552</identifier>
 <datestamp>2011-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3552</id><created>2011-06-17</created><updated>2011-07-18</updated><authors><author><keyname>Hwang</keyname><forenames>Sung-Ha</forenames></author><author><keyname>Rey-Bellet</keyname><forenames>Luc</forenames></author></authors><title>Decompositions of two player games: potential, zero-sum, and stable
  games</title><categories>cs.GT cs.SY math-ph math.MP math.OC q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce several methods of decomposition for two player normal form
games. Viewing the set of all games as a vector space, we exhibit explicit
orthonormal bases for the subspaces of potential games, zero-sum games, and
their orthogonal complements which we call anti-potential games and
anti-zero-sum games, respectively. Perhaps surprisingly, every anti-potential
game comes either from the Rock-Paper-Scissors type games (in the case of
symmetric games) or from the Matching Pennies type games (in the case of
asymmetric games). Using these decompositions, we prove old (and some new)
cycle criteria for potential and zero-sum games (as orthogonality relations
between subspaces). We illustrate the usefulness of our decomposition by (a)
analyzing the generalized Rock-Paper-Scissors game, (b) completely
characterizing the set of all null-stable games, (c) providing a large class of
strict stable games, (d) relating the game decomposition to the decomposition
of vector fields for the replicator equations, (e) constructing Lyapunov
functions for some replicator dynamics, and (f) constructing Zeeman games
-games with an interior asymptotically stable Nash equilibrium and a pure
strategy ESS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3554</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3554</id><created>2011-06-17</created><authors><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Cui</keyname><forenames>Ai-Xiang</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Impact of Heterogeneous Human Activities on Epidemic Spreading</title><categories>physics.data-an cs.SI physics.soc-ph</categories><comments>11 pages, 4 figures</comments><journal-ref>Physica A 390 (2011) 4543-4548</journal-ref><doi>10.1016/j.physa.2011.06.068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent empirical observations suggest a heterogeneous nature of human
activities. The heavy-tailed inter-event time distribution at population level
is well accepted, while whether the individual acts in a heterogeneous way is
still under debate. Motivated by the impact of temporal heterogeneity of human
activities on epidemic spreading, this paper studies the susceptible-infected
model on a fully mixed population, where each individual acts in a completely
homogeneous way but different individuals have different mean activities.
Extensive simulations show that the heterogeneity of activities at population
level remarkably affects the speed of spreading, even though each individual
behaves regularly. Further more, the spreading speed of this model is more
sensitive to the change of system heterogeneity compared with the model
consisted of individuals acting with heavy-tailed inter-event time
distribution. This work refines our understanding of the impact of
heterogeneous human activities on epidemic spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3574</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3574</id><created>2011-06-17</created><authors><author><keyname>Rangineni</keyname><forenames>Sandhya</forenames></author></authors><title>Cryptographic Hardening of d-Sequences</title><categories>cs.CR</categories><comments>8 pages, 5 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how a one-way mapping using majority information on adjacent
bits will improve the randomness of d-sequences. Supporting experimental
results are presented. It is shown that the behavior of d-sequences is
different from that of other RNG sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3579</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3579</id><created>2011-06-17</created><updated>2012-05-31</updated><authors><author><keyname>Godard</keyname><forenames>Emmanuel</forenames></author><author><keyname>Peters</keyname><forenames>Joseph</forenames></author></authors><title>Consensus vs Broadcast in Communication Networks with Arbitrary Mobile
  Omission Faults</title><categories>cs.DC</categories><comments>Erratum for Def. 4.8 and 4.9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the solvability of the Consensus and Broadcast problems in
synchronous communication networks in which the delivery of messages is not
reliable. The failure model is the mobile omission faults model. During each
round, some messages can be lost and the set of possible simultaneous losses is
the same for each round. We investigate these problems for the first time for
arbitrary sets of possible failures. Previously, these sets were defined by
bounding the numbers of failures.
  In this setting, we present a new necessary condition for the solvability of
Consensus that unifies previous impossibility results in this area. This
condition is expressed using Broadcastability properties. As a very important
application, we show that when the sets of omissions that can occur are defined
by bounding the numbers of failures, counted in any way (locally, globally,
etc.), then the Consensus problem is actually equivalent to the Broadcast
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3582</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3582</id><created>2011-06-17</created><authors><author><keyname>Lichter</keyname><forenames>Shaun</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Friesz</keyname><forenames>Terry</forenames></author></authors><title>Link Biased Strategies in Network Formation Games</title><categories>math.OC cs.SI physics.soc-ph</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a simple method for constructing an infinite family of graph
formation games with link bias so that the resulting games admits, as a
\textit{pairwise stable} solution, a graph with an arbitrarily specified degree
distribution. Pairwise stability is used as the equilibrium condition over the
more commonly used Nash equilibrium to prevent the occurrence of ill-behaved
equilibrium strategies that do not occur in ordinary play. We construct this
family of games by solving an integer programming problem whose constraints
enforce the terminal pairwise stability property we desire.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3586</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3586</id><created>2011-05-06</created><updated>2011-06-21</updated><authors><author><keyname>Hassan</keyname><forenames>Sk. S.</forenames></author><author><keyname>Roy</keyname><forenames>A.</forenames></author><author><keyname>Choudhury</keyname><forenames>P. Pal</forenames></author><author><keyname>Nayak</keyname><forenames>B. K.</forenames></author></authors><title>One Dimensional p-adic Integral Value Transformations</title><categories>cs.DM</categories><comments>The mathematics of Integral Value Transformations have been
  deciphered</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, a set of transformations T^(p,k) is defined on N_0^K to N_0.
Some basic and na\&quot;ive mathematical structure of T^(p,1) are adumbrated and
introduced the concept of discrete dynamical systems through IVTs. Latterly,
some further research scope of IVTs is highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3595</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3595</id><created>2011-06-17</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Rao</keyname><forenames>Anup</forenames></author></authors><title>Information Equals Amortized Communication</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to efficiently simulate the sending of a message M to a receiver
who has partial information about the message, so that the expected number of
bits communicated in the simulation is close to the amount of additional
information that the message reveals to the receiver. This is a generalization
and strengthening of the Slepian-Wolf theorem, which shows how to carry out
such a simulation with low amortized communication in the case that M is a
deterministic function of X. A caveat is that our simulation is interactive.
  As a consequence, we prove that the internal information cost (namely the
information revealed to the parties) involved in computing any relation or
function using a two party interactive protocol is exactly equal to the
amortized communication complexity of computing independent copies of the same
relation or function. We also show that the only way to prove a strong direct
sum theorem for randomized communication complexity is by solving a particular
variant of the pointer jumping problem that we define. Our work implies that a
strong direct sum theorem for communication complexity holds if and only if
efficient compression of communication protocols is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3600</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3600</id><created>2011-06-17</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Ranjan</keyname><forenames>Apara</forenames></author></authors><title>How Insight Emerges in a Distributed, Content-addressable Memory</title><categories>q-bio.NC cs.AI</categories><comments>in press; 17 pages; 2 figures</comments><journal-ref>Gabora, L. &amp; Ranjan, A. (2012). How insight emerges in a
  distributed, content-addressable memory. In A. Bristol, O. Vartanian, &amp; J.
  Kaufman (Eds.) The Neuroscience of Creativity. New York: Oxford University
  Press</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin this chapter with the bold claim that it provides a neuroscientific
explanation of the magic of creativity. Creativity presents a formidable
challenge for neuroscience. Neuroscience generally involves studying what
happens in the brain when someone engages in a task that involves responding to
a stimulus, or retrieving information from memory and using it the right way,
or at the right time. If the relevant information is not already encoded in
memory, the task generally requires that the individual make systematic use of
information that is encoded in memory. But creativity is different. It
paradoxically involves studying how someone pulls out of their brain something
that was never put into it! Moreover, it must be something both new and useful,
or appropriate to the task at hand. The ability to pull out of memory something
new and appropriate that was never stored there in the first place is what we
refer to as the magic of creativity. Even if we are so fortunate as to
determine which areas of the brain are active and how these areas interact
during creative thought, we will not have an answer to the question of how the
brain comes up with solutions and artworks that are new and appropriate. On the
other hand, since the representational capacity of neurons emerges at a level
that is higher than that of the individual neurons themselves, the inner
workings of neurons is too low a level to explain the magic of creativity. Thus
we look to a level that is midway between gross brain regions and neurons.
Since creativity generally involves combining concepts from different domains,
or seeing old ideas from new perspectives, we focus our efforts on the neural
mechanisms underlying the representation of concepts and ideas. Thus we ask
questions about the brain at the level that accounts for its representational
capacity, i.e. at the level of distributed aggregates of neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3625</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3625</id><created>2011-06-18</created><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Huang</keyname><forenames>Cheng</forenames></author><author><keyname>Simitci</keyname><forenames>Huseyin</forenames></author><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author></authors><title>On the Locality of Codeword Symbols</title><categories>cs.IT cs.CC cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a linear [n,k,d]_q code C. We say that that i-th coordinate of C has
locality r, if the value at this coordinate can be recovered from accessing
some other r coordinates of C. Data storage applications require codes with
small redundancy, low locality for information coordinates, large distance, and
low locality for parity coordinates. In this paper we carry out an in-depth
study of the relations between these parameters.
  We establish a tight bound for the redundancy n-k in terms of the message
length, the distance, and the locality of information coordinates. We refer to
codes attaining the bound as optimal. We prove some structure theorems about
optimal codes, which are particularly strong for small distances. This gives a
fairly complete picture of the tradeoffs between codewords length, worst-case
distance and locality of information symbols.
  We then consider the locality of parity check symbols and erasure correction
beyond worst case distance for optimal codes. Using our structure theorem, we
obtain a tight bound for the locality of parity symbols possible in such codes
for a broad class of parameter settings. We prove that there is a tradeoff
between having good locality for parity checks and the ability to correct
erasures beyond the minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3627</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3627</id><created>2011-06-18</created><authors><author><keyname>Liu</keyname><forenames>Binyue</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author></authors><title>Analog Network Coding in the Generalized High-SNR Regime</title><categories>cs.IT math.IT</categories><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper [4], Mari\'c et al. analyzed the performance of the analog
network coding (ANC) in a layered relay network for the high-SNR regime. They
have proved that under the ANC scheme, if each relay transmits the received
signals at the upper bound of the power constraint, the transmission rate will
approach the network capacity. In this paper, we consider a more general
scenario defined as the generalized high-SNR regime, where the relays at layer
$l$ in a layered relay network with $L$ layers do not satisfy the high-SNR
conditions, and then determine an ANC relay scheme in such network. By relating
the received SNR at the nodes with the propagated noise, we derive the rate
achievable by the ANC scheme proposed in this paper. The result shows that the
achievable ANC rate approaches the upper bound of the ANC capacity as the
received powers at relays in high SNR increase. A comparison of the two ANC
schemes implies that the scheme proposed in [4] may not always be the optimal
one in the generalized high-SNR regime. The result also demonstrates that the
upper and lower bounds of the ANC rate coincide in the limit as the number of
relays at layer L-1 dissatisfying the high-SNR conditions tends to infinity,
yielding an asymptotic capacity result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3628</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3628</id><created>2011-06-18</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Finding the Maximal Empty Rectangle Containing a Query Point</title><categories>cs.CG cs.DS</categories><acm-class>F.2.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in an axis-parallel rectangle $B$ in the
plane. We present an $O(n\alpha(n)\log^4 n)$-time algorithm to preprocess $P$
into a data structure of size $O(n\alpha(n)\log^3 n)$, such that, given a query
point $q$, we can find, in $O(\log^4 n)$ time, the largest-area axis-parallel
rectangle that is contained in $B$, contains $q$, and its interior contains no
point of $P$. This is a significant improvement over the previous solution of
Augustine {\em et al.} \cite{qmex}, which uses slightly superquadratic
preprocessing and storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3629</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3629</id><created>2011-06-18</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Total Variation Minimization Based Compressive Wideband Spectrum Sensing
  for Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wideband spectrum sensing is a critical component of a functioning cognitive
radio system. Its major challenge is the too high sampling rate requirement.
Compressive sensing (CS) promises to be able to deal with it. Nearly all the
current CS based compressive wideband spectrum sensing methods exploit only the
frequency sparsity to perform. Motivated by the achievement of a fast and
robust detection of the wideband spectrum change, total variation mnimization
is incorporated to exploit the temporal and frequency structure information to
enhance the sparse level. As a sparser vector is obtained, the spectrum sensing
period would be shorten and sensing accuracy would be enhanced. Both
theoretical evaluation and numerical experiments can demonstrate the
performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3632</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3632</id><created>2011-06-18</created><updated>2012-01-16</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Minimal resolving sets for the hypercube</title><categories>cs.DM math.CO</categories><journal-ref>Graph Theory Notes of New York, vol. LXVII, pp. 50-53, November
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given undirected graph $G$, an \emph{ordered} subset $S =
{s_1,s_2,...,s_k} \subseteq V$ of vertices is a resolving set for the graph if
the vertices of the graph are distinguishable by their vector of distances to
the vertices in $S$. While a superset of any resolving set is always a
resolving set, a proper subset of a resolving set is not necessarily a
resolving set, and we are interested in determining resolving sets that are
minimal or that are minimum (of minimal cardinality). Let $Q^n$ denote the
$n$-dimensional hypercube with vertex set ${0,1}^n$. In Erd\&quot;os and Renyi
(Erdos &amp; Renyi, 1963) it was shown that a particular set of $n$ vertices forms
a resolving set for the hypercube. The main purpose of this note is to prove
that a proper subset of that set of size $n-1$ is also a resolving set for the
hypercube for all $n \ge 5$ and that this proper subset is a minimal resolving
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3634</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3634</id><created>2011-06-18</created><authors><author><keyname>Berezovsky</keyname><forenames>Vladimir</forenames></author><author><keyname>Popov</keyname><forenames>Alexander</forenames></author></authors><title>Strategies for Development of a Distributed Framework for Computational
  Sciences</title><categories>cs.DC physics.comp-ph</categories><comments>11 pages, 2 figures</comments><acm-class>J.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses some generic approach for developing grid-based
framework for enabling establishment of workflows comprising existing software
in computational sciences areas. We highlight the main requirements addressed
the developing of such framework. Some strategies for enabling interoperability
between convenient computation software in the grid environment has been shown.
The UML based instruments of graphical description of workflows for the
developing system has been suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3651</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3651</id><created>2011-06-18</created><updated>2011-11-11</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Robust Bayesian reinforcement learning through tight lower bounds</title><categories>cs.LG stat.ML</categories><comments>Corrected version. 12 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Bayesian approach to sequential decision making, exact calculation of
the (subjective) utility is intractable. This extends to most special cases of
interest, such as reinforcement learning problems. While utility bounds are
known to exist for this problem, so far none of them were particularly tight.
In this paper, we show how to efficiently calculate a lower bound, which
corresponds to the utility of a near-optimal memoryless policy for the decision
problem, which is generally different from both the Bayes-optimal policy and
the policy which is optimal for the expected MDP under the current belief. We
then show how these can be applied to obtain robust exploration policies in a
Bayesian reinforcement learning setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3652</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3652</id><created>2011-06-18</created><updated>2012-12-18</updated><authors><author><keyname>Stefanov</keyname><forenames>Emil</forenames></author><author><keyname>Shi</keyname><forenames>Elaine</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Towards Practical Oblivious RAM</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take an important step forward in making Oblivious RAM (O-RAM) practical.
We propose an O-RAM construction achieving an amortized overhead of 20X-35X
(for an O-RAM roughly 1 terabyte in size), about 63 times faster than the best
existing scheme. On the theoretic front, we propose a fundamentally novel
technique for constructing Oblivious RAMs: specifically, we partition a bigger
O-RAM into smaller O-RAMs, and employ a background eviction technique to
obliviously evict blocks from the client-side cache into a randomly assigned
server-side partition. This novel technique is the key to achieving the gains
in practical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3655</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3655</id><created>2011-06-18</created><updated>2011-11-17</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author><author><keyname>Rothkopf</keyname><forenames>Constantin</forenames></author></authors><title>Bayesian multitask inverse reinforcement learning</title><categories>stat.ML cs.AI</categories><comments>Corrected version. 13 pages, 8 figures</comments><msc-class>62C10, 91B08, 91B10</msc-class><acm-class>G.3</acm-class><journal-ref>Recent Advances in Reinforcement Learning LNCS 7188, pp. 273-284,
  2012</journal-ref><doi>10.1007/978-3-642-29946-9_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalise the problem of inverse reinforcement learning to multiple
tasks, from multiple demonstrations. Each one may represent one expert trying
to solve a different task, or as different experts trying to solve the same
task. Our main contribution is to formalise the problem as statistical
preference elicitation, via a number of structured priors, whose form captures
our biases about the relatedness of different tasks or expert policies. In
doing so, we introduce a prior on policy optimality, which is more natural to
specify. We show that our framework allows us not only to learn to efficiently
from multiple experts but to also effectively differentiate between the goals
of each. Possible applications include analysing the intrinsic motivations of
subjects in behavioural experiments and learning from multiple teachers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3677</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3677</id><created>2011-06-18</created><authors><author><keyname>Bodean</keyname><forenames>Diana</forenames></author><author><keyname>Bodean</keyname><forenames>Ghenadie</forenames></author><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author></authors><title>Pseudo-Ring Testing Schemes and Algorithms of RAM Built-In and Embedded
  Self-Testing</title><categories>cs.AR</categories><comments>4 pages</comments><journal-ref>14 th IEEE Symposium on Design and Diagnostics of Electronic
  Circuits and Systems, April 13-15, 2011, Cottbus, Germany</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scan and ring schemes of the pseudo-ring memory selftesting are investigated.
Both schemes are based on emulation of the linear or nonlinear feedback shift
register by memory itself. Peculiarities of the pseudo-ring schemes
implementation for multi-port and embedded memories, and for register file are
described. It is shown that only small additional logic is required and allows
microcontrollers at-speed testing. Also, in this article,are given the a
posteriori values of some type of memories faults coverage when pseudo-ring
testing schemes are applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3678</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3678</id><created>2011-06-18</created><authors><author><keyname>Yeung</keyname><forenames>Man-Chung</forenames></author></authors><title>An introduction to ML(n)BiCGStab</title><categories>math.NA cs.NA math.OC</categories><comments>This paper introduces the three algorithms of the ML(n)BiCGStab
  method for the solution of large, nonsymmetric liear systems. One of the
  algorithms involves A-transpose in its implmentation. This paper will be
  presented in the 33rd International Conference on Boundary Elements and other
  Mesh Reduction Methods, 28-30 June 2011, New Forest, UK</comments><report-no>Report 11-06, Math Dept., UW</report-no><msc-class>65D15, 65F30, 65G20, 68W10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ML(n)BiCGStab is a Krylov subspace method for the solution of large, sparse
and non-symmetric linear systems. In theory, it is a method that lies between
the well-known BiCGStab and GMRES/FOM. In fact, when n = 1, ML(1)BiCGStab is
BiCGStab and when n = N, ML(N)BiCGStab is GMRES/FOM where N is the size of the
linear system. Therefore, ML(n)BiCGStab is a bridge that connects the
Lanczos-based BiCGStab and the Arnoldi-based GMRES/FOM. In computation,
ML(n)BiCGStab can be much more stable and converge much faster than BiCGStab
when a problem with ill-condition is solved. We have tested ML(n)BiCGStab on
the standard oil reservoir simulation test data called SPE9 and found that
ML(n)BiCGStab reduced the total computational time by more than 60% when
compared to BiCGStab. Tests made on the data from Matrix Market also support
the superiority of ML(n)BiCGStab over BiCGStab. Because of the O(N^2) storage
requirement in the full GMRES, one has to adopt a restart strategy to get the
storage under control when GMRES is implemented. In comparison, ML(n)BiCGStab
is a method with only O(nN) storage requirement and therefore it does not need
a restart strategy. In this paper, we introduce ML(n)BiCGStab (in particular, a
new algorithm involving A-transpose), its relations to some existing methods
and its implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3679</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3679</id><created>2011-06-18</created><updated>2012-03-10</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>Solution of Wald's game using loadings and allowed strategies</title><categories>cs.GT</categories><comments>5 pages. The result of this paper is now part of the paper &quot;Existence
  of equilibria for countable games: an algebraic approach&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new interpretation of the strange phenomena that some authors
have observed about the Wald game. This interpretation is possible thanks to
the new language of \emph{loadings} that Morrison and the author have
introduced in a previous work. Using the theory of loadings and allowed
strategies, we are also able to prove that Wald's game admits a \emph{natural}
solution and, as one can expect, the game turns out to be fair for this
solution. As a technical tool, we introduce the notion of \emph{embedding a
game into another game} that could be of interest from a theoretical point of
view. \emph{En passant} we find a very easy example of a game which is loadable
in infinitely many different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3680</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3680</id><created>2011-06-18</created><authors><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Efficient Two-Stage Group Testing Algorithms for DNA Screening</title><categories>cs.DM cs.CE cs.DS cs.IT math.CO math.IT q-bio.QM</categories><comments>12 pages; accepted for ICALP 2011 Group Testing Workshop: Algorithms
  and Data Structures for selection, identification and encoding (ICALP 2011
  GT, Zurich)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group testing algorithms are very useful tools for DNA library screening.
Building on recent work by Levenshtein (2003) and Tonchev (2008), we construct
in this paper new infinite classes of combinatorial structures, the existence
of which are essential for attaining the minimum number of individual tests at
the second stage of a two-stage disjunctive testing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3681</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3681</id><created>2011-06-18</created><authors><author><keyname>Chumachenko</keyname><forenames>Svetlana</forenames></author><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Hahanova</keyname><forenames>Anna</forenames></author><author><keyname>Sushanov</keyname><forenames>Aleksey</forenames></author></authors><title>SoC Software Components Diagnosis Technology</title><categories>cs.AR</categories><comments>4 pages</comments><journal-ref>Proceedings of IEEE, East-West Design &amp; Test, Symposium
  (EWDTS'08),Lviv, Ukraine, October 9 - 12, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach to evaluation of hardware and software testability,
represented in the form of register transfer graph, is proposed. Instances of
making of software graph models for their subsequent testing and diagnosis are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3684</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3684</id><created>2011-06-18</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author><author><keyname>Balas</keyname><forenames>Valentina E.</forenames></author></authors><title>Exploratory simulation of an Intelligent Iris Verifier Distributed
  System</title><categories>cs.CV cs.ET cs.LO</categories><comments>4 pages, 2 figures, latest version: http://fmi.spiruharet.ro/bodorin/</comments><msc-class>68T10</msc-class><acm-class>I.5</acm-class><journal-ref>Proc. 6th IEEE International Symposium on Applied Computational
  Intelligence and Informatics, pp. 259 - 262, IEEE Press, June 2011</journal-ref><doi>10.1109/SACI.2011.5873010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses some topics related to the latest trends in the field of
evolutionary approaches to iris recognition. It presents the results of an
exploratory experimental simulation whose goal was to analyze the possibility
of establishing an Interchange Protocol for Digital Identities evolved in
different geographic locations interconnected through and into an Intelligent
Iris Verifier Distributed System (IIVDS) based on multi-enrollment. Finding a
logically consistent model for the Interchange Protocol is the key factor in
designing the future large-scale iris biometric networks. Therefore, the
logical model of such a protocol is also investigated here. All tests are made
on Bath Iris Database and prove that outstanding power of discrimination
between the intra- and the inter-class comparisons can be achieved by an IIVDS,
even when practicing 52.759.182 inter-class and 10.991.943 intra-class
comparisons. Still, the test results confirm that inconsistent enrollment can
change the logic of recognition from a fuzzified 2-valent consistent logic of
biometric certitudes to a fuzzified 3-valent inconsistent possibilistic logic
of biometric beliefs justified through experimentally determined probabilities,
or to a fuzzified 8-valent logic which is almost consistent as a biometric
theory - this quality being counterbalanced by an absolutely reasonable loss in
the user comfort level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3685</identifier>
 <datestamp>2011-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3685</id><created>2011-06-18</created><updated>2011-08-17</updated><authors><author><keyname>Benzmueller</keyname><forenames>Christoph</forenames></author><author><keyname>Gabbay</keyname><forenames>Dov</forenames></author><author><keyname>Genovese</keyname><forenames>Valerio</forenames></author><author><keyname>Rispoli</keyname><forenames>Daniele</forenames></author></authors><title>Embedding and Automating Conditional Logics in Classical Higher-Order
  Logic</title><categories>cs.AI cs.LO math.LO</categories><comments>15 pages, 1 Figure, 1 Table</comments><msc-class>03B60, 03B15, 68T27, 68T30, 68T15</msc-class><acm-class>I.2.3; I.2.4; I.2.0; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sound and complete embedding of conditional logics into classical
higher-order logic is presented. This embedding enables the application of
off-the-shelf higher-order automated theorem provers and model finders for
reasoning within and about conditional logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3693</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3693</id><created>2011-06-18</created><updated>2011-12-01</updated><authors><author><keyname>Narang</keyname><forenames>Sunil K.</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Perfect Reconstruction Two-Channel Wavelet Filter-Banks for Graph
  Structured Data</title><categories>cs.DC cs.SI</categories><comments>32 pages double spaced 12 Figures, to appear in IEEE Transactions of
  Signal Processing</comments><doi>10.1109/TSP.2012.2188718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose the construction of two-channel wavelet filterbanks
for analyzing functions defined on the vertices of any arbitrary finite
weighted undirected graph. These graph based functions are referred to as
graph-signals as we build a framework in which many concepts from the classical
signal processing domain, such as Fourier decomposition, signal filtering and
downsampling can be extended to graph domain. Especially, we observe a spectral
folding phenomenon in bipartite graphs which occurs during downsampling of
these graphs and produces aliasing in graph signals. This property of bipartite
graphs, allows us to design critically sampled two-channel filterbanks, and we
propose quadrature mirror filters (referred to as graph-QMF) for bipartite
graph which cancel aliasing and lead to perfect reconstruction. For arbitrary
graphs we present a bipartite subgraph decomposition which produces an
edge-disjoint collection of bipartite subgraphs. Graph-QMFs are then
constructed on each bipartite subgraph leading to &quot;multi-dimensional&quot; separable
wavelet filterbanks on graphs. Our proposed filterbanks are critically sampled
and we state necessary and sufficient conditions for orthogonality, aliasing
cancellation and perfect reconstruction. The filterbanks are realized by
Chebychev polynomial approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3694</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3694</id><created>2011-06-18</created><authors><author><keyname>Jim&#xe9;nez-P&#xe9;rez</keyname><forenames>Hugo</forenames></author><author><keyname>Laskar</keyname><forenames>Jacques</forenames></author></authors><title>A time-parallel algorithm for almost integrable Hamiltonian systems</title><categories>cs.NA math.NA</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a time-parallel algorithm for solving numerically almost
integrable Hamiltonian systems in action-angle coordinates. This algorithm is a
refinement of that introduced by Saha, Stadel and Tremaine in 1997 (SST97) for
the same type of problems. Our refined algorithm has a better convergence
obtained from the use of derivatives of the perturbing term not considered in
the original SST97 algorithm. An advantage of this algorithm is its
independence of the step-size for the parallelized procedures which can be
consider as a particular case of the parareal scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3703</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3703</id><created>2011-06-19</created><updated>2015-01-16</updated><authors><author><keyname>Kolchinsky</keyname><forenames>Artemy</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Prediction and Modularity in Dynamical Systems</title><categories>nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME</categories><comments>v1 published in ECAL 2011 (European Conference on Artificial Life).
  v2 fixes error in causal risk (number of parameters should be based on
  training distribution)</comments><msc-class>62H20, 62M20, 62B10, 60G25, 68T05, 90B15, 05C82</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying and understanding modular organizations is centrally important in
the study of complex systems. Several approaches to this problem have been
advanced, many framed in information-theoretic terms. Our treatment starts from
the complementary point of view of statistical modeling and prediction of
dynamical systems. It is known that for finite amounts of training data,
simpler models can have greater predictive power than more complex ones. We use
the trade-off between model simplicity and predictive accuracy to generate
optimal multiscale decompositions of dynamical networks into weakly-coupled,
simple modules. State-dependent and causal versions of our method are also
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3705</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3705</id><created>2011-06-19</created><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>The taming of recurrences in computability logic through cirquent
  calculus, Part II</title><categories>cs.LO math.LO</categories><msc-class>03B47, 03B70, 68Q10, 68T27, 68T15</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>Archive for Mathematical Logic 52 (2013), pp. 213-259</journal-ref><doi>10.1007/s00153-012-0314-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructs a cirquent calculus system and proves its soundness and
completeness with respect to the semantics of computability logic (see
http://www.cis.upenn.edu/~giorgi/cl.html). The logical vocabulary of the system
consists of negation, parallel conjunction, parallel disjunction, branching
recurrence, and branching corecurrence. The article is published in two parts,
with (the previous) Part I containing preliminaries and a soundness proof, and
(the present) Part II containing a completeness proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3711</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3711</id><created>2011-06-19</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Sidelobe Suppression for Capon Beamforming with Mainlobe to Sidelobe
  Power Ratio Maximization</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures</comments><doi>10.1109/LAWP.2012.2223451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High sidelobe level is a major disadvantage of the Capon beamforming. To
suppress the sidelobe, this paper introduces a mainlobe to sidelobe power ratio
constraint to the Capon beamforming. it minimizes the sidelobe power while
keeping the mainlobe power constant. Simulations show that the obtained
beamformer outperforms the Capon beamformer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3713</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3713</id><created>2011-06-19</created><updated>2013-05-02</updated><authors><author><keyname>Murin</keyname><forenames>Yonathan</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Source-Channel Coding Theorems for the Multiple-Access Relay Channel</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study reliable transmission of arbitrarily correlated sources over
multiple-access relay channels (MARCs) and multiple-access broadcast relay
channels (MABRCs). In MARCs only the destination is interested in
reconstructing the sources, while in MABRCs both the relay and the destination
want to reconstruct them. In addition to arbitrary correlation among the source
signals at the users, both the relay and the destination have side information
correlated with the source signals. Our objective is to determine whether a
given pair of sources can be losslessly transmitted to the destination for a
given number of channel symbols per source sample, defined as the
source-channel rate. Sufficient conditions for reliable communication based on
operational separation, as well as necessary conditions on the achievable
source-channel rates are characterized. Since operational separation is
generally not optimal for MARCs and MABRCs, sufficient conditions for reliable
communication using joint source-channel coding schemes based on a combination
of the correlation preserving mapping technique with Slepian-Wolf source coding
are also derived. For correlated sources transmitted over fading Gaussian MARCs
and MABRCs, we present conditions under which separation (i.e., separate and
stand-alone source and channel codes) is optimal. This is the first time
optimality of separation is proved for MARCs and MABRCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3725</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3725</id><created>2011-06-19</created><updated>2012-04-20</updated><authors><author><keyname>Staworko</keyname><forenames>S&#x142;awomir</forenames></author><author><keyname>Wieczorek</keyname><forenames>Piotr</forenames></author></authors><title>Learning XML Twig Queries</title><categories>cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of learning XML queries, path queries and tree
pattern queries, from examples given by the user. A learning algorithm takes on
the input a set of XML documents with nodes annotated by the user and returns a
query that selects the nodes in a manner consistent with the annotation. We
study two learning settings that differ with the types of annotations. In the
first setting the user may only indicate required nodes that the query must
return. In the second, more general, setting, the user may also indicate
forbidden nodes that the query must not return. The query may or may not return
any node with no annotation. We formalize what it means for a class of queries
to be \emph{learnable}. One requirement is the existence of a learning
algorithm that is sound i.e., always returns a query consistent with the
examples given by the user. Furthermore, the learning algorithm should be
complete i.e., able to produce every query with a sufficiently rich example.
Other requirements involve tractability of learning and its robustness to
nonessential examples. We show that the classes of simple path queries and
path-subsumption-free tree queries are learnable from positive examples. The
learnability of the full class of tree pattern queries (and the full class of
path queries) remains an open question. We show also that adding negative
examples to the picture renders the learning unfeasible.
  Published in ICDT 2012, Berlin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3739</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3739</id><created>2011-06-19</created><updated>2011-09-05</updated><authors><author><keyname>Gitlin</keyname><forenames>Roman</forenames></author></authors><title>Reducing Interpolation on Multi-Dimensional Grid to Quantizing Grid's
  Data-Base as a Recursion</title><categories>cs.CG</categories><comments>This article has been withdrawn by arXiv administrators as containing
  excessive overlap with arXiv:1106.3314</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his article &quot;Powerlist: A Structure for Parallel Recursion&quot; Jayadev Misra
wrote:
  &quot;Many data parallel algorithms - Fast Fourier Transform, Batcher's sorting
schemes and prefix sum - exhibit recursive structure. We propose a data
structure, powerlist, that permits succinct descriptions of such algorithms,
highlighting the roles of both parallelism and recursion. Simple algebraic
properties of this data structure can be exploited to derive properties of
these algorithms and establish equivalence of different algorithms that solve
the same problem.&quot;
  The quote above illustrates a widely shared assumption about recursion
implementations: either they are done in purely structural terms or they cannot
be done at all.
  Multi-dimensional interpolation on a grid is one of hosts of semi-recursive
schemes that, while often referred to as recursive and routinely described in
vaguely recursive terms, cannot be implemented as a recursion in their
structural entirety.
  This article describes a computer-implemented scheme for isolating the
recursive core of interpolation on a multi-grid, an arrangement that both stems
from and provides a structural framework to a number of multi-dimensional
interpolation optimization techniques that, once implemented, provide gains in
multi-dimensional interpolation speed that, compared to some known benchmarks,
measure in multiple orders of magnitude.
  Categories and Subject Descriptors: Multi-dimensional Programming; Concurrent
Programming; Recursion
  General terms: Parallel Processing, Prioritized Processing, Interpolation,
Recursion, Multi-Cube
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3740</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3740</id><created>2011-06-19</created><updated>2011-06-21</updated><authors><author><keyname>Li</keyname><forenames>Li</forenames></author></authors><title>The Asymptotic Mandelbrot Law of Some Evolution Networks</title><categories>physics.data-an cs.SI physics.soc-ph</categories><comments>We had revised some typos in the first version. Sorry for that!</comments><journal-ref>Tsinghua Science and Technology, vol. 17, no. 3, pp. 310-312, 2012</journal-ref><doi>10.1109/TST.2012.6216761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we study some evolution networks that grow with linear
preferential attachment. Based upon some recent results on the quotient Gamma
function, we give a rigorous proof of the asymptotic Mandelbrot law for the
degree distribution $p_k \propto (k + c)^{-\gamma}$ in certain conditions. We
also analytically derive the best fitting values for the scaling exponent
$\gamma$ and the shifting coefficient $c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3742</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3742</id><created>2011-06-19</created><authors><author><keyname>Tavrov</keyname><forenames>Dan</forenames></author><author><keyname>Chertov</keyname><forenames>Oleg</forenames></author></authors><title>SSA-Caterpillar in Group Anonymity</title><categories>cs.CR</categories><comments>8 pages, 1 table, 4 figures, presented at the World Conference on
  Soft Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, it is a common practice to protect various types of statistical
data before publishing them for different researches. For instance, when
conducting extensive demographic surveys such as national census, the collected
data should be at least depersonalized to guarantee proper level of privacy
preservation. In practice, even more complicated methods of data protection
need to be used. All these methods can be generally divided into two classes.
The first ones aim at providing individual data anonymity, whereas the other
ones are focused on protecting information about a group of respondents. In
this paper, we propose a novel technique of providing group anonymity in
statistical data using singular spectrum analysis (SSA).Also, we apply SSA to
defining hidden patterns in demographic data distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3745</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3745</id><created>2011-06-19</created><updated>2011-09-07</updated><authors><author><keyname>Arenas</keyname><forenames>Marcelo</forenames><affiliation>Pontificia Universidad Catolica de Chile</affiliation></author><author><keyname>Fagin</keyname><forenames>Ronald</forenames><affiliation>IBM Research--Almaden</affiliation></author><author><keyname>Nash</keyname><forenames>Alan</forenames><affiliation>Aleph One LLC</affiliation></author></authors><title>Composition with Target Constraints</title><categories>cs.DB</categories><comments>This paper is an extended version of: M. Arenas, R. Fagin, and A.
  Nash. Composition with Target Constraints. In 13th International Conference
  on Database Theory (ICDT), pages 129-142, 2010</comments><proxy>LMCS</proxy><acm-class>H.2.5</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  8, 2011) lmcs:905</journal-ref><doi>10.2168/LMCS-7(3:13)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the composition of schema mappings, each specified by
source-to-target tgds (st-tgds), can be specified by a second-order tgd (SO
tgd). We consider the question of what happens when target constraints are
allowed. Specifically, we consider the question of specifying the composition
of standard schema mappings (those specified by st-tgds, target egds, and a
weakly acyclic set of target tgds). We show that SO tgds, even with the
assistance of arbitrary source constraints and target constraints, cannot
specify in general the composition of two standard schema mappings. Therefore,
we introduce source-to-target second-order dependencies (st-SO dependencies),
which are similar to SO tgds, but allow equations in the conclusion. We show
that st-SO dependencies (along with target egds and target tgds) are sufficient
to express the composition of every finite sequence of standard schema
mappings, and further, every st-SO dependency specifies such a composition. In
addition to this expressive power, we show that st-SO dependencies enjoy other
desirable properties. In particular, they have a polynomial-time chase that
generates a universal solution. This universal solution can be used to find the
certain answers to unions of conjunctive queries in polynomial time. It is easy
to show that the composition of an arbitrary number of standard schema mappings
is equivalent to the composition of only two standard schema mappings. We show
that surprisingly, the analogous result holds also for schema mappings
specified by just st-tgds (no target constraints). This is proven by showing
that every SO tgd is equivalent to an unnested SO tgd (one where there is no
nesting of function symbols). Similarly, we prove unnesting results for st-SO
dependencies, with the same types of consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3746</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3746</id><created>2011-06-19</created><authors><author><keyname>Fiazza</keyname><forenames>M. Camilla</forenames></author><author><keyname>Peroli</keyname><forenames>Michele</forenames></author><author><keyname>Vigan&#xf2;</keyname><forenames>Luca</forenames></author></authors><title>Attack Interference in Non-Collaborative Scenarios for Security Protocol
  Analysis [Extended Version]</title><categories>cs.CR</categories><comments>International Conference on Security and Cryptography, SECRYPT 2011
  (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In security protocol analysis, the traditional choice to consider a single
Dolev-Yao attacker is supported by the fact that models with multiple
collaborating Dolev-Yao attackers have been shown to be reducible to models
with one Dolev-Yao attacker. In this paper, we take a fundamentally different
approach and investigate the case of multiple non-collaborating attackers.
After formalizing the framework for multi-attacker scenarios, we show with a
case study that concurrent competitive attacks can interfere with each other.
We then present a new strategy to defend security protocols, based on active
exploitation of attack interference. The paper can be seen as providing two
proof-of-concept results: (i) it is possible to exploit interference to
mitigate protocol vulnerabilities, thus providing a form of protection to
protocols; (ii) the search for defense strategies requires scenarios with at
least two attackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3754</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3754</id><created>2011-06-19</created><authors><author><keyname>K&#xf6;rner</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Messuti</keyname><forenames>Silvia</forenames></author><author><keyname>Simonyi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Families of graph-different Hamilton paths</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let D be an arbitrary subset of the natural numbers. For every n, let M(n;D)
be the maximum of the cardinality of a set of Hamiltonian paths in the complete
graph K_n such that the union of any two paths from the family contains a not
necessarily induced cycle of some length from D. We determine or bound the
asymptotics of M(n;D) in various special cases. This problem is closely related
to that of the permutation capacity of graphs and constitutes a further
extension of the problem area around Shannon capacity. We also discuss how to
generalize our cycle-difference problems and present an example where cycles
are replaced by 4-cliques. These problems are in a natural duality to those of
graph intersection, initiated by Erd\&quot;os, Simonovits and S\'os. The lack of
kernel structure as a natural candidate for optimum makes our problems quite
challenging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3759</identifier>
 <datestamp>2011-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3759</id><created>2011-06-19</created><updated>2011-08-01</updated><authors><author><keyname>Nalitolela</keyname><forenames>Peter Situmbeko</forenames></author><author><keyname>Dokuchaev</keyname><forenames>Nikolai</forenames></author></authors><title>Frequency Theorem for discrete time stochastic system with
  multiplicative noise</title><categories>math.OC cs.SY</categories><msc-class>93E20, 49N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of minimizing a quadratic functional
for a discrete-time linear stochastic system with multiplicative noise, on a
standard probability space, in infinite time horizon. We show that the
necessary and sufficient conditions for the existence of the optimal control
can be formulated as matrix inequalities in frequency domain. Furthermore, we
show that if the optimal control exists, then certain Lyapunov equations must
have a solution. The optimal control is obtained by solving a deterministic
linear-quadratic optimal control problem whose functional depends on the
solution to the Lyapunov equations. Moreover, we show that under certain
conditions, solvability of the Lyapunov equations is guaranteed. We also show
that, if the frequency inequalities are strict, then the solution is unique up
to equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3767</identifier>
 <datestamp>2011-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3767</id><created>2011-06-19</created><updated>2011-07-23</updated><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author></authors><title>Rewriting Ontological Queries into Small Nonrecursive Datalog Programs</title><categories>cs.AI cs.DB cs.LO</categories><comments>A shorter version is presented at the 24th International Workshop on
  Description Logics, DL 2011, Barcelona, Spain, July 13-16, 2011. The present
  version mainly extends the proof of Theorem 1 in Section 3. We plan to post
  further extended versions of this paper in the near future</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting of ontological database access, where an Abox is
given in form of a relational database D and where a Boolean conjunctive query
q has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear
Datalog+/-. It is well-known that (T,q) can be rewritten into an equivalent
nonrecursive Datalog program P that can be directly evaluated over D. However,
for Linear Datalog? or for DL-Lite versions that allow for role inclusion, the
rewriting methods described so far result in a nonrecursive Datalog program P
of size exponential in the joint size of T and q. This gives rise to the
interesting question of whether such a rewriting necessarily needs to be of
exponential size. In this paper we show that it is actually possible to
translate (T,q) into a polynomially sized equivalent nonrecursive Datalog
program P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3773</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3773</id><created>2011-06-19</created><authors><author><keyname>Jer-Chin</keyname><affiliation>Luke</affiliation></author><author><keyname>Chuang</keyname></author></authors><title>Convex Geometry and Stoichiometry</title><categories>cs.CG math.CO</categories><comments>34 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the benefits of a convex geometric perspective for questions
on chemical stoichiometry. We show that the balancing of chemical equations,
the use of &quot;mixtures&quot; to explain multiple stoichiometry, and the half-reaction
for balancing redox actions all yield nice convex geometric interpretations. We
also relate some natural questions on reaction mechanisms with the enumeration
of lattice points in polytopes. Lastly, it is known that a given reaction
mechanism imposes linear constraints on observed stoichiometries. We consider
the inverse question of deducing reaction mechanism consistent with a given set
of linear stoichiometric restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3787</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3787</id><created>2011-06-19</created><authors><author><keyname>Hughes</keyname><forenames>Gary B.</forenames></author><author><keyname>Chraibi</keyname><forenames>Mohcine</forenames></author></authors><title>Calculating ellipse overlap areas</title><categories>physics.comp-ph cs.CG</categories><comments>85 pages, 10 figures, code in c</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a general algorithm for finding the overlap area between two
ellipses. The algorithm is based on finding a segment area (the area between an
ellipse and a secant line) given two points on the ellipse. The Gauss-Green
formula is used to determine the ellipse sector area between two points, and a
triangular area is added or subtracted to give the segment area. For two
ellipses, overlap area is calculated by adding the areas of appropriate sectors
and polygons. Intersection points for two general ellipses are found using
Ferrari's quartic formula to solve the polynomial that results from combining
the two ellipse equations. All cases for the number of intersection points (0,
1, 2, 3, 4) are handled. The algorithm is implemented in c-code, and has been
tested with a range of input ellipses. The code is efficient enough for use in
simulations that require many overlap area calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3791</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3791</id><created>2011-06-19</created><authors><author><keyname>Kuruppu</keyname><forenames>Shanika</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon</forenames></author><author><keyname>Zobel</keyname><forenames>Justin</forenames></author></authors><title>Reference Sequence Construction for Relative Compression of Genomes</title><categories>q-bio.QM cs.CE cs.IT math.IT</categories><comments>12 pages, 2 figures, to appear in the Proceedings of SPIRE2011 as a
  short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relative compression, where a set of similar strings are compressed with
respect to a reference string, is a very effective method of compressing DNA
datasets containing multiple similar sequences. Relative compression is fast to
perform and also supports rapid random access to the underlying data. The main
difficulty of relative compression is in selecting an appropriate reference
sequence. In this paper, we explore using the dictionary of repeats generated
by Comrad, Re-pair and Dna-x algorithms as reference sequences for relative
compression. We show this technique allows better compression and supports
random access just as well. The technique also allows more general repetitive
datasets to be compressed using relative compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3809</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3809</id><created>2011-06-20</created><authors><author><keyname>Tune</keyname><forenames>Paul</forenames></author><author><keyname>Veitch</keyname><forenames>Darryl</forenames></author></authors><title>Fisher Information in Flow Size Distribution</title><categories>cs.IT cs.NI math.IT</categories><comments>25 pages, 18 figures. Presented in part at the ACM SIGCOMM SIGMETRICS
  Internet Measurement Conference 2008, Vouliagmeni, Greece. Accepted in IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The flow size distribution is a useful metric for traffic modeling and
management. Its estimation based on sampled data, however, is problematic.
Previous work has shown that flow sampling (FS) offers enormous statistical
benefits over packet sampling but high resource requirements precludes its use
in routers. We present Dual Sampling (DS), a two-parameter family, which, to a
large extent, provide FS-like statistical performance by approaching FS
continuously, with just packet-sampling-like computational cost. Our work
utilizes a Fisher information based approach recently used to evaluate a number
of sampling schemes, excluding FS, for TCP flows. We revise and extend the
approach to make rigorous and fair comparisons between FS, DS and others. We
show how DS significantly outperforms other packet based methods, including
Sample and Hold, the closest packet sampling-based competitor to FS. We
describe a packet sampling-based implementation of DS and analyze its key
computational costs to show that router implementation is feasible. Our
approach offers insights into numerous issues, including the notion of `flow
quality' for understanding the relative performance of methods, and how and
when employing sequence numbers is beneficial. Our work is theoretical with
some simulation support and case studies on Internet data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3826</identifier>
 <datestamp>2011-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3826</id><created>2011-06-20</created><updated>2011-08-02</updated><authors><author><keyname>Fazli</keyname><forenames>MohammadAmin</forenames></author><author><keyname>Ghodsi</keyname><forenames>Mohammad</forenames></author><author><keyname>Habibi</keyname><forenames>Jafar</forenames></author><author><keyname>Khalilabadi</keyname><forenames>Pooya Jalaly</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Sadeghabad</keyname><forenames>Sina Sadeghian</forenames></author></authors><title>On the Non-Progressive Spread of Influence through Social Networks</title><categories>cs.SI cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of influence in social networks is studied in two main categories:
the progressive model and the non-progressive model (see e.g. the seminal work
of Kempe, Kleinberg, and Tardos in KDD 2003). While the progressive models are
suitable for modeling the spread of influence in monopolistic settings,
non-progressive are more appropriate for modeling non-monopolistic settings,
e.g., modeling diffusion of two competing technologies over a social network.
Despite the extensive work on the progressive model, non-progressive models
have not been studied well. In this paper, we study the spread of influence in
the non-progressive model under the strict majority threshold: given a graph
$G$ with a set of initially infected nodes, each node gets infected at time
$\tau$ iff a majority of its neighbors are infected at time $\tau-1$. Our goal
in the \textit{MinPTS} problem is to find a minimum-cardinality initial set of
infected nodes that would eventually converge to a steady state where all nodes
of $G$ are infected.
  We prove that while the MinPTS is NP-hard for a restricted family of graphs,
it admits an improved constant-factor approximation algorithm for power-law
graphs. We do so by proving lower and upper bounds in terms of the minimum and
maximum degree of nodes in the graph. The upper bound is achieved in turn by
applying a natural greedy algorithm. Our experimental evaluation of the greedy
algorithm also shows its superior performance compared to other algorithms for
a set of real-world graphs as well as the random power-law graphs. Finally, we
study the convergence properties of these algorithms and show that the
non-progressive model converges in at most $O(|E(G)|)$ steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3834</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3834</id><created>2011-06-20</created><authors><author><keyname>Choi</keyname><forenames>Suyong</forenames><affiliation>Korea University</affiliation></author></authors><title>Dimensionally Constrained Symbolic Regression</title><categories>stat.ML cs.NE physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe dimensionally constrained symbolic regression which has been
developed for mass measurement in certain classes of events in high-energy
physics (HEP). With symbolic regression, we can derive equations that are well
known in HEP. However, in problems with large number of variables, we find that
by constraining the terms allowed in the symbolic regression, convergence
behavior is improved. Dimensionally constrained symbolic regression (DCSR)
finds solutions with much better fitness than is normally possible with
symbolic regression. In some cases, novel solutions are found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3858</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3858</id><created>2011-06-20</created><authors><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Hua</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>A Packet Dropping Mechanism for Efficient Operation of M/M/1 Queues with
  Selfish Users</title><categories>cs.GT cs.NI math.OC</categories><comments>This work is an extended version of the conference paper: Y. Gai, H.
  Liu and B. Krishnamachari, &quot;A packet dropping-based incentive mechanism for
  M/M/1 queues with selfish users&quot;, the 30th IEEE International Conference on
  Computer Communications (IEEE INFOCOM 2011), China, April, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fundamental game theoretic problem concerning selfish users
contributing packets to an M/M/1 queue. In this game, each user controls its
own input rate so as to optimize a desired tradeoff between throughput and
delay. We first show that the original game has an inefficient Nash Equilibrium
(NE), with a Price of Anarchy (PoA) that scales linearly or worse in the number
of users. In order to improve the outcome efficiency, we propose an easily
implementable mechanism design whereby the server randomly drops packets with a
probability that is a function of the total arrival rate. We show that this
results in a modified M/M/1 queueing game that is an ordinal potential game
with at least one NE. In particular, for a linear packet dropping function,
which is similar to the Random Early Detection (RED) algorithm used in Internet
Congestion Control, we prove that there is a unique NE. We also show that the
simple best response dynamic converges to this unique equilibrium. Finally, for
this scheme, we prove that the social welfare (expressed either as the
summation of utilities of all players, or as the summation of the logarithm of
utilities of all players) at the equilibrium point can be arbitrarily close to
the social welfare at the global optimal point, i.e. the PoA can be made
arbitrarily close to 1. We also study the impact of arrival rate estimation
error on the PoA through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3862</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3862</id><created>2011-06-20</created><authors><author><keyname>Caulton</keyname><forenames>Adam</forenames></author><author><keyname>Butterfield</keyname><forenames>Jeremy</forenames></author></authors><title>On Kinds of Indiscernibility in Logic and Metaphysics</title><categories>physics.hist-ph cs.AI quant-ph</categories><comments>55 pages, 21 figures. Forthcoming, after an Appendectomy, in the
  British Journal for the Philosophy of Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the Hilbert-Bernays account as a spring-board, we first define four
ways in which two objects can be discerned from one another, using the
non-logical vocabulary of the language concerned. (These definitions are based
on definitions made by Quine and Saunders.) Because of our use of the
Hilbert-Bernays account, these definitions are in terms of the syntax of the
language. But we also relate our definitions to the idea of permutations on the
domain of quantification, and their being symmetries. These relations turn out
to be subtle---some natural conjectures about them are false. We will see in
particular that the idea of symmetry meshes with a species of indiscernibility
that we will call `absolute indiscernibility'. We then report all the logical
implications between our four kinds of discernibility. We use these four kinds
as a resource for stating four metaphysical theses about identity. Three of
these theses articulate two traditional philosophical themes: viz. the
principle of the identity of indiscernibles (which will come in two versions),
and haecceitism. The fourth is recent. Its most notable feature is that it
makes diversity (i.e. non-identity) weaker than what we will call individuality
(being an individual): two objects can be distinct but not individuals. For
this reason, it has been advocated both for quantum particles and for spacetime
points. Finally, we locate this fourth metaphysical thesis in a broader
position, which we call structuralism. We conclude with a discussion of the
semantics suitable for a structuralist, with particular reference to physical
theories as well as elementary model theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3876</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3876</id><created>2011-06-20</created><authors><author><keyname>Bellenger</keyname><forenames>Amandine</forenames><affiliation>LITIS</affiliation></author><author><keyname>Gatepaille</keyname><forenames>Sylvain</forenames></author></authors><title>Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion
  Applications</title><categories>cs.AI</categories><comments>Workshop on Theory of Belief Functions, Brest: France (2010)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays ontologies present a growing interest in Data Fusion applications.
As a matter of fact, the ontologies are seen as a semantic tool for describing
and reasoning about sensor data, objects, relations and general domain
theories. In addition, uncertainty is perhaps one of the most important
characteristics of the data and information handled by Data Fusion. However,
the fundamental nature of ontologies implies that ontologies describe only
asserted and veracious facts of the world. Different probabilistic, fuzzy and
evidential approaches already exist to fill this gap; this paper recaps the
most popular tools. However none of the tools meets exactly our purposes.
Therefore, we constructed a Dempster-Shafer ontology that can be imported into
any specific domain ontology and that enables us to instantiate it in an
uncertain manner. We also developed a Java application that enables reasoning
about these uncertain ontological instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3898</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3898</id><created>2011-06-20</created><authors><author><keyname>He</keyname><forenames>Debiao</forenames></author><author><keyname>Chen</keyname><forenames>Yitao</forenames></author></authors><title>An efficient certificateless authenticated key agreement protocol
  without bilinear pairings</title><categories>cs.CR</categories><journal-ref>Mathematical and Computer Modelling, 54 (11-12), 3143-3152, 2011</journal-ref><doi>10.1016/j.mcm.2011.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certificateless public key cryptography simplifies the complex certificate
management in the traditional public key cryptography and resolves the key
escrow problem in identity-based cryptography. Many certificateless
authenticated key agreement protocols using bilinear pairings have been
proposed. But the relative computation cost of the pairing is approximately
twenty times higher than that of the scalar multiplication over elliptic curve
group. Recently, several certificateless authenticated key agreement protocols
without pairings were proposed to improve the performance. In this paper, we
propose a new certificateless authenticated key agreement protocol without
pairing. The user in our just needs to compute five scale multiplication to
finish the key agreement. We also show the proposed protocol is secure in the
random oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3932</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3932</id><created>2011-06-20</created><authors><author><keyname>Dessalles</keyname><forenames>Jean-Louis J. -L.</forenames><affiliation>IC2</affiliation></author></authors><title>Coincidences and the encounter problem: A formal account</title><categories>cs.AI</categories><comments>30th Annual Conference of the Cognitive Science Society, Washington :
  United States (2008)</comments><proxy>ccsd</proxy><report-no>jld-08020201</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals have an intuitive perception of what makes a good coincidence.
Though the sensitivity to coincidences has often been presented as resulting
from an erroneous assessment of probability, it appears to be a genuine
competence, based on non-trivial computations. The model presented here
suggests that coincidences occur when subjects perceive complexity drops.
Co-occurring events are, together, simpler than if considered separately. This
model leads to a possible redefinition of subjective probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3940</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3940</id><created>2011-06-20</created><authors><author><keyname>de Paula</keyname><forenames>Amanda</forenames></author><author><keyname>Panazio</keyname><forenames>Cristiano</forenames></author></authors><title>Cooperative spectrum sensing over unreliable reporting channel</title><categories>stat.OT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article aims to analyze a cooperative spectrum sensing scheme using a
centralized approach with unreliable reporting channel. The spectrum sensing is
applied to a cognitive radio system, where each cognitive radio performs a
simple energy detection and send the decision to a fusion center through a
reporting channel. When the decisions are available at the fusion center, a
n-out-of-K rule is applied. The impact of the choice of the parameter n in the
cognitive radio system performance is analyzed in the case where the reporting
channel introduces errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3951</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3951</id><created>2011-06-20</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Wang</keyname><forenames>Carol</forenames></author></authors><title>Optimal rate list decoding via derivative codes</title><categories>cs.IT cs.CC cs.DS math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical family of $[n,k]_q$ Reed-Solomon codes over a field $\F_q$
consist of the evaluations of polynomials $f \in \F_q[X]$ of degree $&lt; k$ at
$n$ distinct field elements. In this work, we consider a closely related family
of codes, called (order $m$) {\em derivative codes} and defined over fields of
large characteristic, which consist of the evaluations of $f$ as well as its
first $m-1$ formal derivatives at $n$ distinct field elements. For large enough
$m$, we show that these codes can be list-decoded in polynomial time from an
error fraction approaching $1-R$, where $R=k/(nm)$ is the rate of the code.
This gives an alternate construction to folded Reed-Solomon codes for achieving
the optimal trade-off between rate and list error-correction radius. Our
decoding algorithm is linear-algebraic, and involves solving a linear system to
interpolate a multivariate polynomial, and then solving another structured
linear system to retrieve the list of candidate polynomials $f$. The algorithm
for derivative codes offers some advantages compared to a similar one for
folded Reed-Solomon codes in terms of efficient unique decoding in the presence
of side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3961</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3961</id><created>2011-06-20</created><updated>2014-11-28</updated><authors><author><keyname>David</keyname><forenames>Alexandre</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Miku&#x10d;ionis</keyname><forenames>Marius</forenames></author><author><keyname>Poulsen</keyname><forenames>Danny B&#xf8;gsted</forenames></author><author><keyname>van Vliet</keyname><forenames>Jonas</forenames></author><author><keyname>Wang</keyname><forenames>Zheng</forenames></author></authors><title>Stochastic Semantics and Statistical Model Checking for Networks of
  Priced Timed Automata</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper offers a natural stochastic semantics of Networks of Priced Timed
Automata (NPTA) based on races between components. The semantics provides the
basis for satisfaction of probabilistic Weighted CTL properties (PWCTL),
conservatively extending the classical satisfaction of timed automata with
respect to TCTL. In particular the extension allows for hard real-time
properties of timed automata expressible in TCTL to be refined by performance
properties, e.g. in terms of probabilistic guarantees of time- and cost-bounded
properties. A second contribution of the paper is the application of
Statistical Model Checking (SMC) to efficiently estimate the correctness of
non-nested PWCTL model checking problems with a desired level of confidence,
based on a number of independent runs of the NPTA. In addition to applying
classical SMC algorithms, we also offer an extension that allows to efficiently
compare performance properties of NPTAs in a parametric setting. The third
contribution is an efficient tool implementation of our result and applications
to several case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3967</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3967</id><created>2011-06-20</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Baumgartner</keyname><forenames>Robert</forenames></author></authors><title>Intelligent Self-Repairable Web Wrappers</title><categories>cs.AI cs.IR</categories><comments>12 pages, 4 figures; Proceedings of the 12th International Conference
  of the Italian Association for Artificial Intelligence, 2011</comments><journal-ref>Lecture Notes in Computer Science, 6934:274-285, 2011</journal-ref><doi>10.1007/978-3-642-23954-0_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of information available on the Web grows at an incredible high
rate. Systems and procedures devised to extract these data from Web sources
already exist, and different approaches and techniques have been investigated
during the last years. On the one hand, reliable solutions should provide
robust algorithms of Web data mining which could automatically face possible
malfunctioning or failures. On the other, in literature there is a lack of
solutions about the maintenance of these systems. Procedures that extract Web
data may be strictly interconnected with the structure of the data source
itself; thus, malfunctioning or acquisition of corrupted data could be caused,
for example, by structural modifications of data sources brought by their
owners. Nowadays, verification of data integrity and maintenance are mostly
manually managed, in order to ensure that these systems work correctly and
reliably. In this paper we propose a novel approach to create procedures able
to extract data from Web sources -- the so called Web wrappers -- which can
face possible malfunctioning caused by modifications of the structure of the
data source, and can automatically repair themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3977</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3977</id><created>2011-06-20</created><updated>2011-06-23</updated><authors><author><keyname>Ostromuhov</keyname><forenames>Leonid A.</forenames></author></authors><title>Models, Calculation and Optimization of Gas Networks, Equipment and
  Contracts for Design, Operation, Booking and Accounting</title><categories>cs.CE</categories><comments>The paper is presented on the World Symposium on Computing in the Gas
  Industry, 26-28.04.1999 in Florence, Italy</comments><msc-class>cs.CE, cs.DM, cs.NA, cs.SY</msc-class><acm-class>J.2; G.1; G.2; I.6.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are proposed models of contracts, technological equipment and gas
networks and methods of their optimization. The flow in network undergoes
restrictions of contracts and equipment to be operated. The values of sources
and sinks are provided by contracts. The contract models represent (sub-)
networks. The simplest contracts represent either nodes or edges. Equipment is
modeled by edges. More sophisticated equipment is represented by sub-networks.
Examples of such equipment are multi-poles and compressor stations with many
entries and exits. The edges can be of different types corresponding to
equipment and contracts. On such edges, there are given systems of equation and
inequalities simulating the contracts and equipment. On this base, the methods
proposed that allow: calculation and control of contract values for booking on
future days and for accounting of sales and purchases; simulation and
optimization of design and of operation of gas networks. These models and
methods are implemented in software systems ACCORD and Graphicord as well as in
the distributed control system used by Wingas, Germany. As numerical example,
the industrial computations are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.3981</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.3981</id><created>2011-06-20</created><authors><author><keyname>Mackenthun</keyname><forenames>Kenneth M.</forenames><suffix>Jr</suffix></author></authors><title>Group Codes and the Schreier matrix form</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a group trellis, the sequence of branches that split from the identity
path and merge to the identity path form two normal chains. The Schreier
refinement theorem can be applied to these two normal chains. The refinement of
the two normal chains can be written in the form of a matrix, called the
Schreier matrix form, with rows and columns determined by the two normal
chains.
  Based on the Schreier matrix form, we give an encoder structure for a group
code which is an estimator. The encoder uses the important idea of shortest
length generator sequences previously explained by Forney and Trott. In this
encoder the generator sequences are shown to have an additional property: the
components of the generators are coset representatives in a chain coset
decomposition of the branch group B of the code. Therefore this encoder appears
to be a natural form for a group code encoder. The encoder has a register
implementation which is somewhat different from the classical shift register
structure.
  This form of the encoder can be extended. We find a composition chain of the
branch group B and give an encoder which uses coset representatives in the
composition chain of B. When B is solvable, the generators are constructed
using coset representatives taken from prime cyclic groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4058</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4058</id><created>2011-06-20</created><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Experimental Support for a Categorical Compositional Distributional
  Model of Meaning</title><categories>cs.CL math.CT</categories><comments>11 pages, to be presented at EMNLP 2011, to be published in
  Proceedings of the 2011 Conference on Empirical Methods in Natural Language
  Processing</comments><msc-class>68T50</msc-class><acm-class>G.1.3; H.3.1; H.3.3</acm-class><journal-ref>Proceedings of the 2011 Conference on Empirical Methods in Natural
  Language Processing (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling compositional meaning for sentences using empirical distributional
methods has been a challenge for computational linguists. We implement the
abstract categorical model of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) using
data from the BNC and evaluate it. The implementation is based on unsupervised
learning of matrices for relational words and applying them to the vectors of
their arguments. The evaluation is based on the word disambiguation task
developed by Mitchell and Lapata (2008) for intransitive sentences, and on a
similar new experiment designed for transitive sentences. Our model matches the
results of its competitors in the first experiment, and betters them in the
second. The general improvement in results with increase in syntactic
complexity showcases the compositional power of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4063</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4063</id><created>2011-06-20</created><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Duan</keyname><forenames>Runyao</forenames></author></authors><title>Verification of Quantum Programs</title><categories>cs.LO quant-ph</categories><acm-class>F.3.1; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops verification methodology for quantum programs, and the
contribution of the paper is two-fold: 1. Sharir, Pnueli and Hart [SIAM J.
Comput. 13(1984)292-314] presented a general method for proving properties of
probabilistic programs, in which a probabilistic program is modeled by a Markov
chain and an assertion on the output distribution is extended into an invariant
assertion on all intermediate distributions. Their method is essentially a
probabilistic generalization of the classical Floyd inductive assertion method.
In this paper, we consider quantum programs modeled by quantum Markov chains
which are defined by super-operators. It is shown that the Sharir-Pnueli-Hart
method can be elegantly generalized to quantum programs by exploiting the
Schr\&quot;odinger-Heisenberg duality between quantum states and observables. In
particular, a completeness theorem for the Sharir-Pnueli-Hart verification
method of quantum programs is established. 2. As indicated by the completeness
theorem, the Sharir-Pnueli-Hart method is in principle effective for verifying
all properties of quantum programs that can be expressed in terms of Hermitian
operators (observables). But it is not feasible for many practical applications
because of the complicated calculation involved in the verification. For the
case of finite-dimensional state spaces, we find a method for verification of
quantum programs much simpler than the Sharir-Pnueli-Hart method by employing
the matrix representation of super-operators and Jordan decomposition of
matrices. In particular, this method enables us to compute easily the average
running time and even to analyze some interesting long-run behaviors of quantum
programs in a finite-dimensional state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4064</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4064</id><created>2011-06-20</created><updated>2011-11-09</updated><authors><author><keyname>Klein</keyname><forenames>David</forenames></author><author><keyname>Murray</keyname><forenames>Kyle</forenames></author><author><keyname>Weber</keyname><forenames>Simon</forenames></author></authors><title>Algorithmic Programming Language Identification</title><categories>cs.LG</categories><comments>11 pages. Code:
  https://github.com/simon-weber/Programming-Language-Identification</comments><acm-class>I.2.6; K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the amount of code that goes unidentified on the web, we
introduce a practical method for algorithmically identifying the programming
language of source code. Our work is based on supervised learning and
intelligent statistical features. We also explored, but abandoned, a
grammatical approach. In testing, our implementation greatly outperforms that
of an existing tool that relies on a Bayesian classifier. Code is written in
Python and available under an MIT license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4075</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4075</id><created>2011-06-20</created><authors><author><keyname>Zhang</keyname><forenames>Haizhang</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>On the Inclusion Relation of Reproducing Kernel Hilbert Spaces</title><categories>math.FA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To help understand various reproducing kernels used in applied sciences, we
investigate the inclusion relation of two reproducing kernel Hilbert spaces.
Characterizations in terms of feature maps of the corresponding reproducing
kernels are established. A full table of inclusion relations among widely-used
translation invariant kernels is given. Concrete examples for Hilbert-Schmidt
kernels are presented as well. We also discuss the preservation of such a
relation under various operations of reproducing kernels. Finally, we briefly
discuss the special inclusion with a norm equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4083</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4083</id><created>2011-06-21</created><authors><author><keyname>Harabor</keyname><forenames>Daniel</forenames></author><author><keyname>Botea</keyname><forenames>Adi</forenames></author><author><keyname>Kilby</keyname><forenames>Philip</forenames></author></authors><title>Symmetry-Based Search Space Reduction For Grid Maps</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore a symmetry-based search space reduction technique
which can speed up optimal pathfinding on undirected uniform-cost grid maps by
up to 38 times. Our technique decomposes grid maps into a set of empty
rectangles, removing from each rectangle all interior nodes and possibly some
from along the perimeter. We then add a series of macro-edges between selected
pairs of remaining perimeter nodes to facilitate provably optimal traversal
through each rectangle. We also develop a novel online pruning technique to
further speed up search. Our algorithm is fast, memory efficient and retains
the same optimality and completeness guarantees as searching on an unmodified
grid map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4090</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4090</id><created>2011-06-21</created><authors><author><keyname>Llano</keyname><forenames>Maria Teresa</forenames><affiliation>Heriot-Watt University</affiliation></author><author><keyname>Ireland</keyname><forenames>Andrew</forenames><affiliation>Heriot-Watt University</affiliation></author><author><keyname>Pease</keyname><forenames>Alison</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Discovery of Invariants through Automated Theory Formation</title><categories>cs.LO cs.AI cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 1-19</journal-ref><doi>10.4204/EPTCS.55.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refinement is a powerful mechanism for mastering the complexities that arise
when formally modelling systems. Refinement also brings with it additional
proof obligations -- requiring a developer to discover properties relating to
their design decisions. With the goal of reducing this burden, we have
investigated how a general purpose theory formation tool, HR, can be used to
automate the discovery of such properties within the context of Event-B. Here
we develop a heuristic approach to the automatic discovery of invariants and
report upon a series of experiments that we undertook in order to evaluate our
approach. The set of heuristics developed provides systematic guidance in
tailoring HR for a given Event-B development. These heuristics are based upon
proof-failure analysis, and have given rise to some promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4091</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4091</id><created>2011-06-21</created><authors><author><keyname>Perrone</keyname><forenames>Gian</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Debois</keyname><forenames>S&#xf8;ren</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Hildebrandt</keyname><forenames>Thomas</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Bigraphical Refinement</title><categories>cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 20-36</journal-ref><doi>10.4204/EPTCS.55.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a mechanism for the vertical refinement of bigraphical reactive
systems, based upon a mechanism for limiting observations and utilising the
underlying categorical structure of bigraphs. We present a motivating example
to demonstrate that the proposed notion of refinement is sensible with respect
to the theory of bigraphical reactive systems; and we propose a sufficient
condition for guaranteeing the existence of a safety-preserving vertical
refinement. We postulate the existence of a complimentary notion of horizontal
refinement for bigraphical agents, and finally we discuss the connection of
this work to the general refinement of Reeves and Streader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4092</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4092</id><created>2011-06-21</created><authors><author><keyname>Derrick</keyname><forenames>John</forenames></author><author><keyname>North</keyname><forenames>Siobh&#xe1;n</forenames></author><author><keyname>Simons</keyname><forenames>Anthony J. H.</forenames></author></authors><title>Building a refinement checker for Z</title><categories>cs.SE cs.LO</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 37-52</journal-ref><doi>10.4204/EPTCS.55.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work we have described how refinements can be checked using a
temporal logic based model-checker, and how we have built a model-checker for Z
by providing a translation of Z into the SAL input language. In this paper we
draw these two strands of work together and discuss how we have implemented
refinement checking in our Z2SAL toolset.
  The net effect of this work is that the SAL toolset can be used to check
refinements between Z specifications supplied as input files written in the
LaTeX mark-up. Two examples are used to illustrate the approach and compare it
with a manual translation and refinement check.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4093</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4093</id><created>2011-06-21</created><authors><author><keyname>Rodrigues</keyname><forenames>C&#xe9;sar</forenames><affiliation>DI-CCTC, Minho University</affiliation></author><author><keyname>Martins</keyname><forenames>Manuel A.</forenames><affiliation>Dep. Mathematics, Aveiro University,</affiliation></author><author><keyname>Madeira</keyname><forenames>Alexandre</forenames><affiliation>DI-CCTC, Minho University, Dep. Mathematics, Aveiro University and Critical Software, SA</affiliation></author><author><keyname>Barbosa</keyname><forenames>Luis S.</forenames><affiliation>DI-CCTC, Minho University</affiliation></author></authors><title>Refinement by interpretation in {\pi}-institutions</title><categories>cs.LO</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 53-64</journal-ref><doi>10.4204/EPTCS.55.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the role of interpretations, understood as multifunctions
that preserve and reflect logical consequence, as refinement witnesses in the
general setting of pi-institutions. This leads to a smooth generalization of
the refinement-by-interpretation approach, recently introduced by the authors
in more specific contexts. As a second, yet related contribution a basis is
provided to build up a refinement calculus of structured specifications in and
across arbitrary pi-institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4094</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4094</id><created>2011-06-21</created><authors><author><keyname>Miyazawa</keyname><forenames>Alvaro</forenames><affiliation>University of York</affiliation></author><author><keyname>Cavalcanti</keyname><forenames>Ana</forenames><affiliation>University of York</affiliation></author></authors><title>Refinement-based verification of sequential implementations of Stateflow
  charts</title><categories>cs.LO</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 65-83</journal-ref><doi>10.4204/EPTCS.55.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulink/Stateflow charts are widely used in industry for the specification
of control systems, which are often safety-critical. This suggests a need for a
formal treatment of such models. In previous work, we have proposed a technique
for automatic generation of formal models of Stateflow blocks to support
refinement-based reasoning. In this article, we present a refinement strategy
that supports the verification of automatically generated sequential C
implementations of Stateflow charts. In particular, we discuss how this
strategy can be specialised to take advantage of architectural features in
order to allow a higher level of automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4095</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4095</id><created>2011-06-21</created><authors><author><keyname>Reeves</keyname><forenames>Steve</forenames><affiliation>University of Waikato</affiliation></author><author><keyname>Streader</keyname><forenames>David</forenames><affiliation>University of Waikato</affiliation></author></authors><title>Refinement for Probabilistic Systems with Nondeterminism</title><categories>cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 84-100</journal-ref><doi>10.4204/EPTCS.55.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Before we combine actions and probabilities two very obvious questions should
be asked. Firstly, what does &quot;the probability of an action&quot; mean? Secondly, how
does probability interact with nondeterminism? Neither question has a single
universally agreed upon answer but by considering these questions at the outset
we build a novel and hopefully intuitive probabilistic event-based formalism.
  In previous work we have characterised refinement via the notion of testing.
Basically, if one system passes all the tests that another system passes (and
maybe more) we say the first system is a refinement of the second. This is, in
our view, an important way of characterising refinement, via the question &quot;what
sort of refinement should I be using?&quot;
  We use testing in this paper as the basis for our refinement. We develop
tests for probabilistic systems by analogy with the tests developed for
non-probabilistic systems. We make sure that our probabilistic tests, when
performed on non-probabilistic automata, give us refinement relations which
agree with for those non-probabilistic automata. We formalise this property as
a vertical refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4096</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4096</id><created>2011-06-21</created><authors><author><keyname>Ndukwu</keyname><forenames>Ukachukwu</forenames><affiliation>Macquarie University, Australia</affiliation></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames><affiliation>Macquarie University, Australia</affiliation></author></authors><title>Model exploration and analysis for quantitative safety refinement in
  probabilistic B</title><categories>cs.LO cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 101-120</journal-ref><doi>10.4204/EPTCS.55.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role played by counterexamples in standard system analysis is well known;
but less common is a notion of counterexample in probabilistic systems
refinement. In this paper we extend previous work using counterexamples to
inductive invariant properties of probabilistic systems, demonstrating how they
can be used to extend the technique of bounded model checking-style analysis
for the refinement of quantitative safety specifications in the probabilistic B
language. In particular, we show how the method can be adapted to cope with
refinements incorporating probabilistic loops. Finally, we demonstrate the
technique on pB models summarising a one-step refinement of a randomised
algorithm for finding the minimum cut of undirected graphs, and that for the
dependability analysis of a controller design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4097</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4097</id><created>2011-06-21</created><authors><author><keyname>Banach</keyname><forenames>Richard</forenames><affiliation>University of Manchester, UK</affiliation></author><author><keyname>Zhu</keyname><forenames>Huibiao</forenames><affiliation>East China Normal University, PRC</affiliation></author><author><keyname>Su</keyname><forenames>Wen</forenames><affiliation>East China Normal University, PRC</affiliation></author><author><keyname>Huang</keyname><forenames>Runlei</forenames><affiliation>Alcatel-Lucent Shanghai Bell, PRC</affiliation></author></authors><title>Formalising the Continuous/Discrete Modeling Step</title><categories>cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><acm-class>D.2.1; D.2.4</acm-class><journal-ref>EPTCS 55, 2011, pp. 121-138</journal-ref><doi>10.4204/EPTCS.55.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formally capturing the transition from a continuous model to a discrete model
is investigated using model based refinement techniques. A very simple model
for stopping (eg. of a train) is developed in both the continuous and discrete
domains. The difference between the two is quantified using generic results
from ODE theory, and these estimates can be compared with the exact solutions.
Such results do not fit well into a conventional model based refinement
framework; however they can be accommodated into a model based retrenchment.
The retrenchment is described, and the way it can interface to refinement
development on both the continuous and discrete sides is outlined. The approach
is compared to what can be achieved using hybrid systems techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4098</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4098</id><created>2011-06-21</created><authors><author><keyname>Schneider</keyname><forenames>Steve</forenames><affiliation>University of Surrey</affiliation></author><author><keyname>Treharne</keyname><forenames>Helen</forenames><affiliation>University of Surrey</affiliation></author><author><keyname>Wehrheim</keyname><forenames>Heike</forenames><affiliation>University of Paderborn</affiliation></author></authors><title>A CSP Account of Event-B Refinement</title><categories>cs.LO cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 139-154</journal-ref><doi>10.4204/EPTCS.55.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-B provides a flexible framework for stepwise system development via
refinement. The framework supports steps for (a) refining events (one-by-one),
(b) splitting events (one-by-many), and (c) introducing new events. In each of
the steps events can moreover possibly be anticipated or convergent. All such
steps are accompanied with precise proof obligations. Still, it remains unclear
what the exact relationship - in terms of a behaviour-oriented semantics -
between an Event-B machine and its refinement is. In this paper, we give a CSP
account of Event-B refinement, with a treatment for the first time of splitting
events and of anticipated events. To this end, we define a CSP semantics for
Event-B and show how the different forms of Event-B refinement can be captured
as CSP refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4099</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4099</id><created>2011-06-21</created><authors><author><keyname>Boiten</keyname><forenames>Eerke</forenames><affiliation>School of Computing, University of Kent</affiliation></author></authors><title>Perspicuity and Granularity in Refinement</title><categories>cs.SE</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 155-165</journal-ref><doi>10.4204/EPTCS.55.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reconsiders refinements which introduce actions on the concrete
level which were not present at the abstract level. It draws a distinction
between concrete actions which are &quot;perspicuous&quot; at the abstract level, and
changes of granularity of actions between different levels of abstraction.
  The main contribution of this paper is in exploring the relation between
these different methods of &quot;action refinement&quot;, and the basic refinement
relation that is used. In particular, it shows how the &quot;refining skip&quot; method
is incompatible with failures-based refinement relations, and consequently some
decisions in designing Event-B refinement are entangled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4100</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4100</id><created>2011-06-21</created><authors><author><keyname>Bostr&#xf6;m</keyname><forenames>Pontus</forenames></author><author><keyname>Degerlund</keyname><forenames>Fredrik</forenames></author><author><keyname>Sere</keyname><forenames>Kaisa</forenames></author><author><keyname>Wald&#xe9;n</keyname><forenames>Marina</forenames></author></authors><title>Concurrent Scheduling of Event-B Models</title><categories>cs.LO cs.DC</categories><comments>In Proceedings Refine 2011, arXiv:1106.3488</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 55, 2011, pp. 166-182</journal-ref><doi>10.4204/EPTCS.55.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-B is a refinement-based formal method that has been shown to be useful
in developing concurrent and distributed programs. Large models can be
decomposed into sub-models that can be refined semi-independently and executed
in parallel. In this paper, we show how to introduce explicit control flow for
the concurrent sub-models in the form of event schedules. We explore how
schedules can be designed so that their application results in a
correctness-preserving refinement step. For practical application, two patterns
for schedule introduction are provided, together with their associated proof
obligations. We demonstrate our method by applying it on the dining
philosophers problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4102</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4102</id><created>2011-06-21</created><authors><author><keyname>Shrinivaasan</keyname><forenames>Ka.</forenames></author></authors><title>Decidability of Existence and Construction of a Complement of a given
  Function</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article defines a complement of a function and conditions for existence
of such a complement function and presents few algorithms to construct a
complement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4106</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4106</id><created>2011-06-21</created><updated>2011-06-24</updated><authors><author><keyname>Harju</keyname><forenames>Tero</forenames></author></authors><title>Square-free Walks on Labelled Graphs</title><categories>math.CO cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite or infinite word is called a $G$-word for a labelled graph $G$ on
the vertex set $A_n = \{0,1,..., n-1\}$ if $w = i_1i_2...i_k \in A_n^*$, where
each factor $i_ji_{j+1}$ is an edge of $E$, i.e, $w$ represents a walk in $G$.
We show that there exists a square-free infinite $G$-word if and only if $G$
has no subgraph isomorphic to one of the cycles $C_3, \ C_4, \ C_5$, the path
$P_5$ or the claw $K_{1,3}$. The colour number $\gamma(G)$ of a graph
$G=(A_n,E)$ is the smallest integer $k$, if it exists, for which there exists a
mapping $\phi\colon A_n \to A_k$ such that $\phi(w)$ is square-free for an
infinite $G$-word $w$. We show that $\gamma(G)=3$ for $G=C_3, C_5, P_5$, but
$\gamma(G)=4$ for $G=C_4, K_{1,3}$. In particular, $\gamma(G) \leq 4$ for all
graphs that have at least five vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4128</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4128</id><created>2011-06-21</created><authors><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author><author><keyname>Ksherim</keyname><forenames>Baruch</forenames></author><author><keyname>Cohen</keyname><forenames>Reuven</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Percolation in Interdependent and Interconnected Networks: Abrupt Change
  from Second to First Order Transition</title><categories>physics.soc-ph cs.SI</categories><comments>4pages,6figures</comments><doi>10.1103/PhysRevE.84.066116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of two coupled networks system has been studied only for
dependency coupling (S. Buldyrev et. al., Nature, 2010) and only for
connectivity coupling (E. A. Leicht and R. M. D'Souza, arxiv:09070894). Here we
study, using a percolation approach, a more realistic coupled networks system
where both interdependent and interconnected links exist. We find a rich and
unusual phase transition phenomena including hybrid transition of mixed first
and second order i.e., discontinuities like a first order transition of the
giant component followed by a continuous decrease to zero like a second order
transition. Moreover, we find unusual discontinuous changes from second order
to first order transition as a function of the dependency coupling between the
two networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4131</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4131</id><created>2011-06-21</created><authors><author><keyname>Garcia-Escartin</keyname><forenames>Juan Carlos</forenames></author><author><keyname>Chamorro-Posada</keyname><forenames>Pedro</forenames></author></authors><title>Repeaters in relativistic communications</title><categories>physics.class-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication efficiency between a transmitter and a receiver is affected
by motion and the presence of gravitational fields. We study the effect of
regenerating the signal in intermediate repeaters in different relativistic
scenarios and comment the differences with respect to nonrelativistic
repeaters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4141</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4141</id><created>2011-06-21</created><updated>2011-12-13</updated><authors><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>Kernel Bounds for Path and Cycle Problems</title><categories>cs.DS cs.CC</categories><msc-class>68Q25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectivity problems like k-Path and k-Disjoint Paths relate to many
important milestones in parameterized complexity, namely the Graph Minors
Project, color coding, and the recent development of techniques for obtaining
kernelization lower bounds. This work explores the existence of polynomial
kernels for various path and cycle problems, by considering nonstandard
parameterizations. We show polynomial kernels when the parameters are a given
vertex cover, a modulator to a cluster graph, or a (promised) max leaf number.
We obtain lower bounds via cross-composition, e.g., for Hamiltonian Cycle and
related problems when parameterized by a modulator to an outerplanar graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4142</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4142</id><created>2011-06-21</created><updated>2011-10-04</updated><authors><author><keyname>Cook</keyname><forenames>Stephen A.</forenames></author><author><keyname>Le</keyname><forenames>Dai Tri Man</forenames></author><author><keyname>Ye</keyname><forenames>Yuli</forenames></author></authors><title>Complexity Classes and Theories for the Comparator Circuit Value Problem</title><categories>cs.CC cs.LO</categories><comments>This version was substantially rewritten, where several proofs were
  rewritten and many typos and mistakes were fixed. A shorter version of this
  paper appeared in CSL 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subramanian defined the complexity class CC as the set of problems log-space
reducible to the comparator circuit value problem. He proved that several other
problems are complete for CC, including the stable marriage problem, and
finding the lexicographical first maximal matching in a bipartite graph. We
suggest alternative definitions of CC based on different reducibilities and
introduce a two-sorted theory VCC* based on one of them. We sharpen and
simplify Subramanian's completeness proofs for the above two problems and
formalize them in VCC*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4176</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4176</id><created>2011-06-21</created><authors><author><keyname>Hickson</keyname><forenames>Michael</forenames></author><author><keyname>Kargakis</keyname><forenames>Yannis</forenames></author><author><keyname>Tzitzikas</keyname><forenames>Yannis</forenames></author></authors><title>Similarity-based Browsing over Linked Open Data</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing amount of data is published on the Web according to the Linked
Open Data (LOD) principles. End users would like to browse these data in a
flexible manner. In this paper we focus on similarity-based browsing and we
introduce a novel method for computing the similarity between two entities of a
given RDF/S graph. The distinctive characteristics of the proposed metric is
that it is generic (it can be used to compare nodes of any kind), it takes into
account the neighborhoods of the nodes, and it is configurable (with respect to
the accuracy vs computational complexity tradeoff). We demonstrate the behavior
of the metric using examples from an application over LOD. Finally, we
generalize and elaborate on implementation approaches harmonized with the
distributed nature of LOD which can be used for computing the most similar
entities using neighborhood-based similarity metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4184</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4184</id><created>2011-06-21</created><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Gerling</keyname><forenames>Sebastian</forenames></author><author><keyname>von Styp-Rekowsky</keyname><forenames>Philipp</forenames></author></authors><title>A Novel Attack against Android Phones</title><categories>cs.CR</categories><comments>1 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first quarter of 2011, Android has become the top-selling operating
system for smartphones. In this paper, we present a novel, highly critical
attack that allows unprompted installation of arbitrary applications from the
Android Market. Our attack is based on a single malicious application, which,
in contrast to previously known attacks, does not require the user to grant it
any permissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4200</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4200</id><created>2011-06-21</created><authors><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Consel</keyname><forenames>Charles</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, ENSEIRB</affiliation></author><author><keyname>Balland</keyname><forenames>Emilie</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Lawall</keyname><forenames>Julia</forenames><affiliation>DIKU</affiliation></author></authors><title>Faire levier sur les architectures logicielles pour guider et v\'erifier
  le d\'eveloppement d'applications SCC</title><categories>cs.PL</categories><proxy>ccsd</proxy><journal-ref>GDR GPL'11: 3\`eme journ\'ees du G\'enie de la programmation et du
  logiciel (2011) 33--34</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software architecture describes the structure of a computing system by
specifying software components and their interactions. Mapping a software
architecture to an implementation is a well known challenge. A key element of
this mapping is the architecture's description of the data and control-flow
interactions between components. The characterization of these interactions can
be rather abstract or very concrete, providing more or less implementation
guidance, programming support, and static verification. In this paper, we
explore one point in the design space between abstract and concrete component
interaction specifications. We introduce a notion of interaction contract that
expresses allowed interactions between components, describing both data and
control-flow constraints. This declaration is part of the architecture
description, allows generation of extensive programming support, and enables
various verifications. We instantiate our approach in an architecture
description language for Sense/Compute/Control applications, and describe
associated compilation and verification strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4213</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4213</id><created>2011-06-21</created><authors><author><keyname>Yao</keyname><forenames>Erlin</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Wenli</forenames></author><author><keyname>Tan</keyname><forenames>Guangming</forenames></author></authors><title>A New and Efficient Algorithm-Based Fault Tolerance Scheme for A Million
  Way Parallelism</title><categories>cs.DC</categories><comments>11 pages, 8 figures, 1 table, submitted to conference SC 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Fault tolerance overhead of high performance computing (HPC) applications is
becoming critical to the efficient utilization of HPC systems at large scale.
HPC applications typically tolerate fail-stop failures by checkpointing.
Another promising method is in the algorithm level, called algorithmic
recovery. These two methods can achieve high efficiency when the system scale
is not very large, but will both lose their effectiveness when systems approach
the scale of Exaflops, where the number of processors including in system is
expected to achieve one million. This paper develops a new and efficient
algorithm-based fault tolerance scheme for HPC applications. When failure
occurs during the execution, we do not stop to wait for the recovery of
corrupted data, but replace them with the corresponding redundant data and
continue the execution. A background accelerated recovery method is also
proposed to rebuild redundancy to tolerate multiple times of failures during
the execution. To demonstrate the feasibility of our new scheme, we have
incorporated it to the High Performance Linpack. Theoretical analysis
demonstrates that our new fault tolerance scheme can still be effective even
when the system scale achieves the Exaflops. Experiment using SiCortex SC5832
verifies the feasibility of the scheme, and indicates that the advantage of our
scheme can be observable even in a small scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4215</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4215</id><created>2011-06-21</created><updated>2012-03-18</updated><authors><author><keyname>Moretti</keyname><forenames>Paolo</forenames></author><author><keyname>Liu</keyname><forenames>Suyu</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author></authors><title>Heterogenous mean-field analysis of a generalized voter-like model on
  networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Eur. Phys. J. B (2012) 85: 88</journal-ref><doi>10.1140/epjb/e2012-20501-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalized framework for the study of voter models in complex
networks at the the heterogeneous mean-field (HMF) level that (i) yields a
unified picture for existing copy/invasion processes and (ii) allows for the
introduction of further heterogeneity through degree-selectivity rules. In the
context of the HMF approximation, our model is capable of providing
straightforward estimates for central quantities such as the exit probability
and the consensus/fixation time, based on the statistical properties of the
complex network alone. The HMF approach has the advantage of being readily
applicable also in those cases in which exact solutions are difficult to work
out. Finally, the unified formalism allows one to understand previously
proposed voter-like processes as simple limits of the generalized model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4218</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4218</id><created>2011-06-21</created><authors><author><keyname>Giardini</keyname><forenames>Francesca</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author><author><keyname>Conte</keyname><forenames>Rosaria</forenames></author></authors><title>Rooting opinions in the minds: a cognitive model and a formal account of
  opinions and their dynamics</title><categories>cs.AI</categories><journal-ref>SNAMAS 2011 : THIRD SOCIAL NETWORKS AND MULTIAGENT SYSTEMS
  SYMPOSIUM SNAMAS@AISB 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
like computer science and complexity, have tried to deal with this issue.
Despite the flourishing of different models and theories in both fields,
several key questions still remain unanswered. The understanding of how
opinions change and the way they are affected by social influence are
challenging issues requiring a thorough analysis of opinion per se but also of
the way in which they travel between agents' minds and are modulated by these
exchanges. To account for the two-faceted nature of opinions, which are mental
entities undergoing complex social processes, we outline a preliminary model in
which a cognitive theory of opinions is put forward and it is paired with a
formal description of them and of their spreading among minds. Furthermore,
investigating social influence also implies the necessity to account for the
way in which people change their minds, as a consequence of interacting with
other people, and the need to explain the higher or lower persistence of such
changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4221</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4221</id><created>2011-06-21</created><authors><author><keyname>Giardini</keyname><forenames>Francesca</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author><author><keyname>Conte</keyname><forenames>Rosaria</forenames></author></authors><title>Understanding opinions. A cognitive and formal account</title><categories>cs.AI</categories><journal-ref>Cultural and opinion dynamics: Modeling, Experiments and
  Challenges for the future @ ECCS 2011</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
as computer science and complexity, have addressed this challenge. Despite the
flourishing of different models and theories in both fields, several key
questions still remain unanswered. The aim of this paper is to challenge the
current theories on opinion by putting forward a cognitively grounded model
where opinions are described as specific mental representations whose main
properties are put forward. A comparison with reputation will be also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4232</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4232</id><created>2011-06-21</created><authors><author><keyname>Cannarsa</keyname><forenames>Piermarco</forenames></author><author><keyname>Floridia</keyname><forenames>Giuseppe</forenames></author></authors><title>Approximate controllability for linear degenerate parabolic problems
  with bilinear control</title><categories>math.AP cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the global approximate multiplicative controllability
for the linear degenerate parabolic Cauchy-Neumann problem $$ \{{array}{l}
\displaystyle{v_t-(a(x) v_x)_x =\alpha (t,x)v\,\,\qquad {in} \qquad Q_T
\,=\,(0,T)\times(-1,1)} [2.5ex] \displaystyle{a(x)v_x(t,x)|_{x=\pm 1} =
0\,\,\qquad\qquad\qquad\,\, t\in (0,T)} [2.5ex] \displaystyle{v(0,x)=v_0 (x)
\,\qquad\qquad\qquad\qquad\quad\,\, x\in (-1,1)}, {array}. $$ with the bilinear
control $\alpha(t,x)\in L^\infty (Q_T).$ The problem is strongly degenerate in
the sense that $a\in C^1([-1,1]),$ positive on $(-1,1),$ is allowed to vanish
at $\pm 1$ provided that a certain integrability condition is fulfilled. We
will show that the above system can be steered in $L^2(\Omega)$ from any
nonzero, nonnegative initial state into any neighborhood of any desirable
nonnegative target-state by bilinear static controls. Moreover, we extend the
above result relaxing the sign constraint on $v_0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4247</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4247</id><created>2011-06-21</created><authors><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>Kletenik</keyname><forenames>Devorah</forenames></author></authors><title>On the gap between ess(f) and cnf_size(f)</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Boolean function f, the quantity ess(f) denotes the largest set of
assignments that falsify f, no two of which falsify a common implicate of f.
Although ess(f)$ is clearly a lower bound on cnf_size(f) (the minimum number of
clauses in a CNF formula for f), Cepek et al. showed that it is not, in
general, a tight lower bound. They gave examples of functions f for which there
is a small gap between ess(f) and cnf_size(f). We demonstrate significantly
larger gaps. We show that the gap can be exponential in n for arbitrary Boolean
functions, and Theta(sqrt{n}) for Horn functions, where n is the number of
variables of f. We also introduce a natural extension of the quantity ess(f),
which we call ess_k(f), which is the largest set of assignments, no k of which
falsify a common implicate of f.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4251</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4251</id><created>2011-06-21</created><authors><author><keyname>Foygel</keyname><forenames>Rina</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Learning with the Weighted Trace-norm under Arbitrary Sampling
  Distributions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide rigorous guarantees on learning with the weighted trace-norm under
arbitrary sampling distributions. We show that the standard weighted trace-norm
might fail when the sampling distribution is not a product distribution (i.e.
when row and column indexes are not selected independently), present a
corrected variant for which we establish strong learning guarantees, and
demonstrate that it works better in practice. We provide guarantees when
weighting by either the true or empirical sampling distribution, and suggest
that even if the true distribution is known (or is uniform), weighting by the
empirical distribution may be beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4267</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4267</id><created>2011-06-21</created><authors><author><keyname>Brassard</keyname><forenames>Gilles</forenames></author><author><keyname>Dupuis</keyname><forenames>Frederic</forenames></author><author><keyname>Gambs</keyname><forenames>Sebastien</forenames></author><author><keyname>Tapp</keyname><forenames>Alain</forenames></author></authors><title>An optimal quantum algorithm to approximate the mean and its application
  for approximating the median of a set of points over an arbitrary distance</title><categories>quant-ph cs.DS</categories><comments>Ten pages, no figures, three algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two quantum algorithms to approximate the mean value of a
black-box function. The first algorithm is novel and asymptotically optimal
while the second is a variation on an earlier algorithm due to Aharonov. Both
algorithms have their own strengths and caveats and may be relevant in
different contexts. We then propose a new algorithm for approximating the
median of a set of points over an arbitrary distance function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4286</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4286</id><created>2011-06-21</created><authors><author><keyname>Ekrem</keyname><forenames>Ersen</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Multi-receiver Wiretap Channel with Public and Confidential Messages</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the multi-receiver wiretap channel with public and confidential
messages. In this channel, there is a transmitter that wishes to communicate
with two legitimate users in the presence of an external eavesdropper. The
transmitter sends a pair of public and confidential messages to each legitimate
user. While there are no secrecy constraints on the public messages,
confidential messages need to be transmitted in perfect secrecy. We study the
discrete memoryless multi-receiver wiretap channel as well as its Gaussian
multi-input multi-output (MIMO) instance. First, we consider the degraded
discrete memoryless channel, and obtain an inner bound for the capacity region
by using an achievable scheme that uses superposition coding and binning. Next,
we obtain an outer bound, and show that this outer bound partially matches the
inner bound, providing a partial characterization for the capacity region of
the degraded channel model. Second, we obtain an inner bound for the general,
not necessarily degraded, discrete memoryless channel by using Marton's inner
bound, superposition coding, rate-splitting and binning. Third, we consider the
degraded Gaussian MIMO channel, and show that, to evaluate both the inner and
outer bounds, considering only jointly Gaussian auxiliary random variables and
channel input is sufficient. Since the inner and outer bounds partially match,
these sufficiency results provide a partial characterization of the capacity
region of the degraded Gaussian MIMO channel. Finally, we provide an inner
bound for the capacity region of the general, not necessarily degraded,
Gaussian MIMO multi-receiver wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4288</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4288</id><created>2011-06-21</created><authors><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Hannig</keyname><forenames>Jan</forenames></author><author><keyname>Estep</keyname><forenames>Donald</forenames></author></authors><title>Continuum Limits of Markov Chains with Application to Network Modeling</title><categories>cs.NI cs.IT math.AP math.IT</categories><comments>15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the continuum limits of a class of Markov
chains. The investigation of such limits is motivated by the desire to model
very large networks. We show that under some conditions, a sequence of Markov
chains converges in some sense to the solution of a partial differential
equation. Based on such convergence we approximate Markov chains modeling
networks with a large number of components by partial differential equations.
While traditional Monte Carlo simulation for very large networks is practically
infeasible, partial differential equations can be solved with reasonable
computational overhead using well-established mathematical tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4289</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4289</id><created>2011-06-21</created><authors><author><keyname>Krzeminski</keyname><forenames>Christophe</forenames><affiliation>IEMN</affiliation></author><author><keyname>Tang</keyname><forenames>Xiaohui</forenames><affiliation>DICE - MLG</affiliation></author><author><keyname>Reckinger</keyname><forenames>Nicolas</forenames><affiliation>DICE - MLG</affiliation></author><author><keyname>Bayot</keyname><forenames>Vincent</forenames><affiliation>DICE - MLG</affiliation></author><author><keyname>Dubois</keyname><forenames>Emmanuel</forenames><affiliation>IEMN</affiliation></author></authors><title>Process Optimization and Downscaling of a Single Electron Single Dot
  Memory</title><categories>cond-mat.mtrl-sci cs.ET</categories><proxy>ccsd</proxy><journal-ref>IEEE Transactions On Nanotechnology 8, 9 (2009) 737-748</journal-ref><doi>10.1109/TNANO.2009.2021653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the process optimization of a single-electron nanoflash
electron memory. Self-aligned single dot memory structures have been fabricated
using a wet anisotropic oxidation of a silicon nanowire. One of the main issue
was to clarify the process conditions for the dot formation. Based on the
process modeling, the influence of various parameters (oxidation temperature,
nanowire shape) has been investigated. The necessity of a sharp compromise
between these different parameters to ensure the presence of the memory dot has
been established. In order to propose an aggressive memory cell, the
downscaling of the device has been carefully studied. Scaling rules show that
the size of the original device could be reduced by a factor of 2. This point
has been previously confirmed by the realization of single-electron memory
devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4300</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4300</id><created>2011-06-21</created><authors><author><keyname>Zhao</keyname><forenames>Siqi</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Wickramasuriya</keyname><forenames>Jehan</forenames></author><author><keyname>Vasudevan</keyname><forenames>Venu</forenames></author></authors><title>Human as Real-Time Sensors of Social and Physical Events: A Case Study
  of Twitter and Sports Games</title><categories>cs.SI physics.soc-ph</categories><report-no>Technical Report TR0620-2011, Rice University and Motorola Labs,
  June 2011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study how Twitter can be used as a sensor to detect frequent
and diverse social and physical events in real-time. We devise efficient data
collection and event recognition solutions that work despite various limits on
free access to Twitter data. We describe a web service implementation of our
solution and report our experience with the 2010-2011 US National Football
League (NFL) games. The service was able to recognize NFL game events within 40
seconds and with accuracy up to 90%. This capability will be very useful for
not only real-time electronic program guide for live broadcast programs but
also refined auction of advertisement slots. More importantly, it demonstrates
for the first time the feasibility of using Twitter for real-time social and
physical event detection for ubiquitous computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4333</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4333</id><created>2011-06-21</created><authors><author><keyname>Kalaitzis</keyname><forenames>Alfredo A.</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Residual Component Analysis</title><categories>stat.ML cs.AI math.ST stat.CO stat.TH</categories><comments>9 pages, 8 figures, submitted to NIPS2011</comments><msc-class>62J10 (Primary), 62-09, 62H25</msc-class><acm-class>G.1.3; G.3; I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an
eigenvalue problem on the sample covariance matrix. In this paper we consider
the situation where the data variance is already partially explained by other
factors, e.g. covariates of interest, or temporal correlations leaving some
residual variance. We decompose the residual variance into its components
through a generalized eigenvalue problem, which we call residual component
analysis (RCA). We show that canonical covariates analysis (CCA) is a special
case of our algorithm and explore a range of new algorithms that arise from the
framework. We illustrate the ideas on a gene expression time series data set
and the recovery of human pose from silhouette.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4337</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4337</id><created>2011-06-21</created><updated>2011-08-01</updated><authors><author><keyname>Grabow</keyname><forenames>Carsten</forenames></author><author><keyname>Grosskinsky</keyname><forenames>Stefan</forenames></author><author><keyname>Timme</keyname><forenames>Marc</forenames></author></authors><title>Speed of complex network synchronization</title><categories>cond-mat.dis-nn cs.SI nlin.CD physics.soc-ph</categories><comments>14 pages, 7 figures, accepted for publication in EPJB, epj style, v2:
  typos corrected</comments><journal-ref>Eur. Phys. J. B 84, 613-626 (2011)</journal-ref><doi>10.1140/epjb/e2011-20038-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchrony is one of the most common dynamical states emerging on networks.
The speed of convergence towards synchrony provides a fundamental collective
time scale for synchronizing systems. Here we study the asymptotic
synchronization times for directed networks with topologies ranging from
completely ordered, grid-like, to completely disordered, random, including
intermediate, partially disordered topologies. We extend the approach of Master
Stability Functions to quantify synchronization times. We find that the
synchronization times strongly and systematically depend on the network
topology. In particular, at fixed in-degree, stronger topological randomness
induces faster synchronization, whereas at fixed path length, synchronization
is slowest for intermediate randomness in the small-world regime. Randomly
rewiring real-world neural, social and transport networks confirms this
picture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4346</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4346</id><created>2011-06-15</created><authors><author><keyname>Topley</keyname><forenames>Kevin</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author></authors><title>Average-Consensus Algorithms in a Deterministic Framework</title><categories>cs.DC cs.SY math.OC</categories><comments>53 pages, 2 figures, 1 table. Short version submitted to IEEE Trans.
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the average-consensus problem in a multi-node network of finite
size. Communication between nodes is modeled by a sequence of directed signals
with arbitrary communication delays. Four distributed algorithms that achieve
average-consensus are proposed. Necessary and sufficient communication
conditions are given for each algorithm to achieve average-consensus. Resource
costs for each algorithm are derived based on the number of scalar values that
are required for communication and storage at each node. Numerical examples are
provided to illustrate the empirical convergence rate of the four algorithms in
comparison with a well-known &quot;gossip&quot; algorithm as well as a randomized
information spreading algorithm when assuming a fully connected random graph
with instantaneous communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4355</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4355</id><created>2011-06-21</created><updated>2011-10-17</updated><authors><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Tight Measurement Bounds for Exact Recovery of Structured Sparse Signals</title><categories>stat.ML cs.LG</categories><comments>Refined previous bound and added new experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard compressive sensing results state that to exactly recover an s
sparse signal in R^p, one requires O(s. log(p)) measurements. While this bound
is extremely useful in practice, often real world signals are not only sparse,
but also exhibit structure in the sparsity pattern. We focus on
group-structured patterns in this paper. Under this model, groups of signal
coefficients are active (or inactive) together. The groups are predefined, but
the particular set of groups that are active (i.e., in the signal support) must
be learned from measurements. We show that exploiting knowledge of groups can
further reduce the number of measurements required for exact signal recovery,
and derive universal bounds for the number of measurements needed. The bound is
universal in the sense that it only depends on the number of groups under
consideration, and not the particulars of the groups (e.g., compositions,
sizes, extents, overlaps, etc.). Experiments show that our result holds for a
variety of overlapping group configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4386</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4386</id><created>2011-06-22</created><authors><author><keyname>Dai</keyname><forenames>Wanyang</forenames></author></authors><title>Optimal Rate Scheduling via Utility-Maximization for J-User MIMO Markov
  Fading Wireless Channels with Cooperation</title><categories>math.PR cs.IT math.IT math.OC math.ST stat.TH</categories><comments>53 pages, Originally submitted on June 17, 2010; Revised version
  submitted on December 24, 2010</comments><journal-ref>Operations Research, Vol. 61, No. 6, 1450-1462 (with additional 26
  page proof of online e-companion ( Supplemental)), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a dynamic rate scheduling policy of Markov type via the solution (a
social optimal Nash equilibrium point) to a utility-maximization problem over a
randomly evolving capacity set for a class of generalized processor-sharing
queues living in a random environment, whose job arrivals to each queue follow
a doubly stochastic renewal process (DSRP). Both the random environment and the
random arrival rate of each DSRP are driven by a finite state continuous time
Markov chain (FS-CTMC). Whereas the scheduling policy optimizes in a greedy
fashion with respect to each queue and environmental state and since the
closed-form solution for the performance of such a queueing system under the
policy is difficult to obtain, we establish a reflecting diffusion with
regime-switching (RDRS) model for its measures of performance and justify its
asymptotic optimality through deriving the stochastic fluid and diffusion
limits for the corresponding system under heavy traffic and identifying a cost
function related to the utility function, which is minimized through minimizing
the workload process in the diffusion limit. More importantly, our queueing
model includes both J-user multi-input multi-output (MIMO) multiple access
channel (MAC) and broadcast channel (BC) with cooperation and admission control
as special cases. In these wireless systems, data from the J users in the MAC
or data to the J users in the BC is transmitted over a common channel that is
fading according to the FS-CTMC. The J-user capacity region for the MAC or the
BC is a set-valued stochastic process that switches with the FS-CTMC fading. In
any particular channel state, we show that each of the J-user capacity regions
is a convex set bounded by a number of linear or smooth curved facets.
Therefore our queueing model can perfectly match the dynamics of these wireless
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4399</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4399</id><created>2011-06-22</created><authors><author><keyname>Kotorowicz</keyname><forenames>M.</forenames></author><author><keyname>Kozitsky</keyname><forenames>Yu.</forenames></author></authors><title>Motif based hierarchical random graphs: structural properties and
  critical points of an Ising model</title><categories>math-ph cond-mat.stat-mech cs.SI math.MP physics.soc-ph</categories><comments>18 pages, 5 figures</comments><journal-ref>Condens. Matter Phys., 2011, vol. 14, No. 1, 13801:1-18</journal-ref><doi>10.5488/CMP.14.13801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of random graphs is introduced and studied. The graphs are
constructed in an algorithmic way from five motifs which were found in [Milo
R., Shen-Orr S., Itzkovitz S., Kashtan N., Chklovskii D., Alon U., Science,
2002, 298, 824-827]. The construction scheme resembles that used in [Hinczewski
M., A. Nihat Berker, Phys. Rev. E, 2006, 73, 066126], according to which the
short-range bonds are non-random, whereas the long-range bonds appear
independently with the same probability. A number of structural properties of
the graphs have been described, among which there are degree distributions,
clustering, amenability, small-world property. For one of the motifs, the
critical point of the Ising model defined on the corresponding graph has been
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4403</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4403</id><created>2011-06-22</created><updated>2011-12-01</updated><authors><author><keyname>Burgarth</keyname><forenames>Daniel</forenames></author><author><keyname>Giovannetti</keyname><forenames>Vittorio</forenames></author><author><keyname>Hogben</keyname><forenames>Leslie</forenames></author><author><keyname>Severini</keyname><forenames>Simone</forenames></author><author><keyname>Young</keyname><forenames>Michael</forenames></author></authors><title>Logic circuits from zero forcing</title><categories>cs.DM cs.ET math.CO quant-ph</categories><comments>5 pages, 10 EPS figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design logic circuits based on the notion of zero forcing on graphs; each
gate of the circuits is a gadget in which zero forcing is performed. We show
that such circuits can evaluate every monotone Boolean function. By using two
vertices to encode each logical bit, we obtain universal computation. We also
highlight a phenomenon of &quot;back forcing&quot; as a property of each function. Such a
phenomenon occurs in a circuit when the input of gates which have been already
used at a given time step is further modified by a computation actually
performed at a later stage. Finally, we point out that zero forcing can be also
used to implement reversible computation. The model introduced here provides a
potentially new tool in the analysis of Boolean functions, with particular
attention to monotonicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4412</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4412</id><created>2011-06-22</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>Space Lower Bounds for Online Pattern Matching</title><categories>cs.DS</categories><comments>10 pages, 7 figures, CPM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present space lower bounds for online pattern matching under a number of
different distance measures. Given a pattern of length m and a text that
arrives one character at a time, the online pattern matching problem is to
report the distance between the pattern and a sliding window of the text as
soon as the new character arrives. We require that the correct answer is given
at each position with constant probability. We give Omega(m) bit space lower
bounds for L_1, L_2, L_\infty, Hamming, edit and swap distances as well as for
any algorithm that computes the cross-correlation/convolution. We then show a
dichotomy between distance functions that have wildcard-like properties and
those that do not. In the former case which includes, as an example, pattern
matching with character classes, we give Omega(m) bit space lower bounds. For
other distance functions, we show that there exist space bounds of Omega(log m)
and O(log^2 m) bits. Finally we discuss space lower bounds for non-binary
inputs and show how in some cases they can be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4426</identifier>
 <datestamp>2011-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4426</id><created>2011-06-22</created><updated>2011-07-24</updated><authors><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>Steepest Descent Preconditioning for Nonlinear GMRES Optimization</title><categories>math.NA cs.NA math.OC</categories><comments>added comparisons with L-BFGS; added some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steepest descent preconditioning is considered for the recently proposed
nonlinear generalized minimal residual (N-GMRES) optimization algorithm for
unconstrained nonlinear optimization. Two steepest descent preconditioning
variants are proposed. The first employs a line search, while the second
employs a predefined small step. A simple global convergence proof is provided
for the N-GMRES optimization algorithm with the first steepest descent
preconditioner (with line search), under mild standard conditions on the
objective function and the line search processes. Steepest descent
preconditioning for N-GMRES optimization is also motivated by relating it to
standard non-preconditioned GMRES for linear systems in the case of a quadratic
optimization problem with symmetric positive definite operator. Numerical tests
on a variety of model problems show that the N-GMRES optimization algorithm is
able to very significantly accelerate convergence of stand-alone steepest
descent optimization. Moreover, performance of steepest-descent preconditioned
N-GMRES is shown to be competitive with standard nonlinear conjugate gradient
and limited-memory Broyden-Fletcher-Goldfarb-Shanno methods for the model
problems considered. These results serve to theoretically and numerically
establish steepest-descent preconditioned N-GMRES as a general optimization
method for unconstrained nonlinear optimization, with performance that appears
promising compared to established techniques. In addition, it is argued that
the real potential of the N-GMRES optimization framework lies in the fact that
it can make use of problem-dependent nonlinear preconditioners that are more
powerful than steepest descent (or, equivalently, N-GMRES can be used as a
simple wrapper around any other iterative optimization process to seek
acceleration of that process), and this potential is illustrated with a further
application example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4448</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4448</id><created>2011-06-22</created><updated>2011-09-22</updated><authors><author><keyname>Braibant</keyname><forenames>Thomas</forenames><affiliation>LIG</affiliation></author><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIG</affiliation></author></authors><title>Tactics for Reasoning modulo AC in Coq</title><categories>cs.MS cs.LO</categories><comments>16p</comments><proxy>ccsd</proxy><journal-ref>Certified Proofs and Programs, Ta\&quot;iwan, Province De Chine (2011)</journal-ref><doi>10.1007/978-3-642-25379-9_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a set of tools for rewriting modulo associativity and
commutativity (AC) in Coq, solving a long-standing practical problem. We use
two building blocks: first, an extensible reflexive decision procedure for
equality modulo AC; second, an OCaml plug-in for pattern matching modulo AC. We
handle associative only operations, neutral elements, uninterpreted function
symbols, and user-defined equivalence relations. By relying on type-classes for
the reification phase, we can infer these properties automatically, so that
end-users do not need to specify which operation is A or AC, or which constant
is a neutral element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4451</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4451</id><created>2011-06-22</created><authors><author><keyname>Karaman</keyname><forenames>Svebor</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Benois-Pineau</keyname><forenames>Jenny</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Dartigues</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>ISPED</affiliation></author><author><keyname>Ga&#xeb;stel</keyname><forenames>Yann</forenames><affiliation>ISPED</affiliation></author><author><keyname>M&#xe9;gret</keyname><forenames>R&#xe9;mi</forenames><affiliation>IMS</affiliation></author><author><keyname>Pinquier</keyname><forenames>Julien</forenames><affiliation>IRIT</affiliation></author></authors><title>Activities of Daily Living Indexing by Hierarchical HMM for Dementia
  Diagnostics</title><categories>cs.MM</categories><comments>2011 9th International Workshop on Content-Based Multimedia Indexing
  (CBMI), Madrid : Spain (2011)</comments><proxy>ccsd</proxy><doi>10.1109/CBMI.2011.5972524</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for indexing human ac- tivities in videos
captured from a wearable camera being worn by patients, for studies of
progression of the dementia diseases. Our method aims to produce indexes to
facilitate the navigation throughout the individual video recordings, which
could help doctors search for early signs of the dis- ease in the activities of
daily living. The recorded videos have strong motion and sharp lighting
changes, inducing noise for the analysis. The proposed approach is based on a
two steps analysis. First, we propose a new approach to segment this type of
video, based on apparent motion. Each segment is characterized by two original
motion de- scriptors, as well as color, and audio descriptors. Second, a
Hidden-Markov Model formulation is used to merge the multimodal audio and video
features, and classify the test segments. Experiments show the good properties
of the ap- proach on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4454</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4454</id><created>2011-06-22</created><updated>2011-11-23</updated><authors><author><keyname>Crowston</keyname><forenames>Robert</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Parameterized Eulerian Strong Component Arc Deletion Problem on
  Tournaments</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the problem {\sc Min-DESC}, we are given a digraph $D$ and an integer $k$,
and asked if there exists a set $A'$ of at most $k$ arcs in $D$, such that if
we remove the arcs of $A'$, in the resulting digraph every strong component is
Eulerian. {\sc Min-DESC} is NP-hard; Cechl\'{a}rov\'{a} and Schlotter (IPEC
2010) asked if the problem is fixed-parameter tractable when parameterized by
$k$. We consider the subproblem of{\sc Min-DESC} when $D$ is a tournament. We
show that this problem is fixed-parameter tractable with respect to $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4469</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4469</id><created>2011-06-22</created><authors><author><keyname>Singh</keyname><forenames>Ajit</forenames></author><author><keyname>Sharma</keyname><forenames>Nidhi</forenames></author></authors><title>Development of mechanism for enhancing data security in quantum
  cryptography</title><categories>cs.CR</categories><comments>11 pages 5 Figures 4 Tables</comments><journal-ref>Advanced Computing: An International Journal (ACIJ), Vol.2, No.3,
  May 2011, 22-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays security in communication is increasingly important to the network
communication because many categories of data are required restriction on
authorization of access, modify, delete and insert. Quantum cryptography is one
of the solutions that use property of polarization to ensure that transmitted
data is not tampered. The research paper provides the mechanism that enhances
the data security in quantum cryptography during exchange of information. In
first phase detailed explanation of Quantum key distribution's BB84 protocol is
given. BB84 protocol is used as the basis for the mechanism. In next phase the
proposed mechanism is explained. The proposed mechanism combines BB84 protocol
at two levels, from sender to receiver and then from receiver to sender.
Moreover, a logic circuit is used to combine the bits hence to reduce the
probability of eavesdropping. The key obtained can be used to exchange the
information securely further it can help in encryption and decryption of
crucial data. Double level BB84 mechanism will help in information
reconciliation as well as privacy amplification. In future the proposed
mechanism will be very beneficial where unconditional security is required
during key and other secret information exchange
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4475</identifier>
 <datestamp>2011-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4475</id><created>2011-06-22</created><updated>2011-09-12</updated><authors><author><keyname>Spyropoulou</keyname><forenames>Eirini</forenames></author><author><keyname>De Bie</keyname><forenames>Tijl</forenames></author></authors><title>Interesting Multi-Relational Patterns</title><categories>cs.DB cs.DS cs.SI</categories><comments>Accepted at ICDM'11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining patterns from multi-relational data is a problem attracting increasing
interest within the data mining community. Traditional data mining approaches
are typically developed for highly simplified types of data, such as an
attribute-value table or a binary database, such that those methods are not
directly applicable to multi-relational data. Nevertheless, multi-relational
data is a more truthful and therefore often also a more powerful representation
of reality. Mining patterns of a suitably expressive syntax directly from this
representation, is thus a research problem of great importance. In this paper
we introduce a novel approach to mining patterns in multi-relational data. We
propose a new syntax for multi-relational patterns as complete connected
subgraphs in a representation of the database as a K-partite graph. We show how
this pattern syntax is generally applicable to multirelational data, while it
reduces to well-known tiles [7] when the data is a simple binary or
attribute-value table. We propose RMiner, an efficient algorithm to mine such
patterns, and we introduce a method for quantifying their interestingness when
contrasted with prior information of the data miner. Finally, we illustrate the
usefulness of our approach by discussing results on real-world and synthetic
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4487</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4487</id><created>2011-06-22</created><authors><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Glasmachers</keyname><forenames>Tobias</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Natural Evolution Strategies</title><categories>stat.ML cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Natural Evolution Strategies (NES), a recent family of
algorithms that constitute a more principled approach to black-box optimization
than established evolutionary algorithms. NES maintains a parameterized
distribution on the set of solution candidates, and the natural gradient is
used to update the distribution's parameters in the direction of higher
expected fitness. We introduce a collection of techniques that address issues
of convergence, robustness, sample complexity, computational complexity and
sensitivity to hyperparameters. This paper explores a number of implementations
of the NES family, ranging from general-purpose multi-variate normal
distributions to heavy-tailed and separable distributions tailored towards
global optimization and search in high dimensional spaces, respectively.
Experimental results show best published performance on various standard
benchmarks, as well as competitive performance on others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4489</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4489</id><created>2011-06-22</created><authors><author><keyname>Lopez-Presa</keyname><forenames>Jose Luis</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fernandez</forenames></author></authors><title>Fast Isomorphism Testing of Graphs with Regularly-Connected Components</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Graph Isomorphism problem has both theoretical and practical interest. In
this paper we present an algorithm, called conauto-1.2, that efficiently tests
whether two graphs are isomorphic, and finds an isomorphism if they are. This
algorithm is an improved version of the algorithm conauto, which has been shown
to be very fast for random graphs and several families of hard graphs. In this
paper we establish a new theorem that allows, at very low cost, the easy
discovery of many automorphisms. This result is especially suited for graphs
with regularly connected components, and can be applied in any isomorphism
testing and canonical labeling algorithm to drastically improve its
performance. In particular, algorithm conauto-1.2 is obtained by the
application of this result to conauto. The resulting algorithm preserves all
the nice features of conauto, but drastically improves the testing of graphs
with regularly connected components. We run extensive experiments, which show
that the most popular algorithms (namely, nauty and bliss) can not compete with
conauto-1.2 for these graph families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4507</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4507</id><created>2011-06-22</created><authors><author><keyname>Pakrooh</keyname><forenames>Pooria</forenames></author><author><keyname>Amini</keyname><forenames>Arash</forenames></author><author><keyname>Marvasti</keyname><forenames>Farrokh</forenames></author></authors><title>OFDM pilot allocation for sparse channel estimation</title><categories>cs.IT math.IT</categories><doi>10.1186/1687-6180-2012-59</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In communication systems, efficient use of the spectrum is an indispensable
concern. Recently the use of compressed sensing for the purpose of estimating
Orthogonal Frequency Division Multiplexing (OFDM) sparse multipath channels has
been proposed to decrease the transmitted overhead in form of the pilot
subcarriers which are essential for channel estimation. In this paper, we
investigate the problem of deterministic pilot allocation in OFDM systems. The
method is based on minimizing the coherence of the submatrix of the unitary
Discrete Fourier Transform (DFT) matrix associated with the pilot subcarriers.
Unlike the usual case of equidistant pilot subcarriers, we show that
non-uniform patterns based on cyclic difference sets are optimal. In cases
where there are no difference sets, we perform a greedy search method for
finding a suboptimal solution. We also investigate the performance of the
recovery methods such as Orthogonal Matching Pursuit (OMP) and Iterative Method
with Adaptive Thresholding (IMAT) for estimation of the channel taps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4509</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4509</id><created>2011-06-22</created><authors><author><keyname>Storkey</keyname><forenames>Amos</forenames></author></authors><title>Machine Learning Markets</title><categories>cs.AI cs.MA cs.NE q-fin.TR stat.ML</categories><comments>Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics 2011</comments><journal-ref>Journal of Machine Learning Research W&amp;CP 15(AISTATS):716-724,
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction markets show considerable promise for developing flexible
mechanisms for machine learning. Here, machine learning markets for
multivariate systems are defined, and a utility-based framework is established
for their analysis. This differs from the usual approach of defining static
betting functions. It is shown that such markets can implement model
combination methods used in machine learning, such as product of expert and
mixture of expert approaches as equilibrium pricing models, by varying agent
utility functions. They can also implement models composed of local potentials,
and message passing methods. Prediction markets also allow for more flexible
combinations, by combining multiple different utility functions. Conversely,
the market mechanisms implement inference in the relevant probabilistic models.
This means that market mechanism can be utilized for implementing parallelized
model building and inference for probabilistic modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4514</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4514</id><created>2011-06-22</created><authors><author><keyname>Mishali</keyname><forenames>Moshe</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Sub-Nyquist Sampling: Bridging Theory and Practice</title><categories>cs.IT cs.ET math.IT</categories><comments>48 pages, 18 figures, to appear in IEEE Signal Processing Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling theory encompasses all aspects related to the conversion of
continuous-time signals to discrete streams of numbers. The famous
Shannon-Nyquist theorem has become a landmark in the development of digital
signal processing. In modern applications, an increasingly number of functions
is being pushed forward to sophisticated software algorithms, leaving only
those delicate finely-tuned tasks for the circuit level.
  In this paper, we review sampling strategies which target reduction of the
ADC rate below Nyquist. Our survey covers classic works from the early 50's of
the previous century through recent publications from the past several years.
The prime focus is bridging theory and practice, that is to pinpoint the
potential of sub-Nyquist strategies to emerge from the math to the hardware. In
that spirit, we integrate contemporary theoretical viewpoints, which study
signal modeling in a union of subspaces, together with a taste of practical
aspects, namely how the avant-garde modalities boil down to concrete signal
processing systems. Our hope is that this presentation style will attract the
interest of both researchers and engineers in the hope of promoting the
sub-Nyquist premise into practical applications, and encouraging further
research into this exciting new frontier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4557</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4557</id><created>2011-06-22</created><authors><author><keyname>Provost</keyname><forenames>F.</forenames></author><author><keyname>Weiss</keyname><forenames>G. M.</forenames></author></authors><title>Learning When Training Data are Costly: The Effect of Class Distribution
  on Tree Induction</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  315-354, 2003</journal-ref><doi>10.1613/jair.1199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For large, real-world inductive learning problems, the number of training
examples often must be limited due to the costs associated with procuring,
preparing, and storing the training examples and/or the computational costs
associated with learning from them. In such circumstances, one question of
practical importance is: if only n training examples can be selected, in what
proportion should the classes be represented? In this article we help to answer
this question by analyzing, for a fixed training-set size, the relationship
between the class distribution of the training data and the performance of
classification trees induced from these data. We study twenty-six data sets
and, for each, determine the best class distribution for learning. The
naturally occurring class distribution is shown to generally perform well when
classifier performance is evaluated using undifferentiated error rate (0/1
loss). However, when the area under the ROC curve is used to evaluate
classifier performance, a balanced distribution is shown to perform well. Since
neither of these choices for class distribution always generates the
best-performing classifier, we introduce a budget-sensitive progressive
sampling algorithm for selecting training examples based on the class
associated with each example. An empirical analysis of this algorithm shows
that the class distribution of the resulting training set yields classifiers
with good (nearly-optimal) classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4561</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4561</id><created>2011-06-22</created><authors><author><keyname>Fox</keyname><forenames>M.</forenames></author><author><keyname>Long</keyname><forenames>D.</forenames></author></authors><title>PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  61-124, 2003</journal-ref><doi>10.1613/jair.1129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years research in the planning community has moved increasingly
toward s application of planners to realistic problems involving both time and
many typ es of resources. For example, interest in planning demonstrated by the
space res earch community has inspired work in observation scheduling,
planetary rover ex ploration and spacecraft control domains. Other temporal and
resource-intensive domains including logistics planning, plant control and
manufacturing have also helped to focus the community on the modelling and
reasoning issues that must be confronted to make planning technology meet the
challenges of application. The International Planning Competitions have acted
as an important motivating fo rce behind the progress that has been made in
planning since 1998. The third com petition (held in 2002) set the planning
community the challenge of handling tim e and numeric resources. This
necessitated the development of a modelling langua ge capable of expressing
temporal and numeric properties of planning domains. In this paper we describe
the language, PDDL2.1, that was used in the competition. We describe the syntax
of the language, its formal semantics and the validation of concurrent plans.
We observe that PDDL2.1 has considerable modelling power --- exceeding the
capabilities of current planning technology --- and presents a number of
important challenges to the research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4569</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4569</id><created>2011-06-22</created><authors><author><keyname>Pynadath</keyname><forenames>D. V.</forenames></author><author><keyname>Tambe</keyname><forenames>M.</forenames></author></authors><title>The Communicative Multiagent Team Decision Problem: Analyzing Teamwork
  Theories and Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 16, pages
  389-423, 2002</journal-ref><doi>10.1613/jair.1024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the significant progress in multiagent teamwork, existing research
does not address the optimality of its prescriptions nor the complexity of the
teamwork problem. Without a characterization of the optimality-complexity
tradeoffs, it is impossible to determine whether the assumptions and
approximations made by a particular theory gain enough efficiency to justify
the losses in overall performance. To provide a tool for use by multiagent
researchers in evaluating this tradeoff, we present a unified framework, the
COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model
combines and extends existing multiagent theories, such as decentralized
partially observable Markov decision processes and economic team theory. In
addition to their generality of representation, COM-MTDPs also support the
analysis of both the optimality of team performance and the computational
complexity of the agents' decision problem. In analyzing complexity, we present
a breakdown of the computational complexity of constructing optimal teams under
various classes of problem domains, along the dimensions of observability and
communication cost. In analyzing optimality, we exploit the COM-MTDP's ability
to encode existing teamwork theories and models to encode two instantiations of
joint intentions theory taken from the literature. Furthermore, the COM-MTDP
model provides a basis for the development of novel team coordination
algorithms. We derive a domain-independent criterion for optimal communication
and provide a comparative analysis of the two joint intentions instantiations
with respect to this optimal policy. We have implemented a reusable,
domain-independent software package based on COM-MTDPs to analyze teamwork
coordination strategies, and we demonstrate its use by encoding and evaluating
the two joint intentions strategies within an example domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4570</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4570</id><created>2011-06-22</created><authors><author><keyname>Tennenholtz</keyname><forenames>M.</forenames></author></authors><title>Competitive Safety Analysis: Robust Decision-Making in Multi-Agent
  Systems</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  363-378, 2002</journal-ref><doi>10.1613/jair.1065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much work in AI deals with the selection of proper actions in a given (known
or unknown) environment. However, the way to select a proper action when facing
other agents is quite unclear. Most work in AI adopts classical game-theoretic
equilibrium analysis to predict agent behavior in such settings. This approach
however does not provide us with any guarantee for the agent. In this paper we
introduce competitive safety analysis. This approach bridges the gap between
the desired normative AI approach, where a strategy should be selected in order
to guarantee a desired payoff, and equilibrium analysis. We show that a safety
level strategy is able to guarantee the value obtained in a Nash equilibrium,
in several classical computer science settings. Then, we discuss the concept of
competitive safety strategies, and illustrate its use in a decentralized load
balancing setting, typical to network problems. In particular, we show that
when we have many agents, it is possible to guarantee an expected payoff which
is a factor of 8/9 of the payoff obtained in a Nash equilibrium. Our discussion
of competitive safety analysis for decentralized load balancing is further
developed to deal with many communication links and arbitrary speeds. Finally,
we discuss the extension of the above concepts to Bayesian games, and
illustrate their use in a basic auctions setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4571</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4571</id><created>2011-06-22</created><authors><author><keyname>Thompson</keyname><forenames>C.</forenames></author></authors><title>Acquiring Word-Meaning Mappings for Natural Language Interfaces</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  1-44, 2003</journal-ref><doi>10.1613/jair.1063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on a system, WOLFIE (WOrd Learning From Interpreted
Examples), that acquires a semantic lexicon from a corpus of sentences paired
with semantic representations. The lexicon learned consists of phrases paired
with meaning representations. WOLFIE is part of an integrated system that
learns to transform sentences into representations such as logical database
queries. Experimental results are presented demonstrating WOLFIE's ability to
learn useful lexicons for a database interface in four different natural
languages. The usefulness of the lexicons learned by WOLFIE are compared to
those acquired by a similar system, with results favorable to WOLFIE. A second
set of experiments demonstrates WOLFIE's ability to scale to larger and more
difficult, albeit artificially generated, corpora. In natural language
acquisition, it is difficult to gather the annotated data needed for supervised
learning; however, unannotated data is fairly plentiful. Active learning
methods attempt to select for annotation and training only the most informative
examples, and therefore are potentially very useful in natural language
applications. However, most results to date for active learning have only
considered standard classification tasks. To reduce annotation effort while
maintaining accuracy, we apply active learning to semantic lexicons. We show
that active learning can significantly reduce the number of annotated examples
required to achieve a given level of performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4572</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4572</id><created>2011-06-22</created><authors><author><keyname>Fern</keyname><forenames>A.</forenames></author><author><keyname>Givan</keyname><forenames>R.</forenames></author><author><keyname>Siskind</keyname><forenames>J. M.</forenames></author></authors><title>Specific-to-General Learning for Temporal Events with Application to
  Learning Event Definitions from Video</title><categories>cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  379-449, 2002</journal-ref><doi>10.1613/jair.1050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop, analyze, and evaluate a novel, supervised, specific-to-general
learner for a simple temporal logic and use the resulting algorithm to learn
visual event definitions from video sequences. First, we introduce a simple,
propositional, temporal, event-description language called AMA that is
sufficiently expressive to represent many events yet sufficiently restrictive
to support learning. We then give algorithms, along with lower and upper
complexity bounds, for the subsumption and generalization problems for AMA
formulas. We present a positive-examples--only specific-to-general learning
method based on these algorithms. We also present a polynomial-time--computable
``syntactic'' subsumption test that implies semantic subsumption without being
equivalent to it. A generalization algorithm based on syntactic subsumption can
be used in place of semantic generalization to improve the asymptotic
complexity of the resulting learning algorithm. Finally, we apply this
algorithm to the task of learning relational event definitions from video and
show that it yields definitions that are competitive with hand-coded ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4573</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4573</id><created>2011-06-22</created><authors><author><keyname>Pynadath</keyname><forenames>D. V.</forenames></author><author><keyname>Scerri</keyname><forenames>P.</forenames></author><author><keyname>Tambe</keyname><forenames>M.</forenames></author></authors><title>Towards Adjustable Autonomy for the Real World</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  171-228, 2002</journal-ref><doi>10.1613/jair.1037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adjustable autonomy refers to entities dynamically varying their own
autonomy, transferring decision-making control to other entities (typically
agents transferring control to human users) in key situations. Determining
whether and when such transfers-of-control should occur is arguably the
fundamental research problem in adjustable autonomy. Previous work has
investigated various approaches to addressing this problem but has often
focused on individual agent-human interactions. Unfortunately, domains
requiring collaboration between teams of agents and humans reveal two key
shortcomings of these previous approaches. First, these approaches use rigid
one-shot transfers of control that can result in unacceptable coordination
failures in multiagent settings. Second, they ignore costs (e.g., in terms of
time delays or effects on actions) to an agent's team due to such
transfers-of-control. To remedy these problems, this article presents a novel
approach to adjustable autonomy, based on the notion of a transfer-of-control
strategy. A transfer-of-control strategy consists of a conditional sequence of
two types of actions: (i) actions to transfer decision-making control (e.g.,
from an agent to a user or vice versa) and (ii) actions to change an agent's
pre-specified coordination constraints with team members, aimed at minimizing
miscoordination costs. The goal is for high-quality individual decisions to be
made with minimal disruption to the coordination of the team. We present a
mathematical model of transfer-of-control strategies. The model guides and
informs the operationalization of the strategies using Markov Decision
Processes, which select an optimal strategy, given an uncertain environment and
costs to the individuals and teams. The approach has been carefully evaluated,
including via its use in a real-world, deployed multi-agent system that assists
a research group in its daily activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4574</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4574</id><created>2011-06-22</created><authors><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Better Mini-Batch Algorithms via Accelerated Gradient Methods</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mini-batch algorithms have been proposed as a way to speed-up stochastic
convex optimization problems. We study how such algorithms can be improved
using accelerated gradient methods. We provide a novel analysis, which shows
how standard gradient methods may sometimes be insufficient to obtain a
significant speed-up and propose a novel accelerated gradient algorithm, which
deals with this deficiency, enjoys a uniformly superior guarantee and works
well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4575</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4575</id><created>2011-06-22</created><authors><author><keyname>Culberson</keyname><forenames>J.</forenames></author><author><keyname>Gao</keyname><forenames>Y.</forenames></author></authors><title>An Analysis of Phase Transition in NK Landscapes</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  309-332, 2002</journal-ref><doi>10.1613/jair.1081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the decision version of the NK landscape model from
the perspective of threshold phenomena and phase transitions under two random
distributions, the uniform probability model and the fixed ratio model. For the
uniform probability model, we prove that the phase transition is easy in the
sense that there is a polynomial algorithm that can solve a random instance of
the problem with the probability asymptotic to 1 as the problem size tends to
infinity. For the fixed ratio model, we establish several upper bounds for the
solubility threshold, and prove that random instances with parameters above
these upper bounds can be solved polynomially. This, together with our
empirical study for random instances generated below and in the phase
transition region, suggests that the phase transition of the fixed ratio model
is also easy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4576</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4576</id><created>2011-06-22</created><authors><author><keyname>Gamberger</keyname><forenames>D.</forenames></author><author><keyname>Lavrac</keyname><forenames>N.</forenames></author></authors><title>Expert-Guided Subgroup Discovery: Methodology and Application</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  501-527, 2002</journal-ref><doi>10.1613/jair.1089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to expert-guided subgroup discovery. The main
step of the subgroup discovery process, the induction of subgroup descriptions,
is performed by a heuristic beam search algorithm, using a novel parametrized
definition of rule quality which is analyzed in detail. The other important
steps of the proposed subgroup discovery process are the detection of
statistically significant properties of selected subgroups and subgroup
visualization: statistically significant properties are used to enrich the
descriptions of induced subgroups, while the visualization shows subgroup
properties in the form of distributions of the numbers of examples in the
subgroups. The approach is illustrated by the results obtained for a medical
problem of early detection of patient risk groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4577</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4577</id><created>2011-06-22</created><authors><author><keyname>Berry</keyname><forenames>P.</forenames></author><author><keyname>Lee</keyname><forenames>T. J.</forenames></author><author><keyname>Wilkins</keyname><forenames>D. E.</forenames></author></authors><title>Interactive Execution Monitoring of Agent Teams</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  217-261, 2003</journal-ref><doi>10.1613/jair.1112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing need for automated support for humans monitoring the
activity of distributed teams of cooperating agents, both human and machine. We
characterize the domain-independent challenges posed by this problem, and
describe how properties of domains influence the challenges and their
solutions. We will concentrate on dynamic, data-rich domains where humans are
ultimately responsible for team behavior. Thus, the automated aid should
interactively support effective and timely decision making by the human. We
present a domain-independent categorization of the types of alerts a plan-based
monitoring system might issue to a user, where each type generally requires
different monitoring techniques. We describe a monitoring framework for
integrating many domain-specific and task-specific monitoring techniques and
then using the concept of value of an alert to avoid operator overload. We use
this framework to describe an execution monitoring approach we have used to
implement Execution Assistants (EAs) in two different dynamic, data-rich,
real-world domains to assist a human in monitoring team behavior. One domain
(Army small unit operations) has hundreds of mobile, geographically distributed
agents, a combination of humans, robots, and vehicles. The other domain (teams
of unmanned ground and air vehicles) has a handful of cooperating robots. Both
domains involve unpredictable adversaries in the vicinity. Our approach
customizes monitoring behavior for each specific task, plan, and situation, as
well as for user preferences. Our EAs alert the human controller when reported
events threaten plan execution or physically threaten team members. Alerts were
generated in a timely manner without inundating the user with too many alerts
(less than 10 percent of alerts are unwanted, as judged by domain experts).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4578</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4578</id><created>2011-06-22</created><authors><author><keyname>Lang</keyname><forenames>J.</forenames></author><author><keyname>Liberatore</keyname><forenames>P.</forenames></author><author><keyname>Marquis</keyname><forenames>P.</forenames></author></authors><title>Propositional Independence - Formula-Variable Independence and
  Forgetting</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  391-443, 2003</journal-ref><doi>10.1613/jair.1113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independence -- the study of what is relevant to a given problem of reasoning
-- has received an increasing attention from the AI community. In this paper,
we consider two basic forms of independence, namely, a syntactic one and a
semantic one. We show features and drawbacks of them. In particular, while the
syntactic form of independence is computationally easy to check, there are
cases in which things that intuitively are not relevant are not recognized as
such. We also consider the problem of forgetting, i.e., distilling from a
knowledge base only the part that is relevant to the set of queries constructed
from a subset of the alphabet. While such process is computationally hard, it
allows for a simplification of subsequent reasoning, and can thus be viewed as
a form of compilation: once the relevant part of a knowledge base has been
extracted, all reasoning tasks to be performed can be simplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4579</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4579</id><created>2011-06-22</created><authors><author><keyname>Rossi</keyname><forenames>Giovanni</forenames></author></authors><title>Partition distances</title><categories>cs.DM</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternative novel measures of the distance between any two partitions of a
n-set are proposed and compared, together with a main existing one, namely
'partition-distance' D(.,.). The comparison achieves by checking their
restriction to modular elements of the partition lattice, as well as in terms
of suitable classifiers. Two of the new measures obtain through the size, a
function mapping every partition into the number of atoms finer than that
partition. One of these size-based distances extends to geometric lattices the
traditional Hamming distance between subsets, when these latter are regarded as
hypercube vertexes or binary n-vectors. After carefully framing the
environment, a main comparison finally results from the following bounding
problem: for every value k, with 0&lt;k&lt;n, of partition-distance D(.,.), determine
the minimum and maximum of the 'indicator-Hamming' distance d(P,Q) proposed
here over all pairs of partitions P,Q such that D(P,Q)=k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4582</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4582</id><created>2011-06-22</created><updated>2013-09-09</updated><authors><author><keyname>Bramson</keyname><forenames>Maury</forenames></author><author><keyname>Lu</keyname><forenames>Yi</forenames></author><author><keyname>Prabhakar</keyname><forenames>Balaji</forenames></author></authors><title>Decay of tails at equilibrium for FIFO join the shortest queue networks</title><categories>math.PR cs.DC cs.PF</categories><comments>Published in at http://dx.doi.org/10.1214/12-AAP888 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP888</report-no><journal-ref>Annals of Applied Probability 2013, Vol. 23, No. 5, 1841-1878</journal-ref><doi>10.1214/12-AAP888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In join the shortest queue networks, incoming jobs are assigned to the
shortest queue from among a randomly chosen subset of $D$ queues, in a system
of $N$ queues; after completion of service at its queue, a job leaves the
network. We also assume that jobs arrive into the system according to a
rate-$\alpha N$ Poisson process, $\alpha&lt;1$, with rate-1 service at each queue.
When the service at queues is exponentially distributed, it was shown in
Vvedenskaya et al. [Probl. Inf. Transm. 32 (1996) 15-29] that the tail of the
equilibrium queue size decays doubly exponentially in the limit as
$N\rightarrow\infty$. This is a substantial improvement over the case D=1,
where the queue size decays exponentially. The reasoning in [Probl. Inf.
Transm. 32 (1996) 15-29] does not easily generalize to jobs with nonexponential
service time distributions. A modularized program for treating general service
time distributions was introduced in Bramson et al. [In Proc. ACM SIGMETRICS
(2010) 275-286]. The program relies on an ansatz that asserts, in equilibrium,
any fixed number of queues become independent of one another as
$N\rightarrow\infty$. This ansatz was demonstrated in several settings in
Bramson et al. [Queueing Syst. 71 (2012) 247-292], including for networks where
the service discipline is FIFO and the service time distribution has a
decreasing hazard rate. In this article, we investigate the limiting behavior,
as $N\rightarrow \infty$, of the equilibrium at a queue when the service
discipline is FIFO and the service time distribution has a power law with a
given exponent $-\beta$, for $\beta&gt;1$. We show under the above ansatz that, as
$N\rightarrow\infty$, the tail of the equilibrium queue size exhibits a wide
range of behavior depending on the relationship between $\beta$ and $D$. In
particular, if $\beta&gt;D/(D-1)$, the tail is doubly exponential and, if
$\beta&lt;D/(D-1)$, the tail has a power law. When $\beta=D/(D-1)$, the tail is
exponentially distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4587</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4587</id><created>2011-06-22</created><authors><author><keyname>Edelman</keyname><forenames>Alan</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy N.</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author></authors><title>An Efficient Partitioning Oracle for Bounded-Treewidth Graphs</title><categories>cs.DS</categories><comments>Full version of a paper to appear in RANDOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioning oracles were introduced by Hassidim et al. (FOCS 2009) as a
generic tool for constant-time algorithms. For any epsilon &gt; 0, a partitioning
oracle provides query access to a fixed partition of the input bounded-degree
minor-free graph, in which every component has size poly(1/epsilon), and the
number of edges removed is at most epsilon*n, where n is the number of vertices
in the graph.
  However, the oracle of Hassidimet al. makes an exponential number of queries
to the input graph to answer every query about the partition. In this paper, we
construct an efficient partitioning oracle for graphs with constant treewidth.
The oracle makes only O(poly(1/epsilon)) queries to the input graph to answer
each query about the partition.
  Examples of bounded-treewidth graph classes include k-outerplanar graphs for
fixed k, series-parallel graphs, cactus graphs, and pseudoforests. Our oracle
yields poly(1/epsilon)-time property testing algorithms for membership in these
classes of graphs. Another application of the oracle is a poly(1/epsilon)-time
algorithm that approximates the maximum matching size, the minimum vertex cover
size, and the minimum dominating set size up to an additive epsilon*n in graphs
with bounded treewidth. Finally, the oracle can be used to test in
poly(1/epsilon) time whether the input bounded-treewidth graph is k-colorable
or perfect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4600</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4600</id><created>2011-06-22</created><authors><author><keyname>Wylie</keyname><forenames>Dennis</forenames></author></authors><title>Perturbed and Permuted: Signal Integration in Network-Structured Dynamic
  Systems</title><categories>q-bio.QM cond-mat.dis-nn cs.SY math.DS q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological systems (among others) may respond to a large variety of distinct
external stimuli, or signals. These perturbations will generally be presented
to the system not singly, but in various combinations, so that a proper
understanding of the system response requires assessment of the degree to which
the effects of one signal modulate the effects of another. This paper develops
a pair of structural metrics for sparse differential equation models of complex
dynamic systems and demonstrates that said metrics correlate with proxies of
the susceptibility of one signal-response to be altered in the context of a
second signal. One of these metrics may be interpreted as a normalized arc
density in the neighborhood of certain influential nodes; this metric appears
to correlate with increased independence of signal response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4606</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4606</id><created>2011-06-22</created><updated>2012-07-23</updated><authors><author><keyname>Manyem</keyname><forenames>Prabhu</forenames></author></authors><title>Expressibility at the machine level versus structure level: ESO
  universal Horn Logic and the class P</title><categories>cs.CC cs.LO</categories><msc-class>90C99, 68Q19, 68Q15, 68Q17, 03C13</msc-class><acm-class>F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that ESO universal Horn logic (existential second logic where the
first order part is a universal Horn formula) is insufficient to capture P, the
class of problems decidable in polynomial time. This statement is true in the
presence of a successor relation in the input vocabulary. We provide two proofs
--- one based on reduced products of two structures, and another based on
approximability theory (the second proof is under the assumption that P is not
the same as NP). We show that the difference between the results here and those
in Gr\&quot;{a}del (1991), is due to the fact that the expressions this paper deals
with are at the &quot;structure level&quot;, whereas the expressions in Gr\&quot;{a}del (1991)
are at the &quot;machine level&quot; --- a case of Easier done than said.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4632</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4632</id><created>2011-06-23</created><authors><author><keyname>Yang</keyname><forenames>Heran</forenames></author><author><keyname>Low</keyname><forenames>Tiffany</forenames></author><author><keyname>Cong</keyname><forenames>Matthew</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Inferring 3D Articulated Models for Box Packaging Robot</title><categories>cs.RO cs.AI cs.CV</categories><comments>For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a point cloud, we consider inferring kinematic models of 3D articulated
objects such as boxes for the purpose of manipulating them. While previous work
has shown how to extract a planar kinematic model (often represented as a
linear chain), such planar models do not apply to 3D objects that are composed
of segments often linked to the other segments in cyclic configurations. We
present an approach for building a model that captures the relation between the
input point cloud features and the object segment as well as the relation
between the neighboring object segments. We use a conditional random field that
allows us to model the dependencies between different segments of the object.
We test our approach on inferring the kinematic structure from partial and
noisy point cloud data for a wide variety of boxes including cake boxes, pizza
boxes, and cardboard cartons of several sizes. The inferred structure enables
our robot to successfully close these boxes by manipulating the flaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4649</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4649</id><created>2011-06-23</created><updated>2012-03-31</updated><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Russo</keyname><forenames>Lu&#xed;s M. S.</forenames></author></authors><title>Space-Efficient Data-Analysis Queries on Grids</title><categories>cs.DS cs.CG cs.DB</categories><comments>20 pages, 2 figures, submitting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider various data-analysis queries on two-dimensional points. We give
new space/time tradeoffs over previous work on geometric queries such as
dominance and rectangle visibility, and on semigroup and group queries such as
sum, average, variance, minimum and maximum. We also introduce new solutions to
queries less frequently considered in the literature such as two-dimensional
quantiles, majorities, successor/predecessor, mode, and various top-$k$
queries, considering static and dynamic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4677</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4677</id><created>2011-06-23</created><authors><author><keyname>Ben-Zwi</keyname><forenames>Oren</forenames></author><author><keyname>Newman</keyname><forenames>Ilan</forenames></author></authors><title>Optimal Bi-Valued Auctions</title><categories>cs.DS</categories><msc-class>68Q25, 68W40</msc-class><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate \emph{bi-valued} auctions in the digital good setting and
construct an explicit polynomial time deterministic auction. We prove an
unconditional tight lower bound which holds even for random superpolynomial
auctions. The analysis of the construction uses the adoption of the finer lens
of \emph{general competitiveness} which considers additive losses on top of
multiplicative ones. The result implies that general competitiveness is the
right notion to use in this setting, as this optimal auction is uncompetitive
with respect to competitive measures which do not consider additive losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4681</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4681</id><created>2011-06-23</created><updated>2011-12-07</updated><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Liu</keyname><forenames>Guizhen</forenames></author><author><keyname>Wu</keyname><forenames>Jian-Liang</forenames></author></authors><title>(1,{\lambda})-embedded graphs and the acyclic edge choosability</title><categories>math.CO cs.DM</categories><comments>Please cite this paper as X. Zhang, G. Liu, J.-L. Wu,
  (1,{\lambda})-embedded graphs and the acyclic edge choosability, Bulletin of
  the Korean Mathematical Society, to appear</comments><msc-class>05C10, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A (1,{\lambda})-embedded graph is a graph that can be embedded on a surface
with Euler characteristic {\lambda} so that each edge is crossed by at most one
other edge. A graph G is called {\alpha}-linear if there exists an integral
constant {\beta} such that e(G') \leq {\alpha} v(G')+{\beta} for each
G'\subseteq G. In this paper, it is shown that every (1,{\lambda})-embedded
graph G is 4-linear for all possible {\lambda}, and is acyclicly
edge-(3{\Delta}(G)+70)-choosable for {\lambda}=1,2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4692</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4692</id><created>2011-06-23</created><authors><author><keyname>Rekouche</keyname><forenames>Koceilah</forenames></author></authors><title>Early Phishing</title><categories>cs.CR cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4700</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4700</id><created>2011-06-23</created><authors><author><keyname>Tschannen</keyname><forenames>Julian</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Nordio</keyname><forenames>Martin</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Verifying Eiffel Programs with Boogie</title><categories>cs.SE</categories><comments>Accepted at BOOGIE 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static program verifiers such as Spec#, Dafny, jStar, and VeriFast define the
state of the art in automated functional verification techniques. The next open
challenges are to make verification tools usable even by programmers not fluent
in formal techniques. This paper presents AutoProof, a verification tool that
translates Eiffel programs to Boogie and uses the Boogie verifier to prove
them. In an effort to be usable with real programs, AutoProof fully supports
several advanced object-oriented features including polymorphism, inheritance,
and function objects. AutoProof also adopts simple strategies to reduce the
amount of annotations needed when verifying programs (e.g., frame conditions).
The paper illustrates the main features of AutoProof's translation, including
some whose implementation is underway, and demonstrates them with examples and
a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4719</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4719</id><created>2011-06-23</created><authors><author><keyname>Moll</keyname><forenames>Lukas</forenames></author><author><keyname>Tazari</keyname><forenames>Siamak</forenames></author><author><keyname>Thurley</keyname><forenames>Marc</forenames></author></authors><title>Computing hypergraph width measures exactly</title><categories>cs.CC cs.DS</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypergraph width measures are a class of hypergraph invariants important in
studying the complexity of constraint satisfaction problems (CSPs). We present
a general exact exponential algorithm for a large variety of these measures. A
connection between these and tree decompositions is established. This enables
us to almost seamlessly adapt the combinatorial and algorithmic results known
for tree decompositions of graphs to the case of hypergraphs and obtain fast
exact algorithms.
  As a consequence, we provide algorithms which, given a hypergraph H on n
vertices and m hyperedges, compute the generalized hypertree-width of H in time
O*(2^n) and compute the fractional hypertree-width of H in time
O(m*1.734601^n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4723</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4723</id><created>2011-06-23</created><authors><author><keyname>Kubler</keyname><forenames>Sylvain</forenames><affiliation>CRAN</affiliation></author><author><keyname>Derigent</keyname><forenames>William</forenames><affiliation>CRAN</affiliation></author><author><keyname>Thomas</keyname><forenames>Andr&#xe9;</forenames><affiliation>CRAN</affiliation></author><author><keyname>Rondeau</keyname><forenames>Eric</forenames><affiliation>CRAN</affiliation></author></authors><title>Key Factors for Information Dissemination on Communicating Products and
  Fixed Databases</title><categories>cs.NI</categories><comments>12 pages</comments><proxy>ccsd</proxy><journal-ref>Service Orientation in Holonic and Multi Agent Manufacturing
  Control, Paris : France (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent products carrying their own information are more and more present
nowadays. In recent years, some authors argued the usage of such products for
the Supply Chain Management Industry. Indeed, a multitude of informational
vectors take place in such environments like fixed databases or manufactured
products on which we are able to embed significant proportion of data. By
considering distributed database systems, we can allocate specific data
fragments to the product useful to manage its own evolution. The paper aims to
analyze the Supply Chain performance according to different strategies of
information distribution. Thus, different distribution patterns between
informational vectors are studied. The purpose is to determine the key factors
which lead to improve information distribution performance in term of time
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4728</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4728</id><created>2011-06-22</created><authors><author><keyname>Gong</keyname><forenames>Guang</forenames></author><author><keyname>Huo</keyname><forenames>Fei</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author></authors><title>Large Zero Autocorrelation Zone of Golay Sequences and $4^q$-QAM Golay
  Complementary Sequences</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sequences with good correlation properties have been widely adopted in modern
communications, radar and sonar applications. In this paper, we present our new
findings on some constructions of single $H$-ary Golay sequence and $4^q$-QAM
Golay complementary sequence with a large zero autocorrelation zone, where
$H\ge 2$ is an arbitrary even integer and $q\ge 2$ is an arbitrary integer.
Those new results on Golay sequences and QAM Golay complementary sequences can
be explored during synchronization and detection at the receiver end and thus
improve the performance of the communication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4847</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4847</id><created>2011-06-23</created><authors><author><keyname>Cui</keyname><forenames>Chenzhou</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Fan</keyname><forenames>Dongwei</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Zhao</keyname><forenames>Yongheng</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Kembhavi</keyname><forenames>Ajit</forenames><affiliation>Inter University Centre for Astronomy and Astrophysics, India</affiliation></author><author><keyname>He</keyname><forenames>Boliang</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Cao</keyname><forenames>Zihuang</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Li</keyname><forenames>Jian</forenames><affiliation>National Astronomical Observatories, Chinese Academy of Sciences</affiliation></author><author><keyname>Nandrekar</keyname><forenames>Deoyani</forenames><affiliation>Inter University Centre for Astronomy and Astrophysics, India</affiliation><affiliation>Department of Physics and Astronomy, The Johns Hopkins University</affiliation></author></authors><title>Enhanced Management of Personal Astronomical Data with FITSManager</title><categories>astro-ph.IM cs.SE</categories><comments>12 pages, 9 figures, Accepted for publication in New Astronomy</comments><doi>10.1016/j.newast.2011.06.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the roles of data centers and computing centers are becoming more
and more important, and on-line research is becoming the mainstream for
astronomy, individual research based on locally hosted data is still very
common. With the increase of personal storage capacity, it is easy to find
hundreds to thousands of FITS files in the personal computer of an
astrophysicist. Because Flexible Image Transport System (FITS) is a
professional data format initiated by astronomers and used mainly in the small
community, data management toolkits for FITS files are very few. Astronomers
need a powerful tool to help them manage their local astronomical data.
Although Virtual Observatory (VO) is a network oriented astronomical research
environment, its applications and related technologies provide useful solutions
to enhance the management and utilization of astronomical data hosted in an
astronomer's personal computer. FITSManager is such a tool to provide
astronomers an efficient management and utilization of their local data,
bringing VO to astronomers in a seamless and transparent way. FITSManager
provides fruitful functions for FITS file management, like thumbnail, preview,
type dependent icons, header keyword indexing and search, collaborated working
with other tools and online services, and so on. The development of the
FITSManager is an effort to fill the gap between management and analysis of
astronomical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4862</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4862</id><created>2011-06-23</created><authors><author><keyname>Ferrandez</keyname><forenames>A.</forenames></author><author><keyname>Peral</keyname><forenames>J.</forenames></author></authors><title>Translation of Pronominal Anaphora between English and Spanish:
  Discrepancies and Evaluation</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  117-147, 2003</journal-ref><doi>10.1613/jair.1115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper evaluates the different tasks carried out in the translation of
pronominal anaphora in a machine translation (MT) system. The MT interlingua
approach named AGIR (Anaphora Generation with an Interlingua Representation)
improves upon other proposals presented to date because it is able to translate
intersentential anaphors, detect co-reference chains, and translate Spanish
zero pronouns into English---issues hardly considered by other systems. The
paper presents the resolution and evaluation of these anaphora problems in AGIR
with the use of different kinds of knowledge (lexical, morphological,
syntactic, and semantic). The translation of English and Spanish anaphoric
third-person personal pronouns (including Spanish zero pronouns) into the
target language has been evaluated on unrestricted corpora. We have obtained a
precision of 80.4% and 84.8% in the translation of Spanish and English
pronouns, respectively. Although we have only studied the Spanish and English
languages, our approach can be easily extended to other languages such as
Portuguese, Italian, or Japanese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4863</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4863</id><created>2011-06-23</created><authors><author><keyname>Cemgil</keyname><forenames>A. T.</forenames></author><author><keyname>Kappen</keyname><forenames>B.</forenames></author></authors><title>Monte Carlo Methods for Tempo Tracking and Rhythm Quantization</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  45-81, 2003</journal-ref><doi>10.1613/jair.1121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic generative model for timing deviations in
expressive music performance. The structure of the proposed model is equivalent
to a switching state space model. The switch variables correspond to discrete
note locations as in a musical score. The continuous hidden variables denote
the tempo. We formulate two well known music recognition problems, namely tempo
tracking and automatic transcription (rhythm quantization) as filtering and
maximum a posteriori (MAP) state estimation tasks. Exact computation of
posterior features such as the MAP state is intractable in this model class, so
we introduce Monte Carlo methods for integration and optimization. We compare
Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated
annealing and iterative improvement) and sequential Monte Carlo methods
(particle filters). Our simulation results suggest better results with
sequential methods. The methods can be applied in both online and batch
scenarios such as tempo tracking and transcription and are thus potentially
useful in a number of music applications such as adaptive automatic
accompaniment, score typesetting and music information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4864</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4864</id><created>2011-06-23</created><authors><author><keyname>Poole</keyname><forenames>D.</forenames></author><author><keyname>Zhang</keyname><forenames>N. L.</forenames></author></authors><title>Exploiting Contextual Independence In Probabilistic Inference</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  263-313, 2003</journal-ref><doi>10.1613/jair.1122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian belief networks have grown to prominence because they provide
compact representations for many problems for which probabilistic inference is
appropriate, and there are algorithms to exploit this compactness. The next
step is to allow compact representations of the conditional probabilities of a
variable given its parents. In this paper we present such a representation that
exploits contextual independence in terms of parent contexts; which variables
act as parents may depend on the value of other variables. The internal
representation is in terms of contextual factors (confactors) that is simply a
pair of a context and a table. The algorithm, contextual variable elimination,
is based on the standard variable elimination algorithm that eliminates the
non-query variables in turn, but when eliminating a variable, the tables that
need to be multiplied can depend on the context. This algorithm reduces to
standard variable elimination when there is no contextual independence
structure to exploit. We show how this can be much more efficient than variable
elimination when there is structure to exploit. We explain why this new method
can exploit more structure than previous methods for structured belief network
inference and an analogous algorithm that uses trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4865</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4865</id><created>2011-06-23</created><authors><author><keyname>Kappen</keyname><forenames>B.</forenames></author><author><keyname>Leisink</keyname><forenames>M.</forenames></author></authors><title>Bound Propagation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  139-154, 2003</journal-ref><doi>10.1613/jair.1130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present an algorithm to compute bounds on the marginals of
a graphical model. For several small clusters of nodes upper and lower bounds
on the marginal values are computed independently of the rest of the network.
The range of allowed probability distributions over the surrounding nodes is
restricted using earlier computed bounds. As we will show, this can be
considered as a set of constraints in a linear programming problem of which the
objective function is the marginal probability of the center nodes. In this way
knowledge about the maginals of neighbouring clusters is passed to other
clusters thereby tightening the bounds on their marginals. We show that sharp
bounds can be obtained for undirected and directed graphs that are used for
practical applications, but for which exact computations are infeasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4866</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4866</id><created>2011-06-23</created><authors><author><keyname>Liberatore</keyname><forenames>P.</forenames></author></authors><title>On Polynomial Sized MDP Succinct Policies</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  551-577, 2004</journal-ref><doi>10.1613/jair.1134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policies of Markov Decision Processes (MDPs) determine the next action to
execute from the current state and, possibly, the history (the past states).
When the number of states is large, succinct representations are often used to
compactly represent both the MDPs and the policies in a reduced amount of
space. In this paper, some problems related to the size of succinctly
represented policies are analyzed. Namely, it is shown that some MDPs have
policies that can only be represented in space super-polynomial in the size of
the MDP, unless the polynomial hierarchy collapses. This fact motivates the
study of the problem of deciding whether a given MDP has a policy of a given
size and reward. Since some algorithms for MDPs work by finding a succinct
representation of the value function, the problem of deciding the existence of
a succinct representation of a value function of a given size and reward is
also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4867</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4867</id><created>2011-06-23</created><authors><author><keyname>Lin</keyname><forenames>F.</forenames></author></authors><title>Compiling Causal Theories to Successor State Axioms and STRIPS-Like
  Systems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  279-314, 2003</journal-ref><doi>10.1613/jair.1135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a system for specifying the effects of actions. Unlike those
commonly used in AI planning, our system uses an action description language
that allows one to specify the effects of actions using domain rules, which are
state constraints that can entail new action effects from old ones.
Declaratively, an action domain in our language corresponds to a nonmonotonic
causal theory in the situation calculus. Procedurally, such an action domain is
compiled into a set of logical theories, one for each action in the domain,
from which fully instantiated successor state-like axioms and STRIPS-like
systems are then generated. We expect the system to be a useful tool for
knowledge engineers writing action specifications for classical AI planning
systems, GOLOG systems, and other systems where formal specifications of
actions are needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4868</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4868</id><created>2011-06-23</created><authors><author><keyname>Simmons</keyname><forenames>R. G.</forenames></author><author><keyname>Younes</keyname><forenames>H. L. S.</forenames></author></authors><title>VHPOP: Versatile Heuristic Partial Order Planner</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  405-430, 2003</journal-ref><doi>10.1613/jair.1136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.
It draws from the experience gained in the early to mid 1990's on flaw
selection strategies for POCL planning, and combines this with more recent
developments in the field of domain independent planning such as distance based
heuristics and reachability analysis. We present an adaptation of the additive
heuristic for plan space planning, and modify it to account for possible reuse
of existing actions in a plan. We also propose a large set of novel flaw
selection strategies, and show how these can help us solve more problems than
previously possible by POCL planners. VHPOP also supports planning with
durative actions by incorporating standard techniques for temporal constraint
reasoning. We demonstrate that the same heuristic techniques used to boost the
performance of classical POCL planning can be effective in domains with
durative actions as well. The result is a versatile heuristic POCL planner
competitive with established CSP-based and heuristic state space planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4869</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4869</id><created>2011-06-23</created><authors><author><keyname>Au</keyname><forenames>T. C.</forenames></author><author><keyname>Ilghami</keyname><forenames>O.</forenames></author><author><keyname>Kuter</keyname><forenames>U.</forenames></author><author><keyname>Murdock</keyname><forenames>J. W.</forenames></author><author><keyname>Nau</keyname><forenames>D. S.</forenames></author><author><keyname>Wu</keyname><forenames>D.</forenames></author><author><keyname>Yaman</keyname><forenames>F.</forenames></author></authors><title>SHOP2: An HTN Planning System</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  379-404, 2003</journal-ref><doi>10.1613/jair.1141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SHOP2 planning system received one of the awards for distinguished
performance in the 2002 International Planning Competition. This paper
describes the features of SHOP2 which enabled it to excel in the competition,
especially those aspects of SHOP2 that deal with temporal and metric planning
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4871</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4871</id><created>2011-06-23</created><authors><author><keyname>Laird</keyname><forenames>J. E.</forenames></author><author><keyname>Wray</keyname><forenames>R. E.</forenames></author></authors><title>An Architectural Approach to Ensuring Consistency in Hierarchical
  Execution</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  355-398, 2003</journal-ref><doi>10.1613/jair.1142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4872</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4872</id><created>2011-06-23</created><authors><author><keyname>Knoblock</keyname><forenames>C. A.</forenames></author><author><keyname>Lerman</keyname><forenames>K.</forenames></author><author><keyname>Minton</keyname><forenames>S. N.</forenames></author></authors><title>Wrapper Maintenance: A Machine Learning Approach</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  149-181, 2003</journal-ref><doi>10.1613/jair.1145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of online information sources has led to an increased use
of wrappers for extracting data from Web sources. While most of the previous
research has focused on quick and efficient generation of wrappers, the
development of tools for wrapper maintenance has received less attention. This
is an important research problem because Web sources often change in ways that
prevent the wrappers from extracting data correctly. We present an efficient
algorithm that learns structural information about data from positive examples
alone. We describe how this information can be used for two wrapper maintenance
applications: wrapper verification and reinduction. The wrapper verification
system detects when a wrapper is not extracting correct data, usually because
the Web source has changed its format. The reinduction algorithm automatically
recovers from changes in the Web source by identifying data on Web pages so
that a new wrapper may be generated for this source. To validate our approach,
we monitored 27 wrappers over a period of a year. The verification algorithm
correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes,
resulting in precision of 0.73 and recall of 0.95. We validated the reinduction
algorithm on ten Web sources. We were able to successfully reinduce the
wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data
extraction task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4880</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4880</id><created>2011-06-23</created><authors><author><keyname>Zhu</keyname><forenames>Qian</forenames></author><author><keyname>Sun</keyname><forenames>Yuyin</forenames></author><author><keyname>Challa</keyname><forenames>Sashikiran</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Lajiness</keyname><forenames>Michael S.</forenames></author><author><keyname>Wild</keyname><forenames>David J.</forenames></author></authors><title>Semantic Inference using Chemogenomics Data for Drug Discovery</title><categories>q-bio.QM cs.DL cs.IR</categories><comments>23 pages, 9 figures, 4 tables</comments><doi>10.1186/1471-2105-12-256</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background Semantic Web Technology (SWT) makes it possible to integrate and
search the large volume of life science datasets in the public domain, as
demonstrated by well-known linked data projects such as LODD, Bio2RDF, and
Chem2Bio2RDF. Integration of these sets creates large networks of information.
We have previously described a tool called WENDI for aggregating information
pertaining to new chemical compounds, effectively creating evidence paths
relating the compounds to genes, diseases and so on. In this paper we examine
the utility of automatically inferring new compound-disease associations (and
thus new links in the network) based on semantically marked-up versions of
these evidence paths, rule-sets and inference engines.
  Results Through the implementation of a semantic inference algorithm, rule
set, Semantic Web methods (RDF, OWL and SPARQL) and new interfaces, we have
created a new tool called Chemogenomic Explorer that uses networks of
ontologically annotated RDF statements along with deductive reasoning tools to
infer new associations between the query structure and genes and diseases from
WENDI results. The tool then permits interactive clustering and filtering of
these evidence paths.
  Conclusions We present a new aggregate approach to inferring links between
chemical compounds and diseases using semantic inference. This approach allows
multiple evidence paths between compounds and diseases to be identified using a
rule-set and semantically annotated data, and for these evidence paths to be
clustered to show overall evidence linking the compound to a disease. We
believe this is a powerful approach, because it allows compound-disease
relationships to be ranked by the amount of evidence supporting them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4907</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4907</id><created>2011-06-24</created><authors><author><keyname>Chennamma</keyname><forenames>H. R.</forenames></author><author><keyname>Rangarajan</keyname><forenames>Lalitha</forenames></author><author><keyname>Veerabhadrappa</keyname></author></authors><title>Face Identification from Manipulated Facial Images using SIFT</title><categories>cs.CV</categories><comments>4 pages, 4 figures, IEEE 3rd International Conference on Emerging
  Trends in Engineering &amp; Technology (ICETET'2010), Nov 19-21, 2010, Goa, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Editing on digital images is ubiquitous. Identification of deliberately
modified facial images is a new challenge for face identification system. In
this paper, we address the problem of identification of a face or person from
heavily altered facial images. In this face identification problem, the input
to the system is a manipulated or transformed face image and the system reports
back the determined identity from a database of known individuals. Such a
system can be useful in mugshot identification in which mugshot database
contains two views (frontal and profile) of each criminal. We considered only
frontal view from the available database for face identification and the query
image is a manipulated face generated by face transformation software tool
available online. We propose SIFT features for efficient face identification in
this scenario. Further comparative analysis has been given with well known
eigenface approach. Experiments have been conducted with real case images to
evaluate the performance of both methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4908</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4908</id><created>2011-06-24</created><updated>2011-11-09</updated><authors><author><keyname>Lin</keyname><forenames>Jason</forenames></author><author><keyname>Yang</keyname><forenames>Chun-Wei</forenames></author><author><keyname>Tsai</keyname><forenames>Chia-Wei</forenames></author><author><keyname>Hwang</keyname><forenames>Tzonelih</forenames></author></authors><title>Comment on &quot;Semiquantum secret sharing using entangled states&quot;</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Li et al. [Phys. Rev. A, 82(2), 022303] presented two semi-quantum
secret sharing (SQSS) protocols using GHZ-like states. The proposed schemes are
rather practical because only the secret dealer requires to equip with advanced
quantum devices such as quantum memory, whereas the other agents can merely
perform classical operations to complete the secret sharing. However, this
study points out that a security pitfall exists in the eavesdropping check
phase of both schemes that could mount to an Intercept-resend attack and a
Trojan horse attack on the two schemes, respectively, to disclose the other
agent's shadow, and further to reveal the master key of the SQSS, which
contradicts to the security requirement of a QSS. Fortunately, two possible
solutions are proposed to avoid this security pitfall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4918</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4918</id><created>2011-06-24</created><authors><author><keyname>Sun</keyname><forenames>Yao</forenames></author><author><keyname>Wang</keyname><forenames>Dingkang</forenames></author></authors><title>A Generalized Criterion for Signature-based Algorithms to Compute
  Gr\&quot;obner Bases</title><categories>cs.SC</categories><comments>This paper is a substantially expanded version of the paper entitled
  &quot;A Generalized Criterion for Signature Related Gr\&quot;obner Basis Algorithms&quot;,
  which was presented at ISSAC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized criterion for signature-based algorithms to compute Gr\&quot;obner
bases is proposed in this paper. This criterion is named by &quot;generalized
criterion&quot;, because it can be specialized to almost all existing criteria for
signature-based algorithms which include the famous F5 algorithm, F5C, extended
F5, G$^2$V and the GVW algorithm. The main purpose of current paper is to study
in theory which kind of criteria is correct in signature-based algorithms and
provide a generalized method to develop new criteria. For this purpose, by
studying some key facts and observations of signature-based algorithms, a
generalized criterion is proposed. The generalized criterion only relies on a
partial order defined on a set of polynomials. When specializing the partial
order to appropriate specific orders, the generalized criterion can specialize
to almost all existing criteria of signature-based algorithms. For {\em
admissible} partial orders, a proof is presented for the correctness of the
algorithm that is based on this generalized criterion. And the partial orders
implied by the criteria of F5 and GVW are also shown to be admissible. More
importantly, the generalized criterion provides an effective method to check
whether a new criterion is correct as well as to develop new criteria for
signature-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4925</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4925</id><created>2011-06-24</created><updated>2012-08-06</updated><authors><author><keyname>Yoon</keyname><forenames>S.</forenames></author><author><keyname>Goltsev</keyname><forenames>A. V.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Belief-propagation algorithm and the Ising model on networks with
  arbitrary distributions of motifs</title><categories>cond-mat.dis-nn cs.AI</categories><comments>9 pages, 4 figures</comments><journal-ref>Physical Review E 84, 041144 (2011)</journal-ref><doi>10.1103/PhysRevE.84.041144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the belief-propagation algorithm to sparse random networks with
arbitrary distributions of motifs (triangles, loops, etc.). Each vertex in
these networks belongs to a given set of motifs (generalization of the
configuration model). These networks can be treated as sparse uncorrelated
hypergraphs in which hyperedges represent motifs. Here a hypergraph is a
generalization of a graph, where a hyperedge can connect any number of
vertices. These uncorrelated hypergraphs are tree-like (hypertrees), which
crucially simplify the problem and allow us to apply the belief-propagation
algorithm to these loopy networks with arbitrary motifs. As natural examples,
we consider motifs in the form of finite loops and cliques. We apply the
belief-propagation algorithm to the ferromagnetic Ising model on the resulting
random networks. We obtain an exact solution of this model on networks with
finite loops or cliques as motifs. We find an exact critical temperature of the
ferromagnetic phase transition and demonstrate that with increasing the
clustering coefficient and the loop size, the critical temperature increases
compared to ordinary tree-like complex networks. Our solution also gives the
birth point of the giant connected component in these loopy networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4927</identifier>
 <datestamp>2011-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4927</id><created>2011-06-24</created><updated>2011-08-22</updated><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fernandez</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author></authors><title>Probabilistic Bounds on the Length of a Longest Edge in Delaunay Graphs
  of Random Points in d-Dimensions</title><categories>cs.CG cs.DM</categories><comments>10 pages. 2 figures. In Proceedings of the 23rd Canadian Conference
  on Computational Geometry (CCCG 2011). Replacement of version 1106.4927,
  reference [5] added</comments><msc-class>05C10</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by low energy consumption in geographic routing in wireless
networks, there has been recent interest in determining bounds on the length of
edges in the Delaunay graph of randomly distributed points. Asymptotic results
are known for random networks in planar domains. In this paper, we obtain upper
and lower bounds that hold with parametric probability in any dimension, for
points distributed uniformly at random in domains with and without boundary.
The results obtained are asymptotically tight for all relevant values of such
probability and constant number of dimensions, and show that the overhead
produced by boundary nodes in the plane holds also for higher dimensions. To
our knowledge, this is the first comprehensive study on the lengths of long
edges in Delaunay graphs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4964</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4964</id><created>2011-06-24</created><authors><author><keyname>Alexandru</keyname><forenames>Andrei</forenames></author><author><keyname>Lujan</keyname><forenames>Michael</forenames></author><author><keyname>Pelissier</keyname><forenames>Craig</forenames></author><author><keyname>Gamari</keyname><forenames>Ben</forenames></author><author><keyname>Lee</keyname><forenames>Frank X.</forenames></author></authors><title>Efficient implementation of the overlap operator on multi-GPUs</title><categories>hep-lat cs.PF</categories><comments>8 pages with 10 figures; accepted for presentation at the 2011
  Symposium on Application Accelerators in High Performance Computing
  (Knoxville, July 19-20, 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice QCD calculations were one of the first applications to show the
potential of GPUs in the area of high performance computing. Our interest is to
find ways to effectively use GPUs for lattice calculations using the overlap
operator. The large memory footprint of these codes requires the use of
multiple GPUs in parallel. In this paper we show the methods we used to
implement this operator efficiently. We run our codes both on a GPU cluster and
a CPU cluster with similar interconnects. We find that to match performance the
CPU cluster requires 20-30 times more CPU cores than GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4985</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4985</id><created>2011-06-24</created><authors><author><keyname>Casanova</keyname><forenames>Henri</forenames><affiliation>CoRG</affiliation></author><author><keyname>Stillwell</keyname><forenames>Mark</forenames><affiliation>LIP, INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIP, INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>Dynamic Fractional Resource Scheduling vs. Batch Scheduling</title><categories>cs.DC</categories><comments>N&amp;deg; RR-7659 (2011)</comments><proxy>ccsd</proxy><report-no>RR-7659</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel job scheduling approach for homogeneous cluster computing
platforms. Its key feature is the use of virtual machine technology to share
fractional node resources in a precise and controlled manner. Other VM-based
scheduling approaches have focused primarily on technical issues or on
extensions to existing batch scheduling systems, while we take a more
aggressive approach and seek to find heuristics that maximize an objective
metric correlated with job performance. We derive absolute performance bounds
and develop algorithms for the online, non-clairvoyant version of our
scheduling problem. We further evaluate these algorithms in simulation against
both synthetic and real-world HPC workloads and compare our algorithms to
standard batch scheduling approaches. We find that our approach improves over
batch scheduling by orders of magnitude in terms of job stretch, while leading
to comparable or better resource utilization. Our results demonstrate that
virtualization technology coupled with lightweight online scheduling strategies
can afford dramatic improvements in performance for executing HPC workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4987</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4987</id><created>2011-06-24</created><authors><author><keyname>Nam</keyname><forenames>Sangnam</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames><affiliation>CS</affiliation></author><author><keyname>Elad</keyname><forenames>Michael</forenames><affiliation>CS</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>The Cosparse Analysis Model and Algorithms</title><categories>math.NA cs.IT math.IT</categories><comments>Submitted (2011)</comments><proxy>ccsd</proxy><doi>10.1016/j.acha.2012.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a decade of extensive study of the sparse representation synthesis
model, we can safely say that this is a mature and stable field, with clear
theoretical foundations, and appealing applications. Alongside this approach,
there is an analysis counterpart model, which, despite its similarity to the
synthesis alternative, is markedly different. Surprisingly, the analysis model
did not get a similar attention, and its understanding today is shallow and
partial. In this paper we take a closer look at the analysis approach, better
define it as a generative model for signals, and contrast it with the synthesis
one. This work proposes effective pursuit methods that aim to solve inverse
problems regularized with the analysis-model prior, accompanied by a
preliminary theoretical study of their performance. We demonstrate the
effectiveness of the analysis model in several experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.4988</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.4988</id><created>2011-06-24</created><authors><author><keyname>Nguyen</keyname><forenames>Thuy</forenames><affiliation>MAPMO</affiliation></author></authors><title>The uniform controllability property of semidiscrete approximations for
  the parabolic distributed parameter systems in Banach spaces</title><categories>math.OC cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem we consider in this work is to minimize the L^q-norm (q &gt; 2) of
the semidiscrete controls. As shown in [LT06], under the main approximation
assumptions that the discretized semigroup is uniformly analytic and that the
degree of unboundedness of control operator is lower than 1/2, the uniform
controllability property of semidiscrete approximations for the parabolic
systems is achieved in L^2. In the present paper, we show that the uniform
controllability property still continue to be asserted in L^q. (q &gt; 2) even
with the con- dition that the degree of unboundedness of control operator is
greater than 1/2. Moreover, the minimization procedure to compute the ap-
proximation controls is provided. An example of application is imple- mented
for the one dimensional heat equation with Dirichlet boundary control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5003</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5003</id><created>2011-06-24</created><authors><author><keyname>Chertkov</keyname><forenames>M.</forenames></author><author><keyname>Backhaus</keyname><forenames>S.</forenames></author><author><keyname>Turtisyn</keyname><forenames>K.</forenames></author><author><keyname>Chernyak</keyname><forenames>V.</forenames></author><author><keyname>Lebedev</keyname><forenames>V.</forenames></author></authors><title>Voltage Collapse and ODE Approach to Power Flows: Analysis of a Feeder
  Line with Static Disorder in Consumption/Production</title><categories>nlin.AO cs.CE physics.soc-ph</categories><comments>8 pages, 5 figures</comments><report-no>LANL-LAUR 11-03433</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a model of a distribution feeder connecting multiple loads to the
sub-station. Voltage is controlled directly at the head of the line
(sub-station), however, voltage anywhere further down the line is subject to
fluctuations, caused by irregularities of real and reactive distributed power
consumption/generation. The lack of a direct control of voltage along the line
may result in the voltage instability, also called voltage collapse -
phenomenon well known and documented in the power engineering literature.
Motivated by emerging photo-voltaic technology, which brings a new source of
renewable generation but also contributes significant increase in power flow
fluctuations, we reexamine the phenomenon of voltage stability and collapse. In
the limit where the number of consumers is large and spatial variations in
power flows are smooth functions of position along the feeder, we derive a set
of the power flow Ordinary Differential Equations (ODE), verify phenomenon of
voltage collapse, and study the effect of disorder and irregularity in
injection and consumption on the voltage profile by simulating the stochastic
ODE. We observe that disorder leads to nonlinear amplification of the voltage
variations at the end of the line as the point of voltage collapse is
approached. We also find that the disorder, when correlated on a scale
sufficiently small compared to the length of the line, self-averages, i.e. the
voltage profile remains spatially smooth for any individual realization of the
disorder and is correlated only at scales comparable to the length of the line.
Finally, we explain why the integrated effect of disorder on the voltage at the
end of the line cannot be described within a naive one-generator-one-load
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5037</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5037</id><created>2011-06-24</created><authors><author><keyname>Do</keyname><forenames>Thong T.</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author><author><keyname>Nguyen</keyname><forenames>Nam H.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Fast and Efficient Compressive Sensing using Structurally Random
  Matrices</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2011.2170977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new framework of fast and efficient sensing matrices
for practical compressive sensing, called Structurally Random Matrix (SRM). In
the proposed framework, we pre-randomize a sensing signal by scrambling its
samples or flipping its sample signs and then fast-transform the randomized
samples and finally, subsample the transform coefficients as the final sensing
measurements. SRM is highly relevant for large-scale, real-time compressive
sensing applications as it has fast computation and supports block-based
processing. In addition, we can show that SRM has theoretical sensing
performance comparable with that of completely random sensing matrices.
Numerical simulation results verify the validity of the theory as well as
illustrate the promising potentials of the proposed sensing framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5039</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5039</id><created>2011-06-24</created><authors><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>The Capacity of MIMO Channels with Per-Antenna Power Constraint</title><categories>cs.IT math.IT math.OC</categories><comments>26 pages, 5 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the optimal input signaling and the capacity of MIMO channels
under per-antenna power constraint. While admitting a linear eigenbeam
structure, the optimal input is no longer diagonalizable by the channel right
singular vectors as with sum power constraint. We formulate the capacity
optimization as an SDP problem and solve in closed-form the optimal input
covariance as a function of the dual variable. We then design an efficient
algorithm to find this optimal input signaling for all channel sizes. The
proposed algorithm allows for straightforward implementation in practical
systems in real time. Simulation results show that with equal constraint per
antenna, capacity with per-antenna power can be close to capacity with sum
power, but as the constraint becomes more skew, the two capacities diverge.
Forcing input eigenbeams to match the channel right singular vectors achieves
no improvement over independent signaling and can even be detrimental to
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5040</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5040</id><created>2011-06-24</created><authors><author><keyname>Guilbaud</keyname><forenames>Fabien</forenames><affiliation>LPMA</affiliation></author><author><keyname>Pham</keyname><forenames>Huyen</forenames><affiliation>LPMA, CREST</affiliation></author></authors><title>Optimal High Frequency Trading with limit and market orders</title><categories>q-fin.TR cs.SY math.OC q-fin.CP</categories><comments>22 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for studying optimal market making policies in a limit
order book (LOB). The bid-ask spread of the LOB is modelled by a Markov chain
with finite values, multiple of the tick size, and subordinated by the Poisson
process of the tick-time clock. We consider a small agent who continuously
submits limit buy/sell orders and submits market orders at discrete dates. The
objective of the market maker is to maximize her expected utility from revenue
over a short term horizon by a tradeoff between limit and market orders, while
controlling her inventory position. This is formulated as a mixed regime
switching regular/ impulse control problem that we characterize in terms of
quasi-variational system by dynamic programming methods. In the case of a
mean-variance criterion with martingale reference price or when the asset price
follows a Levy process and with exponential utility criterion, the dynamic
programming system can be reduced to a system of simple equations involving
only the inventory and spread variables. Calibration procedures are derived for
estimating the transition matrix and intensity parameters for the spread and
for Cox processes modelling the execution of limit orders. Several
computational tests are performed both on simulated and real data, and
illustrate the impact and profit when considering execution priority in limit
orders and market orders
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5053</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5053</id><created>2011-06-24</created><authors><author><keyname>Kim</keyname><forenames>Myunghwan</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Modeling Social Networks with Node Attributes using the Multiplicative
  Attribute Graph Model</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 7 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks arising from social, technological and natural domains exhibit rich
connectivity patterns and nodes in such networks are often labeled with
attributes or features. We address the question of modeling the structure of
networks where nodes have attribute information. We present a Multiplicative
Attribute Graph (MAG) model that considers nodes with categorical attributes
and models the probability of an edge as the product of individual attribute
link formation affinities. We develop a scalable variational expectation
maximization parameter estimation method. Experiments show that MAG model
reliably captures network connectivity as well as provides insights into how
different attributes shape the network structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5063</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5063</id><created>2011-06-24</created><updated>2011-11-18</updated><authors><author><keyname>Li</keyname><forenames>Qiao</forenames></author><author><keyname>Cui</keyname><forenames>Tao</forenames></author><author><keyname>Negi</keyname><forenames>Rohit</forenames></author><author><keyname>Franchetti</keyname><forenames>Franz</forenames></author><author><keyname>Ilic</keyname><forenames>Marija D.</forenames></author></authors><title>On-line Decentralized Charging of Plug-In Electric Vehicles in Power
  Systems</title><categories>math.OC cs.SY</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of plug-in electric vehicles (PEV) are gaining increasing
popularity in recent years, due to the growing societal awareness of reducing
greenhouse gas (GHG) emissions, and gaining independence on foreign oil or
petroleum. Large-scale deployment of PEVs currently faces many challenges. One
particular concern is that the PEV charging can potentially cause significant
impacts on the existing power distribution system, due to the increase in peak
load. As such, this work tries to mitigate the impacts of PEV charging by
proposing a decentralized smart PEV charging algorithm to minimize the
distribution system load variance, so that a `flat' total load profile can be
obtained. The charging algorithm is myopic, in that it controls the PEV
charging processes in each time slot based entirely on the current power system
states, without knowledge about future system dynamics. We provide theoretical
guarantees on the asymptotic optimality of the proposed charging algorithm.
Thus, compared to other forecast based smart charging approaches in the
literature, the charging algorithm not only achieves optimality asymptotically
in an on-line, and decentralized manner, but also is robust against various
uncertainties in the power system, such as random PEV driving patterns and
distributed generation (DG) with highly intermittent renewable energy sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5076</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5076</id><created>2011-06-24</created><updated>2013-05-08</updated><authors><author><keyname>He</keyname><forenames>Meng</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author></authors><title>Dynamic Range Selection in Linear Space</title><categories>cs.CG cs.DS</categories><comments>11 pages (lncs fullpage). This is a corrected version of the
  preliminary version of the paper that appeared in ISAAC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $S$ of $n$ points in the plane, we consider the problem of
answering range selection queries on $S$: that is, given an arbitrary $x$-range
$Q$ and an integer $k &gt; 0$, return the $k$-th smallest $y$-coordinate from the
set of points that have $x$-coordinates in $Q$. We present a linear space data
structure that maintains a dynamic set of $n$ points in the plane with real
coordinates, and supports range selection queries in $O((\lg n / \lg \lg n)^2)$
time, as well as insertions and deletions in $O((\lg n / \lg \lg n)^2)$
amortized time. The space usage of this data structure is an $\Theta(\lg n /
\lg \lg n)$ factor improvement over the previous best result, while maintaining
asymptotically matching query and update times. We also present a succinct data
structure that supports range selection queries on a dynamic array of $n$
values drawn from a bounded universe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5100</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5100</id><created>2011-06-24</created><authors><author><keyname>Rahmati</keyname><forenames>Ahmad</forenames></author><author><keyname>Shepard</keyname><forenames>Clayton</forenames></author><author><keyname>Tossell</keyname><forenames>Chad</forenames></author><author><keyname>Dong</keyname><forenames>Mian</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Kortum</keyname><forenames>Philip</forenames></author></authors><title>Tales of 34 iPhone Users: How they change and why they are different</title><categories>cs.HC cs.CY</categories><report-no>Technical Report TR-2011-0624, Rice University</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results from a longitudinal study of 34 iPh-one 3GS users, called
LiveLab. LiveLab collected unprecedented usage data through an in-device,
programmable logger and several structured interviews with the participants
throughout the study. We have four objectives in writing this paper: (i) share
the findings with the research community; (ii) provide insights guiding the
design of smartphone systems and applications; (iii) demonstrate the power of
prudently designed longitudinal field studies and the power of advanced
research methods; and (iv) raise important questions that the research
community can help answer in a collaborative, multidisciplinary manner.
  We show how the smartphone usage changes over the year and why the users are
different (and similar) in their usage. In particular, our findings highlight
application and web usage dynamics, the influence of socioeconomic status (SES)
on usage, and the shortcomings of iPhone 3GS and its ecosystem. We further show
that distinct classes of usage patterns exist, and these classes are best
served by different phone designs, instead of the one-size-fits-all phone Apple
provides. Our findings are significant not only for understanding smartphone
users but also in guiding device and application development and optimizations.
While we present novel results that can only be produced by a study of this
nature, we also raise new research questions to be investigated by the mobile
research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5111</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5111</id><created>2011-06-25</created><authors><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author><author><keyname>Conte</keyname><forenames>Rosaria</forenames></author></authors><title>Exploiting Reputation in Distributed Virtual Environments</title><categories>cs.AI</categories><journal-ref>Essa 2011 - The 7th European Social Simulation Association
  Conference</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cognitive research on reputation has shown several interesting properties
that can improve both the quality of services and the security in distributed
electronic environments. In this paper, the impact of reputation on
decision-making under scarcity of information will be shown. First, a cognitive
theory of reputation will be presented, then a selection of simulation
experimental results from different studies will be discussed. Such results
concern the benefits of reputation when agents need to find out good sellers in
a virtual market-place under uncertainty and informational cheating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5112</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5112</id><created>2011-06-25</created><authors><author><keyname>Kursa</keyname><forenames>Miron B.</forenames></author><author><keyname>Rudnicki</keyname><forenames>Witold R.</forenames></author></authors><title>The All Relevant Feature Selection using Random Forest</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the application of the random forest classifier for
the all relevant feature selection problem. To this end we first examine two
recently proposed all relevant feature selection algorithms, both being a
random forest wrappers, on a series of synthetic data sets with varying size.
We show that reasonable accuracy of predictions can be achieved and that
heuristic algorithms that were designed to handle the all relevant problem,
have performance that is close to that of the reference ideal algorithm. Then,
we apply one of the algorithms to four families of semi-synthetic data sets to
assess how the properties of particular data set influence results of feature
selection. Finally we test the procedure using a well-known gene expression
data set. The relevance of nearly all previously established important genes
was confirmed, moreover the relevance of several new ones is discovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5113</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5113</id><created>2011-06-25</created><authors><author><keyname>Tassa</keyname><forenames>Tamir</forenames></author></authors><title>Secure Mining of Association Rules in Horizontally Distributed Databases</title><categories>cs.DB cs.CR cs.DC</categories><acm-class>H.2.4; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a protocol for secure mining of association rules in horizontally
distributed databases. The current leading protocol is that of Kantarcioglu and
Clifton (TKDE 2004). Our protocol, like theirs, is based on the Fast
Distributed Mining (FDM) algorithm of Cheung et al. (PDIS 1996), which is an
unsecured distributed version of the Apriori algorithm. The main ingredients in
our protocol are two novel secure multi-party algorithms --- one that computes
the union of private subsets that each of the interacting players hold, and
another that tests the inclusion of an element held by one player in a subset
held by another. Our protocol offers enhanced privacy with respect to the
protocol of Kantarcioglu and Clifton. In addition, it is simpler and is
significantly more efficient in terms of communication rounds, communication
cost and computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5122</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5122</id><created>2011-06-25</created><authors><author><keyname>Chertov</keyname><forenames>Oleg</forenames></author><author><keyname>Aleksandrova</keyname><forenames>Marharyta</forenames></author></authors><title>Clustering with Prototype Extraction for Census Data Analysis</title><categories>cs.DB cs.SI physics.soc-ph</categories><comments>World Conference on Soft Computing WConSC-2011 (San Francisco State
  University, California, USA), 8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not long ago primary census data became available to publicity. It opened
qualitatively new perspectives not only for researchers in demography and
sociology, but also for those people, who somehow face processes occurring in
society. In this paper authors propose using Data Mining methods for searching
hidden patterns in census data. A novel clustering-based technique is described
as well. It allows determining factors which influence people behavior, in
particular decision-making process (as an example, a decision whether to have a
baby or not). Proposed technique is based on clustering a set of respondents,
for whom a certain event have already happened (for instance, a baby was born),
and discovering clusters' prototypes from a set of respondents, for whom this
event hasn't occurred yet. By means of analyzing clusters' and their
prototypes' characteristics it is possible to identify which factors influence
the decision-making process. Authors also provide an experimental example of
the described approach usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5128</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5128</id><created>2011-06-25</created><updated>2011-09-01</updated><authors><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Rathke</keyname><forenames>Julian</forenames><affiliation>Southampton University</affiliation></author><author><keyname>Sassone</keyname><forenames>Vladimiro</forenames><affiliation>Southampton University</affiliation></author></authors><title>Permission-Based Separation Logic for Message-Passing Concurrency</title><categories>cs.LO</categories><comments>47 pages</comments><proxy>LMCS</proxy><acm-class>F.3.1, F.3.2, F.3.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  1, 2011) lmcs:772</journal-ref><doi>10.2168/LMCS-7(3:7)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop local reasoning techniques for message passing concurrent programs
based on ideas from separation logics and resource usage analysis. We extend
processes with permission- resources and define a reduction semantics for this
extended language. This provides a foundation for interpreting separation
formulas for message-passing concurrency. We also define a sound proof system
permitting us to infer satisfaction compositionally using local,
separation-based reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5130</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5130</id><created>2011-06-25</created><updated>2013-02-27</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Stanojevi&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>&#x160;enk</keyname><forenames>Vojin</forenames></author></authors><title>Some Properties of R\'{e}nyi Entropy over Countably Infinite Alphabets</title><categories>cs.IT math.IT</categories><comments>13 pages (single-column)</comments><msc-class>94A17</msc-class><acm-class>H.1.1</acm-class><journal-ref>Probl. Inf. Transm., vol. 49, no. 2, pp. 99-110, Apr. 2013</journal-ref><doi>10.1134/S0032946013020014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study certain properties of R\'{e}nyi entropy functionals
$H_\alpha(\mathcal{P})$ on the space of probability distributions over
$\mathbb{Z}_+$. Primarily, continuity and convergence issues are addressed.
Some properties shown parallel those known in the finite alphabet case, while
others illustrate a quite different behaviour of R\'enyi entropy in the
infinite case. In particular, it is shown that, for any distribution $\mathcal
P$ and any $r\in[0,\infty]$, there exists a sequence of distributions
$\mathcal{P}_n$ converging to $\mathcal{P}$ with respect to the total variation
distance, such that $\lim_{n\to\infty}\lim_{\alpha\to{1+}}
H_\alpha(\mathcal{P}_n) = \lim_{\alpha\to{1+}}\lim_{n\to\infty}
H_\alpha(\mathcal{P}_n) + r$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5149</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5149</id><created>2011-06-25</created><updated>2011-11-16</updated><authors><author><keyname>Birkner</keyname><forenames>Peter</forenames></author><author><keyname>Longa</keyname><forenames>Patrick</forenames></author><author><keyname>Sica</keyname><forenames>Francesco</forenames></author></authors><title>Four-Dimensional Gallant-Lambert-Vanstone Scalar Multiplication</title><categories>cs.CR math.NT</categories><comments>23 pages, 3 figures. Changes from v3: corrected typo in proof of
  Lemma 5</comments><msc-class>11A05, 11R04</msc-class><acm-class>G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GLV method of Gallant, Lambert and Vanstone (CRYPTO 2001) computes any
multiple $kP$ of a point $P$ of prime order $n$ lying on an elliptic curve with
a low-degree endomorphism $\Phi$ (called GLV curve) over $\mathbb{F}_p$ as [kP
= k_1P + k_2\Phi(P), \quad\text{with} \max{|k_1|,|k_2|}\leq C_1\sqrt n] for
some explicit constant $C_1&gt;0$. Recently, Galbraith, Lin and Scott (EUROCRYPT
2009) extended this method to all curves over $\mathbb{F}_{p^2}$ which are
twists of curves defined over $\mathbb{F}_p$. We show in this work how to merge
the two approaches in order to get, for twists of any GLV curve over
$\mathbb{F}_{p^2}$, a four-dimensional decomposition together with fast
endomorphisms $\Phi, \Psi$ over $\mathbb{F}_{p^2}$ acting on the group
generated by a point $P$ of prime order $n$, resulting in a proved
decomposition for any scalar $k\in[1,n]$ $$ kP=k_1P+ k_2\Phi(P)+ k_3\Psi(P) +
k_4\Psi\Phi(P)\quad \text{with} \max_i (|k_i|)&lt; C_2\, n^{1/4} $$ for some
explicit $C_2&gt;0$. Furthermore, taking the best $C_1, C_2$, we get
$C_2/C_1&lt;408$, independently of the curve, ensuring a constant relative
speedup. We also derive new families of GLV curves, corresponding to those
curves with degree 3 endomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5150</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5150</id><created>2011-06-25</created><updated>2011-08-26</updated><authors><author><keyname>Del Genio</keyname><forenames>Charo I.</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author></authors><title>All scale-free networks are sparse</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>4 pages, 2 figures</comments><journal-ref>Phys. Rev. Lett. 107, 178701 (2011)</journal-ref><doi>10.1103/PhysRevLett.107.178701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the realizability of scale free-networks with a given degree
sequence, showing that the fraction of realizable sequences undergoes two
first-order transitions at the values 0 and 2 of the power-law exponent. We
substantiate this finding by analytical reasoning and by a numerical method,
proposed here, based on extreme value arguments, which can be applied to any
given degree distribution. Our results reveal a fundamental reason why large
scale-free networks without constraints on minimum and maximum degree must be
sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5156</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5156</id><created>2011-06-25</created><updated>2011-07-04</updated><authors><author><keyname>Dhandra</keyname><forenames>B. V.</forenames></author><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author></authors><title>Morphological Reconstruction for Word Level Script Identification</title><categories>cs.CV</categories><comments>11 Pages, 8 Figures,5 Tables; Revised: 15-06-2007,Published:
  30-06-2007</comments><acm-class>I.4.1</acm-class><journal-ref>International Journal of Computer Science and Security
  (IJCSS),Volume (1) : Issue (1), 41 - 51, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A line of a bilingual document page may contain text words in regional
language and numerals in English. For Optical Character Recognition (OCR) of
such a document page, it is necessary to identify different script forms before
running an individual OCR system. In this paper, we have identified a tool of
morphological opening by reconstruction of an image in different directions and
regional descriptors for script identification at word level, based on the
observation that every text has a distinct visual appearance. The proposed
system is developed for three Indian major bilingual documents, Kannada, Telugu
and Devnagari containing English numerals. The nearest neighbour and k-nearest
neighbour algorithms are applied to classify new word images. The proposed
algorithm is tested on 2625 words with various font styles and sizes. The
results obtained are quite encouraging
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5158</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5158</id><created>2011-06-25</created><authors><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Stratan</keyname><forenames>Corina</forenames></author></authors><title>MONARC Simulation Framework</title><categories>cs.DC</categories><comments>Buletinul Stiintific al Universitatii &quot;Politehnica&quot; din Timisoara,
  Romania, Seria Automatica si Calculatoare, Periodica Politechnica,
  Transactions on Automatic Control and Computer Science Vol.49 (63), 2004,
  ISSN 1224-600X Special issue on 3rd Edition of RoEduNet International
  Conference, Timisoara, Romania, 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the latest generation of the MONARC (MOdels of Networked
Analysis at Regional Centers) simulation framework, as a design and modelling
tool for large scale distributed systems applied to HEP experiments. A
process-oriented approach for discrete event simulation is well-suited for
describing concurrent running programs, as well as the stochastic arrival
patterns that characterize how such systems are used. The simulation engine is
based on Threaded Objects (or Active Objects), which offer great flexibility in
simulating the complex behavior of distributed data processing programs. The
engine provides an appropriate scheduling mechanism for the Active objects with
support for interrupts. This approach offers a natural way of describing
complex running programs that are data dependent and which concurrently compete
for shared resources as well as large numbers of concurrent data transfers on
shared resources. The framework provides a complete set of basic components
(processing nodes, data servers, network components) together with dynamically
loadable decision units (scheduling or data replication modules) for easily
building complex Computing Model simulations. Examples of simulating complex
data processing systems are presented, and the way the framework is used to
compare different decision making algorithms or to optimize the overall Grid
architecture and/or the policies that govern the Grid's use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5161</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5161</id><created>2011-06-25</created><authors><author><keyname>Legrand</keyname><forenames>Iosif</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Voicu</keyname><forenames>Ramiro</forenames></author><author><keyname>Stratan</keyname><forenames>Corina</forenames></author><author><keyname>Cirstoiu</keyname><forenames>Catalin</forenames></author><author><keyname>Musat</keyname><forenames>Lucian</forenames></author></authors><title>A Simulation Study for T0/T1 Data Replication and Production Activities</title><categories>cs.DC hep-ex</categories><comments>in Proc. of 15th International Conference on Control Systems and
  Computer Science (CSCS-15), pp. 131-135, 2005, Bucharest, Romania, Ed.
  Politehnica Press, ISBN: 973-8449-89-8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the latest generation of the MONARC (MOdels of Networked
Analysis at Regional Centers) simulation framework, as a design and modeling
tool for large scale distributed systems applied to HEP experiments. The
simulation of Grid architectures has a vital importance in the future
deployment of Grid systems for providing the users an appropriate feed-back. We
present here an example of simulating complex data processing systems and the
way the framework is used to optimize the overall Grid architecture and/or the
policies that govern the Grid's use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5168</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5168</id><created>2011-06-25</created><authors><author><keyname>Legrand</keyname><forenames>Iosif C.</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Voicu</keyname><forenames>Ramiro</forenames></author><author><keyname>Stratan</keyname><forenames>Corina</forenames></author><author><keyname>Cirstoiu</keyname><forenames>Catalin</forenames></author><author><keyname>Musat</keyname><forenames>Lucian</forenames></author></authors><title>LISA (Localhost Information Service Agent)</title><categories>cs.DC</categories><comments>Proc. of the 15th International Conference on Control Systems and
  Computer Science (CSCS-15), Bucharest, Romania, 2005, pp. 127-130, ISBN:
  973-8449-89-8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing has gained an increasing importance in the last years,
especially in the academic environments, offering the possibility to rapidly
solve complex scientific problems. The monitoring of the Grid jobs has a vital
importance for analyzing the system's performance, for providing the users an
appropriate feed-back, and for obtaining historical data which may be used for
performance prediction. Several monitoring systems have been developed, with
different strategies to collect and store the information. We shall present
here a solution based on MonALISA, a distributed service for monitoring,
control and global optimization of complex systems, and LISA, a component
application of MonALISA which can help in optimizing other applications by
means of monitoring services. The advantages of this system are, among others,
flexibility, dynamic configuration, high communication performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5170</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5170</id><created>2011-06-25</created><updated>2011-07-06</updated><authors><author><keyname>Lewko</keyname><forenames>Allison</forenames></author></authors><title>The Contest Between Simplicity and Efficiency in Asynchronous Byzantine
  Agreement</title><categories>cs.DC</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the wake of the decisive impossibility result of Fischer, Lynch, and
Paterson for deterministic consensus protocols in the aynchronous model with
just one failure, Ben-Or and Bracha demonstrated that the problem could be
solved with randomness, even for Byzantine failures. Both protocols are natural
and intuitive to verify, and Bracha's achieves optimal resilience. However, the
expected running time of these protocols is exponential in general. Recently,
Kapron, Kempe, King, Saia, and Sanwalani presented the first efficient
Byzantine agreement algorithm in the asynchronous, full information model,
running in polylogarithmic time. Their algorithm is Monte Carlo and drastically
departs from the simple structure of Ben-Or and Bracha's Las Vegas algorithms.
  In this paper, we begin an investigation of the question: to what extent is
this departure necessary? Might there be a much simpler and intuitive Las Vegas
protocol that runs in expected polynomial time? We will show that the
exponential running time of Ben-Or and Bracha's algorithms is no mere accident
of their specific details, but rather an unavoidable consequence of their
general symmetry and round structure. We define a natural class of &quot;fully
symmetric round protocols&quot; for solving Byzantine agreement in an asynchronous
setting and show that any such protocol can be forced to run in expected
exponential time by an adversary in the full information model. We assume the
adversary controls $t$ Byzantine processors for $t = cn$, where $c$ is an
arbitrary positive constant $&lt; 1/3$. We view our result as a step toward
identifying the level of complexity required for a polynomial-time algorithm in
this setting, and also as a guide in the search for new efficient algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5171</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5171</id><created>2011-06-25</created><authors><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Voicu</keyname><forenames>Ramiro</forenames></author><author><keyname>Muraru</keyname><forenames>Adrian</forenames></author><author><keyname>Legrand</keyname><forenames>Iosif C.</forenames></author></authors><title>A Distributed Agent Based System to Control and Coordinate Large Scale
  Data Transfers</title><categories>cs.DC</categories><comments>Proc. of 16th International Conference on Control Systems and
  Computer Science (CSCS-16), pp. 64-68, Bucharest, Romania, ISBN:
  978-973-718-741-3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed agent based system used to monitor, configure and
control complex, large scale data transfers in the Wide Area Network. The
Localhost Information Service Agent (LISA) is a lightweight dynamic service
that provides complete system and applications monitoring, is capable to
dynamically configure system parameters and can help in optimizing distributed
applications.
  As part of the MonALISA (Monitoring Agents in A Large Integrated Services
Architecture) system, LISA is an end host agent capable to collect any type of
monitoring information, to distribute them, and to take actions based on local
or global decision units. The system has been used for the Bandwidth Challenge
at Supercomputing 2006 to coordinate global large scale data transfers using
Fast Data Transfer (FDT) application between hundreds of servers distributed on
major Grid sites involved in processing High Energy Physics data for the future
Large Hadron Collider experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5174</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5174</id><created>2011-06-25</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author><author><keyname>Duong</keyname><forenames>Vy</forenames></author></authors><title>A Game Theoretical Approach to Broadcast Information Diffusion in Social
  Networks</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>6 pages, 3 figures, presented at ANSS-2011 (Boston, MA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major function of social networks (e.g., massive online social networks)
is the dissemination of information, such as scientific knowledge, news, and
rumors. Information can be propagated by the users of the network via natural
connections in written, oral or electronic form. The information passing from a
sender to receivers and back (in the form of comments) involves all of the
actors considering their knowledge, trust, and popularity, which shape their
publishing and commenting strategies. To understand such human aspects of the
information dissemination, we propose a game theoretical model of a one-way
information forwarding and feedback mechanism in a star-shaped social network
that takes into account the personalities of the communicating actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5177</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5177</id><created>2011-06-25</created><authors><author><keyname>Fannjiang</keyname><forenames>A.</forenames></author><author><keyname>Liao</keyname><forenames>W.</forenames></author></authors><title>Coherence-Pattern Guided Compressive Sensing with Unresolved Grids</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly coherent sensing matrices arise in discretization of continuum imaging
problems such as radar and medical imaging when the grid spacing is below the
Rayleigh threshold.
  Algorithms based on techniques of band exclusion (BE) and local optimization
(LO) are proposed to deal with such coherent sensing matrices. These techniques
are embedded in the existing compressed sensing algorithms such as Orthogonal
Matching Pursuit (OMP), Subspace Pursuit (SP), Iterative Hard Thresholding
(IHT), Basis Pursuit (BP) and Lasso, and result in the modified algorithms
BLOOMP, BLOSP, BLOIHT, BP-BLOT and Lasso-BLOT, respectively.
  Under appropriate conditions, it is proved that BLOOMP can reconstruct
sparse, widely separated objects up to one Rayleigh length in the Bottleneck
distance {\em independent} of the grid spacing. One of the most distinguishing
attributes of BLOOMP is its capability of dealing with large dynamic ranges.
  The BLO-based algorithms are systematically tested with respect to four
performance metrics: dynamic range, noise stability, sparsity and resolution.
With respect to dynamic range and noise stability, BLOOMP is the best
performer. With respect to sparsity, BLOOMP is the best performer for high
dynamic range while for dynamic range near unity BP-BLOT and Lasso-BLOT with
the optimized regularization parameter have the best performance. In the
noiseless case, BP-BLOT has the highest resolving power up to certain dynamic
range.
  The algorithms BLOSP and BLOIHT are good alternatives to
  BLOOMP and BP/Lasso-BLOT: they are faster than both BLOOMP and BP/Lasso-BLOT
and shares, to a lesser degree, BLOOMP's amazing attribute with respect to
dynamic range.
  Detailed comparisons with existing algorithms such as Spectral Iterative Hard
Thresholding (SIHT) and the frame-adapted BP are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5178</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5178</id><created>2011-06-25</created><authors><author><keyname>Haslhofer</keyname><forenames>Bernhard</forenames></author><author><keyname>Simon</keyname><forenames>Rainer</forenames></author><author><keyname>Sanderson</keyname><forenames>Robert</forenames></author><author><keyname>van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>The Open Annotation Collaboration (OAC) Model</title><categories>cs.DL</categories><comments>5 pages, 3 figures</comments><acm-class>H.3.7</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Annotations allow users to associate additional information with existing
resources. Using proprietary and closed systems on the Web, users are already
able to annotate multimedia resources such as images, audio and video. So far,
however, this information is almost always kept locked up and inaccessible to
the Web of Data. We believe that an important step to take is the integration
of multimedia annotations and the Linked Data principles. This should allow
clients to easily publish and consume, thus exchange annotations about
resources via common Web standards. We first present the current status of the
Open Annotation Collaboration, an international initiative that is currently
working on annotation interoperability specifications based on best practices
from the Linked Data effort. Then we present two use cases and early prototypes
that make use of the proposed annotation model and present lessons learned and
discuss yet open technical issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5186</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5186</id><created>2011-06-25</created><authors><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Caban</keyname><forenames>Jesus</forenames></author><author><keyname>Suffredini</keyname><forenames>Anthony F.</forenames></author><author><keyname>Palmore</keyname><forenames>Tara N.</forenames></author><author><keyname>Mollura</keyname><forenames>Daniel J.</forenames></author></authors><title>Learning Shape and Texture Characteristics of CT Tree-in-Bud Opacities
  for CAD Systems</title><categories>cs.CV</categories><comments>7 pages, 4 figures. Published in Proc. of Medical Image Computing and
  Computer Assisted Interventions (MICCAI), 2011</comments><report-no>NIH-CIDI-MICCAI2011</report-no><msc-class>68</msc-class><acm-class>I.4; I.5</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Although radiologists can employ CAD systems to characterize malignancies,
pulmonary fibrosis and other chronic diseases; the design of imaging techniques
to quantify infectious diseases continue to lag behind. There exists a need to
create more CAD systems capable of detecting and quantifying characteristic
patterns often seen in respiratory tract infections such as influenza,
bacterial pneumonia, or tuborculosis. One of such patterns is Tree-in-bud (TIB)
which presents \textit{thickened} bronchial structures surrounding by clusters
of \textit{micro-nodules}. Automatic detection of TIB patterns is a challenging
task because of their weak boundary, noisy appearance, and small lesion size.
In this paper, we present two novel methods for automatically detecting TIB
patterns: (1) a fast localization of candidate patterns using information from
local scale of the images, and (2) a M\&quot;{o}bius invariant feature extraction
method based on learned local shape and texture properties. A comparative
evaluation of the proposed methods is presented with a dataset of 39 laboratory
confirmed viral bronchiolitis human parainfluenza (HPIV) CTs and 21 normal lung
CTs. Experimental results demonstrate that the proposed CAD system can achieve
high detection rate with an overall accuracy of 90.96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5204</identifier>
 <datestamp>2011-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5204</id><created>2011-06-26</created><updated>2011-08-09</updated><authors><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Avoiding Three Consecutive Blocks of the Same Size and Same Sum</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists an infinite word over the alphabet {0, 1, 3, 4}
containing no three consecutive blocks of the same size and the same sum. This
answers an open problem of Pirillo and Varricchio from 1994.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5213</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5213</id><created>2011-06-26</created><authors><author><keyname>Clements</keyname><forenames>Maarten</forenames></author><author><keyname>Serdyukov</keyname><forenames>Pavel</forenames></author><author><keyname>de Vries</keyname><forenames>Arjen P.</forenames></author><author><keyname>Reinders</keyname><forenames>Marcel J. T.</forenames></author></authors><title>Personalised Travel Recommendation based on Location Co-occurrence</title><categories>cs.IR</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><acm-class>H.3.5; H.3.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new task of recommending touristic locations based on a user's
visiting history in a geographically remote region. This can be used to plan a
touristic visit to a new city or country, or by travel agencies to provide
personalised travel deals.
  A set of geotags is used to compute a location similarity model between two
different regions. The similarity between two landmarks is derived from the
number of users that have visited both places, using a Gaussian density
estimation of the co-occurrence space of location visits to cluster related
geotags. The standard deviation of the kernel can be used as a scale parameter
that determines the size of the recommended landmarks.
  A personalised recommendation based on the location similarity model is
evaluated on city and country scale and is able to outperform a location
ranking based on popularity. Especially when a tourist filter based on visit
duration is enforced, the prediction can be accurately adapted to the
preference of the user. An extensive evaluation based on manual annotations
shows that more strict ranking methods like cosine similarity and a proposed
RankDiff algorithm provide more serendipitous recommendations and are able to
link similar locations on opposite sides of the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5223</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5223</id><created>2011-06-26</created><updated>2013-07-05</updated><authors><author><keyname>Lin</keyname><forenames>Tianrong</forenames></author></authors><title>Some results on equivalence of multi-letter quantum finite automata</title><categories>cs.CC cs.FL</categories><comments>This paper has been withdrawn by the author due to a more general
  result having been obtained in arXiv:1203.0113</comments><msc-class>94A25, 94A30, 02F10</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Two quantum finite automata are equivalent if for all input string $\omega$
over the input alphabet the two automata accept $\omega$ with equal
probability. In [Theoret. Comput. Sci. 410 (2009) 3006-3017], it was shown that
a $k_1$-letter QFA $\mathcal{A}_1$ and a $k_2$-letter QFA $\mathcal{A}_2$ over
$\Sigma=\{\sigma\}$, are equivalent if and only if they are
$(n_1+n_2)^4+k-1$-equivalent where $n_i$ is the number of states of
$\mathcal{A}_i$, $i=1,2$, and $k=\max\{k_1,k_2\}$. In this letter, we improve
the above upper-bound to $(n_1^2+n_2^2-1)+k$. This also answers an open problem
of Qiu et al. [Acta Informatica 48 (2011) 271-290]. Further, we show that, in
the case of $\Sigma=\{\sigma_1,...,\sigma_t\}$ with $2\leq t&lt;\infty$, there
exists an integer $z$ such that $\mathcal{A}_1$ and $\mathcal{A}_2$ are
equivalent if and only if they satisfy $z$-equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5230</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5230</id><created>2011-06-26</created><authors><author><keyname>Javan</keyname><forenames>Mohammad R.</forenames></author><author><keyname>Sharafat</keyname><forenames>Ahmad R.</forenames></author></authors><title>Opportunistic Power Control for Multi-Carrier Interference Channels</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a new method for opportunistic power control in multi-carrier
interference channels for delay-tolerant data services. In doing so, we utilize
a game theoretic framework with novel constraints, where each user tries to
maximize its utility in a distributed and opportunistic manner, while
satisfying the game's constraints by adapting its transmit power to its
channel. In this scheme, users transmit with more power on good sub-channels
and do the opposite on bad sub-channels. In this way, in addition to the
allocated power on each sub-channel, the total power of all users also depends
on channel conditions. Since each user's power level depends on power levels of
other users, the game belongs to the \emph{generalized} Nash equilibrium (GNE)
problems, which in general, is hard to analyze. We show that the proposed game
has a GNE, and derive the sufficient conditions for its uniqueness. Besides, we
propose a new pricing scheme for maximizing each user's throughput in an
opportunistic manner under its total power constraint; and provide the
sufficient conditions for the algorithm's convergence and its GNE's uniqueness.
Simulations confirm that our proposed scheme yields a higher throughput for
each user and/or has a significantly improved efficiency as compared to other
existing opportunistic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5236</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5236</id><created>2011-06-26</created><authors><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>Morales</keyname><forenames>Jean</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author></authors><title>A General Framework for Structured Sparsity via Proximal Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generalized framework for structured sparsity. It extends the
well-known methods of Lasso and Group Lasso by incorporating additional
constraints on the variables as part of a convex optimization problem. This
framework provides a straightforward way of favouring prescribed sparsity
patterns, such as orderings, contiguous regions and overlapping groups, among
others. Existing optimization methods are limited to specific constraint sets
and tend to not scale well with sample size and dimensionality. We propose a
novel first order proximal method, which builds upon results on fixed points
and successive approximations. The algorithm can be applied to a general class
of conic and norm constraints sets and relies on a proximity operator
subproblem which can be computed explicitly. Experiments on different
regression problems demonstrate the efficiency of the optimization algorithm
and its scalability with the size of the problem. They also demonstrate state
of the art statistical performance, which improves over Lasso and StructOMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5249</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5249</id><created>2011-06-26</created><updated>2012-01-11</updated><authors><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>The strength of strong ties in scientific collaboration networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 Pages, 6 Figures, Published version, Minor changes, Results also
  verified using new weight-scheme</comments><journal-ref>Europhys. Lett. 97, 18007 (2012)</journal-ref><doi>10.1209/0295-5075/97/18007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network topology and its relationship to tie strengths may hinder or enhance
the spreading of information in social networks. We study the correlations
between tie strengths and topology in networks of scientific collaboration, and
show that these are very different from ordinary social networks. For the
latter, it has earlier been shown that strong ties are associated with dense
network neighborhoods, while weaker ties act as bridges between these. Because
of this, weak links act as bottlenecks for the diffusion of information. We
show that on the contrary, in co-authorship networks dense local neighborhoods
mainly consist of weak links, whereas strong links are more important for
overall connectivity. The important role of strong links is further highlighted
in simulations of information spreading, where their topological position is
seen to dramatically speed up spreading dynamics. Thus, in contrast to ordinary
social networks, weight-topology correlations enhance the flow of information
across scientific collaboration networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5253</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5253</id><created>2011-06-26</created><authors><author><keyname>Nosrat-Makouei</keyname><forenames>Behrang</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>User Arrival in MIMO Interference Alignment Networks</title><categories>cs.IT math.IT</categories><comments>17 pages, 6 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, vol.11, no.2, pp.
  842-851, February 2012</journal-ref><doi>10.1109/TWC.2011.120511.111088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a constant multiple-input multiple-output
interference channel where a set of active users are cooperating through
interference alignment while a set of secondary users desire access to the
channel. We derive the minimum number of secondary transmit antennas required
so that a secondary user can use the channel without affecting the sum rate of
the active users, under a zero-forcing equalization assumption. When the
secondary users have enough antennas, we derive several secondary user
precoders that approximately maximize the secondary users' sum rate without
changing the sum rate of the active users. When the secondary users do not have
enough antennas, we perform numerical optimization to find secondary user
precoders that cause minimum degradation to the sum rate of the active users.
Through simulations, we confirm that i) with enough antennas at the secondary
users, gains equivalent to the case of all the users cooperating through
interference alignment is obtainable, and ii) when the secondary users do not
have enough antennas, large rate losses at the active users can be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5256</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5256</id><created>2011-06-26</created><authors><author><keyname>Brafman</keyname><forenames>R. I.</forenames></author><author><keyname>Domshlak</keyname><forenames>C.</forenames></author></authors><title>Structure and Complexity in Planning with Unary Operators</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  315-349, 2003</journal-ref><doi>10.1613/jair.1146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unary operator domains -- i.e., domains in which operators have a single
effect -- arise naturally in many control problems. In its most general form,
the problem of STRIPS planning in unary operator domains is known to be as hard
as the general STRIPS planning problem -- both are PSPACE-complete. However,
unary operator domains induce a natural structure, called the domain's causal
graph. This graph relates between the preconditions and effect of each domain
operator. Causal graphs were exploited by Williams and Nayak in order to
analyze plan generation for one of the controllers in NASA's Deep-Space One
spacecraft. There, they utilized the fact that when this graph is acyclic, a
serialization ordering over any subgoal can be obtained quickly. In this paper
we conduct a comprehensive study of the relationship between the structure of a
domain's causal graph and the complexity of planning in this domain. On the
positive side, we show that a non-trivial polynomial time plan generation
algorithm exists for domains whose causal graph induces a polytree with a
constant bound on its node indegree. On the negative side, we show that even
plan existence is hard when the graph is a directed-path singly connected DAG.
More generally, we show that the number of paths in the causal graph is closely
related to the complexity of planning in the associated domain. Finally we
relate our results to the question of complexity of planning with serializable
subgoals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5257</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5257</id><created>2011-06-26</created><authors><author><keyname>Eiter</keyname><forenames>T.</forenames></author><author><keyname>Faber</keyname><forenames>W.</forenames></author><author><keyname>Leone</keyname><forenames>N.</forenames></author><author><keyname>Pfeifer</keyname><forenames>G.</forenames></author><author><keyname>Polleres</keyname><forenames>A.</forenames></author></authors><title>Answer Set Planning Under Action Costs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  25-71, 2003</journal-ref><doi>10.1613/jair.1148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, planning based on answer set programming has been proposed as an
approach towards realizing declarative planning systems. In this paper, we
present the language Kc, which extends the declarative planning language K by
action costs. Kc provides the notion of admissible and optimal plans, which are
plans whose overall action costs are within a given limit resp. minimum over
all plans (i.e., cheapest plans). As we demonstrate, this novel language allows
for expressing some nontrivial planning tasks in a declarative way.
Furthermore, it can be utilized for representing planning problems under other
optimality criteria, such as computing ``shortest'' plans (with the least
number of steps), and refinement combinations of cheapest and fastest plans. We
study complexity aspects of the language Kc and provide a transformation to
logic programs, such that planning problems are solved via answer set
programming. Furthermore, we report experimental results on selected problems.
Our experience is encouraging that answer set planning may be a valuable
approach to expressive planning systems in which intricate planning problems
can be naturally specified and solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5258</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5258</id><created>2011-06-26</created><authors><author><keyname>Brafman</keyname><forenames>R. I.</forenames></author><author><keyname>Tennenholtz</keyname><forenames>M.</forenames></author></authors><title>Learning to Coordinate Efficiently: A Model-based Approach</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  11-23, 2003</journal-ref><doi>10.1613/jair.1154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In common-interest stochastic games all players receive an identical payoff.
Players participating in such games must learn to coordinate with each other in
order to receive the highest-possible value. A number of reinforcement learning
algorithms have been proposed for this problem, and some have been shown to
converge to good solutions in the limit. In this paper we show that using very
simple model-based algorithms, much better (i.e., polynomial) convergence rates
can be attained. Moreover, our model-based algorithms are guaranteed to
converge to the optimal value, unlike many of the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5260</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5260</id><created>2011-06-26</created><authors><author><keyname>Do</keyname><forenames>M.</forenames></author><author><keyname>Kambhampati</keyname><forenames>S.</forenames></author></authors><title>SAPA: A Multi-objective Metric Temporal Planner</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  155-194, 2003</journal-ref><doi>10.1613/jair.1156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SAPA is a domain-independent heuristic forward chaining planner that can
handle durative actions, metric resource constraints, and deadline goals. It is
designed to be capable of handling the multi-objective nature of metric
temporal planning. Our technical contributions include (i) planning-graph based
methods for deriving heuristics that are sensitive to both cost and makespan
(ii) techniques for adjusting the heuristic estimates to take action
interactions and metric resource limitations into account and (iii) a linear
time greedy post-processing technique to improve execution flexibility of the
solution plans. An implementation of SAPA using many of the techniques
presented in this paper was one of the best domain independent planners for
domains with metric and temporal constraints in the third International
Planning Competition, held at AIPS-02. We describe the technical details of
extracting the heuristics and present an empirical evaluation of the current
implementation of SAPA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5261</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5261</id><created>2011-06-26</created><authors><author><keyname>Patel-Schneider</keyname><forenames>P. F.</forenames></author><author><keyname>Sebastiani</keyname><forenames>R.</forenames></author></authors><title>A New General Method to Generate Random Modal Formulae for Testing
  Decision Procedures</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  351-389, 2003</journal-ref><doi>10.1613/jair.1166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent emergence of heavily-optimized modal decision procedures has
highlighted the key role of empirical testing in this domain. Unfortunately,
the introduction of extensive empirical tests for modal logics is recent, and
so far none of the proposed test generators is very satisfactory. To cope with
this fact, we present a new random generation method that provides benefits
over previous methods for generating empirical tests. It fixes and much
generalizes one of the best-known methods, the random CNF_[]m test, allowing
for generating a much wider variety of problems, covering in principle the
whole input space. Our new method produces much more suitable test sets for the
current generation of modal decision procedures. We analyze the features of the
new method by means of an extensive collection of empirical tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5262</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5262</id><created>2011-06-26</created><authors><author><keyname>Kambhampati</keyname><forenames>S.</forenames></author><author><keyname>Sanchez</keyname><forenames>R.</forenames></author></authors><title>AltAltp: Online Parallelization of Plans with Heuristic State Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  631-657, 2003</journal-ref><doi>10.1613/jair.1168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their near dominance, heuristic state search planners still lag
behind disjunctive planners in the generation of parallel plans in classical
planning. The reason is that directly searching for parallel solutions in state
space planners would require the planners to branch on all possible subsets of
parallel actions, thus increasing the branching factor exponentially. We
present a variant of our heuristic state search planner AltAlt, called AltAltp
which generates parallel plans by using greedy online parallelization of
partial plans. The greedy approach is significantly informed by the use of
novel distance heuristics that AltAltp derives from a graphplan-style planning
graph for the problem. While this approach is not guaranteed to provide optimal
parallel plans, empirical results show that AltAltp is capable of generating
good quality parallel plans at a fraction of the cost incurred by the
disjunctive planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5263</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5263</id><created>2011-06-26</created><authors><author><keyname>Zanuttini</keyname><forenames>B.</forenames></author></authors><title>New Polynomial Classes for Logic-Based Abduction</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  1-10, 2003</journal-ref><doi>10.1613/jair.1170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of propositional logic-based abduction, i.e., the
problem of searching for a best explanation for a given propositional
observation according to a given propositional knowledge base. We give a
general algorithm, based on the notion of projection; then we study
restrictions over the representations of the knowledge base and of the query,
and find new polynomial classes of abduction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5264</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5264</id><created>2011-06-26</created><authors><author><keyname>Reiter</keyname><forenames>E.</forenames></author><author><keyname>Robertson</keyname><forenames>R.</forenames></author><author><keyname>Sripada</keyname><forenames>S. G.</forenames></author></authors><title>Acquiring Correct Knowledge for Natural Language Generation</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  491-516, 2003</journal-ref><doi>10.1613/jair.1176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language generation (NLG) systems are computer software systems that
produce texts in English and other human languages, often from non-linguistic
input data. NLG systems, like most AI systems, need substantial amounts of
knowledge. However, our experience in two NLG projects suggests that it is
difficult to acquire correct knowledge for NLG systems; indeed, every knowledge
acquisition (KA) technique we tried had significant problems. In general terms,
these problems were due to the complexity, novelty, and poorly understood
nature of the tasks our systems attempted, and were worsened by the fact that
people write so differently. This meant in particular that corpus-based KA
approaches suffered because it was impossible to assemble a sizable corpus of
high-quality consistent manually written texts in our domains; and structured
expert-oriented KA techniques suffered because experts disagreed and because we
could not get enough information about special and unusual cases to build
robust systems. We believe that such problems are likely to affect many other
NLG systems as well. In the long term, we hope that new KA techniques may
emerge to help NLG system builders. In the shorter term, we believe that
understanding how individual KA techniques can fail, and using a mixture of
different KA techniques with different strengths and weaknesses, can help
developers acquire NLG knowledge that is mostly correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5265</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5265</id><created>2011-06-26</created><authors><author><keyname>Gerevini</keyname><forenames>A.</forenames></author><author><keyname>Saetti</keyname><forenames>A.</forenames></author><author><keyname>Serina</keyname><forenames>I.</forenames></author></authors><title>Planning Through Stochastic Local Search and Temporal Action Graphs in
  LPG</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  239-290, 2003</journal-ref><doi>10.1613/jair.1183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some techniques for planning in domains specified with the recent
standard language PDDL2.1, supporting 'durative actions' and numerical
quantities. These techniques are implemented in LPG, a domain-independent
planner that took part in the 3rd International Planning Competition (IPC). LPG
is an incremental, any time system producing multi-criteria quality plans. The
core of the system is based on a stochastic local search method and on a
graph-based representation called 'Temporal Action Graphs' (TA-graphs). This
paper focuses on temporal planning, introducing TA-graphs and proposing some
techniques to guide the search in LPG using this representation. The
experimental results of the 3rd IPC, as well as further results presented in
this paper, show that our techniques can be very effective. Often LPG
outperforms all other fully-automated planners of the 3rd IPC in terms of speed
to derive a solution, or quality of the solutions that can be produced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5266</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5266</id><created>2011-06-26</created><authors><author><keyname>Kvarnstr&#xf6;m</keyname><forenames>J.</forenames></author><author><keyname>Magnusson</keyname><forenames>M.</forenames></author></authors><title>TALplanner in IPC-2002: Extensions and Control Rules</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  343-377, 2003</journal-ref><doi>10.1613/jair.1189</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TALplanner is a forward-chaining planner that relies on domain knowledge in
the shape of temporal logic formulas in order to prune irrelevant parts of the
search space. TALplanner recently participated in the third International
Planning Competition, which had a clear emphasis on increasing the complexity
of the problem domains being used as benchmark tests and the expressivity
required to represent these domains in a planning system. Like many other
planners, TALplanner had support for some but not all aspects of this increase
in expressivity, and a number of changes to the planner were required. After a
short introduction to TALplanner, this article describes some of the changes
that were made before and during the competition. We also describe the process
of introducing suitable domain knowledge for several of the competition
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5267</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5267</id><created>2011-06-26</created><authors><author><keyname>Wiewiora</keyname><forenames>E.</forenames></author></authors><title>Potential-Based Shaping and Q-Value Initialization are Equivalent</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  205-208, 2003</journal-ref><doi>10.1613/jair.1190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shaping has proven to be a powerful but precarious means of improving
reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the
potential-based shaping algorithm for adding shaping rewards in a way that
guarantees the learner will learn optimal behavior. In this note, we prove
certain similarities between this shaping algorithm and the initialization step
required for several reinforcement learning algorithms. More specifically, we
prove that a reinforcement learner with initial Q-values based on the shaping
algorithm's potential function make the same updates throughout learning as a
learner receiving potential-based shaping rewards. We further prove that under
a broad category of policies, the behavior of these two learners are
indistinguishable. The comparison provides intuition on the theoretical
properties of the shaping algorithm as well as a suggestion for a simpler
method for capturing the algorithm's benefit. In addition, the equivalence
raises previously unaddressed issues concerning the efficiency of learning with
potential-based shaping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5268</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5268</id><created>2011-06-26</created><authors><author><keyname>Console</keyname><forenames>L.</forenames></author><author><keyname>Picardi</keyname><forenames>C.</forenames></author><author><keyname>Dupr&#xe8;</keyname><forenames>D. Theseider</forenames></author></authors><title>Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems
  On-Board</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  469-512, 2003</journal-ref><doi>10.1613/jair.1194</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic generation of decision trees based on off-line reasoning on
models of a domain is a reasonable compromise between the advantages of using a
model-based approach in technical domains and the constraints imposed by
embedded applications. In this paper we extend the approach to deal with
temporal information. We introduce a notion of temporal decision tree, which is
designed to make use of relevant information as long as it is acquired, and we
present an algorithm for compiling such trees from a model-based reasoning
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5269</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5269</id><created>2011-06-26</created><authors><author><keyname>Finkelstein</keyname><forenames>L.</forenames></author><author><keyname>Markovitch</keyname><forenames>S.</forenames></author><author><keyname>Rivlin</keyname><forenames>E.</forenames></author></authors><title>Optimal Schedules for Parallelizing Anytime Algorithms: The Case of
  Shared Resources</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  73-138, 2003</journal-ref><doi>10.1613/jair.1195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of anytime algorithms can be improved by simultaneously
solving several instances of algorithm-problem pairs. These pairs may include
different instances of a problem (such as starting from a different initial
state), different algorithms (if several alternatives exist), or several runs
of the same algorithm (for non-deterministic algorithms). In this paper we
present a methodology for designing an optimal scheduling policy based on the
statistical characteristics of the algorithms involved. We formally analyze the
case where the processes share resources (a single-processor model), and
provide an algorithm for optimal scheduling. We analyze, theoretically and
empirically, the behavior of our scheduling algorithm for various distribution
types. Finally, we present empirical results of applying our scheduling
algorithm to the Latin Square problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5270</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5270</id><created>2011-06-26</created><authors><author><keyname>Csirik</keyname><forenames>J. A.</forenames></author><author><keyname>Littman</keyname><forenames>M. L.</forenames></author><author><keyname>McAllester</keyname><forenames>D.</forenames></author><author><keyname>Schapire</keyname><forenames>R. E.</forenames></author><author><keyname>Stone</keyname><forenames>P.</forenames></author></authors><title>Decision-Theoretic Bidding Based on Learned Density Models in
  Simultaneous, Interacting Auctions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  209-242, 2003</journal-ref><doi>10.1613/jair.1200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auctions are becoming an increasingly popular method for transacting
business, especially over the Internet. This article presents a general
approach to building autonomous bidding agents to bid in multiple simultaneous
auctions for interacting goods. A core component of our approach learns a model
of the empirical price dynamics based on past data and uses the model to
analytically calculate, to the greatest extent possible, optimal bids. We
introduce a new and general boosting-based algorithm for conditional density
estimation problems of this kind, i.e., supervised learning problems in which
the goal is to estimate the entire conditional distribution of the real-valued
label. This approach is fully implemented as ATTac-2001, a top-scoring agent in
the second Trading Agent Competition (TAC-01). We present experiments
demonstrating the effectiveness of our boosting-based price predictor relative
to several reasonable alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5271</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5271</id><created>2011-06-26</created><authors><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author></authors><title>The Metric-FF Planning System: Translating &quot;Ignoring Delete Lists&quot; to
  Numeric State Variables</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  291-341, 2003</journal-ref><doi>10.1613/jair.1144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning with numeric state variables has been a challenge for many years,
and was a part of the 3rd International Planning Competition (IPC-3). Currently
one of the most popular and successful algorithmic techniques in STRIPS
planning is to guide search by a heuristic function, where the heuristic is
based on relaxing the planning task by ignoring the delete lists of the
available actions. We present a natural extension of ``ignoring delete lists''
to numeric state variables, preserving the relevant theoretical properties of
the STRIPS relaxation under the condition that the numeric task at hand is
``monotonic''. We then identify a subset of the numeric IPC-3 competition
language, ``linear tasks'', where monotonicity can be achieved by
pre-processing. Based on that, we extend the algorithms used in the heuristic
planning system FF to linear tasks. The resulting system Metric-FF is,
according to the IPC-3 results which we discuss, one of the two currently most
efficient numeric planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5273</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5273</id><created>2011-06-26</created><updated>2012-09-03</updated><authors><author><keyname>Yokota</keyname><forenames>R.</forenames></author><author><keyname>Barba</keyname><forenames>L. A.</forenames></author><author><keyname>Narumi</keyname><forenames>T.</forenames></author><author><keyname>Yasuoka</keyname><forenames>K.</forenames></author></authors><title>Petascale turbulence simulation using a highly parallel fast multipole
  method on GPUs</title><categories>cs.NA physics.comp-ph physics.flu-dyn</categories><msc-class>76F05</msc-class><acm-class>G.1.2; G.1.9</acm-class><doi>10.1016/j.cpc.2012.09.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports large-scale direct numerical simulations of
homogeneous-isotropic fluid turbulence, achieving sustained performance of 1.08
petaflop/s on gpu hardware using single precision. The simulations use a vortex
particle method to solve the Navier-Stokes equations, with a highly parallel
fast multipole method (FMM) as numerical engine, and match the current record
in mesh size for this application, a cube of 4096^3 computational points solved
with a spectral method. The standard numerical approach used in this field is
the pseudo-spectral method, relying on the FFT algorithm as numerical engine.
The particle-based simulations presented in this paper quantitatively match the
kinetic energy spectrum obtained with a pseudo-spectral method, using a trusted
code. In terms of parallel performance, weak scaling results show the fmm-based
vortex method achieving 74% parallel efficiency on 4096 processes (one gpu per
mpi process, 3 gpus per node of the TSUBAME-2.0 system). The FFT-based spectral
method is able to achieve just 14% parallel efficiency on the same number of
mpi processes (using only cpu cores), due to the all-to-all communication
pattern of the FFT algorithm. The calculation time for one time step was 108
seconds for the vortex method and 154 seconds for the spectral method, under
these conditions. Computing with 69 billion particles, this work exceeds by an
order of magnitude the largest vortex method calculations to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5294</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5294</id><created>2011-06-27</created><authors><author><keyname>Akama</keyname><forenames>Yohji</forenames></author></authors><title>Set systems: order types, continuous nondeterministic deformations, and
  quasi-orders</title><categories>cs.LO cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By reformulating a learning process of a set system L as a game between
Teacher and Learner, we define the order type of L to be the order type of the
game tree, if the tree is well-founded. The features of the order type of L
(dim L in symbol) are (1) We can represent any well-quasi-order (wqo for short)
by the set system L of the upper-closed sets of the wqo such that the maximal
order type of the wqo is equal to dim L. (2) dim L is an upper bound of the
mind-change complexity of L. dim L is defined iff L has a finite elasticity (fe
for short), where, according to computational learning theory, if an indexed
family of recursive languages has fe then it is learnable by an algorithm from
positive data. Regarding set systems as subspaces of Cantor spaces, we prove
that fe of set systems is preserved by any continuous function which is
monotone with respect to the set-inclusion. By it, we prove that finite
elasticity is preserved by various (nondeterministic) language operators
(Kleene-closure, shuffle-closure, union, product, intersection,. . ..) The
monotone continuous functions represent nondeterministic computations. If a
monotone continuous function has a computation tree with each node followed by
at most n immediate successors and the order type of a set system L is
{\alpha}, then the direct image of L is a set system of order type at most
n-adic diagonal Ramsey number of {\alpha}. Furthermore, we provide an
order-type-preserving contravariant embedding from the category of quasi-orders
and finitely branching simulations between them, into the complete category of
subspaces of Cantor spaces and monotone continuous functions having Girard's
linearity between them. Keyword: finite elasticity, shuffle-closure
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5299</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5299</id><created>2011-06-27</created><authors><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author></authors><title>DistHash: A robust P2P DHT-based system for replicated objects</title><categories>cs.DC</categories><comments>Proceedings of 17th International Conference on Control Systems and
  Computer Science (CSCS 17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp.
  453-460, ISSN: 2066-4451</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the Internet today, computing and communications environments are
significantly more complex and chaotic than classical distributed systems,
lacking any centralized organization or hierarchical control. There has been
much interest in emerging Peer-to-Peer (P2P) network overlays because they
provide a good substrate for creating large-scale data sharing, content
distribution and application-level multicast applications. In this paper we
present DistHash, a P2P overlay network designed to share large sets of
replicated distributed objects in the context of large-scale highly dynamic
infrastructures. We present original solutions to achieve optimal message
routing in hop-count and throughput, provide an adequate consistency approach
among replicas, as well as provide a fault-tolerant substrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5301</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5301</id><created>2011-06-27</created><authors><author><keyname>xu</keyname><forenames>Xiao-ke</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Small</keyname><forenames>Michael</forenames></author></authors><title>Optimizing and controlling functions of complex networks by manipulating
  rich-club connections</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 3 figures</comments><doi>10.1016/j.physa.2011.06.069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, there is no evidence suggesting that there are strong ties
between the rich-club property and the function of complex networks. In this
study, we find that whether a very small portion of rich nodes connected to
each other or not can strongly affect the frequency of occurrence of basic
building blocks (motif) within networks, and therefore the function, of a
heterogeneous network. Conversely whether a homogeneous network has a rich-club
property or not generally has no significant effect on its structure and
function. These findings open the possibility to optimize and control the
function of complex networks by manipulating rich-club connections.
Furthermore, based on the subgraph ratio profile, we develop a more rigorous
approach to judge whether a network has a rich-club or not. The new method does
not calculate how many links there are among rich nodes but depends on how the
links among rich nodes can affect the overall structure as well as function of
a given network. These results can also help us to understand the evolution of
dynamical networks and design new models for characterizing real-world
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5302</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5302</id><created>2011-06-27</created><authors><author><keyname>Tudor</keyname><forenames>Dacian</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author><author><keyname>Cretu</keyname><forenames>Vladimir</forenames></author></authors><title>Towards an IO intensive Grid application instrumentation in MedioGRID</title><categories>cs.DC</categories><journal-ref>Proceedings of the 16th International Conference on Control
  Systems and Computer Science (CSCS16'07), pp. 130-135, May 22-25, Bucharest,
  Romania, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining high performance in IO intensive applications requires systems that
support reliable fast transfer, data replication, and caching. In this paper we
present an architecture designed for supporting IO intensive applications in
MedioGRID, a system for real-time processing of satellite images, operating in
a Grid environment. The solution ensures that applications which are processing
geographical data have uniform access to data and is based on continuous
monitoring of the data transfers using MonALISA and its extensions. The
MedioGRID architecture is also built on Globus, Condor and PBS and based on
this middleware we aim to extract information about the running systems. The
results obtained in testing MedioGRID system for large data transfers show that
monitoring system provides a very good view of system evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5303</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5303</id><created>2011-06-27</created><authors><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author></authors><title>Intelligent strategies for DAG scheduling optimization in Grid
  environments</title><categories>cs.DC</categories><comments>Printech, ISBN 978-973-718-743-7</comments><journal-ref>Proceedings of the 16th International Conference on Control
  Systems and Computer Science (CSCS16'07), pp. 98-103, May 22-25, Bucharest,
  Romania, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a solution to the dynamic DAG scheduling problem in Grid
environments. It presents a distributed, scalable, efficient and fault-tolerant
algorithm for optimizing tasks assignment. The scheduler algorithm for tasks
with dependencies uses a heuristic model to optimize the total cost of tasks
execution. Also, a method based on genetic algorithms is proposed to optimize
the procedure of resources assignment. The experiments used the MonALISA
monitoring environment and its extensions. The results demonstrate very good
behavior in comparison with other scheduling approaches for this kind of DAG
scheduling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5304</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5304</id><created>2011-06-27</created><authors><author><keyname>Milescu</keyname><forenames>George</forenames></author><author><keyname>Noaje</keyname><forenames>Gabriel</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author></authors><title>OpenPh - Numerical Physics Library</title><categories>cs.DL physics.comp-ph</categories><comments>(ISSN 1223-7027)</comments><journal-ref>UPB Scientific Bulletin, Series A: Applied Mathematics and
  Physics, volume 68, number 1, pp: 73-78, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical physics has gained a lot of importance in the last decade, its
efficiency being motivated and sustained by the growth of computational power.
This paper presents a concept that is to be developed in the next few years:
OpenPh. OpenPh is a numerical physics library that makes use of the advantages
of both open source software and MATLAB programming. Its aim is to deliver the
instruments for providing numerical and graphical solutions for various physics
problems. It has a modular structure, allowing the user to add new modules to
the existing ones and to create its own modules according to its needs, being
virtually unlimited extendable. The modules of OpenPh are implemented using
MATLAB engine because it is the best solution used in engineering and science,
providing a wide range of optimized methods to accomplish even the toughest
jobs. Current version of OpenPh includes two modules, the first one providing
tools for quantum physics and the second one for mechanics. The quantum physics
module deals with the photoelectric effect, the radioactive decay of carbon-11,
and the Schr\&quot;odinger equation - particle in a box. The classical mechanics
module includes the study of the uniform circular motion, the forced damped
harmonic oscillations and the vibration of a fixed-fixed string.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5305</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5305</id><created>2011-06-27</created><updated>2015-02-02</updated><authors><author><keyname>Lesnick</keyname><forenames>Michael</forenames></author></authors><title>The Theory of the Interleaving Distance on Multidimensional Persistence
  Modules</title><categories>cs.CG math.AT</categories><comments>Major revision; exposition improved throughout. To appear in
  Foundations of Computational Mathematics. 36 pages</comments><journal-ref>Foundations of Computational Mathematics: Volume 15, Issue 3
  (2015), Page 613-650</journal-ref><doi>10.1007/s10208-015-9255-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2009, Chazal et al. introduced $\epsilon$-interleavings of persistence
modules. $\epsilon$-interleavings induce a pseudometric $d_I$ on (isomorphism
classes of) persistence modules, the interleaving distance. The definitions of
$\epsilon$-interleavings and $d_I$ generalize readily to multidimensional
persistence modules. In this paper, we develop the theory of multidimensional
interleavings, with a view towards applications to topological data analysis.
We present four main results. First, we show that on 1-D persistence modules,
$d_I$ is equal to the bottleneck distance $d_B$. This result, which first
appeared in an earlier preprint of this paper, has since appeared in several
other places, and is now known as the isometry theorem. Second, we present a
characterization of the $\epsilon$-interleaving relation on multidimensional
persistence modules. This expresses transparently the sense in which two
$\epsilon$-interleaved modules are algebraically similar. Third, using this
characterization, we show that when we define our persistence modules over a
prime field, $d_I$ satisfies a universality property. This universality result
is the central result of the paper. It says that $d_I$ satisfies a stability
property generalizing one which $d_B$ is known to satisfy, and that in
addition, if $d$ is any other pseudometric on multidimensional persistence
modules satisfying the same stability property, then $d\leq d_I$. We also show
that a variant of this universality result holds for $d_B$, over arbitrary
fields. Finally, we show that $d_I$ restricts to a metric on isomorphism
classes of finitely presented multidimensional persistence modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5308</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5308</id><created>2011-06-27</created><authors><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Petrescu</keyname><forenames>Diana</forenames></author><author><keyname>Trau&#x15f;an-Matu</keyname><forenames>&#x15e;tefan</forenames></author></authors><title>Clasificarea distribuita a mesajelor de e-mail</title><categories>cs.HC cs.CL</categories><comments>ISSN 1453-1305</comments><journal-ref>A Treia Conferin\c{t}\u{a} Na\c{t}ional\u{a} de Interac\c{t}iune
  Om-Calculator 2006, Informatica Economica, vol. X, Bucuresti, pp. 79-82, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic component in Internet applications is the electronic mail and its
various implications. The paper proposes a mechanism for automatically
classifying emails and create dynamic groups that belong to these messages.
Proposed mechanisms will be based on natural language processing techniques and
will be designed to facilitate human-machine interaction in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5309</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5309</id><created>2011-06-27</created><authors><author><keyname>Moise</keyname><forenames>Diana</forenames></author><author><keyname>Moise</keyname><forenames>Eliza</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author></authors><title>Resource CoAllocation for Scheduling Tasks with Dependencies, in Grid</title><categories>cs.DC</categories><comments>ISSN: 2065-0701</comments><journal-ref>Proceedings of The Second International Workshop on High
  Performance in Grid Middleware (HiPerGRID 2008), Bucharest, Romania,
  Published by IEEE Romania, 2008, pages: 41-48</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling applications on wide-area distributed systems is useful for
obtaining quick and reliable results in an efficient manner. Optimized
scheduling algorithms are fundamentally important in order to achieve optimized
resources utilization. The existing and potential applications include many
fields of activity like satellite image processing and medicine. The paper
proposes a scheduling algorithm for tasks with dependencies in Grid
environments. CoAllocation represents a strategy that provides a schedule for
task with dependencies, having as main purpose the efficiency of the schedule,
in terms of load balancing and minimum time for the execution of the tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5310</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5310</id><created>2011-06-27</created><authors><author><keyname>Moise</keyname><forenames>Eliza</forenames></author><author><keyname>Moise</keyname><forenames>Diana</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author></authors><title>Advance Reservation of Resources for Task Execution in Grid Environments</title><categories>cs.DC</categories><comments>ISSN: 2065-0701</comments><journal-ref>Proceedings of The Second International Workshop on High
  Performance in Grid Middleware (HiPerGRID 2008), Bucharest, Romania,
  Published by IEEE Romania, 2008, pages: 57-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a solution for the Grid scheduling problem, addressing in
particular the requirement of high performance an efficient algorithm must
fulfill. Advance Reservation engages a distributed, dynamic, fault-tolerant and
efficient strategy which reserves resources for future task execution. The
paper presents the main features of the strategy, the functioning mechanism the
strategy is based on and the methods used for evaluating the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5312</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5312</id><created>2011-06-27</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Manipulation of Nanson's and Baldwin's Rules</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanson's and Baldwin's voting rules select a winner by successively
eliminating candidates with low Borda scores. We show that these rules have a
number of desirable computational properties. In particular, with unweighted
votes, it is NP-hard to manipulate either rule with one manipulator, whilst
with weighted votes, it is NP-hard to manipulate either rule with a small
number of candidates and a coalition of manipulators. As only a couple of other
voting rules are known to be NP-hard to manipulate with a single manipulator,
Nanson's and Baldwin's rules appear to be particularly resistant to
manipulation from a theoretical perspective. We also propose a number of
approximation methods for manipulating these two rules. Experiments demonstrate
that both rules are often difficult to manipulate in practice. These results
suggest that elimination style voting rules deserve further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5316</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5316</id><created>2011-06-27</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Online Cake Cutting (published version)</title><categories>cs.AI cs.GT cs.MA</categories><comments>To appear in the Proceedings of the Second International Conference
  on Algorithmic Decision Theory (ADT 2011)</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an online form of the cake cutting problem. This models situations
where agents arrive and depart during the process of dividing a resource. We
show that well known fair division procedures like cut-and-choose and the
Dubins-Spanier moving knife procedure can be adapted to apply to such online
problems. We propose some fairness properties that online cake cutting
procedures can possess like online forms of proportionality and envy-freeness.
We also consider the impact of collusion between agents. Finally, we study
theoretically and empirically the competitive ratio of these online cake
cutting procedures. Based on its resistance to collusion, and its good
performance in practice, our results favour the online version of the
cut-and-choose procedure over the online version of the moving knife procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5321</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5321</id><created>2011-06-27</created><authors><author><keyname>Yang</keyname><forenames>Zhi</forenames></author><author><keyname>Wilson</keyname><forenames>Christo</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Gao</keyname><forenames>Tingting</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author><author><keyname>Dai</keyname><forenames>Yafei</forenames></author></authors><title>Uncovering Social Network Sybils in the Wild</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sybil accounts are fake identities created to unfairly increase the power or
resources of a single malicious user. Researchers have long known about the
existence of Sybil accounts in online communities such as file-sharing systems,
but have not been able to perform large scale measurements to detect them or
measure their activities. In this paper, we describe our efforts to detect,
characterize and understand Sybil account activity in the Renren online social
network (OSN). We use ground truth provided by Renren Inc. to build measurement
based Sybil account detectors, and deploy them on Renren to detect over 100,000
Sybil accounts. We study these Sybil accounts, as well as an additional 560,000
Sybil accounts caught by Renren, and analyze their link creation behavior. Most
interestingly, we find that contrary to prior conjecture, Sybil accounts in
OSNs do not form tight-knit communities. Instead, they integrate into the
social graph just like normal users. Using link creation timestamps, we verify
that the large majority of links between Sybil accounts are created
accidentally, unbeknownst to the attacker. Overall, only a very small portion
of Sybil accounts are connected to other Sybils with social links. Our study
shows that existing Sybil defenses are unlikely to succeed in today's OSNs, and
we must design new techniques to effectively detect and defend against Sybil
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5326</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5326</id><created>2011-06-27</created><authors><author><keyname>Javan</keyname><forenames>Mohammad R.</forenames></author><author><keyname>Sharafat</keyname><forenames>Ahmad R.</forenames></author></authors><title>Efficient and Distributed SINR-based Joint Resource Allocation and Base
  Station Assignment in Wireless CDMA Networks</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We formulate the resource allocation problem for the uplink of code division
multiple access (CDMA) networks using a game theoretic framework, propose an
efficient and distributed algorithm for a joint rate and power allocation, and
show that the proposed algorithm converges to the unique Nash equilibrium (NE)
of the game. Our choice for the utility function enables each user to adapt its
transmit power and throughput to its channel. Due to users' selfish behavior,
the output of the game (its NE) may not be a desirable one. To avoid such
cases, we use pricing to control each user's behavior, and analytically show
that similar to the no-pricing case, our pricing-based algorithm converges to
the unique NE of the game, at which, each user achieves its target
signal-to-interference-plus-noise ratio (SINR). We also extend our distributed
resource allocation scheme to multi-cell environments for base station
assignment. Simulation results confirm that our algorithm is computationally
efficient and its signalling overhead is low. In particular, we will show that
in addition to its ability to attain the required QoS of users, our scheme
achieves better fairness in allocating resources and can significantly reduce
transmit power as compared to existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5341</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5341</id><created>2011-06-27</created><authors><author><keyname>Ly</keyname><forenames>Daniel L.</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Pose Estimation from a Single Depth Image for Arbitrary Kinematic
  Skeletons</title><categories>cs.CV cs.AI cs.LG</categories><comments>2 pages, 2 figures, RGB-D workshop in Robotics: Science and Systems
  (RSS 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for estimating pose information from a single depth image
given an arbitrary kinematic structure without prior training. For an arbitrary
skeleton and depth image, an evolutionary algorithm is used to find the optimal
kinematic configuration to explain the observed image. Results show that our
approach can correctly estimate poses of 39 and 78 degree-of-freedom models
from a single depth image, even in cases of significant self-occlusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5346</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5346</id><created>2011-06-27</created><updated>2011-11-17</updated><authors><author><keyname>Oktay</keyname><forenames>Onur</forenames></author><author><keyname>Pfander</keyname><forenames>G&#xf6;tz</forenames></author><author><keyname>Zheltov</keyname><forenames>Pavel</forenames></author></authors><title>Reconstruction and Estimation of Scattering Functions of Overspread
  Radar Targets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many radar scenarios, the radar target or the medium is assumed to possess
randomly varying parts. The properties of a target are described by a random
process known as the spreading function. Its second order statistics under the
WSSUS assumption are given by the scattering function. Recent developments in
the operator identification theory suggest a channel sounding procedure that
allows to determine the spreading function given complete statistical knowledge
of the operator echo. We show that in a continuous model it is indeed
theoretically possible to identify a scattering function of an overspread
target given full statistics of a received echo from a single sounding by a
custom weighted delta train. Our results apply whenever the scattering function
is supported on a set of area less than one. Absent such complete statistics,
we construct and analyze an estimator that can be used as a replacement of the
averaged periodogram estimator in case of poor geometry of the support set of
the scattering function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5349</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5349</id><created>2011-06-27</created><authors><author><keyname>Ryckelynck</keyname><forenames>Philippe</forenames></author><author><keyname>Smoch</keyname><forenames>Laurent</forenames></author></authors><title>Discrete calculus of variations for quadratic lagrangians</title><categories>math.OC cs.SY</categories><report-no>LMPA Technical Report 2010</report-no><msc-class>49K21, 49K15, 65L03, 65L12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop in this paper a new framework for discrete calculus of variations
when the actions have densities involving an arbitrary discretization operator.
We deduce the discrete Euler-Lagrange equations for piecewise continuous
critical points of sampled actions. Then we characterize the discretization
operators such that, for all quadratic lagrangian, the discrete Euler-Lagrange
equations converge to the classical ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5350</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5350</id><created>2011-06-27</created><authors><author><keyname>Ryckelynck</keyname><forenames>Philippe</forenames></author><author><keyname>Smoch</keyname><forenames>Laurent</forenames></author></authors><title>Discrete Calculus of Variations for Quadratic Lagrangians. Convergence
  Issues</title><categories>math.OC cs.SY</categories><comments>6 figures</comments><report-no>LMPA Technical Report 2010</report-no><msc-class>49K21, 49K15, 65L03, 65L12, 34K14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper the continuous and discrete Euler-Lagrange equations
arising from a quadratic lagrangian. Those equations may be thought as
numerical schemes and may be solved through a matrix based framework. When the
lagrangian is time-independent, we can solve both continuous and discrete
Euler-Lagrange equations under convenient oscillatory and non-resonance
properties. The convergence of the solutions is also investigated. In the
simplest case of the harmonic oscillator, unconditional convergence does not
hold, we give results and experiments in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5351</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5351</id><created>2011-06-27</created><authors><author><keyname>Ryckelynck</keyname><forenames>Philippe</forenames></author><author><keyname>Smoch</keyname><forenames>Laurent</forenames></author></authors><title>Quadratic choreographies</title><categories>math.OC cs.SY</categories><comments>10 figures 10th IMACS International Symposium on Iterative Methods in
  Scientific Computing</comments><report-no>LMPA Technical Report 2011</report-no><msc-class>49K21, 49K15, 65L03, 65L12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the classical and discrete Euler-Lagrange equations for
systems of $n$ particles interacting quadratically in $\mathbb{R}^d$. By
highlighting the role played by the center of mass of the particles, we solve
the previous systems via the classical quadratic eigenvalue problem (QEP) and
its discrete transcendental generalization. The roots of classical and discrete
QEP being given, we state some conditional convergence results. Next, we focus
especially on periodic and choreographic solutions and we provide some
numerical experiments which confirm the convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5353</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5353</id><created>2011-06-27</created><updated>2011-11-19</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>On a diameter bound for Cayley graphs generated by transposition trees</title><categories>math.CO cs.DM</categories><comments>See abs/1111.3114 for the latest version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Gamma$ be a Cayley graph generated by a transposition tree. A natural
problem is to understand how the properties of the Cayley graph depend on those
of the underlying transposition tree. We focus here on diameter and distance
related questions. We examine some related bounds and prove some properties
about them.
  This paper has been withdrawn and a new version with these results and
further extensions is at abs/1111.3114
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5364</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5364</id><created>2011-06-27</created><authors><author><keyname>Plainchault</keyname><forenames>M&#xe9;lanie</forenames></author><author><keyname>Gresset</keyname><forenames>Nicolas</forenames></author><author><keyname>Ben-Othman</keyname><forenames>Ghaya Rekaya</forenames></author></authors><title>Macro and Micro Diversity Behaviors of Practical Dynamic Decode and
  Forward Relaying schemes</title><categories>cs.IT math.IT</categories><comments>19 pages, 10 figures, 2 tables. Submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a practical implementation of the Dynamic Decode
and Forward (DDF) protocol based on rateless codes and HARQ. We define the
macro diversity order of a transmission from several intermittent sources to a
single destination. Considering finite symbol alphabet used by the different
sources, upper bounds on the achievable macro diversity order are derived. We
analyse the diversity behavior of several relaying schemes for the DDF
protocol, and we propose the Patching technique to increase both the macro and
the micro diversity orders. The coverage gain for the open-loop transmission
case and the spectral efficiency gain for the closed loop transmission case are
illustrated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5367</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5367</id><created>2011-06-27</created><authors><author><keyname>Huang</keyname><forenames>Huang</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Partial Interference Alignment for K-user MIMO Interference Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2011.2161297</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a Partial Interference Alignment and Interference
Detection (PIAID) design for $K$-user quasi-static MIMO interference channels
with discrete constellation inputs. Each transmitter has M antennas and
transmits L independent data streams to the desired receiver with N receive
antennas. We focus on the case where not all K-1 interfering transmitters can
be aligned at every receiver. As a result, there will be residual interference
at each receiver that cannot be aligned. Each receiver detects and cancels the
residual interference based on the constellation map. However, there is a
window of unfavorable interference profile at the receiver for Interference
Detection (ID). In this paper, we propose a low complexity Partial Interference
Alignment scheme in which we dynamically select the user set for IA so as to
create a favorable interference profile for ID at each receiver. We first
derive the average symbol error rate (SER) by taking into account of the
non-Guassian residual interference due to discrete constellation. Using graph
theory, we then devise a low complexity user set selection algorithm for the
PIAID scheme,which minimizes the asymptotically tight bound for the average
end-to-end SER performance. Moreover, we substantially simplify interference
detection at the receiver using Semi-Definite Relaxation (SDR) techniques. It
is shown that the SER performance of the proposed PIAID scheme has significant
gain compared with various conventional baseline solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5387</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5387</id><created>2011-06-27</created><updated>2011-11-16</updated><authors><author><keyname>Siavoshani</keyname><forenames>Mahdi Jafari</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Subspace Properties of Network Coding and their Applications</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE transactions on information theory. The paper has
  21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems that employ network coding for content distribution convey to the
receivers linear combinations of the source packets. If we assume randomized
network coding, during this process the network nodes collect random subspaces
of the space spanned by the source packets. We establish several fundamental
properties of the random subspaces induced in such a system, and show that
these subspaces implicitly carry topological information about the network and
its state that can be passively collected and inferred. We leverage this
information towards a number of applications that are interesting in their own
right, such as topology inference, bottleneck discovery in peer-to-peer systems
and locating Byzantine attackers. We thus argue that, randomized network
coding, apart from its better known properties for improving information
delivery rate, can additionally facilitate network management and control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5413</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5413</id><created>2011-06-27</created><authors><author><keyname>Huang</keyname><forenames>Bo</forenames></author><author><keyname>Ma</keyname><forenames>Shiqian</forenames></author><author><keyname>Goldfarb</keyname><forenames>Donald</forenames></author></authors><title>Accelerated Linearized Bregman Method</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and analyze an accelerated linearized Bregman (ALB)
method for solving the basis pursuit and related sparse optimization problems.
This accelerated algorithm is based on the fact that the linearized Bregman
(LB) algorithm is equivalent to a gradient descent method applied to a certain
dual formulation. We show that the LB method requires $O(1/\epsilon)$
iterations to obtain an $\epsilon$-optimal solution and the ALB algorithm
reduces this iteration complexity to $O(1/\sqrt{\epsilon})$ while requiring
almost the same computational effort on each iteration. Numerical results on
compressed sensing and matrix completion problems are presented that
demonstrate that the ALB method can be significantly faster than the LB method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5423</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5423</id><created>2011-06-27</created><updated>2012-05-30</updated><authors><author><keyname>Neeman</keyname><forenames>Joe</forenames></author></authors><title>A law of large numbers for weighted plurality</title><categories>math.PR cs.GT math.CO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Consider an election between k candidates in which each voter votes randomly
(but not necessarily independently) and suppose that there is a single
candidate that every voter prefers (in the sense that each voter is more likely
to vote for this special candidate than any other candidate). Suppose we have a
voting rule that takes all of the votes and produces a single outcome and
suppose that each individual voter has little effect on the outcome of the
voting rule. If the voting rule is a weighted plurality, then we show that with
high probability, the preferred candidate will win the election. Conversely, we
show that this statement fails for all other reasonable voting rules.
  This result is an extension of H\&quot;aggstr\&quot;om, Kalai and Mossel, who proved
the above in the case k=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5427</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5427</id><created>2011-06-27</created><authors><author><keyname>Xu</keyname><forenames>You</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author><author><keyname>Lu</keyname><forenames>Qiang</forenames></author><author><keyname>Huang</keyname><forenames>Ruoyun</forenames></author></authors><title>Theory and Algorithms for Partial Order Based Reduction in Planning</title><categories>cs.AI</categories><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search is a major technique for planning. It amounts to exploring a state
space of planning domains typically modeled as a directed graph. However,
prohibitively large sizes of the search space make search expensive. Developing
better heuristic functions has been the main technique for improving search
efficiency. Nevertheless, recent studies have shown that improving heuristics
alone has certain fundamental limits on improving search efficiency. Recently,
a new direction of research called partial order based reduction (POR) has been
proposed as an alternative to improving heuristics. POR has shown promise in
speeding up searches.
  POR has been extensively studied in model checking research and is a key
enabling technique for scalability of model checking systems. Although the POR
theory has been extensively studied in model checking, it has never been
developed systematically for planning before. In addition, the conditions for
POR in the model checking theory are abstract and not directly applicable in
planning. Previous works on POR algorithms for planning did not establish the
connection between these algorithms and existing theory in model checking.
  In this paper, we develop a theory for POR in planning. The new theory we
develop connects the stubborn set theory in model checking and POR methods in
planning. We show that previous POR algorithms in planning can be explained by
the new theory. Based on the new theory, we propose a new, stronger POR
algorithm. Experimental results on various planning domains show further search
cost reduction using the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5433</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5433</id><created>2011-06-27</created><authors><author><keyname>Muchnik</keyname><forenames>Andrej A.</forenames></author></authors><title>Kolmogorov complexity and cryptography</title><categories>cs.CR cs.IT math.IT</categories><comments>In Russian and English: English text follows the Russian one</comments><msc-class>94A60, 68P30</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains some results of An.A.Muchnik (1958-2007) reported in his
talks at the Kolmogorov seminar (Moscow State Lomonosov University, Math.
Department, Logic and Algorithms theory division, March 11, 2003 and April 8,
2003) but not published at that time. These results were stated (without
proofs) in the joint talk of Andrej Muchnik and Alexei Semenov at Dagstuhl
Seminar 03181, 27.04.2003-03.05.2003. This text was prepared by Alexey Chernov
and Alexander Shen in 2008-2009. We consider (in the framework of algorithmic
information theory) questions of the following type: construct a message that
contains different amounts of information for recipients that have (or do not
have) certain a priori information. Assume, for example, that the recipient
knows some string $a$, and we want to send her some information that allows her
to reconstruct some string $b$ (using $a$). On the other hand, this information
alone should not allow the eavesdropper (who does not know $a$) to reconstruct
$b$. It is indeed possible (if the strings $a$ and $b$ are not too simple).
Then we consider more complicated versions of this question. What if the
eavesdropper knows some string $c$? How long should be our message? We provide
some conditions that guarantee the existence of a polynomial-size message; we
show then that without these conditions this is not always possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5448</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5448</id><created>2011-06-27</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Dominating Manipulations in Voting with Partial Information</title><categories>cs.AI cs.CC cs.GT cs.MA</categories><comments>7 pages by arxiv pdflatex, 1 figure. The 6-page version has the same
  content and will be published in Proceedings of the Twenty-Fifth AAAI
  Conference on Artificial Intelligence (AAAI-11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider manipulation problems when the manipulator only has partial
information about the votes of the nonmanipulators. Such partial information is
described by an information set, which is the set of profiles of the
nonmanipulators that are indistinguishable to the manipulator. Given such an
information set, a dominating manipulation is a non-truthful vote that the
manipulator can cast which makes the winner at least as preferable (and
sometimes more preferable) as the winner when the manipulator votes truthfully.
When the manipulator has full information, computing whether or not there
exists a dominating manipulation is in P for many common voting rules (by known
results). We show that when the manipulator has no information, there is no
dominating manipulation for many common voting rules. When the manipulator's
information is represented by partial orders and only a small portion of the
preferences are unknown, computing a dominating manipulation is NP-hard for
many common voting rules. Our results thus throw light on whether we can
prevent strategic behavior by limiting information about the votes of other
voters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5451</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5451</id><created>2011-06-27</created><authors><author><keyname>Sriram</keyname><forenames>Ilango</forenames></author><author><keyname>Cliff</keyname><forenames>Dave</forenames></author></authors><title>Hybrid complex network topologies are preferred for
  component-subscription in large-scale data-centres</title><categories>cs.DC</categories><journal-ref>CompleNet 2010, CCIS vol. 116, pp. 130-137, Springer Heidelberg,
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on experiments exploring the interplay between the topology of the
complex network of dependent components in a large-scale data-centre, and the
robustness and scaling properties of that data-centre. In a previous paper [1]
we used the SPECI large-scale data-centre simulator [2] to compare the
robustness and scaling characteristics of data-centres whose dependent
components are connected via Strogatz-Watts small-world (SW) networks [3],
versus those organized as Barabasi-Albert scale-free (SF) networks [4], and
found significant differences. In this paper, we present results from using the
Klemm-Eguiliz (KE) construction method [5] to generate complex network
topologies for data-centre component dependencies. The KE model has a control
parameter {\mu}\in[0,1]\inR that determines whether the networks generated are
SW (0&lt;{\mu}&lt;&lt;1) or SF ({\mu}=1) or a &quot;hybrid&quot; network topology part-way between
SW and SF (0&lt;{\mu}&lt;1). We find that the best scores for system-level
performance metrics of the simulated data-centres are given by &quot;hybrid&quot; values
of {\mu} significantly different from pure-SW or pure-SF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5457</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5457</id><created>2011-06-27</created><authors><author><keyname>Cartlidge</keyname><forenames>John</forenames></author><author><keyname>Sriram</keyname><forenames>Ilango</forenames></author></authors><title>Modelling Resilience in Cloud-Scale Data Centres</title><categories>cs.DC</categories><comments>To appear in: Proceedings of the 23rd European Modeling &amp; Simulation
  Symposium (Simulation in Industry) 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trend for cloud computing has initiated a race towards data centres (DC)
of an ever-increasing size. The largest DCs now contain many hundreds of
thousands of virtual machine (VM) services. Given the finite lifespan of
hardware, such large DCs are subject to frequent hardware failure events that
can lead to disruption of service. To counter this, multiple redundant copies
of task threads may be distributed around a DC to ensure that individual
hardware failures do not cause entire jobs to fail. Here, we present results
demonstrating the resilience of different job scheduling algorithms in a
simulated DC with hardware failure. We use a simple model of jobs distributed
across a hardware network to demonstrate the relationship between resilience
and additional communication costs of different scheduling methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5460</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5460</id><created>2011-06-27</created><authors><author><keyname>Wala</keyname><forenames>Jeremiah</forenames></author><author><keyname>Fotin</keyname><forenames>Sergei</forenames></author><author><keyname>Lee</keyname><forenames>Jaesung</forenames></author><author><keyname>Jirapatnakul</keyname><forenames>Artit</forenames></author><author><keyname>Biancardi</keyname><forenames>Alberto</forenames></author><author><keyname>Reeves</keyname><forenames>Anthony</forenames></author></authors><title>Automated segmentation of the pulmonary arteries in low-dose CT by
  vessel tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fully automated method for top-down segmentation of the
pulmonary arterial tree in low-dose thoracic CT images. The main basal
pulmonary arteries are identified near the lung hilum by searching for
candidate vessels adjacent to known airways, identified by our previously
reported airway segmentation method. Model cylinders are iteratively fit to the
vessels to track them into the lungs. Vessel bifurcations are detected by
measuring the rate of change of vessel radii, and child vessels are segmented
by initiating new trackers at bifurcation points. Validation is accomplished
using our novel sparse surface (SS) evaluation metric. The SS metric was
designed to quantify the magnitude of the segmentation error per vessel while
significantly decreasing the manual marking burden for the human user. A total
of 210 arteries and 205 veins were manually marked across seven test cases.
134/210 arteries were correctly segmented, with a specificity for arteries of
90%, and average segmentation error of 0.15 mm. This fully-automated
segmentation is a promising method for improving lung nodule detection in
low-dose CT screening scans, by separating vessels from surrounding
iso-intensity objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5465</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5465</id><created>2011-06-27</created><authors><author><keyname>Sriram</keyname><forenames>Ilango Leonardo</forenames></author><author><keyname>Cliff</keyname><forenames>Dave</forenames></author></authors><title>SPECI-2: An open-source framework for predictive simulation of
  cloud-scale data-centres</title><categories>cs.DC</categories><comments>To appear in Proceedings of SIMULTECH 2011, see also speci.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Version 2 of SPECI, a system for predictive simulation modeling
of large-scale data-centres, i.e. warehouse-sized facilities containing
hundreds of thousands of servers, as used to provide cloud services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5470</identifier>
 <datestamp>2011-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5470</id><created>2011-06-27</created><updated>2011-08-09</updated><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Connection and Dispersion of Computation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper talk about the influence of Connection and Dispersion on
Computational Complexity. And talk about the HornCNF's connection and CNF's
dispersion, and show the difference between CNFSAT and HornSAT. First, I talk
the relation between MUC decision problem and classifying the truth value
assignment. Second, I define the two inner products (&quot;inner product&quot; and &quot;inner
harmony&quot;) and talk about the influence of orthogonal and correlation to MUC.
And we can not reduce MUC to Orthogonalization MUC by using HornMUC in
polynomial size because HornMUC have high orthogonal of inner harmony and MUC
do not. So DP is not P, and NP is not P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5489</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5489</id><created>2011-06-27</created><updated>2011-06-30</updated><authors><author><keyname>Pastorello</keyname><forenames>Gilberto Z.</forenames></author><author><keyname>Sanchez-Azofeifa</keyname><forenames>G. Arturo</forenames></author><author><keyname>Nascimento</keyname><forenames>Mario A.</forenames></author></authors><title>A Review of the Enviro-Net Project</title><categories>cs.NI</categories><comments>v2: 29 pages, 5 figures, reflects changes addressing reviewers'
  comments v1: 38 pages, 8 figures</comments><journal-ref>G. Z. Pastorello, G. A. Sanchez-Azofeifa, M. A. Nascimento.
  Enviro-Net: From Networks of Ground-Based Sensor Systems to a Web Platform
  for Sensor Data Management. Sensors. 2011; 11(6):6454-6479</journal-ref><doi>10.3390/s110606454</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ecosystems monitoring is essential to properly understand their development
and the effects of events, both climatological and anthropological in nature.
The amount of data used in these assessments is increasing at very high rates.
This is due to increasing availability of sensing systems and the development
of new techniques to analyze sensor data. The Enviro-Net Project encompasses
several of such sensor system deployments across five countries in the
Americas. These deployments use a few different ground-based sensor systems,
installed at different heights monitoring the conditions in tropical dry
forests over long periods of time. This paper presents our experience in
deploying and maintaining these systems, retrieving and pre-processing the
data, and describes the Web portal developed to help with data management,
visualization and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5524</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5524</id><created>2011-06-27</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Robust network community detection using balanced propagation</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>Eur. Phys. J. B 81(3), 353-362 (2011)</journal-ref><doi>10.1140/epjb/e2011-10979-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Label propagation has proven to be an extremely fast method for detecting
communities in large complex networks. Furthermore, due to its simplicity, it
is also currently one of the most commonly adopted algorithms in the
literature. Despite various subsequent advances, an important issue of the
algorithm has not yet been properly addressed. Random (node) update orders
within the algorithm severely hamper its robustness, and consequently also the
stability of the identified community structure. We note that an update order
can be seen as increasing propagation preferences from certain nodes, and
propose a balanced propagation that counteracts for the introduced randomness
by utilizing node balancers. We have evaluated the proposed approach on
synthetic networks with planted partition, and on several real-world networks
with community structure. The results confirm that balanced propagation is
significantly more robust than label propagation, when the performance of
community detection is even improved. Thus, balanced propagation retains high
scalability and algorithmic simplicity of label propagation, but improves on
its stability and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5536</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5536</id><created>2011-06-27</created><updated>2011-10-19</updated><authors><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames></author><author><keyname>Christakis</keyname><forenames>Nicholas A.</forenames></author></authors><title>Spreading paths in partially observed social networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 9 figures, 1 table</comments><doi>10.1103/PhysRevE.85.036106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how and how far information, behaviors, or pathogens spread in
social networks is an important problem, having implications for both
predicting the size of epidemics, as well as for planning effective
interventions. There are, however, two main challenges for inferring spreading
paths in real-world networks. One is the practical difficulty of observing a
dynamic process on a network, and the other is the typical constraint of only
partially observing a network. Using a static, structurally realistic social
network as a platform for simulations, we juxtapose three distinct paths: (1)
the stochastic path taken by a simulated spreading process from source to
target; (2) the topologically shortest path in the fully observed network, and
hence the single most likely stochastic path, between the two nodes; and (3)
the topologically shortest path in a partially observed network. In a sampled
network, how closely does the partially observed shortest path (3) emulate the
unobserved spreading path (1)? Although partial observation inflates the length
of the shortest path, the stochastic nature of the spreading process also
frequently derails the dynamic path from the shortest path. We find that the
partially observed shortest path does not necessarily give an inflated estimate
of the length of the process path; in fact, partial observation may,
counterintuitively, make the path seem shorter than it actually is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5551</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5551</id><created>2011-06-27</created><authors><author><keyname>Koppula</keyname><forenames>Hema Swetha</forenames></author><author><keyname>Anand</keyname><forenames>Abhishek</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Labeling 3D scenes for Personal Assistant Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inexpensive RGB-D cameras that give an RGB image together with depth data
have become widely available. We use this data to build 3D point clouds of a
full scene. In this paper, we address the task of labeling objects in this 3D
point cloud of a complete indoor scene such as an office. We propose a
graphical model that captures various features and contextual relations,
including the local visual appearance and shape cues, object co-occurrence
relationships and geometric relationships. With a large number of object
classes and relations, the model's parsimony becomes important and we address
that by using multiple types of edge potentials. The model admits efficient
approximate inference, and we train it using a maximum-margin learning
approach. In our experiments over a total of 52 3D scenes of homes and offices
(composed from about 550 views, having 2495 segments labeled with 27 object
classes), we get a performance of 84.06% in labeling 17 object classes for
offices, and 73.38% in labeling 17 object classes for home scenes. Finally, we
applied these algorithms successfully on a mobile robot for the task of finding
an object in a large cluttered room.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5562</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5562</id><created>2011-06-28</created><authors><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhou</keyname><forenames>Changsong</forenames></author></authors><title>Relative clock demonstrates the endogenous heterogeneity of human
  dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages 7 figures 2 Tables</comments><journal-ref>EPL 97 (2012) 18006</journal-ref><doi>10.1209/0295-5075/97/18006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heavy-tailed inter-event time distributions are widely observed in many
human-activated systems, which may result from both endogenous mechanisms like
the highest-priority-first protocol and exogenous factors like the varying
global activity versus time. To distinguish the effects on temporal statistics
from different mechanisms is this of theoretical significance. In this Letter,
we propose a new timing method by using a relative clock, where the time length
between two consecutive events of an individual is counted as the number of
other individuals' events appeared during this interval. We propose a model, in
which agents act either in a constant rate or with a power-law inter-event time
distribution, and the global activity either keeps unchanged or varies
periodically versus time. Our analysis shows that the heavy tails caused by the
heterogeneity of global activity can be eliminated by setting the relative
clock, yet the heterogeneity due to real individual behaviors still exists. We
perform extensive experiments on four large-scale systems, the search engine by
AOL, a social bookmarking system--Delicious, a short-message communication
network, and a microblogging system--Twitter. Strong heterogeneity and clear
seasonality of global activity are observed, but the heavy tails cannot be
eliminated by using the relative clock. Our results suggest the existence of
endogenous heterogeneity of human dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5568</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5568</id><created>2011-06-28</created><authors><author><keyname>Sani</keyname><forenames>Ardalan Amiri</forenames></author><author><keyname>Richter</keyname><forenames>Wolfgang</forenames></author><author><keyname>Bao</keyname><forenames>Xuan</forenames></author><author><keyname>Narayan</keyname><forenames>Trevor</forenames></author><author><keyname>Satyanarayanan</keyname><forenames>Mahadev</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Choudhury</keyname><forenames>Romit Roy</forenames></author></authors><title>Opportunistic Content Search of Smartphone Photos</title><categories>cs.IR cs.DB</categories><report-no>Technical Report TR0627-2011, Rice University</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photos taken by smartphone users can accidentally contain content that is
timely and valuable to others, often in real-time. We report the system design
and evaluation of a distributed search system, Theia, for crowd-sourced
real-time content search of smartphone photos. Because smartphones are
resource-constrained, Theia incorporates two key innovations to control search
cost and improve search efficiency. Incremental Search expands search scope
incrementally and exploits user feedback. Partitioned Search leverages the
cloud to reduce the energy consumption of search in smartphones. Through user
studies, measurement studies, and field studies, we show that Theia reduces the
cost per relevant photo by an average of 59%. It reduces the energy consumption
of search by up to 55% and 81% compared to alternative strategies of executing
entirely locally or entirely in the cloud. Search results from smartphones are
obtained in seconds. Our experiments also suggest approaches to further improve
these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5569</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5569</id><created>2011-06-28</created><authors><author><keyname>Prochazka</keyname><forenames>David</forenames></author><author><keyname>Koubek</keyname><forenames>Tomas</forenames></author></authors><title>Augmented Reality Implementation Methods in Mainstream Applications</title><categories>cs.CV</categories><acm-class>H.5.1</acm-class><journal-ref>Acta Universitatis agriculturae et silviculturae Mendelianae
  Brunensis, Vol. LIX, No. 4, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5570</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5570</id><created>2011-06-28</created><authors><author><keyname>Voicu</keyname><forenames>Ramiro</forenames></author><author><keyname>Legrand</keyname><forenames>Iosif</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Tapus</keyname><forenames>Nicolae</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author></authors><title>A distributed service for on demand end to end optical circuits</title><categories>cs.DC</categories><comments>17th International Conference on Control Systems and Computer Science
  (CSCS 17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 155-161, ISSN:
  2066-4451</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a system for monitoring and controlling dynamic
network circuits inside the USLHCNet network. This distributed service system
provides in near real-time complete topological information for all the
circuits, resource allocation and usage, accounting, detects automatically
failures in the links and network equipment, generate alarms and has the
functionality to take automatic actions. The system is developed based on the
MonALISA framework, which provides a robust monitoring and controlling service
oriented architecture, with no single points of failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5571</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5571</id><created>2011-06-28</created><authors><author><keyname>Prochazka</keyname><forenames>David</forenames></author><author><keyname>Stencl</keyname><forenames>Michael</forenames></author><author><keyname>Popelka</keyname><forenames>Ondrej</forenames></author><author><keyname>Stastny</keyname><forenames>Jiri</forenames></author></authors><title>Mobile Augmented Reality Applications</title><categories>cs.CV</categories><acm-class>H.5.1</acm-class><journal-ref>Proceedings of Mendel 2011: 17th International Conference on Soft
  Computing, pp. 469-476, ISBN 978-80-214-4302-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5576</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5576</id><created>2011-06-28</created><authors><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Stratan</keyname><forenames>Corina</forenames></author><author><keyname>Costan</keyname><forenames>Alexandru</forenames></author><author><keyname>Leordeanu</keyname><forenames>Catalin</forenames></author></authors><title>Models and Techniques for Ensuring Reliability, Safety, Availability and
  Security of Large Scale Distributed Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  17th International Conference on Control Systems and Computer Science (CSCS
17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 401-406, ISSN: 2066-4451.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5593</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5593</id><created>2011-06-28</created><authors><author><keyname>Perling</keyname><forenames>Markus</forenames></author></authors><title>Resolutions and Cohomologies of Toric Sheaves. The affine case</title><categories>math.AG cs.CG</categories><comments>39 pages, requires packages ams*, enumerate</comments><msc-class>14M25, 13C14, 13A02, 52C35</msc-class><journal-ref>Int. J. Math. 24(9), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study equivariant resolutions and local cohomologies of toric sheaves for
affine toric varieties, where our focus is on the construction of new examples
of decomposable maximal Cohen-Macaulay modules of higher rank. A result of
Klyachko states that the category of reflexive toric sheaves is equivalent to
the category of vector spaces together with a certain family of filtrations.
Within this setting, we develop machinery which facilitates the construction of
minimal free resolutions for the smooth case as well as resolutions which are
acyclic with respect to local cohomology functors for the general case. We give
two main applications. First, over the polynomial ring, we determine in
explicit combinatorial terms the Z^n-graded Betti numbers and local cohomology
of reflexive modules whose associated filtrations form a hyperplane
arrangement. Second, for the non-smooth, simplicial case in dimension d &gt;= 3,
we construct new examples of indecomposable maximal Cohen-Macaulay modules of
rank d - 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5594</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5594</id><created>2011-06-28</created><authors><author><keyname>Daolio</keyname><forenames>Fabio</forenames></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames></author><author><keyname>Bitkov</keyname><forenames>Konstantin</forenames></author></authors><title>The Swiss Board Directors Network in 2009</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted to The European Physical Journal B</comments><doi>10.1140/epjb/e2011-20182-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the networks formed by the directors of the most important Swiss
boards and the boards themselves for the year 2009. The networks are obtained
by projection from the original bipartite graph. We highlight a number of
important statistical features of those networks such as degree distribution,
weight distribution, and several centrality measures as well as their
interrelationships. While similar statistics were already known for other board
systems, and are comparable here, we have extended the study with a careful
investigation of director and board centrality, a k-core analysis, and a
simulation of the speed of information propagation and its relationships with
the topological aspects of the network such as clustering and link weight and
betweenness. The overall picture that emerges is one in which the topological
structure of the Swiss board and director networks has evolved in such a way
that special actors and links between actors play a fundamental role in the
flow of information among distant parts of the network. This is shown in
particular by the centrality measures and by the simulation of a simple
epidemic process on the directors network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5596</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5596</id><created>2011-06-28</created><authors><author><keyname>Bravo</keyname><forenames>Mario</forenames><affiliation>IMJ</affiliation></author></authors><title>An adjusted payoff-based procedure for normal form games</title><categories>cs.GT math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple adaptive model in the framework of an $N$-player normal
form game. The model consists of a repeated game where the players only know
their own strategy space and their own payoff scored at each stage. The
information about the other agents (actions and payoffs) is unknown. In
particular, we consider a variation of the procedure studied by Cominetti et
al. (2010) where, in our case, each player is allowed to use the number of
times she has played each action to update her strategy. The resultant
stochastic process is analyzed via the so-called ODE method from stochastic
approximation theory. We are interested in the convergence of the process to
rest points of a related continuous dynamics. Results concerning almost sure
convergence and convergence with positive probability are obtained and applied
to a traffic game. Also, we provide some examples where convergence occurs with
probability zero. Finally, we verify that part of the analysis holds when
players are facing a random environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5601</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5601</id><created>2011-06-28</created><updated>2011-07-04</updated><authors><author><keyname>Chai</keyname><forenames>Junyi</forenames></author><author><keyname>Liu</keyname><forenames>James N. K.</forenames></author></authors><title>Class-based Rough Approximation with Dominance Principle</title><categories>cs.CC cs.AI</categories><comments>Submitted to IEEE-GrC2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough
Set theory, is effective and fundamentally important in Multiple Criteria
Decision Analysis (MCDA). In previous DRSA models, the definitions of the upper
and lower approximations are preserving the class unions rather than the
singleton class. In this paper, we propose a new Class-based Rough
Approximation with respect to a series of previous DRSA models, including
Classical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new
class-based reducts are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5615</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5615</id><created>2011-06-28</created><updated>2013-05-06</updated><authors><author><keyname>Lindblom</keyname><forenames>Johannes</forenames></author><author><keyname>Karipidis</keyname><forenames>Eleftherios</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Achievable Outage Rate Regions for the MISO Interference Channel</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the slow-fading two-user multiple-input single-output (MISO)
interference channel. We want to understand which rate points can be achieved,
allowing a non-zero outage probability. We do so by defining four different
outage rate regions. The definitions differ on whether the rates are declared
in outage jointly or individually and whether the transmitters have
instantaneous or statistical channel state information (CSI). The focus is on
the instantaneous CSI case with individual outage, where we propose a
stochastic mapping from the rate point and the channel realization to the
beamforming vectors. A major contribution is that we prove that the stochastic
component of this mapping is independent of the actual channel realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5622</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5622</id><created>2011-06-28</created><authors><author><keyname>Cesena</keyname><forenames>Emanuele</forenames></author><author><keyname>Pedicini</keyname><forenames>Marco</forenames></author><author><keyname>Roversi</keyname><forenames>Luca</forenames></author></authors><title>Typing a Core Binary Field Arithmetic in a Light Logic</title><categories>cs.LO</categories><msc-class>03F52, 68N18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a library for binary field arithmetic and we supply a core API
which is completely developed in DLAL, extended with a fix point formula. Since
DLAL is a restriction of linear logic where only functional programs with
polynomial evaluation cost can be typed, we obtain the core of a functional
programming setting for binary field arithmetic with built-in polynomial
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5626</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5626</id><created>2011-06-28</created><updated>2012-10-02</updated><authors><author><keyname>Bolognani</keyname><forenames>Saverio</forenames></author><author><keyname>Zampieri</keyname><forenames>Sandro</forenames></author></authors><title>A distributed control strategy for reactive power compensation in smart
  microgrids</title><categories>math.OC cs.SY</categories><journal-ref>IEEE Transactions on Automatic Control, November 2013 (Volume: 58,
  Issue: 11)</journal-ref><doi>10.1109/TAC.2013.2270317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal reactive power compensation for the
minimization of power distribution losses in a smart microgrid. We first
propose an approximate model for the power distribution network, which allows
us to cast the problem into the class of convex quadratic, linearly
constrained, optimization problems. We then consider the specific problem of
commanding the microgenerators connected to the microgrid, in order to achieve
the optimal injection of reactive power. For this task, we design a randomized,
gossip-like optimization algorithm. We show how a distributed approach is
possible, where microgenerators need to have only a partial knowledge of the
problem parameters and of the state, and can perform only local measurements.
For the proposed algorithm, we provide conditions for convergence together with
an analytic characterization of the convergence speed. The analysis shows that,
in radial networks, the best performance can be achieved when we command
cooperation among units that are neighbors in the electric topology. Numerical
simulations are included to validate the proposed model and to confirm the
analytic results about the performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5644</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5644</id><created>2011-06-28</created><authors><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author></authors><title>The ADS in the Information Age - Impact on Discovery</title><categories>astro-ph.IM cs.DL</categories><comments>10 pages, 5 figures, to appear in &quot;Organizations, People and
  Strategies in Astronomy (OPSA)&quot;, volume 8</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The SAO/NASA Astrophysics Data System (ADS) grew up with and has been riding
the waves of the Information Age, closely monitoring and anticipating the needs
of its end-users. By now, all professional astronomers are using the ADS on a
daily basis, and a substantial fraction have been using it for their entire
professional career. In addition to being an indispensable tool for
professional scientists, the ADS also moved into the public domain, as a tool
for science education. In this paper we will highlight and discuss some aspects
indicative of the impact the ADS has had on research and the access to
scholarly publications.
  The ADS is funded by NASA Grant NNX09AB39G
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5648</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5648</id><created>2011-06-28</created><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Zhao</keyname><forenames>Chunming</forenames></author><author><keyname>You</keyname><forenames>Xiaohu</forenames></author></authors><title>Joint LDPC and Physical-layer Network Coding for Asynchronous
  Bi-directional Relaying</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures, submitted to IEEE Journal on Selected Areas in
  Communications - Special Issue on Theories and Methods for Advanced Wireless
  Relays</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical asynchronous bi-directional relaying, symbols transmitted by two
sources cannot arrive at the relay with perfect frame and symbol alignments and
the asynchronous multiple-access channel (MAC) should be seriously considered.
Recently, Lu et al. proposed a Tanner-graph representation of the
symbol-asynchronous MAC with rectangular-pulse shaping and further developed
the message-passing algorithm for optimal decoding of the symbol-asynchronous
physical-layer network coding. In this paper, we present a general channel
model for the asynchronous MAC with arbitrary pulse-shaping. Then, the Bahl,
Cocke, Jelinek, and Raviv (BCJR) algorithm is developed for optimal decoding of
the asynchronous MAC channel. For Low-Density Parity-Check (LDPC)-coded BPSK
signalling over the symbol-asynchronous MAC, we present a formal log-domain
generalized sum-product-algorithm (Log-G-SPA) for efficient decoding.
Furthermore, we propose to use cyclic codes for combating the
frame-asynchronism and the resolution of the relative delay inherent in this
approach can be achieved by employing the simple cyclic-redundancy-check (CRC)
coding technique. Simulation results demonstrate the effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5651</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5651</id><created>2011-06-28</created><updated>2015-01-06</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Hyperincursive Cogitata and Incursive Cogitantes: Scholarly Discourse as
  a Strongly Anticipatory System</title><categories>cs.DL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1011.3244</comments><journal-ref>International Journal of Computing Anticipatory Systems, 28,
  173-186</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly anticipatory systems-that is, systems which use models of themselves
for their further development-and which additionally may be able to run
hyperincursive routines-that is, develop only with reference to their future
states-cannot exist in res extensa, but can only be envisaged in res cogitans.
One needs incursive routines in cogitantes to instantiate these systems. Unlike
historical systems (with recursion), these hyper-incursive routines generate
redundancies by opening horizons of other possible states. Thus, intentional
systems can enrich our perceptions of the cases that have happened to occur.
The perspective of hindsight codified at the above-individual level enables us
furthermore to intervene technologically. The theory and computation of
anticipatory systems have made these loops between supra-individual
hyper-incursion, individual incursion (in instantiation), and historical
recursion accessible for modeling and empirical investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5675</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5675</id><created>2011-06-28</created><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Altenbach</keyname><forenames>Fabian</forenames></author><author><keyname>Malsbender</keyname><forenames>Martina</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>Writing on the Facade of RWTH ICT Cubes: Cost Constrained Geometric
  Huffman Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISWCS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a coding technique called cost constrained Geometric Huffman
coding (ccGhc) is developed. ccGhc minimizes the Kullback-Leibler distance
between a dyadic probability mass function (pmf) and a target pmf subject to an
affine inequality constraint. An analytical proof is given that when ccGhc is
applied to blocks of symbols, the optimum is asymptotically achieved when the
blocklength goes to infinity. The derivation of ccGhc is motivated by the
problem of encoding a text to a sequence of slats subject to architectural
design criteria. For the considered architectural problem, for a blocklength of
3, the codes found by ccGhc match the design criteria. For communications
channels with average cost constraints, ccGhc can be used to efficiently find
prefix-free modulation codes that are provably capacity achieving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5683</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5683</id><created>2011-06-28</created><authors><author><keyname>Ma</keyname><forenames>Yanjun</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Chen</keyname><forenames>Rui</forenames></author></authors><title>Distributed Interference Alignment with Low Overhead</title><categories>cs.IT math.IT</categories><comments>3 pages, 3 figs, submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on closed-form interference alignment (IA) solutions, a low overhead
distributed interference alignment (LOIA) scheme is proposed in this paper for
the $K$-user SISO interference channel, and extension to multiple antenna
scenario is also considered. Compared with the iterative interference alignment
(IIA) algorithm proposed by Gomadam et al., the overhead is greatly reduced.
Simulation results show that the IIA algorithm is strictly suboptimal compared
with our LOIA algorithm in the overhead-limited scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5694</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5694</id><created>2011-06-28</created><updated>2011-06-30</updated><authors><author><keyname>Roverso</keyname><forenames>Roberto</forenames></author><author><keyname>Naiem</keyname><forenames>Amgad</forenames></author><author><keyname>El-Beltagy</keyname><forenames>Mohammed</forenames></author><author><keyname>El-Ansary</keyname><forenames>Sameh</forenames></author></authors><title>GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under
  Real-time Constraints</title><categories>cs.DC cs.MS cs.PF math.OC</categories><comments>White Paper, Peerialism Inc. (www.peerialism.com)</comments><acm-class>G.1.6; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we modify a fast heuristic solver for the Linear Sum Assignment
Problem (LSAP) for use on Graphical Processing Units (GPUs). The motivating
scenario is an industrial application for P2P live streaming that is moderated
by a central node which is periodically solving LSAP instances for assigning
peers to one another. The central node needs to handle LSAP instances involving
thousands of peers in as near to real-time as possible. Our findings are
generic enough to be applied in other contexts. Our main result is a parallel
version of a heuristic algorithm called Deep Greedy Switching (DGS) on GPUs
using the CUDA programming language. DGS sacrifices absolute optimality in
favor of low computation time and was designed as an alternative to classical
LSAP solvers such as the Hungarian and auctioning methods. The contribution of
the paper is threefold: First, we present the process of trial and error we
went through, in the hope that our experience will be beneficial to adopters of
GPU programming for similar problems. Second, we show the modifications needed
to parallelize the DGS algorithm. Third, we show the performance gains of our
approach compared to both a sequential CPU-based implementation of DGS and a
parallel GPU-based implementation of the auctioning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5700</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5700</id><created>2011-06-28</created><authors><author><keyname>Duret-Lutz</keyname><forenames>Alexandre</forenames></author><author><keyname>Klai</keyname><forenames>Kais</forenames></author><author><keyname>Poitrenaud</keyname><forenames>Denis</forenames></author><author><keyname>Thierry-Mieg</keyname><forenames>Yann</forenames></author></authors><title>Combining Explicit and Symbolic Approaches for Better On-the-Fly LTL
  Model Checking</title><categories>cs.LO cs.FL</categories><comments>Extended version of the paper titled &quot;Self-loop aggregation product -
  a new hybrid approach to on-the-fly LTL model checking&quot; to appear in
  Proceedings of the 9th International Symposium on Automated Technology for
  Verification and Analysis (ATVA'11), Lecture Notes in Computer Science,
  Taipei, Taiwan, October 2011. Springer</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two new hybrid techniques that replace the synchronized product
used in the automata-theoretic approach for LTL model checking. The proposed
products are explicit graphs of aggregates (symbolic sets of states) that can
be interpreted as B\&quot;uchi automata. These hybrid approaches allow on the one
hand to use classical emptiness-check algorithms and build the graph
on-the-fly, and on the other hand, to have a compact encoding of the state
space thanks to the symbolic representation of the aggregates. The Symbolic
Observation Product assumes a globally stuttering property (e.g., LTL \ X) to
aggregate states. The Self-Loop Aggregation Product} does not require the
property to be globally stuttering (i.e., it can tackle full LTL), but
dynamically detects and exploits a form of stuttering where possible. Our
experiments show that these two variants, while incomparable with each other,
can outperform other existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5714</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5714</id><created>2011-06-28</created><updated>2011-07-28</updated><authors><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Cruise</keyname><forenames>James</forenames></author><author><keyname>Ganesh</keyname><forenames>Ayalvadi</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>Non-parametric change-point detection using string matching algorithms</title><categories>math.PR cs.IT math.IT stat.ME</categories><journal-ref>Methodology and Computing in Applied Probability. 16(4) p.
  987-1008 (2014)</journal-ref><doi>10.1007/s11009-013-9359-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the output of a data source taking values in a finite alphabet, we wish
to detect change-points, that is times when the statistical properties of the
source change. Motivated by ideas of match lengths in information theory, we
introduce a novel non-parametric estimator which we call CRECHE (CRossings
Enumeration CHange Estimator). We present simulation evidence that this
estimator performs well, both for simulated sources and for real data formed by
concatenating text sources. For example, we show that we can accurately detect
the point at which a source changes from a Markov chain to an IID source with
the same stationary distribution. Our estimator requires no assumptions about
the form of the source distribution, and avoids the need to estimate its
probabilities. Further, we establish consistency of the CRECHE estimator under
a related toy model, by establishing a fluid limit and using martingale
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5723</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5723</id><created>2011-06-28</created><updated>2012-04-22</updated><authors><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lasserre</keyname><forenames>Jean</forenames></author><author><keyname>Pasechnik</keyname><forenames>Dmitrii</forenames></author><author><keyname>Robins</keyname><forenames>Sinai</forenames></author></authors><title>The inverse moment problem for convex polytopes</title><categories>math.NA cs.CG math.AG math.CO</categories><comments>LaTeX2e, 24 pages including 1 appendix</comments><msc-class>52B11, 42B10, 47B35</msc-class><acm-class>F.2.1; F.2.2</acm-class><journal-ref>Discrete and Computational Geometry, 48(2012), pp. 596--621</journal-ref><doi>10.1007/s00454-012-9426-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to present a general and novel approach for the
reconstruction of any convex d-dimensional polytope P, from knowledge of its
moments. In particular, we show that the vertices of an N-vertex polytope in
R^d can be reconstructed from the knowledge of O(DN) axial moments (w.r.t. to
an unknown polynomial measure od degree D) in d+1 distinct generic directions.
Our approach is based on the collection of moment formulas due to Brion,
Lawrence, Khovanskii-Pukhikov, and Barvinok that arise in the discrete geometry
of polytopes, and what variously known as Prony's method, or Vandermonde
factorization of finite rank Hankel matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5730</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5730</id><created>2011-06-28</created><updated>2011-11-11</updated><authors><author><keyname>Niu</keyname><forenames>Feng</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Re</keyname><forenames>Christopher</forenames></author><author><keyname>Wright</keyname><forenames>Stephen J.</forenames></author></authors><title>HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient
  Descent</title><categories>math.OC cs.LG</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve
state-of-the-art performance on a variety of machine learning tasks. Several
researchers have recently proposed schemes to parallelize SGD, but all require
performance-destroying memory locking and synchronization. This work aims to
show using novel theoretical analysis, algorithms, and implementation that SGD
can be implemented without any locking. We present an update scheme called
HOGWILD! which allows processors access to shared memory with the possibility
of overwriting each other's work. We show that when the associated optimization
problem is sparse, meaning most gradient updates only modify small parts of the
decision variable, then HOGWILD! achieves a nearly optimal rate of convergence.
We demonstrate experimentally that HOGWILD! outperforms alternative schemes
that use locking by an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5736</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5736</id><created>2011-06-28</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Eisenstat</keyname><forenames>Sarah</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Algorithms for Solving Rubik's Cubes</title><categories>cs.DS cs.CC cs.CG math.CO</categories><comments>34 pages, 9 figures. A short version of this paper is to appear at
  the 19th Annual European Symposium on Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rubik's Cube is perhaps the world's most famous and iconic puzzle,
well-known to have a rich underlying mathematical structure (group theory). In
this paper, we show that the Rubik's Cube also has a rich underlying
algorithmic structure. Specifically, we show that the n x n x n Rubik's Cube,
as well as the n x n x 1 variant, has a &quot;God's Number&quot; (diameter of the
configuration space) of Theta(n^2/log n). The upper bound comes from
effectively parallelizing standard Theta(n^2) solution algorithms, while the
lower bound follows from a counting argument. The upper bound gives an
asymptotically optimal algorithm for solving a general Rubik's Cube in the
worst case. Given a specific starting state, we show how to find the shortest
solution in an n x O(1) x O(1) Rubik's Cube. Finally, we show that finding this
optimal solution becomes NP-hard in an n x n x 1 Rubik's Cube when the
positions and colors of some of the cubies are ignored (not used in determining
whether the cube is solved).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5737</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5737</id><created>2011-06-23</created><authors><author><keyname>Bennet</keyname><forenames>D.</forenames></author><author><keyname>Perumal</keyname><forenames>Dr. S. Arumuga</forenames></author></authors><title>Fingerprint: DWT, SVD Based Enhancement and Significant Contrast for
  Ridges and Valleys Using Fuzzy Measures</title><categories>cs.CV</categories><comments>Submitted to Journal of Computer Science and Engineering, see
  http://sites.google.com/site/jcseuk/volume-6-issue-1-march</comments><journal-ref>Journal of Computer Science and Engineering, Volume 6, Issue 1,
  p28-32, March 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of the Fingerprint recognition system will be more accurate
with respect of enhancement for the fingerprint images. In this paper we
develop a novel method for Fingerprint image contrast enhancement technique
based on the discrete wavelet transform (DWT) and singular value decomposition
(SVD) has been proposed. This technique is compared with conventional image
equalization techniques such as standard general histogram equalization and
local histogram equalization. An automatic histogram threshold approach based
on a fuzziness measure is presented. Then, using an index of fuzziness, a
similarity process is started to find the threshold point. A significant
contrast between ridges and valleys of the best, medium and poor finger image
features to extract from finger images and get maximum recognition rate using
fuzzy measures. The experimental results show the recognition of superiority of
the proposed method to get maximum performance up gradation to the
implementation of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5742</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5742</id><created>2011-06-28</created><updated>2012-02-05</updated><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Wireless Network Coding with Local Network Views: Coded Layer Scheduling</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental challenges in the design of distributed wireless
networks is the large dynamic range of network state. Since continuous tracking
of global network state at all nodes is practically impossible, nodes can only
acquire limited local views of the whole network to design their transmission
strategies. In this paper, we study multi-layer wireless networks and assume
that each node has only a limited knowledge, namely 1-local view, where each
S-D pair has enough information to perform optimally when other pairs do not
interfere, along with connectivity information for rest of the network. We
investigate the information-theoretic limits of communication with such limited
knowledge at the nodes. We develop a novel transmission strategy, namely Coded
Layer Scheduling, that solely relies on 1-local view at the nodes and
incorporates three different techniques: (1) per layer interference avoidance,
(2) repetition coding to allow overhearing of the interference, and (3) network
coding to allow interference neutralization. We show that our proposed scheme
can provide a significant throughput gain compared with the conventional
interference avoidance strategies. Furthermore, we show that our strategy
maximizes the achievable normalized sum-rate for some classes of networks,
hence, characterizing the normalized sum-capacity of those networks with
1-local view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5767</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5767</id><created>2011-06-28</created><updated>2011-07-11</updated><authors><author><keyname>Henshall</keyname><forenames>D.</forenames></author><author><keyname>Rampersad</keyname><forenames>N.</forenames></author><author><keyname>Shallit</keyname><forenames>J.</forenames></author></authors><title>Shuffling and Unshuffling</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider various shuffling and unshuffling operations on languages and
words, and examine their closure properties. Although the main goal is to
provide some good and novel exercises and examples for undergraduate formal
language theory classes, we also provide some new results and some open
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5778</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5778</id><created>2011-06-28</created><updated>2012-02-20</updated><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Effective Theories for Circuits and Automata</title><categories>q-bio.QM cs.FL nlin.AO</categories><comments>11 pages, 9 figures</comments><report-no>SFI Working Paper #11-09-47</report-no><journal-ref>Chaos 21, 037106 (2011)</journal-ref><doi>10.1063/1.3640747</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstracting an effective theory from a complicated process is central to the
study of complexity. Even when the underlying mechanisms are understood, or at
least measurable, the presence of dissipation and irreversibility in
biological, computational and social systems makes the problem harder. Here we
demonstrate the construction of effective theories in the presence of both
irreversibility and noise, in a dynamical model with underlying feedback. We
use the Krohn-Rhodes theorem to show how the composition of underlying
mechanisms can lead to innovations in the emergent effective theory. We show
how dissipation and irreversibility fundamentally limit the lifetimes of these
emergent structures, even though, on short timescales, the group properties may
be enriched compared to their noiseless counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5793</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5793</id><created>2011-06-28</created><authors><author><keyname>Hu</keyname><forenames>Dandan</forenames></author><author><keyname>Ronhovde</keyname><forenames>Peter</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>A Replica Inference Approach to Unsupervised Multi-Scale Image
  Segmentation</title><categories>cond-mat.stat-mech cs.CV physics.soc-ph</categories><comments>26 pages, 22 figures</comments><journal-ref>Phys. Rev. E 85, 016101 (2012)</journal-ref><doi>10.1103/PhysRevE.85.016101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a replica inference based Potts model method to unsupervised image
segmentation on multiple scales. This approach was inspired by the statistical
mechanics problem of &quot;community detection&quot; and its phase diagram. Specifically,
the problem is cast as identifying tightly bound clusters (&quot;communities&quot; or
&quot;solutes&quot;) against a background or &quot;solvent&quot;. Within our multiresolution
approach, we compute information theory based correlations among multiple
solutions (&quot;replicas&quot;) of the same graph over a range of resolutions.
Significant multiresolution structures are identified by replica correlations
as manifest in information theory overlaps. With the aid of these correlations
as well as thermodynamic measures, the phase diagram of the corresponding Potts
model is analyzed both at zero and finite temperatures. Optimal parameters
corresponding to a sensible unsupervised segmentation correspond to the &quot;easy
phase&quot; of the Potts model. Our algorithm is fast and shown to be at least as
accurate as the best algorithms to date and to be especially suited to the
detection of camouflaged images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5815</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5815</id><created>2011-06-28</created><authors><author><keyname>Aguilar</keyname><forenames>Cesar O.</forenames></author><author><keyname>Krener</keyname><forenames>Arthur J.</forenames></author></authors><title>Patchy Solution of a Francis-Byrnes-Isidori Partial Differential
  Equation</title><categories>math.OC cs.SY math.DS</categories><comments>21 pages, 12 figures</comments><msc-class>93C10, 34C45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The solution to the nonlinear output regulation problem requires one to solve
a first order PDE, known as the Francis-Byrnes-Isidori (FBI) equations. In this
paper we propose a method to compute approximate solutions to the FBI equations
when the zero dynamics of the plant are hyperbolic and the exosystem is
two-dimensional. With our method we are able to produce approximations that
converge uniformly to the true solution. Our method relies on the periodic
nature of two-dimensional analytic center manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5818</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5818</id><created>2011-06-28</created><authors><author><keyname>Wu</keyname><forenames>Jinn-Wen</forenames></author><author><keyname>Luo</keyname><forenames>Yu-Pin</forenames></author><author><keyname>Huang</keyname><forenames>Ming-Chang</forenames></author></authors><title>Characterizing the process of reaching consensus for social systems</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel way of characterizing the process of reaching consensus for a social
system is given. The foundation of the characterization is based on the theorem
which states that the sufficient and necessary condition for a system to reach
the state of consensus is the occurrence of communicators, defined as the units
of the system that can directly communicate with all the others simultaneously.
A model is proposed to illustrate the characterization explicitly. The
existence of communicators provides an efficient way for unifying two systems
that a state of consensus is guaranteed after the mergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5825</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5825</id><created>2011-06-28</created><authors><author><keyname>Xia</keyname><forenames>Ping</forenames></author><author><keyname>Jo</keyname><forenames>Han-Shin</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Fundamentals of Inter-cell Overhead Signaling in Heterogeneous Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>21 pages, 9 figures</comments><doi>10.1109/JSTSP.2011.2181939</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous base stations (e.g. picocells, microcells, femtocells and
distributed antennas) will become increasingly essential for cellular network
capacity and coverage. Up until now, little basic research has been done on the
fundamentals of managing so much infrastructure -- much of it unplanned --
together with the carefully planned macro-cellular network. Inter-cell
coordination is in principle an effective way of ensuring different
infrastructure components behave in a way that increases, rather than
decreases, the key quality of service (QoS) metrics. The success of such
coordination depends heavily on how the overhead is shared, and the rate and
delay of the overhead sharing. We develop a novel framework to quantify
overhead signaling for inter-cell coordination, which is usually ignored in
traditional 1-tier networks, and assumes even more importance in multi-tier
heterogeneous cellular networks (HCNs). We derive the overhead quality contour
for general K-tier HCNs -- the achievable set of overhead packet rate, size,
delay and outage probability -- in closed-form expressions or computable
integrals under general assumptions on overhead arrivals and different overhead
signaling methods (backhaul and/or wireless). The overhead quality contour is
further simplified for two widely used models of overhead arrivals: Poisson and
deterministic arrival process. This framework can be used in the design and
evaluation of any inter-cell coordination scheme. It also provides design
insights on backhaul and wireless overhead channels to handle specific overhead
signaling requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5826</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5826</id><created>2011-06-28</created><authors><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Ravikumar</keyname><forenames>Pradeep</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>A Dirty Model for Multiple Sparse Regression</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>The primary result is accepted to NIPS 2010 for Oral Presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse linear regression -- finding an unknown vector from linear
measurements -- is now known to be possible with fewer samples than variables,
via methods like the LASSO. We consider the multiple sparse linear regression
problem, where several related vectors -- with partially shared support sets --
have to be recovered. A natural question in this setting is whether one can use
the sharing to further decrease the overall number of samples required. A line
of recent research has studied the use of \ell_1/\ell_q norm
block-regularizations with q&gt;1 for such problems; however these could actually
perform worse in sample complexity -- vis a vis solving each problem separately
ignoring sharing -- depending on the level of sharing.
  We present a new method for multiple sparse linear regression that can
leverage support and parameter overlap when it exists, but not pay a penalty
when it does not. A very simple idea: we decompose the parameters into two
components and regularize these differently. We show both theoretically and
empirically, our method strictly and noticeably outperforms both \ell_1 or
\ell_1/\ell_q methods, over the entire range of possible overlaps (except at
boundary cases, where we match the best method). We also provide theoretical
guarantees that the method performs well under high-dimensional scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5829</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5829</id><created>2011-06-28</created><authors><author><keyname>Hollinger</keyname><forenames>Geoffrey A.</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author><author><keyname>Sukhatme</keyname><forenames>Gaurav S.</forenames></author></authors><title>Active Classification: Theory and Application to Underwater Inspection</title><categories>cs.RO cs.AI cs.CV</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the problem in which an autonomous vehicle must classify an object
based on multiple views. We focus on the active classification setting, where
the vehicle controls which views to select to best perform the classification.
The problem is formulated as an extension to Bayesian active learning, and we
show connections to recent theoretical guarantees in this area. We formally
analyze the benefit of acting adaptively as new information becomes available.
The analysis leads to a probabilistic algorithm for determining the best views
to observe based on information theoretic costs. We validate our approach in
two ways, both related to underwater inspection: 3D polyhedra recognition in
synthetic depth maps and ship hull inspection with imaging sonar. These tasks
encompass both the planning and recognition aspects of the active
classification problem. The results demonstrate that actively planning for
informative views can reduce the number of necessary views by up to 80% when
compared to passive methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5841</identifier>
 <datestamp>2012-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5841</id><created>2011-06-29</created><updated>2012-01-10</updated><authors><author><keyname>Kabir</keyname><forenames>P.</forenames></author><author><keyname>Shafinia</keyname><forenames>M. H.</forenames></author><author><keyname>Marvasti</keyname><forenames>F.</forenames></author><author><keyname>Pad</keyname><forenames>P.</forenames></author></authors><title>Capacity Bounds of Finite Dimensional CDMA Systems with Fading/Near-Far
  Effects and Power Control</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with fading and/or near-far effects with or without power
control on the evaluation of the sum capacity of finite dimensional Code
Division Multiple Access (CDMA) systems for binary and finite nonbinary inputs
and signature matrices. Important results of this paper are that the knowledge
of the received power variations due to input power differences, fading and/or
near-far effects can significantly improve the sum capacity. Also traditional
power controls can not improve the sum capacity; for the asymptotic case, any
type of power control on the near-far effects is equivalent to the case without
any power control. Moreover, for the asymptotic case, we have developed a
method that determines bounds for the fading/near-far sum capacity with
imperfect power estimation from the actual sum capacity of a CDMA system with
perfect power estimation. To show the power and utility of the results, a
number of sum capacity bounds for special cases are numerically evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5845</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5845</id><created>2011-06-29</created><authors><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author><author><keyname>Izumi</keyname><forenames>Tomoko</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Wada</keyname><forenames>Koichi</forenames></author></authors><title>Minimum Certificate Dispersal with Tree Structures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an n-vertex graph G=(V,E) and a set R \subseteq {{x,y} | x,y \in V} of
requests, we consider to assign a set of edges to each vertex in G so that for
every request {u, v} in R the union of the edge sets assigned to u and v
contains a path from u to v. The Minimum Certificate Dispersal Problem (MCD) is
defined as one to find an assignment that minimizes the sum of the cardinality
of the edge set assigned to each vertex. This problem has been shown to be
LOGAPX-complete for the most general setting, and APX-hard and 2-approximable
in polynomial time for dense request sets, where R forms a clique. In this
paper, we investigate the complexity of MCD with sparse (tree) structures. We
first show that MCD is APX-hard when R is a tree, even a star. We then explore
the problem from the viewpoint of the maximum degree \Delta of the tree: MCD
for tree request set with constant \Delta is solvable in polynomial time, while
that with \Delta=\Omega(n) is 2.56-approximable in polynomial time but hard to
approximate within 1.01 unless P=NP. As for the structure of G itself, we show
that the problem can be solved in polynomial time if G is a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5846</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5846</id><created>2011-06-29</created><authors><author><keyname>Costan</keyname><forenames>Alexandru</forenames></author><author><keyname>Pop</keyname><forenames>Florin</forenames></author><author><keyname>Stratan</keyname><forenames>Corina</forenames></author><author><keyname>Dobre</keyname><forenames>Ciprian</forenames></author><author><keyname>Leordeanu</keyname><forenames>Catalin</forenames></author><author><keyname>Cristea</keyname><forenames>Valentin</forenames></author></authors><title>An Architectural Model for a Grid based Workflow Management Platform in
  Scientific Applications</title><categories>cs.DC</categories><comments>17th International Conference on Control Systems and Computer Science
  (CSCS 17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 407-414, ISSN:
  2066-4451</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent increasing computational and data requirements of scientific
applications, the use of large clustered systems as well as distributed
resources is inevitable. Although executing large applications in these
environments brings increased performance, the automation of the process
becomes more and more challenging. While the use of complex workflow management
systems has been a viable solution for this automation process in business
oriented environments, the open source engines available for scientific
applications lack some functionalities or are too difficult to use for
non-specialists. In this work we propose an architectural model for a grid
based workflow management platform providing features like an intuitive way to
describe workflows, efficient data handling mechanisms and flexible fault
tolerance support. Our integrated solution introduces a workflow engine
component based on ActiveBPEL extended with additional functionalities and a
scheduling component providing efficient mapping between tasks and available
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5890</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5890</id><created>2011-06-29</created><authors><author><keyname>Law</keyname><forenames>Yat-Chiu</forenames></author><author><keyname>Lee</keyname><forenames>Jimmy Ho-Man</forenames></author><author><keyname>Woo</keyname><forenames>May Hiu-Chun</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>A Comparison of Lex Bounds for Multiset Variables in Constraint
  Programming</title><categories>cs.AI</categories><comments>7 pages, Proceedings of the Twenty-Fifth AAAI Conference on
  Artificial Intelligence (AAAI-11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set and multiset variables in constraint programming have typically been
represented using subset bounds. However, this is a weak representation that
neglects potentially useful information about a set such as its cardinality.
For set variables, the length-lex (LL) representation successfully provides
information about the length (cardinality) and position in the lexicographic
ordering. For multiset variables, where elements can be repeated, we consider
richer representations that take into account additional information. We study
eight different representations in which we maintain bounds according to one of
the eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC),
length-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC)
orderings. These representations integrate together information about the
cardinality, variety (number of distinct elements in the multiset), and
position in some total ordering. Theoretical and empirical comparisons of
expressiveness and compactness of the eight representations suggest that
length-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually
give tighter bounds after constraint propagation. We implement the eight
representations and evaluate them against the subset bounds representation with
cardinality and variety reasoning. Results demonstrate that they offer
significantly better pruning and runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5906</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5906</id><created>2011-06-29</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>L&#xfc;</keyname><forenames>Linyuan</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Reconstructing directed networks for better synchronization</title><categories>physics.soc-ph cs.DS nlin.CD</categories><comments>7 pages, 5 figures</comments><journal-ref>New J. Phys. 14, 083006 (2012)</journal-ref><doi>10.1088/1367-2630/14/8/083006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we studied the strategies to enhance synchronization on
directed networks by manipulating a fixed number of links. We proposed a
centrality-based reconstructing (CBR) method, where the node centrality is
measured by the well-known PageRank algorithm. Extensive numerical simulation
on many modeled networks demonstrated that the CBR method is more effective in
facilitating synchronization than the degree-based reconstructing method and
random reconstructing method for adding or removing links. The reason is that
CBR method can effectively narrow the incoming degree distribution and
reinforce the hierarchical structure of the network. Furthermore, we apply the
CBR method to links rewiring procedure where at each step one link is removed
and one new link is added. The CBR method helps to decide which links should be
removed or added. After several steps, the resulted networks are very close to
the optimal structure from the evolutionary optimization algorithm. The
numerical simulations on the Kuramoto model further demonstrate that our method
has advantage in shortening the convergence time to synchronization on directed
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5908</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5908</id><created>2011-06-29</created><authors><author><keyname>Schubert</keyname><forenames>Gerald</forenames></author><author><keyname>Fehske</keyname><forenames>Holger</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Hybrid-parallel sparse matrix-vector multiplication with explicit
  communication overlap on current multicore-based systems</title><categories>cs.DC</categories><comments>16 pages, 10 figures</comments><journal-ref>Parallel Processing Letters 21(3), 339-358 (2011)</journal-ref><doi>10.1142/S0129626411000254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate optimized parallel sparse matrix-vector operations for several
representative application areas on widespread multicore-based cluster
configurations. First the single-socket baseline performance is analyzed and
modeled with respect to basic architectural properties of standard multicore
chips. Beyond the single node, the performance of parallel sparse matrix-vector
operations is often limited by communication overhead. Starting from the
observation that nonblocking MPI is not able to hide communication cost using
standard MPI implementations, we demonstrate that explicit overlap of
communication and computation can be achieved by using a dedicated
communication thread, which may run on a virtual core. Moreover we identify
performance benefits of hybrid MPI/OpenMP programming due to improved load
balancing even without explicit communication overlap. We compare performance
results for pure MPI, the widely used &quot;vector-like&quot; hybrid programming
strategies, and explicit overlap on a modern multicore-based cluster and a Cray
XE6 system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5917</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5917</id><created>2011-06-29</created><authors><author><keyname>Dundas</keyname><forenames>Jitesh</forenames></author><author><keyname>Chik</keyname><forenames>David</forenames></author></authors><title>Implementing Human-like Intuition Mechanism in Artificial Intelligence</title><categories>cs.AI cs.NE</categories><comments>14 pages with 1 figure + 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human intuition has been simulated by several research projects using
artificial intelligence techniques. Most of these algorithms or models lack the
ability to handle complications or diversions. Moreover, they also do not
explain the factors influencing intuition and the accuracy of the results from
this process. In this paper, we present a simple series based model for
implementation of human-like intuition using the principles of connectivity and
unknown entities. By using Poker hand datasets and Car evaluation datasets, we
compare the performance of some well-known models with our intuition model. The
aim of the experiment was to predict the maximum accurate answers using
intuition based models. We found that the presence of unknown entities,
diversion from the current problem scenario, and identifying weakness without
the normal logic based execution, greatly affects the reliability of the
answers. Generally, the intuition based models cannot be a substitute for the
logic based mechanisms in handling such problems. The intuition can only act as
a support for an ongoing logic based model that processes all the steps in a
sequential manner. However, when time and computational cost are very strict
constraints, this intuition based model becomes extremely important and useful,
because it can give a reasonably good performance. Factors affecting intuition
are analyzed and interpreted through our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5928</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5928</id><created>2011-06-29</created><authors><author><keyname>Gabarda</keyname><forenames>Salvador</forenames></author><author><keyname>Cristobal</keyname><forenames>Gabriel</forenames></author></authors><title>Image denoising assessment using anisotropic stack filtering</title><categories>cs.CV</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a measure of anisotropy as a quality parameter to
estimate the amount of noise in noisy images. The anisotropy of an image can be
determined through a directional measure, using an appropriate statistical
distribution of the information contained in the image. This new measure is
achieved through a stack filtering paradigm. First, we define a local
directional entropy, based on the distribution of 0's and 1's in the
neigborhood of every pixel location of each stack level. Then the entropy
variation of this directional entropy is used to define an anisotropic measure.
The empirical results have shown that this measure can be regarded as an
excellent image noise indicator, which is particularly relevant for quality
assessment of denoising algorithms. The method has been evaluated with
artificial and real-world degraded images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5930</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5930</id><created>2011-06-29</created><updated>2012-01-23</updated><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>Bouyukliev</keyname><forenames>Iliya</forenames></author></authors><title>An Algorithm for Classification of Binary Self-Dual Codes</title><categories>math.CO cs.DS cs.IT math.IT</categories><comments>The title is changed</comments><msc-class>05A05, 11T71, 94B04</msc-class><acm-class>G.2.1; F.2.2</acm-class><journal-ref>IEEE Transactions on Information Theory, vol. 58, pp. 3933--3940,
  2012</journal-ref><doi>10.1109/TIT.2012.2190134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient algorithm for classification of binary self-dual codes is
presented. As an application, a complete classification of the self-dual codes
of length 38 is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5936</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5936</id><created>2011-06-29</created><updated>2012-01-23</updated><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author></authors><title>Singly-even self-dual codes with minimal shadow</title><categories>math.CO cs.IT math.IT</categories><msc-class>94B05, 11T71</msc-class><journal-ref>IEEE Transactions. Inform. Theory, vol. 58, pp. 3856--3860, 2012</journal-ref><doi>10.1109/TIT.2012.2183114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we investigate extremal singly-even self-dual codes with minimal
shadow. For particular parameters we prove non-existence of such codes. By a
result of Rains \cite{Rains-asymptotic}, the length of extremal singly-even
self-dual codes is bounded. We give explicit bounds in case the shadow is
minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5960</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5960</id><created>2011-06-29</created><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>Yankov</keyname><forenames>Nikolay</forenames></author><author><keyname>Russeva</keyname><forenames>Radka</forenames></author></authors><title>On the classification of binary self-dual [44,22,8] codes with an
  automorphism of order 3 or 7</title><categories>math.CO cs.IT math.IT</categories><msc-class>11T71</msc-class><journal-ref>International Journal of Information and Coding Theory (IJICOT),
  vol. 2, pp. 21--37, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All binary self-dual [44,22,8] codes with an automorphism of order 3 or 7 are
classified. In this way we complete the classification of extremal self-dual
codes of length 44 having an automorphism of odd prime order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5962</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5962</id><created>2011-06-29</created><authors><author><keyname>Dur&#xe1;n</keyname><forenames>Francisco</forenames><affiliation>Universidad de M&#xe1;laga</affiliation></author><author><keyname>Rusu</keyname><forenames>Vlad</forenames><affiliation>IRISA-INRIA, Rennes, France</affiliation></author></authors><title>Proceedings Second International Workshop on Algebraic Methods in
  Model-based Software Engineering</title><categories>cs.SE</categories><comments>EPTCS 56, 2011</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.56</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past years there has been quite a lot of activity in the algebraic
community about using algebraic methods for providing support to model-driven
software engineering. The aim of this workshop is to gather researchers working
on the development and application of algebraic methods to provide rigorous
support to model-based software engineering. The topics relevant to the
workshop are all those related to the use of algebraic methods in software
engineering, including but not limited to: formally specifying and verifying
model-based software engineering concepts and related ones (MDE, UML, OCL, MOF,
DSLs, ...); tool support for the above; integration of formal and informal
methods; and theoretical frameworks (algebraic, rewriting-based, category
theory-based, ...). The workshop's main goal is to examine, discuss, and relate
the existing projects within the algebraic community that address common
open-issues in model-driven software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5971</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5971</id><created>2011-06-29</created><updated>2013-10-14</updated><authors><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames></author></authors><title>Perfect Simulation Of Processes With Long Memory: A `Coupling Into And
  From The Past' Algorithm</title><categories>math.PR cs.DS</categories><comments>22 pages, 8 figures</comments><msc-class>60J22</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm for the perfect simulation of variable length
Markov chains and random systems with perfect connections. This algorithm,
which generalizes Propp and Wilson's simulation scheme, is based on the idea of
coupling into and from the past. It improves on existing algorithms by relaxing
the conditions on the kernel and by accelerating convergence, even in the
simple case of finite order Markov chains. Although chains of variable or
infinite order have been widely investigated for decades, their use in applied
probability, from information theory to bio-informatics and linguistics, has
recently led to considerable renewed interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5973</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5973</id><created>2011-06-27</created><authors><author><keyname>Paruchuri</keyname><forenames>Venkata Ravinder</forenames></author></authors><title>Entropy of Telugu</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an investigation of the entropy of the Telugu script.
Since this script is syllabic, and not alphabetic, the computation of entropy
is somewhat complicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5979</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5979</id><created>2011-06-29</created><authors><author><keyname>Ali</keyname><forenames>Mohammed Eunus</forenames></author><author><keyname>Tanin</keyname><forenames>Egemen</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Kotagiri</keyname><forenames>Ramamohanarao</forenames></author></authors><title>Probabilistic Voronoi Diagrams for Probabilistic Moving Nearest Neighbor
  Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large spectrum of applications such as location based services and
environmental monitoring demand efficient query processing on uncertain
databases. In this paper, we propose the probabilistic Voronoi diagram (PVD)
for processing moving nearest neighbor queries on uncertain data, namely the
probabilistic moving nearest neighbor (PMNN) queries. A PMNN query finds the
most probable nearest neighbor of a moving query point continuously. To process
PMNN queries efficiently, we provide two techniques: a pre-computation approach
and an incremental approach. In the pre-computation approach, we develop an
algorithm to efficiently evaluate PMNN queries based on the pre-computed PVD
for the entire data set. In the incremental approach, we propose an incremental
probabilistic safe region based technique that does not require to pre-compute
the whole PVD to answer the PMNN query. In this incremental approach, we
exploit the knowledge for a known region to compute the lower bound of the
probability of an object being the nearest neighbor. Experimental results show
that our approaches significantly outperform a sampling based approach by
orders of magnitude in terms of I/O, query processing time, and communication
overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5988</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5988</id><created>2011-06-05</created><authors><author><keyname>Gkatzikis</keyname><forenames>Lazaros</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios S.</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author></authors><title>The impact of energy constraints on the medium access</title><categories>cs.OH</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary mobile devices are battery powered and due to their shrinking
size and increasing complexity operate on a tight energy budget. Thus, energy
consumption is becoming one of the major concerns regarding the current and
upcoming wireless communication systems. On the other hand, the available
bandwidth resources are limited and modern applications are throughput
demanding, leading thus to strong competition for the medium. In this
direction, we consider a stochastic contention based medium access scheme,
where the devices may choose to turn off for some time in order to save energy.
We perform an analysis for a slotted ALOHA scenario and we show that the energy
constraints, if properly exploited, may reduce contention for the medium. Our
results give valuable insights on the energy--throughput tradeoff for any
contention based system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5992</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5992</id><created>2011-06-29</created><authors><author><keyname>Panisson</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author><author><keyname>Broeck</keyname><forenames>Wouter Van den</forenames></author><author><keyname>Ruffo</keyname><forenames>Giancarlo</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author></authors><title>On the Dynamics of Human Proximity for Data Diffusion in Ad-Hoc Networks</title><categories>cs.NI cs.HC cs.SI physics.soc-ph</categories><comments>A. Panisson et al., On the dynamics of human proximity for data
  diffusion in ad-hoc networks, Ad Hoc Netw. (2011)</comments><journal-ref>Ad Hoc Networks Volume 10, Issue 8, November 2012, Pages
  1532--1543</journal-ref><doi>10.1016/j.adhoc.2011.06.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on a data-driven investigation aimed at understanding the dynamics
of message spreading in a real-world dynamical network of human proximity. We
use data collected by means of a proximity-sensing network of wearable sensors
that we deployed at three different social gatherings, simultaneously involving
several hundred individuals. We simulate a message spreading process over the
recorded proximity network, focusing on both the topological and the temporal
properties. We show that by using an appropriate technique to deal with the
temporal heterogeneity of proximity events, a universal statistical pattern
emerges for the delivery times of messages, robust across all the data sets.
Our results are useful to set constraints for generic processes of data
dissemination, as well as to validate established models of human mobility and
proximity that are frequently used to simulate realistic behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5995</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5995</id><created>2011-06-18</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author><author><keyname>Balas</keyname><forenames>Valentina E.</forenames></author></authors><title>From Cognitive Binary Logic to Cognitive Intelligent Agents</title><categories>cs.AI cs.LO math.LO</categories><comments>3 figures, 4 pages, latest version: http://fmi.spiruharet.ro/bodorin/</comments><msc-class>03B70, 97E30, 03F03</msc-class><acm-class>F.4.1</acm-class><journal-ref>Proc. 14th Int. Conf. on Intelligent Engineering Systems, pp.
  337-340, Conference Publishing Services - IEEE Computer Society, ISBN
  978-1-4244-7651-0, May 2010</journal-ref><doi>10.1109/INES.2010.5483820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation between self awareness and intelligence is an open problem these
days. Despite the fact that self awarness is usually related to Emotional
Intelligence, this is not the case here. The problem described in this paper is
how to model an agent which knows (Cognitive) Binary Logic and which is also
able to pass (without any mistake) a certain family of Turing Tests designed to
verify its knowledge and its discourse about the modal states of truth
corresponding to well-formed formulae within the language of Propositional
Binary Logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.5998</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.5998</id><created>2011-06-29</created><authors><author><keyname>Fox</keyname><forenames>M.</forenames></author><author><keyname>Long</keyname><forenames>D.</forenames></author></authors><title>The 3rd International Planning Competition: Results and Analysis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  1-59, 2003</journal-ref><doi>10.1613/jair.1240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports the outcome of the third in the series of biennial
international planning competitions, held in association with the International
Conference on AI Planning and Scheduling (AIPS) in 2002. In addition to
describing the domains, the planners and the objectives of the competition, the
paper includes analysis of the results. The results are analysed from several
perspectives, in order to address the questions of comparative performance
between planners, comparative difficulty of domains, the degree of agreement
between planners about the relative difficulty of individual problem instances
and the question of how well planners scale relative to one another over
increasingly difficult problems. The paper addresses these questions through
statistical analysis of the raw results of the competition, in order to
determine which results can be considered to be adequately supported by the
data. The paper concludes with a discussion of some challenges for the future
of the competition series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6022</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6022</id><created>2011-06-29</created><authors><author><keyname>Birmingham</keyname><forenames>W. P.</forenames></author><author><keyname>Durfee</keyname><forenames>E. H.</forenames></author><author><keyname>Park</keyname><forenames>S.</forenames></author></authors><title>Use of Markov Chains to Design an Agent Bidding Strategy for Continuous
  Double Auctions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  175-214, 2004</journal-ref><doi>10.1613/jair.1466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computational agents are developed for increasingly complicated e-commerce
applications, the complexity of the decisions they face demands advances in
artificial intelligence techniques. For example, an agent representing a seller
in an auction should try to maximize the seller?s profit by reasoning about a
variety of possibly uncertain pieces of information, such as the maximum prices
various buyers might be willing to pay, the possible prices being offered by
competing sellers, the rules by which the auction operates, the dynamic arrival
and matching of offers to buy and sell, and so on. A naive application of
multiagent reasoning techniques would require the seller?s agent to explicitly
model all of the other agents through an extended time horizon, rendering the
problem intractable for many realistically-sized problems. We have instead
devised a new strategy that an agent can use to determine its bid price based
on a more tractable Markov chain model of the auction process. We have
experimentally identified the conditions under which our new strategy works
well, as well as how well it works in comparison to the optimal performance the
agent could have achieved had it known the future. Our results show that our
new strategy in general performs well, outperforming other tractable heuristic
strategies in a majority of experiments, and is particularly effective in a
'seller?s market', where many buy offers are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6024</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6024</id><created>2011-06-29</created><authors><author><keyname>Mukherjee</keyname><forenames>Indraneel</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>The Rate of Convergence of AdaBoost</title><categories>math.OC cs.AI stat.ML</categories><comments>A preliminary version will appear in COLT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AdaBoost algorithm was designed to combine many &quot;weak&quot; hypotheses that
perform slightly better than random guessing into a &quot;strong&quot; hypothesis that
has very low error. We study the rate at which AdaBoost iteratively converges
to the minimum of the &quot;exponential loss.&quot; Unlike previous work, our proofs do
not require a weak-learning assumption, nor do they require that minimizers of
the exponential loss are finite. Our first result shows that at iteration $t$,
the exponential loss of AdaBoost's computed parameter vector will be at most
$\epsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by
$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\epsilon$.
We also provide lower bounds showing that a polynomial dependence on these
parameters is necessary. Our second result is that within $C/\epsilon$
iterations, AdaBoost achieves a value of the exponential loss that is at most
$\epsilon$ more than the best possible value, where $C$ depends on the dataset.
We show that this dependence of the rate on $\epsilon$ is optimal up to
constant factors, i.e., at least $\Omega(1/\epsilon)$ rounds are necessary to
achieve within $\epsilon$ of the optimal exponential loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6037</identifier>
 <datestamp>2011-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6037</id><created>2011-06-29</created><updated>2011-07-28</updated><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>LIF</affiliation></author><author><keyname>Das</keyname><forenames>Shantanu</forenames><affiliation>LIF</affiliation></author><author><keyname>Labourel</keyname><forenames>Arnaud</forenames><affiliation>LIF</affiliation></author><author><keyname>Markou</keyname><forenames>Euripides</forenames></author></authors><title>Black Hole Search with Finite Automata Scattered in a Synchronous Torus</title><categories>cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of locating a black hole in synchronous anonymous
networks using finite state agents. A black hole is a harmful node in the
network that destroys any agent visiting that node without leaving any trace.
The objective is to locate the black hole without destroying too many agents.
This is difficult to achieve when the agents are initially scattered in the
network and are unaware of the location of each other. Previous studies for
black hole search used more powerful models where the agents had non-constant
memory, were labelled with distinct identifiers and could either write messages
on the nodes of the network or mark the edges of the network. In contrast, we
solve the problem using a small team of finite-state agents each carrying a
constant number of identical tokens that could be placed on the nodes of the
network. Thus, all resources used in our algorithms are independent of the
network size. We restrict our attention to oriented torus networks and first
show that no finite team of finite state agents can solve the problem in such
networks, when the tokens are not movable. In case the agents are equipped with
movable tokens, we determine lower bounds on the number of agents and tokens
required for solving the problem in torus networks of arbitrary size. Further,
we present a deterministic solution to the black hole search problem for
oriented torus networks, using the minimum number of agents and tokens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6061</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6061</id><created>2011-06-29</created><updated>2013-09-20</updated><authors><author><keyname>Eschen</keyname><forenames>Elaine M.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoqiang</forenames></author></authors><title>Algorithms for Unipolar and Generalized Split Graphs</title><categories>cs.DM</categories><comments>Please cite this article in press as: E.M. Eschen, X. Wang,
  Algorithms for unipolar and generalized split graphs. Discrete Applied
  Mathematics (2013),http://dx.doi.org/10.1016/j.dam.2013.08011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G=(V,E)$ is a {\it unipolar graph} if there exits a partition $V=V_1
\cup V_2$ such that, $V_1$ is a clique and $V_2$ induces the disjoint union of
cliques. The complement-closed class of {\it generalized split graphs} are
those graphs $G$ such that either $G$ {\it or} the complement of $G$ is
unipolar. Generalized split graphs are a large subclass of perfect graphs. In
fact, it has been shown that almost all $C_5$-free (and hence, almost all
perfect graphs) are generalized split graphs. In this paper we present a
recognition algorithm for unipolar graphs that utilizes a minimal triangulation
of the given graph, and produces a partition when one exists. Our algorithm has
running time O($nm^\prime$), where $m^\prime$ is the number of edges in a
minimal triangulation of the given graph. Generalized split graphs can
recognized via this algorithm in O($nm' + n\OL{m}'$) = O($n^3$) time. We give
algorithms on unipolar graphs for finding a maximum independent set and a
minimum clique cover in O($n+m$) time and for finding a maximum clique and a
minimum proper coloring in O($n^{2.5}/\log n$), when a unipolar partition is
given. These algorithms yield algorithms for the four optimization problems on
generalized split graphs that have the same worst-case time bound. We also
prove that the perfect code problem is NP-Complete for unipolar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6062</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6062</id><created>2011-06-29</created><updated>2011-07-01</updated><authors><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author></authors><title>The Life and Death of Unwanted Bits: Towards Proactive Waste Data
  Management in Digital Ecosystems</title><categories>cs.ET</categories><comments>Fixed references</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Our everyday data processing activities create massive amounts of data. Like
physical waste and trash, unwanted and unused data also pollutes the digital
environment by degrading the performance and capacity of storage systems and
requiring costly disposal. In this paper, we propose using the lessons from
real life waste management in handling waste data. We show the impact of waste
data on the performance and operational costs of our computing systems. To
allow better waste data management, we define a waste hierarchy for digital
objects and provide insights into how to identify and categorize waste data.
Finally, we introduce novel ways of reusing, reducing, and recycling data and
software to minimize the impact of data wastage
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6069</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6069</id><created>2011-06-29</created><authors><author><keyname>Chintakunta</keyname><forenames>Harish</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author></authors><title>Topological Fidelity in Sensor Networks</title><categories>cs.NI cs.DC</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor Networks are inherently complex networks, and many of their associated
problems require analysis of some of their global characteristics. These are
primarily affected by the topology of the network. We present in this paper, a
general framework for a topological analysis of a network, and develop
distributed algorithms in a generalized combinatorial setting in order to solve
two seemingly unrelated problems, 1) Coverage hole detection and Localization
and 2) Worm hole attack detection and Localization. We also note these
solutions remain coordinate free as no priori localization information of the
nodes is assumed. For the coverage hole problem, we follow a &quot;divide and
conquer approach&quot;, by strategically dissecting the network so that the overall
topology is preserved, while efficiently pursuing the detection and
localization of failures. The detection of holes, is enabled by first
attributing a combinatorial object called a &quot;Rips Complex&quot; to each network
segment, and by subsequently checking the existence/non-existence of holes by
way of triviality of the first homology class of this complex. Our estimate
exponentially approaches the location of potential holes with each iteration,
yielding a very fast convergence coupled with optimal usage of valuable
resources such as power and memory. We then show a simple extension of the
above problem to address a well known problem in networks, namely the
localization of a worm hole attack. We demonstrate the effectiveness of the
presented algorithm with several substantiating examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6104</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6104</id><created>2011-06-29</created><updated>2013-03-09</updated><authors><author><keyname>Vakili</keyname><forenames>Sattar</forenames></author><author><keyname>Liu</keyname><forenames>Keqin</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Deterministic Sequencing of Exploration and Exploitation for Multi-Armed
  Bandit Problems</title><categories>math.OC cs.LG cs.SY math.PR math.ST stat.TH</categories><comments>22 pages, 2 figures</comments><msc-class>62L05, 93E35 (Primary), 60G40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Multi-Armed Bandit (MAB) problem, there is a given set of arms with
unknown reward models. At each time, a player selects one arm to play, aiming
to maximize the total expected reward over a horizon of length T. An approach
based on a Deterministic Sequencing of Exploration and Exploitation (DSEE) is
developed for constructing sequential arm selection policies. It is shown that
for all light-tailed reward distributions, DSEE achieves the optimal
logarithmic order of the regret, where regret is defined as the total expected
reward loss against the ideal case with known reward models. For heavy-tailed
reward distributions, DSEE achieves O(T^1/p) regret when the moments of the
reward distributions exist up to the pth order for 1&lt;p&lt;=2 and O(T^1/(1+p/2))
for p&gt;2. With the knowledge of an upperbound on a finite moment of the
heavy-tailed reward distributions, DSEE offers the optimal logarithmic regret
order. The proposed DSEE approach complements existing work on MAB by providing
corresponding results for general reward distributions. Furthermore, with a
clearly defined tunable parameter-the cardinality of the exploration sequence,
the DSEE approach is easily extendable to variations of MAB, including MAB with
various objectives, decentralized MAB with multiple players and incomplete
reward observations under collisions, MAB with unknown Markov dynamics, and
combinatorial MAB with dependent arms that often arise in network optimization
problems such as the shortest path, the minimum spanning, and the dominating
set problems under unknown random weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6122</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6122</id><created>2011-06-30</created><authors><author><keyname>Ciprian</keyname><forenames>Dobre</forenames></author><author><keyname>Valentin</keyname><forenames>Cristea</forenames></author><author><keyname>Legrand</keyname><forenames>Iosif C.</forenames></author></authors><title>Simulation Framework for Modeling Large-Scale Distributed Systems</title><categories>cs.DC</categories><comments>International Conference on Control Systems and Computer Science
  (CSCS-14), Ed. Politehnica Press, Bucharest, Romania, pp. 145-149</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation has become the evaluation method of choice for many areas of
distributing computing research. However, most existing simulation packages
have several limitations on the size and complexity of the system being
modeled. Fine grained simulation of complex systems such as Grids requires high
computational effort which can only be obtained by using an underlying
distributed architecture. We are proposing a new distributed simulation system
that has the advantage of being able to model very complex distributed systems
while hiding the computational effort from the end-user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6136</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6136</id><created>2011-06-30</created><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author><author><keyname>Maiti</keyname><forenames>Abyayananda</forenames></author></authors><title>A Comparison of Performance Measures via Online Search</title><categories>cs.DS</categories><comments>IMADA-preprint 2011</comments><msc-class>68Q25</msc-class><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though competitive analysis has been a very useful performance measure for
the quality of online algorithms, it is recognized that it sometimes fails to
distinguish between algorithms of different quality in practice. A number of
alternative measures have been proposed, but, with a few exceptions, these have
generally been applied only to the online problem they were developed in
connection with. Recently, a systematic study of performance measures for
online algorithms was initiated [Boyar, Irani, Larsen: Eleventh International
Algorithms and Data Structures Symposium 2009], first focusing on a simple
server problem. We continue this work by studying a fundamentally different
online problem, online search, and the Reservation Price Policies in
particular. The purpose of this line of work is to learn more about the
applicability of various performance measures in different situations and the
properties that the different measures emphasize. We investigate the following
analysis techniques: Competitive, Relative Worst Order, Bijective, Average,
Relative Interval, Random Order, and Max/Max. In addition to drawing
conclusions on this work, we also investigate the measures' sensitivity to
integral vs. real-valued domains, and as a part of this work, generalize some
of the known performance measures. Finally, we have established the first
optimality proof for Relative Interval Analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6148</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6148</id><created>2011-06-30</created><authors><author><keyname>Teissandier</keyname><forenames>Denis</forenames><affiliation>LMP</affiliation></author><author><keyname>Delos</keyname><forenames>Vincent</forenames><affiliation>LMP</affiliation></author><author><keyname>Cou&#xe9;tard</keyname><forenames>Yves</forenames><affiliation>LMP</affiliation></author></authors><title>Operations on polytopes: application to tolerance analysis</title><categories>cs.CG physics.class-ph</categories><comments>6th CIRP Seminar on CAT, Enschede : Netherlands (1999)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents numerical methods in order to solve problems of
tolerance analysis. A geometric specification, a contact specification and a
functional requirement can be respectively characterized by a finite set of
geometric constraints, a finite set of contact constraints and a finite set of
functional constraints. Mathematically each constraint formalises a n-face
(hyperplan of dimension n) of a n-polytope (1 {\leq} n {\leq} 6). Thus the
relative position between two any surfaces of a mechanism can be calculated
with two operations on polytopes : the Minkowski sum and the Intersection. The
result is a new polytope: the calculated polytope. The inclusion of the
calculated polytope inside the functional polytope indicates if the functional
requirement is satisfied or not satisfied. Examples illustrate these numerical
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6159</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6159</id><created>2011-06-30</created><authors><author><keyname>Dundas</keyname><forenames>Jitesh</forenames></author></authors><title>Understanding Code Patterns - Analysis, Interpretation &amp; Measurement</title><categories>cs.SE cs.PL</categories><comments>Index Terms - code area, code patterns, efficiency, lines of code,
  pattern recognition, software quality; http://www.ijcee.org/papers/008.pdf</comments><journal-ref>International Journal of Computer and Electrical Engineering, Vol.
  1, No. 1, April 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research paper aims to find, analyze and understand code patterns in any
software system and measure its quality by defining standards and proposing a
formula for the same. Every code that is written can be divided into different
code segments, each having its own impact on the overall system. We can analyze
these code segments to get the code quality. The measures used in this paper
include Lines of Code, Number of calls made by a module, Execution time, the
system knowledge of user and developers, the use of generalization,
inheritance, reusability and other object-oriented concepts. The entire
software code is divided into code snippets, based on the logic that they
implement. Each of these code snippets has an impact. This measure is called
Impact Factor and is valued by the software developer and/or other system
stakeholders. Efficiency = (Code Area / Execution Time) * Qr
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6173</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6173</id><created>2011-06-30</created><authors><author><keyname>Wang</keyname><forenames>Xiaowei</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Power and Subcarrier Allocation for Physical-Layer Security in
  OFDMA-based Broadband Wireless Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. on Information Forensics &amp; Security, special
  issue on using the physical layer for securing the next generation of
  communication systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing physical-layer security for mobile users in future broadband
wireless networks is of both theoretical and practical importance. In this
paper, we formulate an analytical framework for resource allocation in a
downlink OFDMA-based broadband network with coexistence of secure users (SU)
and normal users (NU). The SU's require secure data transmission at the
physical layer while the NU's are served with conventional best-effort data
traffic. The problem is formulated as joint power and subcarrier allocation
with the objective of maximizing average aggregate information rate of all NU's
while maintaining an average secrecy rate for each individual SU under a total
transmit power constraint for the base station. We solve this problem in an
asymptotically optimal manner using dual decomposition. Our analysis shows that
an SU becomes a candidate competing for a subcarrier only if its channel gain
on this subcarrier is the largest among all and exceeds the second largest by a
certain threshold. Furthermore, while the power allocation for NU's follows the
conventional water-filling principle, the power allocation for SU's depends on
both its own channel gain and the largest channel gain among others. We also
develop a suboptimal algorithm to reduce the computational cost. Numerical
studies are conducted to evaluate the performance of the proposed algorithms in
terms of the achievable pair of information rate for NU and secrecy rate for SU
at different power consumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6174</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6174</id><created>2011-06-30</created><updated>2012-03-01</updated><authors><author><keyname>Liu</keyname><forenames>Jianquan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Pairwise Check Decoding for LDPC Coded Two-Way Relay Block Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. on Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial decoding has the potential to achieve a larger capacity region than
full decoding in two-way relay (TWR) channels. Existing partial decoding
realizations are however designed for Gaussian channels and with a static
physical layer network coding (PLNC). In this paper, we propose a new solution
for joint network coding and channel decoding at the relay, called pairwise
check decoding (PCD), for low-density parity-check (LDPC) coded TWR system over
block fading channels. The main idea is to form a check relationship table
(check-relation-tab) for the superimposed LDPC coded packet pair in the
multiple access (MA) phase in conjunction with an adaptive PLNC mapping in the
broadcast (BC) phase. Using PCD, we then present a partial decoding method,
two-stage closest-neighbor clustering with PCD (TS-CNC-PCD), with the aim of
minimizing the worst pairwise error probability. Moreover, we propose the
minimum correlation optimization (MCO) for selecting the better
check-relation-tabs. Simulation results confirm that the proposed TS-CNC-PCD
offers a sizable gain over the conventional XOR with belief propagation (BP) in
fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6185</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6185</id><created>2011-06-30</created><authors><author><keyname>Rowan</keyname><forenames>Mark</forenames></author></authors><title>Effects of Compensation, Connectivity and Tau in a Computational Model
  of Alzheimer's Disease</title><categories>cs.NE q-bio.NC</categories><comments>8 pages, submitted to International Joint Conference on Neural
  Networks 2011</comments><journal-ref>The 2011 International Joint Conference on Neural Networks
  (IJCNN), (2011) 543--550</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work updates an existing, simplistic computational model of Alzheimer's
Disease (AD) to investigate the behaviour of synaptic compensatory mechanisms
in neural networks with small-world connectivity, and varying methods of
calculating compensation. It additionally introduces a method for simulating
tau neurofibrillary pathology, resulting in a more dramatic damage profile.
Small-world connectivity is shown to have contrasting effects on capacity,
retrieval time, and robustness to damage, whilst the use of more
easily-obtained remote memories rather than recent memories for synaptic
compensation is found to lead to rapid network damage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6186</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6186</id><created>2011-06-30</created><authors><author><keyname>Dundas</keyname><forenames>Jitesh</forenames></author><author><keyname>Chik</keyname><forenames>David</forenames></author></authors><title>IBSEAD: - A Self-Evolving Self-Obsessed Learning Algorithm for Machine
  Learning</title><categories>cs.LG</categories><comments>Keywords: Self-evolving algorithm; machine learning; decision-trees;
  learning algorithms, Hidden Markov Models;
  http://ijcset.excelingtech.co.uk/vol1issue4/14-vol1issue4.pdf</comments><journal-ref>International Journal of Computer Science &amp; Emerging Technologies
  (E-ISSN: 2044-6004) 74 Volume 1, Issue 4, December 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present IBSEAD or distributed autonomous entity systems based Interaction
- a learning algorithm for the computer to self-evolve in a self-obsessed
manner. This learning algorithm will present the computer to look at the
internal and external environment in series of independent entities, which will
interact with each other, with and/or without knowledge of the computer's
brain. When a learning algorithm interacts, it does so by detecting and
understanding the entities in the human algorithm. However, the problem with
this approach is that the algorithm does not consider the interaction of the
third party or unknown entities, which may be interacting with each other.
These unknown entities in their interaction with the non-computer entities make
an effect in the environment that influences the information and the behaviour
of the computer brain. Such details and the ability to process the dynamic and
unsettling nature of these interactions are absent in the current learning
algorithm such as the decision tree learning algorithm. IBSEAD is able to
evaluate and consider such algorithms and thus give us a better accuracy in
simulation of the highly evolved nature of the human brain. Processes such as
dreams, imagination and novelty, that exist in humans are not fully simulated
by the existing learning algorithms. Also, Hidden Markov models (HMM) are
useful in finding &quot;hidden&quot; entities, which may be known or unknown. However,
this model fails to consider the case of unknown entities which maybe unclear
or unknown. IBSEAD is better because it considers three types of entities-
known, unknown and invisible. We present our case with a comparison of existing
algorithms in known environments and cases and present the results of the
experiments using dry run of the simulated runs of the existing machine
learning algorithms versus IBSEAD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6191</identifier>
 <datestamp>2011-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6191</id><created>2011-06-30</created><updated>2011-12-21</updated><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>R&#xf3;nyai</keyname><forenames>Lajos</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Splitting full matrix algebras over algebraic number fields</title><categories>math.RA cs.SC math.NT</categories><comments>15 pages; Theorem 2 and Lemma 8 corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let K be an algebraic number field of degree d and discriminant D over Q. Let
A be an associative algebra over K given by structure constants such that A is
isomorphic to the algebra M_n(K) of n by n matrices over K for some positive
integer n. Suppose that d, n and D are bounded. Then an isomorphism of A with
M_n(K) can be constructed by a polynomial time ff-algorithm. (An ff-algorithm
is a deterministic procedure which is allowed to call oracles for factoring
integers and factoring univariate polynomials over finite fields.)
  As a consequence, we obtain a polynomial time ff-algorithm to compute
isomorphisms of central simple algebras of bounded degree over K.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6196</identifier>
 <datestamp>2012-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6196</id><created>2011-06-30</created><updated>2012-06-11</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>On the behaviours produced by instruction sequences under execution</title><categories>cs.PL cs.DC cs.LO</categories><comments>36 pages, consolidates material from arXiv:0811.0436 [cs.PL],
  arXiv:0902.2859 [cs.PL], and arXiv:0905.2257 [cs.PL]; abstract and
  introduction rewritten, examples and proofs added</comments><msc-class>68N19, 68N30, 68Q05, 68Q10, 68Q55, 68Q85</msc-class><acm-class>D.1.4; D.2.1; F.1.1; F.1.2; F.3.2</acm-class><journal-ref>Fundamenta Informaticae, 120(2):111--144, 2012</journal-ref><doi>10.3233/FI-2012-753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study several aspects of the behaviours produced by instruction sequences
under execution in the setting of the algebraic theory of processes known as
ACP. We use ACP to describe the behaviours produced by instruction sequences
under execution and to describe two protocols implementing these behaviours in
the case where the processing of instructions takes place remotely. We also
show that all finite-state behaviours considered in ACP can be produced by
instruction sequences under execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6206</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6206</id><created>2011-06-30</created><authors><author><keyname>Tolhuizen</keyname><forenames>Ludo</forenames></author></authors><title>A generalisation of the Gilbert-Varshamov bound and its asymptotic
  evaluation</title><categories>cs.IT math.IT</categories><comments>To be presented at 3ICMCTA, the 3rd International Castle Meeting on
  Coding Theory and Applications, Sept. 11-15, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gilbert-Varshamov (GV) lower bound on the maximum cardinality of a q-ary
code of length n with minimum Hamming distance at least d can be obtained by
application of Turan's theorem to the graph with vertex set {0,1,..,q-1}^n in
which two vertices are joined if and only if their Hamming distance is at least
d. We generalize the GV bound by applying Turan's theorem to the graph with
vertex set C^n, where C is a q-ary code of length m and two vertices are joined
if and only if their Hamming distance at least d. We asymptotically evaluate
the resulting bound for n-&gt; \infty and d \delta mn for fixed \delta &gt; 0, and
derive conditions on the distance distribution of C that are necessary and
sufficient for the asymptotic generalized bound to beat the asymptotic GV
bound. By invoking the Delsarte inequalities, we conclude that no improvement
on the asymptotic GV bound is obtained. By using a sharpening of Turan's
theorem due to Caro and Wei, we improve on our bound. It is undecided if there
exists a code C for which the improved bound can beat the asymptotic GV bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6214</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6214</id><created>2011-06-30</created><updated>2012-11-02</updated><authors><author><keyname>Kim</keyname><forenames>Hyo-Sil</forenames></author><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author></authors><title>The Cost of Bounded Curvature</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the motion-planning problem for a car-like robot whose turning
radius is bounded from below by one and which is allowed to move in the forward
direction only (Dubins car). For two robot configurations $\sigma, \sigma'$,
let $\ell(\sigma, \sigma')$ be the shortest bounded-curvature path from
$\sigma$ to $\sigma'$. For $d \geq 0$, let $\ell(d)$ be the supremum of
$\ell(\sigma, \sigma')$, over all pairs $(\sigma, \sigma')$ that are at
Euclidean distance $d$. We study the function $\dub(d) = \ell(d) - d$, which
expresses the difference between the bounded-curvature path length and the
Euclidean distance of its endpoints. We show that $\dub(d)$ decreases
monotonically from $\dub(0) = 7\pi/3$ to $\dub(\ds) = 2\pi$, and is constant
for $d \geq \ds$. Here $\ds \approx 1.5874$. We describe pairs of
configurations that exhibit the worst-case of $\dub(d)$ for every distance $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6215</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6215</id><created>2011-06-30</created><updated>2012-03-02</updated><authors><author><keyname>Ermann</keyname><forenames>Leonardo</forenames></author><author><keyname>Chepelianskii</keyname><forenames>Alexei D.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Towards two-dimensional search engines</title><categories>cs.IR cond-mat.stat-mech</categories><comments>22 pages, 16 figures. Additional data available at
  http://www.quantware.ups-tlse.fr/QWLIB/dvvadi/</comments><journal-ref>J. Phys. A: Math. Theor. 45 (2012) 275101 (20pp)</journal-ref><doi>10.1088/1751-8113/45/27/275101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the statistical properties of various directed networks using
ranking of their nodes based on the dominant vectors of the Google matrix known
as PageRank and CheiRank. On average PageRank orders nodes proportionally to a
number of ingoing links, while CheiRank orders nodes proportionally to a number
of outgoing links. In this way the ranking of nodes becomes two-dimensional
that paves the way for development of two-dimensional search engines of new
type. Statistical properties of information flow on PageRank-CheiRank plane are
analyzed for networks of British, French and Italian Universities, Wikipedia,
Linux Kernel, gene regulation and other networks. A special emphasis is done
for British Universities networks using the large database publicly available
at UK. Methods of spam links control are also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6217</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6217</id><created>2011-06-30</created><authors><author><keyname>Niculescu</keyname><forenames>Mihai</forenames></author><author><keyname>Zgura</keyname><forenames>Sorin-Ion</forenames></author></authors><title>Computing trends using graphic processor in high energy physics</title><categories>cs.DC hep-ex physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges in Heavy Energy Physics is to make fast analysis
of high amount of experimental and simulated data. At LHC-CERN one p-p event is
approximate 1 Mb in size. The time taken to analyze the data and obtain fast
results depends on high computational power. The main advantage of using
GPU(Graphic Processor Unit) programming over traditional CPU one is that
graphical cards bring a lot of computing power at a very low price. Today a
huge number of application(scientific, financial etc) began to be ported or
developed for GPU, including Monte Carlo tools or data analysis tools for High
Energy Physics. In this paper, we'll present current status and trends in HEP
using GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6223</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6223</id><created>2011-06-30</created><authors><author><keyname>Gauci</keyname><forenames>Melvin</forenames></author><author><keyname>Dodd</keyname><forenames>Tony J.</forenames></author><author><keyname>Gross</keyname><forenames>Roderich</forenames></author></authors><title>Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on
  the Law of Gravity</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on the Law
of Gravity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6224</identifier>
 <datestamp>2011-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6224</id><created>2011-06-30</created><updated>2011-07-28</updated><authors><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Structured Compressed Sensing: From Theory to Applications</title><categories>cs.IT math.IT</categories><comments>To appear as an overview paper in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2011.2161982</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is an emerging field that has attracted considerable
research interest over the past few years. Previous review articles in CS limit
their scope to standard discrete-to-discrete measurement architectures using
matrices of randomized nature and signal models based on standard sparsity. In
recent years, CS has worked its way into several new application areas. This,
in turn, necessitates a fresh look on many of the basics of CS. The random
matrix measurement operator must be replaced by more structured sensing
architectures that correspond to the characteristics of feasible acquisition
hardware. The standard sparsity prior has to be extended to include a much
richer class of signals and to encode broader data models, including
continuous-time signals. In our overview, the theme is exploiting signal and
measurement structure in compressive sensing. The prime focus is bridging
theory and practice; that is, to pinpoint the potential of structured CS
strategies to emerge from the math to the hardware. Our summary highlights new
directions as well as relations to more traditional CS, with the hope of
serving both as a review to practitioners wanting to join this emerging field,
and as a reference for researchers that attempts to put some of the existing
ideas in perspective of practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6242</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6242</id><created>2011-06-30</created><authors><author><keyname>Katta</keyname><forenames>Sandeep</forenames></author></authors><title>Visual Secret Sharing Scheme using Grayscale Images</title><categories>cs.CR cs.CV</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pixel expansion and the quality of the reconstructed secret image has been a
major issue of visual secret sharing (VSS) schemes. A number of probabilistic
VSS schemes with minimum pixel expansion have been proposed for black and white
(binary) secret images. This paper presents a probabilistic (2, 3)-VSS scheme
for gray scale images. Its pixel expansion is larger in size but the quality of
the image is perfect when it's reconstructed. The construction of the shadow
images (transparent shares) is based on the binary OR operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6251</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6251</id><created>2011-06-30</created><updated>2012-04-16</updated><authors><author><keyname>Alvarez</keyname><forenames>Mauricio A.</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Kernels for Vector-Valued Functions: a Review</title><categories>stat.ML cs.AI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods are among the most popular techniques in machine learning.
From a frequentist/discriminative perspective they play a central role in
regularization theory as they provide a natural choice for the hypotheses space
and the regularization functional through the notion of reproducing kernel
Hilbert spaces. From a Bayesian/generative perspective they are the key in the
context of Gaussian processes, where the kernel function is also known as the
covariance function. Traditionally, kernel methods have been used in supervised
learning problem with scalar outputs and indeed there has been a considerable
amount of work devoted to designing and learning kernels. More recently there
has been an increasing interest in methods that deal with multiple outputs,
motivated partly by frameworks like multitask learning. In this paper, we
review different methods to design or learn valid kernel functions for multiple
outputs, paying particular attention to the connection between probabilistic
and functional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6254</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6254</id><created>2011-06-30</created><updated>2011-10-03</updated><authors><author><keyname>Karavelas</keyname><forenames>Menelaos I.</forenames></author><author><keyname>Tzanaki</keyname><forenames>Eleni</forenames></author></authors><title>The maximum number of faces of the Minkowski sum of two convex polytopes</title><categories>cs.CG math.CO</categories><comments>37 pages, 8 figures, conference version to appear at SODA 2012; v2:
  fixed typos, made stylistic changes, added figures</comments><msc-class>52B05, 52B11, 52C45, 68U05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive tight expressions for the maximum number of $k$-faces,
$0\le{}k\le{}d-1$, of the Minkowski sum, $P_1\oplus{}P_2$, of two
$d$-dimensional convex polytopes $P_1$ and $P_2$, as a function of the number
of vertices of the polytopes.
  For even dimensions $d\ge{}2$, the maximum values are attained when $P_1$ and
$P_2$ are cyclic $d$-polytopes with disjoint vertex sets. For odd dimensions
$d\ge{}3$, the maximum values are attained when $P_1$ and $P_2$ are
$\lfloor\frac{d}{2}\rfloor$-neighborly $d$-polytopes, whose vertex sets are
chosen appropriately from two distinct $d$-dimensional moment-like curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6258</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6258</id><created>2011-06-30</created><updated>2014-05-12</updated><authors><author><keyname>Hussain</keyname><forenames>Zakria</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author></authors><title>A Note on Improved Loss Bounds for Multiple Kernel Learning</title><categories>cs.LG</categories><comments>Extended proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we correct an upper bound, presented in~\cite{hs-11}, on the
generalisation error of classifiers learned through multiple kernel learning.
The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive}
dependence on the logarithm of the number of kernels and the margin achieved by
the classifier. However, there are some errors in parts of the proof which are
corrected in this paper. Unfortunately, the final result turns out to be a risk
bound which has a \emph{multiplicative} dependence on the logarithm of the
number of kernels and the margin achieved by the classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6261</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6261</id><created>2011-06-30</created><authors><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author></authors><title>External Memory Orthogonal Range Reporting with Fast Updates</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe data structures for orthogonal range reporting in
external memory that support fast update operations. The query costs either
match the query costs of the best previously known data structures or differ by
a small multiplicative factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6267</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6267</id><created>2011-06-29</created><authors><author><keyname>Ksystra</keyname><forenames>Katerina</forenames></author><author><keyname>Barlas</keyname><forenames>Konstantinos</forenames></author><author><keyname>Triantafyllou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Stefaneas</keyname><forenames>Petros</forenames></author></authors><title>A Dynamic Algebraic Specification for Social Networks</title><categories>cs.LO</categories><comments>8 apges, 1 figure, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the help of the Internet, social networks have grown rapidly. This has
increased security requirements. We present a formalization of social networks
as composite behavioral objects, defined using the Observational Transition
System (OTS) approach. Our definition is then translated to the OTS/CafeOBJ
algebraic specification methodology. This translation allows the formal
verification of safety properties for social networks via the Proof Score
method. Finally, using this methodology we formally verify some security
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6271</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6271</id><created>2011-06-04</created><authors><author><keyname>Hadei</keyname><forenames>Sayed A.</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Azmi</keyname><forenames>Paeiz</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>Low-Complexity Adaptive Channel Estimation over Multipath Rayleigh
  Fading Non-Stationary Channels Under CFO</title><categories>cs.IT math.IT</categories><comments>18th IEEE International Conference on Telecommunications (ICT2011)
  Ayia Napa, Cyprus</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose novel low-complexity adaptive channel estimation
techniques for mob ile wireless chan- n els in presence of Rayleigh fading,
carrier frequency offsets (CFO) and random channel variations. We show that the
selective p artial update of the estimated channel tap-weight vector offers a
better trade-off between the performance and computational complexity, compared
to the full update of the estimated channel tap-weight vector. We evaluate the
mean-square weight error of th e proposed methods and demonstrate the
usefulness of its via simulation studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6304</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6304</id><created>2011-06-30</created><authors><author><keyname>Bar-Nissan</keyname><forenames>Gal</forenames></author><author><keyname>Hendler</keyname><forenames>Danny</forenames></author><author><keyname>Suissa</keyname><forenames>Adi</forenames></author></authors><title>A Dynamic Elimination-Combining Stack Algorithm</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two key synchronization paradigms for the construction of scalable concurrent
data-structures are software combining and elimination. Elimination-based
concurrent data-structures allow operations with reverse semantics (such as
push and pop stack operations) to &quot;collide&quot; and exchange values without having
to access a central location. Software combining, on the other hand, is
effective when colliding operations have identical semantics: when a pair of
threads performing operations with identical semantics collide, the task of
performing the combined set of operations is delegated to one of the threads
and the other thread waits for its operation(s) to be performed. Applying this
mechanism iteratively can reduce memory contention and increase throughput. The
most highly scalable prior concurrent stack algorithm is the
elimination-backoff stack. The elimination-backoff stack provides high
parallelism for symmetric workloads in which the numbers of push and pop
operations are roughly equal, but its performance deteriorates when workloads
are asymmetric. We present DECS, a novel Dynamic Elimination-Combining Stack
algorithm, that scales well for all workload types. While maintaining the
simplicity and low-overhead of the elimination-bakcoff stack, DECS manages to
benefit from collisions of both identical- and reverse-semantics operations.
Our empirical evaluation shows that DECS scales significantly better than both
blocking and non-blocking best prior stack algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6323</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6323</id><created>2011-06-30</created><authors><author><keyname>Karmakar</keyname><forenames>Sanjay</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The Diversity Multiplexing Tradeoff of the MIMO Half-Duplex Relay
  Channel</title><categories>cs.IT math.IT</categories><comments>42 pages, 7 figures; submitted to the Trans. of IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental diversity-multiplexing tradeoff of the three-node,
multi-input, multi-output (MIMO), quasi-static, Rayleigh faded, half-duplex
relay channel is characterized for an arbitrary number of antennas at each node
and in which opportunistic scheduling (or dynamic operation) of the relay is
allowed, i.e., the relay can switch between receive and transmit modes at a
channel dependent time. In this most general case, the diversity-multiplexing
tradeoff is characterized as a solution to a simple, two-variable optimization
problem. This problem is then solved in closed form for special classes of
channels defined by certain restrictions on the numbers of antennas at the
three nodes. The key mathematical tool developed here that enables the explicit
characterization of the diversity-multiplexing tradeoff is the joint eigenvalue
distribution of three mutually correlated random Wishart matrices. Previously,
without actually characterizing the diversity-multiplexing tradeoff, the
optimality in this tradeoff metric of the dynamic compress-and-forward (DCF)
protocol based on the classical compress-and-forward scheme of Cover and El
Gamal was shown by Yuksel and Erkip. However, this scheme requires global
channel state information (CSI) at the relay. In this work, the so-called
quantize-map and forward (QMF) coding scheme due to Avestimehr {\em et} {\em
al} is adopted as the achievability scheme with the added benefit that it
achieves optimal tradeoff with only the knowledge of the (channel dependent)
switching time at the relay node. Moreover, in special classes of the MIMO
half-duplex relay channel, the optimal tradeoff is shown to be attainable even
without this knowledge. Such a result was previously known only for the
half-duplex relay channel with a single antenna at each node, also via the QMF
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6328</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6328</id><created>2011-06-30</created><updated>2012-06-26</updated><authors><author><keyname>Cho</keyname><forenames>Jeong-woo</forenames></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author></authors><title>On the Asymptotic Validity of the Decoupling Assumption for Analyzing
  802.11 MAC Protocol</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>16 pages, 4 figures, accepted for publication in IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance evaluation of the 802.11 MAC protocol is classically based on the
decoupling assumption, which hypothesizes that the backoff processes at
different nodes are independent. This decoupling assumption results from mean
field convergence and is generally true in transient regime in the asymptotic
sense (when the number of wireless nodes tends to infinity), but, contrary to
widespread belief, may not necessarily hold in stationary regime. The issue is
often related with the existence and uniqueness of a solution to a fixed point
equation; however, it was also recently shown that this condition is not
sufficient; in contrast, a sufficient condition is a global stability property
of the associated ordinary differential equation. In this paper, we give a
simple condition that establishes the asymptotic validity of the decoupling
assumption for the homogeneous case. We also discuss the heterogeneous and the
differentiated service cases and formulate a new ordinary differential
equation. We show that the uniqueness of a solution to the associated fixed
point equation is not sufficient; we exhibit one case where the fixed point
equation has a unique solution but the decoupling assumption is not valid in
the asymptotic sense in stationary regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6333</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6333</id><created>2011-06-30</created><authors><author><keyname>Davids</keyname><forenames>Carol</forenames></author><author><keyname>Johnston</keyname><forenames>Alan</forenames></author><author><keyname>Singh</keyname><forenames>Kundan</forenames></author><author><keyname>Sinnreich</keyname><forenames>Henry</forenames></author><author><keyname>Wimmreuter</keyname><forenames>Wilhelm</forenames></author></authors><title>SIP APIs for Voice and Video Communications on the Web</title><categories>cs.NI cs.MM</categories><comments>Accepted at IPTcomm 2011, 7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing standard protocols for the web and Internet telephony fail to
deliver real-time interactive communication from within a web browser. In
particular, the client-server web protocol over reliable TCP is not always
suitable for end-to-end low latency media path needed for interactive voice and
video communication. To solve this, we compare the available platform options
using the existing technologies such as modifying the web programming language
and protocol, using an existing web browser plugin, and a separate host
resident application that the web browser can talk to. We argue that using a
separate application as an adaptor is a promising short term as well as
long-term strategy for voice and video communications on the web. Our project
aims at developing the open technology and sample implementations for web-based
real-time voice and video communication applications. We describe the
architecture of our project including (1) a RESTful web communication API over
HTTP inspired by SIP message flows, (2) a web-friendly set of metadata for
session description, and (3) an UDP-based end-to-end media path. All other
telephony functions reside in the web application itself and/or in web feature
servers. The adaptor approach allows us to easily add new voice and video
codecs and NAT traversal technologies such as Host Identity Protocol. We want
to make web-based communication accessible to millions of web developers,
maximize the end user experience and security, and preserve the huge global
investment in and experience from SIP systems while adhering to web standards
and development tools as much as possible. We have created an open source
prototype that allows you to freely use the conference application by directing
a browser to the conference URL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6335</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6335</id><created>2011-06-30</created><authors><author><keyname>Noseda</keyname><forenames>Francesco</forenames></author><author><keyname>Oliveira</keyname><forenames>Gilvan</forenames></author><author><keyname>Quoos</keyname><forenames>Luciane</forenames></author></authors><title>Bases for Riemann-Roch spaces of one point divisors on an optimal tower
  of function fields</title><categories>math.NT cs.IT math.AG math.IT</categories><comments>15 pages</comments><msc-class>14H05, 14H55, 14G50, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For applications in algebraic geometric codes, an explicit description of
bases of Riemann-Roch spaces of divisors on function fields over finite fields
is needed. We give an algorithm to compute such bases for one point divisors,
and Weierstrass semigroups over an optimal tower of function fields. We also
explicitly compute Weierstrass semigroups till level eight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6336</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6336</id><created>2011-06-30</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Pszona</keyname><forenames>Pawel</forenames></author></authors><title>External-Memory Network Analysis Algorithms for Naturally Sparse Graphs</title><categories>cs.DS</categories><comments>23 pages, 2 figures. To appear at the 19th Annual European Symposium
  on Algorithms (ESA 2011)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a number of network-analysis algorithms in the
external-memory model. We focus on methods for large naturally sparse graphs,
that is, n-vertex graphs that have O(n) edges and are structured so that this
sparsity property holds for any subgraph of such a graph. We give efficient
external-memory algorithms for the following problems for such graphs: -
Finding an approximate d-degeneracy ordering; - Finding a cycle of length
exactly c; - Enumerating all maximal cliques. Such problems are of interest,
for example, in the analysis of social networks, where they are used to study
network cohesion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6341</identifier>
 <datestamp>2011-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6341</id><created>2011-06-30</created><updated>2011-08-16</updated><authors><author><keyname>Lerner</keyname><forenames>Ronen</forenames></author><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Rivlin</keyname><forenames>Ehud</forenames></author></authors><title>Vision-Based Navigation III: Pose and Motion from Omnidirectional
  Optical Flow and a Digital Terrain Map</title><categories>cs.CV cs.AI</categories><comments>6 pages, 9 figures</comments><msc-class>68T45</msc-class><acm-class>E.5; E.4; E.2; H.1.1; F.1.1; F.1.3</acm-class><journal-ref>Proceedings of the 2006 IEEE/RSJ International Conference on
  Intelligent Robots and Systems October 9 - 15, 2006, Beijing, China</journal-ref><doi>10.1109/IROS.2006.282569</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for pose and motion estimation using corresponding features in
omnidirectional images and a digital terrain map is proposed. In previous
paper, such algorithm for regular camera was considered. Using a Digital
Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables
recovering the absolute position and orientation of the camera. In order to do
this, the DTM is used to formulate a constraint between corresponding features
in two consecutive frames. In this paper, these constraints are extended to
handle non-central projection, as is the case with many omnidirectional
systems. The utilization of omnidirectional data is shown to improve the
robustness and accuracy of the navigation algorithm. The feasibility of this
algorithm is established through lab experimentation with two kinds of
omnidirectional acquisition systems. The first one is polydioptric cameras
while the second is catadioptric camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1106.6342</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1106.6342</id><created>2011-06-30</created><authors><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author></authors><title>Quadratic-time Algorithm for the String Constrained LCS Problem</title><categories>cs.DS</categories><comments>6 pages</comments><msc-class>68W32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding a longest common subsequence of two main sequences
with some constraint that must be a substring of the result (STR-IC-LCS) was
formulated recently. It is a variant of the constrained longest common
subsequence problem. As the known algorithms for the STR-IC-LCS problem are
cubic-time, the presented quadratic-time algorithm is significantly faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0011</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0011</id><created>2011-06-30</created><authors><author><keyname>Singh</keyname><forenames>Kundan</forenames></author><author><keyname>Davids</keyname><forenames>Carol</forenames></author></authors><title>Flash-based Audio and Video Communication in the Cloud</title><categories>cs.NI cs.MM</categories><comments>Technical Implementation Report, 13 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet telephony and multimedia communication protocols have matured over
the last fifteen years. Recently, the web is evolving as a popular platform for
everything we do on the Internet including email, text chat, voice calls,
discussions, enterprise apps and multi-party collaboration. Unfortunately,
there is a disconnect between web and traditional Internet telephony protocols
as they have ignored the constraints and requirements of each other.
Consequently, the Flash Player is being used as a web browser plugin by many
developers for web-based voice and video calls. We describe the challenges of
video communication using a web browser, present a simple API using a Flash
Player application, show how it supports wide range of web communication
scenarios in the cloud, and describe how it can interoperate with Session
Initiation Protocol (SIP)-based systems. We describe both the advantages and
challenges of Flash Player based communication applications. The presented API
could guide future work on communication-related web protocol extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0015</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0015</id><created>2011-06-30</created><authors><author><keyname>Dundas</keyname><forenames>Jitesh</forenames></author></authors><title>Automaton based detection of affected cells in three dimensional
  biological system</title><categories>cs.CE</categories><comments>38 pages including 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this research review is to propose the logic and search mechanism
for the development of an artificially intelligent automaton (AIA) that can
find affected cells in a 3-dimensional biological system. Research on the
possible application of such automatons to detect and control cancer cells in
the human body are greatly focused MRI and PET scans finds the affected regions
at the tissue level even as we can find the affected regions at the cellular
level using the framework. The AIA may be designed to ensure optimum
utilization as they record and might control the presence of affected cells in
a human body. The proposed models and techniques can be generalized and used in
any application where cells are injured or affected by some disease or
accident. The best method to import AIA into the body without surgery or
injection is to insert small pill like automata, carrying material viz drugs or
leukocytes that is needed to correct the infection. In this process, the AIA
can be compared to nano pills to deliver or support therapy. NanoHive
simulation software was used to validate the framework of this paper. The
existing nanomedicine models such as obstacle avoidance algorithm based models
(Hla K H S et al 2008) and the framework in this model were tested in different
simulation based experiments. The existing models such as obstacle avoidance
based models failed in complex environmental conditions (such as changing
environmental conditions, presence of semi-solid particles, etc) while the
model in this paper executed its framework successfully.Come systems biology,
this field of automatons deserves a bigger leap of understanding especially
when pharmacogenomics is at its peak. The results also indicate the importance
of artificial intelligence and other computational capabilities in the proposed
model for the successful detection of affected cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0018</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0018</id><created>2011-06-30</created><authors><author><keyname>Al-Ani</keyname><forenames>A.</forenames></author><author><keyname>Deriche</keyname><forenames>M.</forenames></author></authors><title>A New Technique for Combining Multiple Classifiers using The
  Dempster-Shafer Theory of Evidence</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 17, pages
  333-361, 2002</journal-ref><doi>10.1613/jair.1026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new classifier combination technique based on the
Dempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a
powerful method for combining measures of evidence from different classifiers.
However, since each of the available methods that estimates the evidence of
classifiers has its own limitations, we propose here a new implementation which
adapts to training data so that the overall mean square error is minimized. The
proposed technique is shown to outperform most available classifier combination
methods when tested on three different classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0019</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0019</id><created>2011-06-30</created><authors><author><keyname>Acid</keyname><forenames>S.</forenames></author><author><keyname>de Campos</keyname><forenames>L. M.</forenames></author></authors><title>Searching for Bayesian Network Structures in the Space of Restricted
  Acyclic Partially Directed Graphs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  445-490, 2003</journal-ref><doi>10.1613/jair.1061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many algorithms have been designed to construct Bayesian network
structures using different approaches and principles, they all employ only two
methods: those based on independence criteria, and those based on a scoring
function and a search procedure (although some methods combine the two). Within
the score+search paradigm, the dominant approach uses local search methods in
the space of directed acyclic graphs (DAGs), where the usual choices for
defining the elementary modifications (local changes) that can be applied are
arc addition, arc deletion, and arc reversal. In this paper, we propose a new
local search method that uses a different search space, and which takes account
of the concept of equivalence between network structures: restricted acyclic
partially directed graphs (RPDAGs). In this way, the number of different
configurations of the search space is reduced, thus improving efficiency.
Moreover, although the final result must necessarily be a local optimum given
the nature of the search method, the topology of the new search space, which
avoids making early decisions about the directions of the arcs, may help to
find better local optima than those obtained by searching in the DAG space.
Detailed results of the evaluation of the proposed search method on several
test problems, including the well-known Alarm Monitoring System, are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0020</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0020</id><created>2011-06-30</created><authors><author><keyname>Grumberg</keyname><forenames>O.</forenames></author><author><keyname>Livne</keyname><forenames>S.</forenames></author><author><keyname>Markovitch</keyname><forenames>S.</forenames></author></authors><title>Learning to Order BDD Variables in Verification</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 18, pages
  83-116, 2003</journal-ref><doi>10.1613/jair.1096</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The size and complexity of software and hardware systems have significantly
increased in the past years. As a result, it is harder to guarantee their
correct behavior. One of the most successful methods for automated verification
of finite-state systems is model checking. Most of the current model-checking
systems use binary decision diagrams (BDDs) for the representation of the
tested model and in the verification process of its properties. Generally, BDDs
allow a canonical compact representation of a boolean function (given an order
of its variables). The more compact the BDD is, the better performance one gets
from the verifier. However, finding an optimal order for a BDD is an
NP-complete problem. Therefore, several heuristic methods based on expert
knowledge have been developed for variable ordering. We propose an alternative
approach in which the variable ordering algorithm gains 'ordering experience'
from training models and uses the learned knowledge for finding good orders.
Our methodology is based on offline learning of pair precedence classifiers
from training models, that is, learning which variable pair permutation is more
likely to lead to a good order. For each training model, a number of training
sequences are evaluated. Every training model variable pair permutation is then
tagged based on its performance on the evaluated orders. The tagged
permutations are then passed through a feature extractor and are given as
examples to a classifier creation algorithm. Given a model for which an order
is requested, the ordering algorithm consults each precedence classifier and
constructs a pair precedence table which is used to create the order. Our
algorithm was integrated with SMV, which is one of the most widely used
verification systems. Preliminary empirical evaluation of our methodology,
using real benchmark models, shows performance that is better than random
ordering and is competitive with existing algorithms that use expert knowledge.
We believe that in sub-domains of models (alu, caches, etc.) our system will
prove even more valuable. This is because it features the ability to learn
sub-domain knowledge, something that no other ordering algorithm does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0021</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0021</id><created>2011-06-30</created><authors><author><keyname>Walsh</keyname><forenames>W. E.</forenames></author><author><keyname>Wellman</keyname><forenames>M. P.</forenames></author></authors><title>Decentralized Supply Chain Formation: A Market Protocol and Competitive
  Equilibrium Analysis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 19, pages
  513-567, 2003</journal-ref><doi>10.1613/jair.1213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supply chain formation is the process of determining the structure and terms
of exchange relationships to enable a multilevel, multiagent production
activity. We present a simple model of supply chains, highlighting two
characteristic features: hierarchical subtask decomposition, and resource
contention. To decentralize the formation process, we introduce a market price
system over the resources produced along the chain. In a competitive
equilibrium for this system, agents choose locally optimal allocations with
respect to prices, and outcomes are optimal overall. To determine prices, we
define a market protocol based on distributed, progressive auctions, and
myopic, non-strategic agent bidding policies. In the presence of resource
contention, this protocol produces better solutions than the greedy protocols
common in the artificial intelligence and multiagent systems literature. The
protocol often converges to high-value supply chains, and when competitive
equilibria exist, typically to approximate competitive equilibria. However,
complementarities in agent production technologies can cause the protocol to
wastefully allocate inputs to agents that do not produce their outputs. A
subsequent decommitment phase recovers a significant fraction of the lost
surplus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0022</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0022</id><created>2011-06-30</created><authors><author><keyname>Monderer</keyname><forenames>D.</forenames></author><author><keyname>Tennenholtz</keyname><forenames>M.</forenames></author></authors><title>K-Implementation</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  37-62, 2004</journal-ref><doi>10.1613/jair.1231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses an interested party who wishes to influence the behavior
of agents in a game (multi-agent interaction), which is not under his control.
The interested party cannot design a new game, cannot enforce agents' behavior,
cannot enforce payments by the agents, and cannot prohibit strategies available
to the agents. However, he can influence the outcome of the game by committing
to non-negative monetary transfers for the different strategy profiles that may
be selected by the agents. The interested party assumes that agents are
rational in the commonly agreed sense that they do not use dominated
strategies. Hence, a certain subset of outcomes is implemented in a given game
if by adding non-negative payments, rational players will necessarily produce
an outcome in this subset. Obviously, by making sufficiently big payments one
can implement any desirable outcome. The question is what is the cost of
implementation? In this paper we introduce the notion of k-implementation of a
desired set of strategy profiles, where k stands for the amount of payment that
need to be actually made in order to implement desirable outcomes. A major
point in k-implementation is that monetary offers need not necessarily
materialize when following desired behaviors. We define and study
k-implementation in the contexts of games with complete and incomplete
information. In the latter case we mainly focus on the VCG games. Our setting
is later extended to deal with mixed strategies using correlation devices.
Together, the paper introduces and studies the implementation of desirable
outcomes by a reliable party who cannot modify game rules (i.e. provide
protocols), complementing previous work in mechanism design, while making it
more applicable to many realistic CS settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0023</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0023</id><created>2011-06-30</created><authors><author><keyname>Boutilier</keyname><forenames>C.</forenames></author><author><keyname>Brafman</keyname><forenames>R. I.</forenames></author><author><keyname>Domshlak</keyname><forenames>C.</forenames></author><author><keyname>Hoos</keyname><forenames>H. H.</forenames></author><author><keyname>Poole</keyname><forenames>D.</forenames></author></authors><title>CP-nets: A Tool for Representing and Reasoning withConditional Ceteris
  Paribus Preference Statements</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  135-191, 2004</journal-ref><doi>10.1613/jair.1234</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information about user preferences plays a key role in automated decision
making. In many domains it is desirable to assess such preferences in a
qualitative rather than quantitative way. In this paper, we propose a
qualitative graphical representation of preferences that reflects conditional
dependence and independence of preference statements under a ceteris paribus
(all else being equal) interpretation. Such a representation is often compact
and arguably quite natural in many circumstances. We provide a formal semantics
for this model, and describe how the structure of the network can be exploited
in several inference tasks, such as determining whether one outcome dominates
(is preferred to) another, ordering a set outcomes according to the preference
relation, and constructing the best outcome subject to available evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0024</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0024</id><created>2011-06-30</created><authors><author><keyname>Darwiche</keyname><forenames>A.</forenames></author><author><keyname>Park</keyname><forenames>J. D.</forenames></author></authors><title>Complexity Results and Approximation Strategies for MAP Explanations</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  101-133, 2006</journal-ref><doi>10.1613/jair.1236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MAP is the problem of finding a most probable instantiation of a set of
variables given evidence. MAP has always been perceived to be significantly
harder than the related problems of computing the probability of a variable
instantiation Pr, or the problem of computing the most probable explanation
(MPE). This paper investigates the complexity of MAP in Bayesian networks.
Specifically, we show that MAP is complete for NP^PP and provide further
negative complexity results for algorithms based on variable elimination. We
also show that MAP remains hard even when MPE and Pr become easy. For example,
we show that MAP is NP-complete when the networks are restricted to polytrees,
and even then can not be effectively approximated. Given the difficulty of
computing MAP exactly, and the difficulty of approximating MAP while providing
useful guarantees on the resulting approximation, we investigate best effort
approximations. We introduce a generic MAP approximation framework. We provide
two instantiations of the framework; one for networks which are amenable to
exact inference Pr, and one for networks for which even exact inference is too
hard. This allows MAP approximation on networks that are too complex to even
exactly solve the easier problems, Pr and MPE. Experimental results indicate
that using these approximation algorithms provides much better solutions than
standard techniques, and provide accurate MAP estimates in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0025</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0025</id><created>2011-06-30</created><authors><author><keyname>Edelkamp</keyname><forenames>S.</forenames></author></authors><title>Taming Numbers and Durations in the Model Checking Integrated Planning
  System</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  195-238, 2003</journal-ref><doi>10.1613/jair.1302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Model Checking Integrated Planning System (MIPS) is a temporal least
commitment heuristic search planner based on a flexible object-oriented
workbench architecture. Its design clearly separates explicit and symbolic
directed exploration algorithms from the set of on-line and off-line computed
estimates and associated data structures. MIPS has shown distinguished
performance in the last two international planning competitions. In the last
event the description language was extended from pure propositional planning to
include numerical state variables, action durations, and plan quality objective
functions. Plans were no longer sequences of actions but time-stamped
schedules. As a participant of the fully automated track of the competition,
MIPS has proven to be a general system; in each track and every benchmark
domain it efficiently computed plans of remarkable quality. This article
introduces and analyzes the most important algorithmic novelties that were
necessary to tackle the new layers of expressiveness in the benchmark problems
and to achieve a high level of performance. The extensions include critical
path analysis of sequentially generated plans to generate corresponding optimal
parallel plans. The linear time algorithm to compute the parallel plan bypasses
known NP hardness results for partial ordering by scheduling plans with respect
to the set of actions and the imposed precedence relations. The efficiency of
this algorithm also allows us to improve the exploration guidance: for each
encountered planning state the corresponding approximate sequential plan is
scheduled. One major strength of MIPS is its static analysis phase that grounds
and simplifies parameterized predicates, functions and operators, that infers
knowledge to minimize the state description length, and that detects domain
object symmetries. The latter aspect is analyzed in detail. MIPS has been
developed to serve as a complete and optimal state space planner, with
admissible estimates, exploration engines and branching cuts. In the
competition version, however, certain performance compromises had to be made,
including floating point arithmetic, weighted heuristic search exploration
according to an inadmissible estimate and parameterized optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0026</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0026</id><created>2011-06-30</created><authors><author><keyname>Nederhof</keyname><forenames>M. J.</forenames></author><author><keyname>Satta</keyname><forenames>G.</forenames></author></authors><title>IDL-Expressions: A Formalism for Representing and Parsing Finite
  Languages in Natural Language Processing</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  287-317, 2004</journal-ref><doi>10.1613/jair.1309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formalism for representation of finite languages, referred to as
the class of IDL-expressions, which combines concepts that were only considered
in isolation in existing formalisms. The suggested applications are in natural
language processing, more specifically in surface natural language generation
and in machine translation, where a sentence is obtained by first generating a
large set of candidate sentences, represented in a compact way, and then by
filtering such a set through a parser. We study several formal properties of
IDL-expressions and compare this new formalism with more standard ones. We also
present a novel parsing algorithm for IDL-expressions and prove a non-trivial
upper bound on its time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0027</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0027</id><created>2011-06-30</created><authors><author><keyname>Kocka</keyname><forenames>T.</forenames></author><author><keyname>Zhang</keyname><forenames>N. L.</forenames></author></authors><title>Effective Dimensions of Hierarchical Latent Class Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  1-17, 2004</journal-ref><doi>10.1613/jair.1311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical latent class (HLC) models are tree-structured Bayesian networks
where leaf nodes are observed while internal nodes are latent. There are no
theoretically well justified model selection criteria for HLC models in
particular and Bayesian networks with latent nodes in general. Nonetheless,
empirical studies suggest that the BIC score is a reasonable criterion to use
in practice for learning HLC models. Empirical studies also suggest that
sometimes model selection can be improved if standard model dimension is
replaced with effective model dimension in the penalty term of the BIC score.
Effective dimensions are difficult to compute. In this paper, we prove a
theorem that relates the effective dimension of an HLC model to the effective
dimensions of a number of latent class models. The theorem makes it
computationally feasible to compute the effective dimensions of large HLC
models. The theorem can also be used to compute the effective dimensions of
general tree models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0028</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0028</id><created>2011-06-30</created><authors><author><keyname>Babaioff</keyname><forenames>M.</forenames></author><author><keyname>Nisan</keyname><forenames>N.</forenames></author></authors><title>Concurrent Auctions Across The Supply Chain</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  595-629, 2004</journal-ref><doi>10.1613/jair.1316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent technological feasibility of electronic commerce over the
Internet, much attention has been given to the design of electronic markets for
various types of electronically-tradable goods. Such markets, however, will
normally need to function in some relationship with markets for other related
goods, usually those downstream or upstream in the supply chain. Thus, for
example, an electronic market for rubber tires for trucks will likely need to
be strongly influenced by the rubber market as well as by the truck market. In
this paper we design protocols for exchange of information between a sequence
of markets along a single supply chain. These protocols allow each of these
markets to function separately, while the information exchanged ensures
efficient global behavior across the supply chain. Each market that forms a
link in the supply chain operates as a double auction, where the bids on one
side of the double auction come from bidders in the corresponding segment of
the industry, and the bids on the other side are synthetically generated by the
protocol to express the combined information from all other links in the chain.
The double auctions in each of the markets can be of several types, and we
study several variants of incentive compatible double auctions, comparing them
in terms of their efficiency and of the market revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0029</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0029</id><created>2011-06-30</created><authors><author><keyname>Goker</keyname><forenames>M. H.</forenames></author><author><keyname>Langley</keyname><forenames>P.</forenames></author><author><keyname>Thompson</keyname><forenames>C. A.</forenames></author></authors><title>A Personalized System for Conversational Recommendations</title><categories>cs.IR cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  393-428, 2004</journal-ref><doi>10.1613/jair.1318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for and making decisions about information is becoming increasingly
difficult as the amount of information and number of choices increases.
Recommendation systems help users find items of interest of a particular type,
such as movies or restaurants, but are still somewhat awkward to use. Our
solution is to take advantage of the complementary strengths of personalized
recommendation systems and dialogue systems, creating personalized aides. We
present a system -- the Adaptive Place Advisor -- that treats item selection as
an interactive, conversational process, with the program inquiring about item
attributes and the user responding. Individual, long-term user preferences are
unobtrusively obtained in the course of normal recommendation dialogues and
used to direct future conversations with the same user. We present a novel user
model that influences both item search and the questions asked during a
conversation. We demonstrate the effectiveness of our system in significantly
reducing the time and number of interactions required to find a satisfactory
item, as compared to a control group of users interacting with a non-adaptive
version of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0030</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0030</id><created>2011-06-30</created><authors><author><keyname>Arieli</keyname><forenames>O.</forenames></author><author><keyname>Bruynooghe</keyname><forenames>M.</forenames></author><author><keyname>Denecker</keyname><forenames>M.</forenames></author><author><keyname>Van Nuffelen</keyname><forenames>B.</forenames></author></authors><title>Coherent Integration of Databases by Abductive Logic Programming</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  245-286, 2004</journal-ref><doi>10.1613/jair.1322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an abductive method for a coherent integration of independent
data-sources. The idea is to compute a list of data-facts that should be
inserted to the amalgamated database or retracted from it in order to restore
its consistency. This method is implemented by an abductive solver, called
Asystem, that applies SLDNFA-resolution on a meta-theory that relates
different, possibly contradicting, input databases. We also give a pure
model-theoretic analysis of the possible ways to `recover' consistent data from
an inconsistent database in terms of those models of the database that exhibit
as minimal inconsistent information as reasonably possible. This allows us to
characterize the `recovered databases' in terms of the `preferred' (i.e., most
consistent) models of the theory. The outcome is an abductive-based application
that is sound and complete with respect to a corresponding model-based,
preferential semantics, and -- to the best of our knowledge -- is more
expressive (thus more general) than any other implementation of coherent
integration of databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0031</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0031</id><created>2011-06-30</created><authors><author><keyname>Gorniak</keyname><forenames>P.</forenames></author><author><keyname>Roy</keyname><forenames>D.</forenames></author></authors><title>Grounded Semantic Composition for Visual Scenes</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  429-470, 2004</journal-ref><doi>10.1613/jair.1327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a visually-grounded language understanding model based on a study
of how people verbally describe objects in scenes. The emphasis of the model is
on the combination of individual word meanings to produce meanings for complex
referring expressions. The model has been implemented, and it is able to
understand a broad range of spatial referring expressions. We describe our
implementation of word level visually-grounded semantics and their embedding in
a compositional parsing framework. The implemented system selects the correct
referents in response to natural language expressions for a large percentage of
test cases. In an analysis of the system's successes and failures we reveal how
visual context influences the semantics of utterances and propose future
extensions to the model that take such context into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0033</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0033</id><created>2011-06-30</created><authors><author><keyname>Bowling</keyname><forenames>M.</forenames></author><author><keyname>Veloso</keyname><forenames>M.</forenames></author></authors><title>Existence of Multiagent Equilibria with Limited Agents</title><categories>cs.MA cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  353-384, 2004</journal-ref><doi>10.1613/jair.1332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiagent learning is a necessary yet challenging problem as multiagent
systems become more prevalent and environments become more dynamic. Much of the
groundbreaking work in this area draws on notable results from game theory, in
particular, the concept of Nash equilibria. Learners that directly learn an
equilibrium obviously rely on their existence. Learners that instead seek to
play optimally with respect to the other players also depend upon equilibria
since equilibria are fixed points for learning. From another perspective,
agents with limitations are real and common. These may be undesired physical
limitations as well as self-imposed rational limitations, such as abstraction
and approximation techniques, used to make learning tractable. This article
explores the interactions of these two important concepts: equilibria and
limitations in learning. We introduce the question of whether equilibria
continue to exist when agents have limitations. We look at the general effects
limitations can have on agent behavior, and define a natural extension of
equilibria that accounts for these limitations. Using this formalization, we
make three major contributions: (i) a counterexample for the general existence
of equilibria with limitations, (ii) sufficient conditions on limitations that
preserve their existence, (iii) three general classes of games and limitations
that satisfy these conditions. We then present empirical results from a
specific multiagent learning algorithm applied to a specific instance of
limited agents. These results demonstrate that learning with limitations is
feasible, when the conditions outlined by our theoretical analysis hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0034</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0034</id><created>2011-06-30</created><authors><author><keyname>Lochner</keyname><forenames>K. M.</forenames></author><author><keyname>Reeves</keyname><forenames>D. M.</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Y.</forenames></author><author><keyname>Wellman</keyname><forenames>M. P.</forenames></author></authors><title>Price Prediction in a Trading Agent Competition</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  19-36, 2004</journal-ref><doi>10.1613/jair.1333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2002 Trading Agent Competition (TAC) presented a challenging market game
in the domain of travel shopping. One of the pivotal issues in this domain is
uncertainty about hotel prices, which have a significant influence on the
relative cost of alternative trip schedules. Thus, virtually all participants
employ some method for predicting hotel prices. We survey approaches employed
in the tournament, finding that agents apply an interesting diversity of
techniques, taking into account differing sources of evidence bearing on
prices. Based on data provided by entrants on their agents' actual predictions
in the TAC-02 finals and semifinals, we analyze the relative efficacy of these
approaches. The results show that taking into account game-specific information
about flight prices is a major distinguishing factor. Machine learning methods
effectively induce the relationship between flight and hotel prices from game
data, and a purely analytical approach based on competitive equilibrium
analysis achieves equal accuracy with no historical data. Employing a new
measure of prediction quality, we relate absolute accuracy to bottom-line
performance in the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0035</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0035</id><created>2011-06-30</created><authors><author><keyname>Keppens</keyname><forenames>J.</forenames></author><author><keyname>Shen</keyname><forenames>Q.</forenames></author></authors><title>Compositional Model Repositories via Dynamic Constraint Satisfaction
  with Order-of-Magnitude Preferences</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  499-550, 2004</journal-ref><doi>10.1613/jair.1335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The predominant knowledge-based approach to automated model construction,
compositional modelling, employs a set of models of particular functional
components. Its inference mechanism takes a scenario describing the constituent
interacting components of a system and translates it into a useful mathematical
model. This paper presents a novel compositional modelling approach aimed at
building model repositories. It furthers the field in two respects. Firstly, it
expands the application domain of compositional modelling to systems that can
not be easily described in terms of interacting functional components, such as
ecological systems. Secondly, it enables the incorporation of user preferences
into the model selection process. These features are achieved by casting the
compositional modelling problem as an activity-based dynamic preference
constraint satisfaction problem, where the dynamic constraints describe the
restrictions imposed over the composition of partial models and the preferences
correspond to those of the user of the automated modeller. In addition, the
preference levels are represented through the use of symbolic values that
differ in orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0036</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0036</id><created>2011-06-30</created><authors><author><keyname>Borodin</keyname><forenames>A.</forenames></author><author><keyname>El-Yaniv</keyname><forenames>R.</forenames></author><author><keyname>Gogan</keyname><forenames>V.</forenames></author></authors><title>Can We Learn to Beat the Best Stock</title><categories>cs.AI q-fin.TR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  579-594, 2004</journal-ref><doi>10.1613/jair.1336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel algorithm for actively trading stocks is presented. While traditional
expert advice and &quot;universal&quot; algorithms (as well as standard technical trading
heuristics) attempt to predict winners or trends, our approach relies on
predictable statistical relations between all pairs of stocks in the market.
Our empirical results on historical markets provide strong evidence that this
type of technical trading can &quot;beat the market&quot; and moreover, can beat the best
stock in the market. In doing so we utilize a new idea for smoothing critical
parameters in the context of expert learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0037</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0037</id><created>2011-06-30</created><authors><author><keyname>Miikkulainen</keyname><forenames>R.</forenames></author><author><keyname>Stanley</keyname><forenames>K. O.</forenames></author></authors><title>Competitive Coevolution through Evolutionary Complexification</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  63-100, 2004</journal-ref><doi>10.1613/jair.1338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two major goals in machine learning are the discovery and improvement of
solutions to complex problems. In this paper, we argue that complexification,
i.e. the incremental elaboration of solutions through adding new structure,
achieves both these goals. We demonstrate the power of complexification through
the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves
increasingly complex neural network architectures. NEAT is applied to an
open-ended coevolutionary robot duel domain where robot controllers compete
head to head. Because the robot duel domain supports a wide range of
strategies, and because coevolution benefits from an escalating arms race, it
serves as a suitable testbed for studying complexification. When compared to
the evolution of networks with fixed structure, complexifying evolution
discovers significantly more sophisticated strategies. The results suggest that
in order to discover and improve complex solutions, evolution, and search in
general, should be allowed to complexify as well as optimize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0038</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0038</id><created>2011-06-30</created><authors><author><keyname>Hnich</keyname><forenames>B.</forenames></author><author><keyname>Smith</keyname><forenames>B. M.</forenames></author><author><keyname>Walsh</keyname><forenames>T.</forenames></author></authors><title>Dual Modelling of Permutation and Injection Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  357-391, 2004</journal-ref><doi>10.1613/jair.1351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When writing a constraint program, we have to choose which variables should
be the decision variables, and how to represent the constraints on these
variables. In many cases, there is considerable choice for the decision
variables. Consider, for example, permutation problems in which we have as many
values as variables, and each variable takes an unique value. In such problems,
we can choose between a primal and a dual viewpoint. In the dual viewpoint,
each dual variable represents one of the primal values, whilst each dual value
represents one of the primal variables. Alternatively, by means of channelling
constraints to link the primal and dual variables, we can have a combined model
with both sets of variables. In this paper, we perform an extensive theoretical
and empirical study of such primal, dual and combined models for two classes of
problems: permutation problems and injection problems. Our results show that it
often be advantageous to use multiple viewpoints, and to have constraints which
channel between them to maintain consistency. They also illustrate a general
methodology for comparing different constraint models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0040</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0040</id><created>2011-06-30</created><authors><author><keyname>Dixon</keyname><forenames>H. E.</forenames></author><author><keyname>Ginsberg</keyname><forenames>M. L.</forenames></author><author><keyname>Parkes</keyname><forenames>A. J.</forenames></author></authors><title>Generalizing Boolean Satisfiability I: Background and Survey of Existing
  Work</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  193-243, 2004</journal-ref><doi>10.1613/jair.1353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the first of three planned papers describing ZAP, a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high-performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper is a
survey of the work underlying ZAP, and discusses previous attempts to improve
the performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting
the structure of the problem being solved. We examine existing ideas including
extensions of the Boolean language to allow cardinality constraints,
pseudo-Boolean representations, symmetry, and a limited form of quantification.
While this paper is intended as a survey, our research results are contained in
the two subsequent articles, with the theoretical structure of ZAP described in
the second paper in this series, and ZAP's implementation described in the
third.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0041</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0041</id><created>2011-06-30</created><authors><author><keyname>Ben-Yair</keyname><forenames>A.</forenames></author><author><keyname>Felner</keyname><forenames>A.</forenames></author><author><keyname>Kraus</keyname><forenames>S.</forenames></author><author><keyname>Netanyahu</keyname><forenames>N.</forenames></author><author><keyname>Stern</keyname><forenames>R.</forenames></author></authors><title>PHA*: Finding the Shortest Path with A* in An Unknown Physical
  Environment</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  631-670, 2004</journal-ref><doi>10.1613/jair.1373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of finding the shortest path between two points in an
unknown real physical environment, where a traveling agent must move around in
the environment to explore unknown territory. We introduce the Physical-A*
algorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes
that A* would expand and returns the shortest path between the two points.
However, due to the physical nature of the problem, the complexity of the
algorithm is measured by the traveling effort of the moving agent and not by
the number of generated nodes, as in standard A*. PHA* is presented as a
two-level algorithm, such that its high level, A*, chooses the next node to be
expanded and its low level directs the agent to that node in order to explore
it. We present a number of variations for both the high-level and low-level
procedures and evaluate their performance theoretically and experimentally. We
show that the travel cost of our best variation is fairly close to the optimal
travel cost, assuming that the mandatory nodes of A* are known in advance. We
then generalize our algorithm to the multi-agent case, where a number of
cooperative agents are designed to solve the problem. Specifically, we provide
an experimental implementation for such a system. It should be noted that the
problem addressed here is not a navigation problem, but rather a problem of
finding the shortest path between two points for future usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0042</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0042</id><created>2011-06-30</created><authors><author><keyname>Zhang</keyname><forenames>N. L.</forenames></author><author><keyname>Zhang</keyname><forenames>W.</forenames></author></authors><title>Restricted Value Iteration: Theory and Algorithms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  123-165, 2005</journal-ref><doi>10.1613/jair.1379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Value iteration is a popular algorithm for finding near optimal policies for
POMDPs. It is inefficient due to the need to account for the entire belief
space, which necessitates the solution of large numbers of linear programs. In
this paper, we study value iteration restricted to belief subsets. We show
that, together with properly chosen belief subsets, restricted value iteration
yields near-optimal policies and we give a condition for determining whether a
given belief subset would bring about savings in space and time. We also apply
restricted value iteration to two interesting classes of POMDPs, namely
informative POMDPs and near-discernible POMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0043</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0043</id><created>2011-06-30</created><authors><author><keyname>Cohen</keyname><forenames>D.</forenames></author><author><keyname>Cooper</keyname><forenames>M.</forenames></author><author><keyname>Jeavons</keyname><forenames>P.</forenames></author><author><keyname>Krokhin</keyname><forenames>A.</forenames></author></authors><title>A Maximal Tractable Class of Soft Constraints</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  1-22, 2004</journal-ref><doi>10.1613/jair.1400</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many researchers in artificial intelligence are beginning to explore the use
of soft constraints to express a set of (possibly conflicting) problem
requirements. A soft constraint is a function defined on a collection of
variables which associates some measure of desirability with each possible
combination of values for those variables. However, the crucial question of the
computational complexity of finding the optimal solution to a collection of
soft constraints has so far received very little attention. In this paper we
identify a class of soft binary constraints for which the problem of finding
the optimal solution is tractable. In other words, we show that for any given
set of such constraints, there exists a polynomial time algorithm to determine
the assignment having the best overall combined measure of desirability. This
tractable class includes many commonly-occurring soft constraints, such as 'as
near as possible' or 'as soon as possible after', as well as crisp constraints
such as 'greater than'. Finally, we show that this tractable class is maximal,
in the sense that adding any other form of soft binary constraint which is not
in the class gives rise to a class of problems which is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0044</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0044</id><created>2011-06-30</created><authors><author><keyname>Beame</keyname><forenames>P.</forenames></author><author><keyname>Kautz</keyname><forenames>H.</forenames></author><author><keyname>Sabharwal</keyname><forenames>A.</forenames></author></authors><title>Towards Understanding and Harnessing the Potential of Clause Learning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  319-351, 2004</journal-ref><doi>10.1613/jair.1410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient implementations of DPLL with the addition of clause learning are
the fastest complete Boolean satisfiability solvers and can handle many
significant real-world problems, such as verification, planning and design.
Despite its importance, little is known of the ultimate strengths and
limitations of the technique. This paper presents the first precise
characterization of clause learning as a proof system (CL), and begins the task
of understanding its power by relating it to the well-studied resolution proof
system. In particular, we show that with a new learning scheme, CL can provide
exponentially shorter proofs than many proper refinements of general resolution
(RES) satisfying a natural property. These include regular and Davis-Putnam
resolution, which are already known to be much stronger than ordinary DPLL. We
also show that a slight variant of CL with unlimited restarts is as powerful as
RES itself. Translating these analytical results to practice, however, presents
a challenge because of the nondeterministic nature of clause learning
algorithms. We propose a novel way of exploiting the underlying problem
structure, in the form of a high level problem description such as a graph or
PDDL specification, to guide clause learning algorithms toward faster
solutions. We show that this leads to exponential speed-ups on grid and
randomized pebbling problems, as well as substantial improvements on certain
ordering formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0045</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0045</id><created>2011-06-30</created><authors><author><keyname>Cayrol</keyname><forenames>C.</forenames></author><author><keyname>Lagasquie-Schiex</keyname><forenames>M. C.</forenames></author></authors><title>Graduality in Argumentation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  245-297, 2005</journal-ref><doi>10.1613/jair.1411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Argumentation is based on the exchange and valuation of interacting
arguments, followed by the selection of the most acceptable of them (for
example, in order to take a decision, to make a choice). Starting from the
framework proposed by Dung in 1995, our purpose is to introduce 'graduality' in
the selection of the best arguments, i.e., to be able to partition the set of
the arguments in more than the two usual subsets of 'selected' and
'non-selected' arguments in order to represent different levels of selection.
Our basic idea is that an argument is all the more acceptable if it can be
preferred to its attackers. First, we discuss general principles underlying a
'gradual' valuation of arguments based on their interactions. Following these
principles, we define several valuation models for an abstract argumentation
system. Then, we introduce 'graduality' in the concept of acceptability of
arguments. We propose new acceptability classes and a refinement of existing
classes taking advantage of an available 'gradual' valuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0046</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0046</id><created>2011-06-30</created><authors><author><keyname>Derbeko</keyname><forenames>P.</forenames></author><author><keyname>El-Yaniv</keyname><forenames>R.</forenames></author><author><keyname>Meir</keyname><forenames>R.</forenames></author></authors><title>Explicit Learning Curves for Transduction and Application to Clustering
  and Compression Algorithms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  117-142, 2004</journal-ref><doi>10.1613/jair.1417</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inductive learning is based on inferring a general rule from a finite data
set and using it to label new data. In transduction one attempts to solve the
problem of using a labeled training set to label a set of unlabeled points,
which are given to the learner prior to learning. Although transduction seems
at the outset to be an easier task than induction, there have not been many
provably useful algorithms for transduction. Moreover, the precise relation
between induction and transduction has not yet been determined. The main
theoretical developments related to transduction were presented by Vapnik more
than twenty years ago. One of Vapnik's basic results is a rather tight error
bound for transductive classification based on an exact computation of the
hypergeometric tail. While tight, this bound is given implicitly via a
computational routine. Our first contribution is a somewhat looser but explicit
characterization of a slightly extended PAC-Bayesian version of Vapnik's
transductive bound. This characterization is obtained using concentration
inequalities for the tail of sums of random variables obtained by sampling
without replacement. We then derive error bounds for compression schemes such
as (transductive) support vector machines and for transduction algorithms based
on clustering. The main observation used for deriving these new error bounds
and algorithms is that the unlabeled test points, which in the transductive
setting are known in advance, can be used in order to construct useful data
dependent prior distributions over the hypothesis space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0047</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0047</id><created>2011-06-30</created><authors><author><keyname>Goldman</keyname><forenames>C. V.</forenames></author><author><keyname>Zilberstein</keyname><forenames>S.</forenames></author></authors><title>Decentralized Control of Cooperative Systems: Categorization and
  Complexity Analysis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  143-174, 2004</journal-ref><doi>10.1613/jair.1427</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized control of cooperative systems captures the operation of a
group of decision makers that share a single global objective. The difficulty
in solving optimally such problems arises when the agents lack full
observability of the global state of the system when they operate. The general
problem has been shown to be NEXP-complete. In this paper, we identify classes
of decentralized control problems whose complexity ranges between NEXP and P.
In particular, we study problems characterized by independent transitions,
independent observations, and goal-oriented objective functions. Two algorithms
are shown to solve optimally useful classes of goal-oriented decentralized
processes in polynomial time. This paper also studies information sharing among
the decision-makers, which can improve their performance. We distinguish
between three ways in which agents can exchange information: indirect
communication, direct communication and sharing state features that are not
controlled by the agents. Our analysis shows that for every class of problems
we consider, introducing direct or indirect communication does not change the
worst-case complexity. The results provide a better understanding of the
complexity of decentralized control problems that arise in practice and
facilitate the development of planning algorithms for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0048</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0048</id><created>2011-06-30</created><authors><author><keyname>Celaya</keyname><forenames>E.</forenames></author><author><keyname>Porta</keyname><forenames>J. M.</forenames></author></authors><title>Reinforcement Learning for Agents with Many Sensors and Actuators Acting
  in Categorizable Environments</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  79-122, 2005</journal-ref><doi>10.1613/jair.1437</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we confront the problem of applying reinforcement learning to
agents that perceive the environment through many sensors and that can perform
parallel actions using many actuators as is the case in complex autonomous
robots. We argue that reinforcement learning can only be successfully applied
to this case if strong assumptions are made on the characteristics of the
environment in which the learning is performed, so that the relevant sensor
readings and motor commands can be readily identified. The introduction of such
assumptions leads to strongly-biased learning systems that can eventually lose
the generality of traditional reinforcement-learning algorithms. In this line,
we observe that, in realistic situations, the reward received by the robot
depends only on a reduced subset of all the executed actions and that only a
reduced subset of the sensor inputs (possibly different in each situation and
for each action) are relevant to predict the reward. We formalize this property
in the so called 'categorizability assumption' and we present an algorithm that
takes advantage of the categorizability of the environment, allowing a decrease
in the learning time with respect to existing reinforcement-learning
algorithms. Results of the application of the algorithm to a couple of
simulated realistic-robotic problems (landmark-based navigation and the
six-legged robot gait generation) are reported to validate our approach and to
compare it to existing flat and generalization-based reinforcement-learning
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0050</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0050</id><created>2011-06-30</created><authors><author><keyname>Felner</keyname><forenames>A.</forenames></author><author><keyname>Hanan</keyname><forenames>S.</forenames></author><author><keyname>Korf</keyname><forenames>R. E.</forenames></author></authors><title>Additive Pattern Database Heuristics</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  279-318, 2004</journal-ref><doi>10.1613/jair.1480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a method for computing admissible heuristic evaluation functions
for search problems. It utilizes pattern databases, which are precomputed
tables of the exact cost of solving various subproblems of an existing problem.
Unlike standard pattern database heuristics, however, we partition our problems
into disjoint subproblems, so that the costs of solving the different
subproblems can be added together without overestimating the cost of solving
the original problem. Previously, we showed how to statically partition the
sliding-tile puzzles into disjoint groups of tiles to compute an admissible
heuristic, using the same partition for each state and problem instance. Here
we extend the method and show that it applies to other domains as well. We also
present another method for additive heuristics which we call dynamically
partitioned pattern databases. Here we partition the problem into disjoint
subproblems for each state of the search dynamically. We discuss the pros and
cons of each of these methods and apply both methods to three different problem
domains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and
finding an optimal vertex cover of a graph. We find that in some problem
domains, static partitioning is most effective, while in others dynamic
partitioning is a better choice. In each of these problem domains, either
statically partitioned or dynamically partitioned pattern database heuristics
are the best known heuristics for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0051</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0051</id><created>2011-06-30</created><authors><author><keyname>Begleiter</keyname><forenames>R.</forenames></author><author><keyname>El-Yaniv</keyname><forenames>R.</forenames></author><author><keyname>Yona</keyname><forenames>G.</forenames></author></authors><title>On Prediction Using Variable Order Markov Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  385-421, 2004</journal-ref><doi>10.1613/jair.1491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with algorithms for prediction of discrete sequences
over a finite alphabet, using variable order Markov models. The class of such
algorithms is large and in principle includes any lossless compression
algorithm. We focus on six prominent prediction algorithms, including Context
Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic
Suffix Trees (PSTs). We discuss the properties of these algorithms and compare
their performance using real life sequences from three domains: proteins,
English text and music pieces. The comparison is made with respect to
prediction quality as measured by the average log-loss. We also compare
classification algorithms based on these predictors with respect to a number of
large protein classification tasks. Our results indicate that a &quot;decomposed&quot;
CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in
sequence prediction tasks. Somewhat surprisingly, a different algorithm, which
is a modification of the Lempel-Ziv compression algorithm, significantly
outperforms all algorithms on the protein classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0052</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0052</id><created>2011-06-30</created><authors><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author><author><keyname>Porteous</keyname><forenames>J.</forenames></author><author><keyname>Sebastia</keyname><forenames>L.</forenames></author></authors><title>Ordered Landmarks in Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  215-278, 2004</journal-ref><doi>10.1613/jair.1492</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many known planning tasks have inherent constraints concerning the best order
in which to achieve the goals. A number of research efforts have been made to
detect such constraints and to use them for guiding search, in the hope of
speeding up the planning process. We go beyond the previous approaches by
considering ordering constraints not only over the (top-level) goals, but also
over the sub-goals that will necessarily arise during planning. Landmarks are
facts that must be true at some point in every valid solution plan. We extend
Koehler and Hoffmann's definition of reasonable orders between top level goals
to the more general case of landmarks. We show how landmarks can be found, how
their reasonable orders can be approximated, and how this information can be
used to decompose a given planning task into several smaller sub-tasks. Our
methodology is completely domain- and planner-independent. The implementation
demonstrates that the approach can yield significant runtime performance
improvements when used as a control loop around state-of-the-art sub-optimal
planning systems, as exemplified by FF and LPG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0053</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0053</id><created>2011-06-30</created><updated>2011-10-04</updated><authors><author><keyname>Roy</keyname><forenames>N.</forenames></author><author><keyname>Gordon</keyname><forenames>G.</forenames></author><author><keyname>Thrun</keyname><forenames>S.</forenames></author></authors><title>Finding Approximate POMDP solutions Through Belief Compression</title><categories>cs.AI</categories><proxy>Daniel Bryce</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  1-40, 2005</journal-ref><doi>10.1613/jair.1496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard value function approaches to finding policies for Partially
Observable Markov Decision Processes (POMDPs) are generally considered to be
intractable for large models. The intractability of these algorithms is to a
large extent a consequence of computing an exact, optimal policy over the
entire belief space. However, in real-world POMDP problems, computing the
optimal policy for the full belief space is often unnecessary for good control
even for problems with complicated policy classes. The beliefs experienced by
the controller often lie near a structured, low-dimensional subspace embedded
in the high-dimensional belief space. Finding a good approximation to the
optimal value function for only this subspace can be much easier than computing
the full value function. We introduce a new method for solving large-scale
POMDPs by reducing the dimensionality of the belief space. We use Exponential
family Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to
represent sparse, high-dimensional belief spaces using small sets of learned
features of the belief state. We then plan only in terms of the low-dimensional
belief features. By planning in this low-dimensional space, we can find
policies for POMDP models that are orders of magnitude larger than models that
can be handled by conventional techniques. We demonstrate the use of this
algorithm on a synthetic problem and on mobile robot navigation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0054</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0054</id><created>2011-06-30</created><authors><author><keyname>Birmingham</keyname><forenames>W. P.</forenames></author><author><keyname>Meek</keyname><forenames>C. J.</forenames></author></authors><title>A Comprehensive Trainable Error Model for Sung Music Queries</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 22, pages
  57-91, 2004</journal-ref><doi>10.1613/jair.1334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model for errors in sung queries, a variant of the hidden Markov
model (HMM). This is a solution to the problem of identifying the degree of
similarity between a (typically error-laden) sung query and a potential target
in a database of musical works, an important problem in the field of music
information retrieval. Similarity metrics are a critical component of
query-by-humming (QBH) applications which search audio and multimedia databases
for strong matches to oral queries. Our model comprehensively expresses the
types of error or variation between target and query: cumulative and
non-cumulative local errors, transposition, tempo and tempo changes,
insertions, deletions and modulation. The model is not only expressive, but
automatically trainable, or able to learn and generalize from query examples.
We present results of simulations, designed to assess the discriminatory
potential of the model, and tests with real sung queries, to demonstrate
relevance to real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0055</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0055</id><created>2011-06-30</created><authors><author><keyname>Zhang</keyname><forenames>W.</forenames></author></authors><title>Phase Transitions and Backbones of the Asymmetric Traveling Salesman
  Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 21, pages
  471-497, 2004</journal-ref><doi>10.1613/jair.1389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been much interest in phase transitions of
combinatorial problems. Phase transitions have been successfully used to
analyze combinatorial optimization problems, characterize their typical-case
features and locate the hardest problem instances. In this paper, we study
phase transitions of the asymmetric Traveling Salesman Problem (ATSP), an
NP-hard combinatorial optimization problem that has many real-world
applications. Using random instances of up to 1,500 cities in which intercity
distances are uniformly distributed, we empirically show that many properties
of the problem, including the optimal tour cost and backbone size, experience
sharp transitions as the precision of intercity distances increases across a
critical value. Our experimental results on the costs of the ATSP tours and
assignment problem agree with the theoretical result that the asymptotic cost
of assignment problem is pi ^2 /6 the number of cities goes to infinity. In
addition, we show that the average computational cost of the well-known
branch-and-bound subtour elimination algorithm for the problem also exhibits a
thrashing behavior, transitioning from easy to difficult as the distance
precision increases. These results answer positively an open question regarding
the existence of phase transitions in the ATSP, and provide guidance on how
difficult ATSP problem instances should be generated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0056</identifier>
 <datestamp>2011-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0056</id><created>2011-06-30</created><updated>2011-09-13</updated><authors><author><keyname>Campos</keyname><forenames>Victor</forenames></author><author><keyname>Linhares-Sales</keyname><forenames>Cl&#xe1;udia</forenames></author><author><keyname>Maia</keyname><forenames>Ana Karolinna</forenames></author><author><keyname>Martins</keyname><forenames>Nicolas</forenames></author><author><keyname>Sampaio</keyname><forenames>Rudini Menezes</forenames></author></authors><title>Fixed parameter algorithms for restricted coloring problems</title><categories>cs.DM cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain polynomial time algorithms to determine the acyclic
chromatic number, the star chromatic number, the Thue chromatic number, the
harmonious chromatic number and the clique chromatic number of $P_4$-tidy
graphs and $(q,q-4)$-graphs, for every fixed $q$. These classes include
cographs, $P_4$-sparse and $P_4$-lite graphs. All these coloring problems are
known to be NP-hard for general graphs. These algorithms are fixed parameter
tractable on the parameter $q(G)$, which is the minimum $q$ such that $G$ is a
$(q,q-4)$-graph. We also prove that every connected $(q,q-4)$-graph with at
least $q$ vertices is 2-clique-colorable and that every acyclic coloring of a
cograph is also nonrepetitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0062</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0062</id><created>2011-06-30</created><authors><author><keyname>Ulusoy</keyname><forenames>Alphan</forenames></author><author><keyname>Smith</keyname><forenames>Stephen L.</forenames></author><author><keyname>Ding</keyname><forenames>Xu Chu</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Optimal Multi-Robot Path Planning with Temporal Logic Constraints</title><categories>cs.RO</categories><comments>Extended version of the IROS 2011 conference paper</comments><report-no>2011-IR-0009</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method for automatically planning optimal paths
for a group of robots that satisfy a common high level mission specification.
Each robot's motion in the environment is modeled as a weighted transition
system. The mission is given as a Linear Temporal Logic formula. In addition,
an optimizing proposition must repeatedly be satisfied. The goal is to minimize
the maximum time between satisfying instances of the optimizing proposition.
Our method is guaranteed to compute an optimal set of robot paths. We utilize a
timed automaton representation in order to capture the relative position of the
robots in the environment. We then obtain a bisimulation of this timed
automaton as a finite transition system that captures the joint behavior of the
robots and apply our earlier algorithm for the single robot case to optimize
the group motion. We present a simulation of a persistent monitoring task in a
road network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0063</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0063</id><created>2011-06-30</created><authors><author><keyname>&#xd6;lveczky</keyname><forenames>Peter Csaba</forenames><affiliation>University of Oslo</affiliation></author></authors><title>Formal Model Engineering for Embedded Systems Using Real-Time Maude</title><categories>cs.LO cs.SE</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 56, 2011, pp. 3-13</journal-ref><doi>10.4204/EPTCS.56.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper motivates why Real-Time Maude should be well suited to provide a
formal semantics and formal analysis capabilities to modeling languages for
embedded systems. One can then use the code generation facilities of the tools
for the modeling languages to automatically synthesize Real-Time Maude
verification models from design models, enabling a formal model engineering
process that combines the convenience of modeling using an informal but
intuitive modeling language with formal verification. We give a brief overview
six fairly different modeling formalisms for which Real-Time Maude has provided
the formal semantics and (possibly) formal analysis. These models include
behavioral subsets of the avionics modeling standard AADL, Ptolemy II
discrete-event models, two EMF-based timed model transformation systems, and a
modeling language for handset software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0064</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0064</id><created>2011-06-30</created><authors><author><keyname>Bos</keyname><forenames>Jeroen van den</forenames><affiliation>CWI</affiliation></author><author><keyname>Hills</keyname><forenames>Mark</forenames><affiliation>CWI</affiliation></author><author><keyname>Klint</keyname><forenames>Paul</forenames><affiliation>CWI</affiliation></author><author><keyname>van der Storm</keyname><forenames>Tijs</forenames><affiliation>CWI</affiliation></author><author><keyname>Vinju</keyname><forenames>Jurgen J.</forenames><affiliation>CWI</affiliation></author></authors><title>Rascal: From Algebraic Specification to Meta-Programming</title><categories>cs.PL cs.SE</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><acm-class>D.3.2</acm-class><journal-ref>EPTCS 56, 2011, pp. 15-32</journal-ref><doi>10.4204/EPTCS.56.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic specification has a long tradition in bridging the gap between
specification and programming by making specifications executable. Building on
extensive experience in designing, implementing and using specification
formalisms that are based on algebraic specification and term rewriting (namely
Asf and Asf+Sdf), we are now focusing on using the best concepts from algebraic
specification and integrating these into a new programming language: Rascal.
This language is easy to learn by non-experts but is also scalable to very
large meta-programming applications.
  We explain the algebraic roots of Rascal and its main application areas:
software analysis, software transformation, and design and implementation of
domain-specific languages. Some example applications in the domain of
Model-Driven Engineering (MDE) are described to illustrate this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0065</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0065</id><created>2011-06-30</created><authors><author><keyname>Boisvert</keyname><forenames>Bertrand</forenames></author><author><keyname>F&#xe9;raud</keyname><forenames>Louis</forenames></author><author><keyname>Soloviev</keyname><forenames>Sergei</forenames></author></authors><title>Typed lambda-terms in categorical attributed graph transformation</title><categories>cs.LO</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 56, 2011, pp. 33-47</journal-ref><doi>10.4204/EPTCS.56.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with model transformation based on attributed graph
rewriting. Our contribution investigates a single pushout approach for applying
the rewrite rules. The computation of graph attributes is obtained through the
use of typed lambda-calculus with inductive types. In this paper we present
solutions to cope with single pushout construction for the graph structure and
the computations functions. As this rewrite system uses inductive types, the
expressiveness of attribute computations is facilitated and appears more
efficient than the one based on Sigma-algebras. Some examples showing the
interest of our computation approach are described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0066</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0066</id><created>2011-06-30</created><authors><author><keyname>Dur&#xe1;n</keyname><forenames>Francisco</forenames><affiliation>Universidad de M&#xe1;laga</affiliation></author><author><keyname>&#xd6;lveczky</keyname><forenames>Peter Csaba</forenames><affiliation>University of Oslo</affiliation></author><author><keyname>Rivera</keyname><forenames>Jos&#xe9; E.</forenames><affiliation>Universidad de M&#xe1;laga</affiliation></author></authors><title>Formal Visual Modeling of Real-Time Systems in e-Motions: Two Case
  Studies</title><categories>cs.SE cs.LO</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 56, 2011, pp. 49-63</journal-ref><doi>10.4204/EPTCS.56.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  e-Motions is an Eclipse-based visual timed model transformation framework
with a Real-Time Maude semantics that supports the usual Maude formal analysis
methods, including simulation, reachability analysis, and LTL model checking.
e-Motions is characterized by a novel and powerful set of constructs for
expressing timed behaviors. In this paper we illustrate the use of these
constructs --- and thereby implicitly investigate their suitability to define
real-time systems in an intuitive way --- to define and formally analyze two
prototypical and very different real-time systems: (i) a simple round trip time
protocol for computing the time it takes a message to travel from one node to
another, and back; and (ii) the EDF scheduling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0067</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0067</id><created>2011-06-30</created><authors><author><keyname>Andova</keyname><forenames>Suzana</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Brand</keyname><forenames>Mark van den</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Engelen</keyname><forenames>Luc</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Prototyping the Semantics of a DSL using ASF+SDF: Link to Formal
  Verification of DSL Models</title><categories>cs.SE</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 56, 2011, pp. 65-79</journal-ref><doi>10.4204/EPTCS.56.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A formal definition of the semantics of a domain-specific language (DSL) is a
key prerequisite for the verification of the correctness of models specified
using such a DSL and of transformations applied to these models. For this
reason, we implemented a prototype of the semantics of a DSL for the
specification of systems consisting of concurrent, communicating objects. Using
this prototype, models specified in the DSL can be transformed to labeled
transition systems (LTS). This approach of transforming models to LTSs allows
us to apply existing tools for visualization and verification to models with
little or no further effort. The prototype is implemented using the ASF+SDF
Meta-Environment, an IDE for the algebraic specification language ASF+SDF,
which offers efficient execution of the transformation as well as the ability
to read models and produce LTSs without any additional pre or post processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0068</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0068</id><created>2011-06-30</created><authors><author><keyname>Dur&#xe1;n</keyname><forenames>Francisco</forenames><affiliation>Universidad de M&#xe1;laga</affiliation></author><author><keyname>Gogolla</keyname><forenames>Martin</forenames><affiliation>University of Bremen</affiliation></author><author><keyname>Rold&#xe1;n</keyname><forenames>Manuel</forenames><affiliation>Universidad de M&#xe1;laga</affiliation></author></authors><title>Tracing Properties of UML and OCL Models with Maude</title><categories>cs.SE cs.LO</categories><comments>In Proceedings AMMSE 2011, arXiv:1106.5962</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 56, 2011, pp. 81-97</journal-ref><doi>10.4204/EPTCS.56.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The starting point of this paper is a system described in form of a UML class
diagram where system states are characterized by OCL invariants and system
transitions are defined by OCL pre- and postconditions. The aim of our approach
is to assist the developer in learning about the consequences of the described
system states and transitions and about the formal implications of the
properties that are explicitly given. We propose to draw conclusions about the
stated constraints by translating the UML and OCL model into the algebraic
specification language and system Maude, which is based on rewrite logic. We
will concentrate in this paper on employing Maude's capabilities for state
search. Maude's state search offers the possibility to describe a start
configuration of the system and then explore all configurations reachable by
rewriting. The search can be adjusted by formulating requirements for the
allowed states and the allowed transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0076</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0076</id><created>2011-06-30</created><authors><author><keyname>Mehta</keyname><forenames>Daryush D.</forenames></author><author><keyname>Rudoy</keyname><forenames>Daniel</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>KARMA: Kalman-based autoregressive moving average modeling and inference
  for formant and antiformant tracking</title><categories>stat.AP cs.SD</categories><comments>13 pages, 7 figures; submitted for publication</comments><journal-ref>Journal of the Acoustical Society of America, vol. 132, pp.
  1732-1746, 2012</journal-ref><doi>10.1121/1.4739462</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vocal tract resonance characteristics in acoustic speech signals are
classically tracked using frame-by-frame point estimates of formant frequencies
followed by candidate selection and smoothing using dynamic programming methods
that minimize ad hoc cost functions. The goal of the current work is to provide
both point estimates and associated uncertainties of center frequencies and
bandwidths in a statistically principled state-space framework. Extended Kalman
(K) algorithms take advantage of a linearized mapping to infer formant and
antiformant parameters from frame-based estimates of autoregressive moving
average (ARMA) cepstral coefficients. Error analysis of KARMA, WaveSurfer, and
Praat is accomplished in the all-pole case using a manually marked formant
database and synthesized speech waveforms. KARMA formant tracks exhibit lower
overall root-mean-square error relative to the two benchmark algorithms, with
third formant tracking more challenging. Antiformant tracking performance of
KARMA is illustrated using synthesized and spoken nasal phonemes. The
simultaneous tracking of uncertainty levels enables practitioners to recognize
time-varying confidence in parameters of interest and adjust algorithmic
settings accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0078</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0078</id><created>2011-06-30</created><updated>2012-04-04</updated><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Optimization of UAV Heading for the Ground-to-Air Uplink</title><categories>cs.IT math.IT</categories><comments>31 pages, 10 figures, accepted by IEEE JSAC special issue on
  &quot;Communications Challenges and Dynamics for Unmanned Autonomous Vehicles&quot;,
  Apr. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a collection of single-antenna ground nodes
communicating with a multi-antenna unmanned aerial vehicle (UAV) over a
multiple-access ground-to-air wireless communications link. The UAV uses
beamforming to mitigate the inter-user interference and achieve spatial
division multiple access (SDMA). First, we consider a simple scenario with two
static ground nodes and analytically investigate the effect of the UAV heading
on the system sum rate. We then study a more general setting with multiple
mobile ground-based terminals, and develop an algorithm for dynamically
adjusting the UAV heading in order to maximize a lower bound on the ergodic sum
rate of the uplink channel, using a Kalman filter to track the positions of the
mobile ground nodes. Fairness among the users can be guaranteed through
weighting the bound for each user's ergodic rate with a factor inversely
proportional to their average data rate. For the common scenario where a high
$K$-factor channel exists between the ground nodes and UAV, we use an
asymptotic analysis to find simplified versions of the algorithm for low and
high SNR. We present simulation results that demonstrate the benefits of
adapting the UAV heading in order to optimize the uplink communications
performance. The simulation results also show that the simplified algorithms
perform near-optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0082</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0082</id><created>2011-06-30</created><authors><author><keyname>Brodzik</keyname><forenames>Andrzej K.</forenames></author><author><keyname>Enders</keyname><forenames>Robert H.</forenames></author></authors><title>A case of combination of evidence in the Dempster-Shafer theory
  inconsistent with evaluation of probabilities</title><categories>math.PR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dempster-Shafer theory of evidence accumulation is one of the main tools
for combining data obtained from multiple sources. In this paper a special case
of combination of two bodies of evidence with non-zero conflict coefficient is
considered. It is shown that application of the Dempster-Shafer rule of
combination in this case leads to an evaluation of masses of the combined
bodies that is different from the evaluation of the corresponding probabilities
obtained by application of the law of total probability. This finding supports
the view that probabilistic interpretation of results of the Dempster-Shafer
analysis in the general case is not appropriate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0088</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0088</id><created>2011-06-30</created><updated>2011-10-17</updated><authors><author><keyname>Silva</keyname><forenames>Marcel K. de Carli</forenames></author><author><keyname>Harvey</keyname><forenames>Nicholas J. A.</forenames></author><author><keyname>Sato</keyname><forenames>Cristiane M.</forenames></author></authors><title>Sparse Sums of Positive Semidefinite Matrices</title><categories>cs.DM cs.DS cs.NA math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been much interest in &quot;sparsifying&quot; sums of rank one
matrices: modifying the coefficients such that only a few are nonzero, while
approximately preserving the matrix that results from the sum. Results of this
sort have found applications in many different areas, including sparsifying
graphs. In this paper we consider the more general problem of sparsifying sums
of positive semidefinite matrices that have arbitrary rank.
  We give several algorithms for solving this problem. The first algorithm is
based on the method of Batson, Spielman and Srivastava (2009). The second
algorithm is based on the matrix multiplicative weights update method of Arora
and Kale (2007). We also highlight an interesting connection between these two
algorithms.
  Our algorithms have numerous applications. We show how they can be used to
construct graph sparsifiers with auxiliary constraints, sparsifiers of
hypergraphs, and sparse solutions to semidefinite programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0089</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0089</id><created>2011-06-30</created><authors><author><keyname>Chai</keyname><forenames>Junyi</forenames></author><author><keyname>Liu</keyname><forenames>James N. K.</forenames></author></authors><title>Towards a Reliable Framework of Uncertainty-Based Group Decision Support
  System</title><categories>cs.SY cs.AI</categories><comments>Accepted paper in IEEE-ICDM2010; Print ISBN: 978-1-4244-9244-2</comments><doi>10.1109/ICDMW.2010.80</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a framework of Uncertainty-based Group Decision Support
System (UGDSS). It provides a platform for multiple criteria decision analysis
in six aspects including (1) decision environment, (2) decision problem, (3)
decision group, (4) decision conflict, (5) decision schemes and (6) group
negotiation. Based on multiple artificial intelligent technologies, this
framework provides reliable support for the comprehensive manipulation of
applications and advanced decision approaches through the design of an
integrated multi-agents architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0098</identifier>
 <datestamp>2012-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0098</id><created>2011-06-30</created><updated>2012-08-02</updated><authors><author><keyname>Davydov</keyname><forenames>Alexander Y.</forenames></author></authors><title>A Probabilistic Attack on NP-complete Problems</title><categories>cs.CC cs.AI cs.DM cs.DS</categories><comments>16 pages, 8 figures, 2 tables</comments><msc-class>90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the probability theory-based approach, this paper reveals the
equivalence of an arbitrary NP-complete problem to a problem of checking
whether a level set of a specifically constructed harmonic cost function (with
all diagonal entries of its Hessian matrix equal to zero) intersects with a
unit hypercube in many-dimensional Euclidean space. This connection suggests
the possibility that methods of continuous mathematics can provide crucial
insights into the most intriguing open questions in modern complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0104</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0104</id><created>2011-07-01</created><updated>2013-06-25</updated><authors><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Werner</keyname><forenames>Daniel</forenames></author></authors><title>Approximating Tverberg Points in Linear Time for Any Fixed Dimension</title><categories>cs.CG cs.DS</categories><comments>14 pages, 2 figures. A preliminary version appeared in SoCG 2012</comments><journal-ref>Discrete and Computational Geometry, 50(2), 2013, pp 520-535</journal-ref><doi>10.1007/s00454-013-9528-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a d-dimensional n-point set. A Tverberg-partition of P is a
partition of P into r sets P_1, ..., P_r such that the convex hulls conv(P_1),
..., conv(P_r) have non-empty intersection. A point in the intersection of the
conv(P_i)'s is called a Tverberg point of depth r for P. A classic result by
Tverberg implies that there always exists a Tverberg partition of size n/(d+1),
but it is not known how to find such a partition in polynomial time. Therefore,
approximate solutions are of interest.
  We describe a deterministic algorithm that finds a Tverberg partition of size
n/4(d+1)^3 in time d^{O(log d)} n. This means that for every fixed dimension we
can compute an approximate Tverberg point (and hence also an approximate
centerpoint) in linear time. Our algorithm is obtained by combining a novel
lifting approach with a recent result by Miller and Sheehy (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0118</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0118</id><created>2011-07-01</created><authors><author><keyname>Choudhary</keyname><forenames>Swadesh</forenames><affiliation>Department of Electrical Engg., Indian Institute of Technology, Bombay</affiliation></author><author><keyname>Sharma</keyname><forenames>Hrishikesh</forenames><affiliation>Department of Electrical Engg., Indian Institute of Technology, Bombay</affiliation></author><author><keyname>Patkar</keyname><forenames>Sachin</forenames><affiliation>Department of Electrical Engg., Indian Institute of Technology, Bombay</affiliation></author></authors><title>Optimal Folding of Data Flow Graphs based on Finite Projective Geometry
  using Lattice Embedding</title><categories>cs.DM math.CO</categories><comments>31 pages, to be submitted to some discrete mathematics journal</comments><msc-class>51E20</msc-class><journal-ref>Discrete Mathematics, Algorithms and Applications Vol. 5, No. 4
  (2013)</journal-ref><doi>10.1142/S1793830913500365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of computations exist, especially in area of error-control coding
and matrix computations, whose underlying data flow graphs are based on finite
projective-geometry(PG) based balanced bipartite graphs. Many of these
applications are actively being researched upon. Almost all these applications
need bipartite graphs of the order of tens of thousands in practice, whose
nodes represent parallel computations. To reduce its implementation cost,
reducing amount of system/hardware resources during design is an important
engineering objective. In this context, we present a scheme to reduce resource
utilization when performing computations derived from PG-based graphs. In a
fully parallel design based on PG concepts, the number of processing units is
equal to the number of vertices, each performing an atomic computation. To
reduce the number of processing units used for implementation, we present an
easy way of partitioning the vertex set. Each block of partition is then
assigned to a processing unit. A processing unit performs the computations
corresponding to the vertices in the block assigned to it in a sequential
fashion, thus creating the effect of folding the overall computation. These
blocks have certain symmetric properties that enable us to develop a
conflict-free schedule. The scheme achieves the best possible throughput, in
lack of any overhead of shuffling data across memories while scheduling another
computation on the same processing unit. This paper reports two folding
schemes, which are based on same lattice embedding approach, based on
partitioning. We first provide a scheme for a projective space of dimension
five, and the corresponding schedules. Both the folding schemes that we present
have been verified by both simulation and hardware prototyping for different
applications. We later generalize this scheme to arbitrary projective spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0124</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0124</id><created>2011-07-01</created><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author></authors><title>A Gel'fand-type spectral radius formula and stability of linear
  constrained switching systems</title><categories>math.OC cs.SY math.DS math.RA</categories><comments>16 pages; to appear in Linear Algebra and its Applications</comments><msc-class>15B52, 93D20, 37N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using ergodic theory, in this paper we present a Gel'fand-type spectral
radius formula which states that the joint spectral radius is equal to the
generalized spectral radius for a matrix multiplicative semigroup $\bS^+$
restricted to a subset that need not carry the algebraic structure of $\bS^+$.
This generalizes the Berger-Wang formula. Using it as a tool, we study the
absolute exponential stability of a linear switched system driven by a compact
subshift of the one-sided Markov shift associated to $\bS$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0132</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0132</id><created>2011-07-01</created><updated>2013-08-30</updated><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Xiao</keyname><forenames>Mingqing</forenames></author></authors><title>Pointwise Stabilization of Discrete-time Stationary Matrix-valued
  Markovian Chains</title><categories>math.PR cs.SY math.DS math.OC</categories><comments>7 pages; submitted</comments><msc-class>61J10, 93C05, 37N35, 93D20, 93A30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the pointwise stabilizability of a discrete-time, time-homogeneous,
and stationary Markovian jump linear system. By using measure theory, ergodic
theory and a splitting theorem of state space we show in a relatively simple
way that if the system is essentially product-bounded, then it is pointwise
convergent if and only if it is pointwise exponentially convergent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0133</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0133</id><created>2011-07-01</created><updated>2011-10-26</updated><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>On the distance between non-isomorphic groups</title><categories>math.GR cs.DM</categories><comments>2 pages; corrected reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A result of Ben-Or, Coppersmith, Luby and Rubinfeld on testing whether a map
be two groups is close to a homomorphism implies a tight lower bound on the
distance between the multiplication tables of two non-isomorphic groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0134</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0134</id><created>2011-07-01</created><updated>2013-12-25</updated><authors><author><keyname>Kurbalija</keyname><forenames>Vladimir</forenames></author><author><keyname>Radovanovi&#x107;</keyname><forenames>Milo&#x161;</forenames></author><author><keyname>Geler</keyname><forenames>Zoltan</forenames></author><author><keyname>Ivanovi&#x107;</keyname><forenames>Mirjana</forenames></author></authors><title>The Influence of Global Constraints on Similarity Measures for
  Time-Series Databases</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A time series consists of a series of values or events obtained over repeated
measurements in time. Analysis of time series represents and important tool in
many application areas, such as stock market analysis, process and quality
control, observation of natural phenomena, medical treatments, etc. A vital
component in many types of time-series analysis is the choice of an appropriate
distance/similarity measure. Numerous measures have been proposed to date, with
the most successful ones based on dynamic programming. Being of quadratic time
complexity, however, global constraints are often employed to limit the search
space in the matrix during the dynamic programming procedure, in order to speed
up computation. Furthermore, it has been reported that such constrained
measures can also achieve better accuracy. In this paper, we investigate two
representative time-series distance/similarity measures based on dynamic
programming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS),
and the effects of global constraints on them. Through extensive experiments on
a large number of time-series data sets, we demonstrate how global constrains
can significantly reduce the computation time of DTW and LCS. We also show
that, if the constraint parameter is tight enough (less than 10-15% of
time-series length), the constrained measure becomes significantly different
from its unconstrained counterpart, in the sense of producing qualitatively
different 1-nearest neighbor graphs. This observation explains the potential
for accuracy gains when using constrained measures, highlighting the need for
careful tuning of constraint parameters in order to achieve a good trade-off
between speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0161</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0161</id><created>2011-07-01</created><updated>2013-06-29</updated><authors><author><keyname>Aronna</keyname><forenames>Maria Soledad</forenames><affiliation>CIFASIS CONICET, INRIA Saclay - Ile de France, CMAP, CIFASIS</affiliation></author><author><keyname>Bonnans</keyname><forenames>J. Frederic</forenames><affiliation>INRIA Saclay - Ile de France, CMAP</affiliation></author><author><keyname>Dmitruk</keyname><forenames>Andrei V.</forenames><affiliation>CEMI, Lomonosov Moscow State University</affiliation></author><author><keyname>Lotito</keyname><forenames>Pablo</forenames><affiliation>PLADEMA CONICET</affiliation></author></authors><title>Quadratic order conditions for bang-singular extremals</title><categories>math.OC cs.SY</categories><comments>41 pages</comments><proxy>ccsd</proxy><report-no>INRIA RR-7664 (2011)</report-no><msc-class>49K15</msc-class><journal-ref>AIMS Journal - Numerical Algebra, Control and Optimization, pp.
  511 - 546, Volume 2, Issue 3, September 2012, special issue in honor of
  Helmut Maurer</journal-ref><doi>10.3934/naco.2012.2.511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with optimal control problems for systems affine in the
control variable. We consider nonnegativity constraints on the control, and
finitely many equality and inequality constraints on the final state. First, we
obtain second order necessary optimality conditions. Secondly, we derive a
second order sufficient condition for the scalar control case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0169</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0169</id><created>2011-07-01</created><updated>2012-02-14</updated><authors><author><keyname>Sung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Ponce</keyname><forenames>Colin</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Unstructured Human Activity Detection from RGBD Images</title><categories>cs.RO cs.CV</categories><comments>2012 IEEE International Conference on Robotics and Automation (A
  preliminary version of this work was presented at AAAI workshop on Pattern,
  Activity and Intent Recognition, 2011)</comments><doi>10.1109/ICRA.2012.6224591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to detect and recognize human activities is essential for several
applications, including personal assistive robotics. In this paper, we perform
detection and recognition of unstructured human activity in unstructured
environments. We use a RGBD sensor (Microsoft Kinect) as the input sensor, and
compute a set of features based on human pose and motion, as well as based on
image and pointcloud information. Our algorithm is based on a hierarchical
maximum entropy Markov model (MEMM), which considers a person's activity as
composed of a set of sub-activities. We infer the two-layered graph structure
using a dynamic programming approach. We test our algorithm on detecting and
recognizing twelve different activities performed by four people in different
environments, such as a kitchen, a living room, an office, etc., and achieve
good performance even when the person was not seen before in the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0176</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0176</id><created>2011-07-01</created><authors><author><keyname>Chimani</keyname><forenames>Markus</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Klus&#xe1;&#x10d;ek</keyname><forenames>Mat&#x11b;j</forenames></author></authors><title>How Not to Characterize Planar-emulable Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the question of which graphs have planar emulators (a
locally-surjective homomorphism from some finite planar graph) -- a problem
raised already in Fellows' thesis (1985) and conceptually related to the better
known planar cover conjecture by Negami (1986). For over two decades, the
planar emulator problem lived poorly in a shadow of Negami's conjecture--which
is still open--as the two were considered equivalent. But, in the end of 2008,
a surprising construction by Rieck and Yamashita falsified the natural &quot;planar
emulator conjecture&quot;, and thus opened a whole new research field. We present
further results and constructions which show how far the planar-emulability
concept is from planar-coverability, and that the traditional idea of likening
it to projective embeddability is actually very out-of-place. We also present
several positive partial characterizations of planar-emulable graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0192</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0192</id><created>2011-07-01</created><authors><author><keyname>Cerf</keyname><forenames>Max</forenames></author></authors><title>Multiple Space Debris Collecting Mission - Debris selection and
  Trajectory optimization</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A possible mean to stabilize the LEO debris population is to remove each year
5 heavy debris like spent satellites or launchers stages from that space
region. This paper investigates the DeltaV requirement for such a Space Debris
Collecting mission. The optimization problem is intrinsically hard since it
mixes combinatorial optimization to select the debris among a list of
candidates and functional optimization to define the orbital maneuvers. The
solving methodology proceeds in two steps : firstly a generic transfer strategy
with impulsive maneuvers is defined so that the problem becomes of finite
dimension, secondly the problem is linearized around an initial reference
solution. A Branch and Bound algorithm is then applied to optimize
simultaneously the debris selection and the orbital maneuvers, yielding a new
reference solution. The process is iterated until the solution stabilizes on
the optimal path. The trajectory controls and dates are finally re-optimized in
order to refine the solution. The method is applicable whatever the numbers of
debris (candidate and to deorbit) and whatever the mission duration. It is
exemplified on an application case consisting in selecting 5 SSO debris among a
list of 11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0193</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0193</id><created>2011-07-01</created><updated>2013-10-01</updated><authors><author><keyname>Fortuny</keyname><forenames>Jordi</forenames></author><author><keyname>Corominas-Murtra</keyname><forenames>Bernat</forenames></author></authors><title>On the origin of ambiguity in efficient communication</title><categories>cs.CL</categories><comments>28 pages, 2 figures</comments><journal-ref>Journal of Logic, Language and Information. 0925-8531, pp 1--19
  (2013)</journal-ref><doi>10.1007/s10849-013-9179-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the emergence of ambiguity in communication through the
concept of logical irreversibility and within the framework of Shannon's
information theory. This leads us to a precise and general expression of the
intuition behind Zipf's vocabulary balance in terms of a symmetry equation
between the complexities of the coding and the decoding processes that imposes
an unavoidable amount of logical uncertainty in natural communication.
Accordingly, the emergence of irreversible computations is required if the
complexities of the coding and the decoding processes are balanced in a
symmetric scenario, which means that the emergence of ambiguous codes is a
necessary condition for natural communication to succeed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0194</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0194</id><created>2011-07-01</created><authors><author><keyname>Dundas</keyname><forenames>Jitesh</forenames></author></authors><title>Law of Connectivity in Machine Learning</title><categories>cs.AI</categories><comments>Keywords- Machine Learning; unknown entities; independence;
  interaction; coverage, silent connections; ISSN 1473-804x online, 1473-8031
  print</comments><journal-ref>I. J. of SIMULATION Vol. 11 No 5 1-10 Dec 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper our law that there is always a connection present
between two entities, with a selfconnection being present at least in each
node. An entity is an object, physical or imaginary, that is connected by a
path (or connection) and which is important for achieving the desired result of
the scenario. In machine learning, we state that for any scenario, a subject
entity is always, directly or indirectly, connected and affected by single or
multiple independent / dependent entities, and their impact on the subject
entity is dependent on various factors falling into the categories such as the
existenc
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0207</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0207</id><created>2011-07-01</created><updated>2012-09-21</updated><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Gravier</keyname><forenames>Sylvain</forenames><affiliation>IF</affiliation></author><author><keyname>Naserasr</keyname><forenames>Reza</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>IF</affiliation></author><author><keyname>Valicov</keyname><forenames>Petru</forenames><affiliation>LaBRI</affiliation></author></authors><title>Identifying codes in line graphs</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><journal-ref>Journal of Graph Theory 73, 4 (2013) 425-448</journal-ref><doi>10.1002/jgt.21686</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An identifying code of a graph is a subset of its vertices such that every
vertex of the graph is uniquely identified by the set of its neighbours within
the code. We study the edge-identifying code problem, i.e. the identifying code
problem in line graphs. If $\ID(G)$ denotes the size of a minimum identifying
code of an identifiable graph $G$, we show that the usual bound $\ID(G)\ge
\lceil\log_2(n+1)\rceil$, where $n$ denotes the order of $G$, can be improved
to $\Theta(\sqrt{n})$ in the class of line graphs. Moreover, this bound is
tight. We also prove that the upper bound $\ID(\mathcal{L}(G))\leq 2|V(G)|-5$,
where $\mathcal{L}(G)$ is the line graph of $G$, holds (with two exceptions).
This implies that a conjecture of R. Klasing, A. Kosowski, A. Raspaud and the
first author holds for a subclass of line graphs. Finally, we show that the
edge-identifying code problem is NP-complete, even for the class of planar
bipartite graphs of maximum degree~3 and arbitrarily large girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0234</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0234</id><created>2011-07-01</created><authors><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Jorge Ram&#xf3;n</forenames></author></authors><title>Unbounded Contention Resolution in Multiple-Access Channels</title><categories>cs.DC cs.DS cs.NI</categories><comments>21 pages, 1 figure. To appear in DISC 2011</comments><msc-class>68Q87</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A frequent problem in settings where a unique resource must be shared among
users is how to resolve the contention that arises when all of them must use
it, but the resource allows only for one user each time. The application of
efficient solutions for this problem spans a myriad of settings such as radio
communication networks or databases. For the case where the number of users is
unknown, recent work has yielded fruitful results for local area networks and
radio networks, although either a (possibly loose) upper bound on the number of
users needs to be known, or the solution is suboptimal, or it is only implicit
or embedded in other problems, with bounds proved only asymptotically. In this
paper, under the assumption that collision detection or information on the
number of contenders is not available, we present a novel protocol for
contention resolution in radio networks, and we recreate a protocol previously
used for other problems, tailoring the constants for our needs. In contrast
with previous work, both protocols are proved to be optimal up to a small
constant factor and with high probability for big enough number of contenders.
Additionally, the protocols are evaluated and contrasted with the previous work
by extensive simulations. The evaluation shows that the complexity bounds
obtained by the analysis are rather tight, and that both protocols proposed
have small and predictable complexity for many system sizes (unlike previous
proposals).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0237</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0237</id><created>2011-07-01</created><updated>2015-01-22</updated><authors><author><keyname>Brandenburger</keyname><forenames>Adam</forenames></author><author><keyname>La Mura</keyname><forenames>Pierfrancesco</forenames></author></authors><title>Team Decision Problems with Classical and Quantum Signals</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>18 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study team decision problems where communication is not possible, but
coordination among team members can be realized via signals in a shared
environment. We consider a variety of decision problems that differ in what
team members know about one another's actions and knowledge. For each type of
decision problem, we investigate how different assumptions on the available
signals affect team performance. Specifically, we consider the cases of
perfectly correlated, i.i.d., and exchangeable classical signals, as well as
the case of quantum signals. We find that, whereas in perfect-recall trees
(Kuhn [1950], [1953]) no type of signal improves performance, in
imperfect-recall trees quantum signals may bring an improvement. Isbell [1957]
proved that in non-Kuhn trees, classical i.i.d. signals may improve
performance. We show that further improvement may be possible by use of
classical exchangeable or quantum signals. We include an example of the effect
of quantum signals in the context of high-frequency trading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0268</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0268</id><created>2011-07-01</created><updated>2011-12-13</updated><authors><author><keyname>Nikolic</keyname><forenames>Mladen</forenames></author><author><keyname>Maric</keyname><forenames>Filip</forenames></author><author><keyname>Janicic</keyname><forenames>Predrag</forenames></author></authors><title>Simple Algorithm Portfolio for SAT</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of algorithm portfolio techniques for SAT has long been noted,
and a number of very successful systems have been devised, including the most
successful one --- SATzilla. However, all these systems are quite complex (to
understand, reimplement, or modify). In this paper we propose a new algorithm
portfolio for SAT that is extremely simple, but in the same time so efficient
that it outperforms SATzilla. For a new SAT instance to be solved, our
portfolio finds its k-nearest neighbors from the training set and invokes a
solver that performs the best at those instances. The main distinguishing
feature of our algorithm portfolio is the locality of the selection procedure
--- the selection of a SAT solver is based only on few instances similar to the
input one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0278</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0278</id><created>2011-07-01</created><authors><author><keyname>&#xc5;gotnes</keyname><forenames>Thomas</forenames></author><author><keyname>Alechina</keyname><forenames>Natasha</forenames></author></authors><title>Completeness of Epistemic Coalition Logic with Group Knowledge</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coalition logic is one of the most popular logics for multi-agent systems.
While epistemic extensions of coalition logic have received much attention,
existence of their complete axiomatisations has so far been an open problem. In
this paper we settle several of those problems. We prove completeness for
epistemic coalition logic with common knowledge, with distributed knowledge,
and with both common and distributed knowledge, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0300</identifier>
 <datestamp>2011-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0300</id><created>2011-07-01</created><authors><author><keyname>Osmane</keyname><forenames>Ali</forenames></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author></authors><title>The Compute-and-Forward Protocol: Implementation and Practical Aspects</title><categories>cs.IT math.IT</categories><comments>3 pages, 3 figures, submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent work, Nazer and Gastpar proposed the Compute-and-Forward strategy
as a physical-layer network coding scheme. They described a code structure
based on nested lattices whose algebraic structure makes the scheme reliable
and efficient. In this work, we consider the implementation of their scheme for
real Gaussian channels and one dimensional lattices. We relate the maximization
of the transmission rate to the lattice shortest vector problem. We explicit,
in this case, the maximum likelihood criterion and show that it can be
implemented by using an Inhomogeneous Diophantine Approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0326</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0326</id><created>2011-07-01</created><authors><author><keyname>Gnedin</keyname><forenames>Alexander</forenames></author></authors><title>The Monty Hall Problem in the Game Theory Class</title><categories>math.HO cs.GT</categories><comments>18 pages, 1 color figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic Monty Hall problem is explored to introduce into the fundamental
concepts of the game theory and to give a complete Bayesian and a
(noncooperative) game-theoretic analysis of the situation. Simple combinatorial
arguments are used to exclude the holding action and to find minimax solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0327</identifier>
 <datestamp>2013-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0327</id><created>2011-07-01</created><updated>2013-09-24</updated><authors><author><keyname>Solymosi</keyname><forenames>J&#xf3;zsef</forenames></author><author><keyname>Stojakovi&#x107;</keyname><forenames>Milo&#x161;</forenames></author></authors><title>Many collinear k-tuples with no k+1 collinear points</title><categories>math.CO cs.CG</categories><msc-class>52C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every $k&gt;3$, we give a construction of planar point sets with many
collinear $k$-tuples and no collinear $(k+1)$-tuples. We show that there are
$n_0=n_0(k)$ and $c=c(k)$ such that if $n\geq n_0$, then there exists a set of
$n$ points in the plane that does not contain $k+1$ points on a line, but it
contains at least $n^{2-\frac{c}{\sqrt{\log n}}}$ collinear $k$-tuples of
points. Thus, we significantly improve the previously best known lower bound
for the largest number of collinear $k$-tuples in such a set, and get
reasonably close to the trivial upper bound $O(n^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0336</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0336</id><created>2011-07-01</created><updated>2012-03-16</updated><authors><author><keyname>Randriambololona</keyname><forenames>Hugues</forenames></author></authors><title>Bilinear complexity of algebras and the Chudnovsky-Chudnovsky
  interpolation method</title><categories>cs.CC math.AG</categories><comments>40 pages; difference with previous version: modified Lemma 5.6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new improvements to the Chudnovsky-Chudnovsky method that provides
upper bounds on the bilinear complexity of multiplication in extensions of
finite fields through interpolation on algebraic curves. Our approach features
three independent key ingredients:
  (1) We allow asymmetry in the interpolation procedure. This allows to prove,
via the usual cardinality argument, the existence of auxiliary divisors needed
for the bounds, up to optimal degree.
  (2) We give an alternative proof for the existence of these auxiliary
divisors, which is constructive, and works also in the symmetric case, although
it requires the curves to have sufficiently many points.
  (3) We allow the method to deal not only with extensions of finite fields,
but more generally with monogenous algebras over finite fields. This leads to
sharper bounds, and is designed also to combine well with base field descent
arguments in case the curves do not have sufficiently many points.
  As a main application of these techniques, we fix errors in, improve, and
generalize, previous works of Shparlinski-Tsfasman-Vladut, Ballet, and
Cenk-Ozbudak. Besides, generalities on interpolation systems, as well as on
symmetric and asymmetric bilinear complexity, are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0349</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0349</id><created>2011-07-01</created><authors><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author></authors><title>First-order finite satisfiability vs tree automata in safety
  verification</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with verification of safety properties of
term-rewriting systems. The verification problem is translated to a purely
logical problem of finding a finite countermodel for a first-order formula,
which further resolved by a generic finite model finding procedure. A finite
countermodel produced during successful verification provides with a concise
description of the system invariant sufficient to demonstrate a specific safety
property.
  We show the relative completeness of this approach with respect to the tree
automata completion technique. On a set of examples taken from the literature
we demonstrate the efficiency of finite model finding approach as well as its
explanatory power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0350</identifier>
 <datestamp>2011-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0350</id><created>2011-07-01</created><updated>2011-07-26</updated><authors><author><keyname>Insa</keyname><forenames>David</forenames></author><author><keyname>Silva</keyname><forenames>Josep</forenames></author></authors><title>Optimal Divide and Query (extended version)</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic debugging is a semi-automatic debugging technique that allows the
programmer to precisely identify the location of bugs without the need to
inspect the source code. The technique has been successfully adapted to all
paradigms and mature implementations have been released for languages such as
Haskell, Prolog or Java. During three decades, the algorithm introduced by
Shapiro and later improved by Hirunkitti has been thought optimal. In this
paper we first show that this algorithm is not optimal, and moreover, in some
situations it is unable to find all possible solutions, thus it is incomplete.
Then, we present a new version of the algorithm that is proven optimal, and we
introduce some equations that allow the algorithm to identify all optimal
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0371</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0371</id><created>2011-07-02</created><updated>2012-02-25</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Rothvo&#xdf;</keyname><forenames>Thomas</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>Extended formulations for polygons</title><categories>cs.DM cs.CG math.CO</categories><comments>10 pages, 2 figures; Revised version accepted for publication in
  Discrete &amp; Computational Geometry</comments><journal-ref>Discrete &amp; Computational Geometry, Vol. 48, No. 3, 2012, pp
  658-668</journal-ref><doi>10.1007/s00454-012-9421-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extension complexity of a polytope $P$ is the smallest integer $k$ such
that $P$ is the projection of a polytope $Q$ with $k$ facets. We study the
extension complexity of $n$-gons in the plane. First, we give a new proof that
the extension complexity of regular $n$-gons is $O(\log n)$, a result
originating from work by Ben-Tal and Nemirovski (2001). Our proof easily
generalizes to other permutahedra and simplifies proofs of recent results by
Goemans (2009), and Kaibel and Pashkovich (2011). Second, we prove a lower
bound of $\sqrt{2n}$ on the extension complexity of generic $n$-gons. Finally,
we prove that there exist $n$-gons whose vertices lie on a $O(n) \times O(n^2)$
integer grid with extension complexity $\Omega(\sqrt{n}/\sqrt{\log n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0385</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0385</id><created>2011-07-02</created><authors><author><keyname>Pollack</keyname><forenames>Steven</forenames></author><author><keyname>Badali</keyname><forenames>Daniel</forenames></author><author><keyname>Pollack</keyname><forenames>Jonathan</forenames></author></authors><title>An algorithm for autonomously plotting solution sets in the presence of
  turning points</title><categories>math.NA cs.CG cs.DS cs.MS</categories><comments>9 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Plotting solution sets for particular equations may be complicated by the
existence of turning points. Here we describe an algorithm which not only
overcomes such problematic points, but does so in the most general of settings.
Applications of the algorithm are highlighted through two examples: the first
provides verification, while the second demonstrates a non-trivial application.
The latter is followed by a thorough run-time analysis. While both examples
deal with bivariate equations, it is discussed how the algorithm may be
generalized for space curves in $\R^{3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0390</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0390</id><created>2011-07-02</created><authors><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>On Linear Index Coding for Random Graphs</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sender wishes to broadcast an n character word x in F^n (for a field F) to
n receivers R_1,...,R_n. Every receiver has some side information on x
consisting of a subset of the characters of x. The side information of the
receivers is represented by a graph G on n vertices in which {i,j} is an edge
if R_i knows x_j. In the index coding problem the goal is to encode x using a
minimum number of characters in F in a way that enables every R_i to retrieve
the ith character x_i using the encoded message and the side information. An
index code is linear if the encoding is linear, and in this case the minimum
possible length is known to be equal to a graph parameter called minrank
(Bar-Yossef et al., FOCS'06). Several bounds on the minimum length of an index
code for side information graphs G were shown in the study of index coding.
However, the minimum length of an index code for the random graph G(n,p) is far
from being understood. In this paper we initiate the study of the typical
minimum length of a linear index code for G(n,p) over a field F. First, we
prove that for every constant size field F and a constant p, the minimum length
of a linear index code for G(n,p) over F is almost surely Omega(\sqrt{n}).
Second, we introduce and study the following two restricted models of index
coding: 1. A locally decodable index code is an index code in which the
receivers are allowed to query at most q characters from the encoded message.
2. A low density index code is a linear index code in which every character of
the word x affects at most q characters in the encoded message. Equivalently,
it is a linear code whose generator matrix has at most q nonzero entries in
each row.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0399</identifier>
 <datestamp>2012-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0399</id><created>2011-07-02</created><updated>2012-07-10</updated><authors><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Voronov</keyname><forenames>Vladimir</forenames></author></authors><title>Vision-Based Navigation I: A navigation filter for fusing
  DTM/correspondence updates</title><categories>cs.CV cs.AI</categories><comments>26 pages, 3 figures, in English and in Russian. arXiv admin note:
  substantial text overlap with arXiv:1106.6341, arXiv:1107.1470</comments><msc-class>68T45</msc-class><acm-class>E.5; E.4; E.2; H.1.1; F.1.1; F.1.3</acm-class><journal-ref>Proceedings of the IEEE International Conference on Robotics and
  Biomimetics (ROBIO), 2011 , Page(s): 1591 - 1596</journal-ref><doi>10.1109/ROBIO.2011.6181516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for pose and motion estimation using corresponding features in
images and a digital terrain map is proposed. Using a Digital Terrain (or
Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the
absolute position and orientation of the camera. In order to do this, the DTM
is used to formulate a constraint between corresponding features in two
consecutive frames. The utilization of data is shown to improve the robustness
and accuracy of the inertial navigation algorithm. Extended Kalman filter was
used to combine results of inertial navigation algorithm and proposed
vision-based navigation algorithm. The feasibility of this algorithms is
established through numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0414</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0414</id><created>2011-07-02</created><authors><author><keyname>Taylor</keyname><forenames>Kye M.</forenames></author><author><keyname>Meyer</keyname><forenames>Francois G.</forenames></author></authors><title>A random walk on image patches</title><categories>physics.data-an cs.DM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of understanding the success of
algorithms that organize patches according to graph-based metrics. Algorithms
that analyze patches extracted from images or time series have led to
state-of-the art techniques for classification, denoising, and the study of
nonlinear dynamics. The main contribution of this work is to provide a
theoretical explanation for the above experimental observations. Our approach
relies on a detailed analysis of the commute time metric on prototypical graph
models that epitomize the geometry observed in general patch graphs. We prove
that a parametrization of the graph based on commute times shrinks the mutual
distances between patches that correspond to rapid local changes in the signal,
while the distances between patches that correspond to slow local changes
expand. In effect, our results explain why the parametrization of the set of
patches based on the eigenfunctions of the Laplacian can concentrate patches
that correspond to rapid local changes, which would otherwise be shattered in
the space of patches. While our results are based on a large sample analysis,
numerical experimentations on synthetic and real data indicate that the results
hold for datasets that are very small in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0416</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0416</id><created>2011-07-02</created><authors><author><keyname>Ho</keyname><forenames>Z. K. M.</forenames></author><author><keyname>Gesbert</keyname><forenames>D.</forenames></author><author><keyname>Jorswieck</keyname><forenames>E.</forenames></author><author><keyname>Mochaourab</keyname><forenames>R.</forenames></author></authors><title>Beamforming on the MISO interference channel with multi-user decoding
  capability</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the multiple-input-single-output interference channel
(MISO-IC) with interference decoding capability (IDC), so that the interference
signal can be decoded and subtracted from the received signal. On the MISO-IC
with single user decoding, transmit beamforming vectors are classically
designed to reach a compromise between mitigating the generated interference
(zero forcing of the interference) or maximizing the energy at the desired
user. The particularly intriguing problem arising in the multi-antenna IC with
IDC is that transmitters may now have the incentive to amplify the interference
generated at the non-intended receivers, in the hope that Rxs have a better
chance of decoding the interference and removing it. This notion completely
changes the previous paradigm of balancing between maximizing the desired
energy and reducing the generated interference, thus opening up a new dimension
for the beamforming design strategy.
  Our contributions proceed by proving that the optimal rank of the transmit
precoders, optimal in the sense of Pareto optimality and therefore sum rate
optimality, is rank one. Then, we investigate suitable transmit beamforming
strategies for different decoding structures and characterize the Pareto
boundary. As an application of this characterization, we obtain a candidate set
of the maximum sum rate point} which at least contains the set of sum rate
optimal beamforming vectors. We derive the Maximum-Ratio-Transmission (MRT)
optimality conditions. Inspired by the MRT optimality conditions, we propose a
simple algorithm that achieves maximum sum rate in certain scenarios and
suboptimal, in other scenarios comparing to the maximum sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0420</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0420</id><created>2011-07-02</created><updated>2013-09-05</updated><authors><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Stable Restoration and Separation of Approximately Sparse Signals</title><categories>cs.IT math.IT</categories><comments>to appear in Applied and Computational Harmonic Analysis (ACHA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops new theory and algorithms to recover signals that are
approximately sparse in some general dictionary (i.e., a basis, frame, or
over-/incomplete matrix) but corrupted by a combination of interference having
a sparse representation in a second general dictionary and measurement noise.
The algorithms and analytical recovery conditions consider varying degrees of
signal and interference support-set knowledge. Particular applications covered
by the proposed framework include the restoration of signals impaired by
impulse noise, narrowband interference, or saturation/clipping, as well as
image in-painting, super-resolution, and signal separation. Two application
examples for audio and image restoration demonstrate the efficacy of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0429</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0429</id><created>2011-07-03</created><updated>2011-11-22</updated><authors><author><keyname>L&#xfc;</keyname><forenames>Linyuan</forenames></author><author><keyname>Chen</keyname><forenames>Duan-Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Small world yields the most effective information spreading</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 pages, 7 figures, accepted by New J. Phys</comments><journal-ref>New J. Phys. 13 (2011) 123005</journal-ref><doi>10.1088/1367-2630/13/12/123005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreading dynamics of information and diseases are usually analyzed by using
a unified framework and analogous models. In this paper, we propose a model to
emphasize the essential difference between information spreading and epidemic
spreading, where the memory effects, the social reinforcement and the
non-redundancy of contacts are taken into account. Under certain conditions,
the information spreads faster and broader in regular networks than in random
networks, which to some extent supports the recent experimental observation of
spreading in online society [D. Centola, Science {\bf 329}, 1194 (2010)]. At
the same time, simulation result indicates that the random networks tend to be
favorable for effective spreading when the network size increases. This
challenges the validity of the above-mentioned experiment for large-scale
systems. More significantly, we show that the spreading effectiveness can be
sharply enhanced by introducing a little randomness into the regular structure,
namely the small-world networks yield the most effective information spreading.
Our work provides insights to the understanding of the role of local clustering
in information spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0431</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0431</id><created>2011-07-03</created><authors><author><keyname>Kumabe</keyname><forenames>Masahiro</forenames></author><author><keyname>Mihara</keyname><forenames>H. Reiju</forenames></author></authors><title>Preference aggregation theory without acyclicity: The core without
  majority dissatisfaction</title><categories>cs.GT math.LO</categories><comments>27+3 pages</comments><msc-class>91A12, 91B14 (Primary) 91A13, 91B12 (Secondary)</msc-class><journal-ref>Games and Economic Behavior (2011) 72:187-201</journal-ref><doi>10.1016/j.geb.2010.06.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acyclicity of individual preferences is a minimal assumption in social choice
theory. We replace that assumption by the direct assumption that preferences
have maximal elements on a fixed agenda. We show that the core of a simple game
is nonempty for all profiles of such preferences if and only if the number of
alternatives in the agenda is less than the Nakamura number of the game. The
same is true if we replace the core by the core without majority
dissatisfaction, obtained by deleting from the agenda all the alternatives that
are non-maximal for all players in a winning coalition. Unlike the core, the
core without majority dissatisfaction depends only on the players' sets of
maximal elements and is included in the union of such sets. A result for an
extended framework gives another sense in which the core without majority
dissatisfaction behaves better than the core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0433</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0433</id><created>2011-07-03</created><authors><author><keyname>Mihara</keyname><forenames>H. Reiju</forenames></author></authors><title>The second-price auction solves King Solomon's dilemma</title><categories>cs.GT</categories><comments>12 pages;To appear in Japanese Economic Review</comments><msc-class>91A10, 91B14, 91B32</msc-class><doi>10.1111/j.1468-5876.2011.00543.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The planner wants to give k identical, indivisible objects to the top k
valuation agents at zero costs. Each agent knows her own valuation of the
object and whether it is among the top k. Modify the (k+1)st-price sealed-bid
auction by introducing a small participation fee and the option not to
participate in it. This simple mechanism implements the desired outcome in
iteratively undominated strategies. Moreover, no pair of agents can profitably
deviate from the equilibrium by coordinating their strategies or bribing each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0434</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0434</id><created>2011-07-03</created><authors><author><keyname>Silvescu</keyname><forenames>Adrian</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant</forenames></author></authors><title>Abstraction Super-structuring Normal Forms: Towards a Theory of
  Structural Induction</title><categories>cs.AI cs.FL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Induction is the process by which we obtain predictive laws or theories or
models of the world. We consider the structural aspect of induction. We answer
the question as to whether we can find a finite and minmalistic set of
operations on structural elements in terms of which any theory can be
expressed. We identify abstraction (grouping similar entities) and
super-structuring (combining topologically e.g., spatio-temporally close
entities) as the essential structural operations in the induction process. We
show that only two more structural operations, namely, reverse abstraction and
reverse super-structuring (the duals of abstraction and super-structuring
respectively) suffice in order to exploit the full power of Turing-equivalent
generative grammars in induction. We explore the implications of this theorem
with respect to the nature of hidden variables, radical positivism and the
2-century old claim of David Hume about the principles of connexion among
ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0439</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0439</id><created>2011-07-03</created><authors><author><keyname>Kumabe</keyname><forenames>Masahiro</forenames></author><author><keyname>Mihara</keyname><forenames>H. Reiju</forenames></author></authors><title>The Nakamura numbers for computable simple games</title><categories>cs.GT cs.LO</categories><comments>24+1 pages</comments><msc-class>91A12, 91B14 (Primary), 91A13, 91B12, 68Q05 (Secondary)</msc-class><acm-class>F.4.1</acm-class><journal-ref>Social Choice and Welfare (2008) 31:621-640</journal-ref><doi>10.1007/s00355-008-0300-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Nakamura number of a simple game plays a critical role in preference
aggregation (or multi-criterion ranking): the number of alternatives that the
players can always deal with rationally is less than this number. We
comprehensively study the restrictions that various properties for a simple
game impose on its Nakamura number. We find that a computable game has a finite
Nakamura number greater than three only if it is proper, nonstrong, and
nonweak, regardless of whether it is monotonic or whether it has a finite
carrier. The lack of strongness often results in alternatives that cannot be
strictly ranked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0478</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0478</id><created>2011-07-03</created><updated>2015-03-24</updated><authors><author><keyname>Presman</keyname><forenames>Noam</forenames></author><author><keyname>Shapira</keyname><forenames>Ofer</forenames></author><author><keyname>Litsyn</keyname><forenames>Simon</forenames></author></authors><title>Polar Codes with Mixed-Kernels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalization of the polar coding scheme called mixed-kernels is
introduced. This generalization exploits several homogeneous kernels over
alphabets of different sizes. An asymptotic analysis of the proposed scheme
shows that its polarization properties are strongly related to the ones of the
constituent kernels. Simulation of finite length instances of the scheme
indicate their advantages both in error correction performance and complexity
compared to the known polar coding structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0511</identifier>
 <datestamp>2011-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0511</id><created>2011-07-03</created><updated>2011-08-17</updated><authors><author><keyname>Tausz</keyname><forenames>Andrew</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author></authors><title>Homological Coordinatization</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review a method for computing and parameterizing the set of
homotopy classes of chain maps between two chain complexes. This is then
applied to finding topologically meaningful maps between simplicial complexes,
which in the context of topological data analysis, can be viewed as an
extension of conventional unsupervised learning methods to simplicial
complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0538</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0538</id><created>2011-07-04</created><authors><author><keyname>Rodrigues</keyname><forenames>Antonio Wendell De Oliveira</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Guyomarc'H</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Dekeyser</keyname><forenames>Jean-Luc</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Menach</keyname><forenames>Yvonnick Le</forenames><affiliation>L2EP</affiliation></author></authors><title>Automatic Multi-GPU Code Generation applied to Simulation of Electrical
  Machines</title><categories>cs.DC</categories><comments>Compumag 2011</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The electrical and electronic engineering has used parallel programming to
solve its large scale complex problems for performance reasons. However, as
parallel programming requires a non-trivial distribution of tasks and data,
developers find it hard to implement their applications effectively. Thus, in
order to reduce design complexity, we propose an approach to generate code for
hybrid architectures (e.g. CPU + GPU) using OpenCL, an open standard for
parallel programming of heterogeneous systems. This approach is based on Model
Driven Engineering (MDE) and the MARTE profile, standard proposed by Object
Management Group (OMG). The aim is to provide resources to non-specialists in
parallel programming to implement their applications. Moreover, thanks to model
reuse capacity, we can add/change functionalities or the target architecture.
Consequently, this approach helps industries to achieve their time-to-market
constraints and confirms by experimental tests, performance improvements using
multi-GPU environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0539</identifier>
 <datestamp>2011-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0539</id><created>2011-07-04</created><authors><author><keyname>Braha</keyname><forenames>Dan</forenames></author><author><keyname>Stacey</keyname><forenames>Blake</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Corporate competition: A self-organized network</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>In Press Social Networks, 2011</comments><journal-ref>Social Networks 33, 3 (July 2011) pp. 219--30</journal-ref><doi>10.1016/j.socnet.2011.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A substantial number of studies have extended the work on universal
properties in physical systems to complex networks in social, biological, and
technological systems. In this paper, we present a complex networks perspective
on interfirm organizational networks by mapping, analyzing and modeling the
spatial structure of a large interfirm competition network across a variety of
sectors and industries within the United States. We propose two micro-dynamic
models that are able to reproduce empirically observed characteristics of
competition networks as a natural outcome of a minimal set of general
mechanisms governing the formation of competition networks. Both models, which
utilize different approaches yet apply common principles to network formation
give comparable results. There is an asymmetry between companies that are
considered competitors, and companies that consider others as their
competitors. All companies only consider a small number of other companies as
competitors; however, there are a few companies that are considered as
competitors by many others. Geographically, the density of corporate
headquarters strongly correlates with local population density, and the
probability two firms are competitors declines with geographic distance. We
construct these properties by growing a corporate network with competitive
links using random incorporations modulated by population density and
geographic distance. Our new analysis, methodology and empirical results are
relevant to various phenomena of social and market behavior, and have
implications to research fields such as economic geography, economic sociology,
and regional economic development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0550</identifier>
 <datestamp>2012-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0550</id><created>2011-07-04</created><updated>2012-01-23</updated><authors><author><keyname>Brodu</keyname><forenames>Nicolas</forenames></author><author><keyname>Lague</keyname><forenames>Dimitri</forenames></author></authors><title>3D Terrestrial lidar data classification of complex natural scenes using
  a multi-scale dimensionality criterion: applications in geomorphology</title><categories>cs.CV physics.geo-ph</categories><comments>Free/Libre software implementation is available at
  http://nicolas.brodu.numerimoire.net, as well as data sets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D point clouds of natural environments relevant to problems in geomorphology
often require classification of the data into elementary relevant classes. A
typical example is the separation of riparian vegetation from ground in fluvial
environments, the distinction between fresh surfaces and rockfall in cliff
environments, or more generally the classification of surfaces according to
their morphology. Natural surfaces are heterogeneous and their distinctive
properties are seldom defined at a unique scale, prompting the use of
multi-scale criteria to achieve a high degree of classification success. We
have thus defined a multi-scale measure of the point cloud dimensionality
around each point, which characterizes the local 3D organization. We can thus
monitor how the local cloud geometry behaves across scales. We present the
technique and illustrate its efficiency in separating riparian vegetation from
ground and classifying a mountain stream as vegetation, rock, gravel or water
surface. In these two cases, separating the vegetation from ground or other
classes achieve accuracy larger than 98 %. Comparison with a single scale
approach shows the superiority of the multi-scale analysis in enhancing class
separability and spatial resolution. The technique is robust to missing data,
shadow zones and changes in point density within the scene. The classification
is fast and accurate and can account for some degree of intra-class
morphological variability such as different vegetation types. A probabilistic
confidence in the classification result is given at each point, allowing the
user to remove the points for which the classification is uncertain. The
process can be both fully automated, but also fully customized by the user
including a graphical definition of the classifiers. Although developed for
fully 3D data, the method can be readily applied to 2.5D airborne lidar data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0577</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0577</id><created>2011-07-04</created><authors><author><keyname>Barcel&#xf3;</keyname><forenames>Pablo</forenames></author><author><keyname>Libkin</keyname><forenames>Leonid</forenames></author><author><keyname>Reutter</keyname><forenames>Juan</forenames></author></authors><title>Parameterized Regular Expressions and their Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study regular expressions that use variables, or parameters, which are
interpreted as alphabet letters. We consider two classes of languages denoted
by such expressions: under the possibility semantics, a word belongs to the
language if it is denoted by some regular expression obtained by replacing
variables with letters; under the certainly semantics, the word must be denoted
by every such expression. Such languages are regular, and we show that they
naturally arise in several applications such as querying graph databases and
program analysis. As the main contribution of the paper, we provide a complete
characterization of the complexity of the main computational problems related
to such languages: nonemptiness, universality, containment, membership, as well
as the problem of constructing NFAs capturing such languages. We also look at
the extension when domains of variables could be arbitrary regular languages,
and show that under the certainty semantics, languages remain regular and the
complexity of the main computational problems does not change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0586</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0586</id><created>2011-07-04</created><updated>2015-01-01</updated><authors><author><keyname>Alvarez-Bermejo</keyname><forenames>Jose' Antonio</forenames></author><author><keyname>Lopez-Ramos</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>Managing key multicasting through orthogonal systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new protocol to manage multicast key distribution.
The protocol is based on the use of orthogonal systems in vector spaces. The
main advantage in comparison to other existing multicast key management
protocols is that the length and the number of the messages which have to be
sent are considerably smaller. This makes the protocol especially attractive
when the number of legitimate receivers is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0607</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0607</id><created>2011-07-04</created><authors><author><keyname>Sahai</keyname><forenames>Achaleshwar</forenames></author><author><keyname>Patel</keyname><forenames>Gaurav</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Pushing the limits of Full-duplex: Design and Real-time Implementation</title><categories>cs.NI</categories><comments>12 page Rice University technical report</comments><report-no>TREE1104</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown the feasibility of single-channel full-duplex wireless
physical layer, allowing nodes to send and receive in the same frequency band
at the same time. In this report, we first design and implement a real-time
64-subcarrier 10 MHz full-duplex OFDM physical layer, FD-PHY. The proposed
FD-PHY not only allows synchronous full-duplex transmissions but also selective
asynchronous full-duplex modes. Further, we show that in over-the-air
experiments using optimal antenna placement on actual devices, the
self-interference can be suppressed upto 80dB, which is 10dB more than prior
reported results. Then we propose a full-duplex MAC protocol, FD-MAC, which
builds on IEEE 802.11 with three new mechanisms -- shared random backoff,
header snooping and virtual backoffs. The new mechanisms allow FD-MAC to
discover and exploit full-duplex opportunities in a distributed manner. Our
over-the-air tests show over 70% throughput gains from using full-duplex over
half-duplex in realistically used cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0622</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0622</id><created>2011-07-04</created><updated>2013-01-24</updated><authors><author><keyname>Font</keyname><forenames>Roberto</forenames></author><author><keyname>Garcia</keyname><forenames>Javier</forenames></author><author><keyname>Murillo</keyname><forenames>Jose Alberto</forenames></author><author><keyname>Periago</keyname><forenames>Francisco</forenames></author></authors><title>Modelling and Control of Blowing-Venting Operations in Manned Submarines</title><categories>math.OC cs.SY</categories><comments>This paper has been withdrawn by the authors. It is currently being
  considered for publication</comments><journal-ref>Journal of Control Engineering and Technology, 4: 37-49, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the study of the potential use of blowing and venting operations
of ballast tanks in manned submarines as a complementary or alternative control
system for manoeuvring, we first propose a mathematical model for these
operations. Then we consider the coupling of blowing and venting with the
Feldman, variable mass, coefficient based hydrodynamic model for the equations
of motion. The final complete model is composed of a system of twenty-four
nonlinear ordinary differential equations. In a second part, we carry out a
rigorous mathematical analysis of the model: existence of a solution is proved.
As one of the possible applications of this model in naval engineering
problems, we consider the problem of roll control in an emergency rising
manoeuvre by using only blowing and venting. To this end, we formulate a
suitable constrained, nonlinear, optimal control problem where controls are
linked to the variable aperture of blowing and venting valves of each of the
tanks. Existence of a solution for this problem is also proved. Finally, we
address the numerical resolution of the control problem by using a descent
algorithm. Numerical experiments seem to indicate that, indeed, an appropriate
use of blowing and venting operations may help in the control of this emergency
manoeuvre.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0624</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0624</id><created>2011-07-04</created><authors><author><keyname>Cadney</keyname><forenames>Josh</forenames></author><author><keyname>Linden</keyname><forenames>Noah</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Infinitely many constrained inequalities for the von Neumann entropy</title><categories>quant-ph cs.IT math.IT</categories><comments>11 pages</comments><journal-ref>IEEE Trans. Inf. Theory 58, 3657 (2012)</journal-ref><doi>10.1109/TIT.2012.2185036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exhibit infinitely many new, constrained inequalities for the von Neumann
entropy, and show that they are independent of each other and the known
inequalities obeyed by the von Neumann entropy (basically strong
subadditivity). The new inequalities were proved originally by Makarychev et
al. [Commun. Inf. Syst., 2(2):147-166, 2002] for the Shannon entropy, using
properties of probability distributions. Our approach extends the proof of the
inequalities to the quantum domain, and includes their independence for the
quantum and also the classical cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0634</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0634</id><created>2011-07-04</created><authors><author><keyname>Gla&#xdf;er</keyname><forenames>Christian</forenames></author><author><keyname>Reitwie&#xdf;ner</keyname><forenames>Christian</forenames></author><author><keyname>Witek</keyname><forenames>Maximilian</forenames></author></authors><title>Applications of Discrepancy Theory in Multiobjective Approximation</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a multi-color extension of the Beck-Fiala theorem to show that the
multiobjective maximum traveling salesman problem is randomized
1/2-approximable on directed graphs and randomized 2/3-approximable on
undirected graphs. Using the same technique we show that the multiobjective
maximum satisfiablilty problem is 1/2-approximable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0639</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0639</id><created>2011-07-04</created><authors><author><keyname>Bergel</keyname><forenames>Itsik</forenames></author><author><keyname>Benedetto</keyname><forenames>Sergio</forenames></author></authors><title>Bounds on the capacity of OFDM underspread frequency selective fading
  channels</title><categories>cs.IT math.IT</categories><comments>55 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of the channel capacity in the absence of prior channel
knowledge (noncoherent channel) has gained increasing interest in recent years,
but it is still unknown for the general case. In this paper we derive bounds on
the capacity of the noncoherent, underspread complex Gaussian, orthogonal
frequency division multiplexing (OFDM), wide sense stationary channel with
uncorrelated scattering (WSSUS), under a peak power constraint or a constraint
on the second and fourth moments of the transmitted signal. These bounds are
characterized only by the system signal-to-noise ratio (SNR) and by a newly
defined quantity termed effective coherence time. Analysis of the effective
coherence time reveals that it can be interpreted as the length of a block in
the block fading model in which a system with the same SNR will achieve the
same capacity as in the analyzed channel. Unlike commonly used coherence time
definitions, it is shown that the effective coherence time depends on the SNR,
and is a nonincreasing function of it. We show that for low SNR the capacity is
proportional to the effective coherence time, while for higher SNR the coherent
channel capacity can be achieved provided that the effective coherence time is
large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0666</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0666</id><created>2011-07-04</created><authors><author><keyname>Bahr</keyname><forenames>Patrick</forenames></author></authors><title>Infinitary Term Graph Rewriting</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Term graph rewriting provides a formalism for implementing term rewriting in
an efficient manner by avoiding duplication. Infinitary term rewriting has been
introduced to study infinite term reduction sequences. Such infinite reductions
can be used to reason about lazy evaluation. In this paper, we combine term
graph rewriting and infinitary term rewriting thereby addressing both
components of lazy evaluation: non-strictness and sharing. Moreover, we show
how our theoretical underpinnings, based on a metric space and a complete
semilattice, provides a unified framework for both term rewriting and term
graph rewriting. This makes it possible to study the correspondences between
these two worlds. As an example, we show how the soundness of term graph
rewriting w.r.t. term rewriting can be extended to the infinitary setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0674</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0674</id><created>2011-07-04</created><updated>2015-10-07</updated><authors><author><keyname>Janson</keyname><forenames>Natalia B.</forenames></author><author><keyname>Marsden</keyname><forenames>Christopher J.</forenames></author></authors><title>&quot;Memory foam&quot; approach to unsupervised learning</title><categories>nlin.AO cs.LG</categories><comments>4 pages, 4 figures, and 2 wave files</comments><msc-class>34F05, 60G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an alternative approach to construct an artificial learning
system, which naturally learns in an unsupervised manner. Its mathematical
prototype is a dynamical system, which automatically shapes its vector field in
response to the input signal. The vector field converges to a gradient of a
multi-dimensional probability density distribution of the input process, taken
with negative sign. The most probable patterns are represented by the stable
fixed points, whose basins of attraction are formed automatically. The
performance of this system is illustrated with musical signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0681</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0681</id><created>2011-07-04</created><authors><author><keyname>Shuai</keyname><forenames>Xin</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Busemeyer</keyname><forenames>Jerome</forenames></author><author><keyname>Sun</keyname><forenames>Yuyin</forenames></author><author><keyname>Chen</keyname><forenames>Shanshan</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author></authors><title>Does Quantum Interference exist in Twitter?</title><categories>cs.SI cs.IT math.IT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It becomes more difficult to explain the social information transfer
phenomena using the classic models based merely on Shannon Information Theory
(SIT) and Classic Probability Theory (CPT), because the transfer process in the
social world is rich of semantic and highly contextualized. This paper aims to
use twitter data to explore whether the traditional models can interpret
information transfer in social networks, and whether quantum-like phenomena can
be spotted in social networks. Our main contributions are: (1) SIT and CPT fail
to interpret the information transfer occurring in Twitter; and (2) Quantum
interference exists in Twitter, and (3) a mathematical model is proposed to
elucidate the spotted quantum phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0687</identifier>
 <datestamp>2011-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0687</id><created>2011-07-04</created><authors><author><keyname>Blankertz</keyname><forenames>Raoul</forenames></author></authors><title>Decomposition of Polynomials</title><categories>math.AC cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This diploma thesis is concerned with functional decomposition $f = g \circ
h$ of polynomials. First an algorithm is described which computes
decompositions in polynomial time. This algorithm was originally proposed by
Zippel (1991). A bound for the number of minimal collisions is derived. Finally
a proof of a conjecture in von zur Gathen, Giesbrecht &amp; Ziegler (2010) is
given, which states a classification for a special class of decomposable
polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0690</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0690</id><created>2011-07-04</created><authors><author><keyname>Catanese</keyname><forenames>Salvatore</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Pagano</keyname><forenames>Francesco</forenames></author></authors><title>A Framework for Designing 3D Virtual Environments</title><categories>cs.GR cs.MM</categories><comments>12 pages, 1 figure, Proceedings of the 4th International ICST
  Conference On Intelligent Technologies For Interactive Entertainment, 2011</comments><journal-ref>Lecture Notes of the Institute for Computer Sciences, Social
  Informatics and Telecommunications Engineering Volume 78, 2012, pp 209-218</journal-ref><doi>10.1007/978-3-642-30214-5_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of design and development of virtual environments can be
supported by tools and frameworks, to save time in technical aspects and
focusing on the content. In this paper we present an academic framework which
provides several levels of abstraction to ease this work. It includes
state-of-the-art components we devised or integrated adopting open-source
solutions in order to face specific problems. Its architecture is modular and
customizable, the code is open-source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0746</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0746</id><created>2011-07-04</created><authors><author><keyname>Massink</keyname><forenames>Mieke</forenames><affiliation>CNR-ISTI, Pisa, Italy</affiliation></author><author><keyname>Norman</keyname><forenames>Gethin</forenames><affiliation>University of Glasgow, UK</affiliation></author></authors><title>Proceedings Ninth Workshop on Quantitative Aspects of Programming
  Languages</title><categories>cs.PL cs.LO cs.PF</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011</journal-ref><doi>10.4204/EPTCS.57</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Ninth Workshop on Quantitative
Aspects of Programming Languages (QAPL 2011), held in Saarbrucken, Germany,
April 1--3, 2011. QAPL 2011 is a satellite event of the European Joint
Conferences on Theory and Practice of Software (ETAPS 2011).
  The workshop theme is on quantitative aspects of computation. These aspects
are related to the use of physical quantities (storage space, time, bandwidth,
etc.) as well as mathematical quantities (e.g. probability and measures for
reliability, security and trust), and play an important (sometimes essential)
role in characterising the behavior and determining the properties of systems.
Such quantities are central to the definition of both the model of systems
(architecture, language design, semantics) and the methodologies and tools for
the analysis and verification of the systems properties. The aim of this
workshop is to discuss the explicit use of quantitative information such as
time and probabilities either directly in the model or as a tool for the
analysis of systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0789</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0789</id><created>2011-07-05</created><updated>2013-10-28</updated><authors><author><keyname>Mackey</keyname><forenames>Lester</forenames></author><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Distributed Matrix Completion and Robust Factorization</title><categories>cs.LG cs.DS cs.NA math.NA stat.ML</categories><comments>35 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If learning methods are to scale to the massive sizes of modern datasets, it
is essential for the field of machine learning to embrace parallel and
distributed computing. Inspired by the recent development of matrix
factorization methods with rich theory but poor computational complexity and by
the relative ease of mapping matrices onto distributed architectures, we
introduce a scalable divide-and-conquer framework for noisy matrix
factorization. We present a thorough theoretical analysis of this framework in
which we characterize the statistical errors introduced by the &quot;divide&quot; step
and control their magnitude in the &quot;conquer&quot; step, so that the overall
algorithm enjoys high-probability estimation guarantees comparable to those of
its base algorithm. We also present experiments in collaborative filtering and
video background modeling that demonstrate the near-linear to superlinear
speed-ups attainable with this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0798</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0798</id><created>2011-07-05</created><updated>2011-11-03</updated><authors><author><keyname>Hlineny</keyname><forenames>Petr</forenames></author><author><keyname>Moris</keyname><forenames>Ondrej</forenames></author></authors><title>Generalized Maneuvers in Route Planning</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an important practical aspect of the route planning problem in
real-world road networks -- maneuvers. Informally, maneuvers represent various
irregularities of the road network graph such as turn-prohibitions, traffic
light delays, round-abouts, forbidden passages and so on. We propose a
generalized model which can handle arbitrarily complex (and even negative)
maneuvers, and outline how to enhance Dijkstra's algorithm in order to solve
route planning queries in this model without prior adjustments of the
underlying road network graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0803</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0803</id><created>2011-07-05</created><authors><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Raveh</keyname><forenames>Barak</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Motion Planning via Manifold Samples</title><categories>cs.CG cs.RO</categories><comments>18 pages</comments><doi>10.1007/s00453-012-9736-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general and modular algorithmic framework for path planning of
robots. Our framework combines geometric methods for exact and complete
analysis of low-dimensional configuration spaces, together with practical,
considerably simpler sampling-based approaches that are appropriate for higher
dimensions. In order to facilitate the transfer of advanced geometric
algorithms into practical use, we suggest taking samples that are entire
low-dimensional manifolds of the configuration space that capture the
connectivity of the configuration space much better than isolated point
samples. Geometric algorithms for analysis of low-dimensional manifolds then
provide powerful primitive operations. The modular design of the framework
enables independent optimization of each modular component. Indeed, we have
developed, implemented and optimized a primitive operation for complete and
exact combinatorial analysis of a certain set of manifolds, using arrangements
of curves of rational functions and concepts of generic programming. This in
turn enabled us to implement our framework for the concrete case of a polygonal
robot translating and rotating amidst polygonal obstacles. We demonstrate that
the integration of several carefully engineered components leads to significant
speedup over the popular PRM sampling-based algorithm, which represents the
more simplistic approach that is prevalent in practice. We foresee possible
extensions of our framework to solving high-dimensional problems beyond motion
planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0845</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0845</id><created>2011-07-05</created><updated>2013-11-26</updated><authors><author><keyname>Suprijadi</keyname></author><author><keyname>Muliawan</keyname><forenames>Thomas</forenames></author><author><keyname>Viridi</keyname><forenames>Sparisoma</forenames></author></authors><title>Automatic Road Lighting System (ARLS) Model Based on Image Processing of
  Moving Object</title><categories>cs.CV</categories><comments>5 pages, 8 figures, 1 table, submitted to ARPN Journal of Science and
  Technology</comments><journal-ref>ARPN Journal of Science and Technology 3 (12), 1105-1109 (2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Using a vehicle toy (in next future called vehicle) as a moving object an
automatic road lighting system (ARLS) model is constructed. A digital video
camera with 25 fps is used to capture the vehicle motion as it moves in the
test segment of the road. Captured images are then processed to calculate
vehicle speed. This information of the speed together with position of vehicle
is then used to control the lighting system along the path that passes by the
vehicle. Length of the road test segment is 1 m, the video camera is positioned
about 1.1 m above the test segment, and the vehicle toy dimension is 13 cm
\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about
1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0848</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0848</id><created>2011-07-05</created><authors><author><keyname>Gnedin</keyname><forenames>Alexander</forenames></author></authors><title>The Unlucky Door</title><categories>math.HO cs.GT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the famous Three-Door-Game Monte cannot help to win all the time by
signaling location of the prize, using only the freedom he allowed to use. No
matter which strategies played, there is always at least one door where the
prize will not be found. However, already in the game with four doors
cooperative Monte can reveal two useless doors in sequence (leaving two doors
unrevealed), to inform Conie about location of the prize, so enabling her to
beat the only-switching strategies and win all the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0871</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0871</id><created>2011-07-05</created><authors><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author></authors><title>A simple algorithm for random colouring G(n, d/n) using (2+\epsilon)d
  colours</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate random k-colouring of a graph G=(V,E) is a very well studied
problem in computer science and statistical physics. It amounts to constructing
a k-colouring of G which is distributed close to Gibbs distribution, i.e. the
uniform distribution over all the k-colourings of G. Here, we deal with the
problem when the underlying graph is an instance of Erdos-Renyi random graph
G(n,p), where p=d/n and d is fixed.
  We propose a novel efficient algorithm for approximate random k-colouring
with the following properties: given an instance of G(n,d/n) and for any
k&gt;(2+\epsilon)d, it returns a k-colouring distributed within total variation
distance n^{-Omega(1)} from the Gibbs distribution, with probability
1-n^{-Omega(1)}.
  What we propose is neither a MCMC algorithm nor some algorithm inspired by
the message passing heuristics that were introduced by statistical physicist.
Our algorithm is of combinatorial nature. It is based on a rather simple
recursion which reduces the random k-colouring of G(n,d/n) to random
k-colouring simpler subgraphs first.
  The lower bound on the number of colours for our algorithm to run in
polynomial time is dramatically smaller than the corresponding bounds we have
for any previous algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0878</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0878</id><created>2011-07-05</created><authors><author><keyname>Bladon</keyname><forenames>Alex J.</forenames></author><author><keyname>Galla</keyname><forenames>Tobias</forenames></author></authors><title>Learning to play public good games</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>14 pages, 12 figures</comments><journal-ref>Phys. Rev. E 84, 041132 (2011)</journal-ref><doi>10.1103/PhysRevE.84.041132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend recent analyses of stochastic effects in game dynamical learning to
cases of multi-player games, and to games defined on networked structures. By
means of an expansion in the noise strength we consider the weak-noise limit,
and present an analytical computation of spectral properties of fluctuations in
multi-player public good games. This extends existing work on two-player games.
In particular we show that coherent cycles may emerge driven by noise in the
adaptation dynamics. These phenomena are not too dissimilar from cyclic
strategy switching observed in experiments of behavioural game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0901</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0901</id><created>2011-07-05</created><updated>2012-04-27</updated><authors><author><keyname>Das</keyname><forenames>Aparna</forenames></author><author><keyname>Gansner</keyname><forenames>Emden R.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Approximating Minimum Manhattan Networks in Higher Dimensions</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimum Manhattan network problem, which is defined as follows.
Given a set of points called \emph{terminals} in $\R^d$, find a minimum-length
network such that each pair of terminals is connected by a set of axis-parallel
line segments whose total length is equal to the pair's Manhattan (that is,
$L_1$-) distance. The problem is NP-hard in 2D and there is no PTAS for 3D
(unless ${\cal P}={\cal NP}$). Approximation algorithms are known for 2D, but
not for 3D.
  We present, for any fixed dimension $d$ and any $\eps&gt;0$, an
$O(n^\eps)$-approximation algorithm. For 3D, we also give a
$4(k-1)$-approximation algorithm for the case that the terminals are contained
in the union of $k \ge 2$ parallel planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0919</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0919</id><created>2011-07-05</created><updated>2014-02-10</updated><authors><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames><affiliation>University of Bremen</affiliation></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames><affiliation>University of Leipzig</affiliation></author></authors><title>The First-Order Theory of Ground Tree Rewrite Graphs</title><categories>cs.LO cs.CC</categories><comments>accepted for Logical Methods in Computer Science</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  12, 2014) lmcs:1223</journal-ref><doi>10.2168/LMCS-10(1:7)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the complexity of the uniform first-order theory of ground tree
rewrite graphs is in ATIME(2^{2^{poly(n)}},O(n)). Providing a matching lower
bound, we show that there is some fixed ground tree rewrite graph whose
first-order theory is hard for ATIME(2^{2^{poly(n)}},poly(n)) with respect to
logspace reductions. Finally, we prove that there exists a fixed ground tree
rewrite graph together with a single unary predicate in form of a regular tree
language such that the resulting structure has a non-elementary first-order
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0922</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0922</id><created>2011-07-05</created><authors><author><keyname>Low</keyname><forenames>Yucheng</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph</forenames></author><author><keyname>Kyrola</keyname><forenames>Aapo</forenames></author><author><keyname>Bickson</keyname><forenames>Danny</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>GraphLab: A Distributed Framework for Machine Learning in the Cloud</title><categories>cs.LG</categories><comments>CMU Tech Report, GraphLab project webpage: http://graphlab.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning (ML) techniques are indispensable in a wide range of fields.
Unfortunately, the exponential increase of dataset sizes are rapidly extending
the runtime of sequential algorithms and threatening to slow future progress in
ML. With the promise of affordable large-scale parallel computing, Cloud
systems offer a viable platform to resolve the computational challenges in ML.
However, designing and implementing efficient, provably correct distributed ML
algorithms is often prohibitively challenging. To enable ML researchers to
easily and efficiently use parallel systems, we introduced the GraphLab
abstraction which is designed to represent the computational patterns in ML
algorithms while permitting efficient parallel and distributed implementations.
In this paper we provide a formal description of the GraphLab parallel
abstraction and present an efficient distributed implementation. We conduct a
comprehensive evaluation of GraphLab on three state-of-the-art ML algorithms
using real large-scale data and a 64 node EC2 cluster of 512 processors. We
find that GraphLab achieves orders of magnitude performance gains over Hadoop
while performing comparably or superior to hand-tuned MPI implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0927</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0927</id><created>2011-07-05</created><authors><author><keyname>Terejanu</keyname><forenames>Gabriel</forenames></author><author><keyname>Oliver</keyname><forenames>Todd</forenames></author><author><keyname>Simmons</keyname><forenames>Chris</forenames></author></authors><title>Application of Predictive Model Selection to Coupled Models</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><comments>Submitted to International Conference on Modeling, Simulation and
  Control 2011 (ICMSC'11), San Francisco, USA, 19-21 October, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A predictive Bayesian model selection approach is presented to discriminate
coupled models used to predict an unobserved quantity of interest (QoI). The
need for accurate predictions arises in a variety of critical applications such
as climate, aerospace and defense. A model problem is introduced to study the
prediction yielded by the coupling of two physics/sub-components. For each
single physics domain, a set of model classes and a set of sensor observations
are available. A goal-oriented algorithm using a predictive approach to
Bayesian model selection is then used to select the combination of single
physics models that best predict the QoI. It is shown that the best coupled
model for prediction is the one that provides the most robust predictive
distribution for the QoI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0940</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0940</id><created>2011-07-05</created><updated>2013-10-21</updated><authors><author><keyname>Paquet</keyname><forenames>Joey</forenames></author><author><keyname>Mokhov</keyname><forenames>Serguei A.</forenames></author></authors><title>Furthering Baseline Core Lucid Standard Specification in the Context of
  the History of Lucid, Intensional Programming, and Context-Aware Computing</title><categories>cs.PL</categories><comments>46 pages, 3 figures, 1 table, 1 listing; a running draft and a
  collection of references on the subject; v4 primarily updates some references</comments><msc-class>68N15, 68N18, 68N19, 68N20, 68T30, 68T27, 68Q55</msc-class><acm-class>D.3.1; D.3.2; D.3.3; D.3.4; D.1.7; D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is multifold. We review the historical literature on the Lucid
programming language, its dialects, intensional logic, intensional programming,
the implementing systems, and context-oriented and context-aware computing and
so on that provide a contextual framework for the converging Core Lucid
standard programming model. We are designing a standard specification of a
baseline Lucid virtual machine for generic execution of Lucid programs. The
resulting Core Lucid language would inherit the properties of generalization
attempts of GIPL (1999-2013) and TransLucid (2008-2013) for all future and
recent Lucid implementing systems to follow. We also maintain this work across
local research group in order to foster deeper collaboration, maintain a list
of recent and historical bibliography and a reference manual and reading list
for students. We form a (for now informal) SIGLUCID group to keep track of this
standard and historical records with eventual long-term goal through iterative
revisions for this work to become a book or an encyclopedia of the referenced
topics, and perhaps, an RFC. We first begin small with this initial set of
notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0989</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0989</id><created>2011-07-05</created><updated>2012-11-25</updated><authors><author><keyname>Ranjan</keyname><forenames>Gyan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author></authors><title>Geometry of Complex Networks and Topological Centrality</title><categories>cs.DM cs.SI physics.soc-ph</categories><comments>17 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the geometry of complex networks in terms of an n-dimensional
Euclidean embedding represented by the Moore-Penrose pseudo-inverse of the
graph Laplacian $(\bb L^+)$. The squared distance of a node $i$ to the origin
in this n-dimensional space $(l^+_{ii})$, yields a topological centrality index
$(\mathcal{C}^{*}(i) = 1/l^+_{ii})$ for node $i$. In turn, the sum of
reciprocals of individual node structural centralities,
$\sum_{i}1/\mathcal{C}^*(i) = \sum_{i} l^+_{ii}$, i.e. the trace of $\bb L^+$,
yields the well-known Kirchhoff index $(\mathcal{K})$, an overall structural
descriptor for the network. In addition to this geometric interpretation, we
provide alternative interpretations of the proposed indices to reveal their
true topological characteristics: first, in terms of forced detour overheads
and frequency of recurrences in random walks that has an interesting analogy to
voltage distributions in the equivalent electrical network; and then as the
average connectedness of $i$ in all the bi-partitions of the graph. These
interpretations respectively help establish the topological centrality
$(\mathcal{C}^{*}(i))$ of node $i$ as a measure of its overall position as well
as its overall connectedness in the network; thus reflecting the robustness of
node $i$ to random multiple edge failures. Through empirical evaluations using
synthetic and real world networks, we demonstrate how the topological
centrality is better able to distinguish nodes in terms of their structural
roles in the network and, along with Kirchhoff index, is appropriately
sensitive to perturbations/rewirings in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.0998</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.0998</id><created>2011-07-05</created><authors><author><keyname>Epstein</keyname><forenames>Samuel</forenames></author><author><keyname>Betke</keyname><forenames>Margrit</forenames></author></authors><title>An Information Theoretic Representation of Agent Dynamics as Set
  Intersections</title><categories>cs.IT cs.AI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We represent agents as sets of strings. Each string encodes a potential
interaction with another agent or environment. We represent the total set of
dynamics between two agents as the intersection of their respective strings, we
prove complexity properties of player interactions using Algorithmic
Information Theory. We show how the proposed construction is compatible with
Universal Artificial Intelligence, in that the AIXI model can be seen as
universal with respect to interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1011</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1011</id><created>2011-07-05</created><updated>2012-02-17</updated><authors><author><keyname>Qiu</keyname><forenames>Hong</forenames></author><author><keyname>Yong</keyname><forenames>Jiongmin</forenames></author></authors><title>Hamilton-Jacobi Equations and Two-Person Zero-Sum Differential Games
  with Unbounded Controls</title><categories>math.OC cs.SY</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-person zero-sum differential game with unbounded controls is
considered. Under proper coercivity conditions, the upper and lower value
functions are characterized as the unique viscosity solutions to the
corresponding upper and lower Hamilton--Jacobi--Isaacs equations, respectively.
Consequently, when the Isaacs' condition is satisfied, the upper and lower
value functions coincide, leading to the existence of the value function. Due
to the unboundedness of the controls, the corresponding upper and lower
Hamiltonians grow super linearly in the gradient of the upper and lower value
functions, respectively. A uniqueness theorem of viscosity solution to
Hamilton--Jacobi equations involving such kind of Hamiltonian is proved,
without relying on the convexity/concavity of the Hamiltonian. Also, it is
shown that the assumed coercivity conditions guaranteeing the finiteness of the
upper and lower value functions are sharp in some sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1012</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1012</id><created>2011-07-05</created><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Tan</keyname><forenames>Xuehou</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author><author><keyname>Wu</keyname><forenames>Gangshan</forenames></author></authors><title>Optimal Point Movement for Covering Circular Regions</title><categories>cs.CG cs.DS</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ points in a circular region $C$ in the plane, we study the problems
of moving the $n$ points to its boundary to form a regular $n$-gon such that
the maximum (min-max) or the sum (min-sum) of the Euclidean distances traveled
by the points is minimized. The problems have applications, e.g., in mobile
sensor barrier coverage of wireless sensor networks. The min-max problem
further has two versions: the decision version and optimization version. For
the min-max problem, we present an $O(n\log^2 n)$ time algorithm for the
decision version and an $O(n\log^3 n)$ time algorithm for the optimization
version. The previously best algorithms for the two problem versions take
$O(n^{3.5})$ time and $O(n^{3.5}\log n)$ time, respectively. For the min-sum
problem, we show that a special case with all points initially lying on the
boundary of the circular region can be solved in $O(n^2)$ time, improving a
previous $O(n^4)$ time solution. For the general min-sum problem, we present a
3-approximation $O(n^2)$ time algorithm, improving the previous
$(1+\pi)$-approximation $O(n^2)$ time algorithm. A by-product of our techniques
is an algorithm for dynamically maintaining the maximum matching of a circular
convex bipartite graph; our algorithm can handle each vertex insertion or
deletion on the graph in $O(\log^2 n)$ time. This result is interesting in its
own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1017</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1017</id><created>2011-07-05</created><authors><author><keyname>Aizatulin</keyname><forenames>Mihhail</forenames></author><author><keyname>Gordon</keyname><forenames>Andrew D.</forenames></author><author><keyname>J&#xfc;rjens</keyname><forenames>Jan</forenames></author></authors><title>Extracting and Verifying Cryptographic Models from C Protocol Code by
  Symbolic Execution</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of verifying security properties of a cryptographic
protocol coded in C. We propose an automatic solution that needs neither a
pre-existing protocol description nor manual annotation of source code. First,
symbolically execute the C program to obtain symbolic descriptions for the
network messages sent by the protocol. Second, apply algebraic rewriting to
obtain a process calculus description. Third, run an existing protocol analyser
(ProVerif) to prove security properties or find attacks. We formalise our
algorithm and appeal to existing results for ProVerif to establish
computational soundness under suitable circumstances. We analyse only a single
execution path, so our results are limited to protocols with no significant
branching. The results in this paper provide the first computationally sound
verification of weak secrecy and authentication for (single execution paths of)
C code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1020</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1020</id><created>2011-07-05</created><authors><author><keyname>Chai</keyname><forenames>Junyi</forenames></author><author><keyname>Liu</keyname><forenames>James N. K.</forenames></author></authors><title>A Novel Multicriteria Group Decision Making Approach With Intuitionistic
  Fuzzy SIR Method</title><categories>cs.AI</categories><comments>Paper presented at the 2010 World Automation Congress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The superiority and inferiority ranking (SIR) method is a generation of the
well-known PROMETHEE method, which can be more efficient to deal with
multi-criterion decision making (MCDM) problem. Intuitionistic fuzzy sets
(IFSs), as an important extension of fuzzy sets (IFs), include both membership
functions and non-membership functions and can be used to, more precisely
describe uncertain information. In real world, decision situations are usually
under uncertain environment and involve multiple individuals who have their own
points of view on handing of decision problems. In order to solve uncertainty
group MCDM problem, we propose a novel intuitionistic fuzzy SIR method in this
paper. This approach uses intuitionistic fuzzy aggregation operators and SIR
ranking methods to handle uncertain information; integrate individual opinions
into group opinions; make decisions on multiple-criterion; and finally
structure a specific decision map. The proposed approach is illustrated in a
simulation of group decision making problem related to supply chain management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1031</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1031</id><created>2011-07-06</created><authors><author><keyname>Hassan</keyname><forenames>Sk. S.</forenames></author><author><keyname>Roy</keyname><forenames>A.</forenames></author><author><keyname>Choudhury</keyname><forenames>P. Pal.</forenames></author><author><keyname>Nayak</keyname><forenames>B. K.</forenames></author></authors><title>Integral Value Transformations: A Class of Discrete Dynamical Systems</title><categories>cs.DM math.DS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Here the Integral Value Transformations (IVTs) are considered to be Discrete
Dynamical System map in the space\mathbb{N}_(0). In this paper, the dynamics of
IVTs is deciphered through the light of Topological Dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1038</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1038</id><created>2011-07-06</created><authors><author><keyname>Ukil</keyname><forenames>Arijit</forenames></author><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author><author><keyname>Bera</keyname><forenames>Debasish</forenames></author></authors><title>Dynamic OFDMA Resource Allocation for QoS Guarantee and System
  Optimization of Best Effort and Non Real-time Traffic</title><categories>cs.NI</categories><comments>5 pages, 4 figures; National Conference on Communications, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve the requirement of high data rate, low latency, user fairness for
next generation wireless networks, proper designing of cross-layer optimized
dynamic resource allocation algorithm is prerequisite. In this paper, we
propose a dynamic resource allocation scheme in Orthogonal Frequency Division
Multiple Access (OFDMA) systems to optimize the non real-time (NRT) traffic,
which requires allocation of minimum quantum of data within a predefined time
that does not incur packet loss. Most existing and proposed works on resource
allocation schemes focused on traffic consisting of delay constraint real-time
(RT) or delay-tolerant (NRT, Best-Effort (BE)) applications in a single scheme.
In this work, we investigate the resource allocation problem in heterogeneous
multiuser OFDMA system with the objective of optimizing the aggregate data
delivery of NRT and BE traffic to maximize the overall system performance, by
exploiting the inherent time-diversity gain in mobile wireless environment for
delay-tolerant applications. Simulation results show that the proposed
algorithm greatly enhances the system capacity, when compared to traditional
proportional fair resource allocation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1052</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1052</id><created>2011-07-06</created><authors><author><keyname>Boyd</keyname><forenames>Sylvia</forenames></author><author><keyname>Sitters</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>van der Ster</keyname><forenames>Suzanne</forenames></author><author><keyname>Stougie</keyname><forenames>Leen</forenames></author></authors><title>The traveling salesman problem on cubic and subcubic graphs</title><categories>cs.DS cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Travelling Salesman Problem (TSP) on the metric completion of
cubic and subcubic graphs, which is known to be NP-hard. The problem is of
interest because of its relation to the famous 4/3 conjecture for metric TSP,
which says that the integrality gap, i.e., the worst case ratio between the
optimal values of the TSP and its linear programming relaxation (the subtour
elimination relaxation), is 4/3. We present the first algorithm for cubic
graphs with approximation ratio 4/3. The proof uses polyhedral techniques in a
surprising way, which is of independent interest. In fact we prove
constructively that for any cubic graph on $n$ vertices a tour of length 4n/3-2
exists, which also implies the 4/3 conjecture, as an upper bound, for this
class of graph-TSP.
  Recently, M\&quot;omke and Svensson presented a randomized algorithm that gives a
1.461-approximation for graph-TSP on general graphs and as a side result a
4/3-approximation algorithm for this problem on subcubic graphs, also settling
the 4/3 conjecture for this class of graph-TSP. We will present a way to
derandomize their algorithm which leads to a smaller running time than the
obvious derandomization. All of the latter also works for multi-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1058</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1058</id><created>2011-07-06</created><authors><author><keyname>Lai</keyname><forenames>Ranch Y. Q.</forenames></author></authors><title>Online Vehicle Detection For Estimating Traffic Status</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a traffic congestion estimation system based on unsupervised
on-line learning algorithm. The system does not rely on background extraction
or motion detection. It extracts local features inside detection regions of
variable size which are drawn on lanes in advance. The extracted features are
then clustered into two classes using K-means and Gaussian Mixture Models(GMM).
A Bayes classifier is used to detect vehicles according to the previous cluster
information which keeps updated whenever system is running by on-line EM
algorithm. Experimental result shows that our system can be adapted to various
traffic scenes for estimating traffic status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1066</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1066</id><created>2011-07-06</created><authors><author><keyname>Giuzzi</keyname><forenames>Luca</forenames></author><author><keyname>Pepe</keyname><forenames>Valentina</forenames></author></authors><title>Families of twisted tensor product codes</title><categories>math.CO cs.IT math.IT</categories><comments>Keywords: Segre Product, Veronesean, Grassmannian, Desarguesian
  spread, Subgeometry, Twisted Product, Constacyclic error correcting code,
  Minimum weight</comments><msc-class>94B05, 94B27, 15A69, 51E20</msc-class><journal-ref>Designs, Codes and Cryptography June 2013, Volume 67, Issue 3, pp
  375-384</journal-ref><doi>10.1007/s10623-012-9613-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using geometric properties of the variety $\cV_{r,t}$, the image under the
Grassmannian map of a Desarguesian $(t-1)$-spread of $\PG(rt-1,q)$, we
introduce error correcting codes related to the twisted tensor product
construction, producing several families of constacyclic codes. We exactly
determine the parameters of these codes and characterise the words of minimum
weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1072</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1072</id><created>2011-07-06</created><updated>2012-04-03</updated><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Goldberg</keyname><forenames>Ian</forenames></author><author><keyname>Kate</keyname><forenames>Aniket</forenames></author><author><keyname>Toft</keyname><forenames>Tomas</forenames></author></authors><title>Adding Query Privacy to Robust DHTs</title><categories>cs.CR cs.DC</categories><comments>To appear at ACM ASIACCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in anonymous communication over distributed hash tables (DHTs) has
increased in recent years. However, almost all known solutions solely aim at
achieving sender or requestor anonymity in DHT queries. In many application
scenarios, it is crucial that the queried key remains secret from intermediate
peers that (help to) route the queries towards their destinations. In this
paper, we satisfy this requirement by presenting an approach for providing
privacy for the keys in DHT queries.
  We use the concept of oblivious transfer (OT) in communication over DHTs to
preserve query privacy without compromising spam resistance. Although our
OT-based approach can work over any DHT, we concentrate on communication over
robust DHTs that can tolerate Byzantine faults and resist spam. We choose the
best-known robust DHT construction, and employ an efficient OT protocol
well-suited for achieving our goal of obtaining query privacy over robust DHTs.
Finally, we compare the performance of our privacy-preserving protocols with
their more privacy-invasive counterparts. We observe that there is no increase
in the message complexity and only a small overhead in the computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1076</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1076</id><created>2011-07-06</created><authors><author><keyname>Thomas</keyname><forenames>Antoine</forenames></author><author><keyname>Ouangraoua</keyname><forenames>A&#xef;da</forenames></author><author><keyname>Varr&#xe9;</keyname><forenames>Jean-St&#xe9;phane</forenames></author></authors><title>Genome Halving by Block Interchange</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of finding the minimal number of block interchanges
(exchange of two intervals) required to transform a duplicated linear genome
into a tandem duplicated linear genome. We provide a formula for the distance
as well as a polynomial time algorithm for the sorting problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1081</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1081</id><created>2011-07-06</created><authors><author><keyname>Dhandra</keyname><forenames>B. V.</forenames></author><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author><author><keyname>Mukarambi</keyname><forenames>Gururaj</forenames></author></authors><title>Spatial Features for Multi-Font/Multi-Size Kannada Numerals and Vowels
  Recognition</title><categories>cs.CV</categories><comments>4 pages, 4 Figures, 4 Tables, &quot;International Conference on
  Communication, Computation, Control and Nanotechnology (2010)&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents multi-font/multi-size Kannada numerals and vowels
recognition based on spatial features. Directional spatial features viz stroke
density, stroke length and the number of stokes in an image are employed as
potential features to characterize the printed Kannada numerals and vowels.
Based on these features 1100 numerals and 1400 vowels are classified with
Multi-class Support Vector Machines (SVM). The proposed system achieves the
recognition accuracy as 98.45% and 90.64% for numerals and vowels respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1086</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1086</id><created>2011-07-06</created><authors><author><keyname>Meyer</keyname><forenames>Steven</forenames></author></authors><title>Breaking GSM with rainbow Tables</title><categories>cs.CR</categories><comments>March 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since 1998 the GSM security has been academically broken but no real attack
has ever been done until in 2008 when two engineers of Pico Computing (FPGA
manufacture) revealed that they could break the GSM encryption in 30 seconds
with 200'000$ hardware and precomputed rainbow tables. Since then the hardware
was either available for rich people only or was confiscated by government
agencies. So Chris Paget and Karsten Nohl decided to react and do the same
thing but in a distributed open source form (on torrent). This way everybody
could &quot;enjoy&quot; breaking GSM security and operators will be forced to upgrade the
GSM protocol that is being used by more than 4 billion users and that is more
than 20 years old.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1089</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1089</id><created>2011-07-06</created><updated>2012-09-27</updated><authors><author><keyname>Sevilla</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Mozo</keyname><forenames>Alberto</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author></authors><title>Node Sampling using Random Centrifugal Walks</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling a network with a given probability distribution has been identified
as a useful operation. In this paper we propose distributed algorithms for
sampling networks, so that nodes are selected by a special node, called the
\emph{source}, with a given probability distribution. All these algorithms are
based on a new class of random walks, that we call Random Centrifugal Walks
(RCW). A RCW is a random walk that starts at the source and always moves away
from it.
  Firstly, an algorithm to sample any connected network using RCW is proposed.
The algorithm assumes that each node has a weight, so that the sampling process
must select a node with a probability proportional to its weight. This
algorithm requires a preprocessing phase before the sampling of nodes. In
particular, a minimum diameter spanning tree (MDST) is created in the network,
and then nodes' weights are efficiently aggregated using the tree. The good
news are that the preprocessing is done only once, regardless of the number of
sources and the number of samples taken from the network. After that, every
sample is done with a RCW whose length is bounded by the network diameter.
  Secondly, RCW algorithms that do not require preprocessing are proposed for
grids and networks with regular concentric connectivity, for the case when the
probability of selecting a node is a function of its distance to the source.
  The key features of the RCW algorithms (unlike previous Markovian approaches)
are that (1) they do not need to warm-up (stabilize), (2) the sampling always
finishes in a number of hops bounded by the network diameter, and (3) it
selects a node with the exact probability distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1099</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1099</id><created>2011-07-06</created><authors><author><keyname>Meyer</keyname><forenames>Steven</forenames></author></authors><title>Selling train tickets by SMS</title><categories>cs.CR</categories><comments>June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selling train tickets has evolved in the last ten years from queuing in the
railway station, to buying tickets on the internet and printing them. Both
alternatives are still viable options, though they are time consuming or need
printing devices. Nowadays it is essential to offer a service that is as fast
and efficient as possible: mobile phones provide an accessible, affordable and
widely available tool for supplying information and transferring data. The goal
of this project is to design a train ticket contained in a SMS message. While
there are several challenges related to the project, the main one is the
security and how we can digitally sign a train ticket that is contained in 160
characters. The solution offered in this project is the implementation of the
MOVA Signature (from the name of the inventors MOnnerat and VAudenay) that uses
an interactive verification and therefore allows signature of 20 bits (roughly
4 characters).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1101</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1101</id><created>2011-07-06</created><authors><author><keyname>Meyer</keyname><forenames>Steven</forenames></author></authors><title>Misbehavior in Mobile Application Markets</title><categories>cs.CR</categories><comments>December 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile application markets facilitate the distribution of applications and
thus help developers advertise their work and customers find useful
applications. In addition, the operators of mobile application markets can
control the quality and the content of the applications. These markets are
growing rapidly with more than 300'000 application in the App Store of Apple
and more than 100'000 in the Android Market of Google. This is not only a great
opportunity for phone manufacturers to earn money but also for indie developers
(single or small teams of developers with small financial support) who can thus
have a great distribution channel. Steve Demeter, the Trim game developer for
iPhone, became millionaire with a single puzzle game . Obviously, as new
markets generate a lot of money, the temptation of misbehavior to steal part of
the benefits is big. The first famous case was the one of Molinker who
self-rated his applications with 5 stars to pump up his ranking in order to
increase its revenue stream. In this report, we will consider the problem of
misbehavior in mobile application markets. We will investigate multiple attacks
by misbehaving developers, users or network operators that aim at breaking
rules for their own benefit, managing to outwit the operators' control on which
applications can be installed. We notably suggest novel attacks that may affect
mobile markets in the future: in particular, we show that it is possible to get
revenue for applications created by someone else, trick a user to download and
buy an application and new ways to pump up an application's ranking. We will
also discuss possible solutions against spyware applications and cheating
developer
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1104</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1104</id><created>2011-07-06</created><authors><author><keyname>Araujo</keyname><forenames>Samur</forenames></author><author><keyname>Hidders</keyname><forenames>Jan</forenames></author><author><keyname>Schwabe</keyname><forenames>Daniel</forenames></author><author><keyname>de Vries</keyname><forenames>Arjen P.</forenames></author></authors><title>SERIMI - Resource Description Similarity, RDF Instance Matching and
  Interlinking</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interlinking of datasets published in the Linked Data Cloud is a
challenging problem and a key factor for the success of the Semantic Web.
Manual rule-based methods are the most effective solution for the problem, but
they require skilled human data publishers going through a laborious, error
prone and time-consuming process for manually describing rules mapping
instances between two datasets. Thus, an automatic approach for solving this
problem is more than welcome. In this paper, we propose a novel interlinking
method, SERIMI, for solving this problem automatically. SERIMI matches
instances between a source and a target datasets, without prior knowledge of
the data, domain or schema of these datasets. Experiments conducted with
benchmark collections demonstrate that our approach considerably outperforms
state-of-the-art automatic approaches for solving the interlinking problem on
the Linked Data Cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1119</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1119</id><created>2011-07-06</created><authors><author><keyname>Hertzberg</keyname><forenames>Christoph</forenames></author><author><keyname>Wagner</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Frese</keyname><forenames>Udo</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author></authors><title>Integrating Generic Sensor Fusion Algorithms with Sound State
  Representations through Encapsulation of Manifolds</title><categories>cs.RO cs.CV cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common estimation algorithms, such as least squares estimation or the Kalman
filter, operate on a state in a state space S that is represented as a
real-valued vector. However, for many quantities, most notably orientations in
3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a
vector space locally but has a more complex global topological structure. For
integrating these quantities, several ad-hoc approaches have been proposed.
  Here, we present a principled solution to this problem where the structure of
the manifold S is encapsulated by two operators, state displacement [+]:S x R^n
--&gt; S and its inverse [-]: S x S --&gt; R^n. These operators provide a local
vector-space view \delta; --&gt; x [+] \delta; around a given state x. Generic
estimation algorithms can then work on the manifold S mainly by replacing +/-
with [+]/[-] where appropriate. We analyze these operators axiomatically, and
demonstrate their use in least-squares estimation and the Unscented Kalman
Filter. Moreover, we exploit the idea of encapsulation from a software
engineering perspective in the Manifold Toolkit, where the [+]/[-] operators
mediate between a &quot;flat-vector&quot; view for the generic algorithm and a
&quot;named-members&quot; view for the problem specific functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1127</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1127</id><created>2011-07-05</created><authors><author><keyname>Sapre</keyname><forenames>Shreeniwas</forenames></author><author><keyname>Sharma</keyname><forenames>Hrishikesh</forenames></author><author><keyname>Patil</keyname><forenames>Abhishek</forenames></author><author><keyname>Adiga</keyname><forenames>B. S.</forenames></author><author><keyname>Patkar</keyname><forenames>Sachin</forenames></author></authors><title>Finite Projective Geometry based Fast, Conflict-free Parallel Matrix
  Computations</title><categories>cs.NA cs.DC math.NA</categories><comments>32 pages, to be submitted to some distributed and parallel computing
  journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix computations, especially iterative PDE solving (and the sparse matrix
vector multiplication subproblem within) using conjugate gradient algorithm,
and LU/Cholesky decomposition for solving system of linear equations, form the
kernel of many applications, such as circuit simulators, computational fluid
dynamics or structural analysis etc. The problem of designing approaches for
parallelizing these computations, to get good speedups as much as possible as
per Amdahl's law, has been continuously researched upon. In this paper, we
discuss approaches based on the use of finite projective geometry graphs for
these two problems. For the problem of conjugate gradient algorithm, the
approach looks at an alternative data distribution based on projective-geometry
concepts. It is proved that this data distribution is an optimal data
distribution for scheduling the main problem of dense matrix-vector
multiplication. For the problem of parallel LU/Cholesky decomposition of
general matrices, the approach is motivated by the recently published scheme
for interconnects of distributed systems, perfect difference networks. We find
that projective-geometry based graphs indeed offer an exciting way of
parallelizing these computations, and in fact many others. Moreover, their
applications ranges from architectural ones (interconnect choice) to
algorithmic ones (data distributions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1128</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1128</id><created>2011-07-05</created><authors><author><keyname>Seeja</keyname><forenames>K. R</forenames></author></authors><title>AISMOTIF-An Artificial Immune System for DNA Motif Discovery</title><categories>cs.CE</categories><comments>7 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 2, March 2011 IJCSI International Journal of Computer Science Issues,
  Vol. 8, Issue 2, March 2011, ISSN (Online): 1694-0814, pages 143-149</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovery of transcription factor binding sites is a much explored and still
exploring area of research in functional genomics. Many computational tools
have been developed for finding motifs and each of them has their own
advantages as well as disadvantages. Most of these algorithms need prior
knowledge about the data to construct background models. However there is not a
single technique that can be considered as best for finding regulatory motifs.
This paper proposes an artificial immune system based algorithm for finding the
transcription factor binding sites or motifs and two new weighted scores for
motif evaluation. The algorithm is enumerative, but sufficient pruning of the
pattern search space has been incorporated using immune system concepts. The
performance of AISMOTIF has been evaluated by comparing it with eight state of
art composite motif discovery algorithms and found that AISMOTIF predicts known
motifs as well as new motifs from the benchmark dataset without any prior
knowledge about the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1138</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1138</id><created>2011-07-06</created><updated>2012-10-30</updated><authors><author><keyname>Chen</keyname><forenames>X.</forenames></author><author><keyname>Chen</keyname><forenames>M.</forenames></author><author><keyname>Li</keyname><forenames>B.</forenames></author><author><keyname>Zhao</keyname><forenames>Y.</forenames></author><author><keyname>Wu</keyname><forenames>Y.</forenames></author><author><keyname>Li</keyname><forenames>J.</forenames></author></authors><title>Celerity: A Low-Delay Multi-Party Conferencing Solution</title><categories>cs.MM</categories><comments>16 pages,23 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we attempt to revisit the problem of multi-party conferencing
from a practical perspective, and to rethink the design space involved in this
problem. We believe that an emphasis on low end-to-end delays between any two
parties in the conference is a must, and the source sending rate in a session
should adapt to bandwidth availability and congestion. We present Celerity, a
multi-party conferencing solution specifically designed to achieve our
objectives. It is entirely Peer-to-Peer (P2P), and as such eliminating the cost
of maintaining centrally administered servers. It is designed to deliver video
with low end-to-end delays, at quality levels commensurate with available
network resources over arbitrary network topologies where bottlenecks can be
anywhere in the network. This is in contrast to commonly assumed P2P scenarios
where bandwidth bottlenecks reside only at the edge of the network. The
highlight in our design is a distributed and adaptive rate control protocol,
that can discover and adapt to arbitrary topologies and network conditions
quickly, converging to efficient link rate allocations allowed by the
underlying network. In accordance with adaptive link rate control, source video
encoding rates are also dynamically controlled to optimize video quality in
arbitrary and unpredictable network conditions. We have implemented Celerity in
a prototype system, and demonstrate its superior performance over existing
solutions in a local experimental testbed and over the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1149</identifier>
 <datestamp>2011-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1149</id><created>2011-07-06</created><updated>2011-07-21</updated><authors><author><keyname>Hoyrup</keyname><forenames>Mathieu</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>The dimension of ergodic random sequences</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let \mu be a computable ergodic shift-invariant measure over the Cantor
space. Providing a constructive proof of Shannon-McMillan-Breiman theorem,
V'yugin proved that if a sequence x is Martin-L\&quot;of random w.r.t. \mu then the
strong effective dimension Dim(x) of x equals the entropy of \mu. Whether its
effective dimension dim(x) also equals the entropy was left as an problem
question. In this paper we settle this problem, providing a positive answer. A
key step in the proof consists in extending recent results on Birkhoff's
ergodic theorem for Martin-L\&quot;of random sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1155</identifier>
 <datestamp>2012-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1155</id><created>2011-07-06</created><updated>2012-02-12</updated><authors><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Limits of modularity maximization in community detection</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 9 figures. Analysis extended to other global optimization
  methods. Final version published in Physical Review E</comments><journal-ref>Physical Review E84, 066122 (2011)</journal-ref><doi>10.1103/PhysRevE.84.066122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity maximization is the most popular technique for the detection of
community structure in graphs. The resolution limit of the method is supposedly
solvable with the introduction of modified versions of the measure, with
tunable resolution parameters. We show that multiresolution modularity suffers
from two opposite coexisting problems: the tendency to merge small subgraphs,
which dominates when the resolution is low; the tendency to split large
subgraphs, which dominates when the resolution is high. In benchmark networks
with heterogeneous distributions of cluster sizes, the simultaneous elimination
of both biases is not possible and multiresolution modularity is not capable to
recover the planted community structure, not even when it is pronounced and
easily detectable by other methods, for any value of the resolution parameter.
This holds for other multiresolution techniques and it is likely to be a
general problem of methods based on global optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1158</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1158</id><created>2011-07-06</created><updated>2014-01-27</updated><authors><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Liao</keyname><forenames>Qi</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Measurement-Adaptive Cellular Random Access Protocols</title><categories>cs.NI</categories><comments>31 pages, 13 figures, 3 tables. Springer Wireless Networks 2014</comments><journal-ref>Springer Wireless Networks, Volume 20, Issue 6, pp. 1495-1514
  (2014)</journal-ref><doi>10.1007/s11276-014-0689-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a single-cell random access channel (RACH) in cellular
wireless networks. Communications over RACH take place when users try to
connect to a base station during a handover or when establishing a new
connection. Within the framework of Self-Organizing Networks (SONs), the system
should self- adapt to dynamically changing environments (channel fading,
mobility, etc.) without human intervention. For the performance improvement of
the RACH procedure, we aim here at maximizing throughput or alternatively
minimizing the user dropping rate. In the context of SON, we propose protocols
which exploit information from measurements and user reports in order to
estimate current values of the system unknowns and broadcast global
action-related values to all users. The protocols suggest an optimal pair of
user actions (transmission power and back-off probability) found by minimizing
the drift of a certain function. Numerical results illustrate considerable
benefits of the dropping rate, at a very low or even zero cost in power
expenditure and delay, as well as the fast adaptability of the protocols to
environment changes. Although the proposed protocol is designed to minimize
primarily the amount of discarded users per cell, our framework allows for
other variations (power or delay minimization) as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1161</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1161</id><created>2011-07-06</created><updated>2012-05-16</updated><authors><author><keyname>Couceiro</keyname><forenames>Miguel</forenames></author><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Waldhauser</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Locally monotone Boolean and pseudo-Boolean functions</title><categories>cs.DM math.CO</categories><msc-class>06E30, 94C10</msc-class><journal-ref>Discrete Applied Mathematics 160 (12) (2012) 1651-1660</journal-ref><doi>10.1016/j.dam.2012.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose local versions of monotonicity for Boolean and pseudo-Boolean
functions: say that a pseudo-Boolean (Boolean) function is p-locally monotone
if none of its partial derivatives changes in sign on tuples which differ in
less than p positions. As it turns out, this parameterized notion provides a
hierarchy of monotonicities for pseudo-Boolean (Boolean) functions. Local
monotonicities are shown to be tightly related to lattice counterparts of
classical partial derivatives via the notion of permutable derivatives. More
precisely, p-locally monotone functions are shown to have p-permutable lattice
derivatives and, in the case of symmetric functions, these two notions
coincide. We provide further results relating these two notions, and present a
classification of p-locally monotone functions, as well as of functions having
p-permutable derivatives, in terms of certain forbidden &quot;sections&quot;, i.e.,
functions which can be obtained by substituting constants for variables. This
description is made explicit in the special case when p=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1163</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1163</id><created>2011-07-06</created><updated>2012-06-20</updated><authors><author><keyname>Luss</keyname><forenames>Ronny</forenames></author><author><keyname>Teboulle</keyname><forenames>Marc</forenames></author></authors><title>Conditional Gradient Algorithms for Rank-One Matrix Approximations with
  a Sparsity Constraint</title><categories>math.OC cs.SY</categories><comments>Minor changes. Final version. To appear in SIAM Review</comments><msc-class>90C30, 62H25, 49M37, 65K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparsity constrained rank-one matrix approximation problem is a difficult
mathematical optimization problem which arises in a wide array of useful
applications in engineering, machine learning and statistics, and the design of
algorithms for this problem has attracted intensive research activities. We
introduce an algorithmic framework, called ConGradU, that unifies a variety of
seemingly different algorithms that have been derived from disparate
approaches, and allows for deriving new schemes. Building on the old and
well-known conditional gradient algorithm, ConGradU is a simplified version
with unit step size and yields a generic algorithm which either is given by an
analytic formula or requires a very low computational complexity. Mathematical
properties are systematically developed and numerical experiments are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1166</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1166</id><created>2011-07-06</created><authors><author><keyname>Bellettini</keyname><forenames>Carlo</forenames></author><author><keyname>Capra</keyname><forenames>Lorenzo</forenames></author></authors><title>Reachability Analysis of Time Basic Petri Nets: a Time Coverage Approach</title><categories>cs.SE</categories><comments>8 pages, submitted to conference for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a technique for reachability analysis of Time-Basic (TB) Petri
nets, a powerful formalism for real- time systems where time constraints are
expressed as intervals, representing possible transition firing times, whose
bounds are functions of marking's time description. The technique consists of
building a symbolic reachability graph relying on a sort of time coverage, and
overcomes the limitations of the only available analyzer for TB nets, based in
turn on a time-bounded inspection of a (possibly infinite) reachability-tree.
The graph construction algorithm has been automated by a tool-set, briefly
described in the paper together with its main functionality and analysis
capability. A running example is used throughout the paper to sketch the
symbolic graph construction. A use case describing a small real system - that
the running example is an excerpt from - has been employed to benchmark the
technique and the tool-set. The main outcome of this test are also presented in
the paper. Ongoing work, in the perspective of integrating with a
model-checking engine, is shortly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1177</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1177</id><created>2011-07-06</created><authors><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Not So Easy Problems for Tree Decomposable Graphs</title><categories>cs.DS cs.DM</categories><comments>Author's self-archived copy</comments><journal-ref>Ramanujan Mathematical Society, Lecture Notes Series no. 13, 2010,
  pp. 179-190</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider combinatorial problems that can be solved in polynomial time for
graphs of bounded treewidth but where the order of the polynomial that bounds
the running time is expected to depend on the treewidth bound. First we review
some recent results for problems regarding list and equitable colorings,
general factors, and generalized satisfiability. Second we establish a new
hardness result for the problem of minimizing the maximum weighted outdegree
for orientations of edge-weighted graphs of bounded treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1198</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1198</id><created>2011-07-06</created><authors><author><keyname>Leitner-Fischer</keyname><forenames>Florian</forenames><affiliation>University Konstanz</affiliation></author><author><keyname>Leue</keyname><forenames>Stefan</forenames><affiliation>University Konstanz</affiliation></author></authors><title>QuantUM: Quantitative Safety Analysis of UML Models</title><categories>cs.SE</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><acm-class>D.2.4;D.3.2;</acm-class><journal-ref>EPTCS 57, 2011, pp. 16-30</journal-ref><doi>10.4204/EPTCS.57.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When developing a safety-critical system it is essential to obtain an
assessment of different design alternatives. In particular, an early safety
assessment of the architectural design of a system is desirable. In spite of
the plethora of available formal quantitative analysis methods it is still
difficult for software and system architects to integrate these techniques into
their every day work. This is mainly due to the lack of methods that can be
directly applied to architecture level models, for instance given as UML
diagrams. Also, it is necessary that the description methods used do not
require a profound knowledge of formal methods. Our approach bridges this gap
and improves the integration of quantitative safety analysis methods into the
development process. All inputs of the analysis are specified at the level of a
UML model. This model is then automatically translated into the analysis model,
and the results of the analysis are consequently represented on the level of
the UML model. Thus the analysis model and the formal methods used during the
analysis are hidden from the user. We illustrate the usefulness of our approach
using an industrial strength case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1199</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1199</id><created>2011-07-06</created><authors><author><keyname>Rutkowski</keyname><forenames>Michal</forenames><affiliation>Department of Computer Science, The University of Warwick</affiliation></author></authors><title>Two-Player Reachability-Price Games on Single-Clock Timed Automata</title><categories>cs.GT cs.DS</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 31-46</journal-ref><doi>10.4204/EPTCS.57.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two player reachability-price games on single-clock timed automata.
The problem is as follows: given a state of the automaton, determine whether
the first player can guarantee reaching one of the designated goal locations.
If a goal location can be reached then we also want to compute the optimum
price of doing so. Our contribution is twofold. First, we develop a theory of
cost functions, which provide a comprehensive methodology for the analysis of
this problem. This theory allows us to establish our second contribution, an
EXPTIME algorithm for computing the optimum reachability price, which improves
the existing 3EXPTIME upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1200</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1200</id><created>2011-07-06</created><authors><author><keyname>Aman</keyname><forenames>Bogdan</forenames><affiliation>''A.I. Cuza'' University of Iasi, Romania</affiliation></author><author><keyname>Ciobanu</keyname><forenames>Gabriel</forenames><affiliation>Institute of Computer Science, Romanian Academy, Iasi</affiliation></author></authors><title>Time Delays in Membrane Systems and Petri Nets</title><categories>cs.DC cs.FL</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><journal-ref>EPTCS 57, 2011, pp. 47-60</journal-ref><doi>10.4204/EPTCS.57.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timing aspects in formalisms with explicit resources and parallelism are
investigated, and it is presented a formal link between timed membrane systems
and timed Petri nets with localities. For both formalisms, timing does not
increase the expressive power; however both timed membrane systems and timed
Petri nets are more flexible in describing molecular phenomena where time is a
critical resource. We establish a link between timed membrane systems and timed
Petri nets with localities, and prove an operational correspondence between
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1201</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1201</id><created>2011-07-06</created><authors><author><keyname>Deng</keyname><forenames>Yuxin</forenames><affiliation>Shanghai Jiao Tong University</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames><affiliation>NICTA</affiliation></author><author><keyname>Hennessy</keyname><forenames>Matthew</forenames><affiliation>Trinity College Dublin</affiliation></author><author><keyname>Morgan</keyname><forenames>Carroll</forenames><affiliation>University of New South Wales</affiliation></author></authors><title>Real-Reward Testing for Probabilistic Processes (Extended Abstract)</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><acm-class>F.3.2; D.3.1</acm-class><journal-ref>EPTCS 57, 2011, pp. 61-73</journal-ref><doi>10.4204/EPTCS.57.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a notion of real-valued reward testing for probabilistic
processes by extending the traditional nonnegative-reward testing with negative
rewards. In this richer testing framework, the may and must preorders turn out
to be inverses. We show that for convergent processes with finitely many states
and transitions, but not in the presence of divergence, the real-reward
must-testing preorder coincides with the nonnegative-reward must-testing
preorder. To prove this coincidence we characterise the usual resolution-based
testing in terms of the weak transitions of processes, without having to
involve policies, adversaries, schedulers, resolutions, or similar structures
that are external to the process under investigation. This requires
establishing the continuity of our function for calculating testing outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1202</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1202</id><created>2011-07-06</created><authors><author><keyname>Song</keyname><forenames>Lei</forenames><affiliation>IT University of Copenhagen, Denmark</affiliation></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames><affiliation>Technical University of Denmark</affiliation></author><author><keyname>Nielsen</keyname><forenames>Bo Friis</forenames><affiliation>Technical University of Denmark</affiliation></author></authors><title>A Stochastic Broadcast Pi-Calculus</title><categories>cs.PF</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 74-88</journal-ref><doi>10.4204/EPTCS.57.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a stochastic broadcast PI-calculus which can be used
to model server-client based systems where synchronization is always governed
by only one participant. Therefore, there is no need to determine the joint
synchronization rates. We also take immediate transitions into account which is
useful to model behaviors with no impact on the temporal properties of a
system. Since immediate transitions may introduce non-determinism, we will show
how these non-determinism can be resolved, and as result a valid CTMC will be
obtained finally. Also some practical examples are given to show the
application of this calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1203</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1203</id><created>2011-07-06</created><authors><author><keyname>Seidel</keyname><forenames>Daniel</forenames></author><author><keyname>Voigtl&#xe4;nder</keyname><forenames>Janis</forenames></author></authors><title>Improvements for Free</title><categories>cs.PL cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><acm-class>D.1.1; D.3.1; F.3.1</acm-class><journal-ref>EPTCS 57, 2011, pp. 89-103</journal-ref><doi>10.4204/EPTCS.57.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Theorems for Free!&quot; (Wadler, FPCA 1989) is a slogan for a technique that
allows to derive statements about functions just from their types. So far, the
statements considered have always had a purely extensional flavor: statements
relating the value semantics of program expressions, but not statements
relating their runtime (or other) cost. Here we study an extension of the
technique that allows precisely statements of the latter flavor, by deriving
quantitative theorems for free. After developing the theory, we walk through a
number of example derivations. Probably none of the statements derived in those
simple examples will be particularly surprising to most readers, but what is
maybe surprising, and at the very least novel, is that there is a general
technique for obtaining such results on a quantitative level in a principled
way. Moreover, there is good potential to bring that technique to bear on more
complex examples as well. We turn our attention to short-cut fusion (Gill et
al., FPCA 1993) in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1204</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1204</id><created>2011-07-06</created><authors><author><keyname>Assouramou</keyname><forenames>Joseph</forenames><affiliation>Universit&#xe9; Laval</affiliation></author><author><keyname>Desharnais</keyname><forenames>Jos&#xe9;e</forenames><affiliation>Universit&#xe9; Laval</affiliation></author></authors><title>Analysis of Non-Linear Probabilistic Hybrid Systems</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 104-119</journal-ref><doi>10.4204/EPTCS.57.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how to compute, for probabilistic hybrid systems, the clock
approximation and linear phase-portrait approximation that have been proposed
for non probabilistic processes by Henzinger et al. The techniques permit to
define a rectangular probabilistic process from a non rectangular one, hence
allowing the model-checking of any class of systems. Clock approximation, which
applies under some restrictions, aims at replacing a non rectangular variable
by a clock variable. Linear phase-approximation applies without restriction and
yields an approximation that simulates the original process. The conditions
that we need for probabilistic processes are the same as those for the classic
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1205</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1205</id><created>2011-07-06</created><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames><affiliation>Irisa/INRIA Rennes</affiliation></author><author><keyname>Thrane</keyname><forenames>Claus</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University</affiliation></author></authors><title>Distances for Weighted Transition Systems: Games and Properties</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 134-147</journal-ref><doi>10.4204/EPTCS.57.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general framework for reasoning about distances between
transition systems with quantitative information. Taking as starting point an
arbitrary distance on system traces, we show how this leads to natural
definitions of a linear and a branching distance on states of such a transition
system. We show that our framework generalizes and unifies a large variety of
previously considered system distances, and we develop some general properties
of our distances. We also show that if the trace distance admits a recursive
characterization, then the corresponding branching distance can be obtained as
a least fixed point to a similar recursive characterization. The central tool
in our work is a theory of infinite path-building games with quantitative
objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1206</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1206</id><created>2011-07-06</created><authors><author><keyname>Tracol</keyname><forenames>Mathieu</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Desharnais</keyname><forenames>Jos&#xe9;e</forenames><affiliation>Departement d'informatique et de g&#xe9;nie logiciel, Universit&#xe9; Laval, Qu&#xe9;bec, Canada</affiliation></author><author><keyname>Zhioua</keyname><forenames>Abir</forenames><affiliation>Departement d'informatique et de g&#xe9;nie logiciel, Universit&#xe9; Laval, Qu&#xe9;bec, Canada</affiliation></author></authors><title>Computing Distances between Probabilistic Automata</title><categories>cs.FL cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 148-162</journal-ref><doi>10.4204/EPTCS.57.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present relaxed notions of simulation and bisimulation on Probabilistic
Automata (PA), that allow some error epsilon. When epsilon is zero we retrieve
the usual notions of bisimulation and simulation on PAs. We give logical
characterisations of these notions by choosing suitable logics which differ
from the elementary ones, L with negation and L without negation, by the modal
operator. Using flow networks, we show how to compute the relations in PTIME.
This allows the definition of an efficiently computable non-discounted distance
between the states of a PA. A natural modification of this distance is
introduced, to obtain a discounted distance, which weakens the influence of
long term transitions. We compare our notions of distance to others previously
defined and illustrate our approach on various examples. We also show that our
distance is not expansive with respect to process algebra operators. Although L
without negation is a suitable logic to characterise epsilon-(bi)simulation on
deterministic PAs, it is not for general PAs; interestingly, we prove that it
does characterise weaker notions, called a priori epsilon-(bi)simulation, which
we prove to be NP-difficult to decide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1222</identifier>
 <datestamp>2012-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1222</id><created>2011-07-06</created><updated>2012-07-30</updated><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames><affiliation>MPI for Intelligent Systems</affiliation></author></authors><title>On the information-theoretic structure of distributed measurements</title><categories>cs.IT cs.DC cs.NE math.CT math.IT nlin.CG</categories><comments>In Proceedings DCM 2011, arXiv:1207.6821</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 88, 2012, pp. 28-42</journal-ref><doi>10.4204/EPTCS.88.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internal structure of a measuring device, which depends on what its
components are and how they are organized, determines how it categorizes its
inputs. This paper presents a geometric approach to studying the internal
structure of measurements performed by distributed systems such as
probabilistic cellular automata. It constructs the quale, a family of sections
of a suitably defined presheaf, whose elements correspond to the measurements
performed by all subsystems of a distributed system. Using the quale we
quantify (i) the information generated by a measurement; (ii) the extent to
which a measurement is context-dependent; and (iii) whether a measurement is
decomposable into independent submeasurements, which turns out to be equivalent
to context-dependence. Finally, we show that only indecomposable measurements
are more informative than the sum of their submeasurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1229</identifier>
 <datestamp>2011-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1229</id><created>2011-07-06</created><authors><author><keyname>Brocklebank</keyname><forenames>Sean</forenames></author><author><keyname>Pauls</keyname><forenames>Scott</forenames></author><author><keyname>Rockmore</keyname><forenames>Daniel</forenames></author><author><keyname>Bates</keyname><forenames>Timothy C.</forenames></author></authors><title>Characteristic Characteristics</title><categories>stat.AP cs.IR physics.data-an</categories><comments>23 pages, 5 Figures, 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While five-factor models of personality are widespread, there is still not
universal agreement on this as a structural framework. Part of the reason for
the lingering debate is its dependence on factor analysis. In particular,
derivation or refutation of the model via other statistical means is a
worthwhile project. In this paper we use the methodology of spectral clustering
to articulate the structure in the dataset of responses of 20,993 subjects on a
300-item item version of the IPIP NEO personality questionnaire, and we compare
our results to those obtained from a factor analytic solution. We found support
for five- and six-cluster solutions. The five-cluster solution was similar to a
conventional five-factor solution, but the six-cluster and six-factor solutions
differed significantly, and only the six-cluster solution was readily
interpretable: it gave a model similar to the HEXACO model. We suggest that
spectral clustering provides a robust alternative view of personality data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1233</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1233</id><created>2011-07-06</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Galpin</keyname><forenames>Vashti</forenames></author><author><keyname>Hillston</keyname><forenames>Jane</forenames></author></authors><title>HYPE with stochastic events</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2011, arXiv:1107.0746</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 57, 2011, pp. 120-133</journal-ref><doi>10.4204/EPTCS.57.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process algebra HYPE was recently proposed as a fine-grained modelling
approach for capturing the behaviour of hybrid systems. In the original
proposal, each flow or influence affecting a variable is modelled separately
and the overall behaviour of the system then emerges as the composition of
these flows. The discrete behaviour of the system is captured by instantaneous
actions which might be urgent, taking effect as soon as some activation
condition is satisfied, or non-urgent meaning that they can tolerate some
(unknown) delay before happening. In this paper we refine the notion of
non-urgent actions, to make such actions governed by a probability
distribution. As a consequence of this we now give HYPE a semantics in terms of
Transition-Driven Stochastic Hybrid Automata, which are a subset of a general
class of stochastic processes termed Piecewise Deterministic Markov Processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1257</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1257</id><created>2011-07-06</created><authors><author><keyname>Mubeen</keyname><forenames>M. Asim</forenames></author><author><keyname>Knuth</keyname><forenames>Kevin H.</forenames></author></authors><title>Evidence-Based Filters for Signal Detection: Application to Evoked Brain
  Responses</title><categories>physics.comp-ph cs.CV physics.med-ph</categories><comments>8 Pages, 3 figures, 7 equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template-based signal detection most often relies on computing a correlation,
or a dot product, between an incoming data stream and a signal template. Such a
correlation results in an ongoing estimate of the magnitude of the signal in
the data stream. However, it does not directly indicate the presence or absence
of the signal. The problem is really one of model-testing, and the relevant
quantity is the Bayesian evidence (marginal likelihood) of the signal model.
Given a signal template and an ongoing data stream, we have developed an
evidence-based filter that computes the Bayesian evidence that a signal is
present in the data. We demonstrate this algorithm by applying it to
brain-machine interface (BMI) data obtained by recording human brain electrical
activity, or electroencephalography (EEG). A very popular and effective
paradigm in EEG-based BMI is based on the detection of the P300 evoked brain
response which is generated in response to particular sensory stimuli. The goal
is to detect the presence of a P300 signal in ongoing EEG activity as
accurately and as fast as possible. Our algorithm uses a subject-specific P300
template to compute the Bayesian evidence that a applying window of EEG data
contains the signal. The efficacy of this algorithm is demonstrated by
comparing receiver operating characteristic (ROC) curves of the evidence-based
filter to the usual correlation method. Our results show a significant
improvement in single-trial P300 detection. The evidence-based filter promises
to improve the accuracy and speed of the detection of evoked brain responses in
BMI applications as well the detection of template signals in more general
signal processing applications
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1265</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1265</id><created>2011-07-06</created><authors><author><keyname>Watson</keyname><forenames>Thomas</forenames></author></authors><title>Lift-and-Project Integrality Gaps for the Traveling Salesperson Problem</title><categories>cs.DS</categories><comments>19 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the lift-and-project procedures of Lov{\'a}sz-Schrijver and
Sherali-Adams applied to the standard linear programming relaxation of the
traveling salesperson problem with triangle inequality. For the asymmetric TSP
tour problem, Charikar, Goemans, and Karloff (FOCS 2004) proved that the
integrality gap of the standard relaxation is at least 2. We prove that after
one round of the Lov{\'a}sz-Schrijver or Sherali-Adams procedures, the
integrality gap of the asymmetric TSP tour problem is at least 3/2, with a
small caveat on which version of the standard relaxation is used. For the
symmetric TSP tour problem, the integrality gap of the standard relaxation is
known to be at least 4/3, and Cheung (SIOPT 2005) proved that it remains at
least 4/3 after $o(n)$ rounds of the Lov{\'a}sz-Schrijver procedure, where $n$
is the number of nodes. For the symmetric TSP path problem, the integrality gap
of the standard relaxation is known to be at least 3/2, and we prove that it
remains at least 3/2 after $o(n)$ rounds of the Lov{\'a}sz-Schrijver procedure,
by a simple reduction to Cheung's result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1270</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1270</id><created>2011-07-06</created><updated>2012-03-03</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Willsky</keyname><forenames>Alan. S.</forenames></author></authors><title>High-Dimensional Gaussian Graphical Model Selection: Walk Summability
  and Local Separation Criterion</title><categories>cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of high-dimensional Gaussian graphical model
selection. We identify a set of graphs for which an efficient estimation
algorithm exists, and this algorithm is based on thresholding of empirical
conditional covariances. Under a set of transparent conditions, we establish
structural consistency (or sparsistency) for the proposed algorithm, when the
number of samples n=omega(J_{min}^{-2} log p), where p is the number of
variables and J_{min} is the minimum (absolute) edge potential of the graphical
model. The sufficient conditions for sparsistency are based on the notion of
walk-summability of the model and the presence of sparse local vertex
separators in the underlying graph. We also derive novel non-asymptotic
necessary conditions on the number of samples required for sparsistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1276</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1276</id><created>2011-07-06</created><updated>2012-07-30</updated><authors><author><keyname>Duarte</keyname><forenames>Melissa</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Experiment-driven Characterization of Full-Duplex Wireless Systems</title><categories>cs.IT math.IT</categories><comments>Revised the submission to IEEE Transactions on Wireless
  Communications, May 2012. Submitted to IEEE Transactions on Wireless
  Communications, July 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an experiment-based characterization of passive suppression and
active self-interference cancellation mechanisms in full-duplex wireless
communication systems. In particular, we consider passive suppression due to
antenna separation at the same node, and active cancellation in analog and/or
digital domain. First, we show that the average amount of cancellation
increases for active cancellation techniques as the received self-interference
power increases. Our characterization of the average cancellation as a function
of the self-interference power allows us to show that for a constant
signal-to-interference ratio at the receiver antenna (before any active
cancellation is applied), the rate of a full-duplex link increases as the
self-interference power increases. Second, we show that applying digital
cancellation after analog cancellation can sometimes increase the
self-interference, and thus digital cancellation is more effective when applied
selectively based on measured suppression values. Third, we complete our study
of the impact of self-interference cancellation mechanisms by characterizing
the probability distribution of the self-interference channel before and after
cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1283</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1283</id><created>2011-07-06</created><updated>2011-11-08</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Spectral Methods for Learning Multivariate Latent Tree Structure</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of learning the structure of multivariate
linear tree models, which include a variety of directed tree graphical models
with continuous, discrete, and mixed latent variables such as linear-Gaussian
models, hidden Markov models, Gaussian mixture models, and Markov evolutionary
trees. The setting is one where we only have samples from certain observed
variables in the tree, and our goal is to estimate the tree structure (i.e.,
the graph of how the underlying hidden variables are connected to each other
and to the observed variables). We propose the Spectral Recursive Grouping
algorithm, an efficient and simple bottom-up procedure for recovering the tree
structure from independent samples of the observed variables. Our finite sample
size bounds for exact recovery of the tree structure reveal certain natural
dependencies on underlying statistical and structural properties of the
underlying joint distribution. Furthermore, our sample complexity guarantees
have no explicit dependence on the dimensionality of the observed variables,
making the algorithm applicable to many high-dimensional settings. At the heart
of our algorithm is a spectral quartet test for determining the relative
topology of a quartet of variables from second-order statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1292</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1292</id><created>2011-07-07</created><authors><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Separator Theorems for Minor-Free and Shallow Minor-Free Graphs with
  Applications</title><categories>cs.DM</categories><comments>To appear at FOCS 2011</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alon, Seymour, and Thomas generalized Lipton and Tarjan's planar separator
theorem and showed that a $K_h$-minor free graph with $n$ vertices has a
separator of size at most $h^{3/2}\sqrt n$. They gave an algorithm that, given
a graph $G$ with $m$ edges and $n$ vertices and given an integer $h\geq 1$,
outputs in $O(\sqrt{hn}m)$ time such a separator or a $K_h$-minor of $G$.
Plotkin, Rao, and Smith gave an $O(hm\sqrt{n\log n})$ time algorithm to find a
separator of size $O(h\sqrt{n\log n})$. Kawarabayashi and Reed improved the
bound on the size of the separator to $h\sqrt n$ and gave an algorithm that
finds such a separator in $O(n^{1 + \epsilon})$ time for any constant $\epsilon
&gt; 0$, assuming $h$ is constant. This algorithm has an extremely large
dependency on $h$ in the running time (some power tower of $h$ whose height is
itself a function of $h$), making it impractical even for small $h$. We are
interested in a small polynomial time dependency on $h$ and we show how to find
an $O(h\sqrt{n\log n})$-size separator or report that $G$ has a $K_h$-minor in
$O(\poly(h)n^{5/4 + \epsilon})$ time for any constant $\epsilon &gt; 0$. We also
present the first $O(\poly(h)n)$ time algorithm to find a separator of size
$O(n^c)$ for a constant $c &lt; 1$. As corollaries of our results, we get improved
algorithms for shortest paths and maximum matching. Furthermore, for integers
$\ell$ and $h$, we give an $O(m + n^{2 + \epsilon}/\ell)$ time algorithm that
either produces a $K_h$-minor of depth $O(\ell\log n)$ or a separator of size
at most $O(n/\ell + \ell h^2\log n)$. This improves the shallow minor algorithm
of Plotkin, Rao, and Smith when $m = \Omega(n^{1 + \epsilon})$. We get a
similar running time improvement for an approximation algorithm for the problem
of finding a largest $K_h$-minor in a given graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1322</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1322</id><created>2011-07-07</created><updated>2011-08-29</updated><authors><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Text Classification: A Sequential Reading Approach</title><categories>cs.AI cs.IR cs.LG</categories><comments>ECIR2011</comments><journal-ref>Lecture Notes in Computer Science, 2011, Volume 6611/2011, 411-423</journal-ref><doi>10.1007/978-3-642-20161-5_41</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to model the text classification process as a sequential decision
process. In this process, an agent learns to classify documents into topics
while reading the document sentences sequentially and learns to stop as soon as
enough information was read for deciding. The proposed algorithm is based on a
modelisation of Text Classification as a Markov Decision Process and learns by
using Reinforcement Learning. Experiments on four different classical
mono-label corpora show that the proposed approach performs comparably to
classical SVM approaches for large training sets, and better for small training
sets. In addition, the model automatically adapts its reading process to the
quantity of training information provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1327</identifier>
 <datestamp>2012-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1327</id><created>2011-07-07</created><updated>2012-02-16</updated><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>On counting untyped lambda terms</title><categories>cs.LO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several results on counting untyped lambda terms, i.e., on telling
how many terms belong to such or such class, according to the size of the terms
and/or to the number of free variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1339</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1339</id><created>2011-07-07</created><authors><author><keyname>Barbotin</keyname><forenames>Yann</forenames></author><author><keyname>Hormati</keyname><forenames>Ali</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Estimation of Sparse MIMO Channels with Common Support</title><categories>cs.NI</categories><comments>12 pages / 7 figures. Submitted to IEEE Transactions on Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating sparse communication channels in the
MIMO context. In small to medium bandwidth communications, as in the current
standards for OFDM and CDMA communication systems (with bandwidth up to 20
MHz), such channels are individually sparse and at the same time share a common
support set. Since the underlying physical channels are inherently
continuous-time, we propose a parametric sparse estimation technique based on
finite rate of innovation (FRI) principles. Parametric estimation is especially
relevant to MIMO communications as it allows for a robust estimation and
concise description of the channels. The core of the algorithm is a
generalization of conventional spectral estimation methods to multiple input
signals with common support. We show the application of our technique for
channel estimation in OFDM (uniformly/contiguous DFT pilots) and CDMA downlink
(Walsh-Hadamard coded schemes). In the presence of additive white Gaussian
noise, theoretical lower bounds on the estimation of SCS channel parameters in
Rayleigh fading conditions are derived. Finally, an analytical spatial channel
model is derived, and simulations on this model in the OFDM setting show the
symbol error rate (SER) is reduced by a factor 2 (0 dB of SNR) to 5 (high SNR)
compared to standard non-parametric methods - e.g. lowpass interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1345</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1345</id><created>2011-07-07</created><authors><author><keyname>Jiang</keyname><forenames>Xianhua</forenames></author><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author></authors><title>Distances and Riemannian metrics for multivariate spectral densities</title><categories>math.OC cs.SY math.ST stat.TH</categories><comments>21 pages, 8 figures</comments><msc-class>93E11</msc-class><journal-ref>Automatic Control, IEEE Transactions on 57.7 (2012): 1723-1735</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first introduce a class of divergence measures between power spectral
density matrices. These are derived by comparing the suitability of different
models in the context of optimal prediction. Distances between &quot;infinitesimally
close&quot; power spectra are quadratic, and hence, they induce a
differential-geometric structure. We study the corresponding Riemannian metrics
and, for a particular case, provide explicit formulae for the corresponding
geodesics and geodesic distances. The close connection between the geometry of
power spectra and the geometry of the Fisher-Rao metric is noted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1347</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1347</id><created>2011-07-07</created><updated>2011-08-16</updated><authors><author><keyname>Xu</keyname><forenames>Shen Chen</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Sequential, successive, and simultaneous decoders for
  entanglement-assisted classical communication</title><categories>quant-ph cs.IT math.IT</categories><comments>33 pages, 2 figures; v2 contains a proof of the quantum simultaneous
  decoding conjecture for two-sender quantum multiple access channels; v3 shows
  how to recover the known unassisted and assisted quantum communication
  regions with a coherent quantum simultaneous decoder</comments><journal-ref>Quantum Information Processing, vol. 12, no. 1, pp. 641-683
  (January 2013)</journal-ref><doi>10.1007/s11128-012-0410-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bennett et al. showed that allowing shared entanglement between a sender and
receiver before communication begins dramatically simplifies the theory of
quantum channels, and these results suggest that it would be worthwhile to
study other scenarios for entanglement-assisted classical communication. In
this vein, the present paper makes several contributions to the theory of
entanglement-assisted classical communication. First, we rephrase the
Giovannetti-Lloyd-Maccone sequential decoding argument as a more general
&quot;packing lemma&quot; and show that it gives an alternate way of achieving the
entanglement-assisted classical capacity. Next, we show that a similar
sequential decoder can achieve the Hsieh-Devetak-Winter region for
entanglement-assisted classical communication over a multiple access channel.
Third, we prove the existence of a quantum simultaneous decoder for
entanglement-assisted classical communication over a multiple access channel
with two senders. This result implies a solution of the quantum simultaneous
decoding conjecture for unassisted classical communication over quantum
multiple access channels with two senders, but the three-sender case still
remains open (Sen recently and independently solved this unassisted two-sender
case with a different technique). We then leverage this result to recover the
known regions for unassisted and assisted quantum communication over a quantum
multiple access channel, though our proof exploits a coherent quantum
simultaneous decoder. Finally, we determine an achievable rate region for
communication over an entanglement-assisted bosonic multiple access channel and
compare it with the Yen-Shapiro outer bound for unassisted communication over
the same channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1351</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1351</id><created>2011-07-07</created><updated>2011-09-01</updated><authors><author><keyname>Honsell</keyname><forenames>Furio</forenames><affiliation>Dipartimento di Matematica e Informatica</affiliation></author><author><keyname>Lenisa</keyname><forenames>Marina</forenames><affiliation>Dipartimento di Matematica e Informatica</affiliation></author></authors><title>Conway games, algebraically and coalgebraically</title><categories>cs.LO</categories><comments>30 pages</comments><proxy>LMCS</proxy><acm-class>F.3.2, F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  1, 2011) lmcs:703</journal-ref><doi>10.2168/LMCS-7(3:8)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using coalgebraic methods, we extend Conway's theory of games to possibly
non-terminating, i.e. non-wellfounded games (hypergames). We take the view that
a play which goes on forever is a draw, and hence rather than focussing on
winning strategies, we focus on non-losing strategies. Hypergames are a
fruitful metaphor for non-terminating processes, Conway's sum being similar to
shuffling. We develop a theory of hypergames, which extends in a non-trivial
way Conway's theory; in particular, we generalize Conway's results on game
determinacy and characterization of strategies. Hypergames have a rather
interesting theory, already in the case of impartial hypergames, for which we
give a compositional semantics, in terms of a generalized Grundy-Sprague
function and a system of generalized Nim games. Equivalences and congruences on
games and hypergames are discussed. We indicate a number of intriguing
directions for future work. We briefly compare hypergames with other notions of
games used in computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1358</identifier>
 <datestamp>2012-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1358</id><created>2011-07-07</created><updated>2012-02-02</updated><authors><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Schwartz</keyname><forenames>Roy</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>On the Furthest Hyperplane Problem and Maximal Margin Clustering</title><categories>cs.CC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Furthest Hyperplane Problem (FHP), which is an
unsupervised counterpart of Support Vector Machines. Given a set of n points in
Rd, the objective is to produce the hyperplane (passing through the origin)
which maximizes the separation margin, that is, the minimal distance between
the hyperplane and any input point. To the best of our knowledge, this is the
first paper achieving provable results regarding FHP. We provide both lower and
upper bounds to this NP-hard problem. First, we give a simple randomized
algorithm whose running time is n^O(1/{\theta}^2) where {\theta} is the optimal
separation margin. We show that its exponential dependency on 1/{\theta}^2 is
tight, up to sub-polynomial factors, assuming SAT cannot be solved in
sub-exponential time. Next, we give an efficient approxima- tion algorithm. For
any {\alpha} \in [0, 1], the algorithm produces a hyperplane whose distance
from at least 1 - 5{\alpha} fraction of the points is at least {\alpha} times
the optimal separation margin. Finally, we show that FHP does not admit a PTAS
by presenting a gap preserving reduction from a particular version of the PCP
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1359</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1359</id><created>2011-07-07</created><authors><author><keyname>Catusse</keyname><forenames>Nicolas</forenames></author><author><keyname>Chepoi</keyname><forenames>Victor</forenames></author><author><keyname>Nouioua</keyname><forenames>Karim</forenames></author><author><keyname>Vaxes</keyname><forenames>Yann</forenames></author></authors><title>Bidirected minimum Manhattan network problem</title><categories>cs.CG</categories><comments>14 pages, 16 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the bidirected minimum Manhattan network problem, given a set T of n
terminals in the plane, we need to construct a network N(T) of minimum total
length with the property that the edges of N(T) are axis-parallel and oriented
in a such a way that every ordered pair of terminals is connected in N(T) by a
directed Manhattan path. In this paper, we present a polynomial factor 2
approximation algorithm for the bidirected minimum Manhattan network problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1382</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1382</id><created>2011-07-07</created><updated>2011-07-07</updated><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Misha</forenames></author></authors><title>Operations-Based Planning for Placement and Sizing of Energy Storage in
  a Grid With a High Penetration of Renewables</title><categories>math.OC cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the penetration level of transmission-scale time-intermittent renewable
generation resources increases, control of flexible resources will become
important to mitigating the fluctuations due to these new renewable resources.
Flexible resources may include new or existing synchronous generators as well
as new energy storage devices. The addition of energy storage, if needed,
should be done optimally to minimize the integration cost of renewable
resources, however, optimal placement and sizing of energy storage is a
difficult optimization problem. The fidelity of such results may be
questionable because optimal planning procedures typically do not consider the
effect of the time dynamics of operations and controls. Here, we use an optimal
energy storage control algorithm to develop a heuristic procedure for energy
storage placement and sizing. We generate many instances of intermittent
generation time profiles and allow the control algorithm access to unlimited
amounts of storage, both energy and power, at all nodes. Based on the activity
of the storage at each node, we restrict the number of storage node in a staged
procedure seeking the minimum number of storage nodes and total network storage
that can still mitigate the effects of renewable fluctuations on network
constraints. The quality of the heuristic is explored by comparing our results
to seemingly &quot;intuitive&quot; placements of storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1383</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1383</id><created>2011-07-06</created><updated>2011-10-07</updated><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Bensalem</keyname><forenames>Saddek</forenames></author><author><keyname>Chen</keyname><forenames>Yu-Fang</forenames></author><author><keyname>Yan</keyname><forenames>Rongjie</forenames></author><author><keyname>Jobstmann</keyname><forenames>Barbara</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author><author><keyname>Buckl</keyname><forenames>Christian</forenames></author><author><keyname>Knoll</keyname><forenames>Alois</forenames></author></authors><title>Algorithms for Synthesizing Priorities in Component-based Systems</title><categories>cs.LO cs.SY</categories><comments>Full version of the ATVA'11 paper (compared to the 1st arXiv version,
  we add one additional sentence to avoid confusion)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms to synthesize component-based systems that are safe and
deadlock-free using priorities, which define stateless-precedence between
enabled actions. Our core method combines the concept of fault-localization
(using safety-game) and fault-repair (using SAT for conflict resolution). For
complex systems, we propose three complementary methods as preprocessing steps
for priority synthesis, namely (a) data abstraction to reduce component
complexities, (b) alphabet abstraction and #-deadlock to ignore components, and
(c) automated assumption learning for compositional priority synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1398</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1398</id><created>2011-07-07</created><authors><author><keyname>Obdrzalek</keyname><forenames>Jan</forenames></author><author><keyname>Trtik</keyname><forenames>Marek</forenames></author></authors><title>Efficient Loop Navigation for Symbolic Execution</title><categories>cs.PL</categories><comments>This is the full version of the extended abstract to appear in ATVA
  2011</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic execution is a successful and very popular technique used in
software verification and testing. A key limitation of symbolic execution is in
dealing with code containing loops. The problem is that even a single loop can
generate a huge number of different symbolic execution paths, corresponding to
different number of loop iterations and taking various paths through the loop.
  We introduce a technique which, given a start location above some loops and a
target location anywhere below these loops, returns a feasible path between
these two locations, if such a path exists. The technique infers a collection
of constraint systems from the program and uses them to steer the symbolic
execution towards the target. On reaching a loop it iteratively solves the
appropriate constraint system to find out which path through this loop to take,
or, alternatively, whether to continue below the loop. To construct the
constraint systems we express the values of variables modified in a loop as
functions of the number of times a given path through the loop was executed.
  We have built a prototype implementation of our technique and compared it to
state-of-the-art symbolic execution tools on simple programs with loops. The
results show significant improvements in the running time. We found instances
where our algorithm finished in seconds, whereas the other tools did not finish
within an hour. Our approach also shows very good results in the case when the
target location is not reachable by any feasible path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1401</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1401</id><created>2011-07-07</created><updated>2012-11-24</updated><authors><author><keyname>Wild</keyname><forenames>Marcel</forenames></author><author><keyname>Janson</keyname><forenames>Svante</forenames></author><author><keyname>Wagner</keyname><forenames>Stephan</forenames></author><author><keyname>Laurie</keyname><forenames>Dirk</forenames></author></authors><title>Coupons collecting with or without replacement, and with multipurpose
  coupons</title><categories>math.PR cs.DM</categories><msc-class>60-04, 60Gxx, 60Cxx</msc-class><journal-ref>Discrete Mathematics and Theoretical Computer Science 15 (2013)
  259-270</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic Coupon-Collector Problem (CCP) is generalized to the extent that
each coupons serves certain &quot;purposes&quot;. Only basic probability theory is used.
Centerpiece rather is an algorithm that efficiently counts all $k$-element
transversals of a set system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1409</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1409</id><created>2011-07-07</created><updated>2012-06-19</updated><authors><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Fluctuations of spiked random matrix models and failure diagnosis in
  sensor networks</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the joint fluctuations of the extreme eigenvalues and
eigenvectors of a large dimensional sample covariance matrix are analyzed when
the associated population covariance matrix is a finite-rank perturbation of
the identity matrix, corresponding to the so-called spiked model in random
matrix theory. The asymptotic fluctuations, as the matrix size grows large, are
shown to be intimately linked with matrices from the Gaussian unitary ensemble
(GUE). When the spiked population eigenvalues have unit multiplicity, the
fluctuations follow a central limit theorem. This result is used to develop an
original framework for the detection and diagnosis of local failures in large
sensor networks, for known or unknown failure magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1434</identifier>
 <datestamp>2012-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1434</id><created>2011-07-07</created><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames></author><author><keyname>Portier</keyname><forenames>Natacha</forenames></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames></author></authors><title>The Limited Power of Powering: Polynomial Identity Testing and a
  Depth-four Lower Bound for the Permanent</title><categories>cs.CC</categories><comments>16 pages</comments><journal-ref>IARCS Annual Conference on Foundations of Software Technology and
  Theoretical Computer Science (FSTTCS'11), Mumbai : India (2011)</journal-ref><doi>10.4230/LIPIcs.FSTTCS.2011.127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial identity testing and arithmetic circuit lower bounds are two
central questions in algebraic complexity theory. It is an intriguing fact that
these questions are actually related. One of the authors of the present paper
has recently proposed a &quot;real {\tau}-conjecture&quot; which is inspired by this
connection. The real {\tau}-conjecture states that the number of real roots of
a sum of products of sparse univariate polynomials should be polynomially
bounded. It implies a superpolynomial lower bound on the size of arithmetic
circuits computing the permanent polynomial. In this paper we show that the
real {\tau}-conjecture holds true for a restricted class of sums of products of
sparse polynomials. This result yields lower bounds for a restricted class of
depth-4 circuits: we show that polynomial size circuits from this class cannot
compute the permanent, and we also give a deterministic polynomial identity
testing algorithm for the same class of circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1437</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1437</id><created>2011-07-07</created><updated>2011-07-14</updated><authors><author><keyname>Formato</keyname><forenames>Richard A.</forenames></author></authors><title>A novel methodology for antenna design and optimization: Variable Zo</title><categories>cs.OH</categories><comments>Ver. 2 (14 July 2011). Adds Yagi-Uda array design example. Updates
  source code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes &quot;Variable Zo,&quot; a novel and proprietary approach to
antenna design and optimization. The new methodology is illustrated by applying
it to the design of a resistively-loaded bowtie antenna and to two broadband
Yagi-Uda arrays. Variable Zo is applicable to any antenna design or
optimization methodology. Using it will result in generally better antenna
designs across any user-specified set of performance objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1445</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1445</id><created>2011-07-07</created><authors><author><keyname>Terejanu</keyname><forenames>Gabriel</forenames></author><author><keyname>Upadhyay</keyname><forenames>Rochan R.</forenames></author><author><keyname>Miki</keyname><forenames>Kenji</forenames></author></authors><title>Bayesian experimental design for the active nitridation of graphite by
  atomic nitrogen</title><categories>physics.data-an cs.IT math.IT stat.AP</categories><comments>Preprint submitted to Experimental Thermal and Fluid Science,
  February 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimal data collection to efficiently learn the model
parameters of a graphite nitridation experiment is studied in the context of
Bayesian analysis using both synthetic and real experimental data. The paper
emphasizes that the optimal design can be obtained as a result of an
information theoretic sensitivity analysis. Thus, the preferred design is where
the statistical dependence between the model parameters and observables is the
highest possible. In this paper, the statistical dependence between random
variables is quantified by mutual information and estimated using a k-nearest
neighbor based approximation. It is shown, that by monitoring the inference
process via measures such as entropy or Kullback-Leibler divergence, one can
determine when to stop the data collection process. The methodology is applied
to select the most informative designs on both a simulated data set and on an
experimental data set, previously published in the literature. It is also shown
that the sequential Bayesian analysis used in the experimental design can also
be useful in detecting conflicting information between measurements and model
predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1456</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1456</id><created>2011-07-07</created><updated>2011-09-01</updated><authors><author><keyname>Hernich</keyname><forenames>Andre</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author></authors><title>Answering Non-Monotonic Queries in Relational Data Exchange</title><categories>cs.DB cs.LO</categories><comments>55 pages, 3 figures</comments><proxy>LMCS</proxy><acm-class>cs.LO</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  1, 2011) lmcs:904</journal-ref><doi>10.2168/LMCS-7(3:9)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational data exchange is the problem of translating relational data from a
source schema into a target schema, according to a specification of the
relationship between the source data and the target data. One of the basic
issues is how to answer queries that are posed against target data. While
consensus has been reached on the definitive semantics for monotonic queries,
this issue turned out to be considerably more difficult for non-monotonic
queries. Several semantics for non-monotonic queries have been proposed in the
past few years. This article proposes a new semantics for non-monotonic
queries, called the GCWA*-semantics. It is inspired by semantics from the area
of deductive databases. We show that the GCWA*-semantics coincides with the
standard open world semantics on monotonic queries, and we further explore the
(data) complexity of evaluating non-monotonic queries under the
GCWA*-semantics. In particular, we introduce a class of schema mappings for
which universal queries can be evaluated under the GCWA*-semantics in
polynomial time (data complexity) on the core of the universal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1458</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1458</id><created>2011-07-07</created><updated>2014-03-20</updated><authors><author><keyname>Epstein</keyname><forenames>Samuel</forenames></author><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Sets Have Simple Members</title><categories>cs.CC</categories><comments>This paper has been withdrawn by the authors. Withdrawn (v8) by S.
  Epstein. Reposted as arXiv:1403.4539 by the coauthor</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combined Universal Probability M(D) of strings x in sets D is close to
max M({x}) over x in D: their ~logs differ by at most D's information j=I(D:H)
about the halting sequence H. Thus if all x have complexity K(x) &gt;k, D carries
&gt;i bits of information on each its x where i+j ~ k. Note that there are no ways
to generate D with significant I(D:H).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1467</identifier>
 <datestamp>2012-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1467</id><created>2011-07-07</created><updated>2012-07-05</updated><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Geometry of Injection Regions of Power Networks</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>To appear in IEEE Transaction on Power Systems. Short version
  appeared in Allerton 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the constraints on power flow in networks and its implications
to the optimal power flow problem. The constraints are described by the
injection region of a network; this is the set of all vectors of power
injections, one at each bus, that can be achieved while satisfying the network
and operation constraints. If there are no operation constraints, we show the
injection region of a network is the set of all injections satisfying the
conservation of energy. If the network has a tree topology, e.g., a
distribution network, we show that under voltage magnitude, line loss
constraints, line flow constraints and certain bus real and reactive power
constraints, the injection region and its convex hull have the same
Pareto-front. The Pareto-front is of interest since these are the the optimal
solutions to the minimization of increasing functions over the injection
region. For non-tree networks, we obtain a weaker result by characterize the
convex hull of the voltage constraint injection region for lossless cycles and
certain combinations of cycles and trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1470</identifier>
 <datestamp>2011-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1470</id><created>2011-07-07</created><updated>2011-08-11</updated><authors><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Lerner</keyname><forenames>Ronen</forenames></author><author><keyname>Rivlin</keyname><forenames>Ehud</forenames></author><author><keyname>Rotstein</keyname><forenames>Hector</forenames></author></authors><title>Vision-Based Navigation II: Error Analysis for a Navigation Algorithm
  based on Optical-Flow and a Digital Terrain Map</title><categories>cs.CV cs.AI</categories><comments>10 pages,12 figures, 2 tables</comments><msc-class>68T45</msc-class><acm-class>E.5; E.4; E.2; H.1.1; F.1.1; F.1.3</acm-class><journal-ref>Proceedings of the 2008 IEEE/ION Position, Location and Navigation
  Symposium, P.1203-1212</journal-ref><doi>10.1109/PLANS.2008.4570040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the error analysis of a navigation algorithm that uses
as input a sequence of images acquired by a moving camera and a Digital Terrain
Map (DTM) of the region been imaged by the camera during the motion. The main
sources of error are more or less straightforward to identify: camera
resolution, structure of the observed terrain and DTM accuracy, field of view
and camera trajectory. After characterizing and modeling these error sources in
the framework of the CDTM algorithm, a closed form expression for their effect
on the pose and motion errors of the camera can be found. The analytic
expression provides a priori measurements for the accuracy in terms of the
parameters mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1520</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1520</id><created>2011-07-07</created><updated>2012-08-01</updated><authors><author><keyname>Azrieli</keyname><forenames>Yaron</forenames></author><author><keyname>Shmaya</keyname><forenames>Eran</forenames></author></authors><title>Lipschitz Games</title><categories>math.CO cs.GT</categories><comments>minor changes, forthcoming in Mathematics of Operations Research</comments><msc-class>91A10, 05D40</msc-class><journal-ref>Mathematics of Operations Research May 2013 vol. 38</journal-ref><doi>10.1287/moor.1120.0557</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lipschitz constant of a finite normal-form game is the maximal change in
some player's payoff when a single opponent changes his strategy. We prove that
games with small Lipschitz constant admit pure {\epsilon}-equilibria, and
pinpoint the maximal Lipschitz constant that is sufficient to imply existence
of pure {\epsilon}-equilibrium as a function of the number of players in the
game and the number of strategies of each player. Our proofs use the
probabilistic method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1525</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1525</id><created>2011-06-21</created><authors><author><keyname>Cloud</keyname><forenames>R. L.</forenames></author><author><keyname>Curry</keyname><forenames>M. L.</forenames></author><author><keyname>Ward</keyname><forenames>H. L.</forenames></author><author><keyname>Skjellum</keyname><forenames>A.</forenames></author><author><keyname>Bangalore</keyname><forenames>P.</forenames></author></authors><title>Accelerating Lossless Data Compression with GPUs</title><categories>cs.IT cs.GR cs.PF math.IT</categories><comments>peer reviewed and published in undergraduate research journal Inquiro
  in 2009 after Summer work in 2009</comments><journal-ref>Inquiro, Volume 3, 2009, p. 26 - 29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Huffman compression is a statistical, lossless, data compression algorithm
that compresses data by assigning variable length codes to symbols, with the
more frequently appearing symbols given shorter codes than the less. This work
is a modification of the Huffman algorithm which permits uncompressed data to
be decomposed into indepen- dently compressible and decompressible blocks,
allowing for concurrent compression and decompression on multiple processors.
We create implementations of this modified algorithm on a current NVIDIA GPU
using the CUDA API as well as on a current Intel chip and the performance
results are compared, showing favorable GPU performance for nearly all tests.
Lastly, we discuss the necessity for high performance data compression in
today's supercomputing ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1529</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1529</id><created>2011-07-07</created><updated>2013-04-12</updated><authors><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author></authors><title>Decoding of Matrix-Product Codes</title><categories>cs.IT math.IT</categories><msc-class>94B05, 94B35</msc-class><journal-ref>Journal of Algebra and its Applications, Volume 12, Issue 4,
  Article ID 1250185, 15 pages (2013)</journal-ref><doi>10.1142/S021949881250185X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decoding algorithm for the $(u\mid u+v)$-construction that
decodes up to half of the minimum distance of the linear code. We extend this
algorithm for a class of matrix-product codes in two different ways. In some
cases, one can decode beyond the error correction capability of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1535</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1535</id><created>2011-07-07</created><updated>2012-06-01</updated><authors><author><keyname>Sahebi</keyname><forenames>Aria G.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Multilevel Polarization of Polar Codes Over Arbitrary Discrete
  Memoryless Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that polar codes achieve the symmetric capacity of discrete
memoryless channels with arbitrary input alphabet sizes. It is shown that in
general, channel polarization happens in several, rather than only two levels
so that the synthesized channels are either useless, perfect or &quot;partially
perfect&quot;. Any subset of the channel input alphabet which is closed under
addition, induces a coset partition of the alphabet through its shifts. For any
such partition of the input alphabet, there exists a corresponding partially
perfect channel whose outputs uniquely determine the coset to which the channel
input belongs. By a slight modification of the encoding and decoding rules, it
is shown that perfect transmission of certain information symbols over
partially perfect channels is possible. Our result is general regarding both
the cardinality and the algebraic structure of the channel input alphabet; i.e
we show that for any channel input alphabet size and any Abelian group
structure on the alphabet, polar codes are optimal. It is also shown through an
example that polar codes when considered as group/coset codes, do not achieve
the capacity achievable using coset codes over arbitrary channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1536</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1536</id><created>2011-07-07</created><authors><author><keyname>Eschenfeldt</keyname><forenames>Patrick</forenames></author><author><keyname>Gross</keyname><forenames>Ben</forenames></author><author><keyname>Pippenger</keyname><forenames>Nicholas</forenames></author></authors><title>The M/M/Infinity Service System with Ranked Servers in Heavy Traffic</title><categories>math.PR cs.PF</categories><comments>i+6 pp</comments><msc-class>60K26, 90B22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an M/M/Infinity service system in which an arriving customer is
served by the first idle server in an infinite sequence S_1, S_2, ... of
servers. We determine the first two terms in the asymptotic expansions of the
moments of L as lambda tends to infinity, where L is the index of the server
S_L serving a newly arriving customer in equilibrium, and lambda is the ratio
of the arrival rate to the service rate. The leading terms of the moments show
that L/lambda tends to a uniform distribution on [0,1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1544</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1544</id><created>2011-07-07</created><authors><author><keyname>Huang</keyname><forenames>Jing</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Cooperative Jamming for Secure Communications in MIMO Relay Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, to appear in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2011.2161295</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure communications can be impeded by eavesdroppers in conventional relay
systems. This paper proposes cooperative jamming strategies for two-hop relay
networks where the eavesdropper can wiretap the relay channels in both hops. In
these approaches, the normally inactive nodes in the relay network can be used
as cooperative jamming sources to confuse the eavesdropper. Linear precoding
schemes are investigated for two scenarios where single or multiple data
streams are transmitted via a decode-and-forward (DF) relay, under the
assumption that global channel state information (CSI) is available. For the
case of single data stream transmission, we derive closed-form jamming
beamformers and the corresponding optimal power allocation. Generalized
singular value decomposition (GSVD)-based secure relaying schemes are proposed
for the transmission of multiple data streams. The optimal power allocation is
found for the GSVD relaying scheme via geometric programming. Based on this
result, a GSVD-based cooperative jamming scheme is proposed that shows
significant improvement in terms of secrecy rate compared to the approach
without jamming. Furthermore, the case involving an eavesdropper with unknown
CSI is also investigated in this paper. Simulation results show that the
secrecy rate is dramatically increased when inactive nodes in the relay network
participate in cooperative jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1545</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1545</id><created>2011-07-07</created><authors><author><keyname>Terejanu</keyname><forenames>Gabriel</forenames></author><author><keyname>Cheng</keyname><forenames>Yang</forenames></author><author><keyname>Singh</keyname><forenames>Tarunraj</forenames></author><author><keyname>Scott</keyname><forenames>Peter D.</forenames></author></authors><title>Comparison of SCIPUFF Plume Prediction with Particle Filter Assimilated
  Prediction for Dipole Pride 26 Data</title><categories>stat.AP cs.DC physics.data-an</categories><comments>The Chemical and Biological Defense Physical Science and Technology
  Conference, New Orleans, November 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the application of a particle filter for data
assimilation in the context of puff-based dispersion models. Particle filters
provide estimates of the higher moments, and are well suited for strongly
nonlinear and/or non-Gaussian models. The Gaussian puff model SCIPUFF, is used
in predicting the chemical concentration field after a chemical incident. This
model is highly nonlinear and evolves with variable state dimension and, after
sufficient time, high dimensionality. While the particle filter formalism
naturally supports variable state dimensionality high dimensionality represents
a challenge in selecting an adequate number of particles, especially for the
Bootstrap version. We present an implementation of the Bootstrap particle
filter and compare its performance with the SCIPUFF predictions. Both the model
and the Particle Filter are evaluated on the Dipole Pride 26 experimental data.
Since there is no available ground truth, the data has been divided in two
sets: training and testing. We show that even with a modest number of
particles, the Bootstrap particle filter provides better estimates of the
concentration field compared with the process model, without excessive increase
in computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1561</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1561</id><created>2011-07-08</created><authors><author><keyname>Siming</keyname><forenames>Wei</forenames></author><author><keyname>Zhouchen</keyname><forenames>Lin</forenames></author></authors><title>Analysis and Improvement of Low Rank Representation for Subspace
  segmentation</title><categories>cs.CV</categories><comments>Disclosed as Microsoft technical report on Auguat 25, 2010</comments><report-no>MSR-TR-2010-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze and improve low rank representation (LRR), the state-of-the-art
algorithm for subspace segmentation of data. We prove that for the noiseless
case, the optimization model of LRR has a unique solution, which is the shape
interaction matrix (SIM) of the data matrix. So in essence LRR is equivalent to
factorization methods. We also prove that the minimum value of the optimization
model of LRR is equal to the rank of the data matrix. For the noisy case, we
show that LRR can be approximated as a factorization method that combines noise
removal by column sparse robust PCA. We further propose an improved version of
LRR, called Robust Shape Interaction (RSI), which uses the corrected data as
the dictionary instead of the noisy data. RSI is more robust than LRR when the
corruption in data is heavy. Experiments on both synthetic and real data
testify to the improved robustness of RSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1563</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1563</id><created>2011-07-08</created><authors><author><keyname>Wang</keyname><forenames>Jiadong</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Xie</keyname><forenames>Bike</forenames></author><author><keyname>Wesel</keyname><forenames>Richard</forenames></author></authors><title>Designing Nonlinear Turbo Codes with a Target Ones Density</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain binary asymmetric channels, such as Z-channels in which one of the
two crossover probabilities is zero, demand optimal ones densities different
from 50%. Some broadcast channels, such as broadcast binary symmetric channels
(BBSC) where each component channel is a binary symmetric channel, also require
a non-uniform input distribution due to the superposition coding scheme, which
is known to achieve the boundary of capacity region. This paper presents a
systematic technique for designing nonlinear turbo codes that are able to
support ones densities different from 50%. To demonstrate the effectiveness of
our design technique, we design and simulate nonlinear turbo codes for the
Z-channel and the BBSC. The best nonlinear turbo code is less than 0.02 bits
from capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1564</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1564</id><created>2011-07-08</created><updated>2014-03-12</updated><authors><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>Polyceptron: A Polyhedral Learning Algorithm</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new algorithm for learning polyhedral classifiers
which we call as Polyceptron. It is a Perception like algorithm which updates
the parameters only when the current classifier misclassifies any training
data. We give both batch and online version of Polyceptron algorithm. Finally
we give experimental results to show the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1580</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1580</id><created>2011-07-08</created><authors><author><keyname>Sassi</keyname><forenames>Mohamed Amin Ben</forenames></author><author><keyname>Girard</keyname><forenames>Antoine</forenames></author></authors><title>Controller Synthesis for Robust Invariance of Polynomial Dynamical
  Systems using Linear Programming</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a control synthesis problem for a class of
polynomial dynamical systems subject to bounded disturbances and with input
constraints. More precisely, we aim at synthesizing at the same time a
controller and an invariant set for the controlled system under all admissible
disturbances. We propose a computational method to solve this problem. Given a
candidate polyhedral invariant, we show that controller synthesis can be
formulated as an optimization problem involving polynomial cost functions over
bounded polytopes for which effective linear programming relaxations can be
obtained. Then, we propose an iterative approach to compute the controller and
the polyhedral invariant at once. Each iteration of the approach mainly
consists in solving two linear programs (one for the controller and one for the
invariant) and is thus computationally tractable. Finally, we show with several
examples the usefulness of our method in applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1585</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1585</id><created>2011-07-08</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wojtaszczyk</keyname><forenames>Jakub Onufry</forenames></author></authors><title>On Multiway Cut parameterized above lower bounds</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider two above lower bound parameterizations of the Node
Multiway Cut problem - above the maximum separating cut and above a natural
LP-relaxation - and prove them to be fixed-parameter tractable. Our results
imply O*(4^k) algorithms for Vertex Cover above Maximum Matching and Almost
2-SAT as well as an O*(2^k) algorithm for Node Multiway Cut with a standard
parameterization by the solution size, improving previous bounds for these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1586</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1586</id><created>2011-07-08</created><authors><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Feng</keyname><forenames>Xu</forenames></author><author><keyname>Dong</keyname><forenames>Li</forenames></author><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Performance of Local Information Based Link Prediction: A Sampling
  Perspective</title><categories>cs.SI physics.soc-ph</categories><comments>19 pages, 5 figures</comments><journal-ref>J. Phys. A: Math. Theor. 45 (2012) 345001</journal-ref><doi>10.1088/1751-8113/45/34/345001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is pervasively employed to uncover the missing links in the
snapshots of real-world networks, which are usually obtained from kinds of
sampling methods. Contrarily, in the previous literature, in order to evaluate
the performance of the prediction, the known edges in the sampled snapshot are
divided into the training set and the probe set randomly, without considering
the diverse sampling approaches beyond. However, different sampling methods
might lead to different missing links, especially for the biased ones. For this
reason, random partition based evaluation of performance is no longer
convincing if we take the sampling method into account. Hence, in this paper,
aim at filling this void, we try to reevaluate the performance of local
information based link predictions through sampling methods governed division
of the training set and the probe set. It is interesting that we find for
different sampling methods, each prediction approach performs unevenly.
Moreover, most of these predictions perform weakly when the sampling method is
biased, which indicates that the performance of these methods is overestimated
in the prior works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1600</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1600</id><created>2011-07-08</created><updated>2011-10-31</updated><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>On fuzzy syndrome hashing with LDPC coding</title><categories>cs.IT cs.CR math.IT</categories><comments>in Proceedings 4th International Symposium on Applied Sciences in
  Biomedical and Communication Technologies (ISABEL), ACM 2011. This is the
  author's version of the work. It is posted here by permission of ACM for your
  personal use. Not for redistribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last decades have seen a growing interest in hash functions that allow
some sort of tolerance, e.g. for the purpose of biometric authentication. Among
these, the syndrome fuzzy hashing construction allows to securely store
biometric data and to perform user authentication without the need of sharing
any secret key. This paper analyzes this model, showing that it offers a
suitable protection against information leakage and several advantages with
respect to similar solutions, such as the fuzzy commitment scheme. Furthermore,
the design and characterization of LDPC codes to be used for this purpose is
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1608</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1608</id><created>2011-07-08</created><authors><author><keyname>Navarro-Barrientos</keyname><forenames>J. -Emeterio</forenames></author></authors><title>Formation of Common Investment Networks by Project Establishment between
  Agents</title><categories>cs.SI cs.CE</categories><journal-ref>SBP 2011, LNCS 6589, Springer (2011) pp.172-179</journal-ref><doi>10.1007/978-3-642-19656-0_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an investment model integrated with trust-reputation mechanisms
where agents interact with each other to establish investment projects. We
investigate the establishment of investment projects, the influence of the
interaction between agents in the evolution of the distribution of wealth, as
well as the formation of common investment networks and some of their
properties. Simulation results show that the wealth distribution presents a
power law in its tail. Also, it is shown that the trust and reputation
mechanism presented leads to the establishment of networks among agents, which
present some of the typical characteristics of real-life networks like a high
clustering coefficient and short average path length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1609</identifier>
 <datestamp>2011-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1609</id><created>2011-07-08</created><updated>2011-07-15</updated><authors><author><keyname>Mimura</keyname><forenames>Kazushi</forenames></author></authors><title>Linear Complexity Lossy Compressor for Binary Redundant Memoryless
  Sources</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>4 pages, 1 figure</comments><journal-ref>J. Phys. Soc. Jpn., 80, 9 093801 (2011)</journal-ref><doi>10.1143/JPSJ.80.093801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lossy compression algorithm for binary redundant memoryless sources is
presented. The proposed scheme is based on sparse graph codes. By introducing a
nonlinear function, redundant memoryless sequences can be compressed. We
propose a linear complexity compressor based on the extended belief
propagation, into which an inertia term is heuristically introduced, and show
that it has near-optimal performance for moderate block lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1626</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1626</id><created>2011-07-08</created><authors><author><keyname>Chatzigiannakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Pyrgelis</keyname><forenames>Apostolos</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author><author><keyname>Stamatiou</keyname><forenames>Yannis C.</forenames></author></authors><title>Elliptic Curve Based Zero Knowledge Proofs and Their Applicability on
  Resource Constrained Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elliptic Curve Cryptography (ECC) is an attractive alternative to
conventional public key cryptography, such as RSA. ECC is an ideal candidate
for implementation on constrained devices where the major computational
resources i.e. speed, memory are limited and low-power wireless communication
protocols are employed. That is because it attains the same security levels
with traditional cryptosystems using smaller parameter sizes. Moreover, in
several application areas such as person identification and eVoting, it is
frequently required of entities to prove knowledge of some fact without
revealing this knowledge. Such proofs of knowledge are called Zero Knowledge
Interactive Proofs (ZKIP) and involve interactions between two communicating
parties, the Prover and the Verifier. In a ZKIP, the Prover demonstrates the
possesion of some information (e.g. authentication information) to the Verifier
without disclosing it. In this paper, we focus on the application of ZKIP
protocols on resource constrained devices. We study well-established ZKIP
protocols based on the discrete logarithm problem and we transform them under
the ECC setting. Then, we implement the proposed protocols on Wiselib, a
generic and open source algorithmic library. Finally, we present a thorough
evaluation of the protocols on two popular hardware platforms equipped with low
end microcontrollers (Jennic JN5139, TI MSP430) and 802.15.4 RF transceivers,
in terms of code size, execution time, message size and energy requirements. To
the best of our knowledge, this is the first attempt of implementing and
evaluating ZKIP protocols with emphasis on low-end devices. This work's results
can be used from developers who wish to achieve certain levels of security and
privacy in their applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1627</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1627</id><created>2011-07-08</created><authors><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>On Codes for Optimal Rebuilding Access</title><categories>cs.IT cs.DC math.IT</categories><comments>Submitted to Allerton 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDS (maximum distance separable) array codes are widely used in storage
systems due to their computationally efficient encoding and decoding
procedures. An MDS code with r redundancy nodes can correct any r erasures by
accessing (reading) all the remaining information in both the systematic nodes
and the parity (redundancy) nodes. However, in practice, a single erasure is
the most likely failure event; hence, a natural question is how much
information do we need to access in order to rebuild a single storage node? We
define the rebuilding ratio as the fraction of remaining information accessed
during the rebuilding of a single erasure. In our previous work we showed that
the optimal rebuilding ratio of 1/r is achievable (using our newly constructed
array codes) for the rebuilding of any systematic node, however, all the
information needs to be accessed for the rebuilding of the parity nodes.
Namely, constructing array codes with a rebuilding ratio of 1/r was left as an
open problem. In this paper, we solve this open problem and present array codes
that achieve the lower bound of 1/r for rebuilding any single systematic or
parity node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1628</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1628</id><created>2011-07-08</created><authors><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author><author><keyname>Williamson</keyname><forenames>David P.</forenames></author><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author></authors><title>A Proof of the Boyd-Carr Conjecture</title><categories>cs.DS</categories><msc-class>90C27, 90C59, 90C05</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the precise integrality gap for the subtour LP relaxation of the
traveling salesman problem is a significant open question, with little progress
made in thirty years in the general case of symmetric costs that obey triangle
inequality. Boyd and Carr [3] observe that we do not even know the worst-case
upper bound on the ratio of the optimal 2-matching to the subtour LP; they
conjecture the ratio is at most 10/9. In this paper, we prove the Boyd-Carr
conjecture. In the case that a fractional 2-matching has no cut edge, we can
further prove that an optimal 2-matching is at most 10/9 times the cost of the
fractional 2-matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1630</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1630</id><created>2011-07-08</created><updated>2014-02-24</updated><authors><author><keyname>Qian</keyname><forenames>Jiawei</forenames></author><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author><author><keyname>Williamson</keyname><forenames>David P.</forenames></author><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author></authors><title>On the Integrality Gap of the Subtour LP for the 1,2-TSP</title><categories>cs.DS</categories><comments>Changes wrt previous version: upper bound on integrality gap improved
  to 5/4 (using the same techniques as in the previous version)</comments><msc-class>90C27, 90C59, 90C05</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the integrality gap of the subtour LP relaxation for
the traveling salesman problem in the special case when all edge costs are
either 1 or 2. For the general case of symmetric costs that obey triangle
inequality, a famous conjecture is that the integrality gap is 4/3. Little
progress towards resolving this conjecture has been made in thirty years. We
conjecture that when all edge costs $c_{ij}\in \{1,2\}$, the integrality gap is
$10/9$. We show that this conjecture is true when the optimal subtour LP
solution has a certain structure. Under a weaker assumption, which is an analog
of a recent conjecture by Schalekamp, Williamson and van Zuylen, we show that
the integrality gap is at most $7/6$. When we do not make any assumptions on
the structure of the optimal subtour LP solution, we can show that integrality
gap is at most $5/4$; this is the first bound on the integrality gap of the
subtour LP strictly less than $4/3$ known for an interesting special case of
the TSP. We show computationally that the integrality gap is at most $10/9$ for
all instances with at most 12 cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1633</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1633</id><created>2011-07-08</created><authors><author><keyname>Kai</keyname><forenames>Cai Hong</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Throughput Computation in CSMA Wireless Networks with Collision Effects</title><categories>cs.NI</categories><comments>12 single-column pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that link throughputs of CSMA wireless networks can be computed
from a time-reversible Markov chain arising from an ideal CSMA network model
(ICN). In particular, this model yields general closed-form equations of link
throughputs. However, an idealized and important assumption made in ICN is that
the backoff countdown process is in &quot;contiuous-time&quot; and carrier sensing is
instantaneous. As a result, there is no collision in ICN. In practical CSMA
protocols such as IEEE 802.11, the stations count down in &quot;mini-timeslot&quot; and
the process is therefore a &quot;discrete-time&quot; process. In particular, two stations
may end their backoff process in the same mini-timeslot and then transmit
simultaneously, resulting in a packet collision. This paper is an attempt to
study how to compute link throughputs after taking such backoff collision
effects into account. We propose a generalized ideal CSMA network model (GICN)
to characterize the collision states as well as the interactions and dependency
among links in the network. We show that link throughputs and collision
probability can be computed from GICN. Simulation results validate GICN's
accuracy. Interestingly, we also find that the original ICN model yields fairly
accurate results despite the fact that collisions are not modeled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1638</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1638</id><created>2011-07-08</created><authors><author><keyname>Ga&#xef;ffas</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Lecu&#xe9;</keyname><forenames>Guillaume</forenames></author></authors><title>Weighted algorithms for compressed sensing and matrix completion</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about iteratively reweighted basis-pursuit algorithms for
compressed sensing and matrix completion problems. In a first part, we give a
theoretical explanation of the fact that reweighted basis pursuit can improve a
lot upon basis pursuit for exact recovery in compressed sensing. We exhibit a
condition that links the accuracy of the weights to the RIP and incoherency
constants, which ensures exact recovery. In a second part, we introduce a new
algorithm for matrix completion, based on the idea of iterative reweighting.
Since a weighted nuclear &quot;norm&quot; is typically non-convex, it cannot be used
easily as an objective function. So, we define a new estimator based on a
fixed-point equation. We give empirical evidences of the fact that this new
algorithm leads to strong improvements over nuclear norm minimization on
simulated and real matrix completion problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1640</identifier>
 <datestamp>2012-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1640</id><created>2011-07-08</created><updated>2012-01-16</updated><authors><author><keyname>Asyhari</keyname><forenames>A. Taufiq</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Nearest Neighbour Decoding with Pilot-Assisted Channel Estimation for
  Fading Multiple-Access Channels</title><categories>cs.IT math.IT</categories><comments>8 pages. Presented at the Forty-Ninth Annual Allerton Conference on
  Communication, Control and Computing, Allerton Retreat Center, Monticello,
  IL, September 28-30, 2011. Corrected some minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a noncoherent multiple-input multiple-output (MIMO) fading
multiple-access channel (MAC), where the transmitters and the receiver are
aware of the statistics of the fading, but not of its realisation. We analyse
the rate region that is achievable with nearest neighbour decoding and
pilot-assisted channel estimation and determine the corresponding pre-log
region, which is defined as the limiting ratio of the rate region to the
logarithm of the SNR as the SNR tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1642</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1642</id><created>2011-07-08</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Indirect Channel Sensing for Cognitive Amplify-and-Forward Relay
  Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio network the primary channel information is beneficial. But
it can not be obtained by direct channel estimation in cognitive system as
pervious methods. And only one possible way is the primary receiver broadcasts
the primary channel information to the cognitive users, but it would require
the modification of the primary receiver and additional precious spectrum
resource. Cooperative communication is also a promising technique. And this
paper introduces an indirect channel sensing method for the primary channel in
cognitive amplify-and-forward (AF) relay network. As the signal retransmitted
from the primary AF relay node includes channel effects, the cognitive radio
can receive retransmitted signal from AF node, and then extract the channel
information from them. Afterwards, Least squares channel estimation and sparse
channel estimation can be used to address the dense and sparse multipath
channels respectively. Numerical experiment demonstrates that the proposed
indirect channel sensing method has an acceptable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1644</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1644</id><created>2011-07-07</created><authors><author><keyname>Baumann</keyname><forenames>Michael</forenames><affiliation>TIMC</affiliation></author><author><keyname>Mozer</keyname><forenames>Pierre</forenames><affiliation>TIMC</affiliation></author><author><keyname>Daanen</keyname><forenames>Vincent</forenames><affiliation>TIMC</affiliation></author><author><keyname>Troccaz</keyname><forenames>Jocelyne</forenames><affiliation>TIMC</affiliation></author></authors><title>Prostate biopsy tracking with deformation estimation</title><categories>cs.CV physics.med-ph</categories><comments>Medical Image Analysis (2011) epub ahead of print</comments><proxy>ccsd</proxy><doi>10.1016/j.media.2011.01.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transrectal biopsies under 2D ultrasound (US) control are the current
clinical standard for prostate cancer diagnosis. The isoechogenic nature of
prostate carcinoma makes it necessary to sample the gland systematically,
resulting in a low sensitivity. Also, it is difficult for the clinician to
follow the sampling protocol accurately under 2D US control and the exact
anatomical location of the biopsy cores is unknown after the intervention.
Tracking systems for prostate biopsies make it possible to generate biopsy
distribution maps for intra- and post-interventional quality control and 3D
visualisation of histological results for diagnosis and treatment planning.
They can also guide the clinician toward non-ultrasound targets. In this paper,
a volume-swept 3D US based tracking system for fast and accurate estimation of
prostate tissue motion is proposed. The entirely image-based system solves the
patient motion problem with an a priori model of rectal probe kinematics.
Prostate deformations are estimated with elastic registration to maximize
accuracy. The system is robust with only 17 registration failures out of 786
(2%) biopsy volumes acquired from 47 patients during biopsy sessions. Accuracy
was evaluated to 0.76$\pm$0.52mm using manually segmented fiducials on 687
registered volumes stemming from 40 patients. A clinical protocol for assisted
biopsy acquisition was designed and implemented as a biopsy assistance system,
which allows to overcome the draw-backs of the standard biopsy procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1660</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1660</id><created>2011-07-08</created><authors><author><keyname>Balakrishnan</keyname><forenames>Raju</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Click Efficiency: A Unified Optimal Ranking for Online Ads and Documents</title><categories>cs.GT cs.IR</categories><comments>Twenty Six Pages, Two Figures, Six Theorems. Initial Version
  Published in Workshop on Web and Databases 2008</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally the probabilistic ranking principle is used to rank the search
results while the ranking based on expected profits is used for paid placement
of ads. These rankings try to maximize the expected utilities based on the user
click models. Recent empirical analysis on search engine logs suggests a
unified click models for both ranked ads and search results. The segregated
view of document and ad rankings does not consider this commonality. Further,
the used models consider parameters of (i) probability of the user abandoning
browsing results (ii) perceived relevance of result snippets. But how to
consider them for improved ranking is unknown currently. In this paper, we
propose a generalized ranking function---namely &quot;Click Efficiency (CE)&quot;---for
documents and ads based on empirically proven user click models. The ranking
considers parameters (i) and (ii) above, optimal and has the same time
complexity as sorting. To exploit its generality, we examine the reduced forms
of CE ranking under different assumptions enumerating a hierarchy of ranking
functions. Some of the rankings in the hierarchy are currently used ad and
document ranking functions; while others suggest new rankings. While optimality
of ranking is sufficient for document ranking, applying CE ranking to ad
auctions requires an appropriate pricing mechanism. We incorporate a second
price based pricing mechanism with the proposed ranking. Our analysis proves
several desirable properties including revenue dominance over VCG for the same
bid vector and existence of a Nash Equilibrium in pure strategies. The
equilibrium is socially optimal, and revenue equivalent to the truthful VCG
equilibrium. Further, we relax the independence assumption in CE ranking and
analyze the diversity ranking problem. We show that optimal diversity ranking
is NP-Hard in general, and that a constant time approximation is unlikely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1663</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1663</id><created>2011-07-08</created><authors><author><keyname>Latif</keyname><forenames>Shahid</forenames></author><author><keyname>Ullah</keyname><forenames>Rahat</forenames></author><author><keyname>Jan</keyname><forenames>Hamid</forenames></author></authors><title>A Step towards an Easy Interconversion of Various Number Systems</title><categories>cs.DM</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any system that is used for naming or representing numbers is a number
system, also known as numeral system. The modern civilization is familiar with
decimal number system using ten digits. However digital devices and computers
use binary number system instead of decimal number system, using only two
digits namely, 0 and 1 based on the fundamental concept of the decimal number
system. Various other number systems also used this fundamental concept of
decimal number system, for example octal system and hexadecimal number systems
using eight and sixteen digits respectively. The knowledge of number systems
and their inter conversion is essential for understanding of computers. More
over, successful programming for digital devices requires a precise
understanding of data formats, number systems and their inter conversion. The
inter conversion (a process in which things are each converted into the other)
of number system requires allot of time and techniques to expertise. In this
paper the interconversion of four most common number systems is taken under the
consideration in tabulated form. It is a step towards the easy interconversion
of theses number systems to understand as well as memorise it. The four number
systems are binary, octal, decimal and hexadecimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1676</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1676</id><created>2011-07-08</created><authors><author><keyname>De Martino</keyname><forenames>Monica</forenames></author><author><keyname>Albertoni</keyname><forenames>Riccardo</forenames></author></authors><title>A multilingual/multicultural semantic-based approach to improve Data
  Sharing in an SDI for Nature Conservation</title><categories>cs.DL</categories><journal-ref>International Journal of Spatial Data Infrastructures Research,
  2011, Vol.6, 206-233</journal-ref><doi>10.2902/1725-0463.2011.06.art10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes an approach to transcend multicultural and multilingual
barriers in the use and reuse of geographical data at the European level. The
approach aims at sharing scientific terms in the field of nature conservation
with the goal of assisting different user communities with metadata compilation
and information discovery. A multi-thesauri solution is proposed, based on a
Common Thesaurus Framework for Nature Conservation, where different well-known
Knowledge Organization Systems are assembled and shared. It has been designed
according to semantic web and W3C recommendations employing SKOS standard
models and Linked Data to publish the thesauri as a whole in
machine-understandable format. The outcome is a powerful framework satisfying
the requirements of modularity and openness for further thesaurus extension and
updating, interlinking among thesauri, and exploitability from other systems.
The paper supports the employment of Linked Data to deal with terminologies in
complex domains such as nature conservation and it proposes a hands-on recipe
to publish thesauri in the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1686</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1686</id><created>2011-07-08</created><authors><author><keyname>Dam&#xe1;sio</keyname><forenames>Carlos Viegas</forenames></author><author><keyname>Preece</keyname><forenames>Alun</forenames></author><author><keyname>Straccia</keyname><forenames>Umberto</forenames></author></authors><title>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI)</title><categories>cs.AI</categories><comments>HTML file with clickable links to papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the first edition of the
Doctoral Consortium of the 5th International Symposium on Rules (RuleML
2011@IJCAI) held on July 19th, 2011 in Barcelona, as well as the poster session
papers of the RuleML 2011@IJCAI main conference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1691</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1691</id><created>2011-07-08</created><authors><author><keyname>Stefanatos</keyname><forenames>Dionisis</forenames></author><author><keyname>Li</keyname><forenames>Jr-Shin</forenames></author></authors><title>Minimum-Time Quantum Transport with Bounded Trap Velocity</title><categories>math.OC cs.SY quant-ph</categories><msc-class>49K15, 93C05, 81V45</msc-class><journal-ref>IEEE Transactions on Automatic Control 59 (3), 733 - 738, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the problem of efficient transport of a quantum particle trapped
in a harmonic potential which can move with a bounded velocity, as a
minimum-time problem on a linear system with bounded input. We completely solve
the corresponding optimal control problem and obtain an interesting bang-bang
solution. These results are expected to find applications in quantum
information processing, where quantum transport between the storage and
processing units of a quantum computer is an essential step. They can also be
extended to the efficient transport of Bose-Einstein condensates, where the
ability to control them is crucial for their potential use as interferometric
sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1695</identifier>
 <datestamp>2011-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1695</id><created>2011-06-23</created><authors><author><keyname>Feinsilver</keyname><forenames>Philip</forenames></author><author><keyname>Schott</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>On Krawtchouk Transforms</title><categories>cs.IT math.CA math.IT</categories><comments>13 pages, presented at 10th International Conference on Artificial
  Intelligence and Symbolic Computation, AISC 2010, Paris, France, 5-6 July
  2010</comments><msc-class>Primary: 15.0, 15A69 Secondary: 05E35, 42C05</msc-class><journal-ref>Intelligent Computer Mathematics, 10th International Conference,
  AISC 2010, Paris, France, July 5-10, 2010. Proceedings. Springer 2010, pp.
  64-75</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Krawtchouk polynomials appear in a variety of contexts, most notably as
orthogonal polynomials and in coding theory via the Krawtchouk transform. We
present an operator calculus formulation of the Krawtchouk transform that is
suitable for computer implementation. A positivity result for the Krawtchouk
transform is shown. Then our approach is compared with the use of the
Krawtchouk transform in coding theory where it appears in MacWilliams' and
Delsarte's theorems on weight enumerators. We conclude with a construction of
Krawtchouk polynomials in an arbitrary finite number of variables, orthogonal
with respect to the multinomial distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1697</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1697</id><created>2011-07-08</created><authors><author><keyname>Chen</keyname><forenames>Aiyou</forenames></author><author><keyname>Cao</keyname><forenames>Jin</forenames></author><author><keyname>Shepp</keyname><forenames>Larry</forenames></author><author><keyname>Nguyen</keyname><forenames>Tuan</forenames></author></authors><title>Distinct counting with a self-learning bitmap</title><categories>stat.CO cs.DS</categories><comments>Journal of the American Statistical Association (accepted)</comments><acm-class>G.3; H.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the number of distinct elements (cardinality) in a dataset is a
fundamental problem in database management. In recent years, due to many of its
modern applications, there has been significant interest to address the
distinct counting problem in a data stream setting, where each incoming data
can be seen only once and cannot be stored for long periods of time. Many
probabilistic approaches based on either sampling or sketching have been
proposed in the computer science literature, that only require limited
computing and memory resources. However, the performances of these methods are
not scale-invariant, in the sense that their relative root mean square
estimation errors (RRMSE) depend on the unknown cardinalities. This is not
desirable in many applications where cardinalities can be very dynamic or
inhomogeneous and many cardinalities need to be estimated. In this paper, we
develop a novel approach, called self-learning bitmap (S-bitmap) that is
scale-invariant for cardinalities in a specified range. S-bitmap uses a binary
vector whose entries are updated from 0 to 1 by an adaptive sampling process
for inferring the unknown cardinality, where the sampling rates are reduced
sequentially as more and more entries change from 0 to 1. We prove rigorously
that the S-bitmap estimate is not only unbiased but scale-invariant. We
demonstrate that to achieve a small RRMSE value of $\epsilon$ or less, our
approach requires significantly less memory and consumes similar or less
operations than state-of-the-art methods for many common practice cardinality
scales. Both simulation and experimental studies are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1709</identifier>
 <datestamp>2011-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1709</id><created>2011-07-08</created><updated>2011-09-23</updated><authors><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Massive MIMO: How many antennas do we need?</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, to be presented at the Allerton Conference on
  Communication, Control and Computing, Urbana-Champaign, Illinois, US, Sep.
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multicell MIMO uplink channel where each base station (BS) is
equipped with a large number of antennas N. The BSs are assumed to estimate
their channels based on pilot sequences sent by the user terminals (UTs).
Recent work has shown that, as N grows infinitely large, (i) the simplest form
of user detection, i.e., the matched filter (MF), becomes optimal, (ii) the
transmit power per UT can be made arbitrarily small, (iii) the system
performance is limited by pilot contamination. The aim of this paper is to
assess to which extent the above conclusions hold true for large, but finite N.
In particular, we derive how many antennas per UT are needed to achieve \eta %
of the ultimate performance. We then study how much can be gained through more
sophisticated minimum-mean-square-error (MMSE) detection and how many more
antennas are needed with the MF to achieve the same performance. Our analysis
relies on novel results from random matrix theory which allow us to derive
tight approximations of achievable rates with a class of linear receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1720</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1720</id><created>2011-07-08</created><updated>2013-06-18</updated><authors><author><keyname>Driemel</keyname><forenames>Anne</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Jaywalking your Dog - Computing the Fr\'echet Distance with Shortcuts</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The similarity of two polygonal curves can be measured using the Fr\'echet
distance. We introduce the notion of a more robust Fr\'echet distance, where
one is allowed to shortcut between vertices of one of the curves. This is a
natural approach for handling noise, in particular batched outliers. We compute
a (3+\eps)-approximation to the minimum Fr\'echet distance over all possible
such shortcuts, in near linear time, if the curve is c-packed and the number of
shortcuts is either small or unbounded.
  To facilitate the new algorithm we develop several new tools:
  (A) A data structure for preprocessing a curve (not necessarily c-packed)
that supports (1+\eps)-approximate Fr\'echet distance queries between a
subcurve (of the original curve) and a line segment.
  (B) A near linear time algorithm that computes a permutation of the vertices
of a curve, such that any prefix of 2k-1 vertices of this permutation, form an
optimal approximation (up to a constant factor) to the original curve compared
to any polygonal curve with k vertices, for any k &gt; 0.
  (C) A data structure for preprocessing a curve that supports approximate
Fr\'echet distance queries between a subcurve and query polygonal curve. The
query time depends quadratically on the complexity of the query curve, and only
(roughly) logarithmically on the complexity of the original curve.
  To our knowledge, these are the first data structures to support these kind
of queries efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1731</identifier>
 <datestamp>2011-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1731</id><created>2011-07-08</created><updated>2011-07-19</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Distributed SIR-Aware Scheduling in Large-Scale Wireless Networks</title><categories>cs.IT math.IT</categories><comments>35 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic scheduling and routing can in principle greatly increase the
throughput of decentralized wireless networks, but to be practical they must do
so with small amounts of timely side information. In this paper, we propose
three techniques for low-overhead distributed opportunistic scheduling (DOS)
and precisely determine their affect on the overall network outage probability
and transmission capacity (TC). The first is distributed channel-aware
scheduling (DCAS), the second is distributed interferer-aware scheduling
(DIAS), and the third generalizes and combines those two and is called
distributed interferer-channel-aware scheduling (DICAS). One contribution is
determining the optimum channel and interference thresholds that a given
isolated transmitter should estimate and apply when scheduling their own
transmissions. Using this threshold, the precise network-wide gain of each
technique is quantified and compared. We conclude by considering interference
cancellation at the receivers, and finding how much it improves the outage
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1736</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1736</id><created>2011-07-08</created><updated>2012-08-20</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>High-dimensional structure estimation in Ising models: Local separation
  criterion</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1009 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1009</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 3, 1346-1375</journal-ref><doi>10.1214/12-AOS1009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of high-dimensional Ising (graphical) model
selection. We propose a simple algorithm for structure estimation based on the
thresholding of the empirical conditional variation distances. We introduce a
novel criterion for tractable graph families, where this method is efficient,
based on the presence of sparse local separators between node pairs in the
underlying graph. For such graphs, the proposed algorithm has a sample
complexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number of
variables, and $J_{\min}$ is the minimum (absolute) edge potential in the
model. We also establish nonasymptotic necessary and sufficient conditions for
structure estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1739</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1739</id><created>2011-07-08</created><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>The entropy functional, the information path functional's essentials and
  their connections to Kolmogorov's entropy, complexity and physics</title><categories>cs.IT cs.SY math.IT math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces the recent results related to an entropy functional on
trajectories of a controlled diffusion process, and the information path
functional (IPF), analyzing their connections to the Kolmogorov's entropy,
complexity and the Lyapunov's characteristics. Considering the IPF's essentials
and specifics, the paper studies the singularities of the IPF extremal
equations and the created invariant relations, which both are useful for the
solution of important mathematical and applied problems.
  Keywords: Additive functional; Entropy; Singularities, Natural Border
Problem; Invariant
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1744</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1744</id><created>2011-07-08</created><updated>2011-10-08</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Foster</keyname><forenames>Dean P.</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author></authors><title>Stochastic convex optimization with bandit feedback</title><categories>math.OC cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of minimizing a convex, Lipschitz function
$f$ over a convex, compact set $\xset$ under a stochastic bandit feedback
model. In this model, the algorithm is allowed to observe noisy realizations of
the function value $f(x)$ at any query point $x \in \xset$. The quantity of
interest is the regret of the algorithm, which is the sum of the function
values at algorithm's query points minus the optimal function value. We
demonstrate a generalization of the ellipsoid algorithm that incurs
$\otil(\poly(d)\sqrt{T})$ regret. Since any algorithm has regret at least
$\Omega(\sqrt{T})$ on this problem, our algorithm is optimal in terms of the
scaling with $T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1750</identifier>
 <datestamp>2011-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1750</id><created>2011-07-08</created><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Rivero</keyname><forenames>Alejandro</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>I&#xf1;igo</forenames></author><author><keyname>Cauh&#xe9;</keyname><forenames>Elisa</forenames></author><author><keyname>Ferrer</keyname><forenames>Alfredo</forenames></author><author><keyname>Ferrer</keyname><forenames>Dar&#xed;o</forenames></author><author><keyname>Francos</keyname><forenames>David</forenames></author><author><keyname>I&#xf1;iguez</keyname><forenames>David</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Mar&#xed;a Pilar</forenames></author><author><keyname>Ruiz</keyname><forenames>Gonzalo</forenames></author><author><keyname>Sanz</keyname><forenames>Francisco</forenames></author><author><keyname>Serrano</keyname><forenames>Ferm&#xed;n</forenames></author><author><keyname>Vi&#xf1;as</keyname><forenames>Cristina</forenames></author><author><keyname>Taranc&#xf3;n</keyname><forenames>Alfonso</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Structural and Dynamical Patterns on Online Social Networks: the Spanish
  May 15th Movement as a case study</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>16 pages, 7 figures</comments><journal-ref>PLoS ONE 6(8): e23883, 2011</journal-ref><doi>10.1371/journal.pone.0023883</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of people using online social networks in their everyday life is
continuously growing at a pace never saw before. This new kind of communication
has an enormous impact on opinions, cultural trends, information spreading and
even in the commercial success of new products. More importantly, social online
networks have revealed as a fundamental organizing mechanism in recent
country-wide social movements. In this paper, we provide a quantitative
analysis of the structural and dynamical patterns emerging from the activity of
an online social network around the ongoing May 15th (15M) movement in Spain.
Our network is made up by users that exchanged tweets in a time period of one
month, which includes the birth and stabilization of the 15M movement. We
characterize in depth the growth of such dynamical network and find that it is
scale-free with communities at the mesoscale. We also find that its dynamics
exhibits typical features of critical systems such as robustness and power-law
distributions for several quantities. Remarkably, we report that the patterns
characterizing the spreading dynamics are asymmetric, giving rise to a clear
distinction between information sources and sinks. Our study represent a first
step towards the use of data from online social media to comprehend modern
societal dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1752</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1752</id><created>2011-07-08</created><authors><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Garone</keyname><forenames>Emanuele</forenames></author><author><keyname>Casavola</keyname><forenames>Alessandro</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>Stochastic Sensor Scheduling for Energy Constrained Estimation in
  Multi-Hop Wireless Sensor Networks</title><categories>math.OC cs.SY</categories><comments>7 pages, 2 figures, complete version of a technical note for the IEEE
  transactions on automatic control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) enable a wealth of new applications where
remote estimation is essential. Individual sensors simultaneously sense a
dynamic process and transmit measured information over a shared channel to a
central fusion center. The fusion center computes an estimate of the process
state by means of a Kalman filter. In this paper we assume that the WSN admits
a tree topology with fusion center at the root. At each time step only a subset
of sensors can be selected to transmit observations to the fusion center due to
a limited energy budget. We propose a stochastic sensor selection algorithm
that randomly selects a subset of sensors according to certain probability
distribution, which is opportunely designed to minimize the asymptotic expected
estimation error covariance matrix. We show that the optimal stochastic sensor
selection problem can be relaxed into a convex optimization problem and thus
solved efficiently. We also provide a possible implementation of our algorithm
which does not introduce any communication overhead. The paper ends with some
numerical examples that show the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1753</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1753</id><created>2011-07-08</created><authors><author><keyname>Parvanov</keyname><forenames>Yavor</forenames></author></authors><title>Notes on Electronic Lexicography</title><categories>cs.CL</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes are a continuation of topics covered by V. Selegej in his article
&quot;Electronic Dictionaries and Computational lexicography&quot;. How can an electronic
dictionary have as its object the description of closely related languages?
Obviously, such a question allows multiple answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1779</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1779</id><created>2011-07-09</created><authors><author><keyname>Ahmed</keyname><forenames>Eya Ben</forenames></author><author><keyname>Nabli</keyname><forenames>Ahlem</forenames></author><author><keyname>Gargouri</keyname><forenames>Fa&#xef;ez</forenames></author></authors><title>A Survey of User-Centric Data Warehouses: From Personalization to
  Recommendation</title><categories>cs.DB</categories><comments>13 pages, 3 figures, 1 table</comments><journal-ref>The International Journal of Database Management Systems (IJDMS),
  May 2011, Volume 3, Number 2</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Providing a customized support for the OLAP brings tremendous challenges to
the OLAP technology. Standing at the crossroads of the preferences and the data
warehouse, two emerging trends are pointed out; namely: (i) the personalization
and (ii) the recommendation. Although the panoply of the proposed approaches,
the user-centric data warehouse community issues have not been addressed yet.
In this paper we draw an overview of several user centric data warehouse
proposals. We also discuss the two promising concepts in this issue, namely,
the personalization and the recommendation of the data warehouses. We compare
the current approaches among each others with respect to some criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1780</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1780</id><created>2011-07-09</created><authors><author><keyname>Keshavarz-Kohjerdi</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Hamiltonian Paths in Two Classes of Grid Graphs</title><categories>cs.DS</categories><comments>11pages, 7figures</comments><msc-class>05C45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give the necessary and sufficient conditions for the
existence of Hamiltonian paths in $L-$alphabet and $C-$alphabet grid graphs. We
also present a linear-time algorithm for finding Hamiltonian paths in these
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1805</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1805</id><created>2011-07-09</created><authors><author><keyname>Volkovs</keyname><forenames>Maksims N.</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author></authors><title>Loss-sensitive Training of Probabilistic Conditional Random Fields</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of training probabilistic conditional random fields
(CRFs) in the context of a task where performance is measured using a specific
loss function. While maximum likelihood is the most common approach to training
CRFs, it ignores the inherent structure of the task's loss function. We
describe alternatives to maximum likelihood which take that loss into account.
These include a novel adaptation of a loss upper bound from the structured SVMs
literature to the CRF context, as well as a new loss-inspired KL divergence
objective which relies on the probabilistic nature of CRFs. These
loss-sensitive objectives are compared to maximum likelihood using ranking as a
benchmark task. This comparison confirms the importance of incorporating loss
information in the probabilistic training of CRFs, with the loss-inspired KL
outperforming all other objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1814</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1814</id><created>2011-07-09</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author></authors><title>Efficient coordination mechanisms for unrelated machine scheduling</title><categories>cs.GT</categories><comments>26 pages, preliminary version appeared in Proceedings of the 20th
  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 815-824, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new coordination mechanisms for scheduling selfish jobs on $m$
unrelated machines. A coordination mechanism aims to mitigate the impact of
selfishness of jobs on the efficiency of schedules by defining a local
scheduling policy on each machine. The scheduling policies induce a game among
the jobs and each job prefers to be scheduled on a machine so that its
completion time is minimum given the assignments of the other jobs. We consider
the maximum completion time among all jobs as the measure of the efficiency of
schedules. The approximation ratio of a coordination mechanism quantifies the
efficiency of pure Nash equilibria (price of anarchy) of the induced game.
  Our mechanisms are deterministic, local, and preemptive. Our first
coordination mechanism has approximation ratio $\Theta(\log m)$ and guarantees
that the induced game has pure Nash equilibria. This result improves a bound of
$O(\log^2 m)$ due to Azar, Jain, and Mirrokni and uses a global ordering of the
jobs according to their distinct IDs. Our second mechanism handles anonymous
jobs and has approximation ratio $O(\frac{\log m}{\log \log m})$ although the
game induced is not a potential game and, hence, the existence of pure Nash
equilibria is not guaranteed by potential function arguments. However, it
provides evidence that the known lower bounds for non-preemptive coordination
mechanisms could be beaten using preemptive scheduling policies. Our third
coordination mechanism also handles anonymous jobs and has a nice
cost-revealing potential function. We use this potential function in order to
prove the existence of equilibria and to upper-bound the price of anarchy of
the induced game by $O(\log^2m)$. Our third coordination mechanism is the first
that handles anonymous jobs and simultaneously guarantees that the induced game
is a potential game and has bounded price of anarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1821</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1821</id><created>2011-07-09</created><authors><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author></authors><title>Where Have You Been? Secure Location Provenance for Mobile Devices</title><categories>cs.CR</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of mobile computing, location-based services have recently
gained popularity. Many applications use the location provenance of users,
i.e., the chronological history of the users' location for purposes ranging
from access control, authentication, information sharing, and evaluation of
policies. However, location provenance is subject to tampering and collusion
attacks by malicious users. In this paper, we examine the secure location
provenance problem. We introduce a witness-endorsed scheme for generating
collusion-resistant location proofs. We also describe two efficient and
privacy-preserving schemes for protecting the integrity of the chronological
order of location proofs. These schemes, based on hash chains and Bloom filters
respectively, allow users to prove the order of any arbitrary subsequence of
their location history to auditors. Finally, we present experimental results
from our proof-of-concept implementation on the Android platform and show that
our schemes are practical in today's mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1824</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1824</id><created>2011-07-09</created><authors><author><keyname>Zahedi</keyname><forenames>Ramin</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author></authors><title>Measurement Design for Detecting Sparse Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of testing for the presence (or detection) of an
unknown sparse signal in additive white noise. Given a fixed measurement
budget, much smaller than the dimension of the signal, we consider the general
problem of designing compressive measurements to maximize the measurement
signal-to-noise ratio (SNR), as increasing SNR improves the detection
performance in a large class of detectors. We use a lexicographic optimization
approach, where the optimal measurement design for sparsity level $k$ is sought
only among the set of measurement matrices that satisfy the optimality
conditions for sparsity level k-1. We consider optimizing two different SNR
criteria, namely a worst-case SNR measure, over all possible realizations of a
k-sparse signal, and an average SNR measure with respect to a uniform
distribution on the locations of the up to k nonzero entries in the signal. We
establish connections between these two criteria and certain classes of tight
frames. We constrain our measurement matrices to the class of tight frames to
avoid coloring the noise covariance matrix. For the worst-case problem, we show
that the optimal measurement matrix is a Grassmannian line packing for
most---and a uniform tight frame for all---sparse signals. For the average SNR
problem, we prove that the optimal measurement matrix is a uniform tight frame
with minimum sum-coherence for most---and a tight frame for all---sparse
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1829</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1829</id><created>2011-07-09</created><authors><author><keyname>Hui</keyname><forenames>Ka Hung</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Berry</keyname><forenames>Randall A.</forenames></author></authors><title>Medium Access Control for Wireless Networks with Peer-to-Peer State
  Exchange</title><categories>cs.IT math.IT</categories><comments>12 pages, 17 figures, submitted to IEEE Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed medium access control (MAC) protocols are proposed for wireless
networks assuming that one-hop peers can periodically exchange a small amount
of state information. Each station maintains a state and makes state
transitions and transmission decisions based on its state and recent state
information collected from its one-hop peers. A station can adapt its packet
length and the size of its state space to the amount of traffic in its
neighborhood. It is shown that these protocols converge to a steady state,
where stations take turns to transmit in each neighborhood without collision.
In other words, an efficient time-division multiple access (TDMA) like schedule
is formed in a distributed manner, as long as the topology of the network
remains static or changes slowly with respect to the execution of the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1837</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1837</id><created>2011-07-10</created><authors><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author><author><keyname>He</keyname><forenames>Ran</forenames></author><author><keyname>Yuan</keyname><forenames>XiaoTong</forenames></author></authors><title>Information-Theoretic Measures for Objective Evaluation of
  Classifications</title><categories>cs.CV cs.IT math.IT</categories><comments>25 Pages, 1 Figure, 10 Tables</comments><journal-ref>Acta Automatica Sinica, 38(7): 1169-1182, 2012</journal-ref><doi>10.3724/SP.J.1004.2012.01169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a systematic study of objective evaluations of abstaining
classifications using Information-Theoretic Measures (ITMs). First, we define
objective measures for which they do not depend on any free parameter. This
definition provides technical simplicity for examining &quot;objectivity&quot; or
&quot;subjectivity&quot; directly to classification evaluations. Second, we propose
twenty four normalized ITMs, derived from either mutual information,
divergence, or cross-entropy, for investigation. Contrary to conventional
performance measures that apply empirical formulas based on users' intuitions
or preferences, the ITMs are theoretically more sound for realizing objective
evaluations of classifications. We apply them to distinguish &quot;error types&quot; and
&quot;reject types&quot; in binary classifications without the need for input data of
cost terms. Third, to better understand and select the ITMs, we suggest three
desirable features for classification assessment measures, which appear more
crucial and appealing from the viewpoint of classification applications. Using
these features as &quot;meta-measures&quot;, we can reveal the advantages and limitations
of ITMs from a higher level of evaluation knowledge. Numerical examples are
given to corroborate our claims and compare the differences among the proposed
measures. The best measure is selected in terms of the meta-measures, and its
specific properties regarding error types and reject types are analytically
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1839</identifier>
 <datestamp>2013-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1839</id><created>2011-07-10</created><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Interference Networks with General Message Sets: A Random Coding Scheme</title><categories>cs.IT math.IT</categories><comments>13 pages, with Appendix, Submitted for Conference Publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Interference Network with General Message Sets (IN-GMS) is
introduced in which several transmitters send messages to several receivers:
Each subset of transmitters transmit an individual message to each subset of
receivers. For such a general scenario, an achievability scheme is presented
using the random coding. This scheme is systematically built based on the
capacity achieving scheme for the Multiple Access Channel (MAC) with common
message as well as the best known achievability scheme for the Broadcast
Channel (BC) with common message. A graphical illustration of the random
codebook construction procedure is also provided, by using which the
achievability scheme is easily understood. Some benefits of the proposed
achievability scheme are described. It is also shown that the resulting rate
region is optimal for a class of orthogonal INs-GMS, which yields the capacity
region. Finally, it is demonstrated that how this general achievability scheme
can be used to derive capacity inner bounds for interference networks with
different distribution of messages; in most cases, the proposed achievability
scheme leads to the best known capacity inner bound for the underlying channel.
Capacity inner bounds can also be derived for new communication scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1847</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1847</id><created>2011-07-10</created><authors><author><keyname>Kushwah</keyname><forenames>Prashant</forenames></author><author><keyname>Lal</keyname><forenames>Sunder</forenames></author></authors><title>Efficient Identity Based Public Verifiable Signcryption Scheme</title><categories>cs.CR</categories><comments>11 pages</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signcryption is a cryptographic primitive which performs encryption and
signature in a single logical step. In conventional signcryption only receiver
of the signcrypted text can verify the authenticity of the origin i.e.
signature of the sender on the message after decrypting the cipher text. In
public verifiable signcryption scheme anyone can verify the authenticity of the
origin who can access the signcrypted text i.e. signature of the sender on the
cipher text. Public verifiable signcryption scheme in which the receiver can
convince a third party, by providing additional information other than his
private key along with the signcryption is called third party verifiable
signcryption schemes. In this paper we proposed an efficient identity based
public verifiable signcryption scheme with third party verification and proved
its security in the random oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1851</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1851</id><created>2011-07-10</created><updated>2013-12-05</updated><authors><author><keyname>Kim</keyname><forenames>Dohan</forenames></author></authors><title>Task swapping networks in distributed systems</title><categories>cs.DC cs.AI cs.NI</categories><comments>This is a preprint of a paper whose final and definite form is
  published in: Int. J. Comput. Math. 90 (2013), 2221-2243 (DOI:
  10.1080/00207160.2013.772985)</comments><journal-ref>Int. J. Comput. Math. 90 (2013), 2221-2243</journal-ref><doi>10.1080/00207160.2013.772985</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we propose task swapping networks for task reassignments by
using task swappings in distributed systems. Some classes of task reassignments
are achieved by using iterative local task swappings between software agents in
distributed systems. We use group-theoretic methods to find a minimum-length
sequence of adjacent task swappings needed from a source task assignment to a
target task assignment in a task swapping network of several well-known
topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1866</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1866</id><created>2011-07-10</created><updated>2014-03-20</updated><authors><author><keyname>Kim</keyname><forenames>Dohan</forenames></author></authors><title>Priority-based task reassignments in hierarchical 2D mesh-connected
  systems using tableaux</title><categories>cs.DC</categories><comments>Preprint of an article published in Discrete Mathematics, Algorithms
  and Applications Vol. 6, No. 2 (2014) 1450022 (16 pages), (c) World
  Scientific Publishing Company, DOI: 10.1142/S1793830914500220</comments><journal-ref>Discrete Mathematics, Algorithms and Applications Vol. 6, No. 2
  (2014) 1450022 (16 pages)</journal-ref><doi>10.1142/S1793830914500220</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Task reassignments in 2D mesh-connected systems (2D-MSs) have been researched
for several decades. We propose a hierarchical 2D mesh-connected system
(2D-HMS) in order to exploit the regular nature of a 2D-MS. In our approach
priority-based task assignments and reassignments in a 2D-HMS are represented
by tableaux and their algorithms. We show how task relocations for a
priority-based task reassignment in a 2D-HMS are reduced to a jeu de taquin
slide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1878</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1878</id><created>2011-07-10</created><authors><author><keyname>Norris</keyname><forenames>Ian</forenames></author><author><keyname>Sieben</keyname><forenames>Nandor</forenames></author></authors><title>Biased Weak Polyform Achievement Games</title><categories>math.CO cs.GT</categories><msc-class>91A46 (05B50)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a biased weak $(a,b)$ polyform achievement game, the maker and the breaker
alternately mark $a,b$ previously unmarked cells on an infinite board,
respectively. The maker's goal is to mark a set of cells congruent to a
polyform. The breaker tries to prevent the maker from achieving this goal. A
winning maker strategy for the $(a,b)$ game can be built from winning
strategies for games involving fewer marks for the maker and the breaker. A new
type of breaker strategy called the priority strategy is introduced. The
winners are determined for all $(a,b)$ pairs for polyiamonds and polyominoes up
to size four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1880</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1880</id><created>2011-07-10</created><updated>2012-07-27</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Hossayni</keyname><forenames>Hicham</forenames><affiliation>DCIS</affiliation></author></authors><title>Matrix powers algorithms for trust evaluation in PKI architectures</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>STM 2012 - 8th International Workshop on Security and Trust
  Management (co-ESORICS 2012), Pise : Italie (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the evaluation of trust in public-key infrastructures.
Different trust models have been proposed to interconnect the various PKI
components in order to propagate the trust between them. In this paper we
provide a new polynomial algorithm using linear algebra to assess trust
relationships in a network using different trust evaluation schemes. The
advantages are twofold: first the use of matrix computations instead of graph
algorithms provides an optimized computational solution; second, our algorithm
can be used for generic graphs, even in the presence of cycles. Our algorithm
is designed to evaluate the trust using all existing (finite) trust paths
between entities as a preliminary to any exchanges between PKIs. This can give
a precise evaluation of trust, and accelerate for instance cross-certificate
validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1886</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1886</id><created>2011-07-10</created><authors><author><keyname>Karumbu</keyname><forenames>Premkumar</forenames></author><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Utility Optimal Coding for Packet Transmission over Wireless Networks -
  Part I: Networks of Binary Symmetric Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to Forty-Ninth Annual Allerton Conference on Communication,
  Control, and Computing, Monticello, IL, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi--hop networks comprising Binary Symmetric Channels
($\mathsf{BSC}$s). The network carries unicast flows for multiple users. The
utility of the network is the sum of the utilities of the flows, where the
utility of each flow is a concave function of its throughput. Given that the
network capacity is shared by the flows, there is a contention for network
resources like coding rate (at the physical layer), scheduling time (at the MAC
layer), etc., among the flows. We propose a proportional fair transmission
scheme that maximises the sum utility of flow throughputs subject to the rate
and the scheduling constraints. This is achieved by {\em jointly optimising the
packet coding rates of all the flows through the network}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1890</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1890</id><created>2011-07-10</created><authors><author><keyname>Karumbu</keyname><forenames>Premkumar</forenames></author><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Utility Optimal Coding for Packet Transmission over Wireless Networks -
  Part II: Networks of Packet Erasure Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to the Forty-Ninth Annual Allerton Conference on
  Communication, Control, and Computing, Monticello, Illinois, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a class of multi--hop erasure networks that approximates a wireless
multi--hop network. The network carries unicast flows for multiple users, and
each information packet within a flow is required to be decoded at the flow
destination within a specified delay deadline. The allocation of coding rates
amongst flows/users is constrained by network capacity. We propose a
proportional fair transmission scheme that maximises the sum utility of flow
throughputs. This is achieved by {\em jointly optimising the packet coding
rates and the allocation of bits of coded packets across transmission slots.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1893</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1893</id><created>2011-07-10</created><updated>2011-12-30</updated><authors><author><keyname>Sviridenko</keyname><forenames>Alexander</forenames></author><author><keyname>Shcherbina</keyname><forenames>Oleg</forenames></author></authors><title>Benchmarking ordering techniques for nonserial dynamic programming</title><categories>cs.DM</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Five ordering algorithms for the nonserial dynamic programming algorithm for
solving sparse discrete optimization problems are compared in this paper. The
benchmarking reveals that the ordering of the variables has a significant
impact on the run-time of these algorithms. In addition, it is shown that
different orderings are most effective for different classes of problems.
Finally, it is shown that, amongst the algorithms considered here, heuristics
based on maximum cardinality search and minimum fill-in perform best for
solving the discrete optimization problems considered in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1895</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1895</id><created>2011-07-10</created><authors><author><keyname>Pirvu</keyname><forenames>Traian A.</forenames></author><author><keyname>Zhang</keyname><forenames>Huayue</forenames></author></authors><title>On Investment-Consumption with Regime-Switching</title><categories>math.OC cs.SY q-fin.PM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a continuous time stochastic economy, this paper considers the problem of
consumption and investment in a financial market in which the representative
investor exhibits a change in the discount rate. The investment opportunities
are a stock and a riskless account. The market coefficients and discount factor
switches according to a finite state Markov chain. The change in the discount
rate leads to time inconsistencies of the investor's decisions. The randomness
in our model is driven by a Brownian motion and Markov chain. Following Ekeland
etc (2008) we introduce and characterize the equilibrium policies for power
utility functions. Moreover, they are computed in closed form for logarithmic
utility function. We show that a higher discount rate leads to a higher
equilibrium consumption rate. Numerical experiments show the effect of both
time preference and risk aversion on the equilibrium policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1900</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1900</id><created>2011-07-10</created><authors><author><keyname>Zhang</keyname><forenames>Cheng-Jun</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author></authors><title>Behavior patterns of online users and the effect on information
  filtering</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>8 pages, 6 figures</comments><journal-ref>Physica A 391, 1822 (2012)</journal-ref><doi>10.1016/j.physa.2011.09.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the structure and evolution of web-based user-object bipartite
networks is an important task since they play a fundamental role in online
information filtering. In this paper, we focus on investigating the patterns of
online users' behavior and the effect on recommendation process. Empirical
analysis on the e-commercial systems show that users have significant taste
diversity and their interests for niche items highly overlap. Additionally,
recommendation process are investigated on both the real networks and the
reshuffled networks in which real users' behavior patterns can be gradually
destroyed. Our results shows that the performance of personalized
recommendation methods is strongly related to the real network structure.
Detail study on each item shows that recommendation accuracy for hot items is
almost maximum and quite robust to the reshuffling process. However, niche
items cannot be accurately recommended after removing users' behavior patterns.
Our work also is meaningful in practical sense since it reveals an effective
direction to improve the accuracy and the robustness of the existing
recommender systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1901</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1901</id><created>2011-07-10</created><updated>2013-08-05</updated><authors><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Anjolina G.</forenames></author></authors><title>Propositional equality, identity types, and direct computational paths</title><categories>cs.LO</categories><comments>41 pages, submitted to a scientific journal. arXiv admin note: text
  overlap with arXiv:1010.1810, arXiv:0906.4521 by other authors</comments><msc-class>03Fxx, 03F03</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In proof theory the notion of canonical proof is rather basic, and it is
usually taken for granted that a canonical proof of a sentence must be unique
up to certain minor syntactical details (such as, e.g., change of bound
variables). When setting up a proof theory for equality one is faced with a
rather unexpected situation where there may not be a unique canonical proof of
an equality statement. Indeed, in a (1994--5) proposal for the formalisation of
proofs of propositional equality in the Curry--Howard style, we have already
uncovered such a peculiarity. Totally independently, and in a different
setting, Hofmann &amp; Streicher (1994) have shown how to build a model of
Martin-L\&quot;of's Type Theory in which uniqueness of canonical proofs of identity
types does not hold. The intention here is to show that, by considering as
sequences of rewrites and substitution, it comes a rather natural fact that two
(or more) distinct proofs may be yet canonical and are none to be preferred
over one another. By looking at proofs of equality as rewriting (or
computational) paths this approach will be in line with the recently proposed
connections between type theory and homotopy theory via identity types, since
elements of identity types will be, concretely, paths (or homotopies).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1932</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1932</id><created>2011-07-11</created><authors><author><keyname>Rabah</keyname><forenames>Sleiman</forenames></author><author><keyname>Ni</keyname><forenames>Dan</forenames></author><author><keyname>Jahanshahi</keyname><forenames>Payam</forenames></author><author><keyname>Guzman</keyname><forenames>Luis Felipe</forenames></author></authors><title>Current State and Challenges of Automatic Planning in Web Service
  Composition</title><categories>cs.DC cs.AI</categories><acm-class>D.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a survey on the current state of Web Service Compositions
and the difficulties and solutions to automated Web Service Compositions. This
first gives a definition of Web Service Composition and the motivation and goal
of it. It then explores into why we need automated Web Service Compositions and
formally defines the domains. Techniques and solutions are proposed by the
papers we surveyed to solve the current difficulty of automated Web Service
Composition. Verification and future work is discussed at the end to further
extend the topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1937</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1937</id><created>2011-07-11</created><updated>2012-02-23</updated><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Ghini</keyname><forenames>Vittorio</forenames></author></authors><title>Scale-Free Opportunistic Networks: is it Possible?</title><categories>cs.DC cs.NI</categories><comments>A revised version of the paper will appear in Proc. of the 8th
  International Workshop on Mobile Peer-to-Peer Computing - IEEE International
  Conference on Pervasive Computing and Communications (PERCOM 2012), Lugano,
  Switzerland, IEEE, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coupling of scale-free networks with mobile unstructured networks is
certainly unusual. In mobile networks, connections active at a given instant
are constrained by the geographical distribution of mobile nodes, and by the
limited signal strength of the wireless technology employed to build the ad-hoc
overlay. This is in contrast with the presence of hubs, typical of scale-free
nets. However, opportunistic (mobile) networks possess the distinctive feature
to be delay tolerant; mobile nodes implement a store, carry and forward
strategy that permits to disseminate data based on a multi-hop route, which is
built in time, when nodes encounter other ones while moving. In this paper, we
consider opportunistic networks as evolving graphs where links represent
contacts among nodes arising during a (non-instantaneous) time interval. We
discuss a strategy to control the way nodes manage contacts and build
&quot;opportunistic overlays&quot;. Based on such an approach, interesting overlays can
be obtained, shaped following given desired topologies, such as scale-free
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1938</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1938</id><created>2011-07-11</created><updated>2012-02-20</updated><authors><author><keyname>Guimei</keyname><forenames>Zhu</forenames></author><author><keyname>Huijie</keyname><forenames>Yang</forenames></author><author><keyname>Rui</keyname><forenames>Yang</forenames></author><author><keyname>Jie</keyname><forenames>Ren</forenames></author><author><keyname>Baowen</keyname><forenames>Li</forenames></author><author><keyname>Ying-Cheng</keyname><forenames>Lai</forenames></author></authors><title>Uncovering Evolutionary Ages of Nodes in Complex Networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>10 pages, 6 figures, accepted by EPJB 2012 Feb</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a complex network, different groups of nodes may have existed for
different amounts of time. To detect the evolutionary history of a network is
of great importance. We present a general method based on spectral analysis to
address this fundamental question in network science. In particular, we argue
and demonstrate, using model and real-world networks, the existence of positive
correlation between the magnitudes of eigenvalues and node ages. In situations
where the network topology is unknown but short time series measured from nodes
are available, we suggest to uncover the network topology at the present (or
any given time of interest) by using compressive sensing and then perform the
spectral analysis. Knowledge of ages of various groups of nodes can provide
significant insights into the evolutionary process underpinning the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1940</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1940</id><created>2011-07-11</created><authors><author><keyname>Meyer</keyname><forenames>David A.</forenames><affiliation>Mathematics Department, UCSD</affiliation></author><author><keyname>Pommersheim</keyname><forenames>James</forenames><affiliation>Mathematics Department, UCSD</affiliation><affiliation>Mathematics Department, Reed</affiliation></author></authors><title>Multi-query quantum sums</title><categories>quant-ph cs.CC</categories><comments>11 pages, 1 figure; presented at TQC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PARITY is the problem of determining the parity of a string $f$ of $n$ bits
given access to an oracle that responds to a query $x\in\{0,1,...,n-1\}$ with
the $x^{\rm th}$ bit of the string, $f(x)$. Classically, $n$ queries are
required to succeed with probability greater than 1/2 (assuming equal prior
probabilities for all length $n$ bitstrings), but only $\lceil n/2\rceil$
quantum queries suffice to determine the parity with probability 1. We consider
a generalization to strings $f$ of $n$ elements of $\Z_k$ and the problem of
determining $\sum f(x)$. By constructing an explicit algorithm, we show that
$n-r$ ($n\ge r\in\N$) entangled quantum queries suffice to compute the sum
correctly with worst case probability $\min\{\lfloor n/r\rfloor/k,1\}$. This
quantum algorithm utilizes the $n-r$ queries sequentially and adaptively, like
Grover's algorithm, but in a different way that is not amplitude amplification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1941</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1941</id><created>2011-07-11</created><authors><author><keyname>Dai</keyname><forenames>Hong-Ning</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>Fu</keyname><forenames>Liqun</forenames></author></authors><title>Link Scheduling in Multi-Transmit-Receive Wireless Networks</title><categories>cs.NI</categories><comments>This manuscript is an extention of our previous paper with the same
  title, which is accepted by IEEE LCN 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of link scheduling to meet traffic
demands with minimum airtime in a multi-transmit-receive (MTR) wireless
network. MTR networks are a new class of networks, in which each node can
simultaneously transmit to a number of other nodes, or simultaneously receive
from a number of other nodes. The MTR capability can be enabled by the use of
multiple directional antennas or multiple channels. Potentially, MTR can boost
the network capacity significantly. However, link scheduling that makes full
use of the MTR capability must be in place before this can happen. We show that
optimal link scheduling can be formulated as a linear program (LP). However,
the problem is NP-hard because we need to find all the maximal independent sets
in a graph first. We propose two computationally efficient algorithms, called
Heavy-Weight-First (HWF) and Max-Degree-First (MDF) to solve this problem.
Simulation results show that both HWF and MDF can achieve superior performance
in terms of runtime and optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1943</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1943</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author><author><keyname>Yashoda</keyname><forenames>M. B.</forenames></author></authors><title>Enhanced Genetic Algorithm approach for Solving Dynamic Shortest Path
  Routing Problems using Immigrants and Memory Schemes</title><categories>cs.NE cs.NI</categories><comments>5 pages,6 figures, International Conference on Frontiers of Computer
  Science, 7TH TO 9TH August 2011, JN Tata Convention Centre, IISc,Bangalore,
  India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Internet Routing, the static shortest path (SP) problem has been addressed
using well known intelligent optimization techniques like artificial neural
networks, genetic algorithms (GAs) and particle swarm optimization. Advancement
in wireless communication lead more and more mobile wireless networks, such as
mobile networks [mobile ad hoc networks (MANETs)] and wireless sensor networks.
Dynamic nature of the network is the main characteristic of MANET. Therefore,
the SP routing problem in MANET turns into dynamic optimization problem (DOP).
Here the nodes ae made aware of the environmental condition, thereby making it
intelligent, which goes as the input for GA. The implementation then uses GAs
with immigrants and memory schemes to solve the dynamic SP routing problem
(DSPRP) in MANETS. In our paper, once the network topology changes, the optimal
solutions in the new environment can be searched using the new immigrants or
the useful information stored in the memory. Results shows GA with new
immigrants shows better convergence result than GA with memory scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1944</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1944</id><created>2011-07-11</created><updated>2012-08-06</updated><authors><author><keyname>Li</keyname><forenames>Yen-Huan</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Cheng</forenames></author></authors><title>An Interpretation of the Moore-Penrose Generalized Inverse of a Singular
  Fisher Information Matrix</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>10 pages, accepted for publication in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2012.2208105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that in a non-Bayesian parametric estimation problem, if the
Fisher information matrix (FIM) is singular, unbiased estimators for the
unknown parameter will not exist. Cramer-Rao bound (CRB), a popular tool to
lower bound the variances of unbiased estimators, seems inapplicable in such
situations. In this paper, we show that the Moore-Penrose generalized inverse
of a singular FIM can be interpreted as the CRB corresponding to the minimum
variance among all choices of minimum constraint functions. This result ensures
the logical validity of applying the Moore-Penrose generalized inverse of an
FIM as the covariance lower bound when the FIM is singular. Furthermore, the
result can be applied as a performance bound on the joint design of constraint
functions and unbiased estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1945</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1945</id><created>2011-07-11</created><updated>2012-06-02</updated><authors><author><keyname>Nair</keyname><forenames>Dr. T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Ms. Kavitha</forenames></author><author><keyname>Shetty</keyname><forenames>Ms. Deepthi D</forenames></author><author><keyname>Hegde</keyname><forenames>Ms. Prapthi</forenames></author><author><keyname>Hegde</keyname><forenames>Ms. Anusha</forenames></author></authors><title>Region-based Approach for Determining the Optimal Path Using PSO</title><categories>cs.NI</categories><comments>This paper has been withdrawn as the authors were unable to present
  the paper for the conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many research works have been carried out recently to find the optimal path
in network routing. Among them the evolutionary algorithms is an area where
work is carried out extensively. We in this paper, have used PSO for finding
the optimal path and the concept of region based network is introduced along
with the use of indirect encoding. A comparative study of genetic algorithm
(GA) and particle swarm optimization (PSO) is carried out, and it was found
that PSO performed better than GA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1950</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1950</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>Dr T. R. Gopalakrishnan</forenames></author><author><keyname>Malhotra</keyname><forenames>Meenakshi</forenames></author></authors><title>Knowledge Embedding and Retrieval Strategies in an Informledge System</title><categories>cs.AI</categories><comments>5 pages, 7 pages, International Conferenceon Information and
  Knowledge Management (ICIKM-IEEE), Haikou, China, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper, we put forward the strategies for knowledge embedding and retrieval in
an ILS. ILS is a powerful knowledge network system dealing with logical storage
and connectivity of information units to form knowledge using autonomous nodes
and multi-lateral links. In ILS, the autonomous nodes known as Knowledge
Network Nodes (KNN)s play vital roles which are not only used in storage,
parsing and in forming the multi-lateral linkages between knowledge points but
also in helping the realization of intelligent retrieval of linked information
units in the form of knowledge. Knowledge built in to the ILS forms the shape
of sphere. The intelligence incorporated into the links of a KNN helps in
retrieving various knowledge threads from a specific set of KNNs. A developed
entity of information realized through KNN forms in to the shape of a knowledge
cone
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1951</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1951</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author></authors><title>Particle Swarm Optimization for Realizing Intelligent Routing in
  Networks with Quality Grading</title><categories>cs.NI</categories><comments>4 pages, 5 figures, 7th International Conference on Wireless
  Communication, Networking and Mobile Computing, Wuhan, China, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant research has been carried out in the recent years for generating
systems exhibiting intelligence for realizing optimized routing in networks. In
this paper, a grade based twolevel based node selection method along with
Particle Swarm Optimization (PSO) technique is proposed. It assumes that the
nodes are intelligent and there exist a knowledge base about the environment in
their local memory. There are two levels for approaching the effective route
selection process through grading. At the first level, grade based selection is
applied and at the second level, the optimum path is explored using PSO. The
simulation has been carried out on different topological structures and it is
observed that a graded network produces a significant reduction in number of
iteration to arrive at the optimal path selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1954</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1954</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>Dr. T. R. Gopalakrishnan</forenames><affiliation>SMIEEE</affiliation></author><author><keyname>Shubhamangala</keyname><forenames>B. R.</forenames><affiliation>MIEEE</affiliation></author><author><keyname>M</keyname><forenames>Vaidehi.</forenames><affiliation>MIEEE</affiliation></author></authors><title>A Novel Agent Based Approach for Controlling Network Storms</title><categories>cs.NI</categories><comments>7 pages, 12 figures IEEE Third International Conference on
  Communications and Electronics (ICCE 2010). Nha Trang, Vietnam, Proceedings,
  11-13 August 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental data transmission mechanisms in Ethernet LAN is
broadcasting. Flooding is a direct broadcasting technique used in these
networks. A significant drawback of this method is that it can lead to
broadcast storms. This phenomenon is more common in multivendor switch
environment. Broadcast storms usually results in dissension, collision and
redundancy leading to degradation of the network performance. Most of the
storms appear without much warning and it affects the efficiency of network
even in situations when the network is expected to work most efficiently. There
are several characteristic patterns by which storm can appear in a LAN, like
rate monotonic repetition, transient appearances with different types of growth
properties and decay profiles. In this paper we discuss the storm build up
pattern in an industry and present various reasons for storm in LAN. We have
identified a strategy for controlling network storms, using multiple static
agents. These agents inhibit storm packet regeneration in the network using the
knowledge of storm growth pattern. A model developed out of empirical studies
is used to differentiate normal packet growth from storm packet growth and used
in control mechanism of storms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1956</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1956</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>Dr T. R. Gopalakrishnan</forenames></author><author><keyname>Malhotra</keyname><forenames>Meenakshi</forenames></author></authors><title>Informledge System: A Modified Knowledge Network with Autonomous Nodes
  using Multi-lateral Links</title><categories>cs.IR cs.AI cs.NE</categories><comments>4 pages, 5 figures, International Conference on Knowledge Engineering
  and Ontology Development, KEOD 2010, Proceeding of KEOD-2010, pp 351-354,
  Valencia-Spain, October 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in the field of Artificial Intelligence is continually progressing
to simulate the human knowledge into automated intelligent knowledge base,
which can encode and retrieve knowledge efficiently along with the capability
of being is consistent and scalable at all times. However, there is no system
at hand that can match the diversified abilities of human knowledge base. In
this position paper, we put forward a theoretical model of a different system
that intends to integrate pieces of knowledge, Informledge System (ILS). ILS
would encode the knowledge, by virtue of knowledge units linked across
diversified domains. The proposed ILS comprises of autonomous knowledge units
termed as Knowledge Network Node (KNN), which would help in efficient
cross-linking of knowledge units to encode fresh knowledge. These links are
reasoned and inferred by the Parser and Link Manager, which are part of KNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1958</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1958</id><created>2011-07-11</created><authors><author><keyname>Chlamtac</keyname><forenames>Eden</forenames></author><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author></authors><title>Linear Index Coding via Semidefinite Programming</title><categories>cs.DS cs.DM cs.IT math.IT</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In the index coding problem, introduced by Birk and Kol (INFOCOM, 1998), the
goal is to broadcast an n bit word to n receivers (one bit per receiver), where
the receivers have side information represented by a graph G. The objective is
to minimize the length of a codeword sent to all receivers which allows each
receiver to learn its bit. For linear index coding, the minimum possible length
is known to be equal to a graph parameter called minrank (Bar-Yossef et al.,
FOCS, 2006).
  We show a polynomial time algorithm that, given an n vertex graph G with
minrank k, finds a linear index code for G of length $\widetilde{O}(n^{f(k)})$,
where f(k) depends only on k. For example, for k=3 we obtain f(3) ~ 0.2574. Our
algorithm employs a semidefinite program (SDP) introduced by Karger, Motwani
and Sudan (J. ACM, 1998) for graph coloring and its refined analysis due to
Arora, Chlamtac and Charikar (STOC, 2006). Since the SDP we use is not a
relaxation of the minimization problem we consider, a crucial component of our
analysis is an upper bound on the objective value of the SDP in terms of the
minrank.
  At the heart of our analysis lies a combinatorial result which may be of
independent interest. Namely, we show an exact expression for the maximum
possible value of the Lovasz theta-function of a graph with minrank k. This
yields a tight gap between two classical upper bounds on the Shannon capacity
of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1963</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1963</id><created>2011-07-11</created><updated>2012-04-25</updated><authors><author><keyname>Mundhenk</keyname><forenames>Martin</forenames></author><author><keyname>Weiss</keyname><forenames>Felix</forenames></author></authors><title>Intuitionistic implication makes model checking hard</title><categories>cs.CC</categories><comments>29 pages, 10 figures</comments><proxy>LMCS</proxy><acm-class>03B20, 68Q15, 68Q17</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (April 27,
  2012) lmcs:1160</journal-ref><doi>10.2168/LMCS-8(2:3)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of the model checking problem for
intuitionistic and modal propositional logics over transitive Kripke models.
More specific, we consider intuitionistic logic IPC, basic propositional logic
BPL, formal propositional logic FPL, and Jankov's logic KC. We show that the
model checking problem is P-complete for the implicational fragments of all
these intuitionistic logics. For BPL and FPL we reach P-hardness even on the
implicational fragment with only one variable. The same hardness results are
obtained for the strictly implicational fragments of their modal companions.
Moreover, we investigate whether formulas with less variables and additional
connectives make model checking easier. Whereas for variable free formulas
outside of the implicational fragment, FPL model checking is shown to be in
LOGCFL, the problem remains P-complete for BPL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1967</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1967</id><created>2011-07-11</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author></authors><title>A Novel Adaptive Routing through Fitness Function Estimation Technique
  with Multiple QoS Parameters Compliance</title><categories>cs.NI</categories><comments>4 pages, 3 figures, Fourth International Conference on Information
  Processing (ICIP- 2010), Proceedings pp 489-491, May 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a method which shows a significant improvement in
discovering the path over the distance vector protocol. The proposed method is
a multi-parameter QoS along with the fitness function which shows that it
overcomes the limitation of DV like routing loops by spanning tree approach,
count-to-infinity problem by decision attribute. The input considered is a
topology satisfying the QoS parameters of size 1 to 64 nodes and it was shown
that an optimal path selection was obtained efficiently over the classical
distance vector algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1970</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1970</id><created>2011-07-11</created><authors><author><keyname>Cherian</keyname><forenames>Mary</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>A Packet Scheduling Strategy in Sensor Networks with SGMH Protocol</title><categories>cs.NI</categories><comments>4 pages, 5 figures, IEEE , International Conference ICCCNT 2010,
  India, IEEE Explore- 2010, ISBN 978-1-4211-6591, pp 1-4, 30th September 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data communication in sensor networks can have timing constraints like end to
end deadlines. If the deadlines are not met either a catastrophe can happen in
hard real time systems or performance deterioration can occur in soft real time
systems. In real time sensor networks, the recovery of data through
retransmission should be minimized due to the stringent requirements on the
worst case time delays. This paper presents the application of Stop and Go
Multihop protocol (SGMH) at node level in wireless sensor networks for
scheduling and hence to meet the hard real time routing requirements. SGMH is a
distributed multihop packet delivery algorithm. The fractions of the total
available bandwidth on each channel is assigned to several traffic classes by
which the time it takes to traverse each of the hops from the source to the
destination is bounded. It is based on the notion of time frames (Tfr). In
sensor networks packets can have different delay guarantees. Multiple frame
sizes can be assigned for different traffic classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1972</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1972</id><created>2011-07-11</created><updated>2013-02-07</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Vogel</keyname><forenames>Christian</forenames></author></authors><title>Influence of Doppler Bin Width on GPS Acquisition Probabilities</title><categories>cs.IT math.IT</categories><comments>24 pages, 12 figures; submitted</comments><journal-ref>IEEE Trans. Aerosp. Electron. Syst, vol. 49, no. 4, p. 2570 -
  2584, 2013</journal-ref><doi>10.1109/TAES.2014.6619949</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acquisition is a search in two continuous dimensions, where the digital
algorithms require a partitioning of the search space into cells. Depending on
the partitioning of the Doppler frequency domain, more than one cell might
contain significant signal energy. We present an expression for the expected
values of the cells' energies to analyze the impact of the Doppler bin width on
detection and false alarm probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1974</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1974</id><created>2011-07-11</created><authors><author><keyname>Dinler</keyname><forenames>Ali</forenames></author><author><keyname>Hasan</keyname><forenames>Cengis</forenames></author><author><keyname>Orucoglu</keyname><forenames>Kamil</forenames></author><author><keyname>Barber</keyname><forenames>Robert W.</forenames></author></authors><title>On an Efficient Marie Curie Initial Training Network</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of the International Conference on Mathematical Finance
  and Economics (ICMFE-2011), Istanbul, Turkey, 6-8 July 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaboration in science is one of the key components of world-class
research. The European Commission supports collaboration between institutions
and funds young researchers appointed by these partner institutions. In these
networks, the mobility of the researchers is enforced in order to enhance the
collaboration. In this study, based on a real Marie Curie Initial Training
Network, an algorithm to construct a collaboration network is investigated. The
algorithm suggests that a strongly efficient expansion leads to a star-like
network. The results might help the design of efficient collaboration networks
for future Initial Training Network proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1987</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1987</id><created>2011-07-11</created><authors><author><keyname>Atanassov</keyname><forenames>Atanas Marinov</forenames></author></authors><title>Median Algorithm for Sector Spectra Calculation from Images Registered
  by the Spectral Airglow Temperature Imager</title><categories>physics.data-an cs.CV</categories><comments>6 pages, 5 figures, Sixth Scientific Conference &quot;Space Ecology
  Safety&quot; 2-4 November 2010, Sofia, Bulgaria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Spectral Airglow Temperature Imager is an instrument, specially designed
for investigation of the wave processes in the Mesosphere-Lower Thermosphere.
In order to determine the kinematic parameters of a wave, the values of a
physical quantity in different space points and their changes in the time
should be known. As a result of the possibilities of the SATI instrument for
space scanning, different parts of the images (sectors of spectrograms)
correspond to the respective mesopause areas (where the radiation is
generated). An approach is proposed for sector spectra determination from SATI
images based on ordered statistics instead of meaning. Comparative results are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.1999</identifier>
 <datestamp>2011-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.1999</id><created>2011-07-11</created><updated>2011-07-17</updated><authors><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Towards a Calculus of Object Programs</title><categories>cs.SE cs.LO cs.PL</categories><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verifying properties of object-oriented software requires a method for
handling references in a simple and intuitive way, closely related to how O-O
programmers reason about their programs. The method presented here, a Calculus
of Object Programs, combines four components: compositional logic, a framework
for describing program semantics and proving program properties; negative
variables to address the specifics of O-O programming, in particular qualified
calls; the alias calculus, which determines whether reference expressions can
ever have the same value; and the calculus of object structures, a
specification technique for the structures that arise during the execution of
an object-oriented program. The article illustrates the Calculus by proving the
standard algorithm for reversing a linked list.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2000</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2000</id><created>2011-07-11</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Schmied</keyname><forenames>Richard</forenames></author><author><keyname>Viehmann</keyname><forenames>Claus</forenames></author></authors><title>Tight Approximation Bounds for Vertex Cover on Dense k-Partite
  Hypergraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish almost tight upper and lower approximation bounds for the Vertex
Cover problem on dense k-partite hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2001</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2001</id><created>2011-07-11</created><authors><author><keyname>Thurley</keyname><forenames>Marc</forenames></author></authors><title>An Approximation Algorithm for #k-SAT</title><categories>cs.DS cs.CC</categories><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple randomized algorithm that approximates the number of
satisfying assignments of Boolean formulas in conjunctive normal form. To the
best of our knowledge this is the first algorithm which approximates #k-SAT for
any k &gt;= 3 within a running time that is not only non-trivial, but also
significantly better than that of the currently fastest exact algorithms for
the problem. More precisely, our algorithm is a randomized approximation scheme
whose running time depends polynomially on the error tolerance and is mildly
exponential in the number n of variables of the input formula. For example,
even stipulating sub-exponentially small error tolerance, the number of
solutions to 3-CNF input formulas can be approximated in time O(1.5366^n). For
4-CNF input the bound increases to O(1.6155^n).
  We further show how to obtain upper and lower bounds on the number of
solutions to a CNF formula in a controllable way. Relaxing the requirements on
the quality of the approximation, on k-CNF input we obtain significantly
reduced running times in comparison to the above bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2003</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2003</id><created>2011-07-11</created><updated>2011-07-12</updated><authors><author><keyname>Guo</keyname><forenames>Qi</forenames></author><author><keyname>Chen</keyname><forenames>Yunji</forenames></author><author><keyname>chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Li</keyname><forenames>Ling</forenames></author></authors><title>Efficient Deterministic Replay Using Complete Race Detection</title><categories>cs.PL cs.OS</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data races can significantly affect the executions of multi-threaded
programs. Hence, one has to recur the results of data races to
deterministically replay a multi-threaded program. However, data races are
concealed in enormous number of memory operations in a program. Due to the
difficulty of accurately identifying data races, previous multi-threaded
deterministic record/replay schemes for commodity multi-processor system give
up to record data races directly. Consequently, they either record all shared
memory operations, which brings remarkable slowdown to the production run, or
record the synchronization only, which introduces significant efforts to
replay.
  Inspired by the advances in data race detection, we propose an efficient
software-only deterministic replay scheme for commodity multi-processor
systems, which is named RacX. The key insight of RacX is as follows: although
it is NP-hard to accurately identify the existence of data races between a pair
of memory operations, we can find out all potential data races in a
multi-threaded program, in which the false positives can be reduced to a small
amount with our automatic false positive reduction techniques. As a result,
RacX can efficiently monitor all potential data races to deterministically
replay a multi-threaded program.
  To evaluate RacX, we have carried out experiments over a number of well-known
multi-threaded programs from SPLASH-2 benchmark suite and large-scale
commercial programs. RacX can precisely recur production runs of these programs
with value determinism. Averagely, RacX causes only about 1.21%, 1.89%, 2.20%,
and 8.41% slowdown to the original run during recording (for 2-, 4-, 8- and
16-thread programs, respectively). The soundness, efficiency, scalability, and
portability of RacX well demonstrate its superiority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2004</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2004</id><created>2011-07-11</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Grosse</keyname><forenames>Andree</forenames></author><author><keyname>Hengst</keyname><forenames>Stefan</forenames></author><author><keyname>Kautzsch</keyname><forenames>Lukas</forenames></author><author><keyname>Pohlmann</keyname><forenames>Andrej</forenames></author><author><keyname>Vortisch</keyname><forenames>Peter</forenames></author></authors><title>Quickest Paths in Simulations of Pedestrians</title><categories>physics.soc-ph cs.MA</categories><comments>revised version submitted</comments><journal-ref>Advances in Complex Systems 14(5) pp. 733-759 (2011)</journal-ref><doi>10.1142/S0219525911003281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution proposes a method to make agents in a microscopic
simulation of pedestrian traffic walk approximately along a path of estimated
minimal remaining travel time to their destination. Usually models of
pedestrian dynamics are (implicitly) built on the assumption that pedestrians
walk along the shortest path. Model elements formulated to make pedestrians
locally avoid collisions and intrusion into personal space do not produce
motion on quickest paths. Therefore a special model element is needed, if one
wants to model and simulate pedestrians for whom travel time matters most (e.g.
travelers in a station hall who are late for a train). Here such a model
element is proposed, discussed and used within the Social Force Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2006</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2006</id><created>2011-07-11</created><updated>2012-09-06</updated><authors><author><keyname>van der Schaft</keyname><forenames>A. J.</forenames></author><author><keyname>Maschke</keyname><forenames>B. M.</forenames></author></authors><title>Port-Hamiltonian systems on graphs</title><categories>math.OC cs.SY math.DS math.SG</categories><comments>45 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a unifying geometric and compositional framework for
modeling complex physical network dynamics as port-Hamiltonian systems on open
graphs. Basic idea is to associate with the incidence matrix of the graph a
Dirac structure relating the flow and effort variables associated to the edges,
internal vertices, as well as boundary vertices of the graph, and to formulate
energy-storing or energy-dissipating relations between the flow and effort
variables of the edges and internal vertices. This allows for state variables
associated to the edges, and formalizes the interconnection of networks.
Examples from different origins such as consensus algorithms are shown to share
the same structure. It is shown how the identified Hamiltonian structure offers
systematic tools for the analysis of the resulting dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2009</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2009</id><created>2011-07-11</created><updated>2012-01-03</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author></authors><title>Robustness of Structurally Equivalent Concurrent Parity Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player stochastic games played on a finite state space for an
infinite number of rounds. The games are concurrent: in each round, the two
players (player 1 and player 2) choose their moves independently and
simultaneously; the current state and the two moves determine a probability
distribution over the successor states. We also consider the important special
case of turn-based stochastic games where players make moves in turns, rather
than concurrently. We study concurrent games with \omega-regular winning
conditions specified as parity objectives. The value for player 1 for a parity
objective is the maximal probability with which the player can guarantee the
satisfaction of the objective against all strategies of the opponent. We study
the problem of continuity and robustness of the value function in concurrent
and turn-based stochastic parity gameswith respect to imprecision in the
transition probabilities. We present quantitative bounds on the difference of
the value function (in terms of the imprecision of the transition
probabilities) and show the value continuity for structurally equivalent
concurrent games (two games are structurally equivalent if the support of the
transition function is same and the probabilities differ). We also show
robustness of optimal strategies for structurally equivalent turn-based
stochastic parity games. Finally we show that the value continuity property
breaks without the structurally equivalent assumption (even for Markov chains)
and show that our quantitative bound is asymptotically optimal. Hence our
results are tight (the assumption is both necessary and sufficient) and optimal
(our quantitative bound is asymptotically optimal).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2018</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2018</id><created>2011-07-04</created><authors><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Wang</keyname><forenames>Kun-Yu</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Distributed Robust Multi-Cell Coordinated Beamforming with Imperfect
  CSI: An ADMM Approach</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2012.2188719</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-cell coordinated beamforming (MCBF), where multiple base stations (BSs)
collaborate with each other in the beamforming design for mitigating the
inter-cell interference, has been a subject drawing great attention recently.
Most MCBF designs assume perfect channel state information (CSI) of mobile
stations (MSs); however CSI errors are inevitable at the BSs in practice.
Assuming elliptically bounded CSI errors, this paper studies the robust MCBF
design problem that minimizes the weighted sum power of BSs subject to
worst-case signal-to-interference-plus-noise ratio (SINR) constraints on the
MSs. Our goal is to devise a distributed optimization method that can obtain
the worst-case robust beamforming solutions in a decentralized fashion, with
only local CSI used at each BS and little backhaul signaling for message
exchange between BSs. However, the considered problem is difficult to handle
even in the centralized form. We first propose an efficient approximation
method in the centralized form, based on the semidefinite relaxation (SDR)
technique. To obtain the robust beamforming solution in a decentralized
fashion, we further propose a distributed robust MCBF algorithm, using a
distributed convex optimization technique known as alternating direction method
of multipliers (ADMM). We analytically show the convergence of the proposed
distributed robust MCBF algorithm to the optimal centralized solution and its
better bandwidth efficiency in backhaul signaling over the existing dual
decomposition based algorithms. Simulation results are presented to examine the
effectiveness of the proposed SDR method and the distributed robust MCBF
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2021</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2021</id><created>2011-07-11</created><updated>2012-08-13</updated><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Multi-Instance Learning with Any Hypothesis Class</title><categories>cs.LG stat.ML</categories><comments>Fixed typos and added some explanations</comments><journal-ref>Journal of Machine Learning Research 13(Oct):1999-3039, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the supervised learning setting termed Multiple-Instance Learning (MIL),
the examples are bags of instances, and the bag label is a function of the
labels of its instances. Typically, this function is the Boolean OR. The
learner observes a sample of bags and the bag labels, but not the instance
labels that determine the bag labels. The learner is then required to emit a
classification rule for bags based on the sample. MIL has numerous
applications, and many heuristic algorithms have been used successfully on this
problem, each adapted to specific settings or applications. In this work we
provide a unified theoretical analysis for MIL, which holds for any underlying
hypothesis class, regardless of a specific application or problem domain. We
show that the sample complexity of MIL is only poly-logarithmically dependent
on the size of the bag, for any underlying hypothesis class. In addition, we
introduce a new PAC-learning algorithm for MIL, which uses a regular supervised
learning algorithm as an oracle. We prove that efficient PAC-learning for MIL
can be generated from any efficient non-MIL supervised learning algorithm that
handles one-sided error. The computational complexity of the resulting
algorithm is only polynomially dependent on the bag size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2027</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2027</id><created>2011-07-11</created><updated>2011-11-23</updated><authors><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yu</keyname><forenames>Huacheng</forenames></author></authors><title>On a Conjecture of Butler and Graham</title><categories>math.CO cs.DM</categories><comments>9 pages, typos corrected, submitted to journal</comments><msc-class>00A08 97A20 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a hat guessing problem proposed by Iwasawa \cite{Iwasawa10},
Butler and Graham \cite{Butler11} made the following conjecture on the
existence of certain way of marking the {\em coordinate lines} in $[k]^n$:
there exists a way to mark one point on each {\em coordinate line} in $[k]^n$,
so that every point in $[k]^n$ is marked exactly $a$ or $b$ times as long as
the parameters $(a,b,n,k)$ satisfies that there are non-negative integers $s$
and $t$ such that $s+t = k^n$ and $as+bt = nk^{n-1}$. In this paper we prove
this conjecture for any prime number $k$. Moreover, we prove the conjecture for
the case when $a=0$ for general $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2031</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2031</id><created>2011-07-11</created><authors><author><keyname>Nagaraja</keyname><forenames>Shishir</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Piyawongwisal</keyname><forenames>Pratch</forenames></author><author><keyname>Singh</keyname><forenames>Vijit</forenames></author><author><keyname>Agarwal</keyname><forenames>Pragya</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Stegobot: construction of an unobservable communication network
  leveraging social behavior</title><categories>cs.CR cs.NI cs.SI physics.soc-ph</categories><comments>Information Hiding, unobservability, anonymity, botnet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the construction of an unobservable communications network using
social networks. The communication endpoints are vertices on a social network.
Probabilistically unobservable communication channels are built by leveraging
image steganography and the social image sharing behavior of users. All
communication takes place along the edges of a social network overlay
connecting friends. We show that such a network can provide decent bandwidth
even with a far from optimal routing mechanism such as restricted flooding. We
show that such a network is indeed usable by constructing a botnet on top of
it, called Stegobot. It is designed to spread via social malware attacks and
steal information from its victims. Unlike conventional botnets, Stegobot
traffic does not introduce new communication endpoints between bots. We
analyzed a real-world dataset of image sharing between members of an online
social network. Analysis of Stegobot's network throughput indicates that
stealthy as it is, it is also functionally powerful -- capable of channeling
fair quantities of sensitive data from its victims to the botmaster at tens of
megabytes every month.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2033</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2033</id><created>2011-07-11</created><authors><author><keyname>Skutella</keyname><forenames>Martin</forenames></author><author><keyname>Williamson</keyname><forenames>David P.</forenames></author></authors><title>A note on the generalized min-sum set cover problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the generalized min-sum set cover problem,
introduced by Azar, Gamzu, and Yin. Bansal, Gupta, and Krishnaswamy give a
485-approximation algorithm for the problem. We are able to alter their
algorithm and analysis to obtain a 28-approximation algorithm, improving the
performance guarantee by an order of magnitude. We use concepts from
$\alpha$-point scheduling to obtain our improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2059</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2059</id><created>2011-07-11</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>J. A. Dom&#xed;nguez</forenames></author><author><keyname>Porras</keyname><forenames>J. M. Mu&#xf1;oz</forenames></author><author><keyname>Sotelo</keyname><forenames>G. Serrano</forenames></author></authors><title>One dimensional Convolutional Goppa Codes over the projective line</title><categories>cs.IT math.AG math.IT</categories><msc-class>94B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a general method to construct MDS one-dimensional convolutional
codes. Our method generalizes previous constructions of H. Gluesing-Luerssen
and B. Langfeld. Moreover we give a classification of one-dimensional
Convolutional Goppa Codes and propose a characterization of MDS codes of this
type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2085</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2085</id><created>2011-07-11</created><authors><author><keyname>Chertov</keyname><forenames>Oleg</forenames></author><author><keyname>Slipets</keyname><forenames>Taras</forenames></author></authors><title>Kunchenko's Polynomials for Template Matching</title><categories>cs.CV</categories><comments>3 pages</comments><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews Kunchenko's polynomials using as template matching method
to recognize template in one-dimensional input signal. Kunchenko's polynomials
method is compared with classical methods - cross-correlation and sum of
squared differences according to numerical statistical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2086</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2086</id><created>2011-07-11</created><authors><author><keyname>Marengo</keyname><forenames>Elisa</forenames></author><author><keyname>Baldoni</keyname><forenames>Matteo</forenames></author><author><keyname>Baroglio</keyname><forenames>Cristina</forenames></author></authors><title>Extend Commitment Protocols with Temporal Regulations: Why and How</title><categories>cs.AI</categories><comments>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI), pages 1-8
  (arXiv:1107.1686)</comments><report-no>RuleML-DC/2011/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposal of Elisa Marengo's thesis is to extend commitment protocols to
explicitly account for temporal regulations. This extension will satisfy two
needs: (1) it will allow representing, in a flexible and modular way, temporal
regulations with a normative force, posed on the interaction, so as to
represent conventions, laws and suchlike; (2) it will allow committing to
complex conditions, which describe not only what will be achieved but to some
extent also how. These two aspects will be deeply investigated in the proposal
of a unified framework, which is part of the ongoing work and will be included
in the thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2087</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2087</id><created>2011-07-11</created><authors><author><keyname>Woznowski</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Preece</keyname><forenames>Alun</forenames></author></authors><title>Rule-Based Semantic Sensing</title><categories>cs.AI</categories><comments>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI), pages 9-16
  (arXiv:1107.1686)</comments><report-no>RuleML-DC/2011/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rule-Based Systems have been in use for decades to solve a variety of
problems but not in the sensor informatics domain. Rules aid the aggregation of
low-level sensor readings to form a more complete picture of the real world and
help to address 10 identified challenges for sensor network middleware. This
paper presents the reader with an overview of a system architecture and a pilot
application to demonstrate the usefulness of a system integrating rules with
sensor middleware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2088</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2088</id><created>2011-07-11</created><authors><author><keyname>Weinzierl</keyname><forenames>Antonius</forenames></author></authors><title>Advancing Multi-Context Systems by Inconsistency Management</title><categories>cs.AI</categories><comments>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI), pages 17-24
  (arXiv:1107.1686)</comments><report-no>RuleML-DC/2011/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Context Systems are an expressive formalism to model (possibly)
non-monotonic information exchange between heterogeneous knowledge bases. Such
information exchange, however, often comes with unforseen side-effects leading
to violation of constraints, making the system inconsistent, and thus unusable.
Although there are many approaches to assess and repair a single inconsistent
knowledge base, the heterogeneous nature of Multi-Context Systems poses
problems which have not yet been addressed in a satisfying way: How to identify
and explain a inconsistency that spreads over multiple knowledge bases with
different logical formalisms (e.g., logic programs and ontologies)? What are
the causes of inconsistency if inference/information exchange is non-monotonic
(e.g., absent information as cause)? How to deal with inconsistency if access
to knowledge bases is restricted (e.g., companies exchange information, but do
not allow arbitrary modifications to their knowledge bases)? Many traditional
approaches solely aim for a consistent system, but automatic removal of
inconsistency is not always desireable. Therefore a human operator has to be
supported in finding the erroneous parts contributing to the inconsistency. In
my thesis those issues will be adressed mainly from a foundational perspective,
while our research project also provides algorithms and prototype
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2089</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2089</id><created>2011-07-11</created><authors><author><keyname>Bak</keyname><forenames>Jaroslaw</forenames></author></authors><title>Rule-based query answering method for a knowledge base of economic
  crimes</title><categories>cs.AI</categories><comments>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI), pages 25-32
  (arXiv:1107.1686)</comments><report-no>RuleML-DC/2011/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a description of the PhD thesis which aims to propose a rule-based
query answering method for relational data. In this approach we use an
additional knowledge which is represented as a set of rules and describes the
source data at concept (ontological) level. Queries are posed in the terms of
abstract level. We present two methods. The first one uses hybrid reasoning and
the second one exploits only forward chaining. These two methods are
demonstrated by the prototypical implementation of the system coupled with the
Jess engine. Tests are performed on the knowledge base of the selected economic
crimes: fraudulent disbursement and money laundering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2090</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2090</id><created>2011-07-11</created><authors><author><keyname>Sellner</keyname><forenames>Alexander</forenames></author><author><keyname>Schwarz</keyname><forenames>Christopher</forenames></author><author><keyname>Zinser</keyname><forenames>Erwin</forenames></author></authors><title>Semantic-ontological combination of Business Rules and Business
  Processes in IT Service Management</title><categories>cs.AI</categories><comments>Proceedings of the Doctoral Consortium and Poster Session of the 5th
  International Symposium on Rules (RuleML 2011@IJCAI), pages 33-40
  (arXiv:1107.1686)</comments><report-no>RuleML-DC/2011/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IT Service Management deals with managing a broad range of items related to
complex system environments. As there is both, a close connection to business
interests and IT infrastructure, the application of semantic expressions which
are seamlessly integrated within applications for managing ITSM environments,
can help to improve transparency and profitability. This paper focuses on the
challenges regarding the integration of semantics and ontologies within ITSM
environments. It will describe the paradigm of relationships and inheritance
within complex service trees and will present an approach of ontologically
expressing them. Furthermore, the application of SBVR-based rules as executable
SQL triggers will be discussed. Finally, the broad range of topics for further
research, derived from the findings, will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2091</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2091</id><created>2011-07-11</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Tracol</keyname><forenames>Mathieu</forenames></author></authors><title>Decidable Problems for Probabilistic Automata on Infinite Words</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider probabilistic automata on infinite words with acceptance defined
by parity conditions. We consider three qualitative decision problems: (i) the
positive decision problem asks whether there is a word that is accepted with
positive probability; (ii) the almost decision problem asks whether there is a
word that is accepted with probability 1; and (iii) the limit decision problem
asks whether for every epsilon &gt; 0 there is a word that is accepted with
probability at least 1 - epsilon. We unify and generalize several decidability
results for probabilistic automata over infinite words, and identify a robust
(closed under union and intersection) subclass of probabilistic automata for
which all the qualitative decision problems are decidable for parity
conditions. We also show that if the input words are restricted to lasso shape
(regular) words, then the positive and almost problems are decidable for all
probabilistic automata with parity conditions. For most decidable problems we
show an optimal PSPACE-complete complexity bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2100</identifier>
 <datestamp>2011-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2100</id><created>2011-07-11</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Interference Focusing for Simplified Optical Fiber Models with
  Dispersion</title><categories>cs.IT math.IT</categories><comments>To appear in ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discrete-time two-user interference channel model is developed that
captures non-linear phenomena that arise in optical fiber communication
employing wavelength-division multiplexing (WDM). The effect of non-linearity
is that an amplitude variation on one carrier induces a phase variation on the
other carrier. Moreover, the model captures the effect of group velocity
mismatch that introduces memory in the channel. It is shown that both users can
achieve the maximum pre-log factor of 1 simultaneously by using an interference
focusing technique introduced in an earlier work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2101</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2101</id><created>2011-07-11</created><updated>2013-01-09</updated><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Schreck</keyname><forenames>Jan</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Nearly Doubling the Throughput of Multiuser MIMO Systems Using Codebook
  Tailored Limited Feedback Protocol</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, vol.11, no.11,
  pp.3921-3931, 2012</journal-ref><doi>10.1109/TWC.2012.091812.111453</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and analyze a new robust feedback and transmit strategy for
multiuser MIMO downlink communication systems, termed Rate Approximation (RA).
RA combines the flexibility and robustness needed for reliable communications
with the user terminal under a limited feedback constraint. It responds to two
important observations. First, it is not so significant to approximate the
channel but rather the rate, such that the optimal scheduling decision can be
mimicked at the base station. Second, a fixed transmit codebook at the
transmitter is often better when therefore the channel state information is
more accurate. In the RA scheme the transmit and feedback codebook are
separated and user rates are delivered to the base station subject to a
controlled uniform error. The scheme is analyzed and proved to have better
performance below a certain interference plus noise margin and better behavior
than the classical Jindal formula. LTE system simulations sustain the analytic
results showing performance gains of up to 50% or 70% compared to zeroforcing
when using multiple antennas at the base station and multiple antennas or a
single antenna at the terminals, respectively. A new feedback protocol is
developed which inherently considers the transmit codebook and which is able to
deal with the complexity issue at the terminal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2104</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2104</id><created>2011-07-11</created><updated>2013-05-29</updated><authors><author><keyname>Velasco</keyname><forenames>Jon&#xe1;s</forenames></author><author><keyname>Saucedo-Espinosa</keyname><forenames>Mario A.</forenames></author><author><keyname>Escalante</keyname><forenames>Hugo Jair</forenames></author><author><keyname>Mendoza</keyname><forenames>Karlo</forenames></author><author><keyname>Villarreal-Rodr&#xed;guez</keyname><forenames>C&#xe9;sar Emilio</forenames></author><author><keyname>Chac&#xf3;n-Mondrag&#xf3;n</keyname><forenames>&#xd3;scar L.</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Adri&#xe1;n</forenames></author><author><keyname>Berrones</keyname><forenames>Arturo</forenames></author></authors><title>An estimation of distribution algorithm with adaptive Gibbs sampling for
  unconstrained global optimization</title><categories>cs.NE math.OC stat.ML</categories><comments>This paper has been withdrawn by the author by request of the journal
  in which has been accepted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper is proposed a new heuristic approach belonging to the field of
evolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds a
probability model and a set of solutions is sampled from the model which
characterizes the distribution of such solutions. The main framework of the
proposed method is an estimation of distribution algorithm, in which an
adaptive Gibbs sampling is used to generate new promising solutions and, in
combination with a local search strategy, it improves the individual solutions
produced in each iteration. The Estimation of Distribution Algorithm with
Adaptive Gibbs Sampling we are proposing in this paper is called AGEDA. We
experimentally evaluate and compare this algorithm against two deterministic
procedures and several stochastic methods in three well known test problems for
unconstrained global optimization. It is empirically shown that our heuristic
is robust in problems that involve three central aspects that mainly determine
the difficulty of global optimization problems, namely high-dimensionality,
multi-modality and non-smoothness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2105</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2105</id><created>2011-07-11</created><updated>2011-07-12</updated><authors><author><keyname>Angel</keyname><forenames>Eric</forenames></author><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Kacem</keyname><forenames>Fadi</forenames></author><author><keyname>Letsios</keyname><forenames>Dimitrios</forenames></author></authors><title>Speed Scaling on Parallel Processors with Migration</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of scheduling a set of jobs with release dates,
deadlines and processing requirements (or works), on parallel speed-scaled
processors so as to minimize the total energy consumption. We consider that
both preemption and migration of jobs are allowed. An exact polynomial-time
algorithm has been proposed for this problem, which is based on the Ellipsoid
algorithm. Here, we formulate the problem as a convex program and we propose a
simpler polynomial-time combinatorial algorithm which is based on a reduction
to the maximum flow problem. Our algorithm runs in $O(nf(n)logP)$ time, where
$n$ is the number of jobs, $P$ is the range of all possible values of
processors' speeds divided by the desired accuracy and $f(n)$ is the complexity
of computing a maximum flow in a layered graph with O(n) vertices.
Independently, Albers et al. \cite{AAG11} proposed an $O(n^2f(n))$-time
algorithm exploiting the same relation with the maximum flow problem. We extend
our algorithm to the multiprocessor speed scaling problem with migration where
the objective is the minimization of the makespan under a budget of energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2126</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2126</id><created>2011-07-11</created><updated>2011-08-21</updated><authors><author><keyname>Amrahov</keyname><forenames>&#x15e;ahin Emrah</forenames></author><author><keyname>Askerzade</keyname><forenames>Iman N.</forenames></author></authors><title>Strong Solutions of the Fuzzy Linear Systems</title><categories>cs.NA cs.AI cs.IT math.IT math.LO math.NA</categories><comments>11 pages</comments><msc-class>03B52, 03E72</msc-class><journal-ref>CMES: Computer Modeling in Engineering &amp; Sciences, Vol. 76, No. 4,
  pp. 207-216, 2011</journal-ref><doi>10.3970/cmes.2011.076.207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fuzzy linear system with crisp coefficient matrix and with an
arbitrary fuzzy number in parametric form on the right-hand side. It is known
that the well-known existence and uniqueness theorem of a strong fuzzy solution
is equivalent to the following: The coefficient matrix is the product of a
permutation matrix and a diagonal matrix. This means that this theorem can be
applicable only for a special form of linear systems, namely, only when the
system consists of equations, each of which has exactly one variable. We prove
an existence and uniqueness theorem, which can be use on more general systems.
The necessary and sufficient conditions of the theorem are dependent on both
the coefficient matrix and the right-hand side. This theorem is a
generalization of the well-known existence and uniqueness theorem for the
strong solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2132</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2132</id><created>2011-07-11</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>de Alfaro</keyname><forenames>Luca</forenames></author><author><keyname>Roy</keyname><forenames>Pritam</forenames></author></authors><title>Magnifying Lens Abstraction for Stochastic Games with Discounted and
  Long-run Average Objectives</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turn-based stochastic games and its important subclass Markov decision
processes (MDPs) provide models for systems with both probabilistic and
nondeterministic behaviors. We consider turn-based stochastic games with two
classical quantitative objectives: discounted-sum and long-run average
objectives. The game models and the quantitative objectives are widely used in
probabilistic verification, planning, optimal inventory control, network
protocol and performance analysis. Games and MDPs that model realistic systems
often have very large state spaces, and probabilistic abstraction techniques
are necessary to handle the state-space explosion. The commonly used
full-abstraction techniques do not yield space-savings for systems that have
many states with similar value, but does not necessarily have similar
transition structure. A semi-abstraction technique, namely Magnifying-lens
abstractions (MLA), that clusters states based on value only, disregarding
differences in their transition relation was proposed for qualitative
objectives (reachability and safety objectives). In this paper we extend the
MLA technique to solve stochastic games with discounted-sum and long-run
average objectives. We present the MLA technique based abstraction-refinement
algorithm for stochastic games and MDPs with discounted-sum objectives. For
long-run average objectives, our solution works for all MDPs and a sub-class of
stochastic games where every state has the same value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2141</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2141</id><created>2011-07-11</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author></authors><title>Partial-Observation Stochastic Games: How to Win when Belief Fails</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two-player finite-state stochastic games of partial observation on graphs,
in every state of the graph, the players simultaneously choose an action, and
their joint actions determine a probability distribution over the successor
states. We consider reachability objectives where player 1 tries to ensure a
target state to be visited almost-surely or positively. On the basis of
information, the game can be one-sided with either (a)player 1 or (b)player 2
having partial observation, or two-sided with both players having partial
observation. On the basis of randomization (a)players may not be allowed to use
randomization (pure strategies), or (b)may choose a probability distribution
over actions but the actual random choice is not visible (actions invisible),
or (c)may use full randomization. Our results for pure strategies are as
follows: (1)For one-sided games with player 2 perfect observation we show that
belief-based strategies are not sufficient, and present an exponential upper
bound on memory both for almost-sure and positive winning strategies; we show
that the problem of deciding the existence of almost-sure and positive winning
strategies for player 1 is EXPTIME-complete and present symbolic algorithms
that avoid the explicit exponential construction. (2)For one-sided games with
player 1 perfect observation we show that non-elementary memory is both
necessary and sufficient for both almost-sure and positive winning strategies.
(3)We show that for the two-sided case finite memory strategies are sufficient
for both positive and almost-sure winning. We establish the equivalence of the
almost-sure winning problem for pure strategies with randomized strategies with
actions invisible. Our equivalence result exhibit serious flaws in previous
results in the literature: we show a non-elementary memory lower bound for
almost-sure winning whereas an exponential upper bound was claimed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2146</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2146</id><created>2011-07-11</created><updated>2013-06-20</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author></authors><title>Bounded Rationality in Concurrent Parity Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider 2-player games played on a finite state space for infinite
rounds. The games are concurrent: in each round, the two players choose their
moves simultaneously; the current state and the moves determine the successor.
We consider omega-regular winning conditions given as parity objectives. We
consider the qualitative analysis problems: the computation of the almost-sure
and limit-sure winning set of states, where player 1 can ensure to win with
probability 1 and with probability arbitrarily close to 1, respectively. In
general the almost-sure and limit-sure winning strategies require both
infinite-memory and infinite-precision. We study the bounded-rationality
problem for qualitative analysis of concurrent parity games, where the strategy
set player 1 is restricted to bounded-resource strategies. In terms of
precision, strategies can be deterministic, uniform, finite-precision or
infinite-precision; and in terms of memory, strategies can be memoryless,
finite-memory or infinite-memory. We present a precise and complete
characterization of the qualitative winning sets for all combinations of
classes of strategies. In particular, we show that uniform memoryless
strategies are as powerful as finite-precision infinite-memory strategies, and
infinite-precision memoryless strategies are as powerful as infinite-precision
finite-memory strategies. We show that the winning sets can be computed in
O(n^{2d+3}) time, where n is the size of the game and 2d is the number of
priorities, and our algorithms are symbolic. The membership problem of whether
a state belongs to a winning set can be decided in NP cap coNP. While this
complexity is the same as for the simpler class of turn-based games, where in
each state only one of the players has a choice of moves, our algorithms, that
are obtained by characterization of the winning sets as mu-calculus formulas,
are considerably more involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2157</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2157</id><created>2011-07-11</created><authors><author><keyname>Sottile</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rasmussen</keyname><forenames>Craig E</forenames></author><author><keyname>Weseloh</keyname><forenames>Wayne N.</forenames></author><author><keyname>Robey</keyname><forenames>Robert W.</forenames></author><author><keyname>Quinlan</keyname><forenames>Daniel</forenames></author><author><keyname>Overbey</keyname><forenames>Jeffrey</forenames></author></authors><title>ForOpenCL: Transformations Exploiting Array Syntax in Fortran for
  Accelerator Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging GPU architectures for high performance computing are well suited to
a data-parallel programming model. This paper presents preliminary work
examining a programming methodology that provides Fortran programmers with
access to these emerging systems. We use array constructs in Fortran to show
how this infrequently exploited, standardized language feature is easily
transformed to lower-level accelerator code. The transformations in ForOpenCL
are based on a simple mapping from Fortran to OpenCL. We demonstrate, using a
stencil code solving the shallow-water fluid equations, that the performance of
the ForOpenCL compiler-generated transformations is comparable with that of
hand-optimized OpenCL code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2168</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2168</id><created>2011-07-11</created><authors><author><keyname>Ellison</keyname><forenames>Christopher J.</forenames></author><author><keyname>Mahoney</keyname><forenames>John R.</forenames></author><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author><author><keyname>Reichardt</keyname><forenames>Joerg</forenames></author></authors><title>Information Symmetries in Irreversible Processes</title><categories>cond-mat.stat-mech cs.IT math.IT math.ST nlin.CD stat.TH</categories><comments>32 pages, 17 figures, 2 tables;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/pratisp2.htm</comments><doi>10.1063/1.3637490</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study dynamical reversibility in stationary stochastic processes from an
information theoretic perspective. Extending earlier work on the reversibility
of Markov chains, we focus on finitary processes with arbitrarily long
conditional correlations. In particular, we examine stationary processes
represented or generated by edge-emitting, finite-state hidden Markov models.
Surprisingly, we find pervasive temporal asymmetries in the statistics of such
stationary processes with the consequence that the computational resources
necessary to generate a process in the forward and reverse temporal directions
are generally not the same. In fact, an exhaustive survey indicates that most
stationary processes are irreversible. We study the ensuing relations between
model topology in different representations, the process's statistical
properties, and its reversibility in detail. A process's temporal asymmetry is
efficiently captured using two canonical unifilar representations of the
generating model, the forward-time and reverse-time epsilon-machines. We
analyze example irreversible processes whose epsilon-machine presentations
change size under time reversal, including one which has a finite number of
recurrent causal states in one direction, but an infinite number in the
opposite. From the forward-time and reverse-time epsilon-machines, we are able
to construct a symmetrized, but nonunifilar, generator of a process---the
bidirectional machine. Using the bidirectional machine, we show how to directly
calculate a process's fundamental information properties, many of which are
otherwise only poorly approximated via process samples. The tools we introduce
and the insights we offer provide a better understanding of the many facets of
reversibility and irreversibility in stochastic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2183</identifier>
 <datestamp>2011-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2183</id><created>2011-07-11</created><updated>2011-12-21</updated><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author></authors><title>Lower bounds in differential privacy</title><categories>cs.CR cs.CC</categories><comments>Corrected some minor errors and typos. To appear in Theory of
  Cryptography Conference (TCC) 2012</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a paper about private data analysis, in which a trusted curator
holding a confidential database responds to real vector-valued queries. A
common approach to ensuring privacy for the database elements is to add
appropriately generated random noise to the answers, releasing only these {\em
noisy} responses. In this paper, we investigate various lower bounds on the
noise required to maintain different kind of privacy guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2188</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2188</id><created>2011-07-12</created><updated>2013-02-26</updated><authors><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Yajun</forenames></author></authors><title>The Simulated Greedy Algorithm for Several Submodular Matroid Secretary
  Problems</title><categories>cs.DS</categories><comments>preliminary version appeared in STACS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the matroid secretary problems with submodular valuation functions.
In these problems, the elements arrive in random order. When one element
arrives, we have to make an immediate and irrevocable decision on whether to
accept it or not. The set of accepted elements must form an {\em independent
set} in a predefined matroid. Our objective is to maximize the value of the
accepted elements. In this paper, we focus on the case that the valuation
function is a non-negative and monotonically non-decreasing submodular
function.
  We introduce a general algorithm for such {\em submodular matroid secretary
problems}. In particular, we obtain constant competitive algorithms for the
cases of laminar matroids and transversal matroids. Our algorithms can be
further applied to any independent set system defined by the intersection of a
{\em constant} number of laminar matroids, while still achieving constant
competitive ratios. Notice that laminar matroids generalize uniform matroids
and partition matroids.
  On the other hand, when the underlying valuation function is linear, our
algorithm achieves a competitive ratio of 9.6 for laminar matroids, which
significantly improves the previous result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2221</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2221</id><created>2011-07-12</created><updated>2011-11-07</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Bidimensionality and Geometric Graphs</title><categories>cs.DS cs.CG math.CO</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use several of the key ideas from Bidimensionality to give a
new generic approach to design EPTASs and subexponential time parameterized
algorithms for problems on classes of graphs which are not minor closed, but
instead exhibit a geometric structure. In particular we present EPTASs and
subexponential time parameterized algorithms for Feedback Vertex Set, Vertex
Cover, Connected Vertex Cover, Diamond Hitting Set, on map graphs and unit disk
graphs, and for Cycle Packing and Minimum-Vertex Feedback Edge Set on unit disk
graphs. Our results are based on the recent decomposition theorems proved by
Fomin et al [SODA 2011], and our algorithms work directly on the input graph.
Thus it is not necessary to compute the geometric representations of the input
graph. To the best of our knowledge, these results are previously unknown, with
the exception of the EPTAS and a subexponential time parameterized algorithm on
unit disk graphs for Vertex Cover, which were obtained by Marx [ESA 2005] and
Alber and Fiala [J. Algorithms 2004], respectively.
  We proceed to show that our approach can not be extended in its full
generality to more general classes of geometric graphs, such as intersection
graphs of unit balls in R^d, d &gt;= 3. Specifically we prove that Feedback Vertex
Set on unit-ball graphs in R^3 neither admits PTASs unless P=NP, nor
subexponential time algorithms unless the Exponential Time Hypothesis fails.
Additionally, we show that the decomposition theorems which our approach is
based on fail for disk graphs and that therefore any extension of our results
to disk graphs would require new algorithmic ideas. On the other hand, we prove
that our EPTASs and subexponential time algorithms for Vertex Cover and
Connected Vertex Cover carry over both to disk graphs and to unit-ball graphs
in R^d for every fixed d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2222</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2222</id><created>2011-07-12</created><authors><author><keyname>Arsinte</keyname><forenames>Radu</forenames></author></authors><title>Study of a Hybrid - Analog TV and Ethernet- Home Data Link using a
  Coaxial Cable</title><categories>cs.MM cs.NI</categories><comments>5 pages, 12 figures; Acta Technica Napocensis, Electronics and
  telecommunications, No.1/2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an implementation and compatibility tests of a simple home
network implemented in a nonconventional manner using a CATV coaxial cable.
Reusing the cable, normally designated to supply RF modulated TV signals from
cable TV networks, makes possible to add data services as well. A short
presentation of the technology is given with an investigation of the main
performances obtained using this technique. The measurements revealed that this
simple solution makes possible to have both TV and data services with
performances close to traditional home data services: cable modems or ADSL,
with minimal investments. This technology keeps also open the possibility for
future improvements of the network: DVB-C or Data via Cable Modems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2229</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2229</id><created>2011-07-12</created><authors><author><keyname>Olmos</keyname><forenames>Pablo M.</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Scaling Behavior of Convolutional LDPC Ensembles over the BEC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the scaling behavior of coupled sparse graph codes over the binary
erasure channel. In particular, let 2L+1 be the length of the coupled chain,
let M be the number of variables in each of the 2L + 1 local copies, let l be
the number of iterations, let Pb denote the bit error probability, and let
{\epsilon} denote the channel parameter. We are interested in how these
quantities scale when we let the blocklength (2L + 1)M tend to infinity. Based
on empirical evidence we show that the threshold saturation phenomenon is
rather stable with respect to the scaling of the various parameters and we
formulate some general rules of thumb which can serve as a guide for the design
of coding systems based on coupled graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2231</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2231</id><created>2011-07-12</created><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Neininger</keyname><forenames>Ralph</forenames></author><author><keyname>Sulzbach</keyname><forenames>Henning</forenames></author></authors><title>Partial match queries in random quadtrees</title><categories>math.PR cs.DS math.CO</categories><comments>12 pages, 2 figures</comments><msc-class>05A16, 05A15, 05C05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering items matching a partially specified
pattern in multidimensional trees (quad trees and k-d trees). We assume the
traditional model where the data consist of independent and uniform points in
the unit square. For this model, in a structure on $n$ points, it is known that
the number of nodes $C_n(\xi)$ to visit in order to report the items matching
an independent and uniformly on $[0,1]$ random query $\xi$ satisfies
$\Ec{C_n(\xi)}\sim \kappa n^{\beta}$, where $\kappa$ and $\beta$ are explicit
constants. We develop an approach based on the analysis of the cost $C_n(x)$ of
any fixed query $x\in [0,1]$, and give precise estimates for the variance and
limit distribution of the cost $C_n(x)$. Our results permit to describe a limit
process for the costs $C_n(x)$ as $x$ varies in $[0,1]$; one of the
consequences is that $E{\max_{x\in [0,1]} C_n(x)} \sim \gamma n^\beta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2248</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2248</id><created>2011-07-12</created><updated>2011-11-11</updated><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Fanelli</keyname><forenames>Angelo</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Skopalik</keyname><forenames>Alexander</forenames></author></authors><title>Approximate Pure Nash Equilibria in Weighted Congestion Games:
  Existence, Efficient Computation, and Structure</title><categories>cs.GT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider structural and algorithmic questions related to the Nash dynamics
of weighted congestion games. In weighted congestion games with linear latency
functions, the existence of (pure Nash) equilibria is guaranteed by potential
function arguments. Unfortunately, this proof of existence is inefficient and
computing equilibria is such games is a {\sf PLS}-hard problem. The situation
gets worse when superlinear latency functions come into play; in this case, the
Nash dynamics of the game may contain cycles and equilibria may not even exist.
Given these obstacles, we consider approximate equilibria as alternative
solution concepts. Do such equilibria exist? And if so, can we compute them
efficiently?
  We provide positive answers to both questions for weighted congestion games
with polynomial latency functions by exploiting an &quot;approximation&quot; of such
games by a new class of potential games that we call $\Psi$-games. This allows
us to show that these games have $d!$-approximate equilibria, where $d$ is the
maximum degree of the latency functions. Our main technical contribution is an
efficient algorithm for computing O(1)-approximate equilibria when $d$ is a
constant. For games with linear latency functions, the approximation guarantee
is $\frac{3+\sqrt{5}}{2}+O(\gamma)$ for arbitrarily small $\gamma&gt;0$; for
latency functions with maximum degree $d\geq 2$, it is $d^{2d+o(d)}$. The
running time is polynomial in the number of bits in the representation of the
game and $1/\gamma$. As a byproduct of our techniques, we also show the
following structural statement for weighted congestion games with polynomial
latency functions of maximum degree $d\geq 2$: polynomially-long sequences of
best-response moves from any initial state to a $d^{O(d^2)}$-approximate
equilibrium exist and can be efficiently identified in such games as long as
$d$ is constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2256</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2256</id><created>2011-07-12</created><updated>2014-02-18</updated><authors><author><keyname>Diaz</keyname><forenames>Josep</forenames></author><author><keyname>Pottonen</keyname><forenames>Olli</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>On the Complexity of Metric Dimension</title><categories>cs.CC</categories><comments>v4: major rewrite in order to fix significant errors. Submitted for
  publication</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metric dimension of a graph $G$ is the size of a smallest subset $L
\subseteq V(G)$ such that for any $x,y \in V(G)$ there is a $z \in L$ such that
the graph distance between $x$ and $z$ differs from the graph distance between
$y$ and $z$. Even though this notion has been part of the literature for almost
40 years, the computational complexity of determining the Metric Dimension of a
graph is still very unclear. Essentially, we only know the problem to be
NP-hard for general graphs, to be polynomial-time solvable on trees, and to
have a $\log n$-approximation algorithm for general graphs. In this paper, we
show tight complexity boundaries for the Metric Dimension problem. We achieve
this by giving two complementary results. First, we show that the Metric
Dimension problem on bounded-degree planar graphs is NP-complete. Then, we give
a polynomial-time algorithm for determining the metric dimension of outerplanar
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2284</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2284</id><created>2011-07-12</created><authors><author><keyname>Xu</keyname><forenames>Wenyan</forenames></author><author><keyname>Liu</keyname><forenames>Sanyang</forenames></author></authors><title>The parallel versus branching recurrences in computability logic</title><categories>cs.LO</categories><comments>14 pages</comments><journal-ref>Notre Dame J. Formal Logic 54, no. 1 (2013), 61-78</journal-ref><doi>10.1215/00294527-1731389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the basic logic induced by the parallel recurrence of
Computability Logic is a proper superset of the basic logic induced by the
branching recurrence. The latter is known to be precisely captured by the
cirquent calculus system CL15, conjectured by Japaridze to remain sound---but
not complete---with parallel recurrence instead of branching recurrence. The
present result is obtained by positively verifying that conjecture. A secondary
result of the paper is showing that parallel recurrence is strictly weaker than
branching recurrence in the sense that, while the latter logically implies the
former, vice versa does not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2299</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2299</id><created>2011-07-12</created><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Wilfong</keyname><forenames>Gordon</forenames></author></authors><title>iBGP and Constrained Connectivity</title><categories>cs.DS cs.NI</categories><comments>27 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the theoretical study of the problem of minimizing the size of an
iBGP overlay in an Autonomous System (AS) in the Internet subject to a natural
notion of correctness derived from the standard &quot;hot-potato&quot; routing rules. For
both natural versions of the problem (where we measure the size of an overlay
by either the number of edges or the maximum degree) we prove that it is
NP-hard to approximate to a factor better than $\Omega(\log n)$ and provide
approximation algorithms with ratio $\tilde{O}(\sqrt{n})$. In addition, we give
a slightly worse $\tilde{O}(n^{2/3})$-approximation based on primal-dual
techniques that has the virtue of being both fast and good in practice, which
we show via simulations on the actual topologies of five large Autonomous
Systems.
  The main technique we use is a reduction to a new connectivity-based network
design problem that we call Constrained Connectivity. In this problem we are
given a graph $G=(V,E)$, and for every pair of vertices $u,v \in V$ we are
given a set $S(u,v) \subseteq V$ called the safe set of the pair. The goal is
to find the smallest subgraph $H$ of $G$ in which every pair of vertices $u,v$
is connected by a path contained in $S(u,v)$. We show that the iBGP problem can
be reduced to the special case of Constrained Connectivity where $G = K_n$ and
safe sets are defined geometrically based on the IGP distances in the AS. We
also believe that Constrained Connectivity is an interesting problem in its own
right, so provide stronger hardness results ($2^{\log^{1-\epsilon} n}$-hardness
of approximation) and integrality gaps ($n^{1/3 - \epsilon}$) for the general
case. On the positive side, we show that Constrained Connectivity turns out to
be much simpler for some interesting special cases other than iBGP: when safe
sets are symmetric and hierarchical, we give a polynomial time algorithm that
computes an optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2312</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2312</id><created>2011-07-12</created><updated>2011-07-13</updated><authors><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Aronov</keyname><forenames>Boris</forenames><affiliation>NYU-Poly</affiliation></author></authors><title>Computing the Distance between Piecewise-Linear Bivariate Functions</title><categories>cs.CG cs.DS cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the distance between two
piecewise-linear bivariate functions $f$ and $g$ defined over a common domain
$M$. We focus on the distance induced by the $L_2$-norm, that is
$\|f-g\|_2=\sqrt{\iint_M (f-g)^2}$. If $f$ is defined by linear interpolation
over a triangulation of $M$ with $n$ triangles, while $g$ is defined over
another such triangulation, the obvious na\&quot;ive algorithm requires
$\Theta(n^2)$ arithmetic operations to compute this distance. We show that it
is possible to compute it in $\O(n\log^4 n)$ arithmetic operations, by reducing
the problem to multi-point evaluation of a certain type of polynomials. We also
present an application to terrain matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2321</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2321</id><created>2011-07-12</created><updated>2012-04-05</updated><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Quintin</keyname><forenames>Guillaume</forenames></author></authors><title>An algorithm for list decoding number field codes</title><categories>math.NT cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for list decoding codewords of algebraic number field
codes in polynomial time. This is the first explicit procedure for decoding
number field codes whose construction were previously described by Lenstra and
Guruswami. We rely on an equivalent of the LLL reduction algorithm for
$\OK$-modules due to Fieker and Stehl\'e and on algorithms due to Cohen for
computing the Hermite normal form of matrices representing modules over
Dedekind domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2336</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2336</id><created>2011-07-12</created><authors><author><keyname>Nikolaidis</keyname><forenames>N. S.</forenames></author><author><keyname>Nikolaidis</keyname><forenames>I. N.</forenames></author><author><keyname>Tsouros</keyname><forenames>C. C.</forenames></author></authors><title>A Variation of the Box-Counting Algorithm Applied to Colour Images</title><categories>cs.CV</categories><comments>10 pages, 3 figures</comments><msc-class>28A78, 28A80</msc-class><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The box counting method for fractal dimension estimation had not been applied
to large or colour images thus far due to the processing time required. In this
letter we present a fast, easy to implement and very easily expandable to any
number of dimensions variation, the box merging method. It is applied here in
RGB images which are considered as sets in 5-D space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2347</identifier>
 <datestamp>2011-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2347</id><created>2011-07-12</created><authors><author><keyname>Pendse</keyname><forenames>Gautam V.</forenames></author></authors><title>BSVM: A Banded Suport Vector Machine</title><categories>stat.ML cs.CV</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a novel binary classification technique called Banded SVM
(B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the
decision rule is encouraged to lie in the interval [1, \infty]. The new B-SVM
objective function contains a penalty term that encourages the decision rule to
lie in a user specified range [\rho_1, \rho_2]. In addition to the standard set
of support vectors (SVs) near the class boundaries, B-SVM results in a second
set of SVs in the interior of each class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2353</identifier>
 <datestamp>2011-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2353</id><created>2011-07-12</created><authors><author><keyname>Bickel</keyname><forenames>David R.</forenames></author></authors><title>Blending Bayesian and frequentist methods according to the precision of
  prior information with an application to hypothesis testing</title><categories>stat.ME cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following zero-sum game between nature and a statistician blends Bayesian
methods with frequentist methods such as p-values and confidence intervals.
Nature chooses a posterior distribution consistent with a set of possible
priors. At the same time, the statistician selects a parameter distribution for
inference with the goal of maximizing the minimum Kullback-Leibler information
gained over a confidence distribution or other benchmark distribution. An
application to testing a simple null hypothesis leads the statistician to
report a posterior probability of the hypothesis that is informed by both
Bayesian and frequentist methodology, each weighted according how well the
prior is known.
  Since neither the Bayesian approach nor the frequentist approach is entirely
satisfactory in situations involving partial knowledge of the prior
distribution, the proposed procedure reduces to a Bayesian method given
complete knowledge of the prior, to a frequentist method given complete
ignorance about the prior, and to a blend between the two methods given partial
knowledge of the prior. The blended approach resembles the Bayesian method
rather than the frequentist method to the precise extent that the prior is
known.
  The problem of testing a point null hypothesis illustrates the proposed
framework. The blended probability that the null hypothesis is true is equal to
the p-value or a lower bound of an unknown Bayesian posterior probability,
whichever is greater. Thus, given total ignorance represented by a lower bound
of 0, the p-value is used instead of any Bayesian posterior probability. At the
opposite extreme of a known prior, the p-value is ignored. In the intermediate
case, the possible Bayesian posterior probability that is closest to the
p-value is used for inference. Thus, both the Bayesian method and the
frequentist method influence the inferences made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2365</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2365</id><created>2011-07-12</created><authors><author><keyname>Das</keyname><forenames>Smarajit</forenames></author><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>Siddharth</forenames></author></authors><title>On some special cases of the Entropy Photon-Number Inequality</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, no figures</comments><journal-ref>Proceedings of Theory of Quantum Computation, Communication, and
  Cryptography (TQC), Tokyo, Japan, May 2012, vol. 7582, Lecture Notes in
  Computer Science</journal-ref><doi>10.1007/978-3-642-35656-8_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Entropy Photon-Number Inequality (EPnI) holds where one of
the input states is the vacuum state and for several candidates of the other
input state that includes the cases when the state has the eigenvectors as the
number states and either has only two non-zero eigenvalues or has arbitrary
number of non-zero eigenvalues but is a high entropy state. We also discuss the
conditions, which if satisfied, would lead to an extension of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2368</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2368</id><created>2011-07-12</created><updated>2011-10-05</updated><authors><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Srivastava</keyname><forenames>Piyush</forenames></author><author><keyname>Thurley</keyname><forenames>Marc</forenames></author></authors><title>Approximation algorithms for two-state anti-ferromagnetic spin systems
  on bounded degree graphs</title><categories>cs.DM cs.CC cs.DS</categories><comments>1 figure. Final Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a seminal paper (Weitz, 2006), Weitz gave a deterministic fully polynomial
approximation scheme for count- ing exponentially weighted independent sets
(equivalently, approximating the partition function of the hard-core model from
statistical physics) on graphs of degree at most d, up to the critical activity
for the uniqueness of the Gibbs measure on the infinite d-regular tree. More
recently Sly (Sly, 2010) showed that this is optimal in the sense that if there
is an FPRAS for the hard-core partition function on graphs of maximum degree d
for activities larger than the critical activity on the infinite d-regular tree
then NP = RP. In this paper, we extend Weitz's approach to derive a
deterministic fully polynomial approx- imation scheme for the partition
function of the anti-ferromagnetic Ising model with arbitrary field on graphs
of maximum degree d, up to the corresponding critical point on the d-regular
tree. The main ingredient of our result is a proof that for two-state
anti-ferromagnetic spin systems on the d-regular tree, weak spatial mixing
implies strong spatial mixing. This in turn uses a message-decay argument which
extends a similar approach proposed recently for the hard-core model by
Restrepo et al (Restrepo et al, 2011) to the case of the anti-ferromagnetic
Ising model with arbitrary field. By a standard correspondence, these results
translate to arbitrary two-state anti-ferromagnetic spin systems with soft
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2379</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2379</id><created>2011-07-12</created><updated>2014-08-29</updated><authors><author><keyname>Ben-David</keyname><forenames>Shalev</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author></authors><title>Data Stability in Clustering: A Closer Look</title><categories>cs.LG cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the model introduced by Bilu and Linial (2010), who study
problems for which the optimal clustering does not change when distances are
perturbed. They show that even when a problem is NP-hard, it is sometimes
possible to obtain efficient algorithms for instances resilient to certain
multiplicative perturbations, e.g. on the order of $O(\sqrt{n})$ for max-cut
clustering. Awasthi et al. (2010) consider center-based objectives, and Balcan
and Liang (2011) analyze the $k$-median and min-sum objectives, giving
efficient algorithms for instances resilient to certain constant multiplicative
perturbations.
  Here, we are motivated by the question of to what extent these assumptions
can be relaxed while allowing for efficient algorithms. We show there is little
room to improve these results by giving NP-hardness lower bounds for both the
$k$-median and min-sum objectives. On the other hand, we show that constant
multiplicative resilience parameters can be so strong as to make the clustering
problem trivial, leaving only a narrow range of resilience parameters for which
clustering is interesting. We also consider a model of additive perturbations
and give a correspondence between additive and multiplicative notions of
stability. Our results provide a close examination of the consequences of
assuming stability in data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2382</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2382</id><created>2011-07-12</created><updated>2012-03-05</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ozlen</keyname><forenames>Melih</forenames></author></authors><title>Computing the crosscap number of a knot using integer programming and
  normal surfaces</title><categories>math.GT cs.CG</categories><comments>19 pages, 7 figures, 1 table; v2: minor revisions; to appear in ACM
  Transactions on Mathematical Software</comments><acm-class>F.2.2; G.4</acm-class><journal-ref>ACM Transactions on Mathematical Software 39 (2012), no. 1,
  4:1-4:18</journal-ref><doi>10.1145/2382585.2382589</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crosscap number of a knot is an invariant describing the non-orientable
surface of smallest genus that the knot bounds. Unlike knot genus (its
orientable counterpart), crosscap numbers are difficult to compute and no
general algorithm is known. We present three methods for computing crosscap
number that offer varying trade-offs between precision and speed: (i) an
algorithm based on Hilbert basis enumeration and (ii) an algorithm based on
exact integer programming, both of which either compute the solution precisely
or reduce it to two possible values, and (iii) a fast but limited precision
integer programming algorithm that bounds the solution from above.
  The first two algorithms advance the theoretical state of the art, but remain
intractable for practical use. The third algorithm is fast and effective, which
we show in a practical setting by making significant improvements to the
current knowledge of crosscap numbers in knot tables. Our integer programming
framework is general, with the potential for further applications in
computational geometry and topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2422</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2422</id><created>2011-07-12</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Kubica</keyname><forenames>Marcin</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Walen</keyname><forenames>Tomasz</forenames></author></authors><title>A Linear Time Algorithm for Seeds Computation</title><categories>cs.DS</categories><comments>full version of paper submitted to SODA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Periodicity in words is one of the most fundamental areas of text algorithms
and combinatorics. Two classical and natural variations of periodicity are
seeds and covers (also called quasiperiods). Linear-time algorithms are known
for finding all the covers of a word, however in case of seeds, for the past 15
years only an $O(n\log{n})$ time algorithm was known (Iliopoulos, Moore and
Park, 1996). Finding an $o(n\log{n})$ time algorithm for the all-seeds problem
was mentioned as one of the most important open problems related to repetitions
in words in a survey by Smyth (2000). We show a linear-time algorithm computing
all the seeds of a word, in particular, the shortest seed. Our approach is
based on the use of a version of LZ-factorization and non-trivial combinatorial
relations between the LZ-factorization and seeds. It is used here for the first
time in context of seeds. It saves the work done for factors processed earlier,
similarly as in Crochemore's square-free testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2432</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2432</id><created>2011-07-12</created><updated>2012-11-15</updated><authors><author><keyname>Bar-Noy</keyname><forenames>Amotz</forenames></author><author><keyname>Gai</keyname><forenames>Yi</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew P.</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author><author><keyname>Rabanca</keyname><forenames>George</forenames></author></authors><title>Funding Games: the Truth but not the Whole Truth</title><categories>cs.GT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Funding Game, in which $m$ identical resources are to be
allocated among $n$ selfish agents. Each agent requests a number of resources
$x_i$ and reports a valuation $\tilde{v}_i(x_i)$, which verifiably {\em
lower}-bounds $i$'s true value for receiving $x_i$ items. The pairs $(x_i,
\tilde{v}_i(x_i))$ can be thought of as size-value pairs defining a knapsack
problem with capacity $m$. A publicly-known algorithm is used to solve this
knapsack problem, deciding which requests to satisfy in order to maximize the
social welfare.
  We show that a simple mechanism based on the knapsack {\it highest ratio
greedy} algorithm provides a Bayesian Price of Anarchy of 2, and for the
complete information version of the game we give an algorithm that computes a
Nash equilibrium strategy profile in $O(n^2 \log^2 m)$ time. Our primary
algorithmic result shows that an extension of the mechanism to $k$ rounds has a
Price of Anarchy of $1 + \frac{1}{k}$, yielding a graceful tradeoff between
communication complexity and the social welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2437</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2437</id><created>2011-07-12</created><authors><author><keyname>McIntosh</keyname><forenames>Harold V.</forenames></author></authors><title>A CONVERT compiler of REC for PDP-8</title><categories>cs.PL</categories><comments>This paper is seminal formal definition for REC language was
  published in &quot;Acta Mexicana de Ciencia y Tecnolog\'ia&quot; of IPN, Jan-April
  1968. REC is a programming language of extremely simple structure and what it
  was proved that the well publicized inconvenience of programming without a
  goto was a myth in Sixties endings</comments><report-no>IBP-Memo 2011-07</report-no><journal-ref>Acta Mexicana de Ciencia y Tecnologia of IPN, Vol. II, No. 1, pp
  33-43, Jan-April 1968, Mexico, D.F</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  REC (REGULAR EXPRESSION COMPILER) is a programming language of simple
structure developed originally for the PDP-8 computer of the Digital Equipment,
Corporation, but readily adaptable to any other general purpose computer. It
has been used extensively in teaching Algebra and Numerical Analysis in the
Escuela Superior de F\'isica y Matem\'aticas of the Instituto Polit\'ecnico
Nacional. Moreover, the fact that the same control language, REC, is equally
applicable and equally efficient over the whole range of computer facilities
available to the students gives a very welcome coherence to the entire teaching
program, including the course of Mathematical Logic which is devoted to the
theoretical aspects of such matters.
  REC; derives its appeal from the fact that computers can be regarded
reasonably well as Turing Machines. The REC notation is simply a manner of
writing regular expression, somewhat more amenable to programming the Turing
Machine which they control. If one does not wish to think so strictly in terms
of Turing Machines, REC expressions still provide a means of defining the flow
of control in a program which is quite convenient for many applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2443</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2443</id><created>2011-07-12</created><authors><author><keyname>Hosoda</keyname><forenames>Jun</forenames></author><author><keyname>Hromkovic</keyname><forenames>Juraj</forenames></author><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author><author><keyname>Ono</keyname><forenames>Horotaka</forenames></author><author><keyname>Steinova</keyname><forenames>Monika</forenames></author><author><keyname>Wada</keyname><forenames>Koichi</forenames></author></authors><title>On the Approximability and Hardness of Minimum Topic Connected Overlay
  and Its Special Instances</title><categories>cs.DS cs.DC cs.SI</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of designing a scalable overlay network to support
decentralized topic-based pub/sub communication, the Minimum Topic-Connected
Overlay problem (Min-TCO in short) has been investigated: Given a set of t
topics and a collection of n users together with the lists of topics they are
interested in, the aim is to connect these users to a network by a minimum
number of edges such that every graph induced by users interested in a common
topic is connected. It is known that Min-TCO is NP-hard and approximable within
O(log t) in polynomial time. In this paper, we further investigate the problem
and some of its special instances. We give various hardness results for
instances where the number of topics in which an user is interested in is
bounded by a constant, and also for the instances where the number of users
interested in a common topic is constant. For the latter case, we present a
?rst constant approximation algorithm. We also present some polynomial-time
algorithms for very restricted instances of Min-TCO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2444</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2444</id><created>2011-07-12</created><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Rothblum</keyname><forenames>Guy N.</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Private Data Release via Learning Thresholds</title><categories>cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers computationally efficient privacy-preserving data
release. We study the task of analyzing a database containing sensitive
information about individual participants. Given a set of statistical queries
on the data, we want to release approximate answers to the queries while also
guaranteeing differential privacy---protecting each participant's sensitive
data.
  Our focus is on computationally efficient data release algorithms; we seek
algorithms whose running time is polynomial, or at least sub-exponential, in
the data dimensionality. Our primary contribution is a computationally
efficient reduction from differentially private data release for a class of
counting queries, to learning thresholded sums of predicates from a related
class.
  We instantiate this general reduction with a variety of algorithms for
learning thresholds. These instantiations yield several new results for
differentially private data release. As two examples, taking {0,1}^d to be the
data domain (of dimension d), we obtain differentially private algorithms for:
  (*) Releasing all k-way conjunctions. For any given k, the resulting data
release algorithm has bounded error as long as the database is of size at least
d^{O(\sqrt{k\log(k\log d)})}. The running time is polynomial in the database
size.
  (*) Releasing a (1-\gamma)-fraction of all parity queries. For any \gamma
\geq \poly(1/d), the algorithm has bounded error as long as the database is of
size at least \poly(d). The running time is polynomial in the database size.
  Several other instantiations yield further results for privacy-preserving
data release. Of the two results highlighted above, the first learning
algorithm uses techniques for representing thresholded sums of predicates as
low-degree polynomial threshold functions. The second learning algorithm is
based on Jackson's Harmonic Sieve algorithm [Jackson 1997].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2446</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2446</id><created>2011-07-12</created><authors><author><keyname>Mark</keyname><forenames>Brian L.</forenames></author><author><keyname>Ephraim</keyname><forenames>Yariv</forenames></author></authors><title>An EM Algorithm for Continuous-time Bivariate Markov Chains</title><categories>stat.ME cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study properties and parameter estimation of finite-state homogeneous
continuous-time bivariate Markov chains. Only one of the two processes of the
bivariate Markov chain is observable. The general form of the bivariate Markov
chain studied here makes no assumptions on the structure of the generator of
the chain, and hence, neither the underlying process nor the observable process
is necessarily Markov. The bivariate Markov chain allows for simultaneous jumps
of the underlying and observable processes. Furthermore, the inter-arrival time
of observed events is phase-type. The bivariate Markov chain generalizes the
batch Markovian arrival process as well as the Markov modulated Markov process.
We develop an expectation-maximization (EM) procedure for estimating the
generator of a bivariate Markov chain, and we demonstrate its performance. The
procedure does not rely on any numerical integration or sampling scheme of the
continuous-time bivariate Markov chain. The proposed EM algorithm is equally
applicable to multivariate Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2451</identifier>
 <datestamp>2011-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2451</id><created>2011-07-12</created><updated>2011-08-02</updated><authors><author><keyname>Matsutani</keyname><forenames>Shigeki</forenames></author><author><keyname>Nakano</keyname><forenames>Kota</forenames></author><author><keyname>Shinjo</keyname><forenames>Katsuhiko</forenames></author></authors><title>Surface tension of multi-phase flow with multiple junctions governed by
  the variational principle</title><categories>cs.NA math-ph math.DG math.MP math.NA physics.comp-ph physics.flu-dyn</categories><comments>54 pages, 3 figures</comments><doi>10.1007/s11040-011-9096-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a computational model of an incompressible fluid with a
multi-phase field in three-dimensional Euclidean space. By investigating an
incompressible fluid with a two-phase field geometrically, we reformulate the
expression of the surface tension for the two-phase field found by Lafaurie,
Nardone, Scardovelli, Zaleski and Zanetti (J. Comp. Phys. \vol{113} \yr{1994}
\pages{134-147}) as a variational problem related to an infinite dimensional
Lie group, the volume-preserving diffeomorphism. The variational principle to
the action integral with the surface energy reproduces their Euler equation of
the two-phase field with the surface tension. Since the surface energy of
multiple interfaces even with singularities is not difficult to be evaluated in
general and the variational formulation works for every action integral, the
new formulation enables us to extend their expression to that of a multi-phase
($N$-phase, $N\ge2$) flow and to obtain a novel Euler equation with the surface
tension of the multi-phase field. The obtained Euler equation governs the
equation of motion of the multi-phase field with different surface tension
coefficients without any difficulties for the singularities at multiple
junctions. In other words, we unify the theory of multi-phase fields which
express low dimensional interface geometry and the theory of the incompressible
fluid dynamics on the infinite dimensional geometry as a variational problem.
We apply the equation to the contact angle problems at triple junctions. We
computed the fluid dynamics for a two-phase field with a wall numerically and
show the numerical computational results that for given surface tension
coefficients, the contact angles are generated by the surface tension as
results of balances of the kinematic energy and the surface energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2462</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2462</id><created>2011-07-13</created><updated>2011-11-09</updated><authors><author><keyname>Rubin</keyname><forenames>Timothy N.</forenames></author><author><keyname>Chambers</keyname><forenames>America</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author><author><keyname>Steyvers</keyname><forenames>Mark</forenames></author></authors><title>Statistical Topic Models for Multi-Label Document Classification</title><categories>stat.ML cs.LG</categories><comments>44 Pages (Including Appendices). To be published in: The Machine
  Learning Journal, special issue on Learning from Multi-Label Data. Version 2
  corrects some typos, updates some of the notation used in the paper for
  clarification of some equations, and incorporates several relatively minor
  changes to the text throughout the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning approaches to multi-label document classification have to
date largely relied on discriminative modeling techniques such as support
vector machines. A drawback of these approaches is that performance rapidly
drops off as the total number of labels and the number of labels per document
increase. This problem is amplified when the label frequencies exhibit the type
of highly skewed distributions that are often observed in real-world datasets.
In this paper we investigate a class of generative statistical topic models for
multi-label documents that associate individual word tokens with different
labels. We investigate the advantages of this approach relative to
discriminative models, particularly with respect to classification problems
involving large numbers of relatively rare labels. We compare the performance
of generative and discriminative approaches on document labeling tasks ranging
from datasets with several thousand labels to datasets with tens of labels. The
experimental results indicate that probabilistic generative models can achieve
competitive multi-label classification performance compared to discriminative
methods, and have advantages for datasets with many labels and skewed label
frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2464</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2464</id><created>2011-07-13</created><authors><author><keyname>Sahneh</keyname><forenames>Faryad Darabi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author></authors><title>Epidemic Spread in Human Networks</title><categories>physics.soc-ph cs.SY math.DS stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the popular dynamics on complex networks is the epidemic spreading. An
epidemic model describes how infections spread throughout a network. Among the
compartmental models used to describe epidemics, the
Susceptible-Infected-Susceptible (SIS) model has been widely used. In the SIS
model, each node can be susceptible, become infected with a given infection
rate, and become again susceptible with a given curing rate. In this paper, we
add a new compartment to the classic SIS model to account for human response to
epidemic spread. Each individual can be infected, susceptible, or alert.
Susceptible individuals can become alert with an alerting rate if infected
individuals exist in their neighborhood. An individual in the alert state is
less probable to become infected than an individual in the susceptible state;
due to a newly adopted cautious behavior. The problem is formulated as a
continuous-time Markov process on a general static graph and then modeled into
a set of ordinary differential equations using mean field approximation method
and the corresponding Kolmogorov forward equations. The model is then studied
using results from algebraic graph theory and center manifold theorem. We
analytically show that our model exhibits two distinct thresholds in the
dynamics of epidemic spread. Below the first threshold, infection dies out
exponentially. Beyond the second threshold, infection persists in the steady
state. Between the two thresholds, the infection spreads at the first stage but
then dies out asymptotically as the result of increased alertness in the
network. Finally, simulations are provided to support our findings. Our results
suggest that alertness can be considered as a strategy of controlling the
epidemics which propose multiple potential areas of applications, from
infectious diseases mitigations to malware impact reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2465</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2465</id><created>2011-07-13</created><updated>2013-02-08</updated><authors><author><keyname>Carli</keyname><forenames>Francesca P.</forenames></author><author><keyname>Ferrante</keyname><forenames>Augusto</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author><author><keyname>Picci</keyname><forenames>Giorgio</forenames></author></authors><title>An Efficient Algorithm for Maximum-Entropy Extension of Block-Circulant
  Covariance Matrices</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with maximum entropy completion of partially specified
block-circulant matrices. Since positive definite symmetric circulants happen
to be covariance matrices of stationary periodic processes, in particular of
stationary reciprocal processes, this problem has applications in signal
processing, in particular to image modeling. In fact it is strictly related to
maximum likelihood estimation of bilateral AR-type representations of acausal
signals subject to certain conditional independence constraints. The maximum
entropy completion problem for block-circulant matrices has recently been
solved by the authors, although leaving open the problem of an efficient
computation of the solution. In this paper, we provide an effcient algorithm
for computing its solution which compares very favourably with existing
algorithms designed for positive definite matrix extension problems. The
proposed algorithm benefits from the analysis of the relationship between our
problem and the band-extension problem for block-Toeplitz matrices also
developed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2473</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2473</id><created>2011-07-13</created><updated>2011-12-22</updated><authors><author><keyname>Chung</keyname><forenames>Ning Ning</forenames></author><author><keyname>Chew</keyname><forenames>Lock Yue</forenames></author><author><keyname>Lai</keyname><forenames>Choy Heng</forenames></author></authors><title>Network Extreme Eigenvalue - from Multimodal to Scale-free Network</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 4 figures</comments><journal-ref>Chaos 22, (2012) 013139</journal-ref><doi>10.1063/1.3697990</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extreme eigenvalues of adjacency matrices are important indicators on the
influences of topological structures to collective dynamical behavior of
complex networks. Recent findings on the ensemble averageability of the extreme
eigenvalue further authenticate its sensibility in the study of network
dynamics. Here we determine the ensemble average of the extreme eigenvalue and
characterize the deviation across the ensemble through the discrete form of
random scale-free network. Remarkably, the analytical approximation derived
from the discrete form shows significant improvement over the previous results.
This has also led us to the same conclusion as [Phys. Rev. Lett. 98, 248701
(2007)] that deviation in the reduced extreme eigenvalues vanishes as the
network size grows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2482</identifier>
 <datestamp>2011-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2482</id><created>2011-07-13</created><updated>2011-08-17</updated><authors><author><keyname>Jindal</keyname><forenames>Anant</forenames></author><author><keyname>Kochar</keyname><forenames>Gazal</forenames></author><author><keyname>Pal</keyname><forenames>Manjish</forenames></author></authors><title>Maximum Matchings via Glauber Dynamics</title><categories>cs.DS</categories><comments>It has been pointed to us independently by Yuval Peres, Jonah
  Sherman, Piyush Srivastava and other anonymous reviewers that the coupling
  used in this paper doesn't have the right marginals because of which the
  mixing time bound doesn't hold, and also the main result presented in the
  paper. We thank them for reading the paper with interest and promptly
  pointing out this mistake</comments><acm-class>F.2.0; G.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we study the classic problem of computing a maximum cardinality
matching in general graphs $G = (V, E)$. The best known algorithm for this
problem till date runs in $O(m \sqrt{n})$ time due to Micali and Vazirani
\cite{MV80}. Even for general bipartite graphs this is the best known running
time (the algorithm of Karp and Hopcroft \cite{HK73} also achieves this bound).
For regular bipartite graphs one can achieve an $O(m)$ time algorithm which,
following a series of papers, has been recently improved to $O(n \log n)$ by
Goel, Kapralov and Khanna (STOC 2010) \cite{GKK10}. In this paper we present a
randomized algorithm based on the Markov Chain Monte Carlo paradigm which runs
in $O(m \log^2 n)$ time, thereby obtaining a significant improvement over
\cite{MV80}.
  We use a Markov chain similar to the \emph{hard-core model} for Glauber
Dynamics with \emph{fugacity} parameter $\lambda$, which is used to sample
independent sets in a graph from the Gibbs Distribution \cite{V99}, to design a
faster algorithm for finding maximum matchings in general graphs. Our result
crucially relies on the fact that the mixing time of our Markov Chain is
independent of $\lambda$, a significant deviation from the recent series of
works \cite{GGSVY11,MWW09, RSVVY10, S10, W06} which achieve computational
transition (for estimating the partition function) on a threshold value of
$\lambda$. As a result we are able to design a randomized algorithm which runs
in $O(m\log^2 n)$ time that provides a major improvement over the running time
of the algorithm due to Micali and Vazirani. Using the conductance bound, we
also prove that mixing takes $\Omega(\frac{m}{k})$ time where $k$ is the size
of the maximum matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2487</identifier>
 <datestamp>2012-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2487</id><created>2011-07-13</created><updated>2012-08-03</updated><authors><author><keyname>Aswani</keyname><forenames>Anil</forenames></author><author><keyname>Gonzalez</keyname><forenames>Humberto</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire</forenames></author></authors><title>Provably Safe and Robust Learning-Based Model Predictive Control</title><categories>math.OC cs.LG cs.SY math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controller design faces a trade-off between robustness and performance, and
the reliability of linear controllers has caused many practitioners to focus on
the former. However, there is renewed interest in improving system performance
to deal with growing energy constraints. This paper describes a learning-based
model predictive control (LBMPC) scheme that provides deterministic guarantees
on robustness, while statistical identification tools are used to identify
richer models of the system in order to improve performance; the benefits of
this framework are that it handles state and input constraints, optimizes
system performance with respect to a cost function, and can be designed to use
a wide variety of parametric or nonparametric statistical tools. The main
insight of LBMPC is that safety and performance can be decoupled under
reasonable conditions in an optimization framework by maintaining two models of
the system. The first is an approximate model with bounds on its uncertainty,
and the second model is updated by statistical methods. LBMPC improves
performance by choosing inputs that minimize a cost subject to the learned
dynamics, and it ensures safety and robustness by checking whether these same
inputs keep the approximate model stable when it is subject to uncertainty.
Furthermore, we show that if the system is sufficiently excited, then the LBMPC
control action probabilistically converges to that of an MPC computed using the
true dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2490</identifier>
 <datestamp>2011-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2490</id><created>2011-07-13</created><updated>2011-12-22</updated><authors><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Towards Optimal One Pass Large Scale Learning with Averaged Stochastic
  Gradient Descent</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For large scale learning problems, it is desirable if we can obtain the
optimal model parameters by going through the data in only one pass. Polyak and
Juditsky (1992) showed that asymptotically the test performance of the simple
average of the parameters obtained by stochastic gradient descent (SGD) is as
good as that of the parameters which minimize the empirical cost. However, to
our knowledge, despite its optimal asymptotic convergence rate, averaged SGD
(ASGD) received little attention in recent research on large scale learning.
One possible reason is that it may take a prohibitively large number of
training samples for ASGD to reach its asymptotic region for most real
problems. In this paper, we present a finite sample analysis for the method of
Polyak and Juditsky (1992). Our analysis shows that it indeed usually takes a
huge number of samples for ASGD to reach its asymptotic region for improperly
chosen learning rate. More importantly, based on our analysis, we propose a
simple way to properly set learning rate so that it takes a reasonable amount
of data for ASGD to reach its asymptotic region. We compare ASGD using our
proposed learning rate with other well known algorithms for training large
scale linear classifiers. The experiments clearly show the superiority of ASGD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2499</identifier>
 <datestamp>2012-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2499</id><created>2011-07-13</created><updated>2012-01-18</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Qiu</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Chengwen</forenames></author></authors><title>Improving Energy Efficiency Through Multimode Transmission in the
  Downlink MIMO Systems</title><categories>cs.IT math.IT</categories><comments>19 pages, 10 figures, EURASIP Journal on Wireless Communications and
  Networking; EURASIP Journal on Wireless Communications and Networking (2011)
  2011:200</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptively adjusting system parameters including bandwidth, transmit power
and mode to maximize the &quot;Bits per-Joule&quot; energy efficiency (BPJ-EE) in the
downlink MIMO systems with imperfect channel state information at the
transmitter (CSIT) is considered in this paper. By mode we refer to choice of
transmission schemes i.e. singular value decomposition (SVD) or block
diagonalization (BD), active transmit/receive antenna number and active user
number. We derive optimal bandwidth and transmit power for each dedicated mode
at first. During the derivation, accurate capacity estimation strategies are
proposed to cope with the imperfect CSIT caused capacity prediction problem.
Then, an ergodic capacity based mode switching strategy is proposed to further
improve the BPJ-EE, which provides insights on the preferred mode under given
scenarios. Mode switching compromises different power parts, exploits the
tradeoff between the multiplexing gain and the imperfect CSIT caused inter-user
interference, improves the BPJ-EE significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2509</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2509</id><created>2011-07-13</created><updated>2012-04-05</updated><authors><author><keyname>Moussallam</keyname><forenames>Manuel</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author><author><keyname>Richard</keyname><forenames>Ga&#xeb;l</forenames></author></authors><title>Matching Pursuits with Random Sequential Subdictionaries</title><categories>cs.DS</categories><comments>20 pages - accepted 2nd April 2012 at Elsevier Signal Processing</comments><doi>10.1016/j.sigpro.2012.03.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching pursuits are a class of greedy algorithms commonly used in signal
processing, for solving the sparse approximation problem. They rely on an atom
selection step that requires the calculation of numerous projections, which can
be computationally costly for large dictionaries and burdens their
competitiveness in coding applications. We propose using a non adaptive random
sequence of subdictionaries in the decomposition process, thus parsing a large
dictionary in a probabilistic fashion with no additional projection cost nor
parameter estimation. A theoretical modeling based on order statistics is
provided, along with experimental evidence showing that the novel algorithm can
be efficiently used on sparse approximation problems. An application to audio
signal compression with multiscale time-frequency dictionaries is presented,
along with a discussion of the complexity and practical implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2513</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2513</id><created>2011-07-13</created><authors><author><keyname>Syropoulos</keyname><forenames>Apostolos</forenames></author><author><keyname>de Paiva</keyname><forenames>Valeria</forenames></author></authors><title>Fuzzy Topological Systems</title><categories>cs.LO</categories><comments>This paper was read at the 8th Panhellenic Logic Symposium, July 4-8,
  2011, Ioannina, Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialectica categories are a very versatile categorical model of linear logic.
These have been used to model many seemingly different things (e.g., Petri nets
and Lambek's calculus). In this note, we expand our previous work on fuzzy
petri nets to deal with fuzzy topological systems. One basic idea is to use as
the dualizing object in the Dialectica categories construction, the unit real
interval [0,1], which has all the properties of a {\em lineale}. The second
basic idea is to generalize Vickers's notion of a topological system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2526</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2526</id><created>2011-07-13</created><updated>2013-12-02</updated><authors><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Jakubowicz</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>Convergence of a Multi-Agent Projected Stochastic Gradient Algorithm for
  Non-Convex Optimization</title><categories>math.OC cs.DC cs.SY</categories><comments>IEEE Transactions on Automatic Control 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for the convergence analysis of a class of
distributed constrained non-convex optimization algorithms in multi-agent
systems. The aim is to search for local minimizers of a non-convex objective
function which is supposed to be a sum of local utility functions of the
agents. The algorithm under study consists of two steps: a local stochastic
gradient descent at each agent and a gossip step that drives the network of
agents to a consensus. Under the assumption of decreasing stepsize, it is
proved that consensus is asymptotically achieved in the network and that the
algorithm converges to the set of Karush-Kuhn-Tucker points. As an important
feature, the algorithm does not require the double-stochasticity of the gossip
matrices. It is in particular suitable for use in a natural broadcast scenario
for which no feedback messages between agents are required. It is proved that
our result also holds if the number of communications in the network per unit
of time vanishes at moderate speed as time increases, allowing for potential
savings of the network's energy. Applications to power allocation in wireless
ad-hoc networks are discussed. Finally, we provide numerical results which
sustain our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2527</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2527</id><created>2011-07-13</created><updated>2012-06-28</updated><authors><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>On the Sensitivity of Continuous-Time Noncoherent Fading Channel
  Capacity</title><categories>cs.IT math.IT</categories><comments>final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The noncoherent capacity of stationary discrete-time fading channels is known
to be very sensitive to the fine details of the channel model. More
specifically, the measure of the support of the fading-process power spectral
density (PSD) determines if noncoherent capacity grows logarithmically in SNR
or slower than logarithmically. Such a result is unsatisfactory from an
engineering point of view, as the support of the PSD cannot be determined
through measurements. The aim of this paper is to assess whether, for general
continuous-time Rayleigh-fading channels, this sensitivity has a noticeable
impact on capacity at SNR values of practical interest.
  To this end, we consider the general class of band-limited continuous-time
Rayleigh-fading channels that satisfy the wide-sense stationary
uncorrelated-scattering (WSSUS) assumption and are, in addition, underspread.
We show that, for all SNR values of practical interest, the noncoherent
capacity of every channel in this class is close to the capacity of an AWGN
channel with the same SNR and bandwidth, independently of the measure of the
support of the scattering function (the two-dimensional channel PSD). Our
result is based on a lower bound on noncoherent capacity, which is built on a
discretization of the channel input-output relation induced by projecting onto
Weyl-Heisenberg (WH) sets. This approach is interesting in its own right as it
yields a mathematically tractable way of dealing with the mutual information
between certain continuous-time random signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2553</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2553</id><created>2011-07-13</created><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Learning Hypergraph Labeling for Feature Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study poses the feature correspondence problem as a hypergraph node
labeling problem. Candidate feature matches and their subsets (usually of size
larger than two) are considered to be the nodes and hyperedges of a hypergraph.
A hypergraph labeling algorithm, which models the subset-wise interaction by an
undirected graphical model, is applied to label the nodes (feature
correspondences) as correct or incorrect. We describe a method to learn the
cost function of this labeling algorithm from labeled examples using a
graphical model training algorithm. The proposed feature matching algorithm is
different from the most of the existing learning point matching methods in
terms of the form of the objective function, the cost function to be learned
and the optimization method applied to minimize it. The results on standard
datasets demonstrate how learning over a hypergraph improves the matching
performance over existing algorithms, notably one that also uses higher order
information without learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2554</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2554</id><created>2011-07-13</created><authors><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>Routing in Undirected Graphs with Constant Congestion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph G=(V,E), a collection (s_1,t_1),...,(s_k,t_k) of k
source-sink pairs, and an integer c, the goal in the Edge Disjoint Paths with
Congestion problem is to connect maximum possible number of the source-sink
pairs by paths, so that the maximum load on any edge (called edge congestion)
does not exceed c.
  We show an efficient randomized algorithm to route $\Omega(OPT/\poly\log k)$
source-sink pairs with congestion at most 14, where OPT is the maximum number
of pairs that can be simultaneously routed on edge-disjoint paths. The best
previous algorithm that routed $\Omega(OPT/\poly\log n)$ pairs required
congestion $\poly(\log \log n)$, and for the setting where the maximum allowed
congestion is bounded by a constant c, the best previous algorithms could only
guarantee the routing of $OPT/n^{O(1/c)}$ pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2559</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2559</id><created>2011-07-13</created><updated>2015-02-07</updated><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Verbin</keyname><forenames>Elad</forenames></author><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>Lower Bounds for Number-in-Hand Multiparty Communication Complexity,
  Made Easy</title><categories>cs.CC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove lower bounds on randomized multiparty communication
complexity, both in the \emph{blackboard model} (where each message is written
on a blackboard for all players to see) and (mainly) in the
\emph{message-passing model}, where messages are sent player-to-player. We
introduce a new technique for proving such bounds, called
\emph{symmetrization}, which is natural, intuitive, and often easy to use.
  For example, for the problem where each of $k$ players gets a bit-vector of
length $n$, and the goal is to compute the coordinate-wise XOR of these
vectors, we prove a tight lower bounds of $\Omega(nk)$ in the blackboard model.
For the same problem with AND instead of XOR, we prove a lower bounds of
roughly $\Omega(nk)$ in the message-passing model (assuming $k \le n/3200$) and
$\Omega(n \log k)$ in the blackboard model. We also prove lower bounds for
bit-wise majority, for a graph-connectivity problem, and for other problems;
the technique seems applicable to a wide range of other problems as well. All
of our lower bounds allow randomized communication protocols with two-sided
error.
  We also use the symmetrization technique to prove several direct-sum-like
results for multiparty communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2615</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2615</id><created>2011-07-13</created><authors><author><keyname>Arsinte</keyname><forenames>Radu</forenames></author></authors><title>Aspects of Entertainment Distribution in an Intelligent Home Environment</title><categories>cs.NI cs.MM</categories><comments>4 pages, 5 figures; Acta Technica Napocensis, Electronics and
  Telecommunications, nr.3/2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an implementation and tests of a simple home entertainment
distribution architecture (server + multiple clients) implemented using two
conventional cabling architectures: CATV coaxial cable and conventional
Ethernet. This architecture is created taking into account the &quot;Home gateway&quot;
concept present in most attempts to solve the problem of the &quot;Intelligent
home&quot;. A short presentation of the experimental is given with an investigation
of the main performances obtained using this architecture. The experiments
revealed that this simple solution makes possible to have entertainment and
data services with performances close to traditional data services in a
cost-effective architecture
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2647</identifier>
 <datestamp>2011-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2647</id><created>2011-07-13</created><authors><author><keyname>Chmiel</keyname><forenames>Anna</forenames></author><author><keyname>Sienkiewicz</keyname><forenames>Julian</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Paltoglou</keyname><forenames>Georgios</forenames></author><author><keyname>Buckley</keyname><forenames>Kevan</forenames></author><author><keyname>Kappas</keyname><forenames>Arvid</forenames></author><author><keyname>Ho&#x142;yst</keyname><forenames>Janusz A.</forenames></author></authors><title>Collective emotions online and their influence on community life</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages including Supporting Information, accepted to PLoS ONE</comments><journal-ref>PLoS ONE 6(7): e22207 (2011)</journal-ref><doi>10.1371/journal.pone.0022207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-communities, social groups interacting online, have recently become an
object of interdisciplinary research. As with face-to-face meetings, Internet
exchanges may not only include factual information but also emotional
information - how participants feel about the subject discussed or other group
members. Emotions are known to be important in affecting interaction partners
in offline communication in many ways. Could emotions in Internet exchanges
affect others and systematically influence quantitative and qualitative aspects
of the trajectory of e-communities? The development of automatic sentiment
analysis has made large scale emotion detection and analysis possible using
text messages collected from the web. It is not clear if emotions in
e-communities primarily derive from individual group members' personalities or
if they result from intra-group interactions, and whether they influence group
activities. We show the collective character of affective phenomena on a large
scale as observed in 4 million posts downloaded from Blogs, Digg and BBC
forums. To test whether the emotions of a community member may influence the
emotions of others, posts were grouped into clusters of messages with similar
emotional valences. The frequency of long clusters was much higher than it
would be if emotions occurred at random. Distributions for cluster lengths can
be explained by preferential processes because conditional probabilities for
consecutive messages grow as a power law with cluster length. For BBC forum
threads, average discussion lengths were higher for larger values of absolute
average emotional valence in the first ten comments and the average amount of
emotion in messages fell during discussions. Our results prove that collective
emotional states can be created and modulated via Internet communication and
that emotional expressiveness is the fuel that sustains some e-communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2677</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2677</id><created>2011-07-13</created><updated>2013-06-19</updated><authors><author><keyname>Halabi</keyname><forenames>Nissim</forenames></author><author><keyname>Even</keyname><forenames>Guy</forenames></author></authors><title>On Decoding Irregular Tanner Codes with Local-Optimality Guarantees</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider decoding of binary Tanner codes using message-passing iterative
decoding and linear programming (LP) decoding in MBIOS channels. We present new
certificates that are based on a combinatorial characterization for
local-optimality of a codeword in irregular Tanner codes with respect to any
MBIOS channel. This characterization is based on a conical combination of
normalized weighted subtrees in the computation trees of the Tanner graph.
These subtrees may have any finite height h (even equal or greater than half of
the girth of the Tanner graph). In addition, the degrees of local-code nodes in
these subtrees are not restricted to two. We prove that local optimality in
this new characterization implies maximum-likelihood (ML) optimality and LP
optimality, and show that a certificate can be computed efficiently.
  We also present a new message-passing iterative decoding algorithm, called
normalized weighted min-sum (NWMS). NWMS decoding is a BP-type algorithm that
applies to any irregular binary Tanner code with single parity-check local
codes. We prove that if a locally-optimal codeword with respect to height
parameter h exists (whereby notably h is not limited by the girth of the Tanner
graph), then NWMS decoding finds this codeword in h iterations. The decoding
guarantee of the NWMS decoding algorithm applies whenever there exists a
locally optimal codeword. Because local optimality of a codeword implies that
it is the unique ML codeword, the decoding guarantee also provides an ML
certificate for this codeword.
  Finally, we apply the new local optimality characterization to regular Tanner
codes, and prove lower bounds on the noise thresholds of LP decoding in MBIOS
channels. When the noise is below these lower bounds, the probability that LP
decoding fails decays doubly exponentially in the girth of the Tanner graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2681</identifier>
 <datestamp>2011-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2681</id><created>2011-07-13</created><updated>2011-11-15</updated><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Coordinate-invariant incremental Lyapunov functions</title><categories>math.OC cs.SY math.DS</categories><comments>8 pages. arXiv admin note: repeats content from arXiv:1005.4957</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of incremental stability was proposed by several researchers as a
strong property of dynamical and control systems. In this type of stability,
trajectories converge to each other, rather than to an equilibrium point or a
particular trajectory. Similarly to stability, Lyapunov functions play an
important role in the study of incremental stability. In this paper, we propose
coordinate-invariant notions of incremental Lyapunov function and provide
characterizations of incremental stability in terms of existence of the
proposed Lyapunov functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2683</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2683</id><created>2011-07-13</created><authors><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author><author><keyname>Morales</keyname><forenames>Jose Angel Quintanar</forenames></author><author><keyname>Santos</keyname><forenames>Hermenegildo Fernandez</forenames></author></authors><title>An IDE to Build and Check Task Flow Models</title><categories>cs.SE</categories><journal-ref>Fernandez-y-Fernandez, C.A., Quintanar Morales, J.A and Fernandez
  Santos, H. An IDE to Build and Check Task Flow Models. Advances in Computer
  Science and Applications, Research in Computer Science 53, 2011, pp. 23-33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Eclipse plug-ins for the Task Flow model in the
Discovery Method. These plug-ins provide an IDE for the Task Algebra compiler
and the model-checking tools. The Task Algebra is the formal representation for
the Task Model and it is based on simple and compound tasks. The model-checking
techniques were developed to validate Task Models represented in the algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2686</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2686</id><created>2011-07-13</created><updated>2011-12-08</updated><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>A Higher-Order Cheeger's Inequality</title><categories>cs.DS</categories><comments>This paper has been withdrawn by the author since a simpler and more
  general result is recently posted at http://arxiv.org/abs/1111.1055</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic fact in algebraic graph theory is that the number of connected
components in an undirected graph is equal to the multiplicity of the
eigenvalue 1 in the normalized adjacency matrix of the graph. In particular,
the graph is disconnected if and only if there are at least two eigenvalues
equal to 1.
  Cheeger's inequality provides an &quot;approximate&quot; version of the latter fact,
and it states that a graph has a sparse cut (it is &quot;almost disconnected&quot;) if
and only if there are at least two eigenvalues that are close to one.
  It has been conjectured that an analogous characterization holds for higher
multiplicities, that is there are $k$ eigenvalues close to 1 if and only if the
vertex set can be partitioned into $k$ subsets, each defining a sparse cut. In
this paper we resolve this conjecture. Our result provides a theoretical
justification for clustering algorithms that use the top $k$ eigenvector to
embed the vertices into $\R^k$, and then apply geometric considerations to the
embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2691</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2691</id><created>2011-07-13</created><authors><author><keyname>D'Alberto</keyname><forenames>Paolo</forenames></author><author><keyname>Dasdan</keyname><forenames>Ali</forenames></author></authors><title>On the Weakenesses of Correlation Measures used for Search Engines'
  Results (Unsupervised Comparison of Search Engine Rankings)</title><categories>stat.CO cs.IR</categories><comments>16 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The correlation of the result lists provided by search engines is fundamental
and it has deep and multidisciplinary ramifications. Here, we present automatic
and unsupervised methods to assess whether or not search engines provide
results that are comparable or correlated. We have two main contributions:
First, we provide evidence that for more than 80% of the input queries -
independently of their frequency - the two major search engines share only
three or fewer URLs in their search results, leading to an increasing
divergence. In this scenario (divergence), we show that even the most robust
measures based on comparing lists is useless to apply; that is, the small
contribution by too few common items will infer no confidence. Second, to
overcome this problem, we propose the fist content-based measures - i.e.,
direct comparison of the contents from search results; these measures are based
on the Jaccard ratio and distribution similarity measures (CDF measures). We
show that they are orthogonal to each other (i.e., Jaccard and distribution)
and extend the discriminative power w.r.t. list based measures. Our approach
stems from the real need of comparing search-engine results, it is automatic
from the query selection to the final evaluation and it apply to any
geographical markets, thus designed to scale and to use as first filtering of
query selection (necessary) for supervised methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2693</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2693</id><created>2011-07-13</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author></authors><title>A Fuzzy View on k-Means Based Signal Quantization with Application in
  Iris Segmentation</title><categories>cs.CV</categories><comments>4, pages, 3 figures, 17th Telecommunications Forum TELFOR 2009,
  Belgrade, Serbia</comments><msc-class>68U10, 68T10</msc-class><acm-class>I.4.6; I.5.1; I.5.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the k-means quantization of a signal can be interpreted
both as a crisp indicator function and as a fuzzy membership assignment
describing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy
indicator functions are defined here as natural generalizations of the ordinary
crisp and fuzzy indicator functions, respectively. An application to iris
segmentation is presented together with a demo program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2696</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2696</id><created>2011-07-13</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author></authors><title>Exploring New Directions in Iris Recognition</title><categories>cs.CV</categories><comments>8 pages, 10 figures, 11th Int. Symp. on Symbolic and Numeric
  Algorithms for Scientific Computing, 2009</comments><msc-class>68U10, 68T10, 62H35</msc-class><acm-class>I.4.6; I.4.9; I.5.4; I.5.3; K.6.5</acm-class><journal-ref>Proc. 11th Int. Symp. on Symbolic and Numeric Algorithms for
  Scientific Computing (2009), CPS-IEEE Computer Society, pp. 384-391, DOI:
  10.1109/SYNASC.2009.45</journal-ref><doi>10.1109/SYNASC.2009.45</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach in iris recognition based on Circular Fuzzy Iris Segmentation
(CFIS) and Gabor Analytic Iris Texture Binary Encoder (GAITBE) is proposed and
tested here. CFIS procedure is designed to guarantee that similar iris segments
will be obtained for similar eye images, despite the fact that the degree of
occlusion may vary from one image to another. Its result is a circular iris
ring (concentric with the pupil) which approximates the actual iris. GAITBE
proves better encoding of statistical independence between the iris codes
extracted from different irides using Hilbert Transform. Irides from University
of Bath Iris Database are binary encoded on two different lengths (768 / 192
bytes) and tested in both single-enrollment and multi-enrollment identification
scenarios. All cases illustrate the capacity of the newly proposed methodology
to narrow down the distribution of inter-class matching scores, and
consequently, to guarantee a steeper descent of the False Accept Rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2699</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2699</id><created>2011-07-13</created><authors><author><keyname>&#xc1;lvarez</keyname><forenames>Mauricio A.</forenames></author><author><keyname>Luengo</keyname><forenames>David</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Linear Latent Force Models using Gaussian Processes</title><categories>stat.ML cs.AI</categories><comments>20 pages, 2 figures. Extended technical report of the Conference
  Paper &quot;Latent force models&quot; in D. van Dyk and M. Welling (eds) Proceedings of
  the Twelfth International Workshop on Artificial Intelligence and Statistics,
  JMLR W&amp;CP 5, Clearwater Beach, FL, pp 9--16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purely data driven approaches for machine learning present difficulties when
data is scarce relative to the complexity of the model or when the model is
forced to extrapolate. On the other hand, purely mechanistic approaches need to
identify and specify all the interactions in the problem at hand (which may not
be feasible) and still leave the issue of how to parameterize the system. In
this paper, we present a hybrid approach using Gaussian processes and
differential equations to combine data driven modelling with a physical model
of the system. We show how different, physically-inspired, kernel functions can
be developed through sensible, simple, mechanistic assumptions about the
underlying system. The versatility of our approach is illustrated with three
case studies from motion capture, computational biology and geostatistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2700</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2700</id><created>2011-07-13</created><updated>2014-09-14</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Learning $k$-Modal Distributions via Testing</title><categories>cs.DS cs.LG math.ST stat.TH</categories><comments>28 pages, full version of SODA'12 paper, to appear in Theory of
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-modal probability distribution over the discrete domain $\{1,...,n\}$
is one whose histogram has at most $k$ &quot;peaks&quot; and &quot;valleys.&quot; Such
distributions are natural generalizations of monotone ($k=0$) and unimodal
($k=1$) probability distributions, which have been intensively studied in
probability theory and statistics.
  In this paper we consider the problem of \emph{learning} (i.e., performing
density estimation of) an unknown $k$-modal distribution with respect to the
$L_1$ distance. The learning algorithm is given access to independent samples
drawn from an unknown $k$-modal distribution $p$, and it must output a
hypothesis distribution $\widehat{p}$ such that with high probability the total
variation distance between $p$ and $\widehat{p}$ is at most $\epsilon.$ Our
main goal is to obtain \emph{computationally efficient} algorithms for this
problem that use (close to) an information-theoretically optimal number of
samples.
  We give an efficient algorithm for this problem that runs in time
$\mathrm{poly}(k,\log(n),1/\epsilon)$. For $k \leq \tilde{O}(\log n)$, the
number of samples used by our algorithm is very close (within an
$\tilde{O}(\log(1/\epsilon))$ factor) to being information-theoretically
optimal. Prior to this work computationally efficient algorithms were known
only for the cases $k=0,1$ \cite{Birge:87b,Birge:97}.
  A novel feature of our approach is that our learning algorithm crucially uses
a new algorithm for \emph{property testing of probability distributions} as a
key subroutine. The learning algorithm uses the property tester to efficiently
decompose the $k$-modal distribution into $k$ (near-)monotone distributions,
which are easier to learn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2702</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2702</id><created>2011-07-13</created><updated>2015-02-16</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Learning Poisson Binomial Distributions</title><categories>cs.DS cs.LG math.ST stat.TH</categories><comments>Revised full version. Improved sample complexity bound of O~(1/eps^2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a basic problem in unsupervised learning: learning an unknown
\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)
over $\{0,1,\dots,n\}$ is the distribution of a sum of $n$ independent
Bernoulli random variables which may have arbitrary, potentially non-equal,
expectations. These distributions were first studied by S. Poisson in 1837
\cite{Poisson:37} and are a natural $n$-parameter generalization of the
familiar Binomial Distribution. Surprisingly, prior to our work this basic
learning problem was poorly understood, and known results for it were far from
optimal.
  We essentially settle the complexity of the learning problem for this basic
class of distributions. As our first main result we give a highly efficient
algorithm which learns to $\eps$-accuracy (with respect to the total variation
distance) using $\tilde{O}(1/\eps^3)$ samples \emph{independent of $n$}. The
running time of the algorithm is \emph{quasilinear} in the size of its input
data, i.e., $\tilde{O}(\log(n)/\eps^3)$ bit-operations. (Observe that each draw
from the distribution is a $\log(n)$-bit string.) Our second main result is a
{\em proper} learning algorithm that learns to $\eps$-accuracy using
$\tilde{O}(1/\eps^2)$ samples, and runs in time $(1/\eps)^{\poly (\log
(1/\eps))} \cdot \log n$. This is nearly optimal, since any algorithm {for this
problem} must use $\Omega(1/\eps^2)$ samples. We also give positive and
negative results for some extensions of this learning problem to weighted sums
of independent Bernoulli random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2715</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2715</id><created>2011-07-13</created><authors><author><keyname>Vogt</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Wagner</keyname><forenames>Alexander Y.</forenames></author></authors><title>Stereo pairs in Astrophysics</title><categories>astro-ph.IM cs.GR</categories><comments>Accepted for Publication in Astrophysics &amp; Space Science, 16pp, 10
  figures</comments><doi>10.1007/s10509-011-0801-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereoscopic visualization is seldom used in Astrophysical publications and
presentations compared to other scientific fields, e.g., Biochemistry, where it
has been recognized as a valuable tool for decades. We put forth the view that
stereo pairs can be a useful tool for the Astrophysics community in
communicating a truer representation of astrophysical data. Here, we review the
main theoretical aspects of stereoscopy, and present a tutorial to easily
create stereo pairs using Python. We then describe how stereo pairs provide a
way to incorporate 3D data in 2D publications of standard journals. We
illustrate the use of stereo pairs with one conceptual and two Astrophysical
science examples: an integral field spectroscopy study of a supernova remnant,
and numerical simulations of a relativistic AGN jet. We also use these examples
to make the case that stereo pairs are not merely an ostentatious way to
present data, but an enhancement in the communication of scientific results in
publications because they provide the reader with a realistic view of
multi-dimensional data, be it of observational or theoretical nature. In
recognition of the ongoing 3D expansion in the commercial sector, we advocate
an increased use of stereo pairs in Astrophysics publications and presentations
as a first step towards new interactive and multi-dimensional publication
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2722</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2722</id><created>2011-07-13</created><updated>2012-02-17</updated><authors><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Mans</keyname><forenames>Bernard</forenames></author><author><keyname>Mathieson</keyname><forenames>Luke</forenames></author></authors><title>On the Feasibility of Maintenance Algorithms in Dynamic Graphs</title><categories>cs.CC cs.DS</categories><msc-class>68Q17</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Near ubiquitous mobile computing has led to intense interest in dynamic graph
theory. This provides a new and challenging setting for algorithmics and
complexity theory. For any graph-based problem, the rapid evolution of a
(possibly disconnected) graph over time naturally leads to the important
complexity question: is it better to calculate a new solution from scratch or
to adapt the known solution on the prior graph to quickly provide a solution of
guaranteed quality for the changed graph?
  In this paper, we demonstrate that the former is the best approach in some
cases, but that there are cases where the latter is feasible. We prove that,
under certain conditions, hard problems cannot even be approximated in any
reasonable complexity bound --- i.e., even with a large amount of time, having
a solution to a very similar graph does not help in computing a solution to the
current graph. To achieve this, we formalize the idea as a maintenance
algorithm. Using r-Regular Subgraph as the primary example we show that
W[1]-hardness for the parameterized approximation problem implies the
non-existence of a maintenance algorithm for the given approximation ratio.
Conversely we show that Vertex Cover, which is fixed-parameter tractable, has a
2-approximate maintenance algorithm. The implications of NP-hardness and
NPO-hardness are also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2723</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2723</id><created>2011-07-14</created><authors><author><keyname>Bag</keyname><forenames>Soumen</forenames></author><author><keyname>Harit</keyname><forenames>Gaurav</forenames></author></authors><title>Topographic Feature Extraction for Bengali and Hindi Character Images</title><categories>cs.CV</categories><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ),
  vol.2, no.2, pp. 181-196, June 2011</journal-ref><doi>10.5121/sipij.2011.2215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection and extraction plays an important role in different
classification based problems such as face recognition, signature verification,
optical character recognition (OCR) etc. The performance of OCR highly depends
on the proper selection and extraction of feature set. In this paper, we
present novel features based on the topography of a character as visible from
different viewing directions on a 2D plane. By topography of a character we
mean the structural features of the strokes and their spatial relations. In
this work we develop topographic features of strokes visible with respect to
views from different directions (e.g. North, South, East, and West). We
consider three types of topographic features: closed region, convexity of
strokes, and straight line strokes. These features are represented as a
shape-based graph which acts as an invariant feature set for discriminating
very similar type characters efficiently. We have tested the proposed method on
printed and handwritten Bengali and Hindi character images. Initial results
demonstrate the efficacy of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2727</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2727</id><created>2011-07-14</created><authors><author><keyname>Shah</keyname><forenames>Umm-e-Mariya</forenames></author><author><keyname>Shaikh</keyname><forenames>Maqbool Uddin</forenames></author><author><keyname>Shamim</keyname><forenames>Azra</forenames></author><author><keyname>Mehmood</keyname><forenames>Yasir</forenames></author></authors><title>Proposed Quality Evaluation Framework to Incorporate Quality Aspects in
  Web Warehouse Creation</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Warehouse is a read only repository maintained on the web to effectively
handle the relevant data. Web warehouse is a system comprised of various
subsystems and process. It supports the organizations in decision making.
Quality of data store in web warehouse can affect the quality of decision made.
For a valuable decision making it is required to consider the quality aspects
in designing and modelling of a web warehouse. Thus data quality is one of the
most important issues of the web warehousing system. Quality must be
incorporated at different stages of the web warehousing system development. It
is necessary to enhance existing data warehousing system to increase the data
quality. It results in the storage of high quality data in the repository and
efficient decision making. In this paper a Quality Evaluation Framework is
proposed keeping in view the quality dimensions associated with different
phases of a web warehouse. Further more, the proposed framework is validated
empirically with the help of quantitative analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2729</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2729</id><created>2011-07-14</created><authors><author><keyname>Goto</keyname><forenames>Keisuke</forenames></author><author><keyname>Maruyama</keyname><forenames>Shirou</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Restructuring Compressed Texts without Explicit Decompression</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of {\em restructuring} compressed texts without
explicit decompression. We present algorithms which allow conversions from
compressed representations of a string $T$ produced by any grammar-based
compression algorithm, to representations produced by several specific
compression algorithms including LZ77, LZ78, run length encoding, and some
grammar based compression algorithms. These are the first algorithms that
achieve running times polynomial in the size of the compressed input and output
representations of $T$. Since most of the representations we consider can
achieve exponential compression, our algorithms are theoretically faster in the
worst case, than any algorithm which first decompresses the string for the
conversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2737</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2737</id><created>2011-07-14</created><updated>2011-09-25</updated><authors><author><keyname>Boufkhad</keyname><forenames>Yacine</forenames></author><author><keyname>Dubois</keyname><forenames>Olivier</forenames></author></authors><title>Second moment method for a family of boolean CSP</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of phase transitions in random boolean Constraint Satisfaction
Problems (CSP) is based on two fundamental tools: the first and second moment
methods. While the first moment method on the number of solutions permits to
compute upper bounds on any boolean CSP, the second moment method used for
computing lower bounds proves to be more tricky and in most cases gives only
the trivial lower bound 0. In this paper, we define a subclass of boolean CSP
covering the monotone versions of many known NP-Complete boolean CSPs. We give
a method for computing non trivial lower bounds for any member of this
subclass. This is achieved thanks to an application of the second moment method
to some selected solutions called characteristic solutions that depend on the
boolean CSP considered. We apply this method with a finer analysis to establish
that the threshold $r_{k}$ (ratio : #constrains/#variables) of monotone
1-in-k-SAT is $\log k/k\leq r_{k}\leq\log^{2}k/k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2757</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2757</id><created>2011-07-14</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Subset sum phase transitions and data compression</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>14 pages, submitted to the Journal of Statistical Mechanics: Theory
  and Experiment</comments><doi>10.1088/1742-5468/2011/09/P09017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a rigorous analysis approach for the subset sum problem in the
context of lossless data compression, where the phase transition of the subset
sum problem is directly related to the passage between ambiguous and
non-ambiguous decompression, for a compression scheme that is based on
specifying the sequence composition. The proposed analysis lends itself to
straightforward extensions in several directions of interest, including
non-binary alphabets, incorporation of side information at the decoder
(Slepian-Wolf coding), and coding schemes based on multiple subset sums. It is
also demonstrated that the proposed technique can be used to analyze the
critical behavior in a more involved situation where the sequence composition
is not specified by the encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2781</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2781</id><created>2011-07-14</created><authors><author><keyname>Cohen</keyname><forenames>Rami</forenames></author></authors><title>Face Recognition using Curvelet Transform</title><categories>cs.CV</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Face recognition has been studied extensively for more than 20 years now.
Since the beginning of 90s the subject has became a major issue. This
technology is used in many important real-world applications, such as video
surveillance, smart cards, database security, internet and intranet access.
This report reviews recent two algorithms for face recognition which take
advantage of a relatively new multiscale geometric analysis tool - Curvelet
transform, for facial processing and feature extraction. This transform proves
to be efficient especially due to its good ability to detect curves and lines,
which characterize the human's face. An algorithm which is based on the two
algorithms mentioned above is proposed, and its performance is evaluated on
three data bases of faces: AT&amp;T (ORL), Essex Grimace and Georgia-Tech.
k-nearest neighbour (k-NN) and Support vector machine (SVM) classifiers are
used, along with Principal Component Analysis (PCA) for dimensionality
reduction. This algorithm shows good results, and it even outperforms other
algorithms in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2782</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2782</id><created>2011-07-14</created><updated>2011-07-15</updated><authors><author><keyname>Cohen</keyname><forenames>Rami</forenames></author></authors><title>The Chan-Vese Algorithm</title><categories>cs.CV math.AP</categories><comments>18 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Segmentation is the process of partitioning a digital image into multiple
segments (sets of pixels). Such common segmentation tasks including segmenting
written text or segmenting tumors from healthy brain tissue in an MRI image,
etc. Chan-Vese model for active contours is a powerful and flexible method
which is able to segment many types of images, including some that would be
quite difficult to segment in means of &quot;classical&quot; segmentation - i.e., using
thresholding or gradient based methods. This model is based on the Mumford-Shah
functional for segmentation, and is used widely in the medical imaging field,
especially for the segmentation of the brain, heart and trachea. The model is
based on an energy minimization problem, which can be reformulated in the level
set formulation, leading to an easier way to solve the problem. In this
project, the model will be presented (there is an extension to color
(vector-valued) images, but it will not be considered here), and Matlab code
that implements it will be introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2788</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2788</id><created>2011-07-14</created><updated>2011-11-07</updated><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author></authors><title>Diverse Consequences of Algorithmic Probability</title><categories>cs.IT cs.AI cs.CY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reminisce and discuss applications of algorithmic probability to a wide
range of problems in artificial intelligence, philosophy and technological
society. We propose that Solomonoff has effectively axiomatized the field of
artificial intelligence, therefore establishing it as a rigorous scientific
discipline. We also relate to our own work in incremental machine learning and
philosophy of complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2794</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2794</id><created>2011-07-14</created><authors><author><keyname>Wang</keyname><forenames>Y.</forenames></author><author><keyname>Zeng</keyname><forenames>A.</forenames></author><author><keyname>Di</keyname><forenames>Z.</forenames></author><author><keyname>Fan</keyname><forenames>Y.</forenames></author></authors><title>Enhancing synchronization in growing networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1209/0295-5075/96/58007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most real systems are growing. In order to model the evolution of real
systems, many growing network models have been proposed to reproduce some
specific topology properties. As the structure strongly influences the network
function, designing the function-aimed growing strategy is also a significant
task with many potential applications. In this letter, we focus on
synchronization in the growing networks. In order to enhance the
synchronizability during the network evolution, we propose the Spectral-Based
Growing (SBG) strategy. Based on the linear stability analysis of
synchronization, we show that our growing mechanism yields better
synchronizability than the existing topology-aimed growing strategies in both
artificial and real-world networks. We also observe that there is an optimal
degree of new added nodes, which means adding nodes with neither too large nor
too low degree could enhance the synchronizability. Furthermore, some topology
measurements are considered in the resultant networks. The results show that
the degree, node betweenness centrality from SBG strategy are more homogenous
than those from other growing strategies. Our work highlights the importance of
the function-aimed growth of the networks and deepens our understanding of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2807</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2807</id><created>2011-07-14</created><authors><author><keyname>Flach</keyname><forenames>Boris</forenames></author><author><keyname>Schlesinger</keyname><forenames>Dmitrij</forenames></author></authors><title>Modelling Distributed Shape Priors by Gibbs Random Fields of Second
  Order</title><categories>cs.CV cs.LG</categories><comments>17 pages, 8 figures</comments><journal-ref>Control Systems and Computers, (2) 2011, pp 14-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the potential of Gibbs Random Fields for shape prior modelling. We
show that the expressive power of second order GRFs is already sufficient to
express simple shapes and spatial relations between them simultaneously. This
allows to model and recognise complex shapes as spatial compositions of simpler
parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2822</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2822</id><created>2011-07-14</created><authors><author><keyname>Sertkaya</keyname><forenames>Baris</forenames></author></authors><title>A Survey on how Description Logic Ontologies Benefit from Formal Concept
  Analysis</title><categories>cs.LO cs.AI</categories><comments>Invited paper that appeared in the Proceedings of the 7th
  International Conference on Concept Lattices and Their Applications, (CLA
  2010)</comments><journal-ref>Proceedings of the 7th International Conference on Concept
  Lattices and Their Applications, (CLA 2010), volume 672 of CEUR Workshop
  Proceedings, pages 2-21. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the notion of a concept as a collection of objects sharing certain
properties, and the notion of a conceptual hierarchy are fundamental to both
Formal Concept Analysis and Description Logics, the ways concepts are described
and obtained differ significantly between these two research areas. Despite
these differences, there have been several attempts to bridge the gap between
these two formalisms, and attempts to apply methods from one field in the
other. The present work aims to give an overview on the research done in
combining Description Logics and Formal Concept Analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2859</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2859</id><created>2011-07-14</created><authors><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Chua</keyname><forenames>Tat-Seng</forenames></author><author><keyname>Jain</keyname><forenames>Ramesh</forenames></author></authors><title>Label-Specific Training Set Construction from Web Resource for Image
  Annotation</title><categories>cs.MM cs.CV</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently many research efforts have been devoted to image annotation by
leveraging on the associated tags/keywords of web images as training labels. A
key issue to resolve is the relatively low accuracy of the tags. In this paper,
we propose a novel semi-automatic framework to construct a more accurate and
effective training set from these web media resources for each label that we
want to learn. Experiments conducted on a real-world dataset demonstrate that
the constructed training set can result in higher accuracy for image
annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2867</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2867</id><created>2011-07-14</created><authors><author><keyname>Kumar</keyname><forenames>A. Dinesh</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>A Two Stage Selective Averaging LDPC Decoding</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low density parity-check (LDPC) codes are a class of linear block codes that
are decoded by running belief propagation (BP) algorithm or log-likelihood
ratio belief propagation (LLR-BP) over the factor graph of the code. One of the
disadvantages of LDPC codes is the onset of an error floor at high values of
signal to noise ratio caused by trapping sets. In this paper, we propose a two
stage decoder to deal with different types of trapping sets. Oscillating
trapping sets are taken care by the first stage of the decoder and the
elementary trapping sets are handled by the second stage of the decoder.
Simulation results on regular PEG (504,252,3,6) code shows that the proposed
two stage decoder performs significantly better than the standard decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2869</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2869</id><created>2011-07-14</created><authors><author><keyname>Badanidiyuru</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author></authors><title>Optimization with Demand Oracles</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study \emph{combinatorial procurement auctions}, where a buyer with a
valuation function $v$ and budget $B$ wishes to buy a set of items. Each item
$i$ has a cost $c_i$ and the buyer is interested in a set $S$ that maximizes
$v(S)$ subject to $\Sigma_{i\in S}c_i\leq B$. Special cases of combinatorial
procurement auctions are classical problems from submodular optimization. In
particular, when the costs are all equal (\emph{cardinality constraint}), a
classic result by Nemhauser et al shows that the greedy algorithm provides an
$\frac e {e-1}$ approximation.
  Motivated by many papers that utilize demand queries to elicit the
preferences of agents in economic settings, we develop algorithms that
guarantee improved approximation ratios in the presence of demand oracles. We
are able to break the $\frac e {e-1}$ barrier: we present algorithms that use
only polynomially many demand queries and have approximation ratios of $\frac 9
8+\epsilon$ for the general problem and $\frac 9 8$ for maximization subject to
a cardinality constraint.
  We also consider the more general class of subadditive valuations. We present
algorithms that obtain an approximation ratio of $2+\epsilon$ for the general
problem and 2 for maximization subject to a cardinality constraint. We
guarantee these approximation ratios even when the valuations are non-monotone.
We show that these ratios are essentially optimal, in the sense that for any
constant $\epsilon&gt;0$, obtaining an approximation ratio of $2-\epsilon$
requires exponentially many demand queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2875</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2875</id><created>2011-07-14</created><authors><author><keyname>Aholt</keyname><forenames>Chris</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>Thomas</keyname><forenames>Rekha</forenames></author></authors><title>A Hilbert Scheme in Computer Vision</title><categories>math.AG cs.CV</categories><comments>26 pages</comments><report-no>Mittag-Leffler-2011spring</report-no><msc-class>14N, 14Q, 68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiview geometry is the study of two-dimensional images of
three-dimensional scenes, a foundational subject in computer vision. We
determine a universal Groebner basis for the multiview ideal of n generic
cameras. As the cameras move, the multiview varieties vary in a family of
dimension 11n-15. This family is the distinguished component of a multigraded
Hilbert scheme with a unique Borel-fixed point. We present a combinatorial
study of ideals lying on that Hilbert scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2879</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2879</id><created>2011-07-14</created><updated>2011-11-04</updated><authors><author><keyname>Beguerisse-Diaz</keyname><forenames>Mariano</forenames></author><author><keyname>Wang</keyname><forenames>Baojun</forenames></author><author><keyname>Desikan</keyname><forenames>Radhika</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Squeeze-and-Breathe Evolutionary Monte Carlo Optimisation with Local
  Search Acceleration and its application to parameter fitting</title><categories>q-bio.QM cs.SY math.OC</categories><comments>15 Pages, 3 Figures, 6 Tables; Availability: Matlab code available
  from the authors upon request</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Estimating parameters from data is a key stage of the modelling
process, particularly in biological systems where many parameters need to be
estimated from sparse and noisy data sets. Over the years, a variety of
heuristics have been proposed to solve this complex optimisation problem, with
good results in some cases yet with limitations in the biological setting.
  Results: In this work, we develop an algorithm for model parameter fitting
that combines ideas from evolutionary algorithms, sequential Monte Carlo and
direct search optimisation. Our method performs well even when the order of
magnitude and/or the range of the parameters is unknown. The method refines
iteratively a sequence of parameter distributions through local optimisation
combined with partial resampling from a historical prior defined over the
support of all previous iterations. We exemplify our method with biological
models using both simulated and real experimental data and estimate the
parameters efficiently even in the absence of a priori knowledge about the
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2881</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2881</id><created>2011-07-14</created><authors><author><keyname>Guti&#xe9;rrez</keyname><forenames>Francisco</forenames></author><author><keyname>Argoty</keyname><forenames>Camilo</forenames></author><author><keyname>Moreno</keyname><forenames>Stefany</forenames></author></authors><title>A generalization of a classical model in contract theory: The agent
  behavior</title><categories>math.OC cs.GT</categories><msc-class>35Q91, 90B70, 91A13, 91A26, 91A30, 91A40, 91A80, 91B06, 91B16,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a first approximation of agent behaviour in a generalized model in
contract theory. This model relaxes some of the the assumptions of one of the
classical models allowing to include a broader range of agents. We introduce
the motivation for the agent and reinterpret the classical definition of risk
perception. Besides, we analyze different scenarios for the relation between
the effort exerted by the agent and the probability that he gets an especfic
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2900</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2900</id><created>2011-07-14</created><updated>2014-01-02</updated><authors><author><keyname>Cominetti</keyname><forenames>Roberto</forenames></author><author><keyname>Guzman</keyname><forenames>Cristobal</forenames></author></authors><title>Network Congestion Control with Markovian Multipath Routing</title><categories>cs.NI cs.SY math.OC</categories><comments>10 pages, 2 figures. Published at NETGCOOP 2011, and Mathematical
  Programming Series A. Final version available at
  http://link.springer.com/article/10.1007/s10107-013-0719-z</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider an integrated model for TCP/IP protocols with
multipath routing. The model combines a Network Utility Maximization for rate
control based on end-to-end queuing delays, with a Markovian Traffic
Equilibrium for routing based on total expected delays. We prove the existence
of a unique equilibrium state which is characterized as the solution of an
unconstrained strictly convex program. A distributed algorithm for solving this
optimization problem is proposed, with a brief discussion of how it can be
implemented by adapting the current Internet protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2949</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2949</id><created>2011-07-14</created><updated>2011-11-29</updated><authors><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>Geometric Packing under Non-uniform Constraints</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of discrete geometric packing. Here, given weighted
regions (say in the plane) and points (with capacities), one has to pick a
maximum weight subset of the regions such that no point is covered more than
its capacity. We provide a general framework and an algorithm for approximating
the optimal solution for packing in hypergraphs arising out of such geometric
settings. Using this framework we get a flotilla of results on this problem
(and also on its dual, where one wants to pick a maximum weight subset of the
points when the regions have capacities). For example, for the case of fat
triangles of similar size, we show an O(1)-approximation and prove that no
\PTAS is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2957</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2957</id><created>2011-07-14</created><authors><author><keyname>Fleischer</keyname><forenames>Lisa</forenames></author><author><keyname>Wang</keyname><forenames>Zhenghui</forenames></author></authors><title>Lower Bound for Envy-Free and Truthful Makespan Approximation on Related
  Machines</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study problems of scheduling jobs on related machines so as to minimize
the makespan in the setting where machines are strategic agents. In this
problem, each job $j$ has a length $l_{j}$ and each machine $i$ has a private
speed $t_{i}$. The running time of job $j$ on machine $i$ is $t_{i}l_{j}$. We
seek a mechanism that obtains speed bids of machines and then assign jobs and
payments to machines so that the machines have incentive to report true speeds
and the allocation and payments are also envy-free. We show that
  1. A deterministic envy-free, truthful, individually rational, and anonymous
mechanism cannot approximate the makespan strictly better than $2-1/m$, where
$m$ is the number of machines. This result contrasts with prior work giving a
deterministic PTAS for envy-free anonymous assignment and a distinct
deterministic PTAS for truthful anonymous mechanism.
  2. For two machines of different speeds, the unique deterministic scalable
allocation of any envy-free, truthful, individually rational, and anonymous
mechanism is to allocate all jobs to the quickest machine. This allocation is
the same as that of the VCG mechanism, yielding a 2-approximation to the
minimum makespan.
  3. No payments can make any of the prior published monotone and locally
efficient allocations that yield better than an $m$-approximation for $\qcmax$
\cite{aas, at,ck10, dddr, kovacs} a truthful, envy-free, individually rational,
and anonymous mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2972</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2972</id><created>2011-07-14</created><authors><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>An MCMC Approach to Universal Lossy Compression of Analog Sources</title><categories>cs.IT math.IT</categories><comments>21 pages, submitted for publication</comments><doi>10.1109/TSP.2012.2206585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the Markov chain Monte Carlo (MCMC) approach to the compression
of discrete sources developed by Jalali and Weissman, we propose a lossy
compression algorithm for analog sources that relies on a finite reproduction
alphabet, which grows with the input length. The algorithm achieves, in an
appropriate asymptotic sense, the optimum Shannon theoretic tradeoff between
rate and distortion, universally for stationary ergodic continuous amplitude
sources. We further propose an MCMC-based algorithm that resorts to a reduced
reproduction alphabet when such reduction does not prevent achieving the
Shannon limit. The latter algorithm is advantageous due to its reduced
complexity and improved rates of convergence when employed on sources with a
finite and small optimum reproduction alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2973</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2973</id><created>2011-07-14</created><authors><author><keyname>Gough</keyname><forenames>J. E.</forenames></author><author><keyname>James</keyname><forenames>M. R.</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author></authors><title>Quantum Master Equation and Filter for Systems Driven by Fields in a
  Single Photon State</title><categories>quant-ph cs.SY math.OC</categories><comments>7 pages, 2 figures. Accepted for publication in the joint 50th IEEE
  Conference on Decision and Control (CDC) and European Control Conference
  (ECC), 2011 (http://control.disp.uniroma2.it/cdcecc2011/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to determine quantum master and filter equations for
systems coupled to continuous-mode single photon fields. The system and field
are described using a quantum stochastic unitary model, where the
continuous-mode single photon state for the field is determined by a wavepacket
pulse shape. The master equation is derived from this model and is given in
terms of a system of coupled equations. The output field carries information
about the system from the scattered photon, and is continuously monitored. The
quantum filter is determined with the aid of an embedding of the system into a
larger system, and is given by a system of coupled stochastic differential
equations. An example is provided to illustrate the main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2974</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2974</id><created>2011-07-14</created><authors><author><keyname>Gough</keyname><forenames>J. E.</forenames></author><author><keyname>James</keyname><forenames>M. R.</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author></authors><title>Quantum Filtering for Systems Driven by Fields in Single Photon States
  and Superposition of Coherent States using Non-Markovian Embeddings</title><categories>quant-ph cs.SY math.OC</categories><comments>28 pages, 1 figure. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to determine quantum master and filter equations
for systems coupled to fields in certain non-classical continuous-mode states.
Specifically, we consider two types of field states (i) single photon states,
and (ii) superpositions of coherent states. The system and field are described
using a quantum stochastic unitary model. Master equations are derived from
this model and are given in terms of systems of coupled equations. The output
field carries information about the system, and is continuously monitored. The
quantum filters are determined with the aid of an embedding of the system into
a larger non-Markovian system, and are given by a system of coupled stochastic
differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2976</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2976</id><created>2011-07-14</created><updated>2012-08-30</updated><authors><author><keyname>Gough</keyname><forenames>J. E.</forenames></author><author><keyname>James</keyname><forenames>M. R.</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author><author><keyname>Combes</keyname><forenames>Joshua</forenames></author></authors><title>Quantum Filtering (Quantum Trajectories) for Systems Driven by Fields in
  Single Photon States and Superposition of Coherent States</title><categories>quant-ph cs.SY math.OC</categories><comments>To appear in Physical Review A, 2012</comments><journal-ref>Phys. Rev. A 86, 043819 (2012)</journal-ref><doi>10.1103/PhysRevA.86.043819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the stochastic master equations, that is to say, quantum filters,
and master equations for an arbitrary quantum system probed by a
continuous-mode bosonic input field in two types of non-classical states.
Specifically, we consider the cases where the state of the input field is a
superposition or combination of: (1) a continuous-mode single photon wave
packet and vacuum, and (2) any number of continuous-mode coherent states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2983</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2983</id><created>2011-07-14</created><updated>2012-07-04</updated><authors><author><keyname>Matsutani</keyname><forenames>Shigeki</forenames></author><author><keyname>Shimosako</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Wang</keyname><forenames>Yunhong</forenames></author></authors><title>Fractal Structure of Equipotential Curves on a Continuum Percolation
  Model</title><categories>cs.NA cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP</categories><comments>17 pages 7 figures</comments><doi>10.1016/j.physa.2012.06.056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We numerically investigate the electric potential distribution over a
two-dimensional continuum percolation model between the electrodes. The model
consists of overlapped conductive particles on the background with an
infinitesimal conductivity. Using the finite difference method, we solve the
generalized Laplace equation and show that in the potential distribution, there
appear the {\it{quasi-equipotential clusters}} which approximately and locally
have the same values like steps and stairs. Since the quasi-equipotential
clusters has the fractal structure, we compute the fractal dimension of
equipotential curves and its dependence on the volume fraction over $[0,1]$.
The fractal dimension in [1.00, 1.257] has a peak at the percolation threshold
$p_c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2984</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2984</id><created>2011-07-14</created><authors><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author><author><keyname>Ikeda</keyname><forenames>Shiro</forenames></author><author><keyname>Manton</keyname><forenames>Jonathan H.</forenames></author></authors><title>An Introductory Review of Information Theory in the Context of
  Computational Neuroscience</title><categories>cs.IT math.IT</categories><comments>18 pages, 7 figures, to appear in Biological Cybernetics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces several fundamental concepts in information theory from
the perspective of their origins in engineering. Understanding such concepts is
important in neuroscience for two reasons. Simply applying formulae from
information theory without understanding the assumptions behind their
definitions can lead to erroneous results and conclusions. Furthermore, this
century will see a convergence of information theory and neuroscience;
information theory will expand its foundations to incorporate more
comprehensively biological processes thereby helping reveal how neuronal
networks achieve their remarkable information processing abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2990</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2990</id><created>2011-07-15</created><updated>2013-12-02</updated><authors><author><keyname>Kentros</keyname><forenames>Sotirios</forenames></author><author><keyname>Kiayias</keyname><forenames>Aggelos</forenames></author></authors><title>Solving the At-Most-Once Problem with Nearly Optimal Effectiveness</title><categories>cs.DC</categories><comments>Updated Version. A Brief Announcement was published in PODC 2011. An
  Extended Abstract was published in the proceeding of ICDCN 2012. A full
  version was published in Theoretical Computer Science, Volume 496, 22 July
  2013, Pages 69 - 88</comments><acm-class>F.1.2; F.2.m</acm-class><journal-ref>Theoretical Computer Science, Volume 496, 22 July 2013, Pages
  69-88, ISSN 0304-3975</journal-ref><doi>10.1016/j.tcs.2013.04.017</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present and analyze a wait-free deterministic algorithm for solving the
at-most-once problem: how m shared-memory fail-prone processes perform
asynchronously n jobs at most once. Our algorithmic strategy provides for the
first time nearly optimal effectiveness, which is a measure that expresses the
total number of jobs completed in the worst case. The effectiveness of our
algorithm equals n-2m+2. This is up to an additive factor of m close to the
known effectiveness upper bound n-m+1 over all possible algorithms and improves
on the previously best known deterministic solutions that have effectiveness
only n-log m o(n). We also present an iterative version of our algorithm that
for any $m = O\left(\sqrt[3+\epsilon]{n/\log n}\right)$ is both
effectiveness-optimal and work-optimal, for any constant $\epsilon &gt; 0$. We
then employ this algorithm to provide a new algorithmic solution for the
Write-All problem which is work optimal for any
$m=O\left(\sqrt[3+\epsilon]{n/\log n}\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2994</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2994</id><created>2011-07-15</created><updated>2012-03-22</updated><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Budget Feasible Mechanism Design via Random Sampling</title><categories>cs.GT</categories><comments>Updated version please refer to http://arxiv.org/abs/1203.4455</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Budget feasible mechanism considers algorithmic mechanism design questions
where there is a budget constraint on the total payment of the mechanism. An
important question in the field is that under which valuation domains there
exist budget feasible mechanisms that admit `small' approximations (compared to
a socially optimal solution). Singer \cite{PS10} showed that additive and
submodular functions admit a constant approximation mechanism. Recently,
Dobzinski, Papadimitriou, and Singer \cite{DPS11} gave an $O(\log^2n)$
approximation mechanism for subadditive functions and remarked that: &quot;A
fundamental question is whether, regardless of computational constraints, a
constant-factor budget feasible mechanism exists for subadditive function.&quot;
  In this paper, we give the first attempt to this question. We give a
polynomial time $O(\frac{\log n}{\log\log n})$ sub-logarithmic approximation
ratio mechanism for subadditive functions, improving the best known ratio
$O(\log^2 n)$. Further, we connect budget feasible mechanism design to the
concept of approximate core in cooperative game theory, and show that there is
a mechanism for subadditive functions whose approximation is, via a
characterization of the integrality gap of a linear program, linear to the
largest value to which an approximate core exists. Our result implies in
particular that the class of XOS functions, which is a superclass of submodular
functions, admits a constant approximation mechanism. We believe that our work
could be a solid step towards solving the above fundamental problem eventually,
and possibly, with an affirmative answer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.2997</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.2997</id><created>2011-07-15</created><authors><author><keyname>Chai</keyname><forenames>Junyi</forenames></author><author><keyname>Liu</keyname><forenames>James N. K.</forenames></author></authors><title>An Ontology-driven Framework for Supporting Complex Decision Process</title><categories>cs.AI</categories><comments>Paper presented at the 2010 World Automation Congress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study proposes a framework of ONTOlogy-based Group Decision Support
System (ONTOGDSS) for decision process which exhibits the complex structure of
decision-problem and decision-group. It is capable of reducing the complexity
of problem structure and group relations. The system allows decision makers to
participate in group decision-making through the web environment, via the
ontology relation. It facilitates the management of decision process as a
whole, from criteria generation, alternative evaluation, and opinion
interaction to decision aggregation. The embedded ontology structure in
ONTOGDSS provides the important formal description features to facilitate
decision analysis and verification. It examines the software architecture, the
selection methods, the decision path, etc. Finally, the ontology application of
this system is illustrated with specific real case to demonstrate its
potentials towards decision-making development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.3013</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.3013</id><created>2011-07-15</created><authors><author><keyname>Jones</keyname><forenames>Thouis R.</forenames></author><author><keyname>Karger</keyname><forenames>David R.</forenames></author></authors><title>Linear-Time Poisson-Disk Patterns</title><categories>cs.GR</categories><comments>4 pages, 2 figures</comments><doi>10.1080/2151237X.2011.617173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for generating Poisson-disc patterns taking O(N) time
to generate $N$ points. The method is based on a grid of regions which can
contain no more than one point in the final pattern, and uses an explicit model
of point arrival times under a uniform Poisson process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.3019</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.3019</id><created>2011-07-15</created><authors><author><keyname>Goto</keyname><forenames>Keisuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Computing q-gram Frequencies on Collage Systems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collage systems are a general framework for representing outputs of various
text compression algorithms. We consider the all $q$-gram frequency problem on
compressed string represented as a collage system, and present an $O((q+h\log
n)n)$-time $O(qn)$-space algorithm for calculating the frequencies for all
$q$-grams that occur in the string. Here, $n$ and $h$ are respectively the size
and height of the collage system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.3022</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.3022</id><created>2011-07-15</created><authors><author><keyname>Goto</keyname><forenames>Keisuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Computing q-gram Non-overlapping Frequencies on SLP Compressed Texts</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Length-$q$ substrings, or $q$-grams, can represent important characteristics
of text data, and determining the frequencies of all $q$-grams contained in the
data is an important problem with many applications in the field of data mining
and machine learning. In this paper, we consider the problem of calculating the
{\em non-overlapping frequencies} of all $q$-grams in a text given in
compressed form, namely, as a straight line program (SLP). We show that the
problem can be solved in $O(q^2n)$ time and $O(qn)$ space where $n$ is the size
of the SLP. This generalizes and greatly improves previous work (Inenaga &amp;
Bannai, 2009) which solved the problem only for $q=2$ in $O(n^4\log n)$ time
and $O(n^3)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.3031</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.3031</id><created>2011-07-15</created><updated>2011-09-07</updated><authors><author><keyname>Fiore</keyname><forenames>Marcelo</forenames><affiliation>University of Cambridge, Computer Laboratory</affiliation></author><author><keyname>Hur</keyname><forenames>Chung-Kil</forenames><affiliation>Universite Paris Diderot - Paris 7, Laboratoire PPS</affiliation></author></authors><title>On the mathematical synthesis of equational logics</title><categories>cs.LO math.CT math.LO</categories><comments>Final version for publication in Logical Methods in Computer Science</comments><proxy>LMCS</proxy><acm-class>D.3.1, F.3.1, F.3.2, F.4.1, I.2.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  8, 2011) lmcs:1071</journal-ref><doi>10.2168/LMCS-7(3:12)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a mathematical theory and methodology for synthesising equational
logics from algebraic metatheories. We illustrate our methodology by means of
two applications: a rational reconstruction of Birkhoff's Equational Logic and
a new equational logic for reasoning about algebraic structure with
name-binding operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1107.3033</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1107.3033</id><created>2011-07-15</created><authors><author><keyname>Jin</keyname><forenames>Emma Yu</forenames></author><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author></authors><title>The Expected Order of Saturated RNA Secondary Structures</title><categories>math.CO cs.IT math.IT</categories><comments>2 figures</comments><msc-class>05A16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the expected order of RNA saturated secondary structures of size $n$
is $\log_4n(1+O(\frac{\log_2n}{n}))$, if we select the saturated secondary
structure uniformly at random. Furthermore, the order of saturated secondary
structures is sharply concentrated around its mean. As a consequence saturated
structures and structures in the traditional model behave the same with respect
to the expected order. Thus we may conclude that the traditional model has
already drawn the right picture and conclusions inferred from it with respect
to the order (the overall shape) of a structure remain valid even if enforcing
saturation (at least in expectation).
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="22000" completeListSize="102538">1122234|23001</resumptionToken>
</ListRecords>
</OAI-PMH>
