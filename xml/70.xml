<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:39:17Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|69001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5878</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5878</id><created>2014-11-18</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Jiang</keyname><forenames>Huaizu</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author></authors><title>Salient Object Detection: A Survey</title><categories>cs.CV cs.AI q-bio.NC</categories><comments>26 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and segmenting salient objects in natural scenes, also known as
salient object detection, has attracted a lot of focused research in computer
vision and has resulted in many applications. However, while many such models
exist, a deep understanding of achievements and issues is lacking. We aim to
provide a comprehensive review of the recent progress in this field. We situate
salient object detection among other closely related areas such as generic
scene segmentation, object proposal generation, and saliency for fixation
prediction. Covering 256 publications we survey i) roots, key concepts, and
tasks, ii) core techniques and main modeling trends, and iii) datasets and
evaluation metrics in salient object detection. We also discuss open problems
such as evaluation metrics and dataset bias in model performance and suggest
future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5879</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5879</id><created>2014-11-18</created><updated>2014-12-09</updated><authors><author><keyname>Hwang</keyname><forenames>Sung Ju</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author></authors><title>A Unified Semantic Embedding: Relating Taxonomies and Attributes</title><categories>cs.CV</categories><comments>To Appear in NIPS 2014 Learning Semantics Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method that learns a discriminative yet semantic space for
object categorization, where we also embed auxiliary semantic entities such as
supercategories and attributes. Contrary to prior work which only utilized them
as side information, we explicitly embed the semantic entities into the same
space where we embed categories, which enables us to represent a category as
their linear combination. By exploiting such a unified model for semantics, we
enforce each category to be represented by a supercategory + sparse combination
of attributes, with an additional exclusive regularization to learn
discriminative composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5881</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5881</id><created>2014-11-20</created><updated>2014-11-25</updated><authors><author><keyname>Hussain</keyname><forenames>Shaista</forenames></author><author><keyname>Liu</keyname><forenames>Shih-Chii</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>Hardware-Amenable Structural Learning for Spike-based Pattern
  Classification using a Simple Model of Active Dendrites</title><categories>cs.NE q-bio.NC</categories><comments>Accepted for publication in Neural Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a spike-based model which employs neurons with
functionally distinct dendritic compartments for classifying high dimensional
binary patterns. The synaptic inputs arriving on each dendritic subunit are
nonlinearly processed before being linearly integrated at the soma, giving the
neuron a capacity to perform a large number of input-output mappings. The model
utilizes sparse synaptic connectivity; where each synapse takes a binary value.
The optimal connection pattern of a neuron is learned by using a simple
hardware-friendly, margin enhancing learning algorithm inspired by the
mechanism of structural plasticity in biological neurons. The learning
algorithm groups correlated synaptic inputs on the same dendritic branch. Since
the learning results in modified connection patterns, it can be incorporated
into current event-based neuromorphic systems with little overhead. This work
also presents a branch-specific spike-based version of this structural
plasticity rule. The proposed model is evaluated on benchmark binary
classification problems and its performance is compared against that achieved
using Support Vector Machine (SVM) and Extreme Learning Machine (ELM)
techniques. Our proposed method attains comparable performance while utilizing
10 to 50% less computational resources than the other reported techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5890</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5890</id><created>2014-11-20</created><authors><author><keyname>Muggli</keyname><forenames>Martin D.</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Ronen</keyname><forenames>Roy</forenames></author><author><keyname>Boucher</keyname><forenames>Christina</forenames></author></authors><title>Misassembly Detection using Paired-End Sequence Reads and Optical
  Mapping Data</title><categories>q-bio.GN cs.CE</categories><comments>14 pages, 4 figures. Submitted to RECOMB. Preparing to submit to
  Genome Biology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crucial problem in genome assembly is the discovery and correction of
misassembly errors in draft genomes. We develop a method that will enhance the
quality of draft genomes by identifying and removing misassembly errors using
paired short read sequence data and optical mapping data. We apply our method
to various assemblies of the loblolly pine and Francisella tularensis genomes.
Our results demonstrate that we detect more than 54% of extensively
misassembled contigs and more than 60% of locally misassembed contigs in an
assembly of Francisella tularensis, and between 31% and 100% of extensively
misassembled contigs and between 57% and 73% of locally misassembed contigs in
the assemblies of loblolly pine. MISSEQUEL can be downloaded at
http://www.cs.colostate.edu/seq/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5899</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5899</id><created>2014-11-21</created><updated>2015-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Fulton</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Falling Rule Lists</title><categories>cs.AI cs.LG</categories><comments>Accepted at AISTATS 2015. Contains number of rules mined, running
  times. in Proceedings of AISTATS 2015. JMLR: W&amp;CP 38</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Falling rule lists are classification models consisting of an ordered list of
if-then rules, where (i) the order of rules determines which example should be
classified by each rule, and (ii) the estimated probability of success
decreases monotonically down the list. These kinds of rule lists are inspired
by healthcare applications where patients would be stratified into risk sets
and the highest at-risk patients should be considered first. We provide a
Bayesian framework for learning falling rule lists that does not rely on
traditional greedy decision tree learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5908</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5908</id><created>2014-11-21</created><updated>2015-06-20</updated><authors><author><keyname>Lenc</keyname><forenames>Karel</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Understanding image representations by measuring their equivariance and
  equivalence</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the importance of image representations such as histograms of
oriented gradients and deep Convolutional Neural Networks (CNN), our
theoretical understanding of them remains limited. Aiming at filling this gap,
we investigate three key mathematical properties of representations:
equivariance, invariance, and equivalence. Equivariance studies how
transformations of the input image are encoded by the representation,
invariance being a special case where a transformation has no effect.
Equivalence studies whether two representations, for example two different
parametrisations of a CNN, capture the same visual information or not. A number
of methods to establish these properties empirically are proposed, including
introducing transformation and stitching layers in CNNs. These methods are then
applied to popular representations to reveal insightful aspects of their
structure, including clarifying at which layers in a CNN certain geometric
invariances are achieved. While the focus of the paper is theoretical, direct
applications to structured-output regression are demonstrated too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5915</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5915</id><created>2014-11-21</created><updated>2016-01-07</updated><authors><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>Robust EM kernel-based methods for linear system identification</title><categories>cs.SY stat.ML</categories><comments>Accepted for publication in Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in system identifi?cation have brought attention to
regularized kernel-based methods. This type of approach has been proven to
compare favorably with classic parametric methods. However, current
formulations are not robust with respect to outliers. In this paper, we
introduce a novel method to robustify kernel-based system identi?cation
methods. To this end, we model the output measurement noise using random
variables with heavy-tailed probability density functions (pdfs), focusing on
the Laplacian and the Student's t distributions. Exploiting the representation
of these pdfs as scale mixtures of Gaussians, we cast our system
identifi?cation problem into a Gaussian process regression framework, which
requires estimating a number of hyperparameters of the data size order. To
overcome this di?culty, we design a new maximum a posteriori (MAP) estimator of
the hyperparameters, and solve the related optimization problem with a novel
iterative scheme based on the Expectation-Maximization (EM) method. In presence
of outliers, tests on simulated data and on a real system show a substantial
performance improvement compared to currently used kernel-based methods for
linear system identifi?cation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5923</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5923</id><created>2014-11-21</created><authors><author><keyname>Lutz</keyname><forenames>Collin C.</forenames></author><author><keyname>Stilwell</keyname><forenames>Daniel J.</forenames></author></authors><title>Stability and disturbance attenuation for a switched Markov jump linear
  system</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a class of Markov jump linear systems that are characterized by
the underlying Markov process being time-inhomogeneous with a priori unknown
transition probabilities. Necessary and sufficient conditions for uniform
stochastic stability and uniform stochastic disturbance attenuation are
reported. In both cases, conditions are expressed as a set of
finite-dimensional linear matrix inequalities that can be solved efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5925</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5925</id><created>2014-11-21</created><updated>2015-12-08</updated><authors><author><keyname>Kariotoglou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kamgarpour</keyname><forenames>Maryam</forenames></author><author><keyname>Summers</keyname><forenames>Tyler</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>A numerical approach to the reach-avoid problem for Markov decision
  processes</title><categories>math.OC cs.SY math.AP math.DS</categories><comments>19 pages, 10 figures, revised draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem in stochastic control is the so-called reach-avoid
problem where one maximizes the probability of reaching a target set while
avoiding unsafe subsets of the state-space. We develop a computational method
for the finite horizon reach-avoid problem for discrete-time Markov decision
processes. Our approach is based on deriving an infinite dimensional linear
program whose solution is equivalent to the value function of the reach-avoid
problem. We then propose a tractable approximation to the infinite linear
program by projecting the value function onto the span of a finite number of
basis functions and using randomized sampling to relax the infinite
constraints. We illustrate the applicability of the method on a large class of
Markov decision processes modeled by Gaussian mixtures and develop benchmark
control and robotic problems to explore the computational tractability and
accuracy of the proposed approach. To the best of our knowledge, this is the
first time that problems up to six state variables and two inputs have been
addressed using the reach-avoid formulation. Our results demonstrate that the
approximation scheme has considerable computational advantages compared to
standard space gridding-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5928</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5928</id><created>2014-11-21</created><updated>2015-12-03</updated><authors><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Tatarchenko</keyname><forenames>Maxim</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Learning to Generate Chairs, Tables and Cars with Convolutional Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>v3: added new object classes, new architectures, many new experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We train generative 'up-convolutional' neural networks which are able to
generate images of objects given object style, viewpoint, and color. We train
the networks on rendered 3D models of chairs, tables, and cars. Our experiments
show that the networks do not merely learn all images by heart, but rather find
a meaningful representation of 3D models allowing them to assess the similarity
of different models, interpolate between given views to generate the missing
ones, extrapolate views, and invent new objects not present in the training set
by recombining training instances, or even two different object classes.
Moreover, we show that such generative networks can be used to find
correspondences between different objects from the dataset, outperforming
existing approaches on this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5935</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5935</id><created>2014-11-18</created><authors><author><keyname>Zia</keyname><forenames>M. Zeeshan</forenames></author><author><keyname>Stark</keyname><forenames>Michael</forenames></author><author><keyname>Schindler</keyname><forenames>Konrad</forenames></author></authors><title>Towards Scene Understanding with Detailed 3D Object Representations</title><categories>cs.CV</categories><comments>International Journal of Computer Vision (appeared online on 4
  November 2014). Online version:
  http://link.springer.com/article/10.1007/s11263-014-0780-y</comments><doi>10.1007/s11263-014-0780-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current approaches to semantic image and scene understanding typically employ
rather simple object representations such as 2D or 3D bounding boxes. While
such coarse models are robust and allow for reliable object detection, they
discard much of the information about objects' 3D shape and pose, and thus do
not lend themselves well to higher-level reasoning. Here, we propose to base
scene understanding on a high-resolution object representation. An object class
- in our case cars - is modeled as a deformable 3D wireframe, which enables
fine-grained modeling at the level of individual vertices and faces. We augment
that model to explicitly include vertex-level occlusion, and embed all
instances in a common coordinate frame, in order to infer and exploit
object-object interactions. Specifically, from a single view we jointly
estimate the shapes and poses of multiple objects in a common 3D frame. A
ground plane in that frame is estimated by consensus among different objects,
which significantly stabilizes monocular 3D pose estimation. The fine-grained
model, in conjunction with the explicit 3D scene model, further allows one to
infer part-level occlusions between the modeled objects, as well as occlusions
by other, unmodeled scene elements. To demonstrate the benefits of such
detailed object class models in the context of scene understanding we
systematically evaluate our approach on the challenging KITTI street scene
dataset. The experiments show that the model's ability to utilize image
evidence at the level of individual parts improves monocular 3D pose estimation
w.r.t. both location and (continuous) viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5943</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5943</id><created>2014-11-21</created><authors><author><keyname>Nienartowicz</keyname><forenames>Krzysztof</forenames></author><author><keyname>Blanco</keyname><forenames>Diego Ord&#xf3;&#xf1;ez</forenames></author><author><keyname>Guy</keyname><forenames>Leanne</forenames></author><author><keyname>Holl</keyname><forenames>Berry</forenames></author><author><keyname>Lecoeur-Ta&#xef;bi</keyname><forenames>Isabelle</forenames></author><author><keyname>Mowlavi</keyname><forenames>Nami</forenames></author><author><keyname>Rimoldini</keyname><forenames>Lorenzo</forenames></author><author><keyname>Ruiz</keyname><forenames>Idoia</forenames></author><author><keyname>S&#xfc;veges</keyname><forenames>Maria</forenames></author><author><keyname>Eyer</keyname><forenames>Laurent</forenames></author></authors><title>Time series data mining for the Gaia variability analysis</title><categories>astro-ph.IM cs.DB cs.DC</categories><comments>4 pages, 3 figures. appears in the Proc. of the 2014 conference on
  Big Data from Space (BiDS14), European Commission, Joint Research Centre, P.
  Soille, P. G. Marchetti (eds)</comments><doi>10.2788/1823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaia is an ESA cornerstone mission, which was successfully launched December
2013 and commenced operations in July 2014. Within the Gaia Data Processing and
Analysis consortium, Coordination Unit 7 (CU7) is responsible for the
variability analysis of over a billion celestial sources and nearly 4 billion
associated time series (photometric, spectrophotometric, and spectroscopic),
encoding information in over 800 billion observations during the 5 years of the
mission, resulting in a petabyte scale analytical problem. In this article, we
briefly describe the solutions we developed to address the challenges of time
series variability analysis: from the structure for a distributed data-oriented
scientific collaboration to architectural choices and specific components used.
Our approach is based on Open Source components with a distributed, partitioned
database as the core to handle incrementally: ingestion, distributed
processing, analysis, results and export in a constrained time window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5945</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5945</id><created>2014-11-21</created><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Penner</keyname><forenames>Orion</forenames></author></authors><title>Inequality and cumulative advantage in science careers: a case study of
  high-impact journals</title><categories>physics.soc-ph cs.DL</categories><comments>2-page summary of the long published version which is available open
  access here at http://www.epjdatascience.com/content/3/1/24</comments><journal-ref>EPJ Data Science 3, 24 (2014)</journal-ref><doi>10.1140/epjds/s13688-014-0024-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing a large data set of publications drawn from the most competitive
journals in the natural and social sciences we show that research careers
exhibit the broad distributions of individual achievement characteristic of
systems in which cumulative advantage plays a key role. While most researchers
are personally aware of the competition implicit in the publication process,
little is known about the levels of inequality at the level of individual
researchers. We analyzed both productivity and impact measures for a large set
of researchers publishing in high-impact journals. For each researcher cohort
we calculated Gini inequality coefficients, with average Gini values around
0.48 for total publications and 0.73 for total citations. For perspective,
these observed values are well in excess of the inequality levels observed for
personal income in developing countries. Investigating possible sources of this
inequality, we identify two potential mechanisms that act at the level of the
individual that may play defining roles in the emergence of the broad
productivity and impact distributions found in science. First, we show that the
average time interval between a researcher's successive publications in top
journals decreases with each subsequent publication. Second, after controlling
for the time dependent features of citation distributions, we compare the
citation impact of subsequent publications within a researcher's publication
record. We find that as researchers continue to publish in top journals, there
is more likely to be a decreasing trend in the relative citation impact with
each subsequent publication. This pattern highlights the difficulty of
repeatedly publishing high-impact research and the intriguing possibility that
confirmation bias plays a role in the evaluation of scientific careers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5951</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5951</id><created>2014-11-21</created><authors><author><keyname>van der Zanden</keyname><forenames>Tom C.</forenames></author><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author></authors><title>PSPACE-completeness of Bloxorz and of Games with 2-Buttons</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bloxorz is an online puzzle game where players move a 1 by 1 by 2 block by
tilting it on a subset of the two dimensional grid. Bloxorz features switches
that open and close trapdoors. The puzzle is to move the block from its initial
position to an upright position on the destination square. We show that the
problem of deciding whether a given Bloxorz level is solvable is
PSPACE-complete and that this remains so even when all trapdoors are initially
closed or all trapdoors are initially open. We also answer an open question of
Viglietta, showing that 2-buttons are sufficient for PSPACE-hardness of general
puzzle games. We also examine the hardness of some variants of Bloxorz,
including variants where the block is a 1 by 1 by 1 cube, and variants with
single-use tiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5977</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5977</id><created>2014-11-21</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Zhou</keyname><forenames>Dengyong</forenames></author></authors><title>On the Impossibility of Convex Inference in Human Computation</title><categories>stat.ML cs.HC cs.LG</categories><comments>AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human computation or crowdsourcing involves joint inference of the
ground-truth-answers and the worker-abilities by optimizing an objective
function, for instance, by maximizing the data likelihood based on an assumed
underlying model. A variety of methods have been proposed in the literature to
address this inference problem. As far as we know, none of the objective
functions in existing methods is convex. In machine learning and applied
statistics, a convex function such as the objective function of support vector
machines (SVMs) is generally preferred, since it can leverage the
high-performance algorithms and rigorous guarantees established in the
extensive literature on convex optimization. One may thus wonder if there
exists a meaningful convex objective function for the inference problem in
human computation. In this paper, we investigate this convexity issue for human
computation. We take an axiomatic approach by formulating a set of axioms that
impose two mild and natural assumptions on the objective function for the
inference. Under these axioms, we show that it is unfortunately impossible to
ensure convexity of the inference problem. On the other hand, we show that
interestingly, in the absence of a requirement to model &quot;spammers&quot;, one can
construct reasonable objective functions for crowdsourcing that guarantee
convex inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5988</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5988</id><created>2014-11-20</created><authors><author><keyname>Langone</keyname><forenames>Rocco</forenames></author></authors><title>Clustering evolving data using kernel-based methods</title><categories>cs.SI cs.LG stat.ML</categories><comments>PhD thesis, Faculty of Engineering, KU Leuven (Leuven, Belgium), July
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we propose several modelling strategies to tackle evolving
data in different contexts. In the framework of static clustering, we start by
introducing a soft kernel spectral clustering (SKSC) algorithm, which can
better deal with overlapping clusters with respect to kernel spectral
clustering (KSC) and provides more interpretable outcomes. Afterwards, a whole
strategy based upon KSC for community detection of static networks is proposed,
where the extraction of a high quality training sub-graph, the choice of the
kernel function, the model selection and the applicability to large-scale data
are key aspects. This paves the way for the development of a novel clustering
algorithm for the analysis of evolving networks called kernel spectral
clustering with memory effect (MKSC), where the temporal smoothness between
clustering results in successive time steps is incorporated at the level of the
primal optimization problem, by properly modifying the KSC formulation. Later
on, an application of KSC to fault detection of an industrial machine is
presented. Here, a smart pre-processing of the data by means of a proper
windowing operation is necessary to catch the ongoing degradation process
affecting the machine. In this way, in a genuinely unsupervised manner, it is
possible to raise an early warning when necessary, in an online fashion.
Finally, we propose a new algorithm called incremental kernel spectral
clustering (IKSC) for online learning of non-stationary data. This ambitious
challenge is faced by taking advantage of the out-of-sample property of kernel
spectral clustering (KSC) to adapt the initial model, in order to tackle
merging, splitting or drifting of clusters across time. Real-world applications
considered in this thesis include image segmentation, time-series clustering,
community detection of static and evolving networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5993</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5993</id><created>2014-11-21</created><authors><author><keyname>Tekumalla</keyname><forenames>Lavanya Sita</forenames></author><author><keyname>Cohen</keyname><forenames>Elaine</forenames></author></authors><title>Reverse Engineering Point Clouds to Fit Tensor Product B-Spline Surfaces
  by Blending Local Fits</title><categories>cs.GR</categories><comments>Masters Thesis, Lavanya Sita Tekumalla, School of Computing,
  University of Utah</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to reverse engineer from point cloud data to obtain 3D models is
important in modeling. As our main contribution, we present a new method to
obtain a tensor product B-spline representation from point cloud data by
fitting surfaces to appropriately segmented data. By blending multiple local
fits our method is more efficient than existing techniques, with the ability to
deal with more detail by efficiently introducing a high number of knots.
Further point cloud data obtained by digitizing 3D data, typically presents
many associated complications like noise and missing data. As our second
contribution, we propose an end-to-end framework for smoothing, hole filling,
parameterization, knot selection and B-spline fitting that addresses these
issues, works robustly with large irregularly shaped data containing holes and
is straightforward to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5995</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5995</id><created>2014-11-20</created><authors><author><keyname>Ovchinnikov</keyname><forenames>G. V.</forenames></author><author><keyname>Kolesnikov</keyname><forenames>D. A.</forenames></author><author><keyname>Oseledets</keyname><forenames>I. V.</forenames></author></authors><title>Algebraic reputation model RepRank and its application to spambot
  detection</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to popularity surge social networks became lucrative targets for spammers
and guerilla marketers, who are trying to game ranking systems and broadcast
their messages at little to none cost. Ranking systems, for example Twitter's
Trends, can be gamed by scripted users also called bots, who are automatically
or semi-automatically twitting essentially the same message. Judging by the
prices and abundance of supply from PR firms this is an easy to implement and
widely used tactic, at least in Russian blogosphere. Aggregative analysis of
social networks should at best mark those messages as spam or at least
correctly downplay their importance as they represent opinions only of a few,
if dedicated, users. Hence bot detection plays a crucial role in social network
mining and analysis. In this paper we propose technique called RepRank which
could be viewed as Markov chain based model for reputation propagation on
graphs utilizing simultaneous trust and anti-trust propagation and provide
effective numerical approach for its computation. Comparison with another
models such as TrustRank and some of its modifications on sample of 320000
Russian speaking Twitter users is presented. The dataset is presented as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6021</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6021</id><created>2014-11-21</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author></authors><title>Joint Beamforming Optimization and Power Control for Full-Duplex MIMO
  Two-way Relay Channel</title><categories>cs.IT math.IT</categories><comments>11 pages, 11 figures, accepted in The IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2376885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the use of full-duplex radio to improve the spectrum
efficiency in a two-way relay channel where two sources exchange information
through an multi-antenna relay, and all nodes work in the full-duplex mode. The
full-duplex operation can reduce the overall communication to only one phase
but suffers from the self-interference. Instead of purely suppressing the
self-interference, we aim to maximize the end-to-end performance by jointly
optimizing the beamforming matrix at the relay which uses the
amplify-and-forward protocol as well as the power control at the sources. To be
specific, we propose iterative algorithms and 1-D search to solve two problems:
finding the achievable rate region and maximizing the sum rate. At each
iteration, either the analytical solution or convex formulation is obtained. We
compare the proposed full-duplex two-way relaying with the conventional
half-duplex two-way relaying, a full-duplex one-way relaying and a performance
upper bound. Numerical results show that the proposed full-duplex scheme
significantly improves the achievable data rates over the conventional scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6027</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6027</id><created>2014-11-10</created><authors><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Concurrent Timed Port Automata</title><categories>cs.FL</categories><comments>34 pages, 3 figures, Technical Report TUM-I9533, TU Munich, 1995</comments><report-no>TUM-I9533</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new and powerful class of automata which are explicitly
concurrent and allow a very simple definition of composition. The novelty of
these automata is their time-synchronous message-asynchronous communication
mechanism. Time synchrony is obtained by using global clock. Message asynchrony
is obtained by requiring the automata to react to every input. Explicit
concurrency is obtained by marking each transition with a set of input and
output messages. We compare these automata with a history based approach which
uses the same communication mechanism and show that they are equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6031</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6031</id><created>2014-11-21</created><authors><author><keyname>Gkioxari</keyname><forenames>Georgia</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Finding Action Tubes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of action detection in videos. Driven by the latest
progress in object detection from 2D images, we build action models using rich
feature hierarchies derived from shape and kinematic cues. We incorporate
appearance and motion in two ways. First, starting from image region proposals
we select those that are motion salient and thus are more likely to contain the
action. This leads to a significant reduction in the number of regions being
processed and allows for faster computations. Second, we extract
spatio-temporal feature representations to build strong classifiers using
Convolutional Neural Networks. We link our predictions to produce detections
consistent in time, which we call action tubes. We show that our approach
outperforms other techniques in the task of action detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6049</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6049</id><created>2014-11-21</created><authors><author><keyname>Alagic</keyname><forenames>Gorjan</forenames></author><author><keyname>Lo</keyname><forenames>Catharine</forenames></author></authors><title>3-manifold diagrams and NP vs $\#$P</title><categories>cs.CC math.QA quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computational complexity, a significant amount of useful theory is built
on a foundation of widely-believed conjectures about the separation of
complexity classes, the most famous of which is P $\neq$ NP. In this work, we
examine the consequences of one such conjecture on the combinatorics of
3-manifold diagrams. We use basic tools from quantum computation to give a
simple (and unconditional) proof that the Witten-Reshetikhin-Turaev invariant
of 3-manifolds is $\#$P-hard to calculate. We then use this fact to show that,
if NP $\neq \#$P, then there exist infinitely many 3-manifold diagrams which
cannot be made logarithmically &quot;thin&quot; (relative to their overall size) except
perhaps by an exponentially large number of local moves. The latter theorem is
an analogue of a result of Freedman for the Jones Polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6057</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6057</id><created>2014-11-21</created><authors><author><keyname>Esmailian</keyname><forenames>Pouya</forenames></author><author><keyname>Abtahi</keyname><forenames>Seyed Ebrahim</forenames></author><author><keyname>Jalili</keyname><forenames>Mahdi</forenames></author></authors><title>Mesoscopic analysis of online social networks - The role of negative
  ties</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>13 pages, 8 figures</comments><journal-ref>Physical Review E 90, no. 4 (2014): 042817</journal-ref><doi>10.1103/PhysRevE.90.042817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of networks are those with both positive and negative links.In this
manuscript, we studied the interplay between positive and negative ties on
mesoscopic level of these networks, i.e., their community structure.A community
is considered as a tightly interconnected group of actors; therefore, it does
not borrow any assumption from balance theory and merely uses the well-known
assumption in the community detection literature.We found that if one detects
the communities based on only positive relations (by ignoring the negative
ones), the majority of negative relations are already placed between the
communities.In other words, negative ties do not have a major role in community
formation of signed networks.Moreover, regarding the internal negative ties, we
proved that most unbalanced communities are maximally balanced, and hence they
cannot be partitioned into k nonempty sub-clusters with higher balancedness (k
&gt;= 2).Furthermore, we showed that although the mediator triad + + -
(hostile-mediator-hostile) is underrepresented, it constitutes a considerable
portion of triadic relations among communities.Hence, mediator triads should
not be ignored by community detection and clustering algorithms.As a result, if
one uses a clustering algorithm that operates merely based on social balance,
mesoscopic structure of signed networks significantly remains hidden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6061</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6061</id><created>2014-11-21</created><updated>2015-01-18</updated><authors><author><keyname>Gupta</keyname><forenames>Sidharth</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Structural Properties of Ego Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Accepted by SBP 2015, to appear in the proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of real-world social networks in large part determines the
evolution of social phenomena, including opinion formation, diffusion of
information and influence, and the spread of disease. Globally, network
structure is characterized by features such as degree distribution, degree
assortativity, and clustering coefficient. However, information about global
structure is usually not available to each vertex. Instead, each vertex's
knowledge is generally limited to the locally observable portion of the network
consisting of the subgraph over its immediate neighbors. Such subgraphs, known
as ego networks, have properties that can differ substantially from those of
the global network. In this paper, we study the structural properties of ego
networks and show how they relate to the global properties of networks from
which they are derived. Through empirical comparisons and mathematical
derivations, we show that structural features, similar to static attributes,
suffer from paradoxes. We quantify the differences between global information
about network structure and local estimates. This knowledge allows us to better
identify and correct the biases arising from incomplete local information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6067</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6067</id><created>2014-11-21</created><updated>2015-04-26</updated><authors><author><keyname>Tulsiani</keyname><forenames>Shubham</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Viewpoints and Keypoints</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the problem of pose estimation for rigid objects in terms of
determining viewpoint to explain coarse pose and keypoint prediction to capture
the finer details. We address both these tasks in two different settings - the
constrained setting with known bounding boxes and the more challenging
detection setting where the aim is to simultaneously detect and correctly
estimate pose of objects. We present Convolutional Neural Network based
architectures for these and demonstrate that leveraging viewpoint estimates can
substantially improve local appearance based keypoint predictions. In addition
to achieving significant improvements over state-of-the-art in the above tasks,
we analyze the error modes and effect of object characteristics on performance
to guide future efforts towards this goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6069</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6069</id><created>2014-11-21</created><updated>2015-05-06</updated><authors><author><keyname>Kar</keyname><forenames>Abhishek</forenames></author><author><keyname>Tulsiani</keyname><forenames>Shubham</forenames></author><author><keyname>Carreira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Category-Specific Object Reconstruction from a Single Image</title><categories>cs.CV</categories><comments>First two authors contributed equally. To appear at CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object reconstruction from a single image -- in the wild -- is a problem
where we can make progress and get meaningful results today. This is the main
message of this paper, which introduces an automated pipeline with pixels as
inputs and 3D surfaces of various rigid categories as outputs in images of
realistic scenes. At the core of our approach are deformable 3D models that can
be learned from 2D annotations available in existing object detection datasets,
that can be driven by noisy automatic object segmentations and which we
complement with a bottom-up module for recovering high-frequency shape details.
We perform a comprehensive quantitative analysis and ablation study of our
approach using the recently introduced PASCAL 3D+ dataset and show very
encouraging automatic reconstructions on PASCAL VOC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6079</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6079</id><created>2014-11-21</created><authors><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Lin</keyname><forenames>Qiuzhen</forenames></author></authors><title>Joint Quantization and Diffusion for Compressed Sensing Measurements of
  Natural Images</title><categories>cs.CR</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research advances have revealed the computational secrecy of the
compressed sensing (CS) paradigm. Perfect secrecy can also be achieved by
normalizing the CS measurement vector. However, these findings are established
on real measurements while digital devices can only store measurements at a
finite precision. Based on the distribution of measurements of natural images
sensed by structurally random ensemble, a joint quantization and diffusion
approach is proposed for these real-valued measurements. In this way, a
nonlinear cryptographic diffusion is intrinsically imposed on the CS process
and the overall security level is thus enhanced. Security analyses show that
the proposed scheme is able to resist known-plaintext attack while the original
CS scheme without quantization cannot. Experimental results demonstrate that
the reconstruction quality of our scheme is comparable to that of the original
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6081</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6081</id><created>2014-11-21</created><authors><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Natarajan</keyname><forenames>Nagarajan</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>PU Learning for Matrix Completion</title><categories>cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the matrix completion problem when the
observations are one-bit measurements of some underlying matrix M, and in
particular the observed samples consist only of ones and no zeros. This problem
is motivated by modern applications such as recommender systems and social
networks where only &quot;likes&quot; or &quot;friendships&quot; are observed. The problem of
learning from only positive and unlabeled examples, called PU
(positive-unlabeled) learning, has been studied in the context of binary
classification. We consider the PU matrix completion problem, where an
underlying real-valued matrix M is first quantized to generate one-bit
observations and then a subset of positive entries is revealed. Under the
assumption that M has bounded nuclear norm, we provide recovery guarantees for
two different observation models: 1) M parameterizes a distribution that
generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For
the first case, we propose a &quot;shifted matrix completion&quot; method that recovers M
using only a subset of indices corresponding to ones, while for the second
case, we propose a &quot;biased matrix completion&quot; method that recovers the
(thresholded) binary matrix. Both methods yield strong error bounds --- if M is
n by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes
the fraction of ones observed. This implies a sample complexity of O(n\log n)
ones to achieve a small error, when M is dense and n is large. We extend our
methods and guarantees to the inductive matrix completion problem, where rows
and columns of M have associated features. We provide efficient and scalable
optimization procedures for both the methods and demonstrate the effectiveness
of the proposed methods for link prediction (on real-world networks consisting
of over 2 million nodes and 90 million links) and semi-supervised clustering
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6082</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6082</id><created>2014-11-21</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Lu</keyname><forenames>Xi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>A new structure entropy of complex networks based on Tsallis
  nonextensive statistical mechanics</title><categories>cs.SI physics.soc-ph</categories><comments>21 pages,10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure entropy is one of the most important parameters to describe the
structure property of the complex networks. Most of the existing struc- ture
entropies are based on the degree or the betweenness centrality. In order to
describe the structure property of the complex networks more reasonably, a new
structure entropy of the complex networks based on the Tsallis nonextensive
statistical mechanics is proposed in this paper. The influence of the degree
and the betweenness centrality on the structure property is combined in the
proposed structure entropy. Compared with the existing structure entropy, the
proposed structure entropy is more reasonable to describe the structure
property of the complex networks in some situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6087</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6087</id><created>2014-11-22</created><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Wireless Communication System with RF-based Energy Harvesting: From
  Information Theory to Green System</title><categories>cs.IT math.IT</categories><comments>12 pages, 12 figures, a short version will appear in GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harvesting energy from ambient environment is a new promising solution to
free electronic devices from electric wire or limited-lifetime battery, which
may find very significant applications in sensor networks and body-area
networks. This paper mainly investigate the fundamental limits of information
transmission in wireless communication system with RF-based energy harvesting,
in which a master node acts not only as an information source but also an
energy source for child node while only information is transmitted back from
child to master node. Three typical structures: optimum receiver, orthogonal
receiver and power splitting receiver are considered where two way information
transmission between two nodes under an unique external power supply constraint
at master node are jointly investigated in the viewpoint of systemic level. We
explicitly characterize the achievable capacity-rate region and also discuss
the effect of signal processing power consumption at child node. The optimal
transmission strategy corresponding to the most energy-efficient status, namely
the point on the boundary of achievable capacity-rate region, is derived with
help of conditional capacity function. Simulation confirms the substantial
gains of employing optimal transmission strategy and optimum receiver
structure. Besides, a typical application on minimizing required transmit power
to green system is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6091</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6091</id><created>2014-11-22</created><authors><author><keyname>Carreira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Kar</keyname><forenames>Abhishek</forenames></author><author><keyname>Tulsiani</keyname><forenames>Shubham</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Virtual View Networks for Object Reconstruction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All that structure from motion algorithms &quot;see&quot; are sets of 2D points. We
show that these impoverished views of the world can be faked for the purpose of
reconstructing objects in challenging settings, such as from a single image, or
from a few ones far apart, by recognizing the object and getting help from a
collection of images of other objects from the same class. We synthesize
virtual views by computing geodesics on novel networks connecting objects with
similar viewpoints, and introduce techniques to increase the specificity and
robustness of factorization-based object reconstruction in this setting. We
report accurate object shape reconstruction from a single image on challenging
PASCAL VOC data, which suggests that the current domain of applications of
rigid structure-from-motion techniques may be significantly extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6114</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6114</id><created>2014-11-22</created><authors><author><keyname>Nanduri</keyname><forenames>Radheshyam</forenames></author><author><keyname>Kakadia</keyname><forenames>Dharmesh</forenames></author><author><keyname>Varma</keyname><forenames>Vasudeva</forenames></author></authors><title>Energy and SLA aware VM Scheduling</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of Cloud Computing over the past few years, there has
been a massive shift from traditional data centers to cloud enabled data
centers. The enterprises with cloud data centers are focusing their attention
on energy savings through effective utilization of resources. In this work, we
propose algorithms which try to minimize the energy consumption in the data
center duly maintaining the SLA guarantees. The algorithms try to utilize least
number of physical machines in the data center by dynamically rebalancing the
physical machines based on their resource utilization. The algorithms also
perform an optimal consolidation of virtual machines on a physical machine,
minimizing SLA violations. In extensive simulation, our algorithms achieve
savings of about 21% in terms of energy consumption and in terms of maintaining
the SLAs, it performs 60% better than Single Threshold algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6118</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6118</id><created>2014-11-22</created><updated>2016-02-16</updated><authors><author><keyname>Acharya</keyname><forenames>Mithun P.</forenames></author><author><keyname>Parnin</keyname><forenames>Chris</forenames></author><author><keyname>Kraft</keyname><forenames>Nicholas A.</forenames></author><author><keyname>Dagnino</keyname><forenames>Aldo</forenames></author><author><keyname>Qu</keyname><forenames>Xiao</forenames></author></authors><title>Code Drones</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose and explore a new paradigm called Code Drones in which every
software artifact such as a class is an intelligent and socially active entity.
In this paradigm, humanized artifacts take the lead and choreograph (socially,
in collaboration with other intelligent software artifacts and humans)
automated software engineering solutions to a myriad of development and
maintenance challenges, including API migration, reuse, documentation, testing,
patching, and refactoring. We discuss the implications of having social and
intelligent/cognitive software artifacts that guide their own self-improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6127</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6127</id><created>2014-11-22</created><authors><author><keyname>Chang</keyname><forenames>Lubin</forenames></author></authors><title>Particle Filtering for Attitude Estimation Using a Minimal Local-Error
  Representation: A Revisit</title><categories>cs.RO</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we have revisited the previously published paper &quot;Particle
Filtering for Attitude Estimation Using a Minimal Local-Error Representation&quot;.
In the revisit, we point out that the quaternion particle filtering based on
the local/global representation structure has not made full use of the
advantage of the particle filtering in terms of accuracy and robustness.
Moreover, a normalized quaternion determining procedure based on the minimum
mean-square error approach has been investigated into the quaternion-based
particle filtering to obtain the fiducial quaternion for the transformation
between quaternion and modified Rodrigues parameter. The modification
investigated in this note is expected to make the quaternion particle filtering
based on the local/global representation structure more strict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6130</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6130</id><created>2014-11-22</created><authors><author><keyname>Darvishi</keyname><forenames>Atena</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author></authors><title>Threshold-based monitoring of cascading outages with PMU measurements of
  area angle</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When power grids are heavily stressed with a bulk power transfer, it is
useful to have a fast indication of the increased stress when multiple line
outages occur. Reducing the bulk power transfer when the outages are severe
could forestall further cascading of the outages. We show that synchrophasor
measurements of voltage angles at all the area tie lines can be used to
indicate the severity of multiple outages. These synchrophasor measurements are
readily combined into an &quot;area angle&quot; that can quickly track the severity of
multiple outages after they occur. We present a procedure to define thresholds
for the area angle that relate to the maximum power that can be transferred
through the area until a line limit is reached. Then in real time we would
monitor the area angle and compare it to the thresholds when line outages occur
to determine the urgency (or not) of actions to reduce the bulk transfer of
power through the area. The procedure also identifies exceptional cases in
which separate actions to resolve local power distribution problems are needed.
We illustrate the thresholds and monitoring with the area angle across several
states of Northwestern USA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6135</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6135</id><created>2014-11-22</created><authors><author><keyname>Delaplace</keyname><forenames>Franck</forenames></author></authors><title>Analogous Dynamics of Boolean Network</title><categories>cs.DM</categories><comments>28 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different Boolean networks may reveal similar dynamics although their
definition differs, then preventing their distinction from the observations.
This raises the question about the sufficiency of a particular Boolean network
for properly reproducing a modeled phenomenon to make realistic predictions.
The question actually depends on the invariant properties of behaviorally
similar Boolean networks. In this article, we address this issue by considering
that the similarity is formalized by isomorphism on graphs modeling their
dynamics. The similarity also depends on the parameter governing the updating
policy, called the mode. We define a general characterization of the group of
isomorphism preserving the mode. From this characterization, we deduce
invariant structural properties of the interaction graph and conditions to
maintain an equivalence through mode variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6137</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6137</id><created>2014-11-22</created><updated>2015-03-06</updated><authors><author><keyname>Zhang</keyname><forenames>Kaiqing</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author></authors><title>Enhanced Multi-Parameter Cognitive Architecture for Future Wireless
  Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 6 figures, IEEE Communications Magazine</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The very original concept of cognitive radio (CR) raised by Mitola targets at
all the environment parameters, including those in physical layer, MAC layer,
application layer as well as the information extracted from reasoning. Hence
the first CR is also referred to as &quot;full cognitive radio&quot;. However, due to its
difficult implementation, FCC and Simon Haykin separately proposed a much more
simplified definition, in which CR mainly detects one single parameter, i.e.,
spectrum occupancy, and is also called as &quot;spectrum sensing cognitive radio&quot;.
With the rapid development of wireless communication, the infrastructure of a
wireless system becomes much more complicated while the functionality at every
node is desired to be as intelligent as possible, say the self-organized
capability in the approaching 5G cellular networks. It is then interesting to
re-look into Mitola's definition and think whether one could, besides obtaining
the &quot;on/off&quot; status of the licensed user only, achieve more parameters in a
cognitive way. In this article, we propose a new cognitive architecture
targeting at multiple parameters in future cellular networks, which is a one
step further towards the &quot;full cognition&quot; compared to the most existing CR
research. The new architecture is elaborated in detailed stages, and three
representative examples are provided based on the recent research progress to
illustrate the feasibility as well as the validity of the proposed
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6147</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6147</id><created>2014-11-22</created><authors><author><keyname>Siyari</keyname><forenames>Peyman</forenames></author><author><keyname>Aghaeinia</keyname><forenames>Hassan</forenames></author></authors><title>Distributed Power Control in Multiuser MIMO Networks with Optimal Linear
  Precoding</title><categories>cs.IT cs.GT math.IT</categories><comments>6 pages, 3 figures, Presented in 7th International Symposium on
  Telecommunications (IST 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contractive interference functions introduced by Feyzmahdavian et al. is the
newest approach in the analysis and design of distributed power control laws.
This approach can be extended to several cases of distributed power control.
One of the distributed power control scenarios wherein the contractive
interference functions have not been employed is the power control in MIMO
systems. In this paper, this scenario will be analyzed. In addition, the
optimal linear precoder is employed in each user to achieve maximum
point-to-point information rate. In our approach, we use the same amount of
signaling as the previous methods did. However, we show that the uniqueness of
Nash equilibria is more probable in our approach, suggesting that our proposed
method improves the convergence performance of distributed power control in
MIMO systems. We also show that the proposed power control algorithm can be
implemented asynchronously, which gives a noticeable flexibility to our
algorithm given the practical communication limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6148</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6148</id><created>2014-11-22</created><authors><author><keyname>Leung</keyname><forenames>Samantha</forenames></author><author><keyname>Lui</keyname><forenames>Edward</forenames></author></authors><title>Bayesian Mechanism Design with Efficiency, Privacy, and Approximate
  Truthfulness</title><categories>cs.GT</categories><comments>A preliminary version of this paper appeared in the 8th Workshop on
  Internet &amp; Network Economics (WINE 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a number of papers relating mechanism design and
privacy (e.g., see \cite{MT07,Xia11,CCKMV11,NST12,NOS12,HK12}). All of these
papers consider a worst-case setting where there is no probabilistic
information about the players' types. In this paper, we investigate mechanism
design and privacy in the \emph{Bayesian} setting, where the players' types are
drawn from some common distribution. We adapt the notion of \emph{differential
privacy} to the Bayesian mechanism design setting, obtaining \emph{Bayesian
differential privacy}. We also define a robust notion of approximate
truthfulness for Bayesian mechanisms, which we call \emph{persistent
approximate truthfulness}. We give several classes of mechanisms (e.g., social
welfare mechanisms and histogram mechanisms) that achieve both Bayesian
differential privacy and persistent approximate truthfulness. These classes of
mechanisms can achieve optimal (economic) efficiency, and do not use any
payments. We also demonstrate that by considering the above mechanisms in a
modified mechanism design model, the above mechanisms can achieve actual
truthfulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6149</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6149</id><created>2014-11-22</created><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author><author><keyname>Zeitouni</keyname><forenames>Ofer</forenames></author></authors><title>On the limitation of spectral methods: From the Gaussian hidden clique
  problem to rank one perturbations of Gaussian tensors</title><categories>math.ST cs.DM cs.IT math.IT stat.TH</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following detection problem: given a realization of a
symmetric matrix ${\mathbf{X}}$ of dimension $n$, distinguish between the
hypothesis that all upper triangular variables are i.i.d. Gaussians variables
with mean 0 and variance $1$ and the hypothesis where ${\mathbf{X}}$ is the sum
of such matrix and an independent rank-one perturbation.
  This setup applies to the situation where under the alternative, there is a
planted principal submatrix ${\mathbf{B}}$ of size $L$ for which all upper
triangular variables are i.i.d. Gaussians with mean $1$ and variance $1$,
whereas all other upper triangular elements of ${\mathbf{X}}$ not in
${\mathbf{B}}$ are i.i.d. Gaussians variables with mean 0 and variance $1$. We
refer to this as the `Gaussian hidden clique problem.'
  When $L=(1+\epsilon)\sqrt{n}$ ($\epsilon&gt;0$), it is possible to solve this
detection problem with probability $1-o_n(1)$ by computing the spectrum of
${\mathbf{X}}$ and considering the largest eigenvalue of ${\mathbf{X}}$. We
prove that this condition is tight in the following sense: when
$L&lt;(1-\epsilon)\sqrt{n}$ no algorithm that examines only the eigenvalues of
${\mathbf{X}}$ can detect the existence of a hidden Gaussian clique, with error
probability vanishing as $n\to\infty$.
  We prove this result as an immediate consequence of a more general result on
rank-one perturbations of $k$-dimensional Gaussian tensors. In this context we
establish a lower bound on the critical signal-to-noise ratio below which a
rank-one signal cannot be detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6156</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6156</id><created>2014-11-22</created><updated>2014-11-30</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author></authors><title>Efficiently learning Ising models on arbitrary graphs</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing the graph underlying an Ising model
from i.i.d. samples. Over the last fifteen years this problem has been of
significant interest in the statistics, machine learning, and statistical
physics communities, and much of the effort has been directed towards finding
algorithms with low computational cost for various restricted classes of
models. Nevertheless, for learning Ising models on general graphs with $p$
nodes of degree at most $d$, it is not known whether or not it is possible to
improve upon the $p^{d}$ computation needed to exhaustively search over all
possible neighborhoods for each node.
  In this paper we show that a simple greedy procedure allows to learn the
structure of an Ising model on an arbitrary bounded-degree graph in time on the
order of $p^2$. We make no assumptions on the parameters except what is
necessary for identifiability of the model, and in particular the results hold
at low-temperatures as well as for highly non-uniform models. The proof rests
on a new structural property of Ising models: we show that for any node there
exists at least one neighbor with which it has a high mutual information. This
structural property may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6160</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6160</id><created>2014-11-22</created><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Copenhaver</keyname><forenames>Martin S.</forenames></author></authors><title>Characterization of the equivalence of robustification and
  regularization in linear, median, and matrix regression</title><categories>math.ST cs.LG math.OC stat.ML stat.TH</categories><msc-class>62J, 90C25, 49M29, 90C11, 15A83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparsity is a key driver in modern statistical problems, from linear
regression via the Lasso to matrix regression with nuclear norm penalties in
matrix completion and beyond. In stark contrast to sparsity motivations for
such problems, it is known in the field of robust optimization that a variety
of vector regression problems, such as Lasso which appears as a loss function
plus a regularization penalty, can arise by simply immunizing a nominal problem
(with only a loss function) to uncertainty in the data. Such a robustification
offers an explanation for why some linear regression methods perform well in
the face of noise, even when these methods do not produce reliably sparse
solutions. In this paper we deepen and extend the understanding of the
connection between robustification and regularization in regression problems.
Specifically, (a) in the context of linear regression, we characterize under
which conditions on the model of uncertainty used and on the loss function
penalties robustification and regularization are equivalent; (b) we show how to
tractably robustify median regression problems; and (c) we extend the
characterization of robustification and regularization to matrix regression
problems (matrix completion and Principal Component Analysis).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6172</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6172</id><created>2014-11-22</created><authors><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Li</keyname><forenames>Chih-ping</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Throughput-Optimal Broadcast on Directed Acyclic Graphs</title><categories>cs.NI</categories><comments>To appear in the proceedings of INFOCOM, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of broadcasting packets in wireless networks. At each
time slot, a network controller activates non-interfering links and forwards
packets to all nodes at a common rate; the maximum rate is referred to as the
broadcast capacity of the wireless network. Existing policies achieve the
broadcast capacity by balancing traffic over a set of spanning trees, which are
difficult to maintain in a large and time-varying wireless network. We propose
a new dynamic algorithm that achieves the broadcast capacity when the
underlying network topology is a directed acyclic graph (DAG). This algorithm
utilizes local queue-length information, does not use any global topological
structures such as spanning trees, and uses the idea of in-order packet
delivery to all network nodes. Although the in-order packet delivery constraint
leads to degraded throughput in cyclic graphs, we show that it is throughput
optimal in DAGs and can be exploited to simplify the design and analysis of
optimal algorithms. Our simulation results show that the proposed algorithm has
superior delay performance as compared to tree-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6179</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6179</id><created>2014-11-22</created><authors><author><keyname>Dobra</keyname><forenames>Adrian</forenames></author><author><keyname>Williams</keyname><forenames>Nathalie E.</forenames></author><author><keyname>Eagle</keyname><forenames>Nathan</forenames></author></authors><title>Spatiotemporal Detection of Unusual Human Population Behavior Using
  Mobile Phone Data</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>40 pages, 32 figures</comments><doi>10.1371/journal.pone.0120449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the aim to contribute to humanitarian response to disasters and violent
events, scientists have proposed the development of analytical tools that could
identify emergency events in real-time, using mobile phone data. The assumption
is that dramatic and discrete changes in behavior, measured with mobile phone
data, will indicate extreme events. In this study, we propose an efficient
system for spatiotemporal detection of behavioral anomalies from mobile phone
data and compare sites with behavioral anomalies to an extensive database of
emergency and non-emergency events in Rwanda. Our methodology successfully
captures anomalous behavioral patterns associated with a broad range of events,
from religious and official holidays to earthquakes, floods, violence against
civilians and protests. Our results suggest that human behavioral responses to
extreme events are complex and multi-dimensional, including extreme increases
and decreases in both calling and movement behaviors. We also find significant
temporal and spatial variance in responses to extreme events. Our behavioral
anomaly detection system and extensive discussion of results are a significant
contribution to the long-term project of creating an effective real-time event
detection system with mobile phone data and we discuss the implications of our
findings for future research to this end.
  KEYWORDS: Big data, call detail record, emergency events, human mobility
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6185</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6185</id><created>2014-11-22</created><authors><author><keyname>Barrera-Cruz</keyname><forenames>Fidel</forenames></author><author><keyname>Haxell</keyname><forenames>Penny</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author></authors><title>Morphing Planar Graph Drawings with Unidirectional Moves</title><categories>cs.CG</categories><comments>13 pages, 9 figures</comments><msc-class>68R10</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alamdari et al. showed that given two straight-line planar drawings of a
graph, there is a morph between them that preserves planarity and consists of a
polynomial number of steps where each step is a \emph{linear morph} that moves
each vertex at constant speed along a straight line. An important step in their
proof consists of converting a \emph{pseudo-morph} (in which contractions are
allowed) to a true morph. Here we introduce the notion of \emph{unidirectional
morphing} step, where the vertices move along lines that all have the same
direction. Our main result is to show that any planarity preserving
pseudo-morph consisting of unidirectional steps and contraction of low degree
vertices can be turned into a true morph without increasing the number of
steps. Using this, we strengthen Alamdari et al.'s result to use only
unidirectional morphs, and in the process we simplify the proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6186</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6186</id><created>2014-11-22</created><authors><author><keyname>Barrera-Cruz</keyname><forenames>Fidel</forenames></author><author><keyname>Haxell</keyname><forenames>Penny</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author></authors><title>Morphing Schnyder drawings of planar triangulations</title><categories>cs.CG math.CO</categories><comments>23 pages, 8 figures</comments><msc-class>68R10</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of morphing between two planar drawings of the same
triangulated graph, maintaining straight-line planarity. A paper in SODA 2013
gave a morph that consists of $O(n^2)$ steps where each step is a linear morph
that moves each of the $n$ vertices in a straight line at uniform speed.
However, their method imitates edge contractions so the grid size of the
intermediate drawings is not bounded and the morphs are not good for
visualization purposes. Using Schnyder embeddings, we are able to morph in
$O(n^2)$ linear morphing steps and improve the grid size to $O(n)\times O(n)$
for a significant class of drawings of triangulations, namely the class of
weighted Schnyder drawings. The morphs are visually attractive. Our method
involves implementing the basic &quot;flip&quot; operations of Schnyder woods as linear
morphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6188</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6188</id><created>2014-11-22</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>A Pair-wise Key Distribution Mechanism and Distributed Trust Evaluation
  Model for Secure Data Aggregation in Mobile Sensor Networks</title><categories>cs.NI cs.CR</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a secure data aggregation (SDA) framework for mobile sensor
networks whose topology changes dynamically with time. The SDA framework
(designed to be resilient to both insider and outsider attacks) comprises of a
pair-wise key establishment mechanism run along the edges of a data gathering
tree and a distributed trust evaluation model that is tightly integrated with
the data aggregation process itself. If an aggregator node already shares a
secret key with its child node, the two nodes locally coordinate to refresh and
establish a new pair-wise secret key; otherwise, the aggregator node requests
the sink to send a seed-secret key message that is used as the basis to
establish a new pair-wise secret key. The trust evaluation model uses the
two-sided Grubbs test to identify outlier data in the periodic beacons
collected from the child nodes (neighbor) nodes. Once the estimated trust score
for a neighbor node falls below a threshold, the sensor node locally classifies
its neighbor node as a Compromised or Faulty (CF) node, and discards the data
or aggregated data received from the CF node. This way, the erroneous data
generated by the CF nodes could be filtered at various levels of the data
gathering tree and are prevented from reaching the root node (sink node).
Finally, we assess the effectiveness of our trust evaluation model through a
comprehensive simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6189</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6189</id><created>2014-11-22</created><authors><author><keyname>Yang</keyname><forenames>Mao</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Hu</keyname><forenames>Long</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author><author><keyname>Yan</keyname><forenames>Zhongjiang</forenames></author></authors><title>Cross-Layer Software-Defined 5G Network</title><categories>cs.NI</categories><comments>9 pages, 5 figures, submitted to Mobile Networks &amp; Applications</comments><report-no>MONE-D-14-00253R1</report-no><doi>10.1007/s11036-014-0554-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few decades, the world has witnessed a rapid growth in mobile
communication and reaped great benefits from it. Even though the fourth
generation (4G) mobile communication system is just being deployed worldwide,
proliferating mobile demands call for newer wireless communication technologies
with even better performance. Consequently, the fifth generation (5G) system is
already emerging in the research field. However, simply evolving the current
mobile networks can hardly meet such great expectations, because over the years
the infrastructures have generally become ossified, closed, and vertically
constructed. Aiming to establish a new paradigm for 5G mobile networks, in this
article, we propose a cross-layer software-defined 5G network architecture. By
jointly considering both the network layer and the physical layer together, we
establish the two software-defined programmable components, the control plane
and the cloud computing pool, which enable an effective control of the mobile
network from the global perspective and benefit technological innovations.
Specifically, by the cross-layer design for software-defining, the logically
centralized and programmable control plane abstracts the control functions from
the network layer down to the physical layer, through which we achieve the
fine-grained controlling of mobile network, while the cloud computing pool
provides powerful computing capability to implement the baseband data
processing of multiple heterogeneous networks. We discuss the main challenges
of our architecture, including the fine-grained control strategies, network
virtualization, and programmability. The architecture significantly benefits
the convergence towards heterogeneous networks and it enables much more
controllable, programmable and evolvable mobile networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6191</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6191</id><created>2014-11-22</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Vanchinathan</keyname><forenames>Hastagiri</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim</forenames></author></authors><title>Kickback cuts Backprop's red-tape: Biologically plausible credit
  assignment in neural networks</title><categories>cs.LG cs.NE q-bio.NC</categories><comments>7 pages. To appear, AAAI-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error backpropagation is an extremely effective algorithm for assigning
credit in artificial neural networks. However, weight updates under Backprop
depend on lengthy recursive computations and require separate output and error
messages -- features not shared by biological neurons, that are perhaps
unnecessary. In this paper, we revisit Backprop and the credit assignment
problem. We first decompose Backprop into a collection of interacting learning
algorithms; provide regret bounds on the performance of these sub-algorithms;
and factorize Backprop's error signals. Using these results, we derive a new
credit assignment algorithm for nonparametric regression, Kickback, that is
significantly simpler than Backprop. Finally, we provide a sufficient condition
for Kickback to follow error gradients, and show that Kickback matches
Backprop's performance on real-world regression benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6197</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6197</id><created>2014-11-23</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author><author><keyname>Shen</keyname><forenames>Zhiqi</forenames></author></authors><title>Identifying Talented Software Engineering Students through Data-driven
  Skill Assessment</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For software development companies, one of the most important objectives is
to identify and acquire talented software engineers in order to maintain a
skilled team that can produce competitive products. Traditional approaches for
finding talented young software engineers are mainly through programming
contests of various forms which mostly test participants' programming skills.
However, successful software engineering in practice requires a wider range of
skills from team members including analysis, design, programming, testing,
communication, collaboration, and self-management, etc. In this paper, we
explore potential ways to identify talented software engineering students in a
data-driven manner through an Agile Project Management (APM) platform. Through
our proposed HASE online APM tool, we conducted a study involving 21 Scrum
teams consisting of over 100 undergraduate software engineering students in
multi-week coursework projects in 2014. During this study, students performed
over 10,000 ASD activities logged by HASE. We demonstrate the possibility and
potentials of this new research direction, and discuss its implications for
software engineering education and industry recruitment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6201</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6201</id><created>2014-11-23</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author><author><keyname>Shen</keyname><forenames>Zhiqi</forenames></author></authors><title>An Empirical Analysis of Task Allocation in Scrum-based Agile
  Programming</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile Software Development (ASD) methodology has become widely used in the
industry. Understanding the challenges facing software engineering students is
important to designing effective training methods to equip students with proper
skills required for effectively using the ASD techniques. Existing empirical
research mostly focused on eXtreme Programming (XP) based ASD methodologies.
There is a lack of empirical studies about Scrum-based ASD programming which
has become the most popular agile methodology among industry practitioners. In
this paper, we present empirical findings regarding the aspects of task
allocation decision-making, collaboration, and team morale related to the Scrum
ASD process which have not yet been well studied by existing research. We draw
our findings from a 12 week long course work project in 2014 involving 125
undergraduate software engineering students from a renowned university working
in 21 Scrum teams. Instead of the traditional survey or interview based
methods, which suffer from limitations in scale and level of details, we obtain
fine grained data through logging students' activities in our online agile
project management (APM) platform - HASE. During this study, the platform
logged over 10,000 ASD activities. Deviating from existing preconceptions, our
results suggest negative correlations between collaboration and team
performance as well as team morale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6202</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6202</id><created>2014-11-23</created><authors><author><keyname>Shen</keyname><forenames>Zhiqi</forenames></author><author><keyname>Yu</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author></authors><title>An Evolutionary Approach for Optimizing Hierarchical Multi-Agent System
  Organization</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been widely recognized that the performance of a multi-agent system is
highly affected by its organization. A large scale system may have billions of
possible ways of organization, which makes it impractical to find an optimal
choice of organization using exhaustive search methods. In this paper, we
propose a genetic algorithm aided optimization scheme for designing
hierarchical structures of multi-agent systems. We introduce a novel algorithm,
called the hierarchical genetic algorithm, in which hierarchical crossover with
a repair strategy and mutation of small perturbation are used. The phenotypic
hierarchical structure space is translated to the genome-like array
representation space, which makes the algorithm genetic-operator-literate. A
case study with 10 scenarios of a hierarchical information retrieval model is
provided. Our experiments have shown that competitive baseline structures which
lead to the optimal organization in terms of utility can be found by the
proposed algorithm during the evolutionary search. Compared with the
traditional genetic operators, the newly introduced operators produced better
organizations of higher utility more consistently in a variety of test cases.
The proposed algorithm extends of the search processes of the state-of-the-art
multi-agent organization design methodologies, and is more computationally
efficient in a large search space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6206</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6206</id><created>2014-11-23</created><authors><author><keyname>Zonoobi</keyname><forenames>Dornoosh</forenames></author><author><keyname>Roohi</keyname><forenames>Shahrooz Faghih</forenames></author><author><keyname>Kassim</keyname><forenames>Ashraf A.</forenames></author></authors><title>Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for
  Dynamic 3D MRI reconstruction</title><categories>cs.CV</categories><comments>conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been recently shown that incorporating priori knowledge significantly
improves the performance of basic compressive sensing based approaches. We have
managed to successfully exploit this idea for recovering a matrix as a
summation of a Low-rank and a Sparse component from compressive measurements.
When applied to the problem of construction of 4D Cardiac MR image sequences in
real-time from highly under-sampled $k-$space data, our proposed method
achieves superior reconstruction quality compared to the other state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6224</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6224</id><created>2014-11-23</created><authors><author><keyname>Bhandari</keyname><forenames>Akshita</forenames></author><author><keyname>Gupta</keyname><forenames>Ashutosh</forenames></author><author><keyname>Das</keyname><forenames>Debasis</forenames></author></authors><title>Improvised Apriori Algorithm using frequent pattern tree for real time
  applications in data mining</title><categories>cs.DB</categories><comments>7 pages, 4 figures, 7 tables, published in Elsevier Procedia in
  Computer Science and presented in ICICT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apriori Algorithm is one of the most important algorithm which is used to
extract frequent itemsets from large database and get the association rule for
discovering the knowledge. It basically requires two important things: minimum
support and minimum confidence. First, we check whether the items are greater
than or equal to the minimum support and we find the frequent itemsets
respectively. Secondly, the minimum confidence constraint is used to form
association rules. Based on this algorithm, this paper indicates the limitation
of the original Apriori algorithm of wasting time and space for scanning the
whole database searching on the frequent itemsets, and present an improvement
on Apriori.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6228</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6228</id><created>2014-11-23</created><updated>2015-04-24</updated><authors><author><keyname>Pinheiro</keyname><forenames>Pedro O.</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>From Image-level to Pixel-level Labeling with Convolutional Networks</title><categories>cs.CV</categories><comments>CVPR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in inferring object segmentation by leveraging only object
class information, and by considering only minimal priors on the object
segmentation task. This problem could be viewed as a kind of weakly supervised
segmentation task, and naturally fits the Multiple Instance Learning (MIL)
framework: every training image is known to have (or not) at least one pixel
corresponding to the image class label, and the segmentation task can be
rewritten as inferring the pixels belonging to the class of the object (given
one image, and its object class). We propose a Convolutional Neural
Network-based model, which is constrained during training to put more weight on
pixels which are important for classifying the image. We show that at test
time, the model has learned to discriminate the right pixels well enough, such
that it performs very well on an existing segmentation benchmark, by adding
only few smoothing priors. Our system is trained using a subset of the Imagenet
dataset and the segmentation experiments are performed on the challenging
Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model
beats the state of the art results in weakly supervised object segmentation
task by a large margin. We also compare the performance of our model with state
of the art fully-supervised segmentation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6231</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6231</id><created>2014-11-23</created><updated>2015-06-03</updated><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Wang</keyname><forenames>Sen</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaofang</forenames></author><author><keyname>Zhang</keyname><forenames>Chengqi</forenames></author></authors><title>Compound Rank-k Projections for Bilinear Analysis</title><categories>cs.LG</categories><comments>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (IEEE T-NNLS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world applications, data are represented by matrices or
high-order tensors. Despite the promising performance, the existing
two-dimensional discriminant analysis algorithms employ a single projection
model to exploit the discriminant information for projection, making the model
less flexible. In this paper, we propose a novel Compound Rank-k Projection
(CRP) algorithm for bilinear analysis. CRP deals with matrices directly without
transforming them into vectors, and it therefore preserves the correlations
within the matrix and decreases the computation complexity. Different from the
existing two dimensional discriminant analysis algorithms, objective function
values of CRP increase monotonically.In addition, CRP utilizes multiple rank-k
projection models to enable a larger search space in which the optimal solution
can be found. In this way, the discriminant ability is enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6232</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6232</id><created>2014-11-23</created><updated>2015-01-11</updated><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author></authors><title>Semi-supervised Feature Analysis by Mining Correlations among Multiple
  Tasks</title><categories>cs.LG</categories><comments>11 pages, submitted to TNNLS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel semi-supervised feature selection framework
by mining correlations among multiple tasks and apply it to different
multimedia applications. Instead of independently computing the importance of
features for each task, our algorithm leverages shared knowledge from multiple
related tasks, thus, improving the performance of feature selection. Note that
we build our algorithm on assumption that different tasks share common
structures. The proposed algorithm selects features in a batch mode, by which
the correlations between different features are taken into consideration.
Besides, considering the fact that labeling a large amount of training data in
real world is both time-consuming and tedious, we adopt manifold learning which
exploits both labeled and unlabeled training data for feature space analysis.
Since the objective function is non-smooth and difficult to solve, we propose
an iterative algorithm with fast convergence. Extensive experiments on
different applications demonstrate that our algorithm outperforms other
state-of-the-art feature selection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6233</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6233</id><created>2014-11-23</created><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Huang</keyname><forenames>Heng</forenames></author></authors><title>A Convex Sparse PCA for Feature Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) has been widely applied to dimensionality
reduction and data pre-processing for different applications in engineering,
biology and social science. Classical PCA and its variants seek for linear
projections of the original variables to obtain a low dimensional feature
representation with maximal variance. One limitation is that it is very
difficult to interpret the results of PCA. In addition, the classical PCA is
vulnerable to certain noisy data. In this paper, we propose a convex sparse
principal component analysis (CSPCA) algorithm and apply it to feature
analysis. First we show that PCA can be formulated as a low-rank regression
optimization problem. Based on the discussion, the l 2 , 1 -norm minimization
is incorporated into the objective function to make the regression coefficients
sparse, thereby robust to the outliers. In addition, based on the sparse model
used in CSPCA, an optimal weight is assigned to each of the original feature,
which in turn provides the output with good interpretability. With the output
of our CSPCA, we can effectively analyze the importance of each feature under
the PCA criteria. The objective function is convex, and we propose an iterative
algorithm to optimize it. We apply the CSPCA algorithm to feature selection and
conduct extensive experiments on six different benchmark datasets. Experimental
results demonstrate that the proposed algorithm outperforms state-of-the-art
unsupervised feature selection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6235</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6235</id><created>2014-11-23</created><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Ma</keyname><forenames>Zhigang</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author></authors><title>Balanced k-Means and Min-Cut Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an effective technique in data mining to generate groups that
are the matter of interest. Among various clustering approaches, the family of
k-means algorithms and min-cut algorithms gain most popularity due to their
simplicity and efficacy. The classical k-means algorithm partitions a number of
data points into several subsets by iteratively updating the clustering centers
and the associated data points. By contrast, a weighted undirected graph is
constructed in min-cut algorithms which partition the vertices of the graph
into two sets. However, existing clustering algorithms tend to cluster minority
of data points into a subset, which shall be avoided when the target dataset is
balanced. To achieve more accurate clustering for balanced dataset, we propose
to leverage exclusive lasso on k-means and min-cut to regulate the balance
degree of the clustering results. By optimizing our objective functions that
build atop the exclusive lasso, we can make the clustering result as much
balanced as possible. Extensive experiments on several large-scale datasets
validate the advantage of the proposed algorithms compared to the
state-of-the-art clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6241</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6241</id><created>2014-11-23</created><updated>2015-10-06</updated><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Huang</keyname><forenames>Heng</forenames></author></authors><title>Improved Spectral Clustering via Embedded Label Propagation</title><categories>cs.LG</categories><comments>Withdraw for a wrong formulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is a key research topic in the field of machine learning
and data mining. Most of the existing spectral clustering algorithms are built
upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose
a novel parameter free, distance consistent Locally Linear Embedding. The
proposed distance consistent LLE promises that edges between closer data points
have greater weight.Furthermore, we propose a novel improved spectral
clustering via embedded label propagation. Our algorithm is built upon two
advancements of the state of the art:1) label propagation,which propagates a
node\'s labels to neighboring nodes according to their proximity; and 2)
manifold learning, which has been widely used in its capacity to leverage the
manifold structure of data points. First we perform standard spectral
clustering on original data and assign each cluster to k nearest data points.
Next, we propagate labels through dense, unlabeled data regions. Extensive
experiments with various datasets validate the superiority of the proposed
algorithm compared to current state of the art spectral algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6243</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6243</id><created>2014-11-23</created><updated>2015-01-30</updated><authors><author><keyname>Sun</keyname><forenames>Xu</forenames></author></authors><title>Structure Regularization for Structured Prediction: Theories and
  Experiments</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While there are many studies on weight regularization, the study on structure
regularization is rare. Many existing systems on structured prediction focus on
increasing the level of structural dependencies within the model. However, this
trend could have been misdirected, because our study suggests that complex
structures are actually harmful to generalization ability in structured
prediction. To control structure-based overfitting, we propose a structure
regularization framework via \emph{structure decomposition}, which decomposes
training samples into mini-samples with simpler structures, deriving a model
with better generalization power. We show both theoretically and empirically
that structure regularization can effectively control overfitting risk and lead
to better accuracy. As a by-product, the proposed method can also substantially
accelerate the training speed. The method and the theoretical results can apply
to general graphical models with arbitrary structures. Experiments on
well-known tasks demonstrate that our method can easily beat the benchmark
systems on those highly-competitive tasks, achieving state-of-the-art
accuracies yet with substantially faster training speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6257</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6257</id><created>2014-11-23</created><authors><author><keyname>Ahmadi</keyname><forenames>Jafar</forenames></author><author><keyname>Di Crescenzo</keyname><forenames>Antonio</forenames></author><author><keyname>Longobardi</keyname><forenames>Maria</forenames></author></authors><title>On dynamic mutual information for bivariate lifetimes</title><categories>math.PR cs.IT math.IT</categories><comments>19 pages, 3 figures, to appear in Advances in Applied Probability,
  Vol. 47 No. 4 (December 2015) (C) by The Applied Probability Trust</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dynamic versions of the mutual information of lifetime
distributions, with focus on past lifetimes, residual lifetimes and mixed
lifetimes evaluated at different instants. This allows to study multicomponent
systems, by measuring the dependence in conditional lifetimes of two components
having possibly different ages. We provide some bounds, and investigate the
mutual information of residual lifetimes within the time-transformed
exponential model (under both the assumptions of unbounded and truncated
lifetimes). Moreover, with reference to the order statistics of a random
sample, we evaluate explicitly the mutual information between the minimum and
the maximum, conditional on inspection at different times, and show that it is
distribution-free. Finally, we develop a copula-based approach aiming to
express the dynamic mutual information for past and residual bivariate
lifetimes in an alternative way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6262</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6262</id><created>2014-11-23</created><authors><author><keyname>Chitour</keyname><forenames>Yacine</forenames></author><author><keyname>Harmouche</keyname><forenames>Mohamed</forenames></author><author><keyname>Laghrouche</keyname><forenames>Salah</forenames></author></authors><title>$L_p$-stabilization of integrator chains subject to input saturation
  using Lyapunov-based homogeneous design</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the $n$-th integrator $\dot x=J_nx+\sigma(u)e_n$, where
$x\in\mathbb{R}^n$, $u\in \mathbb{R}$, $J_n$ is the $n$-th Jordan block and
$e_n=(0\ \cdots 0\ 1)^T\in\mathbb{R}^n$. We provide easily implementable state
feedback laws $u=k(x)$ which not only render the closed-loop system globally
asymptotically stable but also are finite-gain $L_p$-stabilizing with
arbitrarily small gain. These $L_p$-stabilizing state feedbacks are built from
homogeneous feedbacks appearing in finite-time stabilization of linear systems.
We also provide additional $L_\infty$-stabilization results for the case of
both internal and external disturbances of the $n$-th integrator, namely for
the perturbed system $\dot x=J_nx+e_n\sigma (k(x)+d)+D$ where $d\in\mathbb{R}$
and $D\in\mathbb{R}^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6272</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6272</id><created>2014-11-23</created><updated>2015-08-04</updated><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Super-Resolution Radar</title><categories>cs.IT math.IT</categories><comments>Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the identification of a time-varying linear system
from its response to a known input signal. More specifically, we consider
systems whose response to the input signal is given by a weighted superposition
of delayed and Doppler shifted versions of the input. This problem arises in a
multitude of applications such as wireless communications and radar imaging.
Due to practical constraints, the input signal has finite bandwidth B, and the
received signal is observed over a finite time interval of length T only. This
gives rise to a delay and Doppler resolution of 1/B and 1/T. We show that this
resolution limit can be overcome, i.e., we can exactly recover the continuous
delay-Doppler pairs and the corresponding attenuation factors, by solving a
convex optimization problem. This result holds provided that the distance
between the delay-Doppler pairs is at least 2.37/B in time or 2.37/T in
frequency. Furthermore, this result allows the total number of delay-Doppler
pairs to be linear up to a log-factor in BT, the dimensionality of the response
of the system, and thereby the limit for identifiability. Stated differently,
we show that we can estimate the time-frequency components of a signal that is
S-sparse in the continuous dictionary of time-frequency shifts of a random
window function, from a number of measurements, that is linear up to a
log-factor in S.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6273</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6273</id><created>2014-11-23</created><authors><author><keyname>P&#xe9;rez-Ros&#xe9;s</keyname><forenames>Hebert</forenames></author><author><keyname>Seb&#xe9;</keyname><forenames>Francesc</forenames></author></authors><title>Synthetic Generation of Social Network Data With Endorsements</title><categories>cs.SI physics.soc-ph</categories><comments>5 figures, 2 algorithms, Journal of Simulation 2015</comments><msc-class>68U20, 65C10, 90C27, 90C59</msc-class><acm-class>I.6.8; I.2.8; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many simulation studies involving networks there is the need to rely on a
sample network to perform the simulation experiments. In many cases, real
network data is not available due to privacy concerns. In that case we can
recourse to synthetic data sets with similar properties to the real data. In
this paper we discuss the problem of generating synthetic data sets for a
certain kind of online social network, for simulation purposes. Some popular
online social networks, such as LinkedIn and ResearchGate, allow user
endorsements for specific skills. For each particular skill, the endorsements
give rise to a directed subgraph of the corresponding network, where the nodes
correspond to network members or users, and the arcs represent endorsement
relations. Modelling these endorsement digraphs can be done by formulating an
optimization problem, which is amenable to different heuristics. Our
construction method consists of two stages: The first one simulates the growth
of the network, and the second one solves the aforementioned optimization
problem to construct the endorsements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6275</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6275</id><created>2014-11-23</created><authors><author><keyname>Casta&#xf1;eda-Garay</keyname><forenames>Miguel</forenames></author><author><keyname>Belmonte-Fern&#xe1;ndez</keyname><forenames>Oscar</forenames></author><author><keyname>P&#xe9;rez-Ros&#xe9;s</keyname><forenames>Hebert</forenames></author><author><keyname>Diaz-Tula</keyname><forenames>Antonio</forenames></author></authors><title>Detection of Non-Stationary Photometric Perturbations on Projection
  Screens</title><categories>cs.CV</categories><comments>20 pages, Journal of Research and Practice in Information Technology,
  vol. 44, num. 4, 2012</comments><acm-class>H.5.2; I.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interfaces based on projection screens have become increasingly more popular
in recent years, mainly due to the large screen size and resolution that they
provide, as well as their stereo-vision capabilities. This work shows a local
method for real-time detection of non-stationary photometric perturbations in
projected images by means of computer vision techniques. The method is based on
the computation of differences between the images in the projector's frame
buffer and the corresponding images on the projection screen observed by the
camera. It is robust under spatial variations in the intensity of light emitted
by the projector on the projection surface and also robust under stationary
photometric perturbations caused by external factors. Moreover, we describe the
experiments carried out to show the reliability of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6276</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6276</id><created>2014-11-23</created><authors><author><keyname>Gupta</keyname><forenames>Naveen</forenames></author><author><keyname>Singh</keyname><forenames>Anurag</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author></authors><title>Community-based Immunization Strategies for Epidemic Control</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 7 figures</comments><doi>10.1109/COMSNETS.2015.7098709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the epidemic dynamics, and finding out efficient techniques to
control it, is a challenging issue. A lot of research has been done on targeted
immunization strategies, exploiting various global network topological
properties. However, in practice, information about the global structure of the
contact network may not be available. Therefore, immunization strategies that
can deal with a limited knowledge of the network structure are required. In
this paper, we propose targeted immunization strategies that require
information only at the community level. Results of our investigations on the
SIR epidemiological model, using a realistic synthetic benchmark with
controlled community structure, show that the community structure plays an
important role in the epidemic dynamics. An extensive comparative evaluation
demonstrates that the proposed strategies are as efficient as the most
influential global centrality based immunization strategies, despite the fact
that they use a limited amount of information. Furthermore, they outperform
alternative local strategies, which are agnostic about the network structure,
and make decisions based on random walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6279</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6279</id><created>2014-11-23</created><authors><author><keyname>Renne</keyname><forenames>Bryan</forenames></author><author><keyname>Sack</keyname><forenames>Joshua</forenames></author><author><keyname>Yap</keyname><forenames>Audrey</forenames></author></authors><title>Logics of Temporal-Epistemic Actions</title><categories>cs.LO cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Dynamic Epistemic Temporal Logic, a framework for reasoning about
operations on multi-agent Kripke models that contain a designated temporal
relation. These operations are natural extensions of the well-known &quot;action
models&quot; from Dynamic Epistemic Logic. Our &quot;temporal action models&quot; may be used
to define a number of informational actions that can modify the &quot;objective&quot;
temporal structure of a model along with the agents' basic and higher-order
knowledge and beliefs about this structure, including their beliefs about the
time. In essence, this approach provides one way to extend the domain of action
model-style operations from atemporal Kripke models to temporal Kripke models
in a manner that allows actions to control the flow of time. We present a
number of examples to illustrate the subtleties involved in interpreting the
effects of our extended action models on temporal Kripke models. We also study
preservation of important epistemic-temporal properties of temporal Kripke
models under temporal action model-induced operations, provide complete
axiomatizations for two theories of temporal action models, and connect our
approach with previous work on time in Dynamic Epistemic Logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6285</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6285</id><created>2014-11-23</created><authors><author><keyname>Afzal</keyname><forenames>Avid M.</forenames></author><author><keyname>Mussa</keyname><forenames>Hamse Y.</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author><author><keyname>Bender</keyname><forenames>Andreas</forenames></author><author><keyname>Glen</keyname><forenames>Robert C.</forenames></author></authors><title>Target Fishing: A Single-Label or Multi-Label Problem?</title><categories>q-bio.BM cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to Cobanoglu et al and Murphy, it is now widely acknowledged that
the single target paradigm (one protein or target, one disease, one drug) that
has been the dominant premise in drug development in the recent past is
untenable. More often than not, a drug-like compound (ligand) can be
promiscuous - that is, it can interact with more than one target protein. In
recent years, in in silico target prediction methods the promiscuity issue has
been approached computationally in different ways. In this study we confine
attention to the so-called ligand-based target prediction machine learning
approaches, commonly referred to as target-fishing. With a few exceptions, the
target-fishing approaches that are currently ubiquitous in cheminformatics
literature can be essentially viewed as single-label multi-classification
schemes; these approaches inherently bank on the single target paradigm
assumption that a ligand can home in on one specific target. In order to
address the ligand promiscuity issue, one might be able to cast target-fishing
as a multi-label multi-class classification problem. For illustrative and
comparison purposes, single-label and multi-label Naive Bayes classification
models (denoted here by SMM and MMM, respectively) for target-fishing were
implemented. The models were constructed and tested on 65,587 compounds and 308
targets retrieved from the ChEMBL17 database. SMM and MMM performed
differently: for 16,344 test compounds, the MMM model returned recall and
precision values of 0.8058 and 0.6622, respectively; the corresponding recall
and precision values yielded by the SMM model were 0.7805 and 0.7596,
respectively. However, at a significance level of 0.05 and one degree of
freedom McNemar test performed on the target prediction results returned by SMM
and MMM for the 16,344 test ligands gave a chi-squared value of 15.656, in
favour of the MMM approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6296</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6296</id><created>2014-11-23</created><updated>2014-11-26</updated><authors><author><keyname>Van Loan</keyname><forenames>Charles</forenames></author><author><keyname>Vokt</keyname><forenames>Joseph</forenames></author></authors><title>Approximating Matrices with Multiple Symmetries</title><categories>cs.NA math.NA</categories><comments>20 pages</comments><msc-class>15A69, 15B57, 15Bxx, 65F25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a tensor with various symmetries is properly unfolded, then the resulting
matrix inherits those symmetries. As tensor computations become increasingly
important it is imperative that we develop efficient structure preserving
methods for matrices with multiple symmetries. In this paper we consider how to
exploit and preserve structure in the pivoted Cholesky factorization when
approximating a matrix $A$ that is both symmetric ($A=A^T$) and what we call
{\em perfect shuffle symmetric}, or {\em perf-symmetric}. The latter property
means that $A = \Pi A\Pi$ where $\Pi$ is a permutation with the property that
$\Pi v = v$ if $v$ is the vec of a symmetric matrix and $\Pi v = -v$ if $v$ is
the vec of a skew-symmetric matrix. Matrices with this structure can arise when
an order-4 tensor $\cal A$ is unfolded and its elements satisfy ${\cal
A}(i_{1},i_{2},i_{3},i_{4}) = {\cal A}(i_{2},i_{1},i_{3},i_{4}) ={\cal
A}(i_{1},i_{2},i_{4},i_{3}) ={\cal A}(i_{3},i_{4},i_{1},i_{2}).$ This is the
case in certain quantum chemistry applications where the tensor entries are
electronic repulsion integrals. Our technique involves a closed-form block
diagonalization followed by one or two half-sized pivoted Cholesky
factorizations. This framework allows for a lazy evaluation feature that is
important if the entries in $\cal A$ are expensive to compute. In addition to
being a structure preserving rank reduction technique, we find that this
approach for obtaining the Cholesky factorization reduces the work by up to a
factor of 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6299</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6299</id><created>2014-11-23</created><updated>2015-03-26</updated><authors><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Almost Optimal Pseudorandom Generators for Spherical Caps</title><categories>cs.CC</categories><comments>28 Pages (including the title page)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Halfspaces or linear threshold functions are widely studied in complexity
theory, learning theory and algorithm design. In this work we study the natural
problem of constructing pseudorandom generators (PRGs) for halfspaces over the
sphere, aka spherical caps, which besides being interesting and basic geometric
objects, also arise frequently in the analysis of various randomized algorithms
(e.g., randomized rounding). We give an explicit PRG which fools spherical caps
within error $\epsilon$ and has an almost optimal seed-length of $O(\log n +
\log(1/\epsilon) \cdot \log\log(1/\epsilon))$. For an inverse-polynomially
growing error $\epsilon$, our generator has a seed-length optimal up to a
factor of $O( \log \log {(n)})$. The most efficient PRG previously known (due
to Kane, 2012) requires a seed-length of $\Omega(\log^{3/2}{(n)})$ in this
setting. We also obtain similar constructions to fool halfspaces with respect
to the Gaussian distribution.
  Our construction and analysis are significantly different from previous works
on PRGs for halfspaces and build on the iterative dimension reduction ideas of
Kane et. al. (2011) and Celis et. al. (2013), the \emph{classical moment
problem} from probability theory and explicit constructions of \emph{orthogonal
designs} based on the seminal work of Bourgain and Gamburd (2011) on expansion
in Lie groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6300</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6300</id><created>2014-11-23</created><authors><author><keyname>Minh</keyname><forenames>Do Le Paul</forenames></author></authors><title>Discrete Bayesian Networks: The Exact Posterior Marginal Distributions</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In a Bayesian network, we wish to evaluate the marginal probability of a
query variable, which may be conditioned on the observed values of some
evidence variables. Here we first present our &quot;border algorithm,&quot; which
converts a BN into a directed chain. For the polytrees, we then present in
details, with some modifications and within the border algorithm framework, the
&quot;revised polytree algorithm&quot; by Peot &amp; Shachter (1991). Finally, we present our
&quot;parentless polytree method,&quot; which, coupled with the border algorithm,
converts any Bayesian network into a polytree, rendering the complexity of our
inferences independent of the size of network, and linear with the number of
its evidence and query variables. All quantities in this paper have
probabilistic interpretations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6305</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6305</id><created>2014-11-23</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Medina</keyname><forenames>Andres Mu&#xf1;oz</forenames></author></authors><title>Revenue Optimization in Posted-Price Auctions with Strategic Buyers</title><categories>cs.LG</categories><comments>At NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study revenue optimization learning algorithms for posted-price auctions
with strategic buyers. We analyze a very broad family of monotone regret
minimization algorithms for this problem, which includes the previously best
known algorithm, and show that no algorithm in that family admits a strategic
regret more favorable than $\Omega(\sqrt{T})$. We then introduce a new
algorithm that achieves a strategic regret differing from the lower bound only
by a factor in $O(\log T)$, an exponential improvement upon the previous best
algorithm. Our new algorithm admits a natural analysis and simpler proofs, and
the ideas behind its design are general. We also report the results of
empirical evaluations comparing our algorithm with the previous state of the
art and show a consistent exponential improvement in several different
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6307</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6307</id><created>2014-11-23</created><authors><author><keyname>Batmanghelich</keyname><forenames>Nematollah Kayhan</forenames></author><author><keyname>Quon</keyname><forenames>Gerald</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Kellis</keyname><forenames>Manolis</forenames></author><author><keyname>Golland</keyname><forenames>Polina</forenames></author><author><keyname>Bornn</keyname><forenames>Luke</forenames></author></authors><title>Diversifying Sparsity Using Variational Determinantal Point Processes</title><categories>cs.LG cs.AI stat.ML</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel diverse feature selection method based on determinantal
point processes (DPPs). Our model enables one to flexibly define diversity
based on the covariance of features (similar to orthogonal matching pursuit) or
alternatively based on side information. We introduce our approach in the
context of Bayesian sparse regression, employing a DPP as a variational
approximation to the true spike and slab posterior distribution. We
subsequently show how this variational DPP approximation generalizes and
extends mean-field approximation, and can be learned efficiently by exploiting
the fast sampling properties of DPPs. Our motivating application comes from
bioinformatics, where we aim to identify a diverse set of genes whose
expression profiles predict a tumor type where the diversity is defined with
respect to a gene-gene interaction network. We also explore an application in
spatial statistics. In both cases, we demonstrate that the proposed method
yields significantly more diverse feature sets than classic sparse methods,
without compromising accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6308</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6308</id><created>2014-11-23</created><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Ma</keyname><forenames>Zhigang</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaofang</forenames></author></authors><title>A Convex Formulation for Spectral Shrunk Clustering</title><categories>cs.LG</categories><comments>AAAI2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is a fundamental technique in the field of data mining
and information processing. Most existing spectral clustering algorithms
integrate dimensionality reduction into the clustering process assisted by
manifold learning in the original space. However, the manifold in
reduced-dimensional subspace is likely to exhibit altered properties in
contrast with the original space. Thus, applying manifold information obtained
from the original space to the clustering process in a low-dimensional subspace
is prone to inferior performance. Aiming to address this issue, we propose a
novel convex algorithm that mines the manifold structure in the low-dimensional
subspace. In addition, our unified learning process makes the manifold learning
particularly tailored for the clustering. Compared with other related methods,
the proposed algorithm results in more structured clustering result. To
validate the efficacy of the proposed algorithm, we perform extensive
experiments on several benchmark datasets in comparison with some
state-of-the-art clustering approaches. The experimental results demonstrate
that the proposed algorithm has quite promising clustering performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6314</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6314</id><created>2014-11-23</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>On the High-dimensional Power of Linear-time Kernel Two-Sample Testing
  under Mean-difference Alternatives</title><categories>math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonparametric two sample testing deals with the question of consistently
deciding if two distributions are different, given samples from both, without
making any parametric assumptions about the form of the distributions. The
current literature is split into two kinds of tests - those which are
consistent without any assumptions about how the distributions may differ
(\textit{general} alternatives), and those which are designed to specifically
test easier alternatives, like a difference in means (\textit{mean-shift}
alternatives).
  The main contribution of this paper is to explicitly characterize the power
of a popular nonparametric two sample test, designed for general alternatives,
under a mean-shift alternative in the high-dimensional setting. Specifically,
we explicitly derive the power of the linear-time Maximum Mean Discrepancy
statistic using the Gaussian kernel, where the dimension and sample size can
both tend to infinity at any rate, and the two distributions differ in their
means. As a corollary, we find that if the signal-to-noise ratio is held
constant, then the test's power goes to one if the number of samples increases
faster than the dimension increases. This is the first explicit power
derivation for a general nonparametric test in the high-dimensional setting,
and also the first analysis of how tests designed for general alternatives
perform when faced with easier ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6317</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6317</id><created>2014-11-23</created><authors><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Lower bounds on the size of semidefinite programming relaxations</title><categories>cs.CC math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method for proving lower bounds on the efficacy of
semidefinite programming (SDP) relaxations for combinatorial problems. In
particular, we show that the cut, TSP, and stable set polytopes on $n$-vertex
graphs are not the linear image of the feasible region of any SDP (i.e., any
spectrahedron) of dimension less than $2^{n^c}$, for some constant $c &gt; 0$.
This result yields the first super-polynomial lower bounds on the semidefinite
extension complexity of any explicit family of polytopes.
  Our results follow from a general technique for proving lower bounds on the
positive semidefinite rank of a matrix. To this end, we establish a close
connection between arbitrary SDPs and those arising from the sum-of-squares SDP
hierarchy. For approximating maximum constraint satisfaction problems, we prove
that SDPs of polynomial-size are equivalent in power to those arising from
degree-$O(1)$ sum-of-squares relaxations. This result implies, for instance,
that no family of polynomial-size SDP relaxations can achieve better than a
7/8-approximation for MAX-3-SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6320</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6320</id><created>2014-11-23</created><authors><author><keyname>Saadoune</keyname><forenames>Meryem</forenames></author><author><keyname>Hajami</keyname><forenames>Abdelmajid</forenames></author><author><keyname>Allali</keyname><forenames>Hakim</forenames></author></authors><title>Distance's Quantification Algorithm in AODV Protocol</title><categories>cs.NI</categories><comments>12 pages, Necom 2014 in Dubai</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility is one of the basic features that define an ad hoc network, an asset
that leaves the field free for the nodes to move. The most important aspect of
this kind of network turns into a great disadvantage when it comes to
commercial applications, take as an example: the automotive networks that allow
communication between a groups of vehicles. The ad hoc on-demand distance
vector (AODV) routing protocol, designed for mobile ad hoc networks, has two
main functions. First, it enables route establishment between a source and a
destination node by initiating a route discovery process. Second, it maintains
the active routes, which means finding alternative routes in a case of a link
failure and deleting routes when they are no longer desired. In a highly mobile
network those are demanding tasks to be performed efficiently and accurately.
In this paper, we focused in the first point to enhance the local decision of
each node in the network by the quantification of the mobility of their
neighbours. Quantification is made around RSSI algorithm a well known distance
estimation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6322</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6322</id><created>2014-11-23</created><updated>2015-10-22</updated><authors><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author><author><keyname>Panageas</keyname><forenames>Ioannis</forenames></author><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Yazdanbod</keyname><forenames>Sadra</forenames></author></authors><title>The Complexity of Genetic Diversity</title><categories>q-bio.PE cs.CC cs.GT math.DS math.SP</categories><comments>24 pages, 2 figues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key question in biological systems is whether genetic diversity persists in
the long run under evolutionary competition or whether a single dominant
genotype emerges. Classic work by Kalmus in 1945 has established that even in
simple diploid species (species with two chromosomes) diversity can be
guaranteed as long as the heterozygote individuals enjoy a selective advantage.
Despite the classic nature of the problem, as we move towards increasingly
polymorphic traits (e.g. human blood types) predicting diversity and
understanding its implications is still not fully understood. Our key
contribution is to establish complexity theoretic hardness results implying
that even in the textbook case of single locus diploid models predicting
whether diversity survives or not given its fitness landscape is
algorithmically intractable. We complement our results by establishing that
under randomly chosen fitness landscapes diversity survives with significant
probability. Our results are structurally robust along several dimensions
(e.g., choice of parameter distribution, different definitions of
stability/persistence, restriction to typical subclasses of fitness
landscapes). Technically, our results exploit connections between game theory,
nonlinear dynamical systems, complexity theory and biology and establish
hardness results for predicting the evolution of a deterministic variant of the
well known multiplicative weights update algorithm in symmetric coordination
games which could be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6326</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6326</id><created>2014-11-23</created><authors><author><keyname>Dey</keyname><forenames>Debadeepta</forenames></author><author><keyname>Shankar</keyname><forenames>Kumar Shaurya</forenames></author><author><keyname>Zeng</keyname><forenames>Sam</forenames></author><author><keyname>Mehta</keyname><forenames>Rupesh</forenames></author><author><keyname>Agcayazi</keyname><forenames>M. Talha</forenames></author><author><keyname>Eriksen</keyname><forenames>Christopher</forenames></author><author><keyname>Daftry</keyname><forenames>Shreyansh</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Vision and Learning for Deliberative Monocular Cluttered Flight</title><categories>cs.RO cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cameras provide a rich source of information while being passive, cheap and
lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work
we present the first implementation of receding horizon control, which is
widely used in ground vehicles, with monocular vision as the only sensing mode
for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a
number of contributions: novel coupling of perception and control via relevant
and diverse, multiple interpretations of the scene around the robot, leveraging
recent advances in machine learning to showcase anytime budgeted cost-sensitive
feature selection, and fast non-linear regression for monocular depth
prediction. We empirically demonstrate the efficacy of our novel pipeline via
real world experiments of more than 2 kms through dense trees with a quadrotor
built from off-the-shelf parts. Moreover our pipeline is designed to combine
information from other modalities like stereo and lidar as well if available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6328</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6328</id><created>2014-11-23</created><authors><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Explicit MDS Codes for Optimal Repair Bandwidth</title><categories>cs.IT math.IT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDS codes are erasure-correcting codes that can correct the maximum number of
erasures for a given number of redundancy or parity symbols. If an MDS code has
$r$ parities and no more than $r$ erasures occur, then by transmitting all the
remaining data in the code, the original information can be recovered. However,
it was shown that in order to recover a single symbol erasure, only a fraction
of $1/r$ of the information needs to be transmitted. This fraction is called
the repair bandwidth (fraction). Explicit code constructions were given in
previous works. If we view each symbol in the code as a vector or a column over
some field, then the code forms a 2D array and such codes are especially widely
used in storage systems. In this paper, we address the following question:
given the length of the column $l$, number of parities $r$, can we construct
high-rate MDS array codes with optimal repair bandwidth of $1/r$, whose code
length is as long as possible? In this paper, we give code constructions such
that the code length is $(r+1)\log_r l$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6335</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6335</id><created>2014-11-23</created><updated>2014-11-30</updated><authors><author><keyname>Peng</keyname><forenames>Peng</forenames></author><author><keyname>Zou</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Dongyan</forenames></author></authors><title>On The Marriage of SPARQL and Keywords</title><categories>cs.DB</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although SPARQL has been the predominant query language over RDF graphs, some
query intentions cannot be well captured by only using SPARQL syntax. On the
other hand, the keyword search enjoys widespread usage because of its intuitive
way of specifying information needs but suffers from the problem of low
precision. To maximize the advantages of both SPARQL and keyword search, we
introduce a novel paradigm that combines both of them and propose a hybrid
query (called an SK query) that integrates SPARQL and keyword search. In order
to answer SK queries efficiently, a structural index is devised, based on a
novel integrated query algorithm is proposed. We evaluate our method in large
real RDF graphs and experiments demonstrate both effectiveness and efficiency
of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6340</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6340</id><created>2014-11-23</created><authors><author><keyname>Ajanthan</keyname><forenames>Thalaiyasingam</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author></authors><title>Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex
  Priors</title><categories>cs.CV</categories><comments>9 pages, 5 figures and 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While widely acknowledged as highly effective in computer vision, multi-label
MRFs with non-convex priors are difficult to optimize. To tackle this, we
introduce an algorithm that iteratively approximates the original energy with
an appropriately weighted surrogate energy that is easier to minimize. Our
algorithm guarantees that the original energy decreases at each iteration. In
particular, we consider the scenario where the global minimizer of the weighted
surrogate energy can be obtained by a multi-label graph cut algorithm, and show
that our algorithm then lets us handle of large variety of non-convex priors.
We demonstrate the benefits of our method over state-of-the-art MRF energy
minimization techniques on stereo and inpainting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6346</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6346</id><created>2014-11-23</created><updated>2014-12-03</updated><authors><author><keyname>Cheng</keyname><forenames>Qi</forenames></author><author><keyname>Gao</keyname><forenames>Shuhong</forenames></author><author><keyname>Rojas</keyname><forenames>J. Maurice</forenames></author><author><keyname>Wan</keyname><forenames>Daqing</forenames></author></authors><title>Sparse Univariate Polynomials with Many Roots Over Finite Fields</title><categories>math.NT cs.SC</categories><comments>9 pages, 1 figure, submitted for presentation at a conference. This
  version cleans up various typos, and adds a remark on how the analogous
  bounds for complex roots of unity (for trinomials) have been settled by
  recent work of Theobald and de Wolff</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $q$ is a prime power and $f\in\mathbf{F}_q[x]$ is a univariate
polynomial with exactly $t$ monomial terms and degree $&lt;q-1$. To establish a
finite field analogue of Descartes' Rule, Bi, Cheng, and Rojas (2013) recently
proved an upper bound of $2(q-1)^{(t-2)/(t-1)}$ on the number of cosets of
$\mathbf{F}^*_q$ needed to cover the roots of $f$ in $\mathbf{F}^*_q$. Here, we
give explicit $f$ with root structure approaching this bound: For $q$ a $t$-th
power we give an explicit $t$-nomial vanishing on $q^{(t-2)/t}+...+q^{1/t}+1$
distinct cosets of $\mathbf{F}^*_q$. Over prime fields $\mathbf{F}_p$,
computational data we provide suggests that it is harder to construct explicit
sparse polynomials with many cosets of roots. Nevertheless, we find trinomials
vanishing on $\Omega(\frac{\log \log p}{\log \log \log p})$ distinct cosets in
$\mathbf{F}^*_p$ and, assuming the Generalized Riemann Hypothesis,
$\Omega(\frac{\log p}{\log \log p})$ distinct cosets in $\mathbf{F}^*_p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6358</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6358</id><created>2014-11-24</created><authors><author><keyname>Wang</keyname><forenames>Junxiong</forenames></author><author><keyname>Wang</keyname><forenames>Hongzhi</forenames></author><author><keyname>Zhao</keyname><forenames>Chenxu</forenames></author></authors><title>A Hybrid Solution to improve Iteration Efficiency in the Distributed
  Learning</title><categories>cs.DC cs.LG</categories><comments>Unfinished job, with completed mathematical proof while experiments
  are in process now. We plan to submit this paper to icml2015</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, many machine learning algorithms contain lots of iterations. When
it comes to existing large-scale distributed systems, some slave nodes may
break down or have lower efficiency. Therefore traditional machine learning
algorithm may fail because of the instability of distributed system.We presents
a hybrid approach which not only own a high fault-tolerant but also achieve a
balance of performance and efficiency.For each iteration, the result of slow
machines will be abandoned. Then, we discuss the relationship between accuracy
and abandon rate. Next we debate the convergence speed of this process.
Finally, our experiments demonstrate our idea can dramatically reduce
calculation time and be used in many platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6359</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6359</id><created>2014-11-24</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Packet-Level Network Compression: Realization and Scaling of the
  Network-Wide Benefits</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of considerable amount of redundancy in the Internet traffic at
the packet level has stimulated the deployment of packet-level redundancy
elimination techniques within the network by enabling network nodes to memorize
data packets. Redundancy elimination results in traffic reduction which in turn
improves the efficiency of network links. In this paper, the concept of network
compression is introduced that aspires to exploit the statistical correlation
beyond removing large duplicate strings from the flow to better suppress
redundancy.
  In the first part of the paper, we introduce &quot;memory-assisted compression&quot;,
which utilizes the memorized content within the network to learn the statistics
of the information source generating the packets which can then be used toward
reducing the length of codewords describing the packets emitted by the source.
Using simulations on data gathered from real network traces, we show that
memory-assisted compression can result in significant traffic reduction.
  In the second part of the paper, we study the scaling of the average
network-wide benefits of memory-assisted compression. We discuss routing and
memory placement problems in network for the reduction of overall traffic. We
derive a closed-form expression for the scaling of the gain in Erdos-Renyi
random network graphs, where obtain a threshold value for the number of
memories deployed in a random graph beyond which network-wide benefits start to
shine. Finally, the network-wide benefits are studied on Internet-like
scale-free networks. We show that non-vanishing network compression gain is
obtained even when only a tiny fraction of the total number of nodes in the
network are memory-enabled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6361</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6361</id><created>2014-11-24</created><authors><author><keyname>Wicht</keyname><forenames>Baptiste</forenames></author><author><keyname>Vitillo</keyname><forenames>Roberto A.</forenames></author><author><keyname>Chen</keyname><forenames>Dehao</forenames></author><author><keyname>Levinthal</keyname><forenames>David</forenames></author></authors><title>Hardware Counted Profile-Guided Optimization</title><categories>cs.PL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profile-Guided Optimization (PGO) is an excellent means to improve the
performance of a compiled program. Indeed, the execution path data it provides
helps the compiler to generate better code and better cacheline packing.
  At the time of this writing, compilers only support instrumentation-based
PGO. This proved effective for optimizing programs. However, few projects use
it, due to its complicated dual-compilation model and its high overhead. Our
solution of sampling Hardware Performance Counters overcome these drawbacks. In
this paper, we propose a PGO solution for GCC by sampling Last Branch Record
(LBR) events and using debug symbols to recreate source locations of binary
instructions.
  By using LBR-Sampling, the generated profiles are very accurate. This
solution achieved an average of 83% of the gains obtained with
instrumentation-based PGO and 93% on C++ benchmarks only. The profiling
overhead is only 1.06% on average whereas instrumentation incurs a 16% overhead
on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6365</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6365</id><created>2014-11-24</created><authors><author><keyname>Won</keyname><forenames>Ha Jong</forenames></author><author><keyname>Hwa</keyname><forenames>Choe Chun</forenames></author><author><keyname>Song</keyname><forenames>Li Kum</forenames></author></authors><title>On the mathematic modeling of non-parametric curves based on cubic
  B\'ezier curves</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  B\'ezier splines are widely available in various systems with the curves and
surface designs. In general, the B\'ezier spline can be specified with the
B\'ezier curve segments and a B\'ezier curve segment can be fitted to any
number of control points. The number of control points determines the degree of
the B\'ezier polynomial. This paper presents a method which determines control
points for B\'ezier curves approximating segments of obtained image
outline(non-parametric curve) by using the properties of cubic B\'ezier curves.
Proposed method is a technique to determine the control points that has
generality and reduces the error of the B\'ezier curve approximation. Main
advantage of proposed method is that it has higher accuracy and compression
rate than previous methods. The cubic B\'ezier spline is obtained from cubic
B\'ezier curve segments. To demonstrate the various performances of the
proposed algorithm, experimental results are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6369</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6369</id><created>2014-11-24</created><authors><author><keyname>Xu</keyname><forenames>Yichong</forenames></author><author><keyname>Xiao</keyname><forenames>Tianjun</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxing</forenames></author><author><keyname>Yang</keyname><forenames>Kuiyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>Scale-Invariant Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>This paper is submitted for CVPR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though convolutional neural networks (CNN) has achieved near-human
performance in various computer vision tasks, its ability to tolerate scale
variations is limited. The popular practise is making the model bigger first,
and then train it with data augmentation using extensive scale-jittering. In
this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a
modeldesigned to incorporate multi-scale feature exaction and classification
into the network structure. SiCNN uses a multi-column architecture, with each
column focusing on a particular scale. Unlike previous multi-column strategies,
these columns share the same set of filter parameters by a scale transformation
among them. This design deals with scale variation without blowing up the model
size. Experimental results show that SiCNN detects features at various scales,
and the classification result exhibits strong robustness against object scale
variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6370</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6370</id><created>2014-11-24</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Hu</keyname><forenames>Wenbo</forenames></author></authors><title>Big Learning with Bayesian Methods</title><categories>cs.LG stat.AP stat.CO stat.ME stat.ML</categories><comments>21 pages, 6 figures</comments><acm-class>F.1.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explosive growth in data and availability of cheap computing resources have
sparked increasing interest in Big learning, an emerging subfield that studies
scalable machine learning algorithms, systems, and applications with Big Data.
Bayesian methods represent one important class of statistic methods for machine
learning, with substantial recent developments on adaptive, flexible and
scalable Bayesian learning. This article provides a survey of the recent
advances in Big learning with Bayesian methods, termed Big Bayesian Learning,
including nonparametric Bayesian methods for adaptively inferring model
complexity, regularized Bayesian inference for improving the flexibility via
posterior regularization, and scalable algorithms and systems based on
stochastic subsampling and distributed computing for dealing with large-scale
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6371</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6371</id><created>2014-11-24</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Hesterberg</keyname><forenames>Adam</forenames></author><author><keyname>Ito</keyname><forenames>Hiro</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>Folding a Paper Strip to Minimize Thickness</title><categories>cs.DS cs.CG</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study how to fold a specified origami crease pattern in
order to minimize the impact of paper thickness. Specifically, origami designs
are often expressed by a mountain-valley pattern (plane graph of creases with
relative fold orientations), but in general this specification is consistent
with exponentially many possible folded states. We analyze the complexity of
finding the best consistent folded state according to two metrics: minimizing
the total number of layers in the folded state (so that a &quot;flat folding&quot; is
indeed close to flat), and minimizing the total amount of paper required to
execute the folding (where &quot;thicker&quot; creases consume more paper). We prove both
problems strongly NP-complete even for 1D folding. On the other hand, we prove
the first problem fixed-parameter tractable in 1D with respect to the number of
layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6372</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6372</id><created>2014-11-24</created><updated>2014-11-24</updated><authors><author><keyname>Akkaya</keyname><forenames>Ali</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Tugcu</keyname><forenames>Tuna</forenames></author></authors><title>Effect of Receptor Density and Size on Signal Reception in Molecular
  Communication via Diffusion with an Absorbing Receiver</title><categories>cs.ET cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of molecular communication is significantly impacted by the
reception process of the messenger molecules. The receptors' size and density,
however, have yet to be investigated. In this letter, we analyze the effect of
receptor density and size on the signal reception of an absorbing receiver with
receptors. The results show that, when the total receptor area is the same,
better hitting probability is achieved by using a higher number of relatively
small receptors. In addition, deploying receptors, which cover a small
percentage of the receiver surface, is able to create an effective
communication channel that has a detectable signal level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6382</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6382</id><created>2014-11-24</created><updated>2015-04-08</updated><authors><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Mid-level Deep Pattern Mining</title><categories>cs.CV</categories><comments>Published in Proc. IEEE Conf. Computer Vision and Pattern Recognition
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mid-level visual element discovery aims to find clusters of image patches
that are both representative and discriminative. In this work, we study this
problem from the prospective of pattern mining while relying on the recently
popularized Convolutional Neural Networks (CNNs). Specifically, we find that
for an image patch, activations extracted from the first fully-connected layer
of CNNs have two appealing properties which enable its seamless integration
with pattern mining. Patterns are then discovered from a large number of CNN
activations of image patches through the well-known association rule mining.
When we retrieve and visualize image patches with the same pattern,
surprisingly, they are not only visually similar but also semantically
consistent. We apply our approach to scene and object classification tasks, and
demonstrate that our approach outperforms all previous works on mid-level
visual element discovery by a sizeable margin with far fewer elements being
used. Our approach also outperforms or matches recent works using CNN for these
tasks. Source code of the complete system is available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6387</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6387</id><created>2014-11-24</created><updated>2014-12-17</updated><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author></authors><title>Deep Convolutional Neural Fields for Depth Estimation from a Single
  Image</title><categories>cs.CV</categories><comments>fixed some typos. in CVPR15 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of depth estimation from a single monocular image in
this work. It is a challenging task as no reliable depth cues are available,
e.g., stereo correspondences, motions, etc. Previous efforts have been focusing
on exploiting geometric priors or additional sources of information, with all
using hand-crafted features. Recently, there is mounting evidence that features
from deep convolutional neural networks (CNN) are setting new records for
various vision applications. On the other hand, considering the continuous
characteristic of the depth values, depth estimations can be naturally
formulated into a continuous conditional random field (CRF) learning problem.
Therefore, we in this paper present a deep convolutional neural field model for
estimating depths from a single image, aiming to jointly explore the capacity
of deep CNN and continuous CRF. Specifically, we propose a deep structured
learning scheme which learns the unary and pairwise potentials of continuous
CRF in a unified deep CNN framework.
  The proposed method can be used for depth estimations of general scenes with
no geometric priors nor any extra information injected. In our case, the
integral of the partition function can be analytically calculated, thus we can
exactly solve the log-likelihood optimization. Moreover, solving the MAP
problem for predicting depths of a new image is highly efficient as closed-form
solutions exist. We experimentally demonstrate that the proposed method
outperforms state-of-the-art depth estimation methods on both indoor and
outdoor scene datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6400</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6400</id><created>2014-11-24</created><updated>2015-03-29</updated><authors><author><keyname>Wei</keyname><forenames>Min</forenames></author><author><keyname>Chow</keyname><forenames>Tommy W. S.</forenames></author><author><keyname>Chan</keyname><forenames>Rosa H. M.</forenames></author></authors><title>Mutual Information-Based Unsupervised Feature Transformation for
  Heterogeneous Feature Subset Selection</title><categories>stat.ML cs.LG</categories><comments>This paper has been withdrawn by the author due to the number of
  datasets and classifiers are not sufficient to support the claim. Need more
  simulation work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional mutual information (MI) based feature selection (FS) methods are
unable to handle heterogeneous feature subset selection properly because of
data format differences or estimation methods of MI between feature subset and
class label. A way to solve this problem is feature transformation (FT). In
this study, a novel unsupervised feature transformation (UFT) which can
transform non-numerical features into numerical features is developed and
tested. The UFT process is MI-based and independent of class label. MI-based FS
algorithms, such as Parzen window feature selector (PWFS), minimum redundancy
maximum relevance feature selection (mRMR), and normalized MI feature selection
(NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unlike
traditional FT methods, the proposed UFT is unbiased while PWFS is utilized to
its full advantage. Simulations and analyses of large-scale datasets showed
that feature subset selected by the integrated method, UFT-PWFS, outperformed
other FT-FS integrated methods in classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6406</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6406</id><created>2014-11-24</created><authors><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Encoding High Dimensional Local Features by Sparse Coding Based Fisher
  Vectors</title><categories>cs.CV</categories><comments>Appearing in Proc. Advances in Neural Information Processing Systems
  (NIPS) 2014, Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deriving from the gradient vector of a generative model of local features,
Fisher vector coding (FVC) has been identified as an effective coding method
for image classification. Most, if not all, % FVC implementations employ the
Gaussian mixture model (GMM) to characterize the generation process of local
features. This choice has shown to be sufficient for traditional low
dimensional local features, e.g., SIFT; and typically, good performance can be
achieved with only a few hundred Gaussian distributions. However, the same
number of Gaussians is insufficient to model the feature space spanned by
higher dimensional local features, which have become popular recently. In order
to improve the modeling capacity for high dimensional features, it turns out to
be inefficient and computationally impractical to simply increase the number of
Gaussians. In this paper, we propose a model in which each local feature is
drawn from a Gaussian distribution whose mean vector is sampled from a
subspace. With certain approximation, this model can be converted to a sparse
coding procedure and the learning/inference problems can be readily solved by
standard sparse coding methods. By calculating the gradient vector of the
proposed model, we derive a new fisher vector encoding strategy, termed Sparse
Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently
developed Deep Convolutional Neural Network (CNN) descriptor as a high
dimensional local feature and implement image classification with the proposed
SCFVC. Our experimental evaluations demonstrate that our method not only
significantly outperforms the traditional GMM based Fisher vector encoding but
also achieves the state-of-the-art performance in generic object recognition,
indoor scene, and fine-grained image classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6408</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6408</id><created>2014-11-24</created><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Sorting Networks: the End Game</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies properties of the back end of a sorting network and
illustrates the utility of these in the search for networks of optimal size or
depth. All previous works focus on properties of the front end of networks and
on how to apply these to break symmetries in the search. The new properties
help shed understanding on how sorting networks sort and speed-up solvers for
both optimal size and depth by an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6409</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6409</id><created>2014-11-24</created><authors><author><keyname>Bjorgvinsdottir</keyname><forenames>H.</forenames></author><author><keyname>Bentley</keyname><forenames>P. M.</forenames></author></authors><title>Warp2: A Method of Email and Messaging with Encrypted Addressing and
  Headers</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure communications are playing increasing roles in society, particularly
in finance, journalism, and military projects. Current methods of securing
e-mail and similar messaging methods rely on encryption of the message body,
but the header with addressing information remains plaintext. This allows third
party eavesdroppers to collect and analyse the header metadata and construct a
network model of the participants in conversations (who, where, when, subject).
In this article, we describe a method of communication where the header is also
encrypted, hindering the assembly of the communication network models, which is
verified with a working prototype application. This provides a useful tool to
journalists and proponents of free speech in oppressed countries, protecting
both the messages and their sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6432</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6432</id><created>2014-11-24</created><updated>2016-01-21</updated><authors><author><keyname>Wild</keyname><forenames>Marcel</forenames></author></authors><title>The joy of implications, aka pure Horn functions: mainly a survey</title><categories>cs.LO</categories><comments>This version is near identical to the accepted version in Theoretical
  Computer Science</comments><msc-class>68Txx, 94C10, 06B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apart from a brief look at applications (Relational Databases, Formal Concept
Analysis, data mining) this article is devoted to the mathematical t h e o r y
of implications (=pure Horn formulas). It is mainly a survey of results
obtained in the last thirty years, but features a few novelties as well. Some
keywords: The Duquenne-Guiges (implicational) base, the canonical direct base,
prime implicates, the consensus method, implications and meet irreducible
closed sets, optimum bases for certain lattices, ordered direct bases,
generating all closed sets, general (i.e. impure) Horn functions. We pose four
open problems to stimulate further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6447</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6447</id><created>2014-11-24</created><authors><author><keyname>Xiao</keyname><forenames>Tianjun</forenames></author><author><keyname>Xu</keyname><forenames>Yichong</forenames></author><author><keyname>Yang</keyname><forenames>Kuiyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxing</forenames></author><author><keyname>Peng</keyname><forenames>Yuxin</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>The Application of Two-level Attention Models in Deep Convolutional
  Neural Network for Fine-grained Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-grained classification is challenging because categories can only be
discriminated by subtle and local differences. Variances in the pose, scale or
rotation usually make the problem more difficult. Most fine-grained
classification systems follow the pipeline of finding foreground object or
object parts (where) to extract discriminative features (what).
  In this paper, we propose to apply visual attention to fine-grained
classification task using deep neural network. Our pipeline integrates three
types of attention: the bottom-up attention that propose candidate patches, the
object-level top-down attention that selects relevant patches to a certain
object, and the part-level top-down attention that localizes discriminative
parts. We combine these attentions to train domain-specific deep nets, then use
it to improve both the what and where aspects. Importantly, we avoid using
expensive annotations like bounding box or part information from end-to-end.
The weak supervision constraint makes our work easier to generalize.
  We have verified the effectiveness of the method on the subsets of ILSVRC2012
dataset and CUB200_2011 dataset. Our pipeline delivered significant
improvements and achieved the best accuracy under the weakest supervision
condition. The performance is competitive against other methods that rely on
additional annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6452</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6452</id><created>2014-11-24</created><updated>2015-12-08</updated><authors><author><keyname>Kroupa</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Teheux</keyname><forenames>Bruno</forenames></author></authors><title>Modal Extensions of {\L}ukasiewicz Logic for Modeling Coalitional Power</title><categories>math.LO cs.LO</categories><comments>Accepted to Journal of Logic and Computation</comments><msc-class>03B50, 03B45 (Primary) 91A40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modal logics for reasoning about the power of coalitions capture the notion
of effectivity functions associated with game forms. The main goal of coalition
logics is to provide formal tools for modeling the dynamics of a game frame
whose states may correspond to different game forms. The two classes of
effectivity functions studied are the families of playable and truly playable
effectivity functions, respectively. In this paper we generalize the concept of
effectivity function beyond the yes/no truth scale. This enables us to describe
the situations in which the coalitions assess their effectivity in degrees,
based on functions over the outcomes taking values in a finite {\L}ukasiewicz
chain. Then we introduce two modal extensions of {\L}ukasiewicz finite-valued
logic together with many-valued neighborhood semantics in order to encode the
properties of many-valued effectivity functions associated with game forms. As
our main results we prove completeness theorems for the two newly introduced
modal logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6462</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6462</id><created>2014-11-24</created><authors><author><keyname>Doran</keyname><forenames>Derek</forenames></author><author><keyname>Gokhale</keyname><forenames>Swapna</forenames></author><author><keyname>Dagnino</keyname><forenames>Aldo</forenames></author></authors><title>Understanding Common Perceptions from Online Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>In Proc. of the International Conference of Software Engineering and
  Knowledge Engineering, pp. 107-112, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern society habitually uses online social media services to publicly share
observations, thoughts, opinions, and beliefs at any time and from any
location. These geotagged social media posts may provide aggregate insights
into people's perceptions on a bad range of topics across a given geographical
area beyond what is currently possible through services such as Yelp and
Foursquare. This paper develops probabilistic language models to investigate
whether collective, topic-based perceptions within a geographical area can be
extracted from the content of geotagged Twitter posts. The capability of the
methodology is illustrated using tweets from three areas of different sizes. An
application of the approach to support power grid restoration following a storm
is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6463</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6463</id><created>2014-11-24</created><authors><author><keyname>Verdier</keyname><forenames>Guillaume</forenames></author><author><keyname>Raclet</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Quotient of Acceptance Specifications under Reachability Constraints</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quotient operation, which is dual to the composition, is crucial in
specification theories as it allows the synthesis of missing specifications and
thus enables incremental design. In this paper, we consider a specification
theory based on marked acceptance specifications (MAS) which are automata
enriched with variability information encoded by acceptance sets and with
reachability constraints on states. We define a sound and complete quotient for
MAS hence ensuring reachability properties by construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6466</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6466</id><created>2014-11-24</created><authors><author><keyname>Moussa</keyname><forenames>May</forenames></author><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Interference Cancellation trough Interference Alignment for Downlink of
  Cognitive Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose the interference cancellation through interference
alignment at the downlink of cognitive cellular networks. Interference
alignment helps the spatial resources to be shared among primary and secondary
cells and thus, it can provide higher degrees of freedom through interference
cancellation. We derive and depict the achievable degrees of freedom. We also
analyse and calculate the achievable sum rates applying water-filling optimal
power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6469</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6469</id><created>2014-11-24</created><updated>2015-03-03</updated><authors><author><keyname>Matthiesen</keyname><forenames>Bho</forenames></author><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Resource Allocation for Energy-Efficient 3-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 14, no. 8, pp.
  4454 - 4468, Aug. 2015</journal-ref><doi>10.1109/TWC.2015.2421496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Throughput and energy efficiency in 3-way relay channels are studied in this
paper. Unlike previous contributions, we consider a circular message exchange.
First, an outer bound and achievable sum rate expressions for different
relaying protocols are derived for 3-way relay channels. The sum capacity is
characterized for certain SNR regimes. Next, leveraging the derived achievable
sum rate expressions, cooperative and competitive maximization of the energy
efficiency are considered. For the cooperative case, both low-complexity and
globally optimal algorithms for joint power allocation at the users and at the
relay are designed so as to maximize the system global energy efficiency. For
the competitive case, a game theoretic approach is taken, and it is shown that
the best response dynamics is guaranteed to converge to a Nash equilibrium. A
power consumption model for mmWave board-to-board communications is developed,
and numerical results are provided to corroborate and provide insight on the
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6478</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6478</id><created>2014-11-24</created><updated>2015-10-22</updated><authors><author><keyname>Friedman</keyname><forenames>Roy</forenames><affiliation>IUF, UR1, ASAP</affiliation></author><author><keyname>Raynal</keyname><forenames>Michel</forenames><affiliation>IUF, UR1, ASAP</affiliation></author><author><keyname>Ta&#xef;ani</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>UR1, ASAP</affiliation></author></authors><title>Fisheye Consistency: Keeping Data in Synch in a Georeplicated World</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last thirty years, numerous consistency conditions for replicated
data have been proposed and implemented. Popular examples of such conditions
include linearizability (or atomicity), sequential consistency, causal
consistency, and eventual consistency. These consistency conditions are usually
defined independently from the computing entities (nodes) that manipulate the
replicated data; i.e., they do not take into account how computing entities
might be linked to one another, or geographically distributed. To address this
lack, as a first contribution, this paper introduces the notion of proximity
graph between computing nodes. If two nodes are connected in this graph, their
operations must satisfy a strong consistency condition, while the operations
invoked by other nodes are allowed to satisfy a weaker condition. The second
contribution is the use of such a graph to provide a generic approach to the
hybridization of data consistency conditions into the same system. We
illustrate this approach on sequential consistency and causal consistency, and
present a model in which all data operations are causally consistent, while
operations by neighboring processes in the proximity graph are sequentially
consistent. The third contribution of the paper is the design and the proof of
a distributed algorithm based on this proximity graph, which combines
sequential consistency and causal consistency (the resulting condition is
called fisheye consistency). In doing so the paper not only extends the domain
of consistency conditions, but provides a generic provably correct solution of
direct relevance to modern georeplicated systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6496</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6496</id><created>2014-11-24</created><authors><author><keyname>Raventos</keyname><forenames>Arnau</forenames></author><author><keyname>Quijada</keyname><forenames>Raul</forenames></author><author><keyname>Torres</keyname><forenames>Luis</forenames></author><author><keyname>Tarres</keyname><forenames>Francesc</forenames></author></authors><title>Automatic Summarization of Soccer Highlights Using Audio-visual
  Descriptors</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic summarization generation of sports video content has been object of
great interest for many years. Although semantic descriptions techniques have
been proposed, many of the approaches still rely on low-level video descriptors
that render quite limited results due to the complexity of the problem and to
the low capability of the descriptors to represent semantic content. In this
paper, a new approach for automatic highlights summarization generation of
soccer videos using audio-visual descriptors is presented. The approach is
based on the segmentation of the video sequence into shots that will be further
analyzed to determine its relevance and interest. Of special interest in the
approach is the use of the audio information that provides additional
robustness to the overall performance of the summarization system. For every
video shot a set of low and mid level audio-visual descriptors are computed and
lately adequately combined in order to obtain different relevance measures
based on empirical knowledge rules. The final summary is generated by selecting
those shots with highest interest according to the specifications of the user
and the results of relevance measures. A variety of results are presented with
real soccer video sequences that prove the validity of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6498</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6498</id><created>2014-11-24</created><authors><author><keyname>Kornerup</keyname><forenames>Peter</forenames></author></authors><title>Correction to the 2005 paper: &quot;Digit Selection for SRT Division and
  Square Root&quot;</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been pointed out by counterexamples in a 2013 paper in the IEEE
Transactions on Computers [1], that there is an error in the previously ibid.\
in 2005 published paper [2] on the construction of valid digit selection tables
for SRT type division and square root algorithms. The error has been corrected,
and new results found on selection constants for maximally redundant digit
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6509</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6509</id><created>2014-11-24</created><authors><author><keyname>Razavian</keyname><forenames>Ali Sharif</forenames></author><author><keyname>Azizpour</keyname><forenames>Hossein</forenames></author><author><keyname>Maki</keyname><forenames>Atsuto</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Ek</keyname><forenames>Carl Henrik</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>Persistent Evidence of Local Image Properties in Generic ConvNets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised training of a convolutional network for object classification
should make explicit any information related to the class of objects and
disregard any auxiliary information associated with the capture of the image or
the variation within the object class. Does this happen in practice? Although
this seems to pertain to the very final layers in the network, if we look at
earlier layers we find that this is not the case. Surprisingly, strong spatial
information is implicit. This paper addresses this, in particular, exploiting
the image representation at the first fully connected layer, i.e. the global
image descriptor which has been recently shown to be most effective in a range
of visual recognition tasks. We empirically demonstrate evidences for the
finding in the contexts of four different tasks: 2d landmark detection, 2d
object keypoints prediction, estimation of the RGB values of input image, and
recovery of semantic label of each pixel. We base our investigation on a simple
framework with ridge rigression commonly across these tasks, and show results
which all support our insight. Such spatial information can be used for
computing correspondence of landmarks to a good accuracy, but should
potentially be useful for improving the training of the convolutional nets for
classification purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6520</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6520</id><created>2014-11-24</created><authors><author><keyname>Trofimov</keyname><forenames>Ilya</forenames></author><author><keyname>Genkin</keyname><forenames>Alexander</forenames></author></authors><title>Distributed Coordinate Descent for L1-regularized Logistic Regression</title><categories>stat.ML cs.LG</categories><journal-ref>Analysis of Images, Social Networks and Texts. Fourth
  International Conference, AIST 2015, Yekaterinburg, Russia, April 9-11, 2015,
  Revised Selected Papers. Communications in Computer and Information Science,
  Vol. 542, 243-254, Springer</journal-ref><doi>10.1007/978-3-319-26123-2_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving logistic regression with L1-regularization in distributed settings is
an important problem. This problem arises when training dataset is very large
and cannot fit the memory of a single machine. We present d-GLMNET, a new
algorithm solving logistic regression with L1-regularization in the distributed
settings. We empirically show that it is superior over distributed online
learning via truncated gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6521</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6521</id><created>2014-11-24</created><updated>2014-11-24</updated><authors><author><keyname>Luo</keyname><forenames>Tie</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author><author><keyname>Srinivasan</keyname><forenames>Vikram</forenames></author></authors><title>Energy-Efficient Strategies for Cooperative Multi-Channel MAC Protocols</title><categories>cs.NI cs.PF</categories><comments>Energy efficiency, cost efficiency, distributed information sharing,
  DISH, altruistic cooperation</comments><journal-ref>IEEE Transactions on Mobile Computing (TMC), vol. 11, no. 4, pp.
  553-566, April 2012</journal-ref><doi>10.1109/TMC.2011.60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Information SHaring (DISH) is a new cooperative approach to
designing multi-channel MAC protocols. It aids nodes in their decision making
processes by compensating for their missing information via information sharing
through other neighboring nodes. This approach was recently shown to
significantly boost the throughput of multi-channel MAC protocols. However, a
critical issue for ad hoc communication devices, i.e., energy efficiency, has
yet to be addressed. In this paper, we address this issue by developing simple
solutions which (1) reduce the energy consumption (2) without compromising the
throughput performance, and meanwhile (3) maximize cost efficiency. We propose
two energy-efficient strategies: in-situ energy conscious DISH which uses
existing nodes only, and altruistic DISH which needs additional nodes called
altruists. We compare five protocols with respect to the strategies and
identify altruistic DISH to be the right choice in general: it (1) conserves
40-80% of energy, (2) maintains the throughput advantage gained from the DISH
approach, and (3) more than doubles the cost efficiency compared to protocols
without applying the strategy. On the other hand, our study shows that in-situ
energy conscious DISH is suitable only in certain limited scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6529</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6529</id><created>2014-11-24</created><authors><author><keyname>Li</keyname><forenames>Tiancheng</forenames></author></authors><title>The Optimal Arbitrary-Proportional Finite-Set-Partitioning</title><categories>cs.NA stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the arbitrary-proportional finite-set-partitioning
problem which involves partitioning a finite set into multiple subsets with
respect to arbitrary nonnegative proportions. This is the core art of many
fundamental problems such as determining quotas for different individuals of
different weights or sampling from a discrete-valued weighted sample set to get
a new identically distributed but non-weighted sample set (e.g. the resampling
needed in the particle filter). The challenge raises as the size of each subset
must be an integer while its unbiased expectation is often not. To solve this
problem, a metric (cost function) is defined on their discrepancies and
correspondingly a solution is proposed to determine the sizes of each subsets,
gaining the minimal bias. Theoretical proof and simulation demonstrations are
provided to demonstrate the optimality of the scheme in the sense of the
proposed metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6538</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6538</id><created>2014-11-24</created><authors><author><keyname>Adelgren</keyname><forenames>Nathan</forenames></author><author><keyname>Belotti</keyname><forenames>Pietro</forenames></author><author><keyname>Gupte</keyname><forenames>Akshay</forenames></author></authors><title>Efficient storage of Pareto points in biobjective mixed integer
  programming</title><categories>cs.DS math.OC</categories><comments>Shorter version accepted to Proceedings of the INFORMS Computing
  Society Meeting 2015</comments><msc-class>90C29, 90C11</msc-class><acm-class>E.1.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biobjective mixed integer linear programs (BOMILP) are optimization problems
where two linear objectives are optimized over a polyhedron while restricting
some of the variables to be integer. Since many of the techniques for solving
BOMILP (or approximating its solution set) are iterative processes which
utilize data discovered during early iterations to aid in the discovery of
improved data during later iterations, it is highly desirable to efficiently
store the nondominated subset of a given set of data. This problem has not
received considerable attention in the context of BOMILP; only naive methods
have been implemented. We seek to bridge this gap by presenting a new data
structure in the form of a modified binary tree that stores, updates, searches
and returns nondominated solutions. This structure takes points and line
segments in $\mathbb{R}^2$ as input and stores the nondominated subset of this
input. We note that when used alongside an exact solution procedure, such as
branch-and-bound (BB), at termination the data stored by this structure is
precisely the set of Pareto optimal solutions. We perform two experiments. The
first is designed to compare the utility of our structure for storing
nondominated data to that of a dynamic list which updates via pairwise
comparison. In the second we use our data structure alongside the biobjective
BB techniques available in the literature and solve specific instances of
BOMILP. The results of our first experiment suggest that the data structure
performs reasonably well in handling input of up to $10^7$ points or segments
and does so much more efficiently than a dynamic list. The results of the
second experiment show that when our structure is utilized alongside BB
fathoming is enhanced and running times improve slightly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6549</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6549</id><created>2014-11-24</created><authors><author><keyname>Bliem</keyname><forenames>Bernhard</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Complexity of Secure Sets</title><categories>cs.CC</categories><comments>12 pages, 7 figures, submitted to CCC 2015</comments><msc-class>68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A secure set $S$ in a graph is defined as a set of vertices such that for any
$X \subseteq S$ the majority of vertices in the neighborhood of $X$ belongs to
$S$. It is known that deciding whether a set $S$ is secure in a graph is
co-NP-complete. However, it is still open how this result contributes to the
actual complexity of deciding whether for a given graph $G$ and integer $k$, a
non-empty secure set for $G$ of size at most $k$ exists. While membership in
the class $\Sigma^P_2$ is rather easy to see for this existence problem,
showing $\Sigma^P_2$-hardness is quite involved. In this paper, we provide such
a hardness result, hence classifying the secure set existence problem as
$\Sigma^P_2$-complete. We do so by first showing hardness for a variant of the
problem, which we then reduce step-by-step to secure set existence. In total,
we obtain eight new completeness results for different variants of the secure
set existence problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6550</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6550</id><created>2014-11-24</created><updated>2015-02-04</updated><authors><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Kolmogorov-Zakharov Model for Optical Fiber Communication</title><categories>cs.IT math.IT</categories><comments>Minor changes. The introduction is now more detailed. Additional
  references and a figure have been added. A few typos in formulas have been
  fixed. The organization has slightly changed in some sections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A power spectral density based on the theory of weak wave turbulence is
suggested for calculating the interference power in dense wavelength-division
multiplexed optical systems. This power spectrum, termed Kolmogorov-Zakharov
(KZ) model, results in a better estimate of the signal spectrum in optical
fiber, compared with the so-called Gaussian noise (GN) model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6562</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6562</id><created>2014-11-12</created><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Garcia-Molina</keyname><forenames>Hector</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Evaluating the Crowd with Confidence</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Worker quality control is a crucial aspect of crowdsourcing systems;
typically occupying a large fraction of the time and money invested on
crowdsourcing. In this work, we devise techniques to generate confidence
intervals for worker error rate estimates, thereby enabling a better evaluation
of worker quality. We show that our techniques generate correct confidence
intervals on a range of real-world datasets, and demonstrate wide applicability
by using them to evict poorly performing workers, and provide confidence
intervals on the accuracy of the answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6573</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6573</id><created>2014-11-24</created><updated>2014-11-28</updated><authors><author><keyname>Tse</keyname><forenames>Rita</forenames></author><author><keyname>Xiao</keyname><forenames>Yubin</forenames></author><author><keyname>Pau</keyname><forenames>Giovanni</forenames></author><author><keyname>Roccetti</keyname><forenames>Marco</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author><author><keyname>Marfia</keyname><forenames>Gustavo</forenames></author></authors><title>On the Feasibility of Social Network-based Pollution Sensing in ITSs</title><categories>cs.SI cs.NI</categories><comments>10 pages, 15 figures, Transaction Format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intense vehicular traffic is recognized as a global societal problem, with a
multifaceted influence on the quality of life of a person. Intelligent
Transportation Systems (ITS) can play an important role in combating such
problem, decreasing pollution levels and, consequently, their negative effects.
One of the goals of ITSs, in fact, is that of controlling traffic flows,
measuring traffic states, providing vehicles with routes that globally pursue
low pollution conditions. How such systems measure and enforce given traffic
states has been at the center of multiple research efforts in the past few
years. Although many different solutions have been proposed, very limited
effort has been devoted to exploring the potential of social network analysis
in such context. Social networks, in general, provide direct feedback from
people and, as such, potentially very valuable information. A post that tells,
for example, how a person feels about pollution at a given time in a given
location, could be put to good use by an environment aware ITS aiming at
minimizing contaminant emissions in residential areas. This work verifies the
feasibility of using pollution related social network feeds into ITS
operations. In particular, it concentrates on understanding how reliable such
information is, producing an analysis that confronts over 1,500,000 posts and
pollution data obtained from on-the- field sensors over a one-year span.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6574</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6574</id><created>2014-11-24</created><authors><author><keyname>Pastor-Escuredo</keyname><forenames>David</forenames></author><author><keyname>Morales-Guzm&#xe1;n</keyname><forenames>Alfredo</forenames></author><author><keyname>Torres-Fern&#xe1;ndez</keyname><forenames>Yolanda</forenames></author><author><keyname>Bauer</keyname><forenames>Jean-Martin</forenames></author><author><keyname>Wadhwa</keyname><forenames>Amit</forenames></author><author><keyname>Castro-Correa</keyname><forenames>Carlos</forenames></author><author><keyname>Romanoff</keyname><forenames>Liudmyla</forenames></author><author><keyname>Lee</keyname><forenames>Jong Gun</forenames></author><author><keyname>Rutherford</keyname><forenames>Alex</forenames></author><author><keyname>Frias-Martinez</keyname><forenames>Vanessa</forenames></author><author><keyname>Oliver</keyname><forenames>Nuria</forenames></author><author><keyname>Frias-Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Luengo-Oroz</keyname><forenames>Miguel</forenames></author></authors><title>Flooding through the lens of mobile phone activity</title><categories>cs.CY</categories><comments>Submitted to IEEE Global Humanitarian Technologies Conference (GHTC)
  2014</comments><journal-ref>IEEE Global Humanitarian Technology Conference (GHTC), 2014 IEEE
  (pp. 279-286)</journal-ref><doi>10.1109/GHTC.2014.6970293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural disasters affect hundreds of millions of people worldwide every year.
Emergency response efforts depend upon the availability of timely information,
such as information concerning the movements of affected populations. The
analysis of aggregated and anonymized Call Detail Records (CDR) captured from
the mobile phone infrastructure provides new possibilities to characterize
human behavior during critical events. In this work, we investigate the
viability of using CDR data combined with other sources of information to
characterize the floods that occurred in Tabasco, Mexico in 2009. An impact map
has been reconstructed using Landsat-7 images to identify the floods. Within
this frame, the underlying communication activity signals in the CDR data have
been analyzed and compared against rainfall levels extracted from data of the
NASA-TRMM project. The variations in the number of active phones connected to
each cell tower reveal abnormal activity patterns in the most affected
locations during and after the floods that could be used as signatures of the
floods - both in terms of infrastructure impact assessment and population
information awareness. The representativeness of the analysis has been assessed
using census data and civil protection records. While a more extensive
validation is required, these early results suggest high potential in using
cell tower activity information to improve early warning and emergency
management mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6580</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6580</id><created>2014-11-20</created><updated>2014-12-03</updated><authors><author><keyname>Reznik</keyname><forenames>Aleksander</forenames></author><author><keyname>Efimov</keyname><forenames>Vitaly</forenames></author><author><keyname>Soloview</keyname><forenames>Aleksander</forenames></author><author><keyname>Torgov</keyname><forenames>Andrey</forenames></author></authors><title>The Solving of the Problems with Random Division of an Interval with Use
  of Computer Analytic Programs</title><categories>cs.OH math.PR</categories><comments>Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An original approach to solving rather difficult probabilistic problems
arising in studying the readout of random discrete fields and having no exact
analytical solutions at the moment is proposed. Several algorithms for direct,
iterative, and combinatorial-recursive calculations of multidimensional
integral expressions, which can describe partial solutions of these problems,
are presented (these solutions are further used to search for the common closed
analytical regularities). The huge volume of necessary calculations forced us
to formalize completely the algorithms and to transfer all the burden of
routine analytical transforms to a computer. The calculations performed helped
us to establish (and to prove later) a number of new earlier unknown
probabilistic formulas responsible for random division of an interval. One more
important feature of this study is the fact that we introduced a new concept of
'three-dimensional generalized Catalan numbers' and found their explicit form
in solving problems associated with random division of an interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6581</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6581</id><created>2014-11-24</created><updated>2015-06-15</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author></authors><title>Optimal Encodings for Range Top-k, Selection, and Min-Max</title><categories>cs.DS</categories><comments>24 pages: a short version of this paper will be presented at ICALP
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider encoding problems for range queries on arrays. In these problems
the goal is to store a structure capable of recovering the answer to all
queries that occupies the information theoretic minimum space possible, to
within lower order terms. As input, we are given an array $A[1..n]$, and a
fixed parameter $k \in [1,n]$. A range top-$k$ query on an arbitrary range
$[i,j] \subseteq [1,n]$ asks us to return the ordered set of indices $\{\ell_1,
..., \ell_{k}\}$ such that $A[\ell_m]$ is the $m$-th largest element in
$A[i..j]$, for $1 \le m \le k$. A range selection query for an arbitrary range
$[i,j] \subseteq [1,n]$ and query parameter $k' \in [1,k]$ asks us to return
the index of the $k'$-th largest element in $A[i..j]$. We completely resolve
the space complexity of both of these heavily studied problems---to within
lower order terms---for all $k = o(n)$. Previously, the constant factor in the
space complexity was known only for $k=1$. We also resolve the space complexity
of another problem, that we call range min-max, in which the goal is to return
the indices of both the minimum and maximum elements in a range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6587</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6587</id><created>2014-11-08</created><updated>2014-11-26</updated><authors><author><keyname>Zandieh</keyname><forenames>Amir</forenames></author><author><keyname>Zareian</keyname><forenames>Alireza</forenames></author><author><keyname>Azghani</keyname><forenames>Masoumeh</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Reconstruction of Sub-Nyquist Random Sampling for Sparse and Multi-Band
  Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As technology grows, higher frequency signals are required to be processed in
various applications. In order to digitize such signals, conventional analog to
digital convertors are facing implementation challenges due to the higher
sampling rates. Hence, lower sampling rates (i.e., sub-Nyquist) are considered
to be cost efficient. A well-known approach is to consider sparse signals that
have fewer nonzero frequency components compared to the highest frequency
component. For the prior knowledge of the sparse positions, well-established
methods already exist. However, there are applications where such information
is not available. For such cases, a number of approaches have recently been
proposed. In this paper, we propose several random sampling recovery algorithms
which do not require any anti-aliasing filter. Moreover, we offer certain
conditions under which these recovery techniques converge to the signal.
Finally, we also confirm the performance of the above methods through extensive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6590</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6590</id><created>2014-11-24</created><authors><author><keyname>Trillos</keyname><forenames>Nicolas Garcia</forenames></author><author><keyname>Slepcev</keyname><forenames>Dejan</forenames></author><author><keyname>von Brecht</keyname><forenames>James</forenames></author><author><keyname>Laurent</keyname><forenames>Thomas</forenames></author><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author></authors><title>Consistency of Cheeger and Ratio Graph Cuts</title><categories>stat.ML cs.LG math.ST stat.TH</categories><msc-class>62H30, 62G20, 49J55, 91C20, 68R10, 60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes the consistency of a family of graph-cut-based
algorithms for clustering of data clouds. We consider point clouds obtained as
samples of a ground-truth measure. We investigate approaches to clustering
based on minimizing objective functionals defined on proximity graphs of the
given sample. Our focus is on functionals based on graph cuts like the Cheeger
and ratio cuts. We show that minimizers of the these cuts converge as the
sample size increases to a minimizer of a corresponding continuum cut (which
partitions the ground truth measure). Moreover, we obtain sharp conditions on
how the connectivity radius can be scaled with respect to the number of sample
points for the consistency to hold. We provide results for two-way and for
multiway cuts. Furthermore we provide numerical experiments that illustrate the
results and explore the optimality of scaling in dimension two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6591</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6591</id><created>2014-10-31</created><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Chen</keyname><forenames>George H.</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>A Latent Source Model for Online Collaborative Filtering</title><categories>cs.LG cs.IR stat.ML</categories><comments>Advances in Neural Information Processing Systems (NIPS 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the prevalence of collaborative filtering in recommendation systems,
there has been little theoretical development on why and how well it works,
especially in the &quot;online&quot; setting, where items are recommended to users over
time. We address this theoretical gap by introducing a model for online
recommendation systems, cast item recommendation under the model as a learning
problem, and analyze the performance of a cosine-similarity collaborative
filtering method. In our model, each of $n$ users either likes or dislikes each
of $m$ items. We assume there to be $k$ types of users, and all the users of a
given type share a common string of probabilities determining the chance of
liking each item. At each time step, we recommend an item to each user, where a
key distinction from related bandit literature is that once a user consumes an
item (e.g., watches a movie), then that item cannot be recommended to the same
user again. The goal is to maximize the number of likable items recommended to
users over time. Our main result establishes that after nearly $\log(km)$
initial learning time steps, a simple collaborative filtering algorithm
achieves essentially optimal performance without knowing $k$. The algorithm has
an exploitation step that uses cosine similarity and two types of exploration
steps, one to explore the space of items (standard in the literature) and the
other to explore similarity between users (novel to this work).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6593</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6593</id><created>2014-11-24</created><authors><author><keyname>Tolpin</keyname><forenames>David</forenames></author><author><keyname>Betzalel</keyname><forenames>Oded</forenames></author><author><keyname>Felner</keyname><forenames>Ariel</forenames></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Rational Deployment of Multiple Heuristics in IDA*</title><categories>cs.AI</categories><comments>7 pages, 6 tables, 20 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in metareasoning for search has shown its usefulness in
improving numerous search algorithms. This paper applies rational metareasoning
to IDA* when several admissible heuristics are available. The obvious basic
approach of taking the maximum of the heuristics is improved upon by lazy
evaluation of the heuristics, resulting in a variant known as Lazy IDA*. We
introduce a rational version of lazy IDA* that decides whether to compute the
more expensive heuristics or to bypass it, based on a myopic expected regret
estimate. Empirical evaluation in several domains supports the theoretical
results, and shows that rational lazy IDA* is a state-of-the-art heuristic
combination method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6622</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6622</id><created>2014-11-24</created><authors><author><keyname>Osoba</keyname><forenames>Osonde Adekorede</forenames></author></authors><title>Noise Benefits in Expectation-Maximization Algorithms</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>A Dissertation Presented to The Faculty of The USC Graduate School
  University of Southern California In Partial Fulfillment of the Requirements
  for the Degree Doctor of Philosophy (Electrical Engineering) August 2013.
  (252 pages, 45 figures), Online:
  http://digitallibrary.usc.edu/cdm/ref/collection/p15799coll3/id/294341</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation shows that careful injection of noise into sample data can
substantially speed up Expectation-Maximization algorithms.
Expectation-Maximization algorithms are a class of iterative algorithms for
extracting maximum likelihood estimates from corrupted or incomplete data. The
convergence speed-up is an example of a noise benefit or &quot;stochastic resonance&quot;
in statistical signal processing. The dissertation presents derivations of
sufficient conditions for such noise-benefits and demonstrates the speed-up in
some ubiquitous signal-processing algorithms. These algorithms include
parameter estimation for mixture models, the $k$-means clustering algorithm,
the Baum-Welch algorithm for training hidden Markov models, and backpropagation
for training feedforward artificial neural networks. This dissertation also
analyses the effects of data and model corruption on the more general Bayesian
inference estimation framework. The main finding is a theorem guaranteeing that
uniform approximators for Bayesian model functions produce uniform
approximators for the posterior pdf via Bayes theorem. This result also applies
to hierarchical and multidimensional Bayesian models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6644</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6644</id><created>2014-11-24</created><updated>2015-01-08</updated><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author></authors><title>Decidability and Universality of Quasiminimal Subshifts</title><categories>math.DS cs.FL</categories><comments>40 pages, 1 figure, submitted to JCSS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the quasiminimal subshifts, subshifts having only finitely many
subsystems. With $\mathbb{N}$-actions, their theory essentially reduces to the
theory of minimal systems, but with $\mathbb{Z}$-actions, the class is much
larger. We show many examples of such subshifts, and in particular construct a
universal system with only a single proper subsystem, refuting a conjecture of
[Delvenne, K\r{u}rka, Blondel, '05].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6646</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6646</id><created>2014-11-24</created><updated>2014-12-26</updated><authors><author><keyname>Bollig</keyname><forenames>Benedikt</forenames><affiliation>LSV, ENS Cachan, CNRS &amp; Inria, France</affiliation></author><author><keyname>Habermehl</keyname><forenames>Peter</forenames><affiliation>LIAFA University Paris Diderot, France</affiliation></author><author><keyname>Leucker</keyname><forenames>Martin</forenames><affiliation>ISP, University of L&#xfc;beck, Germany</affiliation></author><author><keyname>Monmege</keyname><forenames>Benjamin</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Belgium</affiliation></author></authors><title>A Robust Class of Data Languages and an Application to Learning</title><categories>cs.LO cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  30, 2014) lmcs:1030</journal-ref><doi>10.2168/LMCS-10(4:19)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce session automata, an automata model to process data words, i.e.,
words over an infinite alphabet. Session automata support the notion of fresh
data values, which are well suited for modeling protocols in which sessions
using fresh values are of major interest, like in security protocols or ad-hoc
networks. Session automata have an expressiveness partly extending, partly
reducing that of classical register automata. We show that, unlike register
automata and their various extensions, session automata are robust: They (i)
are closed under intersection, union, and (resource-sensitive) complementation,
(ii) admit a symbolic regular representation, (iii) have a decidable inclusion
problem (unlike register automata), and (iv) enjoy logical characterizations.
Using these results, we establish a learning algorithm to infer session
automata through membership and equivalence queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6651</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6651</id><created>2014-11-24</created><authors><author><keyname>Soltani</keyname><forenames>Amir Arsalan</forenames></author></authors><title>A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network
  Structure</title><categories>cs.AI stat.ML</categories><comments>This was my Advanced Machine Learning's course project in Spring 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report paper we first present a report of the Advanced Machine
Learning Course Project on the provided data set and then present a novel
heuristic algorithm for exact Bayesian network (BN) structure discovery that
uses decomposable scoring functions. Our algorithm follows a different approach
to solve the problem of BN structure discovery than the previously used methods
such as Dynamic Programming (DP) and Branch and Bound to reduce the search
space and find the global optima space for the problem. The algorithm we
propose has some degree of flexibility that can make it more or less greedy.
The more the algorithm is set to be greedy, the more the speed of the algorithm
will be, and the less optimal the final structure. Our algorithm runs in a much
less time than the previously known methods and guarantees to have an
optimality of close to 99%. Therefore, it sacrifices less than one percent of
score of an optimal structure in order to gain a much lower running time and
make the algorithm feasible for large data sets (we may note that we never used
any toolbox except for result validation)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6660</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6660</id><created>2014-11-24</created><updated>2015-04-19</updated><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Lin</keyname><forenames>Ming</forenames></author><author><keyname>Li</keyname><forenames>Xuanchong</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art action feature extractors involve differential
operators, which act as highpass filters and tend to attenuate low frequency
action information. This attenuation introduces bias to the resulting features
and generates ill-conditioned feature matrices. The Gaussian Pyramid has been
used as a feature enhancing technique that encodes scale-invariant
characteristics into the feature space in an attempt to deal with this
attenuation. However, at the core of the Gaussian Pyramid is a convolutional
smoothing operation, which makes it incapable of generating new features at
coarse scales. In order to address this problem, we propose a novel feature
enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks
features extracted using a family of differential filters parameterized with
multiple time skips and encodes shift-invariance into the frequency space. MIFS
compensates for information lost from using differential operators by
recapturing information at coarse scales. This recaptured information allows us
to match actions at different speeds and ranges of motion. We prove that MIFS
enhances the learnability of differential-based features exponentially. The
resulting feature matrices from MIFS have much smaller conditional numbers and
variances than those from conventional methods. Experimental results show
significantly improved performance on challenging action recognition and event
detection tasks. Specifically, our method exceeds the state-of-the-arts on
Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on
HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup
strategy for feature extraction with minimal or no accuracy cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6663</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6663</id><created>2014-11-24</created><updated>2016-03-07</updated><authors><author><keyname>Hartinger</keyname><forenames>Tatiana Romina</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Partial characterizations of 1-perfectly orientable graphs</title><categories>math.CO cs.DM</categories><msc-class>05C20, 05C75, 05C05, 05C62, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the class of 1-perfectly orientable graphs, that is, graphs having
an orientation in which every out-neighborhood induces a tournament.
1-perfectly orientable graphs form a common generalization of chordal graphs
and circular arc graphs. Even though they can be recognized in polynomial time,
little is known about their structure. In this paper, we develop several
results on 1-perfectly orientable graphs. In particular, we: (i) give a
characterization of 1-perfectly orientable graphs in terms of edge clique
covers, (ii) identify several graph transformations preserving the class of
1-perfectly orientable graphs, (iii) exhibit an infinite family of minimal
forbidden induced minors for the class of 1-perfectly orientable graphs, and
(iv) characterize the class of 1-perfectly orientable graphs within the classes
of cographs and of cobipartite graphs. The class of 1-perfectly orientable
co-bipartite graphs coincides with the class of co-bipartite circular arc
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6667</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6667</id><created>2014-11-24</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Wang</keyname><forenames>Carol</forenames></author></authors><title>Deletion codes in the high-noise and high-rate regimes</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The noise model of deletions poses significant challenges in coding theory,
with basic questions like the capacity of the binary deletion channel still
being open. In this paper, we study the harder model of worst-case deletions,
with a focus on constructing efficiently decodable codes for the two extreme
regimes of high-noise and high-rate. Specifically, we construct polynomial-time
decodable codes with the following trade-offs (for any eps &gt; 0):
  (1) Codes that can correct a fraction 1-eps of deletions with rate poly(eps)
over an alphabet of size poly(1/eps);
  (2) Binary codes of rate 1-O~(sqrt(eps)) that can correct a fraction eps of
deletions; and
  (3) Binary codes that can be list decoded from a fraction (1/2-eps) of
deletions with rate poly(eps)
  Our work is the first to achieve the qualitative goals of correcting a
deletion fraction approaching 1 over bounded alphabets, and correcting a
constant fraction of bit deletions with rate aproaching 1. The above results
bring our understanding of deletion code constructions in these regimes to a
similar level as worst-case errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6672</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6672</id><created>2014-11-24</created><authors><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Ma&#x148;uch</keyname><forenames>J&#xe1;n</forenames></author><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author><author><keyname>Stacho</keyname><forenames>Ladislav</forenames></author></authors><title>Pattern overlap implies runaway growth in hierarchical tile systems</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that in the hierarchical tile assembly model, if there is a
producible assembly that overlaps a nontrivial translation of itself
consistently (i.e., the pattern of tile types in the overlap region is
identical in both translations), then arbitrarily large assemblies are
producible. The significance of this result is that tile systems intended to
controllably produce finite structures must avoid pattern repetition in their
producible assemblies that would lead to such overlap. This answers an open
question of Chen and Doty (SODA 2012), who showed that so-called
&quot;partial-order&quot; systems producing a unique finite assembly *and&quot; avoiding such
overlaps must require time linear in the assembly diameter. An application of
our main result is that any system producing a unique finite assembly is
automatically guaranteed to avoid such overlaps, simplifying the hypothesis of
Chen and Doty's main theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6673</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6673</id><created>2014-11-24</created><updated>2015-03-02</updated><authors><author><keyname>Dixit</keyname><forenames>Kashyap</forenames></author><author><keyname>F&#xfc;rer</keyname><forenames>Martin</forenames></author></authors><title>Counting cliques and clique covers in random graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of counting the number of {\em isomorphic} copies of a
given {\em template} graph, say $H$, in the input {\em base} graph, say $G$. In
general, it is believed that polynomial time algorithms that solve this problem
exactly are unlikely to exist. So, a lot of work has gone into designing
efficient {\em approximation schemes}, especially, when $H$ is a perfect
matching.
  In this work, we present efficient approximation schemes to count
$k$-Cliques, $k$-Independent sets and $k$-Clique covers in random graphs. We
present {\em fully polynomial time randomized approximation schemes} (fpras) to
count $k$-Cliques and $k$-Independent sets in a random graph on $n$ vertices
when $k$ is at most $(1+o(1))\log n$, and $k$-Clique covers when $k$ is a
constant. [Grimmett and McDiarmid, 1975] present a simple greedy algorithm that
{\em detects} a clique (independent set) of size $(1+o(1))\log_2 n$ in $G\in
\mathcal{G}(n,\frac{1}{2})$ with high probability. No algorithm is known to
detect a clique or an independent set of larger size with non-vanishing
probability. Furthermore, [Coja-Oghlan and Efthymiou, 2011] present some
evidence that one cannot hope to easily improve a similar, almost 40 years old
bound for sparse random graphs. Therefore, our results are unlikely to be
easily improved.
  We use a novel approach to obtain a recurrence corresponding to the variance
of each estimator. Then we upper bound the variance using the corresponding
recurrence. This leads us to obtain a polynomial upper bound on the critical
ratio. As an aside, we also obtain an alternate derivation of the closed form
expression for the $k$-th moment of a binomial random variable using our
techniques. The previous derivation [Knoblauch (2008)] was based on the moment
generating function of a binomial random variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6675</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6675</id><created>2014-11-24</created><updated>2014-11-26</updated><authors><author><keyname>Leva</keyname><forenames>Federico</forenames></author></authors><title>From orphan works, a new role of libraries for the public domain and
  public interest (Dalle opere orfane, un nuovo ruolo delle biblioteche per il
  pubblico dominio e l'utilit\`a sociale)</title><categories>cs.DL</categories><comments>17 pages, 7 figures. Text in Italian. Preprint for digitalia.sbn.it.
  Second revision corrected some misunderstandings. ODT, PNG sources. Keywords:
  copyright; public domain; digital libraries; european union; directive
  2012/28/eu; orphan works; hostage works; mass-digitization; Google Books;
  Internet Archive</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Summarising the new orphan works law of Italy, we show how it makes the
public interest prevail and allows libraries and other beneficiaries to improve
their services. We then argue that such services are part of their mission
towards the public domain and are a first step for its complete accomplishment,
by the work of each and a reform of european copyright. Failing that, European
culture will disappear.
  --
  Sintetizzando le nuove norme sulle opere orfane, mostriamo come esse
affermino la prevalenza dell'interesse pubblico e consentano a biblioteche e
altri enti beneficiari di migliorare i propri servizi. Sosteniamo quindi che
questi si inquadrano nella loro missione nei confronti del pubblico dominio e
sono un primo passo per la sua completa realizzazione, mediante il lavoro di
ciascuno e la riforma del diritto d'autore europeo. In caso contrario, la
cultura europea sparir\`a.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6685</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6685</id><created>2014-11-24</created><updated>2015-05-19</updated><authors><author><keyname>Patras</keyname><forenames>Paul</forenames></author><author><keyname>Garcia-Saavedra</keyname><forenames>Andres</forenames></author><author><keyname>Malone</keyname><forenames>David</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Rigorous and Practical Proportional-fair Allocation for Multi-rate Wi-Fi</title><categories>cs.NI</categories><comments>21 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent experimental studies confirm the prevalence of the widely known
performance anomaly problem in current Wi-Fi networks, and report on the severe
network utility degradation caused by this phenomenon. Although a large body of
work addressed this issue, we attribute the refusal of prior solutions to their
poor implementation feasibility with off-the-shelf hardware and their imprecise
modelling of the 802.11 protocol. Their applicability is further challenged
today by very high throughput enhancements (802.11n/ac) whereby link speeds can
vary by two orders of magnitude. Unlike earlier approaches, in this paper we
introduce the first rigorous analytical model of 802.11 stations' throughput
and airtime in multi-rate settings, without sacrificing accuracy for
tractability. We use the proportional-fair allocation criterion to formulate
network utility maximisation as a convex optimisation problem for which we give
a closed-form solution. We present a fully functional light-weight
implementation of our scheme on commodity access points and evaluate this
extensively via experiments in a real deployment, over a broad range of network
conditions. Results demonstrate that our proposal achieves up to 100\% utility
gains, can double video streaming goodput and reduces TCP download times by 8x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6699</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6699</id><created>2014-11-24</created><authors><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>One Vector is Not Enough: Entity-Augmented Distributional Semantics for
  Discourse Relations</title><categories>cs.CL cs.LG</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discourse relations bind smaller linguistic units into coherent texts.
However, automatically identifying discourse relations is difficult, because it
requires understanding the semantics of the linked arguments. A more subtle
challenge is that it is not enough to represent the meaning of each argument of
a discourse relation, because the relation may depend on links between
lower-level components, such as entity mentions. Our solution computes
distributional meaning representations by composition up the syntactic parse
tree. A key difference from previous work on compositional distributional
semantics is that we also compute representations for entity mentions, using a
novel downward compositional pass. Discourse relations are predicted from the
distributional representations of the arguments, and also of their coreferent
entity mentions. The resulting system obtains substantial improvements over the
previous state-of-the-art in predicting implicit discourse relations in the
Penn Discourse Treebank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6704</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6704</id><created>2014-11-24</created><updated>2015-07-13</updated><authors><author><keyname>Chandra</keyname><forenames>Bikash</forenames></author><author><keyname>Chawda</keyname><forenames>Bhupesh</forenames></author><author><keyname>Kar</keyname><forenames>Biplab</forenames></author><author><keyname>Reddy</keyname><forenames>K. V. Maheshwara</forenames></author><author><keyname>Shah</keyname><forenames>Shetal</forenames></author><author><keyname>Sudarshan</keyname><forenames>S.</forenames></author></authors><title>Data Generation for Testing and Grading SQL Queries</title><categories>cs.DB</categories><comments>34 pages, The final publication is available at Springer via
  http://dx.doi.org/10.1007/s00778-015-0395-0</comments><acm-class>H.2</acm-class><doi>10.1007/s00778-015-0395-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correctness of SQL queries is usually tested by executing the queries on one
or more datasets. Erroneous queries are often the results of small changes, or
mutations of the correct query. A mutation Q' of a query Q is killed by a
dataset D if Q(D) $\neq$ Q'(D). Earlier work on the XData system showed how to
generate datasets that kill all mutations in a class of mutations that included
join type and comparison operation mutations.
  In this paper, we extend the XData data generation techniques to handle a
wider variety of SQL queries and a much larger class of mutations. We have also
built a system for grading SQL queries using the datasets generated by XData.
We present a study of the effectiveness of the datasets generated by the
extended XData approach, using a variety of queries including queries submitted
by students as part of a database course. We show that the XData datasets
outperform predefined datasets as well as manual grading done earlier by
teaching assistants, while also avoiding the drudgery of manual correction.
Thus, we believe that our techniques will be of great value to database course
instructors and TAs, particularly to those of MOOCs. It will also be valuable
to database application developers and testers for testing SQL queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6712</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6712</id><created>2014-11-24</created><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author></authors><title>The square root rank of the correlation polytope is exponential</title><categories>cs.CC math.CO math.OC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The square root rank of a nonnegative matrix $A$ is the minimum rank of a
matrix $B$ such that $A=B \circ B$, where $\circ$ denotes entrywise product. We
show that the square root rank of the slack matrix of the correlation polytope
is exponential. Our main technique is a way to lower bound the rank of certain
matrices under arbitrary sign changes of the entries using properties of the
roots of polynomials in number fields. The square root rank is an upper bound
on the positive semidefinite rank of a matrix, and corresponds the special case
where all matrices in the factorization are rank-one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6714</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6714</id><created>2014-11-24</created><authors><author><keyname>Spratt</keyname><forenames>Emily L.</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>The Digital Humanities Unveiled: Perceptions Held by Art Historians and
  Computer Scientists about Computer Vision Technology</title><categories>cs.CY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.2488</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although computer scientists are generally familiar with the achievements of
computer vision technology in art history, these accomplishments are little
known and often misunderstood by scholars in the humanities. To clarify the
parameters of this seeming disjuncture, we have addressed the concerns that one
example of the digitization of the humanities poses on social, philosophical,
and practical levels. In support of our assessment of the perceptions held by
computer scientists and art historians about the use of computer vision
technology to examine art, we based our interpretations on two surveys that
were distributed in August 2014. In this paper, the development of these
surveys and their results are discussed in the context of the major
philosophical conclusions of our research in this area to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6718</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6718</id><created>2014-11-24</created><updated>2015-05-03</updated><authors><author><keyname>Nabil</keyname><forenames>Mahmoud</forenames></author><author><keyname>Aly</keyname><forenames>Mohamed</forenames></author><author><keyname>Atiya</keyname><forenames>Amir</forenames></author></authors><title>LABR: A Large Scale Arabic Sentiment Analysis Benchmark</title><categories>cs.CL cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce LABR, the largest sentiment analysis dataset to-date for the
Arabic language. It consists of over 63,000 book reviews, each rated on a scale
of 1 to 5 stars. We investigate the properties of the dataset, and present its
statistics. We explore using the dataset for two tasks: (1) sentiment polarity
classification; and (2) ratings classification. Moreover, we provide standard
splits of the dataset into training, validation and testing, for both polarity
and ratings classification, in both balanced and unbalanced settings. We extend
our previous work by performing a comprehensive analysis on the dataset. In
particular, we perform an extended survey of the different classifiers
typically used for the sentiment polarity classification problem. We also
construct a sentiment lexicon from the dataset that contains both single and
compound sentiment words and we explore its effectiveness. We make the dataset
and experimental details publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6719</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6719</id><created>2014-11-24</created><updated>2015-01-25</updated><authors><author><keyname>Kalogerias</keyname><forenames>Dionysios S.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>Asymptotically Optimal Discrete Time Nonlinear Filters From
  Stochastically Convergent State Process Approximations</title><categories>math.ST cs.SY math.OC stat.AP stat.ME stat.TH</categories><comments>EXTENDED version of an original paper submitted to the IEEE
  Transactions on Signal Processing; (nearly) 32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating optimal in the MMSE sense nonlinear
filters in a discrete time setting, exploiting properties of appropriately
stochastically convergent state process approximations. More specifically, we
consider a class of nonlinear, partially observable stochastic systems,
comprised by a (possibly nonstationary) hidden stochastic process (the state),
observed through another conditionally Gaussian stochastic process (the
observations). Under general assumptions, we show that given an approximating
process which, for each time step, is stochastically convergent to the state
process in some appropriate sense, an approximate filtering operator can be
defined, which converges to the true optimal nonlinear filter of the state in a
strong and well defined sense, i.e., compactly in time and uniformly in a
measurable set of probability measure almost unity. The results presented in
this paper can form a common basis for the analysis and characterization of a
number of heuristic approaches for approximating a large class of optimal
nonlinear filters, including approximate grid based techniques, known to
perform well in a variety of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6721</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6721</id><created>2014-11-24</created><authors><author><keyname>Solanas</keyname><forenames>Marc</forenames></author><author><keyname>Hernandez-Castro</keyname><forenames>Julio</forenames></author><author><keyname>Dutta</keyname><forenames>Debojyoti</forenames></author></authors><title>Detecting fraudulent activity in a cloud using privacy-friendly data
  aggregates</title><categories>cs.CR cs.AI cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  More users and companies make use of cloud services every day. They all
expect a perfect performance and any issue to remain transparent to them. This
last statement is very challenging to perform. A user's activities in our cloud
can affect the overall performance of our servers, having an impact on other
resources. We can consider these kind of activities as fraudulent. They can be
either illegal activities, such as launching a DDoS attack or just activities
which are undesired by the cloud provider, such as Bitcoin mining, which uses
substantial power, reduces the life of the hardware and can possibly slow down
other user's activities. This article discusses a method to detect such
activities by using non-intrusive, privacy-friendly data: billing data. We use
OpenStack as an example with data provided by Telemetry, the component in
charge of measuring resource usage for billing purposes. Results will be shown
proving the efficiency of this method and ways to improve it will be provided
as well as its advantages and disadvantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6725</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6725</id><created>2014-11-24</created><authors><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Haffner</keyname><forenames>Patrick</forenames></author><author><keyname>Paiement</keyname><forenames>Jean-Francois</forenames></author></authors><title>Accelerated Parallel Optimization Methods for Large Scale Machine
  Learning</title><categories>cs.LG</categories><comments>Appear in the 7th NIPS Workshop on Optimization for Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing amount of high dimensional data in different machine learning
applications requires more efficient and scalable optimization algorithms. In
this work, we consider combining two techniques, parallelism and Nesterov's
acceleration, to design faster algorithms for L1-regularized loss. We first
simplify BOOM, a variant of gradient descent, and study it in a unified
framework, which allows us to not only propose a refined measurement of
sparsity to improve BOOM, but also show that BOOM is provably slower than
FISTA. Moving on to parallel coordinate descent methods, we then propose an
efficient accelerated version of Shotgun, improving the convergence rate from
$O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysis
compared to previous work, and also allows one to study several connected work
in a unified way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6727</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6727</id><created>2014-11-24</created><updated>2015-01-23</updated><authors><author><keyname>G&#x105;gol</keyname><forenames>Adam</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Graph sharing game and the structure of weighted graphs with a forbidden
  subdivision</title><categories>math.CO cs.DM</categories><comments>Minor changes</comments><msc-class>05C57, 05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph sharing game is played by two players, Alice and Bob, on a
connected graph $G$ with non-negative weights assigned to the vertices.
Starting with Alice, the players take the vertices of $G$ one by one, in each
move keeping the set of all taken vertices connected, until the whole $G$ has
been taken. Each player wants to maximize the total weight of the vertices they
have gathered.
  It is proved that for any class $\mathcal{G}$ of graphs with an odd number of
vertices and with forbidden subdivision of a fixed graph, there is a constant
$c_{\mathcal{G}}&gt;0$ such that Alice can guarantee herself at least a
$c_{\mathcal{G}}$-fraction of the total weight of $G$ whenever
$G\in\mathcal{G}$. Known examples show that this is no longer true if any of
the two conditions on the class $\mathcal{G}$ (an odd number of vertices or a
forbidden subdivision) is dropped. The main ingredient in the proof is a new
structural result on weighted graphs with a forbidden subdivision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6739</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6739</id><created>2014-11-25</created><authors><author><keyname>Alshamary</keyname><forenames>Haider Ali Jasim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq</forenames></author><author><keyname>Zaib</keyname><forenames>Alam</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Optimal non-coherent data detection for massive SIMO wireless systems: A
  polynomial complexity solution</title><categories>cs.IT math.IT</categories><comments>7pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO systems have made significant progress in increasing spectral
and energy efficiency over traditional MIMO systems by exploiting large antenna
arrays. In this paper we consider the joint maximum likelihood (ML) channel
estimation and data detection problem for massive SIMO (single input multiple
output) wireless systems. Despite the large number of unknown channel
coefficients for massive SIMO systems, we improve an algorithm to achieve the
exact ML non-coherent data detection with a low expected complexity. We show
that the expected computational complexity of this algorithm is linear in the
number of receive antennas and polynomial in channel coherence time. Simulation
results show the performance gain of the optimal non-coherent data detection
with a low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6741</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6741</id><created>2014-11-25</created><authors><author><keyname>Ahuja</keyname><forenames>Chaitanya</forenames></author><author><keyname>Nathwani</keyname><forenames>Karan</forenames></author><author><keyname>Hegde</keyname><forenames>Rajesh M.</forenames></author></authors><title>A Complex Matrix Factorization approach to Joint Modeling of Magnitude
  and Phase for Source Separation</title><categories>cs.SD</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional NMF methods for source separation factorize the matrix of
spectral magnitudes. Spectral Phase is not included in the decomposition
process of these methods. However, phase of the speech mixture is generally
used in reconstructing the target speech signal. This results in undesired
traces of interfering sources in the target signal. In this paper the spectral
phase is incorporated in the decomposition process itself. Additionally, the
complex matrix factorization problem is reduced to an NMF problem using simple
transformations. This results in effective separation of speech mixtures since
both magnitude and phase are utilized jointly in the separation process.
Improvement in source separation results are demonstrated using objective
quality evaluations on the GRID corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6749</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6749</id><created>2014-11-25</created><authors><author><keyname>Luo</keyname><forenames>Tie</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author><author><keyname>Srinivasan</keyname><forenames>Vikram</forenames></author></authors><title>Analyzing DISH for Multi-Channel MAC Protocols in Wireless Networks</title><categories>cs.NI cs.PF</categories><comments>Multi-channel multi-hop networks, availability of cooperation,
  cooperative protocol, distributed information sharing, ACM MobiHoc, May 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For long, node cooperation has been exploited as a data relaying mechanism.
However, the wireless channel allows for much richer interaction between nodes.
One such scenario is in a multi-channel environment, where transmitter-receiver
pairs may make incorrect decisions (e.g., in selecting channels) but idle
neighbors could help by sharing information to prevent undesirable consequences
(e.g., data collisions). This represents a Distributed Information SHaring
(DISH) mechanism for cooperation and suggests new ways of designing cooperative
protocols. However, what is lacking is a theoretical understanding of this new
notion of cooperation. In this paper, we view cooperation as a network resource
and evaluate the availability of cooperation via a metric, $p_{co}$, the
probability of obtaining cooperation. First, we analytically evaluate $p_{co}$
in the context of multi-channel multi-hop wireless networks. Second, we verify
our analysis via simulations and the results show that our analysis accurately
characterizes the behavior of $p_{co}$ as a function of underlying network
parameters. This step also yields important insights into DISH with respect to
network dynamics. Third, we investigate the correlation between $p_{co}$ and
network performance in terms of collision rate, packet delay, and throughput.
The results indicate a near-linear relationship, which may significantly
simplify performance analysis for cooperative networks and suggests that
$p_{co}$ be used as an appropriate performance indicator itself. Throughout
this work, we utilize, as appropriate, three different DISH contexts ---
model-based DISH, ideal DISH, and real DISH --- to explore $p_{co}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6753</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6753</id><created>2014-11-25</created><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Chana</keyname><forenames>Inderveer</forenames></author></authors><title>Metrics based Workload Analysis Technique for IaaS Cloud</title><categories>cs.DC</categories><comments>Including 5 Tables and 3 Figures, Presented in the International
  Conference on Next Generation Computing and Communication Technologies
  (ICNGCCT 2014), Dubai, UAE on 23-24 April, 2014</comments><report-no>CCT0036</report-no><msc-class>97P70</msc-class><acm-class>J.7</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Dynamic Scalability of resources, a problem in Infrastructure as a
Service (IaaS) has been the hotspot for research and industry communities. The
heterogeneous and dynamic nature of the Cloud workloads depends on the Quality
of Service (QoS) allocation of appropriate workloads to appropriate resources.
A workload is an abstraction of work that instance or set of instances that are
going to perform. Running a web service or being a Hadoop data node is valid
workloads. The efficient management of dynamic nature resources can be done
with the help of workloads. Until workload is considered a fundamental
capability, the Cloud resources cannot be utilized in an efficient manner. In
this paper, different workloads have been identified and categorized along with
their characteristics and constraints. The metrics based on Quality of Service
(QoS) requirements have been identified for each workload and have been
analyzed for creating better application design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6754</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6754</id><created>2014-11-25</created><authors><author><keyname>Hu</keyname><forenames>Xiaosong</forenames></author><author><keyname>Zhu</keyname><forenames>Wen</forenames></author><author><keyname>Li</keyname><forenames>Qing</forenames></author></authors><title>HCRS: A hybrid clothes recommender system based on user ratings and
  product features</title><categories>cs.AI cs.IR</categories><comments>ICMECG '13 Proceedings of the 2013 International Conference on
  Management of e-Commerce and e-Government Pages 270-274</comments><doi>10.1109/ICMeCG.2013.60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, online clothes-selling business has become popular and extremely
attractive because of its convenience and cheap-and-fine price. Good examples
of these successful Web sites include Yintai.com, Vancl.com and
Shop.vipshop.com which provide thousands of clothes for online shoppers. The
challenge for online shoppers lies on how to find a good product from lots of
options. In this article, we propose a collaborative clothes recommender for
easy shopping. One of the unique features of this system is the ability to
recommend clothes in terms of both user ratings and clothing attributes.
Experiments in our simulation environment show that the proposed recommender
can better satisfy the needs of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6756</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6756</id><created>2014-11-25</created><updated>2015-04-24</updated><authors><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Michal</forenames></author></authors><title>Fast Algorithms for Parameterized Problems with Relaxed Disjointness
  Constraints</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In parameterized complexity, it is a natural idea to consider different
generalizations of classic problems. Usually, such generalization are obtained
by introducing a &quot;relaxation&quot; variable, where the original problem corresponds
to setting this variable to a constant value. For instance, the problem of
packing sets of size at most $p$ into a given universe generalizes the Maximum
Matching problem, which is recovered by taking $p=2$. Most often, the
complexity of the problem increases with the relaxation variable, but very
recently Abasi et al. have given a surprising example of a problem ---
$r$-Simple $k$-Path --- that can be solved by a randomized algorithm with
running time $O^*(2^{O(k \frac{\log r}{r})})$. That is, the complexity of the
problem decreases with $r$. In this paper we pursue further the direction
sketched by Abasi et al. Our main contribution is a derandomization tool that
provides a deterministic counterpart of the main technical result of Abasi et
al.: the $O^*(2^{O(k \frac{\log r}{r})})$ algorithm for $(r,k)$-Monomial
Detection, which is the problem of finding a monomial of total degree $k$ and
individual degrees at most $r$ in a polynomial given as an arithmetic circuit.
Our technique works for a large class of circuits, and in particular it can be
used to derandomize the result of Abasi et al. for $r$-Simple $k$-Path. On our
way to this result we introduce the notion of representative sets for
multisets, which may be of independent interest. Finally, we give two more
examples of problems that were already studied in the literature, where the
same relaxation phenomenon happens. The first one is a natural relaxation of
the Set Packing problem, where we allow the packed sets to overlap at each
element at most $r$ times. The second one is Degree Bounded Spanning Tree,
where we seek for a spanning tree of the graph with a small maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6757</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6757</id><created>2014-11-25</created><updated>2016-02-22</updated><authors><author><keyname>Mayer</keyname><forenames>Norbert Michael</forenames></author></authors><title>Echo State Condition at the Critical Point</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent networks that have transfer functions that fulfill the Lipschitz
continuity with L=1, may be echo state networks if certain limitations on the
recurrent connectivity are applied. Initially it has been shown that it is
sufficient if the largest singular value of the recurrent connectivity S is
smaller than 1. The main achievement of this paper is a proof under which
conditions the network is an echo state network even if S=1. It turns out that
in this critical case the exact shape of the transfer function plays a decisive
role whether or not the network still fulfills the echo state condition. In
addition, several intuitive examples with one neuron networks are outlined to
illustrate effects of critical connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6758</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6758</id><created>2014-11-25</created><updated>2014-11-27</updated><authors><author><keyname>Dattani</keyname><forenames>Nikesh S.</forenames><affiliation>Kyoto University, Oxford University</affiliation></author><author><keyname>Bryans</keyname><forenames>Nathaniel</forenames><affiliation>University of Calgary</affiliation></author></authors><title>Quantum factorization of 56153 with only 4 qubits</title><categories>quant-ph cs.DM</categories><comments>Replaced 44929 with larger number (56153) that results in same
  Hamiltonian as 143, edited corresponding table and equations. Similarly
  replaced 13081 with 11663. Fixed typo in equations 14, 15, 19-21</comments><msc-class>05C50, 11A41, 11A51, 11N35, 11N36, 11N80, 11Y05, 65K10, 65P10,
  65Y20, 68Q12, 81P68, 81P94, 94A60, 81-08</msc-class><acm-class>B.2.4; B.8.2; C.1.3; C.1.m; F.2.1; F.2.3; F.4.1; G.1.0; G.1.3;
  G.1.5; G.1.6; G.2.0; G.2.1; I.1.2; I.6.4; C.4; E.3; G.0; J.2; K.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The largest number factored on a quantum device reported until now was 143.
That quantum computation, which used only 4 qubits at 300K, actually also
factored much larger numbers such as 3599, 11663, and 56153, without the
awareness of the authors of that work. Furthermore, unlike the implementations
of Shor's algorithm performed thus far, these 4-qubit factorizations do not
need to use prior knowledge of the answer. However, because they only use 4
qubits, these factorizations can also be performed trivially on classical
computers. We discover a class of numbers for which the power of quantum
information actually comes into play. We then demonstrate a 3-qubit
factorization of 175, which would be the first quantum factorization of a
triprime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6762</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6762</id><created>2014-11-25</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Jariwala</keyname><forenames>Juhi</forenames></author><author><keyname>Innani</keyname><forenames>Payal</forenames></author><author><keyname>Bablani</keyname><forenames>Sheetal</forenames></author><author><keyname>Kothawale</keyname><forenames>Sushama</forenames></author></authors><title>A Tool to Automate the Sizing of Application Process for SOA based
  Platform</title><categories>cs.SE</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service Oriented Architecture is a loosely coupled architecture designed to
tackle the problem of Business Infrastructure alignment to meet the needs of an
organization. A SOA based platform enables the enterprises to develop
applications in the form of independent services. To provide scalable service
interactions, there is a need to maintain services performance and have a good
sizing guideline of the underlying software platform. Sizing aids in finding
the optimum resources required to configure and implement a system that would
satisfy the requirements of Business Process Integration being planned. A web
based Sizing Tool prototype is developed using Java Application Programming
Interfaces to automate the process of sizing the applications deployed on SOA
platform that not only scales the performance of the system but also predicts
its business growth in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6763</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6763</id><created>2014-11-25</created><updated>2016-02-26</updated><authors><author><keyname>Peng</keyname><forenames>Peng</forenames></author><author><keyname>Zou</keyname><forenames>Lei</forenames></author><author><keyname>&#xd6;zsu</keyname><forenames>M. Tamer</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Dongyan</forenames></author></authors><title>Processing SPARQL Queries Over Distributed RDF Graphs</title><categories>cs.DB cs.DC</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose techniques for processing SPARQL queries over a large RDF graph in
a distributed environment. We adopt a &quot;partial evaluation and assembly&quot;
framework. Answering a SPARQL query Q is equivalent to finding subgraph matches
of the query graph Q over RDF graph G. Based on properties of subgraph matching
over a distributed graph, we introduce local partial match as partial answers
in each fragment of RDF graph G. For assembly, we propose two methods:
centralized and distributed assembly. We analyze our algorithms from both
theoretically and experimentally. Extensive experiments over both real and
benchmark RDF repositories of billions of triples confirm that our method is
superior to the state-of-the-art methods in both the system's performance and
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6767</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6767</id><created>2014-11-25</created><authors><author><keyname>Waghmare</keyname><forenames>Vijayata</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Mobile Agent based Market Basket Analysis on Cloud</title><categories>cs.CY cs.DC</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design and development of a location-based mobile
shopping application for bakery product shops. Whole application is deployed on
cloud. The three-tier architecture consists of, front-end, middle-ware and
back-end. The front-end level is a location-based mobile shopping application
for android mobile devices, for purchasing bakery products of nearby places.
Front-end level also displays association among the purchased products. The
middle-ware level provides a web service to generate JSON (JavaScript Object
Notation) output from the relational database. It exchanges information and
data between mobile application and servers in cloud. The back-end level
provides the Apache Tomcat Web server and MySQL database. The application also
uses the Google Cloud Messaging for generating and sending notification of
orders to shopkeeper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6768</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6768</id><created>2014-11-25</created><authors><author><keyname>Parzhin</keyname><forenames>Yuri</forenames></author></authors><title>Hypotheses of neural code and the information model of the
  neuron-detector</title><categories>cs.NE cs.AI q-bio.NC</categories><comments>15 pages, 5 figures</comments><acm-class>I.2.0; I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of neural code solving. On the basis of the
formulated hypotheses the information model of a neuron-detector is suggested,
the detector being one of the basic elements of an artificial neural network
(ANN). The paper subjects the connectionist paradigm of ANN building to
criticism and suggests a new presentation paradigm for ANN building and
neuroelements (NE) learning. The adequacy of the suggested model is proved by
the fact that is does not contradict the modern propositions of neuropsychology
and neurophysiology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6771</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6771</id><created>2014-11-25</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Shirwadkar</keyname><forenames>Ashay</forenames></author><author><keyname>Gaikar</keyname><forenames>Pratik</forenames></author><author><keyname>Agrawal</keyname><forenames>Tanmay</forenames></author></authors><title>Securing the Data in Clouds with Hyperelliptic Curve Cryptography</title><categories>cs.CR</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays world, Cloud computing has attracted research communities as it
provides services in reduced cost due to virtualizing all the necessary
resources. Even modern business architecture depends upon Cloud computing .As
it is a internet based utility, which provides various services over a network,
it is prone to network based attacks. Hence security in clouds is the most
important in case of cloud computing. Cloud Security concerns the customer to
fully rely on storing data on clouds. That is why Cloud security has attracted
attention of the research community. This paper will discuss securing the data
in clouds by implementing key agreement, encryption and signature
verification/generation with hyperelliptic curve cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6773</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6773</id><created>2014-11-25</created><authors><author><keyname>Bijral</keyname><forenames>Simran</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Efficient Fuzzy Search Engine with B-Tree Search Mechanism</title><categories>cs.IR</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines play a vital role in day to day life on internet. People use
search engines to find content on internet. Cloud computing is the computing
concept in which data is stored and accessed with the help of a third party
server called as cloud. Data is not stored locally on our machines and the
softwares and information are provided to user if user demands for it. Search
queries are the most important part in searching data on internet. A search
query consists of one or more than one keywords. A search query is searched
from the database for exact match, and the traditional searchable schemes do
not tolerate minor typos and format inconsistencies, which happen quite
frequently. This drawback makes the existing techniques unsuitable and they
offer very low efficiency. In this paper, we will for the first time formulate
the problem of effective fuzzy search by introducing tree search methodologies.
We will explore the benefits of B trees in search mechanism and use them to
have an efficient keyword search. We have taken into consideration the security
analysis strictly so as to get a secure and privacy-preserving system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6775</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6775</id><created>2014-11-25</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Agrawal</keyname><forenames>Chetan</forenames></author><author><keyname>Maru</keyname><forenames>Devesh</forenames></author><author><keyname>Yedale</keyname><forenames>Pooja</forenames></author><author><keyname>Gadekar</keyname><forenames>Pranav</forenames></author></authors><title>Addressing NameNode Scalability Issue in Hadoop Distributed File System
  using Cache Approach</title><categories>cs.DC</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadoop is a distributed batch processing infrastructure which is currently
being used for big data management. The foundation of Hadoop consists of Hadoop
Distributed File System or HDFS. HDFS presents a client server architecture
comprised of a NameNode and many DataNodes. The NameNode stores the metadata
for the DataNodes and DataNode stores application data. The NameNode holds file
system metadata in memory, and thus the limit to the number of files in a file
system is governed by the amount of memory on the NameNode. Thus when the
memory on NameNode is full there is no further chance of increasing the cluster
capacity. In this paper we have used the concept of cache memory for handling
the issue of NameNode scalability. The focus of this paper is to highlight our
approach that tries to enhance the current architecture and ensure that
NameNode does not reach its threshold value soon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6777</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6777</id><created>2014-11-25</created><authors><author><keyname>Lahoti</keyname><forenames>Laxmi</forenames></author><author><keyname>Chandankhede</keyname><forenames>Chaitali</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Modified Apriori Approach for Evade Network Intrusion Detection System</title><categories>cs.CR</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion Detection System or IDS is a software or hardware tool that
repeatedly scans and monitors events that took place in a computer or a
network. A set of rules are used by Signature based Network Intrusion Detection
Systems or NIDS to detect hostile traffic in network segments or packets, which
are so important in detecting malicious and anomalous behaviour over the
network like known attacks that hackers look for new techniques to go unseen.
Sometime, a single failure at any layer will cause the NIDS to miss that
attack. To overcome this problem, a technique is used that will trigger a
failure in that layer. Such technique is known as Evasive technique. An Evasion
can be defined as any technique that modifies a visible attack into any other
form in order to stay away from being detect. The proposed system is used for
detecting attacks which are going on the network and also gives actual
categorization of attacks. The proposed system has advantage of getting low
false alarm rate and high detection rate. So that leads into decrease in
complexity and overhead on the system. The paper presents the Evasion technique
for customized apriori algorithm. The paper aims to make a new functional
structure to evade NIDS. This framework can be used to audit NIDS. This
framework shows that a proof of concept showing how to evade a self built NIDS
considering two publicly available datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6784</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6784</id><created>2014-11-25</created><authors><author><keyname>Cheng</keyname><forenames>Minquan</forenames></author><author><keyname>Fu</keyname><forenames>Hung-Lin</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Lo</keyname><forenames>Yuan-Hsun</forenames></author><author><keyname>Miao</keyname><forenames>Ying</forenames></author></authors><title>Codes with the Identifiable Parent Property for Multimedia
  Fingerprinting</title><categories>cs.IT math.IT</categories><comments>7 pages, submitted to IEEE transction on information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let ${\cal C}$ be a $q$-ary code of length $n$ and size $M$, and ${\cal C}(i)
= \{{\bf c}(i) \ | \ {\bf c}=({\bf c}(1), {\bf c}(2), \ldots, {\bf c}(n))^{T}
\in {\cal C}\}$ be the set of $i$th coordinates of ${\cal C}$. The descendant
code of a sub-code ${\cal C}^{'} \subseteq {\cal C}$ is defined to be ${\cal
C}^{'}(1) \times {\cal C}^{'}(2) \times \cdots \times {\cal C}^{'}(n)$. In this
paper, we introduce a multimedia analogue of codes with the identifiable parent
property (IPP), called multimedia IPP codes or $t$-MIPPC$(n, M, q)$, so that
given the descendant code of any sub-code ${\cal C}^{'}$ of a multimedia
$t$-IPP code ${\cal C}$, one can always identify, as IPP codes do in the
generic digital scenario, at least one codeword in ${\cal C}^{'}$. We first
derive a general upper bound on the size $M$ of a multimedia $t$-IPP code, and
then investigate multimedia $3$-IPP codes in more detail. We characterize a
multimedia $3$-IPP code of length $2$ in terms of a bipartite graph and a
generalized packing, respectively. By means of these combinatorial
characterizations, we further derive a tight upper bound on the size of a
multimedia $3$-IPP code of length $2$, and construct several infinite families
of (asymptotically) optimal multimedia $3$-IPP codes of length $2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6791</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6791</id><created>2014-11-25</created><authors><author><keyname>Luo</keyname><forenames>Tie</forenames></author><author><keyname>Srinivasan</keyname><forenames>Vikram</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>A Metric for DISH Networks: Analysis, Implications, and Applications</title><categories>cs.NI cs.PF</categories><comments>Cooperative protocol, availability of cooperation, multi-channel
  multi-hop wireless network, multi-channel MAC protocols, distributed
  information sharing (DISH), channel bandwidth allocation</comments><journal-ref>IEEE Transactions on Mobile Computing, vol. 9, no. 3, pp.
  376--389, March 2010</journal-ref><doi>10.1109/TMC.2009.138</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks, node cooperation has been exploited as a data relaying
mechanism for decades. However, the wireless channel allows for much richer
interaction among nodes. In particular, Distributed Information SHaring (DISH)
represents a new improvement to multi-channel MAC protocol design by using a
cooperative element at the control plane. In this approach, nodes exchange
control information to make up for other nodes' insufficient knowledge about
the environment, and thereby aid in their decision making. To date, what is
lacking is a theoretical understanding of DISH. In this paper, we view
cooperation as a network resource and evaluate the availability of cooperation,
$p_{co}$. We first analyze $p_{co}$ in the context of a multi-channel multi-hop
wireless network, and then perform simulations which show that the analysis
accurately characterizes $p_{co}$ as a function of underlying network
parameters. Next, we investigate the correlation between $p_{co}$ and network
metrics such as collision rate, packet delay, and throughput. We find a
near-linear relationship between $p_{co}$ and the metrics, which suggests that
$p_{co}$ can be used as an appropriate performance indicator itself. Finally,
we apply our analysis to solving a channel bandwidth allocation problem, where
we derive optimal schemes and provide general guidelines on bandwidth
allocation for DISH networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6792</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6792</id><created>2014-11-25</created><authors><author><keyname>Terekhov</keyname><forenames>I. S.</forenames></author><author><keyname>Vergeles</keyname><forenames>S. S.</forenames></author><author><keyname>Turitsyn</keyname><forenames>S. K.</forenames></author></authors><title>Conditional probability calculations for the nonlinear Schr\&quot;odinger
  equation with additive noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method for computation of conditional probability density function for
the nonlinear Schr\&quot;odinger equation with additive noise is developed. We
present in a constructive form the conditional probability density function in
the limit of a small noise and analytically derive it in a weakly nonlinear
case. The general theory results are illustrated using fibre-optic
communications as a particular, albeit practically very important, example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6794</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6794</id><created>2014-11-25</created><authors><author><keyname>Pereira-Fari&#xf1;a</keyname><forenames>M.</forenames></author></authors><title>Some Reflections on the Set-based and the Conditional-based
  Interpretations of Statements in Syllogistic Reasoning</title><categories>cs.AI cs.LO</categories><comments>16 pages, Artificial Intelligence</comments><journal-ref>Archives for the Philosophy and History of Soft Computing 1, 2014,
  pp. 1-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two interpretations about syllogistic statements are described in this paper.
One is the so-called set-based interpretation, which assumes that quantified
statements and syllogisms talk about quantity-relationships between sets. The
other one, the so-called conditional interpretation, assumes that quantified
propositions talk about conditional propositions and how strong are the links
between the antecedent and the consequent. Both interpretations are compared
attending to three different questions (existential import, singular statements
and non-proportional quantifiers) from the point of view of their impact on the
further development of this type of reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6804</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6804</id><created>2014-11-25</created><authors><author><keyname>Huber</keyname><forenames>Katharina</forenames></author><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Moulton</keyname><forenames>Vincent</forenames></author><author><keyname>Scornavacca</keyname><forenames>Celine</forenames></author><author><keyname>Wu</keyname><forenames>Taoyang</forenames></author></authors><title>Reconstructing phylogenetic level-1 networks from nondense binet and
  trinet sets</title><categories>cs.DS q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binets and trinets are phylogenetic networks with two and three leaves,
respectively. Here we consider the problem of deciding if there exists a binary
level-1 phylogenetic network displaying a given set $\mathcal{T}$ of binary
binets or trinets over a set $X$ of taxa, and constructing such a network
whenever it exists. We show that this is NP-hard for trinets but
polynomial-time solvable for binets. Moreover, we show that the problem is
still polynomial-time solvable for inputs consisting of binets and trinets as
long as the cycles in the trinets have size three. Finally, we present an
$O(3^{|X|} poly(|X|))$ time algorithm for general sets of binets and trinets.
The latter two algorithms generalise to instances containing level-1 networks
with arbitrarily many leaves, and thus provide some of the first supernetwork
algorithms for computing networks from a set of rooted phylogenetic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6810</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6810</id><created>2014-11-25</created><authors><author><keyname>Jang</keyname><forenames>Dae-Sung</forenames></author><author><keyname>Choi</keyname><forenames>Han-Lim</forenames></author></authors><title>Discretization of Planar Geometric Cover Problems</title><categories>cs.CG cs.DM</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider discretization of the 'geometric cover problem' in the plane:
Given a set $P$ of $n$ points in the plane and a compact planar object $T_0$,
find a minimum cardinality collection of planar translates of $T_0$ such that
the union of the translates in the collection contains all the points in $P$.
We show that the geometric cover problem can be converted to a form of the
geometric set cover, which has a given finite-size collection of translates
rather than the infinite continuous solution space of the former. We propose a
reduced finite solution space that consists of distinct canonical translates
and present polynomial algorithms to find the reduce solution space for disks,
convex/non-convex polygons (including holes), and planar objects consisting of
finite Jordan curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6818</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6818</id><created>2014-11-25</created><authors><author><keyname>Cseh</keyname><forenames>&#xc1;gnes</forenames></author><author><keyname>Dean</keyname><forenames>Brian C.</forenames></author></authors><title>Improved Algorithmic Results for Unsplittable Stable Allocation Problems</title><categories>cs.DS cs.DM</categories><comments>15 pages</comments><msc-class>05C85</msc-class><acm-class>G.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable allocation problem is a many-to-many generalization of the
well-known stable marriage problem, where we seek a bipartite assignment
between, say, jobs (of varying sizes) and machines (of varying capacities) that
is &quot;stable&quot; based on a set of underlying preference lists submitted by the jobs
and machines. We study a natural &quot;unsplittable&quot; variant of this problem, where
each assigned job must be fully assigned to a single machine. Such unsplittable
bipartite assignment problems generally tend to be NP-hard, including
previously-proposed variants of the unsplittable stable allocation problem. Our
main result is to show that under an alternative model of stability, the
unsplittable stable allocation problem becomes solvable in polynomial time;
although this model is less likely to admit feasible solutions than the model
proposed iby McDermid and Manlove, we show that in the event there is no
feasible solution, our approach computes a solution of minimal total congestion
(overfilling of all machines collectively beyond their capacities). We also
describe a technique for rounding the solution of a stable allocation problem
to produce &quot;relaxed&quot; unsplit solutions that are only mildly infeasible, where
each machine is overcongested by at most a single job.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6829</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6829</id><created>2014-11-25</created><updated>2015-04-21</updated><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Gysel</keyname><forenames>Rob</forenames></author><author><keyname>Lapinskas</keyname><forenames>John</forenames></author></authors><title>Approximately counting locally-optimal structures</title><categories>cs.CC</categories><comments>Accepted to ICALP 2015 (Track A)</comments><msc-class>03D15, 05C40, 05C69, 68Q17, 68R10</msc-class><acm-class>F.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A locally-optimal structure is a combinatorial structure such as a maximal
independent set that cannot be improved by certain (greedy) local moves, even
though it may not be globally optimal. It is trivial to construct an
independent set in a graph. It is easy to (greedily) construct a maximal
independent set. However, it is NP-hard to construct a globally-optimal
(maximum) independent set. In general, constructing a locally-optimal structure
is somewhat more difficult than constructing an arbitrary structure, and
constructing a globally-optimal structure is more difficult than constructing a
locally-optimal structure. The same situation arises with listing. The
differences between the problems become obscured when we move from listing to
counting because nearly everything is #P-complete. However, we highlight an
interesting phenomenon that arises in approximate counting, where the situation
is apparently reversed. Specifically, we show that counting maximal independent
sets is complete for #P with respect to approximation-preserving reductions,
whereas counting all independent sets, or counting maximum independent sets is
complete for an apparently smaller class, $\mathrm{\#RH}\Pi_1$ which has a
prominent role in the complexity of approximate counting. Motivated by the
difficulty of approximately counting maximal independent sets in bipartite
graphs, we also study the problem of approximately counting other
locally-optimal structures that arise in algorithmic applications, particularly
problems involving minimal separators and minimal edge separators. Minimal
separators have applications via fixed-parameter-tractable algorithms for
constructing triangulations and phylogenetic trees. Although exact
(exponential-time) algorithms exist for listing these structures, we show that
the counting problems are #P-complete with respect to both exact and
approximation-preserving reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6831</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6831</id><created>2014-11-25</created><authors><author><keyname>Whiting</keyname><forenames>James G. H.</forenames></author><author><keyname>Costello</keyname><forenames>Ben P. J. de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Physarum Chip: Developments in growing computers from slime mould</title><categories>cs.ET</categories><comments>Conference abstract presented at the Unconventional Computation &amp;
  Natural Computation 2014. University of Western Ontario, London, Ontario,
  Canada, July 14-18. 2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Phychip project is a collaborative European research initiative to design
and implement computation using the organism Physarum polycephalum; it is
funded by the Seventh Framework Programme (FP7) by the European Commission
within CORDIS and the FET Proactive scheme. Included in this presentation are
details the development of a Physarum based biosensor and biological logic
gate, offering significant advancements in the respective fields. The work
demonstrates the first steps towards Physarum computation and practical
Physarum Biosensor; subsequent work will focus on development of a hybrid
electronic-Physarum device capable of implementing computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6835</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6835</id><created>2014-11-25</created><authors><author><keyname>Ravi</keyname><forenames>Jithin</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author></authors><title>Zero-Error Function Computation through a Bidirectional Relay</title><categories>cs.IT math.IT</categories><comments>Submitted to ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider zero error function computation in a three node wireless network.
Nodes A and B observe $X$ and $Y$ respectively, and want to compute a function
$f(X,Y)$ with zero error. To achieve this, nodes A and B send messages to a
relay node C at rates $R_A$ and $R_B$ respectively. The relay C then broadcasts
a message to A and B at rate $R_C$ to help them compute $f(X,Y)$ with zero
error. We allow block coding, and study the region of rate-triples
$(R_A,R_B,R_C)$ that are feasible. The rate region is characterized in terms of
graph coloring of some suitably defined probabilistic graphs. We give single
letter inner and outer bounds which meet for some simple examples. We provide a
sufficient condition on the joint distribution $p_{XY}$ under which the relay
can also compute $f(X,Y)$ if A and B can compute it with zero error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6836</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6836</id><created>2014-11-25</created><updated>2015-07-09</updated><authors><author><keyname>Cimpoi</keyname><forenames>Mircea</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Deep convolutional filter banks for texture recognition and segmentation</title><categories>cs.CV</categories><comments>Accepted to CVPR15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in texture recognition often concentrates on the problem of material
recognition in uncluttered conditions, an assumption rarely met by
applications. In this work we conduct a first study of material and describable
texture at- tributes recognition in clutter, using a new dataset derived from
the OpenSurface texture repository. Motivated by the challenge posed by this
problem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector
pooling of a Convolutional Neural Network (CNN) filter bank. D-CNN
substantially improves the state-of-the-art in texture, mate- rial and scene
recognition. Our approach achieves 82.3% accuracy on Flickr material dataset
and 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than
10% over existing approaches. D-CNN easily trans- fers across domains without
requiring feature adaptation as for methods that build on the fully-connected
layers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale
information and describe regions of arbitrary shapes and sizes. Our approach is
particularly suited at lo- calizing stuff categories and obtains
state-of-the-art re- sults on MSRC segmentation dataset, as well as promising
results on recognizing materials and surface attributes in clutter on the
OpenSurfaces dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6837</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6837</id><created>2014-11-25</created><authors><author><keyname>Maiolino</keyname><forenames>Perla</forenames></author><author><keyname>Maggiali</keyname><forenames>Marco</forenames></author><author><keyname>Cannata</keyname><forenames>Giorgio</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>A Flexible and Robust Large Scale Capacitive Tactile System for Robots</title><categories>cs.RO</categories><comments>IEEE Sensor Journal, Vol. 13, Issue 10, 2013</comments><doi>10.1109/JSEN.2013.2258149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacitive technology allows building sensors that are small, compact and
have high sensitivity. For this reason it has been widely adopted in robotics.
In a previous work we presented a compliant skin system based on capacitive
technology consisting of triangular modules interconnected to form a system of
sensors that can be deployed on non-flat surfaces. This solution has been
successfully adopted to cover various humanoid robots. The main limitation of
this and all the approaches based on capacitive technology is that they require
to embed a deformable dielectric layer (usually made using an elastomer)
covered by a conductive layer. This complicates the production process
considerably, introduces hysteresis and limits the durability of the sensors
due to ageing and mechanical stress.
  In this paper we describe a novel solution in which the dielectric is made
using a thin layer of 3D fabric which is glued to conductive and protective
layers using techniques adopted in the clothing industry. As such, the sensor
is easier to produce and has better mechanical properties. Furthermore, the
sensor proposed in this paper embeds transducers for thermal compensation of
the pressure measurements. We report experimental analysis that demonstrates
that the sensor has good properties in terms of sensitivity and resolution.
Remarkably we show that the sensor has very low hysteresis and effectively
allows compensating drifts due to temperature variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6841</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6841</id><created>2014-11-25</created><authors><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Cheng</keyname><forenames>Minquan</forenames></author><author><keyname>Miao</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Dianhua</forenames></author></authors><title>Multimedia IPP Codes with Efficient Tracing</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to IEEE transction on information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary multimedia identifiable parent property codes (binary $t$-MIPPCs) are
used in multimedia fingerprinting schemes where the identification of users
taking part in the averaging collusion attack to illegally redistribute content
is required. In this paper, we first introduce a binary strong multimedia
identifiable parent property code (binary $t$-SMIPPC) whose tracing algorithm
is more efficient than that of a binary $t$-MIPPC. Then a composition
construction for binary $t$-SMIPPCs from $q$-ary $t$-SMIPPCs is provided.
Several infinite series of optimal $q$-ary $t$-SMIPPCs of length $2$ with $t =
2, 3$ are derived from the relationships among $t$-SMIPPCs and other
fingerprinting codes, such as $\overline{t}$-separable codes and $t$-MIPPCs.
Finally, combinatorial properties of $q$-ary $2$-SMIPPCs of length $3$ are
investigated, and optimal $q$-ary $2$-SMIPPCs of length $3$ with $q \equiv 0,
1, 2, 5 \pmod 6$ are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6850</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6850</id><created>2014-11-25</created><authors><author><keyname>Dik</keyname><forenames>Amina</forenames></author><author><keyname>Jebari</keyname><forenames>Khalid</forenames></author><author><keyname>Bouroumi</keyname><forenames>Abdelaziz</forenames></author><author><keyname>Ettouhami</keyname><forenames>Aziz</forenames></author></authors><title>Similarity- based approach for outlier detection</title><categories>cs.CV</categories><comments>International Journal of Computer Science Issues 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for detecting outliers by introducing the
notion of object's proximity. The main idea is that normal point has similar
characteristics with several neighbors. So the point in not an outlier if it
has a high degree of proximity and its neighbors are several. The performance
of this approach is illustrated through real datasets
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6852</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6852</id><created>2014-11-25</created><authors><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author><author><keyname>Sagot</keyname><forenames>Marie-France</forenames></author></authors><title>Efficiently listing bounded length st-paths</title><categories>cs.DS</categories><comments>12 pages, accepted to IWOCA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of listing the $K$ shortest simple (loopless) $st$-paths in a
graph has been studied since the early 1960s. For a non-negatively weighted
graph with $n$ vertices and $m$ edges, the most efficient solution is an
$O(K(mn + n^2 \log n))$ algorithm for directed graphs by Yen and Lawler
[Management Science, 1971 and 1972], and an $O(K(m+n \log n))$ algorithm for
the undirected version by Katoh et al. [Networks, 1982], both using $O(Kn + m)$
space. In this work, we consider a different parameterization for this problem:
instead of bounding the number of $st$-paths output, we bound their length. For
the bounded length parameterization, we propose new non-trivial algorithms
matching the time complexity of the classic algorithms but using only $O(m+n)$
space. Moreover, we provide a unified framework such that the solutions to both
parameterizations -- the classic $K$-shortest and the new length-bounded paths
-- can be seen as two different traversals of a same tree, a Dijkstra-like and
a DFS-like traversal, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6853</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6853</id><created>2014-11-25</created><updated>2014-11-26</updated><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>Soejima</keyname><forenames>Makoto</forenames></author></authors><title>Simple strategies versus optimal schedules in multi-agent patrolling</title><categories>cs.DS cs.CG cs.DM</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we want to patrol a fence (line segment) using k mobile agents
with given speeds v_1, ..., v_k so that every point on the fence is visited by
an agent at least once in every unit time period. A simple strategy where the
i-th agent moves back and forth in a segment of length v_i/2 patrols the length
(v_1 + ... + v_k)/2, but it has been shown recently that this is not always
optimal. Thus a natural question is to determine the smallest c such that a
fence of length c(v_1 + ... + v_k)/2 cannot be patrolled. We give an example
showing c &gt;= 4/3 (and conjecture that this is the best possible). We also
consider a variant of this problem where we want to patrol a circle and the
agents can move only clockwise. We can patrol a circle of perimeter r v_r by a
simple strategy where the r fastest agents move at the same speed. We give an
example where we can achieve the perimeter of 1.05 max_r r v_r (and conjecture
that this constant can be arbitrary big). We propose another variant where we
want to patrol a single point under the constraint that each agent i = 1, ...,
k can visit the point only at a predefined interval of a_i or longer. This
problem can be reduced to the discretized version where the a_i are integers
and the goal is to visit the point at every integer time. It is easy to see
that this discretized patrolling is impossible if 1/a_1 + ... + 1/a_k &lt; 1, and
that there is a simple strategy if 1/a_1 + ... + 1/a_k &gt;= 2. Thus we are
interested in the smallest c such that patrolling is always possible if 1/a_1 +
... + 1/a_k &gt;= c. We prove that alpha &lt;= c &lt; 1.546, where alpha = 1.264... (and
conjecture that c = alpha). We also discuss the computational complexity of
related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6871</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6871</id><created>2014-11-25</created><updated>2015-05-16</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author></authors><title>Tail-scope: Using friends to estimate heavy tails of degree
  distributions in large-scale complex networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>9 pages, 6 figures</comments><journal-ref>Scientific Reports 5, 09752 (2015)</journal-ref><doi>10.1038/srep09752</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex networks in natural and social phenomena have often been
characterized by heavy-tailed degree distributions. However, due to rapidly
growing size of network data and concerns on privacy issues about using these
data, it becomes more difficult to analyze complete data sets. Thus, it is
crucial to devise effective and efficient estimation methods for heavy tails of
degree distributions in large-scale networks only using local information of a
small fraction of sampled nodes. Here we propose a tail-scope method based on
local observational bias of the friendship paradox. We show that the tail-scope
method outperforms the uniform node sampling for estimating heavy tails of
degree distributions, while the opposite tendency is observed in the range of
small degrees. In order to take advantages of both sampling methods, we devise
the hybrid method that successfully recovers the whole range of degree
distributions. Our tail-scope method shows how structural heterogeneities of
large-scale complex networks can be used to effectively reveal the network
structure only with limited local information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6880</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6880</id><created>2014-11-25</created><updated>2015-04-30</updated><authors><author><keyname>Roncal</keyname><forenames>William Gray</forenames></author><author><keyname>Kleissas</keyname><forenames>Dean M.</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Manavalan</keyname><forenames>Priya</forenames></author><author><keyname>Lillaney</keyname><forenames>Kunal</forenames></author><author><keyname>Pekala</keyname><forenames>Michael</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author><author><keyname>Chevillet</keyname><forenames>Mark A.</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author></authors><title>An Automated Images-to-Graphs Framework for High Resolution Connectomics</title><categories>q-bio.QM cs.CV</categories><comments>13 pages, first two authors contributed equally V2: Added additional
  experiments and clarifications; added information on infrastructure and
  pipeline environment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing a map of neuronal connectivity is a critical challenge in
contemporary neuroscience. Recent advances in high-throughput serial section
electron microscopy (EM) have produced massive 3D image volumes of nanoscale
brain tissue for the first time. The resolution of EM allows for individual
neurons and their synaptic connections to be directly observed. Recovering
neuronal networks by manually tracing each neuronal process at this scale is
unmanageable, and therefore researchers are developing automated image
processing modules. Thus far, state-of-the-art algorithms focus only on the
solution to a particular task (e.g., neuron segmentation or synapse
identification).
  In this manuscript we present the first fully automated images-to-graphs
pipeline (i.e., a pipeline that begins with an imaged volume of neural tissue
and produces a brain graph without any human interaction). To evaluate overall
performance and select the best parameters and methods, we also develop a
metric to assess the quality of the output graphs. We evaluate a set of
algorithms and parameters, searching possible operating points to identify the
best available brain graph for our assessment metric. Finally, we deploy a
reference end-to-end version of the pipeline on a large, publicly available
data set. This provides a baseline result and framework for community analysis
and future algorithm development and testing. All code and data derivatives
have been made publicly available toward eventually unlocking new biofidelic
computational primitives and understanding of neuropathologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6884</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6884</id><created>2014-11-21</created><authors><author><keyname>Biyikli</keyname><forenames>Emre</forenames></author><author><keyname>To</keyname><forenames>Albert C.</forenames></author></authors><title>Proportional Topology Optimization: A new non-gradient method for
  solving stress constrained and minimum compliance problems and its
  implementation in MATLAB</title><categories>cs.CE</categories><comments>18 pages, 8 figures, and 2 appendices (MATLAB codes)</comments><doi>10.1371/journal.pone.0145041</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A new topology optimization method called the Proportional Topology
Optimization (PTO) is presented. As a non-gradient method, PTO is simple to
understand, easy to implement, and is also efficient and accurate at the same
time. It is implemented into two MATLAB programs to solve the stress
constrained and minimum compliance problems. Descriptions of the algorithm and
computer programs are provided in detail. The method is applied to solve three
numerical examples for both types of problems. The method shows comparable
efficiency and accuracy with an existing gradient optimality criteria method.
Also, the PTO stress constrained algorithm and minimum compliance algorithm are
compared by feeding output from one algorithm to the other in an alternative
manner, where the former yields lower maximum stress and volume fraction but
higher compliance compared to the latter. Advantages and disadvantages of the
proposed method and future works are discussed. The computer programs are
self-contained and publicly shared in the website www.ptomethod.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6892</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6892</id><created>2014-11-25</created><updated>2015-06-09</updated><authors><author><keyname>Secondini</keyname><forenames>Marco</forenames></author><author><keyname>Foggi</keyname><forenames>Tommaso</forenames></author><author><keyname>Fresi</keyname><forenames>Francesco</forenames></author><author><keyname>Meloni</keyname><forenames>Gianluca</forenames></author><author><keyname>Cavaliere</keyname><forenames>Fabio</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Forestieri</keyname><forenames>Enrico</forenames></author><author><keyname>Pot&#xec;</keyname><forenames>Luca</forenames></author><author><keyname>Sabella</keyname><forenames>Roberto</forenames></author><author><keyname>Prati</keyname><forenames>Giancarlo</forenames></author></authors><title>Optical Time-Frequency Packing: Principles, Design, Implementation, and
  Experimental Demonstration</title><categories>physics.optics cs.IT math.IT</categories><comments>This paper has been accepted for publication in the IEEE/OSA Journal
  of Lightwave Technology</comments><journal-ref>Journal of Lightwave Technology, vol. 33, n. 17, pp. 3558-3570,
  Sept. 2015</journal-ref><doi>10.1109/JLT.2015.2443876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-frequency packing (TFP) transmission provides the highest achievable
spectral efficiency with a constrained symbol alphabet and detector complexity.
In this work, the application of the TFP technique to fiber-optic systems is
investigated and experimentally demonstrated. The main theoretical aspects,
design guidelines, and implementation issues are discussed, focusing on those
aspects which are peculiar to TFP systems. In particular, adaptive compensation
of propagation impairments, matched filtering, and maximum a posteriori
probability detection are obtained by a combination of a butterfly equalizer
and four 8-state parallel Bahl-Cocke-Jelinek-Raviv (BCJR) detectors. A novel
algorithm that ensures adaptive equalization, channel estimation, and a proper
distribution of tasks between the equalizer and BCJR detectors is proposed. A
set of irregular low-density parity-check codes with different rates is
designed to operate at low error rates and approach the spectral efficiency
limit achievable by TFP at different signal-to-noise ratios. An experimental
demonstration of the designed system is finally provided with five
dual-polarization QPSK-modulated optical carriers, densely packed in a 100 GHz
bandwidth, employing a recirculating loop to test the performance of the system
at different transmission distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6897</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6897</id><created>2014-11-24</created><updated>2015-02-10</updated><authors><author><keyname>Viteri-Mera</keyname><forenames>Carlos Andr&#xe9;s</forenames></author><author><keyname>Teixeira</keyname><forenames>Fernando L.</forenames></author></authors><title>Equalized Time Reversal Beamforming for Indoor Wireless Communications</title><categories>cs.NI cs.IT math.IT</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the technologies with potential to provide advanced beamforming
capabilities for future indoor wireless communications is time-reversal (TR).
By using the channel impulse response as a pre-filter at the transmitter, TR
provides automatic spatial focusing of the signal at the receiver. In this
paper we present two contributions on single-user indoor wideband TR systems.
First, we provide a novel analysis of a baseband time-reversal (TR) beamforming
system using two propagation models commonly used in indoor wireless
communications. We derive a new closed-form approximation for the inter-symbol
interference (ISI) power in such scenarios, which leads to a more accurate
estimation of the probability of bit error compared to previous works. We
define performance parameters for the spatial focusing and time compression
properties of TR beamforming and find closed-form approximations for them. We
use this parameters to compare TR performance under different propagation
conditions and channel models. Second, we propose an Equalized TR (ETR)
technique that mitigates the ISI of conventional TR. ETR uses a zero-forcing
pre-equalizer at the transmitter in cascade configuration with the TR
pre-filter. We derive theoretical performance bounds for ETR and show that it
greatly enhances the BER performance of conventional TR with minimal impact to
its beamforming capability. By means of numerical simulations, we verify our
closed-form approximations and show that the proposed ETR technique outperforms
conventional TR with respect to the BER under any SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6907</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6907</id><created>2014-11-25</created><updated>2016-01-21</updated><authors><author><keyname>Nevelsteen</keyname><forenames>Kim J. L.</forenames></author></authors><title>Spatiotemporal Modeling of a Pervasive Game</title><categories>cs.OH</categories><comments>11 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Given pervasive games that maintain a virtual spatiotemporal model of the
physical world, game designers must contend with space and time in the virtual
and physical, but an integrated conceptual model is lacking. Because the
problem domains of GIS and Pervasive Games overlap, Peuquet's Triad
Representational Framework is exapted, from the domain of GIS, and applied to
Pervasive Games. Using Dix et al.'s three types of space and Langran's notion
of time, virtual time and space are then be mapped to the physical world and
vice versa. The approach is evaluated using the pervasive game called Codename:
Heroes, as case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6909</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6909</id><created>2014-11-25</created><authors><author><keyname>Izadinia</keyname><forenames>Hamid</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author><author><keyname>Hertzmann</keyname><forenames>Aaron</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author></authors><title>Image Classification and Retrieval from User-Supplied Tags</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes direct learning of image classification from
user-supplied tags, without filtering. Each tag is supplied by the user who
shared the image online. Enormous numbers of these tags are freely available
online, and they give insight about the image categories important to users and
to image classification. Our approach is complementary to the conventional
approach of manual annotation, which is extremely costly. We analyze of the
Flickr 100 Million Image dataset, making several useful observations about the
statistics of these tags. We introduce a large-scale robust classification
algorithm, in order to handle the inherent noise in these tags, and a
calibration procedure to better predict objective annotations. We show that
freely available, user-supplied tags can obtain similar or superior results to
large databases of costly manual annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6912</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6912</id><created>2014-11-25</created><authors><author><keyname>Hubert</keyname><forenames>Julien</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Short-Term Memory Through Persistent Activity: Evolution of
  Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks</title><categories>cs.NE q-bio.NC</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memories in the brain are separated in two categories: short-term and
long-term memories. Long-term memories remain for a lifetime, while short-term
ones exist from a few milliseconds to a few minutes. Within short-term memory
studies, there is debate about what neural structure could implement it.
Indeed, mechanisms responsible for long-term memories appear inadequate for the
task. Instead, it has been proposed that short-term memories could be sustained
by the persistent activity of a group of neurons. In this work, we explore what
topology could sustain short-term memories, not by designing a model from
specific hypotheses, but through Darwinian evolution in order to obtain new
insights into its implementation. We evolved 10 networks capable of retaining
information for a fixed duration between 2 and 11s. Our main finding has been
that the evolution naturally created two functional modules in the network: one
which sustains the information containing primarily excitatory neurons, while
the other, which is responsible for forgetting, was composed mainly of
inhibitory neurons. This demonstrates how the balance between inhibition and
excitation plays an important role in cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6915</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6915</id><created>2014-11-25</created><updated>2015-01-26</updated><authors><author><keyname>Fernau</keyname><forenames>Henning</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Romero</keyname><forenames>Jazm&#xed;n</forenames></author></authors><title>Kernelization Algorithms for Packing Problems Allowing Overlaps
  (Extended Version)</title><categories>cs.DS</categories><comments>25 pages, 24 references, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of discovering overlapping communities in networks
which we model as generalizations of Graph Packing problems with overlap.
  We seek a collection $\mathcal{S}' \subseteq \mathcal{S}$ consisting of at
least $k$ sets subject to certain disjointness restrictions. In the $r$-Set
Packing with $t$-Membership, each element of $\mathcal{U}$ belongs to at most
$t$ sets of $\mathcal{S'}$ while in $t$-Overlap each pair of sets in
$\mathcal{S'}$ overlaps in at most $t$ elements. Each set of $\mathcal{S}$ has
at most $r$ elements.
  Similarly, both of our graph packing problems seek a collection $\mathcal{K}$
of at least $k$ subgraphs in a graph $G$ each isomorphic to a graph $H \in
\mathcal{H}$. In $\mathcal{H}$-Packing with $t$-Membership, each vertex of $G$
belongs to at most $t$ subgraphs of $\mathcal{K}$ while in $t$-Overlap each
pair of subgraphs in $\mathcal{K}$ overlaps in at most $t$ vertices. Each
member of $\mathcal{H}$ has at most $r$ vertices and $m$ edges.
  We show NP-Completeness results for all of our packing problems and we give a
dichotomy result for the $\mathcal{H}$-Packing with $t$-Membership problem
analogous to the Kirkpatrick and Hell \cite{Kirk78}. We reduce the $r$-Set
Packing with $t$-Membership to a problem kernel with $O((r+1)^r k^{r})$
elements while we achieve a kernel with $O(r^r k^{r-t-1})$ elements for the
$r$-Set Packing with $t$-Overlap. In addition, we reduce the
$\mathcal{H}$-Packing with $t$-Membership and its edge version to problem
kernels with $O((r+1)^r k^{r})$ and $O((m+1)^{m} k^{{m}})$ vertices,
respectively. On the other hand, we achieve kernels with $O(r^r k^{r-t-1})$ and
$O(m^{m} k^{m-t-1})$ vertices for the $\mathcal{H}$-Packing with $t$-Overlap
and its edge version, respectively. In all cases, $k$ is the input parameter
while $t$, $r$, and $m$ are constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6919</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6919</id><created>2014-11-25</created><authors><author><keyname>Caravantes</keyname><forenames>Jorge</forenames></author><author><keyname>Gomez-Molleda</keyname><forenames>M. Angeles</forenames></author><author><keyname>Gonzalez-Vega</keyname><forenames>Laureano</forenames></author></authors><title>A canonical form for the continuous piecewise polynomial functions</title><categories>cs.SC</categories><comments>12 pages, to appear in Journal of Computational and Applied
  Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a canonical form for the elements in the ring of
continuous piecewise polynomial functions. This new representation is based on
the use of a particular class of functions
$$\{C_i(P):P\in\Q[x],i=0,\ldots,\deg(P)\}$$ defined by $$C_i(P)(x)= \left\{
\begin{array}{cll}0 &amp; \mbox{ if } &amp; x \leq \alpha \\ P(x) &amp; \mbox{ if } &amp; x
\geq \alpha \end{array} \right.$$ where $\alpha$ is the $i$-th real root of the
polynomial $P$. These functions will allow us to represent and manipulate
easily every continuous piecewise polynomial function through the use of the
corresponding canonical form.
  It will be also shown how to produce a &quot;rational&quot; representation of each
function $C_{i}(P)$ allowing its evaluation by performing only operations in
$\Q$ and avoiding the use of any real algebraic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6928</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6928</id><created>2014-11-25</created><authors><author><keyname>Lin</keyname><forenames>Jianbiao</forenames></author><author><keyname>Ji</keyname><forenames>Ke</forenames></author><author><keyname>Lin</keyname><forenames>Hui</forenames></author><author><keyname>Wu</keyname><forenames>Enyan</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>A Tag Identification Approach Based On Fragile Watermark</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a tag identify approach based on fragile Watermark that
based on Least significant bit of the replacement that we first use a special
way to initialize the cover to ensure that we can use random positions to embed
the information of tag. Using this way enhance the security of other to get the
right information of this tag. Finally as long as the covered information can
be decoded, the completeness and accuracy of the tag information can be
guaranteed. the result of simulation experiment show that this approach has
high sensitivity and security .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6942</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6942</id><created>2014-11-25</created><authors><author><keyname>Jaroslav</keyname><forenames>Kultan</forenames></author><author><keyname>Matej</keyname><forenames>Kultan</forenames></author></authors><title>Computer networks new generation in the use of RES</title><categories>cs.SY cs.NI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is aimed at analyzing the potential of new information networks to
solve the problems of energy management network with the use of renewable
energy sources. One of the basic problems of renewable energy sources is their
temporal and spatial variability. It is mainly about resources based on direct
solar radiation and wind speed. New computer systems that use only classical
connection-based solid structure of computer network connections but also on
the basis of short-range connections allow accurate prediction of the active
intensity changes observed energy. Using the system thus created can control
precisely the basic energy equipment / generator and operable appliances /
gradient to reduce the power needed resources or from work. This approach is
one of the directions of further development of smart appliances and smart
elements in the energy sector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6970</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6970</id><created>2014-11-25</created><updated>2015-05-27</updated><authors><author><keyname>Hanslovsky</keyname><forenames>Philipp</forenames><affiliation>HHMI Janelia Research Campus</affiliation></author><author><keyname>Bogovic</keyname><forenames>John A.</forenames><affiliation>HHMI Janelia Research Campus</affiliation></author><author><keyname>Saalfeld</keyname><forenames>Stephan</forenames><affiliation>HHMI Janelia Research Campus</affiliation></author></authors><title>Post-acquisition image based compensation for thickness variation in
  microscopy section series</title><categories>cs.CV q-bio.QM stat.AP</categories><journal-ref>IEEE International Symposium on Biomedical Imaging, 2015, pages
  507--511</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serial section Microscopy is an established method for volumetric anatomy
reconstruction. Section series imaged with Electron Microscopy are currently
vital for the reconstruction of the synaptic connectivity of entire animal
brains such as that of Drosophila melanogaster. The process of removing
ultrathin layers from a solid block containing the specimen, however, is a
fragile procedure and has limited precision with respect to section thickness.
We have developed a method to estimate the relative z-position of each
individual section as a function of signal change across the section series.
First experiments show promising results on both serial section Transmission
Electron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron
Microscopy (FIB-SEM) series. We made our solution available as Open Source
plugins for the TrakEM2 software and the ImageJ distribution Fiji.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6972</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6972</id><created>2014-11-25</created><updated>2014-12-05</updated><authors><author><keyname>Yura</keyname><forenames>Fumitaka</forenames></author></authors><title>Hankel Determinant Solution for Elliptic Sequence</title><categories>nlin.SI cs.CR math-ph math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Hankel determinants of a generalized Catalan sequence
satisfy the equations of the elliptic sequence. As a consequence, the
coordinates of the multiples of an arbitrary point on the elliptic curve are
expressed by the Hankel determinants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6973</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6973</id><created>2014-11-21</created><updated>2016-01-20</updated><authors><author><keyname>Sinha</keyname><forenames>Mohit</forenames></author><author><keyname>Dorfler</keyname><forenames>Florian</forenames></author><author><keyname>Johnson</keyname><forenames>Brian B.</forenames></author><author><keyname>Dhople</keyname><forenames>Sairaj V.</forenames></author></authors><title>Uncovering Droop Control Laws Embedded Within the Nonlinear Dynamics of
  Van der Pol Oscillators</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the dynamics of power-electronic inverters in islanded
microgrids that are controlled to emulate the dynamics of Van der Pol
oscillators. The general strategy of controlling inverters to emulate the
behavior of nonlinear oscillators presents a compelling time-domain alternative
to ubiquitous droop control methods which presume the existence of a
quasi-stationary sinusoidal steady state and operate on phasor quantities. We
present two main results in this work. First, by leveraging the method of
periodic averaging, we demonstrate that droop laws are intrinsically embedded
within a slower time scale in the nonlinear dynamics of Van der Pol
oscillators. Second, we establish the global convergence of amplitude and phase
dynamics in a resistive network interconnecting inverters controlled as Van der
Pol oscillators. Furthermore, under a set of non-restrictive decoupling
approximations, we derive sufficient conditions for local exponential stability
of desirable equilibria of the linearized amplitude and phase dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6993</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6993</id><created>2014-11-25</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Velingker</keyname><forenames>Ameya</forenames></author></authors><title>An Entropy Sumset Inequality and Polynomially Fast Convergence to
  Shannon Capacity Over All Alphabets</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a lower estimate on the increase in entropy when two copies of a
conditional random variable $X | Y$, with $X$ supported on
$\mathbb{Z}_q=\{0,1,\dots,q-1\}$ for prime $q$, are summed modulo $q$.
Specifically, given two i.i.d copies $(X_1,Y_1)$ and $(X_2,Y_2)$ of a pair of
random variables $(X,Y)$, with $X$ taking values in $\mathbb{Z}_q$, we show \[
H(X_1 + X_2 \mid Y_1, Y_2) - H(X|Y) \ge \alpha(q) \cdot H(X|Y) (1-H(X|Y)) \]
for some $\alpha(q) &gt; 0$, where $H(\cdot)$ is the normalized (by factor $\log_2
q$) entropy. Our motivation is an effective analysis of the finite-length
behavior of polar codes, and the assumption of $q$ being prime is necessary.
For $X$ supported on infinite groups without a finite subgroup and no
conditioning, a sumset inequality for the absolute increase in (unnormalized)
entropy was shown by Tao (2010).
  We use our sumset inequality to analyze Ar{\i}kan's construction of polar
codes and prove that for any $q$-ary source $X$, where $q$ is any fixed prime,
and any $\epsilon &gt; 0$, polar codes allow {\em efficient} data compression of
$N$ i.i.d. copies of $X$ into $(H(X)+\epsilon)N$ $q$-ary symbols, as soon as
$N$ is polynomially large in $1/\epsilon$. We can get capacity-achieving source
codes with similar guarantees for composite alphabets, by factoring $q$ into
primes and combining different polar codes for each prime in factorization.
  A consequence of our result for noisy channel coding is that for {\em all}
discrete memoryless channels, there are explicit codes enabling reliable
communication within $\epsilon &gt; 0$ of the symmetric Shannon capacity for a
block length and decoding complexity bounded by a polynomial in $1/\epsilon$.
The result was previously shown for the special case of binary input channels
(Guruswami-Xia '13 and Hassani-Alishahi-Urbanke '13), and this work extends the
result to channels over any alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6997</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6997</id><created>2014-11-25</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Perarnau</keyname><forenames>Guillem</forenames></author></authors><title>Fast Recoloring of Sparse Graphs</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that for every graph of maximum average degree bounded
away from $d$, any $(d+1)$-coloring can be transformed into any other one
within a polynomial number of vertex recolorings so that, at each step, the
current coloring is proper. In particular, it implies that we can transform any
$8$-coloring of a planar graph into any other $8$-coloring with a polynomial
number of recolorings. These results give some evidence on a conjecture of
Cereceda, van den Heuvel and Johnson which asserts that any $(d+2)$ coloring of
a $d$-degenerate graph can be transformed into any other one using a polynomial
number of recolorings.
  We also show that any $(2d+2)$-coloring of a $d$-degenerate graph can be
transformed into any other one using a linear number of recolorings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.6998</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.6998</id><created>2014-11-24</created><authors><author><keyname>Arenas</keyname><forenames>Diego</forenames><affiliation>IFSTTAR/COSYS/ESTAS, LAMIH</affiliation></author><author><keyname>Chevirer</keyname><forenames>Remy</forenames><affiliation>IFSTTAR/COSYS/ESTAS</affiliation></author><author><keyname>Hanafi</keyname><forenames>Said</forenames><affiliation>LAMIH</affiliation></author><author><keyname>Rodriguez</keyname><forenames>Joaquin</forenames><affiliation>IFSTTAR/COSYS/ESTAS</affiliation></author></authors><title>Solving the Periodic Timetabling Problem using a Genetic Algorithm</title><categories>cs.AI cs.NE</categories><comments>XVIII Congreso Panamericano de Ingenier\'ia de Transito, Transporte y
  Logistica (PANAM 2014), Jun 2014, Santander, Spain.
  http://www.panam2014.unican.es</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In railway operations, a timetable is established to determine the departure
and arrival times for the trains or other rolling stock at the different
stations or relevant points inside the rail network or a subset of this
network. The elaboration of this timetable is done to respond to the commercial
requirements for both passenger and freight traffic, but also it must respect a
set of security and capacity constraints associated with the railway network,
rolling stock and legislation. Combining these requirements and constraints, as
well as the important number of trains and schedules to plan, makes the
preparation of a feasible timetable a complex and time-consuming process, that
normally takes several months to be completed. This article addresses the
problem of generating periodic timetables, which means that the involved trains
operate in a recurrent pattern. For instance, the trains belonging to the same
train line, depart from some station every 15 minutes or one hour. To tackle
the problem, we present a constraint-based model suitable for this kind of
problem. Then, we propose a genetic algorithm, allowing a rapid generation of
feasible periodic timetables. Finally, two case studies are presented, the
first, describing a sub-set of the Netherlands rail network, and the second a
large portion of the Nord-pas-de-Calais regional rail network, both of them are
then solved using our algorithm and the results are presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7004</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7004</id><created>2014-11-25</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Fang</keyname><forenames>Zhichao</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author></authors><title>Continuous, Dynamic and Comprehensive Article-Level Evaluation of
  Scientific Literature</title><categories>cs.DL physics.soc-ph</categories><comments>The interactive visualization is available at this URL:
  http://xianwenwang.com/research/ale/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is time to make changes to the current research evaluation system, which
is built on the journal selection. In this study, we propose the idea of
continuous, dynamic and comprehensive article-level-evaluation based on
article-level-metrics. Different kinds of metrics are integrated into a
comprehensive indicator, which could quantify both the academic and societal
impact of the article. At different phases after the publication, the weights
of different metrics are dynamically adjusted to mediate the long term and
short term impact of the paper. Using the sample data, we make empirical study
of the article-level-evaluation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7014</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7014</id><created>2014-11-25</created><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Mohan</keyname><forenames>Karthika</forenames></author><author><keyname>Choi</keyname><forenames>Arthur</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Efficient Algorithms for Bayesian Network Parameter Learning from
  Incomplete Data</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient family of algorithms to learn the parameters of a
Bayesian network from incomplete data. In contrast to textbook approaches such
as EM and the gradient method, our approach is non-iterative, yields closed
form parameter estimates, and eliminates the need for inference in a Bayesian
network. Our approach provides consistent parameter estimates for missing data
problems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach
is orders of magnitude faster than EM (as our approach requires no inference).
Given sufficient data, we learn parameters that can be orders of magnitude more
accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7018</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7018</id><created>2014-11-25</created><updated>2015-08-24</updated><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Froese</keyname><forenames>Brittany D.</forenames></author><author><keyname>Oberman</keyname><forenames>Adam M.</forenames></author><author><keyname>Xiao</keyname><forenames>Mingqing</forenames></author></authors><title>A multigrid solver for the three dimensional Monge-Amp\`ere equation</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The elliptic Monge-Amp\`ere equation is a fully nonlinear partial
differential equation which has been the focus of increasing attention from the
scientific computing community. Fast three dimensional solvers are needed, for
example in medical image registration but are not yet available.
  We build fast solvers for smooth solutions in three dimensions using a
nonlinear full-approximation storage multigrid method. Starting from a
second-order accurate centered finite difference approximation, we present a
nonlinear Gauss-Seidel iterative method which has a mechanism for selecting the
convex solution of the equation. The iterative method is used as an effective
smoother, combined with the full-approximation storage multigrid method.
  Numerical experiments are provided to validate the accuracy of the finite
difference scheme and illustrate the computational efficiency of the multigrid
algorithm. The solution time is almost linear in the number of variables.
Problems of size $64^3$ are solved in seconds and of size $128^3$ are solved in
a couple of minutes on a recent model laptop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7055</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7055</id><created>2014-11-25</created><updated>2015-12-22</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Nayyeri</keyname><forenames>Amir</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>All-Pairs Minimum Cuts in Near-Linear Time for Surface-Embedded Graphs</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an undirected $n$-vertex graph $G$ with non-negative edge-weights, we
consider the following type of query: given two vertices $s$ and $t$ in $G$,
what is the weight of a minimum $st$-cut in $G$? We solve this problem in
preprocessing time $O(n\log^3 n)$ for graphs of bounded genus, giving the first
sub-quadratic time algorithm for this class of graphs. Our result also improves
by a logarithmic factor a previous algorithm by Borradaile, Sankowski and
Wulff-Nilsen (FOCS 2010) that applied only to planar graphs. Our algorithm
constructs a Gomory-Hu tree for the given graph, providing a data structure
with space $O(n)$ that can answer minimum-cut queries in constant time. The
dependence on the genus of the input graph in our preprocessing time is
$2^{O(g^2)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7084</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7084</id><created>2014-11-25</created><authors><author><keyname>Zhao</keyname><forenames>Hong</forenames></author><author><keyname>Chen</keyname><forenames>Yifan</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Malik</keyname><forenames>Hafiz</forenames></author></authors><title>Audio Splicing Detection and Localization Using Environmental Signature</title><categories>cs.CR cs.MM</categories><comments>Submitted to IEEE Transactions on Information Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio splicing is one of the most common manipulation techniques in the area
of audio forensics. In this paper, the magnitudes of acoustic channel impulse
response and ambient noise are proposed as the environmental signature.
Specifically, the spliced audio segments are detected according to the
magnitude correlation between the query frames and reference frames via a
statically optimal threshold. The detection accuracy is further refined by
comparing the adjacent frames. The effectiveness of the proposed method is
tested on two data sets. One is generated from TIMIT database, and the other
one is made in four acoustic environments using a commercial grade microphones.
Experimental results show that the proposed method not only detects the
presence of spliced frames, but also localizes the forgery segments with near
perfect accuracy. Comparison results illustrate that the identification
accuracy of the proposed scheme is higher than the previous schemes. In
addition, experimental results also show that the proposed scheme is robust to
MP3 compression attack, which is also superior to the previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7086</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7086</id><created>2014-11-25</created><updated>2014-11-27</updated><authors><author><keyname>Siripuram</keyname><forenames>Aditya</forenames></author><author><keyname>Wu</keyname><forenames>William</forenames></author><author><keyname>Osgood</keyname><forenames>Brad</forenames></author></authors><title>Discrete Sampling and Interpolation: Orthogonal Interpolation for
  Discrete Bandlimited Signals</title><categories>cs.IT math.IT</categories><comments>23 pages, 8 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding unitary submatrices of the discrete Fourier
transform matrix. This problem is related to a diverse set of questions on
idempotents on Z_N, tiling Z_N, difference graphs and maximal cliques. Each of
these is related to the problem of interpolating a discrete bandlimited signal
using an orthogonal basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7087</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7087</id><created>2014-11-25</created><updated>2015-07-14</updated><authors><author><keyname>Yamagata</keyname><forenames>Yoriyuki</forenames></author></authors><title>Consistency proof of a feasible arithmetic inside a bounded arithmetic</title><categories>math.LO cs.LO</categories><comments>This paper has been withdrawn by the author due to a critical flaw in
  the proof. Proposition 1 has a counter-example f(x) = \epsilon, where f(x) is
  defined by composition f(x) = \epsilon(s_i g(x)) using a fast growing
  function g</comments><msc-class>03F03</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents proof that the weakest theory of bounded arithmetic of
Buss's hierarchy is capable of proving the consistency of a system based on a
feasible arithmetic of Cook and Urquhart from which induction has been removed.
This result apparently contradicts the result of Buss and Ignjatovi\'c, who
stated that it is not possible to prove such a result. However, their proof
actually shows that it is not possible to prove the consistency of the system,
which is obtained by the addition of propositional logic and other axioms to a
system such as ours. On the other hand, the system we considered is strictly
equational, which is a property on which our proof relies.
  Our proof relies on the big-step semantics of the terms that appear in the
theory of Cook and Urquhart. In our work, we first prove that, in the system
under consideration, if an equation is proved and either its left or right hand
side is computed, then there is a corresponding computation for its right or
left hand side, respectively. By carefully computing the bound of the size of
the computation, the proof inside a bounded arithmetic of this theorem is
obtained, from which the consistency of the system is readily proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7090</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7090</id><created>2014-11-25</created><authors><author><keyname>Yu</keyname><forenames>Han</forenames></author><author><keyname>Shen</keyname><forenames>Zhiqi</forenames></author><author><keyname>Wu</keyname><forenames>Qiong</forenames></author><author><keyname>Miao</keyname><forenames>Chunyan</forenames></author></authors><title>Designing Socially Intelligent Virtual Companions</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual companions that interact with users in a socially complex environment
require a wide range of social skills. Displaying curiosity is simultaneously a
factor to improve a companion's believability and to unobtrusively affect the
user's activities over time. Curiosity represents a drive to know new things.
It is a major driving force for engaging learners in active learning. Existing
research work pays little attention in curiosity. In this paper, we enrich the
social skills of a virtual companion by infusing curiosity into its mental
model. A curious companion residing in a Virtual Learning Environment (VLE) to
stimulate user's curiosity is proposed. The curious companion model is
developed based on multidisciplinary considerations. The effectiveness of the
curious companion is demonstrated by a preliminary field study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7091</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7091</id><created>2014-11-25</created><authors><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Maehara</keyname><forenames>Takanori</forenames></author><author><keyname>Toyoda</keyname><forenames>Masashi</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Existence of outsiders as a characteristic of online communication
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>40 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networking services (SNSs) involve communication activities
between large number of individuals over the public Internet and their crawled
records are often regarded as proxies of real (i.e., offline) interaction
structure. However, structure observed in these records might differ from real
counterparts because individuals may behave differently online and non-human
accounts may even participate. To understand the difference between online and
real social networks, we investigate an empirical communication network between
users on Twitter, which is perhaps one of the largest SNSs. We define a network
of user pairs that send reciprocal messages. Based on the mixing pattern
observed in this network, we argue that this network differs from conventional
understandings in the sense that there is a small number of distinctive users
that we call outsiders. Outsiders do not belong to any user groups but they are
connected with different groups, while not being well connected with each
other. We identify outsiders by maximizing the degree assortativity coefficient
of the network via node removal, thereby confirming that local structural
properties of outsiders identified are consistent with our hypothesis. Our
findings suggest that the existence of outsiders should be considered when
using Twitter communication networks for social network analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7099</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7099</id><created>2014-11-25</created><updated>2014-11-26</updated><authors><author><keyname>Eyal</keyname><forenames>Ittay</forenames></author></authors><title>The Miner's Dilemma</title><categories>cs.CR cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open distributed system can be secured by requiring participants to
present proof of work and rewarding them for participation. The Bitcoin digital
currency introduced this mechanism, which is adopted by almost all contemporary
digital currencies and related services.
  A natural process leads participants of such systems to form pools, where
members aggregate their power and share the rewards. Experience with Bitcoin
shows that the largest pools are often open, allowing anyone to join. It has
long been known that a member can sabotage an open pool by seemingly joining it
but never sharing its proofs of work. The pool shares its revenue with the
attacker, and so each of its participants earns less.
  We define and analyze a game where pools use some of their participants to
infiltrate other pools and perform such an attack. With any number of pools,
no-pool-attacks is not a Nash equilibrium. With two pools, or any number of
identical pools, there exists an equilibrium that constitutes a tragedy of the
commons where the pools attack one another and all earn less than they would
have if none had attacked.
  For two pools, the decision whether or not to attack is the miner's dilemma,
an instance of the iterative prisoner's dilemma. The game is played daily by
the active Bitcoin pools, which apparently choose not to attack. If this
balance breaks, the revenue of open pools might diminish, making them
unattractive to participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7101</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7101</id><created>2014-11-25</created><authors><author><keyname>Umang</keyname><forenames>Nitish</forenames></author><author><keyname>Erera</keyname><forenames>Alan L.</forenames></author><author><keyname>Bierlaire</keyname><forenames>Michel</forenames></author></authors><title>The robust single machine scheduling problem with uncertain release and
  processing times</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the single machine scheduling problem with uncertain
release times and processing times of jobs. We adopt a robust scheduling
approach, in which the measure of robustness to be minimized for a given
sequence of jobs is the worst-case objective function value from the set of all
possible realizations of release and processing times. The objective function
value is the total flow time of all jobs. We discuss some important properties
of robust schedules for zero and non-zero release times, and illustrate the
added complexity in robust scheduling given non-zero release times. We propose
heuristics based on variable neighborhood search and iterated local search to
solve the problem and generate robust schedules. The algorithms are tested and
their solution performance is compared with optimal solutions or lower bounds
through numerical experiments based on synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7113</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7113</id><created>2014-11-26</created><authors><author><keyname>Aly</keyname><forenames>Mohamed</forenames></author></authors><title>Real time Detection of Lane Markers in Urban Streets</title><categories>cs.CV cs.RO</categories><comments>6 pages</comments><journal-ref>IEEE Intelligent Vehicles Symposium, Eindhoven, The Netherlands,
  June 2008</journal-ref><doi>10.1109/IVS.2008.4621152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robust and real time approach to lane marker detection in urban
streets. It is based on generating a top view of the road, filtering using
selective oriented Gaussian filters, using RANSAC line fitting to give initial
guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is
then followed by a post-processing step. Our algorithm can detect all lanes in
still images of the street in various conditions, while operating at a rate of
50 Hz and achieving comparable results to previous techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7121</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7121</id><created>2014-11-26</created><authors><author><keyname>Kim</keyname><forenames>Donggun</forenames></author><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>Two-Stage Beamformer Design for Massive MIMO Downlink By Trace Quotient
  Formulation</title><categories>cs.IT math.IT</categories><comments>27 pages, 5 figures, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of outer beamformer design based only on channel
statistic information is considered for two-stage beamforming for multi-user
massive MIMO downlink, and the problem is approached based on
signal-to-leakage-plus-noise ratio (SLNR). To eliminate the dependence on the
instantaneous channel state information, a lower bound on the average SLNR is
derived by assuming zero-forcing (ZF) inner beamforming, and an outer
beamformer design method that maximizes the lower bound on the average SLNR is
proposed. It is shown that the proposed SLNR-based outer beamformer design
problem reduces to a trace quotient problem (TQP), which is often encountered
in the field of machine learning. An iterative algorithm is presented to obtain
an optimal solution to the proposed TQP. The proposed method has the capability
of optimally controlling the weighting factor between the signal power to the
desired user and the interference leakage power to undesired users according to
different channel statistics. Numerical results show that the proposed outer
beamformer design method yields significant performance gain over existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7131</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7131</id><created>2014-11-26</created><authors><author><keyname>Tahan</keyname><forenames>Oussama</forenames></author></authors><title>Towards Efficient OpenMP Strategies for Non-Uniform Architectures</title><categories>cs.DC</categories><comments>International Journal of Advanced Studies in Computer Science and
  Engineering (IJASCSE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel processing is considered as todays and future trend for improving
performance of computers. Computing devices ranging from small embedded systems
to big clusters of computers rely on parallelizing applications to reduce
execution time. Many of current computing systems rely on Non-Uniform Memory
Access (NUMA) based processors architectures. In these architectures, analyzing
and considering the non-uniformity is of high importance for improving
scalability of systems. In this paper, we analyze and develop a NUMA based
approach for the OpenMP parallel programming model. Our technique applies a
smart threads allocation method and an advanced tasks scheduling strategy for
reducing remote memory accesses and consequently their extra time consumption.
We implemented our approach within the NANOS runtime system. A set of tests was
conducted using the BOTS benchmarks and results showed the capacity of our
technique in improving the performance of OpenMP applications especially those
dealing with a large amount of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7139</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7139</id><created>2014-11-26</created><authors><author><keyname>Ekici</keyname><forenames>Burak</forenames></author></authors><title>Certification of programs with computational effects</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In purely functional programming languages imperative features, more
generally computational effects are prohibited. However, non-functional lan-
guages do involve effects. The theory of decorated logic provides a rigorous
for- malism (with a refinement in operation signatures) for proving program
properties with respect to computational effects. The aim of this thesis is to
first develop Coq libraries and tools for verifying program properties in
decorated settings as- sociated with several effects: states, local state,
exceptions, non-termination, etc. Then, these tools will be combined to deal
with several effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7140</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7140</id><created>2014-11-26</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames></author><author><keyname>Duval</keyname><forenames>Dominique</forenames></author><author><keyname>Ekici</keyname><forenames>Burak</forenames></author><author><keyname>Pous</keyname><forenames>Damien</forenames></author></authors><title>Program certification with computational effects</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic evaluation is a paradigm in computer algebra which was introduced for
computing with algebraic numbers. In linear algebra, for instance, dynamic
evaluation can be used to apply programs which have been written for matrices
with coefficients modulo some prime number to matrices with coefficients modulo
some composite number. A way to implement dynamic evaluation in modern
computing languages is to use the exceptions mechanism provided by the
language. In this paper, we pesent a proof system for exceptions which involves
both raising and handling, by extending Moggi's approach based on monads.
Moreover, the core part of this proof system is dual to a proof system for the
state effect in imperative languages, which relies on the categorical notion of
comonad. Both proof systems are implemented in the Coq proof assistant, and
they are combined in order to deal with both effects at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7148</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7148</id><created>2014-11-26</created><authors><author><keyname>Gherardi</keyname><forenames>Luca</forenames></author><author><keyname>Hochgeschwender</keyname><forenames>Nico</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>Proceedings of the Fifth International Workshop on Domain-Specific
  Languages and Models for Robotic Systems (DSLRob 2014)</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fifth International Workshop on Domain-Specific Languages and Models for
Robotic Systems (DSLRob'14) was held in conjunction with the 2014 International
Conference on Simulation, Modeling, and Programming for Autonomous Robots
(SIMPAR 2014), October 2014 in Bergamo, Italy.
  The main topics of the workshop were Domain-Specific Languages (DSLs) and
Model-driven Software Development (MDSD) for robotics. A domain-specific
language is a programming language dedicated to a particular problem domain
that offers specific notations and abstractions that increase programmer
productivity within that domain. Model-driven software development offers a
high-level way for domain users to specify the functionality of their system at
the right level of abstraction. DSLs and models have historically been used for
programming complex systems. However recently they have garnered interest as a
separate field of study. Robotic systems blend hardware and software in a
holistic way that intrinsically raises many crosscutting concerns (concurrency,
uncertainty, time constraints, ...), for which reason, traditional
general-purpose languages often lead to a poor fit between the language
features and the implementation requirements. DSLs and models offer a powerful,
systematic way to overcome this problem, enabling the programmer to quickly and
precisely implement novel software solutions to complex problems within the
robotics domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7149</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7149</id><created>2014-11-26</created><authors><author><keyname>Pereira-Fari&#xf1;a</keyname><forenames>M.</forenames></author><author><keyname>Vidal</keyname><forenames>Juan C.</forenames></author><author><keyname>D&#xed;az-Hermida</keyname><forenames>F.</forenames></author><author><keyname>Bugar&#xed;n</keyname><forenames>A.</forenames></author></authors><title>A Fuzzy Syllogistic Reasoning Schema for Generalized Quantifiers</title><categories>cs.AI</categories><comments>22 pages, 6 figures, journal paper</comments><msc-class>artificial intelligence, approximate reasoning</msc-class><journal-ref>(2014) Fuzzy Sets and Systems 234, 79-96</journal-ref><doi>10.1016/j.fss.2013.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new approximate syllogistic reasoning schema is described
that expands some of the approaches expounded in the literature into two ways:
(i) a number of different types of quantifiers (logical, absolute,
proportional, comparative and exception) taken from Theory of Generalized
Quantifiers and similarity quantifiers, taken from statistics, are considered
and (ii) any number of premises can be taken into account within the reasoning
process. Furthermore, a systematic reasoning procedure to solve the syllogism
is also proposed, interpreting it as an equivalent mathematical optimization
problem, where the premises constitute the constraints of the searching space
for the quantifier in the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7158</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7158</id><created>2014-11-26</created><authors><author><keyname>Evans</keyname><forenames>Richard Prideaux</forenames></author><author><keyname>Berger</keyname><forenames>Martin</forenames></author></authors><title>Cathoristic logic: A modal logic of incompatible propositions</title><categories>cs.LO</categories><acm-class>D.1.6; F.3.0; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cathoristic logic is a multi-modal logic where negation is replaced by a
novel operator allowing the expression of incompatible sentences. We present
the syntax and semantics of the logic including complete proof rules, and
establish a number of results such as compactness, a semantic characterisation
of elementary equivalence, the existence of a quadratic-time decision
procedure, and Brandom's incompatibility semantics property. We demonstrate the
usefulness of the logic as a language for knowledge representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7159</identifier>
 <datestamp>2014-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7159</id><created>2014-11-26</created><authors><author><keyname>Mart&#xed;n</keyname><forenames>Pedro</forenames></author><author><keyname>Martini</keyname><forenames>Horst</forenames></author></authors><title>Algorithms for ball hulls and ball intersections in strictly convex
  normed planes</title><categories>math.MG cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1409.8055</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extending results of Hershberger and Suri for the Euclidean plane, we show
that ball hulls and ball intersections of sets of $n$ points in strictly convex
normed planes can be constructed in $O(n \log n)$ time. In addition, we confirm
that, like in the Euclidean subcase, the $2$-center problem with constrained
circles can be solved also for strictly convex normed planes in $O(n^2)$ time.
Some ideas for extending these results to more general types of normed planes
are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7191</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7191</id><created>2014-11-26</created><updated>2016-02-15</updated><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Rotenberg</keyname><forenames>Eva</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Hashing for statistics over k-partitions</title><categories>cs.DS</categories><comments>Appear at FOCS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a hash function for $k$-partitioning a set into
bins, obtaining strong concentration bounds for standard algorithms combining
statistics from each bin.
  This generic method was originally introduced by Flajolet and
Martin~[FOCS'83] in order to save a factor $\Omega(k)$ of time per element over
$k$ independent samples when estimating the number of distinct elements in a
data stream. It was also used in the widely used HyperLogLog algorithm of
Flajolet et al.~[AOFA'97] and in large-scale machine learning by Li et
al.~[NIPS'12] for minwise estimation of set similarity.
  The main issue of $k$-partition, is that the contents of different bins may
be highly correlated when using popular hash functions. This means that methods
of analyzing the marginal distribution for a single bin do not apply. Here we
show that a tabulation based hash function, mixed tabulation, does yield strong
concentration bounds on the most popular applications of $k$-partitioning
similar to those we would get using a truly random hash function. The analysis
is very involved and implies several new results of independent interest for
both simple and double tabulation, e.g. a simple and efficient construction for
invertible bloom filters and uniform hashing on a given set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7192</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7192</id><created>2014-11-26</created><authors><author><keyname>Nandy</keyname><forenames>Ayan</forenames></author><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author><author><keyname>Sopena</keyname><forenames>Eric</forenames></author></authors><title>Outerplanar and planar oriented cliques</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clique number of an undirected graph $G$ is the maximum order of a
complete subgraph of $G$ and is a well-known lower bound for the chromatic
number of $G$. Every proper $k$-coloring of $G$ may be viewed as a homomorphism
(an edge-preserving vertex mapping) of $G$ to the complete graph of order $k$.
By considering homomorphisms of oriented graphs (digraphs without cycles of
length at most 2), we get a natural notion of (oriented) colorings and oriented
chromatic number of oriented graphs. An oriented clique is then an oriented
graph whose number of vertices and oriented chromatic number coincide. However,
the structure of oriented cliques is much less understood than in the
undirected case.
  In this paper, we study the structure of outerplanar and planar oriented
cliques. We first provide a list of 11 graphs and prove that an outerplanar
graph can be oriented as an oriented clique if and only if it contains one of
these graphs as a spanning subgraph. Klostermeyer and MacGillivray conjectured
that the order of a planar oriented clique is at most 15, which was later
proved by Sen [S. Sen. Maximum Order of a Planar Oclique Is 15. Proc.
IWOCA'2012. {\em Lecture Notes Comput. Sci.} 7643:130--142]. We show that any
planar oriented clique on 15 vertices must contain a particular oriented graph
as a spanning subgraph, thus reproving the above conjecture. We also provide
tight upper bounds for the order of planar oriented cliques of girth $k$ for
all $k \ge 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7193</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7193</id><created>2014-11-26</created><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Karetsos</keyname><forenames>George</forenames></author></authors><title>Collision Avoidance in TV White Spaces: A Cross-layer Design Approach
  for Cognitive Radio Networks</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1312.2188</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most promising applications of cognitive radio networks (CRNs)is
the efficient exploitation of TV white spaces (TVWSs) for enhancing the
performance of wireless networks. In this paper, we propose a cross-layer
design (CLD) of carrier sense multiple access with collision avoidance
(CSMA/CA) mechanism at the medium access control (MAC) layer with spectrum
sensing (SpSe) at the physical layer, for identifying the occupancy status of
TV bands. The proposed CLD relies on a Markov chain model with a state pair
containing both the SpSe and the CSMA/CA from which we derive the collision
probability and the achievable throughput. Analytical and simulation results
are obtained for different collision avoidance and spectrum sensing
implementation scenarios by varying the contention window, backoff stage and
probability of detection. The obtained results depict the achievable throughput
under different collision avoidance and spectrum sensing implementation
scenarios indicating thereby the performance of collision avoidance in TVWSs
based cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7197</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7197</id><created>2014-11-26</created><authors><author><keyname>Gustavsson</keyname><forenames>Ulf</forenames></author><author><keyname>Sanch&#xe9;z-Perez</keyname><forenames>Cesar</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Athley</keyname><forenames>Fredrik</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Landin</keyname><forenames>Per</forenames></author><author><keyname>Hausmair</keyname><forenames>Katharina</forenames></author><author><keyname>Fager</keyname><forenames>Christian</forenames></author><author><keyname>Svensson</keyname><forenames>Lars</forenames></author></authors><title>On the Impact of Hardware Impairments on Massive MIMO</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures, Accepted for presentation at Globe-Com workshop
  on Massive MIMO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multi-user (MU) multiple-input multiple-output (MIMO) systems are one
possible key technology for next generation wireless communication systems.
Claims have been made that massive MU-MIMO will increase both the radiated
energy efficiency as well as the sum-rate capacity by orders of magnitude,
because of the high transmit directivity. However, due to the very large number
of transceivers needed at each base-station (BS), a successful implementation
of massive MU-MIMO will be contingent on of the availability of very cheap,
compact and power-efficient radio and digital-processing hardware. This may in
turn impair the quality of the modulated radio frequency (RF) signal due to an
increased amount of power-amplifier distortion, phase-noise, and quantization
noise.
  In this paper, we examine the effects of hardware impairments on a massive
MU-MIMO single-cell system by means of theory and simulation. The simulations
are performed using simplified, well-established statistical hardware
impairment models as well as more sophisticated and realistic models based upon
measurements and electromagnetic antenna array simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7200</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7200</id><created>2014-11-26</created><authors><author><keyname>Tolstikhin</keyname><forenames>Ilya</forenames></author><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author></authors><title>Localized Complexities for Transductive Learning</title><categories>stat.ML cs.LG</categories><comments>Appeared in Conference on Learning Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show two novel concentration inequalities for suprema of empirical
processes when sampling without replacement, which both take the variance of
the functions into account. While these inequalities may potentially have broad
applications in learning theory in general, we exemplify their significance by
studying the transductive setting of learning theory. For which we provide the
first excess risk bounds based on the localized complexity of the hypothesis
class, which can yield fast rates of convergence also in the transductive
learning setting. We give a preliminary analysis of the localized complexities
for the prominent case of kernel classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7210</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7210</id><created>2014-11-26</created><updated>2015-04-01</updated><authors><author><keyname>Fett</keyname><forenames>Daniel</forenames></author><author><keyname>K&#xfc;sters</keyname><forenames>Ralf</forenames></author><author><keyname>Schmitz</keyname><forenames>Guido</forenames></author></authors><title>Analyzing the BrowserID SSO System with Primary Identity Providers Using
  an Expressive Model of the Web</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.1866</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BrowserID is a complex, real-world Single Sign-On (SSO) System for web
applications recently developed by Mozilla. It employs new HTML5 features (such
as web messaging and web storage) and cryptographic assertions to provide
decentralized login, with the intent to respect users' privacy. It can operate
in a primary and a secondary identity provider mode. While in the primary mode
BrowserID runs with arbitrary identity providers (IdPs), in the secondary mode
there is one IdP only, namely Mozilla's default IdP.
  We recently proposed an expressive general model for the web infrastructure
and, based on this web model, analyzed the security of the secondary IdP mode
of BrowserID. The analysis revealed several severe vulnerabilities.
  In this paper, we complement our prior work by analyzing the even more
complex primary IdP mode of BrowserID. We do not only study authentication
properties as before, but also privacy properties. During our analysis we
discovered new and practical attacks that do not apply to the secondary mode:
an identity injection attack, which violates a central authentication property
of SSO systems, and attacks that break an important privacy promise of
BrowserID and which do not seem to be fixable without a major redesign of the
system. Some of our attacks on privacy make use of a browser side channel that
has not gained a lot of attention so far.
  For the authentication bug, we propose a fix and formally prove in a slight
extension of our general web model that the fixed system satisfies all the
requirements we consider. This constitutes the most complex formal analysis of
a web application based on an expressive model of the web infrastructure so
far.
  As another contribution, we identify and prove important security properties
of generic web features in the extended web model to facilitate future analysis
efforts of web standards and web applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7217</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7217</id><created>2014-11-26</created><authors><author><keyname>Foggi</keyname><forenames>Tommaso</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Bononi</keyname><forenames>Alberto</forenames></author><author><keyname>Serena</keyname><forenames>Paolo</forenames></author></authors><title>Spectral Efficiency Optimization in Flexi-Grid Long-Haul Optical Systems</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible grid optical networks allow a better exploitation of fiber capacity,
by enabling a denser frequency allocation. A tighter channel spacing, however,
requires narrower filters, which increase linear intersymbol interference
(ISI), and may dramatically reduce system reach. Commercial coherent receivers
are based on symbol by symbol detectors, which are quite sensitive to ISI. In
this context, Nyquist spacing is considered as the ultimate limit to
wavelength-division multiplexing (WDM) packing.
  In this paper, we show that by introducing a limited-complexity trellis
processing at the receiver, either the reach of Nyquist WDM flexi-grid networks
can be significantly extended, or a denser-than-Nyquist channel packing (i.e.,
a higher spectral efficiency (SE)) is possible at equal reach. By adopting
well-known information-theoretic techniques, we design a limited-complexity
trellis processing and quantify its SE gain in flexi-grid architectures where
wavelength selective switches over a frequency grid of 12.5GHz are employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7224</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7224</id><created>2014-11-26</created><authors><author><keyname>Garetto</keyname><forenames>Michele</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author></authors><title>Efficient analysis of caching strategies under dynamic content
  popularity</title><categories>cs.PF</categories><comments>to appear at Infocom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a novel technique to analyze both isolated and
interconnected caches operating under different caching strategies and
realistic traffic conditions. The main strength of our approach is the ability
to consider dynamic contents which are constantly added into the system
catalogue, and whose popularity evolves over time according to desired
profiles. We do so while preserving the simplicity and computational efficiency
of models developed under stationary popularity conditions, which are needed to
analyze several caching strategies. Our main achievement is to show that the
impact of content popularity dynamics on cache performance can be effectively
captured into an analytical model based on a fixed content catalogue (i.e., a
catalogue whose size and objects' popularity do not change over time).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7225</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7225</id><created>2014-11-26</created><authors><author><keyname>Baaziz</keyname><forenames>Abdelkader</forenames></author><author><keyname>Quoniam</keyname><forenames>Luc</forenames></author></authors><title>Patents used by NPE as an Open Information System in Web 2.0 - Two mini
  case studies</title><categories>cs.CY</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Information Systems around patents are complex, their study coupled with
a creative vision of &quot;out of the box&quot;, overcomes the strict basic functions of
the patent. We have, on several occasions, guiding research around the patent
solely-based on information, since the writing of new patents ; invalidation of
existing patents, the creation of value-added information and their links to
other Information Systems. The traditional R&amp;D based on heavy investments is
one type of technology transfer. But, patent information is also, another
powerful tool of technology transfer, innovation and creativity. Indeed,
conduct research on the patent, from an academic viewpoint, although not always
focusing only on financial revenue, can be considered as a form of &quot;Non
Practicing Entities&quot; (NPE) activity, called rightly or wrongly &quot;Patent Trolls&quot;.
We'll see why the term &quot;patent troll&quot; for this activity is controversial and
inappropriate. The research we will describe in this paper falls within this
context. We show two case studies of efficient use of patent information in
Emerging countries, the first concern the pharmaceutical industry in Brazil and
the second, the oil industry in Algeria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7228</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7228</id><created>2014-11-26</created><authors><author><keyname>Maehara</keyname><forenames>Takanori</forenames></author><author><keyname>Kusumoto</keyname><forenames>Mitsuru</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Efficient SimRank Computation via Linearization</title><categories>cs.DS</categories><comments>46 pages, journal version of our papers appeared in SIGMOD14 and
  ICDE15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SimRank, proposed by Jeh and Widom, provides a good similarity measure that
has been successfully used in numerous applications. While there are many
algorithms proposed for computing SimRank, their computational costs are very
high. In this paper, we propose a new computational technique, &quot;SimRank
linearization,&quot; for computing SimRank, which converts the SimRank problem to a
linear equation problem. By using this technique, we can solve many SimRank
problems, such as single-pair compuation, single-source computation, all-pairs
computation, top k searching, and similarity join problems, efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7231</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7231</id><created>2014-11-26</created><authors><author><keyname>Djehiche</keyname><forenames>Boualem</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author></authors><title>Risk-Sensitive Mean-Field Type Control under Partial Observation</title><categories>math.OC cs.SY math.PR q-fin.MF</categories><comments>arXiv admin note: text overlap with arXiv:1404.1441</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a stochastic maximum principle (SMP) for control problems of
partially observed diffusions of mean-field type with risk-sensitive
performance functionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7245</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7245</id><created>2014-11-26</created><authors><author><keyname>Vandaele</keyname><forenames>Arnaud</forenames></author><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Glineur</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Tuyttens</keyname><forenames>Daniel</forenames></author></authors><title>Heuristics for Exact Nonnegative Matrix Factorization</title><categories>math.OC cs.LG cs.NA stat.ML</categories><comments>32 pages, 2 figures, 16 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exact nonnegative matrix factorization (exact NMF) problem is the
following: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank
$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$
nonnegative matrix $H$ such that $X = WH$. In this paper, we propose two
heuristics for exact NMF, one inspired from simulated annealing and the other
from the greedy randomized adaptive search procedure. We show that these two
heuristics are able to compute exact nonnegative factorizations for several
classes of nonnegative matrices (namely, linear Euclidean distance matrices,
slack matrices, unique-disjointness matrices, and randomly generated matrices)
and as such demonstrate their superiority over standard multi-start strategies.
We also consider a hybridization between these two heuristics that allows us to
combine the advantages of both methods. Finally, we discuss the use of these
heuristics to gain insight on the behavior of the nonnegative rank, i.e., the
minimum factorization rank such that an exact NMF exists. In particular, we
disprove a conjecture on the nonnegative rank of a Kronecker product, propose a
new upper bound on the extension complexity of generic $n$-gons and conjecture
the exact value of (i) the extension complexity of regular $n$-gons and (ii)
the nonnegative rank of a submatrix of the slack matrix of the correlation
polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7267</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7267</id><created>2014-11-26</created><updated>2015-08-07</updated><authors><author><keyname>Scheper</keyname><forenames>Kirk Y. W.</forenames></author><author><keyname>Tijmons</keyname><forenames>Sjoerd</forenames></author><author><keyname>de Visser</keyname><forenames>Coen C.</forenames></author><author><keyname>de Croon</keyname><forenames>Guido C. H. E.</forenames></author></authors><title>Behaviour Trees for Evolutionary Robotics</title><categories>cs.RO</categories><comments>Preprint version of article accepted for publication in Artificial
  Life, MIT Press. http://www.mitpressjournals.org/loi/artl</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Robotics allows robots with limited sensors and processing to
tackle complex tasks by means of sensory-motor coordination. In this paper we
show the first application of the Behaviour Tree framework to a real robotic
platform using the Evolutionary Robotics methodology. This framework is used to
improve the intelligibility of the emergent robotic behaviour as compared to
the traditional Neural Network formulation. As a result, the behaviour is
easier to comprehend and manually adapt when crossing the reality gap from
simulation to reality. This functionality is shown by performing real-world
flight tests with the 20-gram DelFly Explorer flapping wing Micro Air Vehicle
equipped with a 4-gram onboard stereo vision system. The experiments show that
the DelFly can fully autonomously search for and fly through a window with only
its onboard sensors and processing. The success rate of the optimised behaviour
in simulation is 88% and the corresponding real-world performance is 54% after
user adaptation. Although this leaves room for improvement, it is higher than
the 46% success rate from a tuned user-defined controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7273</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7273</id><created>2014-11-26</created><authors><author><keyname>Ben-Avraham</keyname><forenames>Rinat</forenames></author><author><keyname>Henze</keyname><forenames>Matthias</forenames></author><author><keyname>Jaume</keyname><forenames>Rafel</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Raz</keyname><forenames>Orit E.</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Tubis</keyname><forenames>Igor</forenames></author></authors><title>Partial-Matching and Hausdorff RMS Distance Under Translation:
  Combinatorics and Algorithms</title><categories>cs.CG math.CO</categories><comments>31 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the RMS distance (sum of squared distances between pairs of
points) under translation between two point sets in the plane, in two different
setups. In the partial-matching setup, each point in the smaller set is matched
to a distinct point in the bigger set. Although the problem is not known to be
polynomial, we establish several structural properties of the underlying
subdivision of the plane and derive improved bounds on its complexity. These
results lead to the best known algorithm for finding a translation for which
the partial-matching RMS distance between the point sets is minimized. In
addition, we show how to compute a local minimum of the partial-matching RMS
distance under translation, in polynomial time. In the Hausdorff setup, each
point is paired to its nearest neighbor in the other set. We develop algorithms
for finding a local minimum of the Hausdorff RMS distance in nearly linear time
on the line, and in nearly quadratic time in the plane. These improve
substantially the worst-case behavior of the popular ICP heuristics for solving
this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7277</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7277</id><created>2014-11-26</created><updated>2015-12-03</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author></authors><title>$1$-String $B_2$-VPG Representation of Planar Graphs</title><categories>cs.CG cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1409.5816</comments><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that every planar graph has a 1-string $B_2$-VPG
representation---a string representation using paths in a rectangular grid that
contain at most two bends. Furthermore, two paths representing vertices $u,v$
intersect precisely once whenever there is an edge between $u$ and $v$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7280</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7280</id><created>2014-11-26</created><authors><author><keyname>Kaniewski</keyname><forenames>Jedrzej</forenames><affiliation>CQT, Singapore, and QuTech, Delft</affiliation></author><author><keyname>Lee</keyname><forenames>Troy</forenames><affiliation>Nanyang Technological University and CQT, Singapore</affiliation></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Query complexity in expectation</title><categories>quant-ph cs.CC</categories><comments>16 pages LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the query complexity of computing a function f:{0,1}^n--&gt;R_+ in
expectation. This requires the algorithm on input x to output a nonnegative
random variable whose expectation equals f(x), using as few queries to the
input x as possible. We exactly characterize both the randomized and the
quantum query complexity by two polynomial degrees, the nonnegative literal
degree and the sum-of-squares degree, respectively. We observe that the quantum
complexity can be unboundedly smaller than the classical complexity for some
functions, but can be at most polynomially smaller for functions with range
{0,1}.
  These query complexities relate to (and are motivated by) the extension
complexity of polytopes. The linear extension complexity of a polytope is
characterized by the randomized communication complexity of computing its slack
matrix in expectation, and the semidefinite (psd) extension complexity is
characterized by the analogous quantum model. Since query complexity can be
used to upper bound communication complexity of related functions, we can
derive some upper bounds on psd extension complexity by constructing efficient
quantum query algorithms. As an example we give an exponentially-close
entrywise approximation of the slack matrix of the perfect matching polytope
with psd-rank only 2^{n^{1/2+epsilon}}. Finally, we show there is a precise
sense in which randomized/quantum query complexity in expectation corresponds
to the Sherali-Adams and Lasserre hierarchies, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7282</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7282</id><created>2014-11-26</created><updated>2014-12-12</updated><authors><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Successive Cancellation List Polar Decoder using Log-likelihood Ratios</title><categories>cs.IT math.IT</categories><comments>accepted by 2014 Asilomar Conference on Signals, Systems, and
  Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive cancellation list (SCL) decoding algorithm is a powerful method
that can help polar codes achieve excellent error-correcting performance.
However, the current SCL algorithm and decoders are based on likelihood or
log-likelihood forms, which render high hardware complexity. In this paper, we
propose a log-likelihood-ratio (LLR)-based SCL (LLR-SCL) decoding algorithm,
which only needs half the computation and storage complexity than the
conventional one. Then, based on the proposed algorithm, we develop
low-complexity VLSI architectures for LLR-SCL decoders. Analysis results show
that the proposed LLR-SCL decoder achieves 50% reduction in hardware and 98%
improvement in hardware efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7286</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7286</id><created>2014-11-26</created><authors><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Algorithm and Architecture for Hybrid Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><comments>accepted by 2014 Asilomar Conference on Signals, Systems, and
  Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are the first provable capacity-achieving forward error
correction (FEC) codes. In general polar codes can be decoded via either
successive cancellation (SC) or belief propagation (BP) decoding algorithm.
However, to date practical applications of polar codes have been hindered by
the long decoding latency and limited error-correcting performance problems. In
this paper, based on our recent proposed early stopping criteria for the BP
algorithm, we propose a hybrid BP-SC decoding scheme to improve the decoding
performance of polar codes with relatively short latency. Simulation results
show that, for (1024, 512) polar codes the proposed approach leads to at least
0.2dB gain over the BP algorithm with the same maximum number of iterations for
the entire SNR region, and also achieves 0.2dB decoding gain over the BP
algorithm with the same worst-case latency in the high SNR region. Besides,
compared to the SC algorithm, the proposed scheme leads to 0.2dB gain in the
medium SNR region with much less average decoding latency. In addition, we also
propose the low-complexity unified hardware architecture for the hybrid
decoding scheme, which is able to implement SC and BP algorithms using same
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7296</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7296</id><created>2014-11-26</created><authors><author><keyname>Chiasserini</keyname><forenames>Carla</forenames></author><author><keyname>Garetto</keyname><forenames>Michele</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author></authors><title>De-anonymizing scale-free social networks by percolation graph matching</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of social network de-anonymization when relationships
between people are described by scale-free graphs. In particular, we propose a
rigorous, asymptotic mathematical analysis of the network de-anonymization
problem while capturing the impact of power-law node degree distribution, which
is a fundamental and quite ubiquitous feature of many complex systems such as
social networks. By applying bootstrap percolation and a novel graph slicing
technique, we prove that large inhomogeneities in the node degree lead to a
dramatic reduction of the initial set of nodes that must be known a priori (the
seeds) in order to successfully identify all other users. We characterize the
size of this set when seeds are selected using different criteria, and we show
that their number can be as small as $n^{\epsilon}$, for any small
${\epsilon&gt;0}$. Our results are validated through simulation experiments on a
real social network graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7315</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7315</id><created>2014-11-26</created><updated>2015-10-19</updated><authors><author><keyname>Saha</keyname><forenames>Barna</forenames></author></authors><title>Language Edit Distance &amp; Maximum Likelihood Parsing of Stochastic
  Grammars: Faster Algorithms &amp; Connection to Fundamental Graph Problems</title><categories>cs.DS cs.FL</categories><comments>36 pages: This is an updated version of the previous submission
  &quot;Faster Language Edit Distance, Connection to All-pairs Shortest Paths and
  Related Problems&quot;. Introduction is rewritten, an error in a previous lower
  bound proof corrected, and the Sidon sequence construction is elaborated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a context free language $L(G)$ over alphabet $\Sigma$ and a string $s
\in \Sigma^*$, the language edit distance (Lan-ED) problem seeks the minimum
number of edits (insertions, deletions and substitutions) required to convert
$s$ into a valid member of $L(G)$. The well-known dynamic programming algorithm
solves this problem in $O(n^3)$ time (ignoring grammar size) where $n$ is the
string length [Aho, Peterson 1972, Myers 1985]. Despite its vast number of
applications, there is no algorithm known till date that computes or
approximates Lan-ED in true sub-cubic time.
  In this paper we give the first such algorithm that computes Lan-ED almost
optimally. For any arbitrary $\epsilon &gt; 0$, our algorithm runs in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time and returns an estimate
within a multiplicative approximation factor of $(1+\epsilon)$, where $\omega$
is the exponent of ordinary matrix multiplication of $n$ dimensional square
matrices. It also computes the edit script. Further, for all substrings of $s$,
we can estimate their Lan-ED within $(1\pm \epsilon)$ factor in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time with high probability. We
also design the very first sub-cubic ($\tilde{O}(n^\omega)$) algorithm to
handle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie at
the foundation of statistical natural language processing, they generalize
hidden Markov models, and have found widespread applications.
  To complement our upper bound result, we show that exact computation of SCFG
parsing, or Lan-ED with insertion as only edit operation in true sub-cubic time
will imply a truly sub-cubic algorithm for all-pairs shortest paths, and hence
to a large range of problems in graphs and matrices. Known lower bound results
on parsing implies no improvement over our time bound of $O(n^\omega)$ is
possible for any nontrivial multiplicative approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7336</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7336</id><created>2014-11-26</created><authors><author><keyname>Talab</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Abdullah</keyname><forenames>Siti Norul Huda Sheikh</forenames></author><author><keyname>Bataineh</keyname><forenames>Bilal</forenames></author></authors><title>Edge direction matrixes-based local binar patterns descriptor for shape
  pattern recognition</title><categories>cs.CV cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Shapes and texture image recognition usage is an essential branch of pattern
recognition. It is made up of techniques that aim at extracting information
from images via human knowledge and works. Local Binary Pattern (LBP) ensures
encoding global and local information and scaling invariance by introducing a
look-up table to reflect the uniformity structure of an object. However, edge
direction matrixes (EDMS) only apply global invariant descriptor which employs
first and secondary order relationships. The main idea behind this methodology
is the need of improved recognition capabilities, a goal achieved by the
combinative use of these descriptors. This collaboration aims to make use of
the major advantages each one presents, by simultaneously complementing each
other, in order to elevate their weak points. By using multiple classifier
approaches such as random forest and multi-layer perceptron neural network, the
proposed combinative descriptor are compared with the state of the art
combinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS),
LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS
image, Enghlishfnt and Arabic calligraphy . The experiments have shown the
superiority of the introduced descriptor over the GLCM with EDMS, LBP and
moment invariants and other well-known descriptor such as Scale Invariant
Feature Transform from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7337</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7337</id><created>2014-11-26</created><authors><author><keyname>Gamble</keyname><forenames>Jennifer</forenames></author><author><keyname>Chintakunta</keyname><forenames>Harish</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author></authors><title>Coordinate-Free Quantification of Coverage in Dynamic Sensor Networks</title><categories>cs.CG</categories><comments>26 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel set of methods for analyzing coverage properties in
dynamic sensor networks. The dynamic sensor network under consideration is
studied through a series of snapshots, and is represented by a sequence of
simplicial complexes, built from the communication graph at each time point. A
method from computational topology called zigzag persistent homology takes this
sequence of simplicial complexes as input, and returns a `barcode' containing
the birth and death times of homological features in this sequence. We derive
useful statistics from this output for analyzing time-varying coverage
properties. Further, we propose a method which returns specific representative
cycles for these homological features, at each point along the birth-death
intervals. These representative cycles are then used to track coverage holes in
the network, and obtain size estimates for individual holes at each time point.
A weighted barcode, incorporating the size information, is then used as a
visual and quantitative descriptor of the dynamic network coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7338</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7338</id><created>2014-11-26</created><updated>2015-08-31</updated><authors><author><keyname>Bernstein</keyname><forenames>Daniel Irving</forenames></author><author><keyname>Ho</keyname><forenames>Lam Si Tung</forenames></author><author><keyname>Long</keyname><forenames>Colby</forenames></author><author><keyname>Steel</keyname><forenames>Mike</forenames></author><author><keyname>John</keyname><forenames>Katherine St.</forenames></author><author><keyname>Sullivant</keyname><forenames>Seth</forenames></author></authors><title>Bounds on the Expected Size of the Maximum Agreement Subtree</title><categories>q-bio.PE cs.DS</categories><comments>Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove polynomial upper and lower bounds on the expected size of the
maximum agreement subtree of two random binary phylogenetic trees under both
the uniform distribution and Yule-Harding distribution. This positively answers
a question posed in earlier work. Determining tight upper and lower bounds
remains an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7341</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7341</id><created>2014-11-26</created><updated>2015-05-16</updated><authors><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Korwar</keyname><forenames>Arpita</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author><author><keyname>Thierauf</keyname><forenames>Thomas</forenames></author></authors><title>Deterministic Identity Testing for Sum of Read-Once Oblivious Arithmetic
  Branching Programs</title><categories>cs.CC</categories><comments>22 pages, Computational Complexity Conference, 2015</comments><acm-class>F.1.3, F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A read-once oblivious arithmetic branching program (ROABP) is an arithmetic
branching program (ABP) where each variable occurs in at most one layer. We
give the first polynomial time whitebox identity test for a polynomial computed
by a sum of constantly many ROABPs. We also give a corresponding blackbox
algorithm with quasi-polynomial time complexity $n^{O(\log n)}$. In both the
cases, our time complexity is double exponential in the number of ROABPs.
  ROABPs are a generalization of set-multilinear depth-$3$ circuits. The prior
results for the sum of constantly many set-multilinear depth-$3$ circuits were
only slightly better than brute-force, i.e. exponential-time.
  Our techniques are a new interplay of three concepts for ROABP: low
evaluation dimension, basis isolating weight assignment and low-support rank
concentration. We relate basis isolation to rank concentration and extend it to
a sum of two ROABPs using evaluation dimension (or partial derivatives).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7343</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7343</id><created>2014-11-26</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author></authors><title>NewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction
  Processing (OLTP) for Big Data Management</title><categories>cs.DB</categories><comments>11 pages, 04 figures and 04 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the key advances in resolving the big-data problem has been the
emergence of an alternative database technology. Today, classic RDBMS are
complemented by a rich set of alternative Data Management Systems (DMS)
specially designed to handle the volume, variety, velocity and variability
ofBig Data collections; these DMS include NoSQL, NewSQL and Search-based
systems. NewSQL is a class of modern relational database management systems
(RDBMS) that provide the same scalable performance of NoSQL systems for online
transaction processing (OLTP) read-write workloads while still maintaining the
ACID guarantees of a traditional database system. This paper discusses about
NewSQL data management system; and compares with NoSQL and with traditional
database system. This paper covers architecture, characteristics,
classification of NewSQL databases for online transaction processing (OLTP) for
Big data management. It also provides the list ofpopular NoSQL as well as
NewSQL databases in separate categorized tables. This paper compares SQL based
RDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed
some research issues ofNoSQL and NewSQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7344</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7344</id><created>2014-11-26</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author></authors><title>Analysis of Memory Ballooning Technique for Dynamic Memory Management of
  Virtual Machines (VMs)</title><categories>cs.DC</categories><comments>11 pages and 07 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Memory ballooning is dynamic memory management technique for virtual machines
(VMs). Ballooning is a part of memory reclamation technique operations used by
a hypervisor to allow the physical host system to retrieve unused memory from
certain guest virtual machines (VMs) and share it with others. Memory
ballooning allows the total amount ofRAM required by guest VMs to exceed the
amount ofphysical RAM available on the host. Memory overcommitment enables a
higher consolidation ratio in a hypervisor. Using memory overcommitment, users
can consolidate VMs on a physical machine such that physical resources are
utilized in an optimal manner while delivering good performance. Hence memory
reclamation is an integral component ofmemory overcommitment. In this paper, we
address that the basic cause of memory that ballooning is memory overcommitment
from using memory-intensive virtual machines. We compared to others reclamation
technique and identify Cost Associate with Memory Ballooning in state of Memory
Overcommitment. The objective of this paper is to analyse memory ballooning
technique for dynamic memory management of VMs. For this analysis, VMware based
virtualization software e.g ESXi Server, vCenter Server, vSphere Client are
installed and configured on the Centre for Innovation and Technology (CIT) Lab,
DIU; for monitor and analyze VM performance for memory ballooning technique.
The performance ofmemory ballooning technique is evaluated with two different
test cases. The purpose is to help users understand, how this technique impact
the performance. Finally, we presents the throughput ofheavy workload with
different memory limits when using ballooning or swapping; and analyse VM
performance issue for this technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7346</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7346</id><created>2014-11-26</created><updated>2015-04-18</updated><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment L.</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author></authors><title>A Chasm Between Identity and Equivalence Testing with Conditional
  Queries</title><categories>cs.DS cs.CC cs.LG math.PR math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent model for property testing of probability distributions enables
tremendous savings in the sample complexity of testing algorithms, by allowing
them to condition the sampling on subsets of the domain.
  In particular, Canonne, Ron, and Servedio showed that, in this setting,
testing identity of an unknown distribution $D$ (i.e., whether $D=D^*$ for an
explicitly known $D^*$) can be done with a constant number of samples,
independent of the support size $n$ -- in contrast to the required $\sqrt{n}$
in the standard sampling model. However, it was unclear whether the same held
for the case of testing equivalence, where both distributions are unknown.
Indeed, while Canonne, Ron, and Servedio established a
$\mathrm{poly}\log(n)$-query upper bound for equivalence testing, very recently
brought down to $\tilde O(\log\log n)$ by Falahatgar et al., whether a
dependence on the domain size $n$ is necessary was still open, and explicitly
posed by Fischer at the Bertinoro Workshop on Sublinear Algorithms. In this
work, we answer the question in the positive, showing that any testing
algorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in the
conditional sampling model. Interestingly, this demonstrates an intrinsic
qualitative gap between identity and equivalence testing, absent in the
standard sampling model (where both problems have sampling complexity
$n^{\Theta(1)}$).
  Turning to another question, we investigate the complexity of support size
estimation. We provide a doubly-logarithmic upper bound for the adaptive
version of this problem, generalizing work of Ron and Tsur to our weaker model.
We also establish a logarithmic lower bound for the non-adaptive version of
this problem. This latter result carries on to the related problem of
non-adaptive uniformity testing, an exponential improvement over previous
results that resolves an open question of Chakraborty et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7357</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7357</id><created>2014-11-26</created><updated>2014-12-15</updated><authors><author><keyname>Kaur</keyname><forenames>Jasleen</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author></authors><title>Quality versus quantity in scientific impact</title><categories>cs.DL physics.soc-ph</categories><comments>20 pages, 7 figures, and 1 table</comments><journal-ref>Journal of Informetrics 9 (2015), pp. 800-808</journal-ref><doi>10.1016/j.joi.2015.07.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation metrics are becoming pervasive in the quantitative evaluation of
scholars, journals and institutions. More then ever before, hiring, promotion,
and funding decisions rely on a variety of impact metrics that cannot
disentangle quality from quantity of scientific output, and are biased by
factors such as discipline and academic age. Biases affecting the evaluation of
single papers are compounded when one aggregates citation-based metrics across
an entire publication record. It is not trivial to compare the quality of two
scholars that during their careers have published at different rates in
different disciplines in different periods of time. We propose a novel solution
based on the generation of a statistical baseline specifically tailored on the
academic profile of each researcher. Our method can decouple the roles of
quantity and quality of publications to explain how a certain level of impact
is achieved. The method is flexible enough to allow for the evaluation of, and
fair comparison among, arbitrary collections of papers --- scholar publication
records, journals, and entire institutions; and can be extended to
simultaneously suppresses any source of bias. We show that our method can
capture the quality of the work of Nobel laureates irrespective of number of
publications, academic age, and discipline, even when traditional metrics
indicate low impact in absolute terms. We further apply our methodology to
almost a million scholars and over six thousand journals to measure the impact
that cannot be explained by the volume of publications alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7359</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7359</id><created>2014-11-26</created><updated>2014-12-07</updated><authors><author><keyname>Farzan</keyname><forenames>Arash</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author><author><keyname>Salinger</keyname><forenames>Alejandro</forenames></author></authors><title>Algorithms in the Ultra-Wide Word Model</title><categories>cs.DS cs.DC</categories><comments>28 pages, 5 figures; minor changes</comments><acm-class>F.1.1; F.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effective use of parallel computing resources to speed up algorithms in
current multi-core parallel architectures remains a difficult challenge, with
ease of programming playing a key role in the eventual success of various
parallel architectures. In this paper we consider an alternative view of
parallelism in the form of an ultra-wide word processor. We introduce the
Ultra-Wide Word architecture and model, an extension of the word-RAM model that
allows for constant time operations on thousands of bits in parallel. Word
parallelism as exploited by the word-RAM model does not suffer from the more
difficult aspects of parallel programming, namely synchronization and
concurrency. For the standard word-RAM algorithms, the speedups obtained are
moderate, as they are limited by the word size. We argue that a large class of
word-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining
speedups comparable to multi-threaded computations while keeping the simplicity
of programming of the sequential RAM model. We show that this is the case by
describing implementations of Ultra-Wide Word algorithms for dynamic
programming and string searching. In addition, we show that the Ultra-Wide Word
model can be used to implement a nonstandard memory architecture, which enables
the sidestepping of lower bounds of important data structure problems such as
priority queues and dynamic prefix sums. While similar ideas about operating on
large words have been mentioned before in the context of multimedia processors
[Thorup 2003], it is only recently that an architecture like the one we propose
has become feasible and that details can be worked out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7376</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7376</id><created>2014-11-26</created><updated>2015-08-26</updated><authors><author><keyname>Bensmail</keyname><forenames>Julien</forenames></author><author><keyname>Duffy</keyname><forenames>Christopher</forenames></author><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author></authors><title>Analogous to cliques for (m,n)-colored mixed graphs</title><categories>math.CO cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.7192</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vertex coloring of a graph $G$ with $n$-colors can be equivalently thought to
be a graph homomorphism (edge preserving vertex mapping) of $G$ to the complete
graph $K_n$ of order $n$. So, in that sense, the chromatic number $\chi(G)$ of
$G$ will be the order of the smallest complete graph to which $G$ admits a
homomorphism to. As every graph, which is not a complete graph, admits a
homomorphism to a smaller complete graph, we can redefine the chromatic number
$\chi(G)$ of $G$ to be the order of the smallest graph to which $G$ admits a
homomorphism to. Of course, such a smallest graph must be a complete graph as
they are the only graphs with chromatic number equal to their order.
  The concept of vertex coloring can be generalize for other types of graphs.
Naturally, the chromatic number is defined to be the order of the smallest
graph (of the same type) to which a graph admits homomorphism to. The analogous
notion of clique turns out to be the graphs with order equal to their (so
defined) &quot;chromatic number&quot;. These &quot;cliques&quot; turns out to be much more
complicated than their undirected counterpart and are interesting objects of
study. In this article, we mainly study different aspects of &quot;cliques&quot; for
signed (graphs with positive or negative signs assigned to each edge) and
switchable signed graphs (equivalence class of signed graph with respect to
switching signs of edges incident to the same vertex).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7399</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7399</id><created>2014-11-26</created><updated>2015-01-24</updated><authors><author><keyname>Klein</keyname><forenames>Benjamin</forenames></author><author><keyname>Lev</keyname><forenames>Guy</forenames></author><author><keyname>Sadeh</keyname><forenames>Gil</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for
  Image Annotation</title><categories>cs.CV</categories><comments>new version includes text synthesis by an RNN and experiments with
  the COCO benchmark</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the traditional object recognition pipeline, descriptors are densely
sampled over an image, pooled into a high dimensional non-linear representation
and then passed to a classifier. In recent years, Fisher Vectors have proven
empirically to be the leading representation for a large variety of
applications. The Fisher Vector is typically taken as the gradients of the
log-likelihood of descriptors, with respect to the parameters of a Gaussian
Mixture Model (GMM). Motivated by the assumption that different distributions
should be applied for different datasets, we present two other Mixture Models
and derive their Expectation-Maximization and Fisher Vector expressions. The
first is a Laplacian Mixture Model (LMM), which is based on the Laplacian
distribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian
Mixture Model (HGLMM) which is based on a weighted geometric mean of the
Gaussian and Laplacian distribution. An interesting property of the
Expectation-Maximization algorithm for the latter is that in the maximization
step, each dimension in each component is chosen to be either a Gaussian or a
Laplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we
achieve state-of-the-art results for both the image annotation and the image
search by a sentence tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7406</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7406</id><created>2014-11-26</created><authors><author><keyname>Potluri</keyname><forenames>Pushpa Sree</forenames></author></authors><title>Error Correction Capacity of Unary Coding</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unary coding has found applications in data compression, neural network
training, and in explaining the production mechanism of birdsong. Unary coding
is redundant; therefore it should have inherent error correction capacity. An
expression for the error correction capability of unary coding for the
correction of single errors has been derived in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7414</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7414</id><created>2014-11-26</created><updated>2015-05-29</updated><authors><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Sandryhaila</keyname><forenames>Aliaksei</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Signal Recovery on Graphs: Variation Minimization</title><categories>cs.SI cs.LG stat.ML</categories><comments>To appear on IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2441042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of signal recovery on graphs as graphs model data
with complex structure as signals on a graph. Graph signal recovery implies
recovery of one or multiple smooth graph signals from noisy, corrupted, or
incomplete measurements. We propose a graph signal model and formulate signal
recovery as a corresponding optimization problem. We provide a general solution
by using the alternating direction methods of multipliers. We next show how
signal inpainting, matrix completion, robust principal component analysis, and
anomaly detection all relate to graph signal recovery, and provide
corresponding specific solutions and theoretical analysis. Finally, we validate
the proposed methods on real-world recovery problems, including online blog
classification, bridge condition identification, temperature estimation,
recommender system, and expert opinion combination of online blog
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7415</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7415</id><created>2014-11-26</created><updated>2016-03-07</updated><authors><author><keyname>Burghardt</keyname><forenames>Keith A.</forenames></author><author><keyname>Rand</keyname><forenames>William</forenames></author><author><keyname>Girvan</keyname><forenames>Michelle</forenames></author></authors><title>Competing opinions and stubbornness: connecting models to data</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 10 figures, 2 column revetx4 format</comments><journal-ref>Phys. Rev. E 93, 032305 (2016)</journal-ref><doi>10.1103/PhysRevE.93.032305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general contagion-like model for competing opinions that
includes dynamic resistance to alternative opinions. We show that this model
can describe candidate vote distributions, spatial vote correlations, and a
slow approach to opinion consensus with sensible parameter values. These
empirical properties of large group dynamics, previously understood using
distinct models, may be different aspects of human behavior that can be
captured by a more unified model, such as the one introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7416</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7416</id><created>2014-11-26</created><authors><author><keyname>Ren</keyname><forenames>Ju</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Yaoxue</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Kuan</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xuemin</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>SACRM: Social Aware Crowdsourcing with Reputation Management in Mobile
  Sensing</title><categories>cs.NI</categories><journal-ref>Computer Communications, Vol. 65, No. 7, pp. 55-65, 2015</journal-ref><doi>10.1016/j.comcom.2015.01.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile sensing has become a promising paradigm for mobile users to obtain
information by task crowdsourcing. However, due to the social preferences of
mobile users, the quality of sensing reports may be impacted by the underlying
social attributes and selfishness of individuals. Therefore, it is crucial to
consider the social impacts and trustworthiness of mobile users when selecting
task participants in mobile sensing. In this paper, we propose a Social Aware
Crowdsourcing with Reputation Management (SACRM) scheme to select the
well-suited participants and allocate the task rewards in mobile sensing.
Specifically, we consider the social attributes, task delay and reputation in
crowdsourcing and propose a participant selection scheme to choose the
well-suited participants for the sensing task under a fixed task budget. A
report assessment and rewarding scheme is also introduced to measure the
quality of the sensing reports and allocate the task rewards based the assessed
report quality. In addition, we develop a reputation management scheme to
evaluate the trustworthiness and cost performance ratio of mobile users for
participant selection. Theoretical analysis and extensive simulations
demonstrate that SACRM can efficiently improve the crowdsourcing utility and
effectively stimulate the participants to improve the quality of their sensing
reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7419</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7419</id><created>2014-11-26</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author><author><keyname>Silva</keyname><forenames>Frederico C.</forenames></author><author><keyname>Porto</keyname><forenames>Fabio</forenames></author></authors><title>$\Upsilon$-DB: A system for data-driven hypothesis management and
  analytics</title><categories>cs.DB</categories><comments>6 pages, 7 figures, submitted to ACM SIGMOD 2015, Demo track. arXiv
  admin note: substantial text overlap with arXiv:1411.5196</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vision of $\Upsilon$-DB introduces deterministic scientific hypotheses as
a kind of uncertain and probabilistic data, and opens some key technical
challenges for enabling data-driven hypothesis management and analytics. The
$\Upsilon$-DB system addresses those challenges throughout a
design-by-synthesis pipeline that defines its architecture. It processes
hypotheses from their XML-based extraction to encoding as uncertain and
probabilistic U-relational data, and eventually to their conditioning in the
presence of observations. In this demo we present a first prototype of the
$\Upsilon$-DB system. We showcase its core innovative features by means of use
case scenarios in computational science in which the hypotheses are extracted
from a model repository on the web and evaluated (rated/ranked) as
probabilistic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7432</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7432</id><created>2014-11-26</created><authors><author><keyname>Tosi</keyname><forenames>Alessandra</forenames></author><author><keyname>Hauberg</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Vellido</keyname><forenames>Alfredo</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Metrics for Probabilistic Geometries</title><categories>stat.ML cs.LG</categories><comments>UAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the geometrical structure of probabilistic generative
dimensionality reduction models using the tools of Riemannian geometry. We
explicitly define a distribution over the natural metric given by the models.
We provide the necessary algorithms to compute expected metric tensors where
the distribution over mappings is given by a Gaussian process. We treat the
corresponding latent variable model as a Riemannian manifold and we use the
expectation of the metric under the Gaussian process prior to define
interpolating paths and measure distance between latent points. We show how
distances that respect the expected metric lead to more appropriate generation
of new data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7439</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7439</id><created>2014-11-26</created><updated>2014-12-18</updated><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Output Feedback Stabilization of Switched Linear Systems with Limited
  Information</title><categories>cs.SY</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an encoding and control strategy for the stabilization of switched
systems with limited information, supposing the controller is given for each
mode. Only the quantized output and the active mode of the plant at each
sampling time are transmitted to the controller. Due to switching, the active
mode of the plant may be different from that of the controller in the
closed-loop system. Hence if switching occurs, the quantizer must recalculate a
bounded set containing the estimation error for quantization at the next
sampling time. We establish the global asymptotic stability under a
slow-switching assumption on dwell time and average dwell time. To this end, we
construct multiple discrete-time Lyapunov functions with respect to the
estimated state and the size of the bounded set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7441</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7441</id><created>2014-11-26</created><authors><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author><author><keyname>Bras</keyname><forenames>Ronan Le</forenames></author><author><keyname>Suram</keyname><forenames>Santosh K.</forenames></author><author><keyname>Gregoire</keyname><forenames>John M.</forenames></author><author><keyname>Gomes</keyname><forenames>Carla</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>van Dover</keyname><forenames>Robert B.</forenames></author></authors><title>Pattern Decomposition with Complex Combinatorial Constraints:
  Application to Materials Discovery</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying important components or factors in large amounts of noisy data is
a key problem in machine learning and data mining. Motivated by a pattern
decomposition problem in materials discovery, aimed at discovering new
materials for renewable energy, e.g. for fuel and solar cells, we introduce
CombiFD, a framework for factor based pattern decomposition that allows the
incorporation of a-priori knowledge as constraints, including complex
combinatorial constraints. In addition, we propose a new pattern decomposition
algorithm, called AMIQO, based on solving a sequence of (mixed-integer)
quadratic programs. Our approach considerably outperforms the state of the art
on the materials discovery problem, scaling to larger datasets and recovering
more precise and physically meaningful decompositions. We also show the
effectiveness of our approach for enforcing background knowledge on other
application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7443</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7443</id><created>2014-11-26</created><authors><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Huang</keyname><forenames>Weiyu</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Diffusion and Superposition Distances for Signals Supported on Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the diffusion and superposition distances as two metrics to
compare signals supported in the nodes of a network. Both metrics consider the
given vectors as initial temperature distributions and diffuse heat trough the
edges of the graph. The similarity between the given vectors is determined by
the similarity of the respective diffusion profiles. The superposition distance
computes the instantaneous difference between the diffused signals and
integrates the difference over time. The diffusion distance determines a
distance between the integrals of the diffused signals. We prove that both
distances define valid metrics and that they are stable to perturbations in the
underlying network. We utilize numerical experiments to illustrate their
utility in classifying signals in a synthetic network as well as in classifying
ovarian cancer histologies using gene mutation profiles of different patients.
We also reinterpret diffusion as a transformation of interrelated feature
spaces and use it as preprocessing tool for learning. We use diffusion to
increase the accuracy of handwritten digit classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7445</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7445</id><created>2014-11-26</created><authors><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author><author><keyname>Loxton</keyname><forenames>Ryan</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author></authors><title>Bi-objective Optimization for Robust RGB-D Visual Odometry</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a new bi-objective optimization formulation for robust
RGB-D visual odometry. We investigate two methods for solving the proposed
bi-objective optimization problem: the weighted sum method (in which the
objective functions are combined into a single objective function) and the
bounded objective method (in which one of the objective functions is optimized
and the value of the other objective function is bounded via a constraint). Our
experimental results for the open source TUM RGB-D dataset show that the new
bi-objective optimization formulation is superior to several existing RGB-D
odometry methods. In particular, the new formulation yields more accurate
motion estimates and is more robust when textural or structural features in the
image sequence are lacking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7450</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7450</id><created>2014-11-26</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author></authors><title>Worst-Case Linear Discriminant Analysis as Scalable Semidefinite
  Feasibility Problems</title><categories>cs.LG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient semidefinite programming (SDP)
approach to worst-case linear discriminant analysis (WLDA). Compared with the
traditional LDA, WLDA considers the dimensionality reduction problem from the
worst-case viewpoint, which is in general more robust for classification.
However, the original problem of WLDA is non-convex and difficult to optimize.
In this paper, we reformulate the optimization problem of WLDA into a sequence
of semidefinite feasibility problems. To efficiently solve the semidefinite
feasibility problems, we design a new scalable optimization method with
quasi-Newton methods and eigen-decomposition being the core components. The
proposed method is orders of magnitude faster than standard interior-point
based SDP solvers.
  Experiments on a variety of classification problems demonstrate that our
approach achieves better performance than standard LDA. Our method is also much
faster and more scalable than standard interior-point SDP solvers based WLDA.
The computational complexity for an SDP with $m$ constraints and matrices of
size $d$ by $d$ is roughly reduced from $\mathcal{O}(m^3+md^3+m^2d^2)$ to
$\mathcal{O}(d^3)$ ($m&gt;d$ in our case).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7455</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7455</id><created>2014-11-26</created><authors><author><keyname>Forbes</keyname><forenames>Michael A.</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author></authors><title>Dimension Expanders via Rank Condensers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging theory of &quot;linear-algebraic pseudorandomness&quot; aims to understand
the linear-algebraic analogs of fundamental Boolean pseudorandom objects where
the rank of subspaces plays the role of the size of subsets. In this work, we
study and highlight the interrelationships between several such algebraic
objects such as subspace designs, dimension expanders, seeded rank condensers,
two-source rank condensers, and rank-metric codes. In particular, with the
recent construction of near-optimal subspace designs by Guruswami and Kopparty
as a starting point, we construct good (seeded) rank condensers (both lossless
and lossy versions), which are a small collection of linear maps $\mathbb{F}^n
\to \mathbb{F}^t$ for $t \ll n$ such that for every subset of $\mathbb{F}^n$ of
small rank, its rank is preserved (up to a constant factor in the lossy case)
by at least one of the maps.
  We then compose a tensoring operation with our lossy rank condenser to
construct constant-degree dimension expanders over polynomially large fields.
That is, we give $O(1)$ explicit linear maps $A_i:\mathbb{F}^n\to \mathbb{F}^n$
such that for any subspace $V \subseteq \mathbb{F}^n$ of dimension at most
$n/2$, $\dim\bigl( \sum_i A_i(V)\bigr) \ge (1+\Omega(1)) \dim(V)$. Previous
constructions of such constant-degree dimension expanders were based on
Kazhdan's property $T$ (for the case when $\mathbb{F}$ has characteristic zero)
or monotone expanders (for every field $\mathbb{F}$); in either case the
construction was harder than that of usual vertex expanders. Our construction,
on the other hand, is simpler.
  Via an equivalence to linear rank-metric codes, we then construct optimal
lossless two-source condensers. We then use our seeded rank condensers to
obtain near-optimal lossy two-source condensers for constant rank sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7460</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7460</id><created>2014-11-26</created><authors><author><keyname>Pattabiraman</keyname><forenames>Bharath</forenames></author><author><keyname>Patwary</keyname><forenames>Md. Mostofa Ali</forenames></author><author><keyname>Gebremedhin</keyname><forenames>Assefaw H.</forenames></author><author><keyname>Liao</keyname><forenames>Wei-keng</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author></authors><title>Fast Algorithms for the Maximum Clique Problem on Massive Graphs with
  Applications to Overlapping Community Detection</title><categories>cs.DS cs.SI</categories><comments>28 pages, 7 figures, 10 tables, 2 algorithms. arXiv admin note:
  substantial text overlap with arXiv:1209.5818</comments><journal-ref>Internet Mathematics 2014, Special Issue (WAW'13)</journal-ref><doi>10.1080/15427951.2014.986778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum clique problem is a well known NP-Hard problem with applications
in data mining, network analysis, information retrieval and many other areas
related to the World Wide Web. There exist several algorithms for the problem
with acceptable runtimes for certain classes of graphs, but many of them are
infeasible for massive graphs. We present a new exact algorithm that employs
novel pruning techniques and is able to find maximum cliques in very large,
sparse graphs quickly. Extensive experiments on different kinds of synthetic
and real-world graphs show that our new algorithm can be orders of magnitude
faster than existing algorithms. We also present a heuristic that runs orders
of magnitude faster than the exact algorithm while providing optimal or
near-optimal solutions. We illustrate a simple application of the algorithms in
developing methods for detection of overlapping communities in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7462</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7462</id><created>2014-11-26</created><authors><author><keyname>Chen</keyname><forenames>Tehuan</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author><author><keyname>Ren</keyname><forenames>Zhigang</forenames></author><author><keyname>Loxton</keyname><forenames>Ryan</forenames></author></authors><title>Optimal Boundary Control for Water Hammer Suppression in Fluid
  Transmission Pipelines</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When fluid flow in a pipeline is suddenly halted, a pressure surge or wave is
created within the pipeline. This phenomenon, called water hammer, can cause
major damage to pipelines, including pipeline ruptures. In this paper, we model
the problem of mitigating water hammer during valve closure by an optimal
boundary control problem involving a nonlinear hyperbolic PDE system that
describes the fluid flow along the pipeline. The control variable in this
system represents the valve boundary actuation implemented at the pipeline
terminus. To solve the boundary control problem, we first use {the method of
lines} to obtain a finite-dimensional ODE model based on the original PDE
system. Then, for the boundary control design, we apply the control
parameterization method to obtain an approximate optimal parameter selection
problem that can be solved using nonlinear optimization techniques such as
Sequential Quadratic Programming (SQP). We conclude the paper with simulation
results demonstrating the capability of optimal boundary control to
significantly reduce flow fluctuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7466</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7466</id><created>2014-11-26</created><authors><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>The Treasure beneath Convolutional Layers: Cross-convolutional-layer
  Pooling for Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of recent studies have shown that a Deep Convolutional Neural
Network (DCNN) pretrained on a large dataset can be adopted as a universal
image description which leads to astounding performance in many visual
classification tasks. Most of these studies, if not all, adopt activations of
the fully-connected layer of a DCNN as the image or region representation and
it is believed that convolutional layer activations are less discriminative.
  This paper, however, advocates that if used appropriately convolutional layer
activations can be turned into a powerful image representation which enjoys
many advantages over fully-connected layer activations. This is achieved by
adopting a new technique proposed in this paper called
cross-convolutional-layer pooling. More specifically, it extracts subarrays of
feature maps of one convolutional layer as local features and pools the
extracted features with the guidance of feature maps of the successive
convolutional layer. Compared with exising methods that apply DCNNs in the
local feature setting, the proposed method is significantly faster since it
requires much fewer times of DCNN forward computation. Moreover, it avoids the
domain mismatch issue which is usually encountered when applying fully
connected layer activations to describe local regions. By applying our method
to four popular visual classification tasks, it is demonstrated that the
proposed method can achieve comparable or in some cases significantly better
performance than existing fully-connected layer based image representations
while incurring much lower computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7469</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7469</id><created>2014-11-26</created><authors><author><keyname>Dey</keyname><forenames>Lopamudra</forenames></author><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author></authors><title>Canonical PSO Based k-Means Clustering Approach for Real Datasets</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  &quot;Clustering&quot; the significance and application of this technique is spread
over various fields. Clustering is an unsupervised process in data mining, that
is why the proper evaluation of the results and measuring the compactness and
separability of the clusters are important issues.The procedure of evaluating
the results of a clustering algorithm is known as cluster validity measure.
Different types of indexes are used to solve different types of problems and
indices selection depends on the kind of available data.This paper first
proposes Canonical PSO based K-means clustering algorithm and also analyses
some important clustering indices (intercluster, intracluster) and then
evaluates the effects of those indices on real-time air pollution
database,wholesale customer, wine, and vehicle datasets using typical K-means,
Canonical PSO based K-means, simple PSO based K-means,DBSCAN, and Hierarchical
clustering algorithms.This paper also describes the nature of the clusters and
finally compares the performances of these clustering algorithms according to
the validity assessment. It also defines which algorithm will be more desirable
among all these algorithms to make proper compact clusters on this particular
real life datasets. It actually deals with the behaviour of these clustering
algorithms with respect to validation indexes and represents their results of
evaluation in terms of mathematical and graphical forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7472</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7472</id><created>2014-11-27</created><updated>2015-04-02</updated><authors><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Teng</keyname><forenames>Yifeng</forenames></author><author><keyname>Wang</keyname><forenames>Zihe</forenames></author><author><keyname>Xiao</keyname><forenames>Shenke</forenames></author><author><keyname>Xu</keyname><forenames>Yichong</forenames></author></authors><title>Computational issues in time-inconsistent planning</title><categories>cs.GT cs.CC cs.DS cs.SI</categories><comments>20 pages, 7 figures, submitted to EC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-inconsistency refers to a paradox in decision making where agents
exhibit inconsistent behaviors over time. Examples are procrastination where
agents tends to costly postpone easy tasks, and abandonments where agents start
a plan and quit in the middle. These behaviors are undesirable in the sense
that agents make clearly suboptimal decisions over optimal ones. To capture
such behaviors and more importantly, to quantify inefficiency caused by such
behaviors, [Kleinberg &amp; Oren 2014] propose a graph model which is essentially
same as the standard planning model except for the cost structure. Using this
model, they initiate the study of several interesting problems: 1) cost ratio:
the worst ratio between the actual cost of the agent and the optimal cost, over
all graph instances; 2) motivating subgraph: how to motivate the agent to reach
the goal by deleting nodes and edges; 3) Intermediate rewards: how to motivate
agents to reach the goal by placing intermediate rewards. Kleinberg and Oren
give partial answers to these questions, but the main problems are still open.
In fact, they raise these problems explicitly as open problems in their paper.
In this paper, we give answers to all three open problems in [Kleinberg &amp; Oren
2014]. First, we show a tight upper bound of cost ratio for graphs without
Akerlof's structure, thus confirm the conjecture by Kleinberg and Oren that
Akerlof's structure is indeed the worst case for cost ratio. Second, we prove
that finding a motivating subgraph is NP-hard, showing that it is generally
inefficient to motivate agents by deleting nodes and edges in the graph. Last
but not least, we show that computing a strategy to place minimum amount of
total reward is also NP-hard. Therefore, it is computational inefficient to
motivate agents by placing intermediate rewards. The techniques we use to prove
these results are nontrivial and of independent interests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7474</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7474</id><created>2014-11-27</created><authors><author><keyname>Kaur</keyname><forenames>Deepinder</forenames></author></authors><title>A Comparative Study of Various Distance Measures for Software fault
  prediction</title><categories>cs.SE</categories><comments>4 pages,2 figures,&quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT),
  17(3): 117-120, Nov 2014</journal-ref><doi>10.14445/22312803/IJCTT-V17P122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different distance measures have been used for efficiently predicting
software faults at early stages of software development. One stereotyped
approach for software fault prediction due to its computational efficiency is
K-means clustering, which partitions the dataset into K number of clusters
using any distance measure. Distance measures by using some metrics are used to
extract similar data objects which help in developing efficient algorithms for
clustering and classification. In this paper, we study K-means clustering with
three different distance measures Euclidean, Sorensen and Canberra by using
datasets that have been collected from NASA MDP (metrics data program) .Results
are displayed with the help of ROC curve. The experimental results shows that
K-means clustering with Sorensen distance is better than Euclidean distance and
Canberra distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7477</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7477</id><created>2014-11-27</created><authors><author><keyname>Terekhov</keyname><forenames>I. S.</forenames></author><author><keyname>Reznichenko</keyname><forenames>A. V.</forenames></author><author><keyname>Turitsyn</keyname><forenames>S. K.</forenames></author></authors><title>Mutual information in nonlinear communication channel: Analytical
  results in large SNR limit</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying quasi-classical perturbation theory in the corresponding Feynman's
path-integral representation, we derive in the limit of the large
signal-to-noise ratio the analytical expression for the mutual information of
the nonlinear communication channel described by the nonlinear Shr\&quot;{o}dinger
equation (NLSE) with the additive Gaussian noise. The NLSE is one of the
fundamental models in nonlinear physics and has a broad range of applications,
including fibre-optic communications --- the backbone of the Internet. Our
analytical result demonstrates that the corrections to the mutual information
in the leading nonlinearity order are positive. This result is somewhat
counterintuitive, that is, that the impact of nonlinearity may increase the
Shannon capacity above the capacity of a corresponding linear channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7480</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7480</id><created>2014-11-27</created><authors><author><keyname>Rosin</keyname><forenames>Christopher D.</forenames></author></authors><title>Unweighted Stochastic Local Search can be Effective for Random CSP
  Benchmarks</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ULSA, a novel stochastic local search algorithm for random binary
constraint satisfaction problems (CSP). ULSA is many times faster than the
prior state of the art on a widely-studied suite of random CSP benchmarks.
Unlike the best previous methods for these benchmarks, ULSA is a simple
unweighted method that does not require dynamic adaptation of weights or
penalties. ULSA obtains new record best solutions satisfying 99 of 100
variables in the challenging frb100-40 benchmark instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7482</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7482</id><created>2014-11-27</created><authors><author><keyname>Bhattacharya</keyname><forenames>Abhijit</forenames></author><author><keyname>Ladwa</keyname><forenames>Sanjay Motilal</forenames></author><author><keyname>Srivastava</keyname><forenames>Rachit</forenames></author><author><keyname>Mallya</keyname><forenames>Aniruddha</forenames></author><author><keyname>Rao</keyname><forenames>Akhila</forenames></author><author><keyname>M</keyname><forenames>Easwar Vivek.</forenames></author><author><keyname>Sahib</keyname><forenames>Deeksha G. Rao</forenames></author><author><keyname>Anand</keyname><forenames>S. V. R.</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>SmartConnect: A System for the Design and Deployment of Wireless Sensor
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed SmartConnect, a tool that addresses the growing need for
the design and deployment of multihop wireless relay networks for connecting
sensors to a control center. Given the locations of the sensors, the traffic
that each sensor generates, the quality of service (QoS) requirements, and the
potential locations at which relays can be placed, SmartConnect helps design
and deploy a low- cost wireless multihop relay network. SmartConnect adopts a
field interactive, iterative approach, with model based network design, field
evaluation and relay augmentation per- formed iteratively until the desired QoS
is met. The design process is based on approximate combinatorial optimization
algorithms. In the paper, we provide the design choices made in SmartConnect
and describe the experimental work that led to these choices. We provide
results from some experimental deployments. Finally, we conduct an experimental
study of the robustness of the network design over long time periods (as
channel conditions slowly change), in terms of the relay augmentation and route
adaptation required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7487</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7487</id><created>2014-11-27</created><authors><author><keyname>Daneshgar</keyname><forenames>Amir</forenames></author><author><keyname>Khadem</keyname><forenames>Behrooz</forenames></author></authors><title>A Self-synchronized Image Encryption Scheme</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a word based chaotic image encryption scheme for gray images
is proposed, that can be used in both synchronous and self-synchronous modes.
The encryption scheme operates in a finite field where we have also analyzed
its performance according to numerical precision used in implementation. We
show that the scheme not only passes a variety of security tests, but also it
is verified that the proposed scheme operates faster than other existing
schemes of the same type even when using lightweight short key sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7492</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7492</id><created>2014-11-27</created><authors><author><keyname>Oliveira</keyname><forenames>Rafael</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author><author><keyname>Volk</keyname><forenames>Ben Lee</forenames></author></authors><title>Subexponential Size Hitting Sets for Bounded Depth Multilinear Formulas</title><categories>cs.CC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give subexponential size hitting sets for bounded depth
multilinear arithmetic formulas. Using the known relation between black-box PIT
and lower bounds we obtain lower bounds for these models.
  For depth-3 multilinear formulas, of size $\exp(n^\delta)$, we give a hitting
set of size $\exp(\tilde{O}(n^{2/3 + 2\delta/3}))$. This implies a lower bound
of $\exp(\tilde{\Omega}(n^{1/2}))$ for depth-3 multilinear formulas, for some
explicit polynomial.
  For depth-4 multilinear formulas, of size $\exp(n^\delta)$, we give a hitting
set of size $\exp(\tilde{O}(n^{2/3 + 4\delta/3}))$. This implies a lower bound
of $\exp(\tilde{\Omega}(n^{1/4}))$ for depth-4 multilinear formulas, for some
explicit polynomial.
  A regular formula consists of alternating layers of $+,\times$ gates, where
all gates at layer $i$ have the same fan-in. We give a hitting set of size
(roughly) $\exp\left(n^{1- \delta} \right)$, for regular depth-$d$ multilinear
formulas of size $\exp(n^\delta)$, where $\delta = O(\frac{1}{\sqrt{5}^d})$.
This result implies a lower bound of roughly
$\exp(\tilde{\Omega}(n^{\frac{1}{\sqrt{5}^d}}))$ for such formulas.
  We note that better lower bounds are known for these models, but also that
none of these bounds was achieved via construction of a hitting set. Moreover,
no lower bound that implies such PIT results, even in the white-box model, is
currently known.
  Our results are combinatorial in nature and rely on reducing the underlying
formula, first to a depth-4 formula, and then to a read-once algebraic
branching program (from depth-3 formulas we go straight to read-once algebraic
branching programs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7493</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7493</id><created>2014-11-27</created><authors><author><keyname>Borges-Quintana</keyname><forenames>Mijail</forenames></author><author><keyname>Borges-Trenard</keyname><forenames>Miguel Angel</forenames></author><author><keyname>Martinez-Moro</keyname><forenames>Edgar</forenames></author></authors><title>On zero neighbours and trial sets of linear codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Th</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the set of leader codewords of a non-binary linear
code. This set has some nice properties related to the monotonicity of the
weight compatible order on the generalized support of a vector in $\mathbb
F_q^n$. This allows us to describe a test set, a trial set and the set zero
neighbours in terms of the leader codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7494</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7494</id><created>2014-11-27</created><updated>2015-01-19</updated><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author></authors><title>An Evolutionary Optimization Approach to Risk Parity Portfolio Selection</title><categories>q-fin.PM cs.NE</categories><journal-ref>Lecture Notes in Computer Science Volume 9028: 279-288. 2015</journal-ref><doi>10.1007/978-3-319-16549-3_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an evolutionary optimization approach to solve the
risk parity portfolio selection problem. While there exist convex optimization
approaches to solve this problem when long-only portfolios are considered, the
optimization problem becomes non-trivial in the long-short case. To solve this
problem, we propose a genetic algorithm as well as a local search heuristic.
This algorithmic framework is able to compute solutions successfully. Numerical
results using real-world data substantiate the practicability of the approach
presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7507</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7507</id><created>2014-11-27</created><authors><author><keyname>MS</keyname><forenames>Akshay</forenames></author><author><keyname>Mohan</keyname><forenames>Suhas</forenames></author><author><keyname>Kuri</keyname><forenames>Vincent</forenames></author><author><keyname>Sitaram</keyname><forenames>Dinkar</forenames></author><author><keyname>Phalachandra</keyname><forenames>H. L.</forenames></author></authors><title>Efficient Support of Big Data Storage Systems on the Cloud</title><categories>cs.DC</categories><comments>Presented at 2nd International Workshop on Cloud Computing
  Applications (ICWA) during IEEE International Conference on High Performance
  Computing (HiPC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its advantages over traditional data centers, there has been a rapid
growth in the usage of cloud infrastructures. These include public clouds
(e.g., Amazon EC2), or private clouds, such as clouds deployed using OpenStack.
A common factor in many of the well known infrastructures, for example
OpenStack and CloudStack, is that networked storage is used for storage of
persistent data. However, traditional Big Data systems, including Hadoop, store
data in commodity local storage for reasons of high performance and low cost.
We present an architecture for supporting Hadoop on Openstack using local
storage. Subsequently, we use benchmarks on Openstack and Amazon to show that
for supporting Hadoop, local storage has better performance and lower cost. We
conclude that cloud systems should support local storage for persistent data
(in addition to networked storage) so as to provide efficient support for
Hadoop and other Big Data systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7516</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7516</id><created>2014-11-27</created><authors><author><keyname>Citkin</keyname><forenames>Alex</forenames></author></authors><title>A Meta-Logic of Inference Rules: Syntax</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work was intended to be an attempt to introduce the meta-language for
working with multiple-conclusion inference rules that admit asserted
propositions along with the rejected propositions. The presence of rejected
propositions, and especially the presence of the rule of reverse substitution,
requires certain change the definition of structurality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7525</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7525</id><created>2014-11-27</created><authors><author><keyname>Pereira-Fari&#xf1;a</keyname><forenames>M.</forenames></author><author><keyname>D&#xed;az-Hermida</keyname><forenames>F.</forenames></author><author><keyname>Bugar&#xed;n</keyname><forenames>A.</forenames></author></authors><title>On the analysis of set-based fuzzy quantified reasoning using classical
  syllogistics</title><categories>cs.AI</categories><comments>19 pages, 4 figures</comments><msc-class>artificial intelligence, approximate reasoning</msc-class><journal-ref>&quot;Fuzzy Sets and Systems&quot;, vol. 214(1), 83-94</journal-ref><doi>10.1016/j.fss.2012.03.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syllogism is a type of deductive reasoning involving quantified statements.
The syllogistic reasoning scheme in the classical Aristotelian framework
involves three crisp term sets and four linguistic quantifiers, for which the
main support is the linguistic properties of the quantifiers. A number of fuzzy
approaches for defining an approximate syllogism have been proposed for which
the main support is cardinality calculus. In this paper we analyze fuzzy
syllogistic models previously described by Zadeh and Dubois et al. and compare
their behavior with that of the classical Aristotelian framework to check which
of the 24 classical valid syllogistic reasoning patterns or moods are
particular crisp cases of these fuzzy approaches. This allows us to assess to
what extent these approaches can be considered as either plausible extensions
of the classical crisp syllogism or a basis for a general approach to the
problem of approximate syllogism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7529</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7529</id><created>2014-11-27</created><authors><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Improving the Performance of the Zero-Forcing Multiuser MISO Downlink
  Precoder through User Grouping</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Multiple Input Single Output (MISO) Gaussian Broadcast
channel with $N_t$ antennas at the base station (BS) and $N_u$ single-antenna
users in the downlink. We propose a novel user grouping precoder which improves
the sum rate performance of the Zero-Forcing (ZF) precoder specially when the
channel is ill-conditioned. The proposed precoder partitions all the users into
small groups of equal size. Downlink beamforming is then done in such a way
that, at each user's receiver the interference from the signal intended for
users not in its group is nulled out. Intra-group interference still remains,
and is cancelled through successive interference pre-subtraction at the BS
using Dirty Paper Coding (DPC). The proposed user grouping method is different
from user selection, since it is a method for precoding of information to the
selected (scheduled) users, and not for selecting which users are to be
scheduled. Through analysis and simulations, the proposed user grouping based
precoder is shown to achieve significant improvement in the achievable sum rate
when compared to the ZF precoder. When users are paired (i.e., each group has
two users), the complexity of the proposed precoder is $O(N_u^3) + O(N_u^2
N_t)$ which is the same as that of the ZF precoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7533</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7533</id><created>2014-11-27</created><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>Constant-Envelope Precoding with Time-Variation Constraint on the
  Transmitted Phase Angles</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Wireless Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider downlink precoding in a frequency-selective multi-user massive
MIMO system with highly efficient but non-linear power amplifiers at the base
station (BS). A low-complexity precoding algorithm is proposed, which generates
constant-envelope (CE) transmit signals for each BS antenna. To avoid large
variations in the phase angle transmitted from each antenna, the difference of
the phase angles transmitted in consecutive channel uses is limited to
$[-\alpha \pi \,,\, \alpha \pi]$ for a fixed $0 &lt; \alpha \leq 1$. To achieve a
desired per-user information rate, the extra total transmit power required
under the time variation constraint when compared to the special case of no
time variation constraint (i.e., $\alpha=1$), is small for many practical
values of $\alpha$. In a i.i.d. Rayleigh fading channel with $80$ BS antennas,
$5$ single-antenna users and a desired per-user information rate of $1$
bit-per-channel-use, the extra total transmit power required is less than $2.0$
dB when $\alpha = 1/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7542</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7542</id><created>2014-11-27</created><authors><author><keyname>Probst</keyname><forenames>Malte</forenames></author><author><keyname>Rothlauf</keyname><forenames>Franz</forenames></author><author><keyname>Grahl</keyname><forenames>J&#xf6;rn</forenames></author></authors><title>Scalability of using Restricted Boltzmann Machines for Combinatorial
  Optimization</title><categories>cs.NE</categories><acm-class>I.2.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Restricted Boltzmann
Machines (RBMs) are generative neural networks with these desired properties.
We integrate an RBM into an EDA and evaluate the performance of this system in
solving combinatorial optimization problems with a single objective. We assess
how the number of fitness evaluations and the CPU time scale with problem size
and with problem complexity. The results are compared to the Bayesian
Optimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires
larger population sizes and a larger number of fitness evaluations, it
outperforms BOA in terms of CPU times, in particular if the problem is large or
complex. RBM-EDA requires less time for model building than BOA. These results
highlight the potential of using generative neural networks for combinatorial
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7554</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7554</id><created>2014-11-27</created><updated>2015-04-13</updated><authors><author><keyname>Bazzi</keyname><forenames>Louay</forenames></author><author><keyname>Audah</keyname><forenames>Hani</forenames></author></authors><title>Impact of redundant checks on the LP decoding thresholds of LDPC codes</title><categories>cs.IT math.IT</categories><acm-class>E.4; H.1.1; G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feldman et al.(2005) asked whether the performance of the LP decoder can be
improved by adding redundant parity checks to tighten the LP relaxation. We
prove that for LDPC codes, even if we include all redundant checks,
asymptotically there is no gain in the LP decoder threshold on the BSC under
certain conditions on the base Tanner graph. First, we show that if the graph
has bounded check-degree and satisfies a condition which we call asymptotic
strength, then including high degree redundant checks in the LP does not
significantly improve the threshold in the following sense: for each constant
delta&gt;0, there is a constant k&gt;0 such that the threshold of the LP decoder
containing all redundant checks of degree at most k improves by at most delta
upon adding to the LP all redundant checks of degree larger than k. We conclude
that if the graph satisfies a rigidity condition, then including all redundant
checks does not improve the threshold of the base LP. We call the graph
asymptotically strong if the LP decoder corrects a constant fraction of errors
even if the LLRs of the correct variables are arbitrarily small. By building on
the work of Feldman et al.(2007) and Viderman(2013), we show that asymptotic
strength follows from sufficiently large expansion. We also give a geometric
interpretation of asymptotic strength in terms pseudocodewords. We call the
graph rigid if the minimum weight of a sum of check nodes involving a cycle
tends to infinity as the block length tends to infinity. Under the assumptions
that the graph girth is logarithmic and the minimum check degree is at least 3,
rigidity is equivalent to the nondegeneracy property that adding at least
logarithmically many checks does not give a constant weight check. We argue
that nondegeneracy is a typical property of random check-regular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7564</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7564</id><created>2014-11-27</created><updated>2016-02-22</updated><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation
  and Applications</title><categories>cs.CV</categories><comments>18 pages. Accepted to IEEE Transactions on Pattern Analysis and
  Machine Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, many problems such as image segmentation, pixel
labelling, and scene parsing can be formulated as binary quadratic programs
(BQPs). For submodular problems, cuts based methods can be employed to
efficiently solve large-scale problems. However, general nonsubmodular problems
are significantly more challenging to solve. Finding a solution when the
problem is of large size to be of practical interest, however, typically
requires relaxation. Two standard relaxation methods are widely used for
solving general BQPs--spectral methods and semidefinite programming (SDP), each
with their own advantages and disadvantages. Spectral relaxation is simple and
easy to implement, but its bound is loose. Semidefinite relaxation has a
tighter bound, but its computational complexity is high, especially for large
scale problems. In this work, we present a new SDP formulation for BQPs, with
two desirable properties. First, it has a similar relaxation bound to
conventional SDP formulations. Second, compared with conventional SDP methods,
the new SDP formulation leads to a significantly more efficient and scalable
dual optimization approach, which has the same degree of complexity as spectral
methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton
methods, for the dual problem. Both of them are significantly more efficiently
than standard interior-point methods. In practice, the smoothing Newton solver
is faster than the quasi-Newton solver for dense or medium-sized problems,
while the quasi-Newton solver is preferable for large sparse/structured
problems. Our experiments on a few computer vision applications including
clustering, image segmentation, co-segmentation and registration show the
potential of our SDP formulation for solving large-scale BQPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7582</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7582</id><created>2014-11-27</created><authors><author><keyname>Hussain</keyname><forenames>Zaeem</forenames></author><author><keyname>Meila</keyname><forenames>Marina</forenames></author></authors><title>Graph Sensitive Indices for Comparing Clusterings</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report discusses two new indices for comparing clusterings of a set of
points. The motivation for looking at new ways for comparing clusterings stems
from the fact that the existing clustering indices are based on set cardinality
alone and do not consider the positions of data points. The new indices,
namely, the Random Walk index (RWI) and Variation of Information with Neighbors
(VIN), are both inspired by the clustering metric Variation of Information
(VI). VI possesses some interesting theoretical properties which are also
desirable in a metric for comparing clusterings. We define our indices and
discuss some of their explored properties which appear relevant for a
clustering index. We also include the results of these indices on clusterings
of some example data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7591</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7591</id><created>2014-11-27</created><updated>2015-11-08</updated><authors><author><keyname>Hoshen</keyname><forenames>Yedid</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author></authors><title>An Egocentric Look at Video Photographer Identity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7593</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7593</id><created>2014-11-27</created><authors><author><keyname>Diaz</keyname><forenames>Rafael</forenames></author><author><keyname>Gomez</keyname><forenames>Laura</forenames></author></authors><title>Indirect Influences in International Trade</title><categories>cs.SI physics.soc-ph q-fin.GN</categories><journal-ref>Networks and Heterogenous Media 10 (2015) 149-165</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of gauging the influence exerted by a given country on
the global trade market from the viewpoint of complex networks. In particular,
we apply the PWP method for computing indirect influences on the world trade
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7607</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7607</id><created>2014-11-27</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Huang</keyname><forenames>Liling</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Universal Compression of a Mixture of Parametric Sources with Side
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the benefits of the side information on the universal
compression of sequences from a mixture of $K$ parametric sources. The output
sequence of the mixture source is chosen from the source $i \in \{1,\ldots
,K\}$ with a $d_i$-dimensional parameter vector at random according to
probability vector $\mathbf{w} = (w_1,\ldots,w_K)$. The average minimax
redundancy of the universal compression of a new random sequence of length $n$
is derived when the encoder and the decoder have a common side information of
$T$ sequences generated independently by the mixture source. Necessary and
sufficient conditions on the distribution $\mathbf{w}$ and the mixture
parameter dimensions $\mathbf{d} = (d_1,\ldots,d_K)$ are determined such that
the side information provided by the previous sequences results in a reduction
in the first-order term of the average codeword length compared with the
universal compression without side information. Further, it is proved that the
optimal compression with side information corresponds to the clustering of the
side information sequences from the mixture source. Then, a clustering
technique is presented to better utilize the side information by classifying
the data sequences from a mixture source. Finally, the performance of the
clustering on the universal compression with side information is validated
using computer simulations on real network data traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7610</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7610</id><created>2014-11-27</created><updated>2015-03-05</updated><authors><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author></authors><title>Learning Stochastic Recurrent Networks</title><categories>stat.ML cs.LG</categories><comments>Submitted to conference track of ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging advances in variational inference, we propose to enhance recurrent
neural networks with latent variables, resulting in Stochastic Recurrent
Networks (STORNs). The model i) can be trained with stochastic gradient
methods, ii) allows structured and multi-modal conditionals at each time step,
iii) features a reliable estimator of the marginal likelihood and iv) is a
generalisation of deterministic recurrent neural networks. We evaluate the
method on four polyphonic musical data sets and motion capture data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7612</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7612</id><created>2014-11-27</created><authors><author><keyname>Chandu</keyname><forenames>Drona Pratap</forenames></author></authors><title>A Parallel Genetic Algorithm for Generalized Vertex Cover Problem</title><categories>cs.DC cs.NE</categories><comments>4 pages, 3 figures, ISSN: 0975-9646. arXiv admin note: substantial
  text overlap with arXiv:1411.4565</comments><journal-ref>International Journal of Computer Science and Information
  Technologies (IJCSIT), Vol. 5 (6) , 2014, 7686-7689</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a parallel genetic algorithm for generalised vertex cover
problem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduce
implementation helps to run the genetic algorithm for generalized vertex cover
problem (GVCP) on multiple machines parallely and computes the solution in
relatively short time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7613</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7613</id><created>2014-11-27</created><updated>2015-05-20</updated><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author></authors><title>Systemic risk analysis in reconstructed economic and financial networks</title><categories>physics.soc-ph cs.SI physics.data-an q-fin.ST</categories><journal-ref>Sci. Rep. 5 (15758) (2015)</journal-ref><doi>10.1038/srep15758</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a fundamental problem that is systematically encountered when
modeling complex systems: the limitedness of the information available. In the
case of economic and financial networks, privacy issues severely limit the
information that can be accessed and, as a consequence, the possibility of
correctly estimating the resilience of these systems to events such as
financial shocks, crises and cascade failures. Here we present an innovative
method to reconstruct the structure of such partially-accessible systems, based
on the knowledge of intrinsic node-specific properties and of the number of
connections of only a limited subset of nodes. This information is used to
calibrate an inference procedure based on fundamental concepts derived from
statistical physics, which allows to generate ensembles of directed weighted
networks intended to represent the real system, so that the real network
properties can be estimated with their average values within the ensemble. Here
we test the method both on synthetic and empirical networks, focusing on the
properties that are commonly used to measure systemic risk. Indeed, the method
shows a remarkable robustness with respect to the limitedness of the
information available, thus representing a valuable tool for gaining insights
on privacy-protected economic and financial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7614</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7614</id><created>2014-11-27</created><authors><author><keyname>Arora</keyname><forenames>Rahul</forenames></author><author><keyname>Gupta</keyname><forenames>Ashu</forenames></author><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Tewari</keyname><forenames>Raghunath</forenames></author></authors><title>Derandomizing Isolation Lemma for $K_{3,3}$-free and $K_5$-free
  Bipartite Graphs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The perfect matching problem has a randomized NC algorithm, using the
celebrated Isolation Lemma of Mulmuley, Vazirani and Vazirani. The Isolation
Lemma states that giving a random weight assignment to the edges of a graph,
ensures that it has a unique minimum weight perfect matching, with a good
probability. We derandomize this lemma for $K_{3,3}$-free and $K_5$-free
bipartite graphs, i.e. we give a deterministic log-space construction of such a
weight assignment for these graphs. Such a construction was known previously
for planar bipartite graphs. Our result implies that the perfect matching
problem for $K_{3,3}$-free and $K_5$-free bipartite graphs is in SPL.
  It also gives an alternate proof for an already known result -- reachability
for $K_{3,3}$-free and $K_5$-free graphs is in UL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7630</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7630</id><created>2014-11-27</created><authors><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Modulated Unit-Norm Tight Frames for Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><journal-ref>Signal Processing, IEEE Transactions on , vol.63, no.15,
  pp.3974-3985, Aug.1, 2015</journal-ref><doi>10.1109/TSP.2015.2425809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a compressed sensing (CS) framework that consists
of three parts: a unit-norm tight frame (UTF), a random diagonal matrix and a
column-wise orthonormal matrix. We prove that this structure satisfies the
restricted isometry property (RIP) with high probability if the number of
measurements $m = O(s \log^2s \log^2n)$ for $s$-sparse signals of length $n$
and if the column-wise orthonormal matrix is bounded. Some existing structured
sensing models can be studied under this framework, which then gives tighter
bounds on the required number of measurements to satisfy the RIP. More
importantly, we propose several structured sensing models by appealing to this
unified framework, such as a general sensing model with arbitrary/determinisic
subsamplers, a fast and efficient block compressed sensing scheme, and
structured sensing matrices with deterministic phase modulations, all of which
can lead to improvements on practical applications. In particular, one of the
constructions is applied to simplify the transceiver design of CS-based channel
estimation for orthogonal frequency division multiplexing (OFDM) systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7631</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7631</id><created>2014-11-27</created><updated>2015-11-16</updated><authors><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Approximate Undirected Maximum Flows in O(m polylog(n)) Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first O(m polylog(n)) time algorithms for approximating maximum
flows in undirected graphs and constructing polylog(n) -quality
cut-approximating hierarchical tree decompositions. Our algorithm invokes
existing algorithms for these two problems recursively while gradually
incorporating size reductions. These size reductions are in turn obtained via
ultra-sparsifiers, which are key tools in solvers for symmetric diagonally
dominant (SDD) linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7632</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7632</id><created>2014-11-27</created><updated>2015-08-24</updated><authors><author><keyname>Tanaka</keyname><forenames>Takashi</forenames></author><author><keyname>Kim</keyname><forenames>Kwang-Ki K.</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy K.</forenames></author></authors><title>Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion
  Trade-offs</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential rate-distortion (SRD) theory is concerned with the fundamental
trade-off between data-rate and data-quality in zero-delay transmission of
stochastic source signals. In this paper, we consider an SRD trade-off problem
for a multi-dimensional and time-varying Gauss-Markov source with a mean-square
distortion constraint. We first revisit the &quot;sensor-estimator separation
principle,&quot; which asserts that a Gaussian SRD problem is equivalent to the
problem of designing the best linear sensor mechanism of the form {y_t=C_t
x_t+v_t}, and computing the least mean-square error estimate of {x_t} given the
history of {y_t} based on the Kalman filter. We then show that the optimal
matrix-valued signal-to-noise ratio the sensor mechanism can be efficiently
computed by semidefinite programming (SDP). This allows us to obtain an
SDP-based solution synthesis procedure for the Gaussian SRD problem and a
semidefinite representation of the corresponding SRD function, whose analytical
expression in the general case is not known in the literature. We also consider
applications of SRD theory in real-time communication theory and networked
control theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7639</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7639</id><created>2014-11-27</created><authors><author><keyname>Narkhede</keyname><forenames>Sayalee</forenames></author><author><keyname>Baraskar</keyname><forenames>Trupti</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Analyzing Web Application Log Files to Find Hit Count Through the
  Utilization of Hadoop MapReduce in Cloud Computing Environment</title><categories>cs.DC</categories><comments>6 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce has been widely applied in various fields of data and compute
intensive applications and also it is important programming model for cloud
computing. Hadoop is an open-source implementation of MapReduce which operates
on terabytes of data using commodity hardware. We have applied this Hadoop
MapReduce programming model for analyzing web log files so that we could get
hit count of specific web application. This system uses Hadoop file system to
store log file and results are evaluated using Map and Reduce function.
Experimental results show hit count for each field in log file. Also due to
MapReduce runtime parallelization response time is reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7640</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7640</id><created>2014-11-27</created><updated>2014-12-02</updated><authors><author><keyname>Mall</keyname><forenames>Raghvendra</forenames></author><author><keyname>Langone</keyname><forenames>Rocco</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Multilevel Hierarchical Kernel Spectral Clustering for Real-Life Large
  Scale Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>PLOS ONE, Vol 9, Issue 6, June 2014</comments><doi>10.1371/journal.pone.0099966</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel spectral clustering corresponds to a weighted kernel principal
component analysis problem in a constrained optimization framework. The primal
formulation leads to an eigen-decomposition of a centered Laplacian matrix at
the dual level. The dual formulation allows to build a model on a
representative subgraph of the large scale network in the training phase and
the model parameters are estimated in the validation stage. The KSC model has a
powerful out-of-sample extension property which allows cluster affiliation for
the unseen nodes of the big data network. In this paper we exploit the
structure of the projections in the eigenspace during the validation stage to
automatically determine a set of increasing distance thresholds. We use these
distance thresholds in the test phase to obtain multiple levels of hierarchy
for the large scale network. The hierarchical structure in the network is
determined in a bottom-up fashion. We empirically showcase that real-world
networks have multilevel hierarchical organization which cannot be detected
efficiently by several state-of-the-art large scale hierarchical community
detection techniques like the Louvain, OSLOM and Infomap methods. We show a
major advantage our proposed approach i.e. the ability to locate good quality
clusters at both the coarser and finer levels of hierarchy using internal
cluster quality metrics on 7 real-life networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7647</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7647</id><created>2014-11-27</created><authors><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Magic coins are useful for small-space quantum machines</title><categories>cs.CC cs.FL quant-ph</categories><comments>16 pages!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although polynomial-time probabilistic Turing machines can utilize
uncomputable transition probabilities to recognize uncountably many languages
with bounded error when allowed to use logarithmic space, it is known that such
&quot;magic coins&quot; give no additional computational power to constant-space versions
of those machines. We show that adding a few quantum bits to the model changes
the picture dramatically. For every language $L$, there exists such a two-way
quantum finite automaton that recognizes a language of the same Turing degree
as $L$ with bounded error in polynomial time. When used as verifiers in
public-coin interactive proof systems, such automata can verify membership in
all languages with bounded error, outperforming their classical counterparts,
which are known to fail for the palindromes language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7655</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7655</id><created>2014-11-27</created><authors><author><keyname>Omari</keyname><forenames>Mounir</forenames></author><author><keyname>Hassouni</keyname><forenames>Mohammed El</forenames></author><author><keyname>Abdelouahad</keyname><forenames>Abdelkaher Ait</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author></authors><title>A statistical reduced-reference method for color image quality
  assessment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although color is a fundamental feature of human visual perception, it has
been largely unexplored in the reduced-reference (RR) image quality assessment
(IQA) schemes. In this paper, we propose a natural scene statistic (NSS)
method, which efficiently uses this information. It is based on the statistical
deviation between the steerable pyramid coefficients of the reference color
image and the degraded one. We propose and analyze the multivariate generalized
Gaussian distribution (MGGD) to model the underlying statistics. In order to
quantify the degradation, we develop and evaluate two measures based
respectively on the Geodesic distance between two MGGDs and on the closed-form
of the Kullback Leibler divergence. We performed an extensive evaluation of
both metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID
2008 benchmark and the FRTV Phase I validation process. Experimental results
demonstrate the effectiveness of the proposed framework to achieve a good
consistency with human visual perception. Furthermore, the best configuration
is obtained with CIELAB color space associated to KLD deviation measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7658</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7658</id><created>2014-11-27</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Waliullah</keyname><forenames>Md.</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Sadekur</forenames></author></authors><title>A High Availability Clusters Model Combined with Load Balancing and
  Shared Storage Technologies for Web Servers</title><categories>cs.DC</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1311.3070 by other
  authors</comments><journal-ref>International Journal of Scientific &amp; Engineering Research, Volume
  5, Issue 12, December-2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper designs and implements a high availability clusters and
incorporated with load balance infrastructure of web servers. The paper
described system can provide full facilities to the website hosting provider
and large business organizations. This system can provide continuous service
though any system components fail uncertainly with the help of Linux Virtual
Server (LVS) loadbalancing cluster technology and combined with virtualization
as well as shared storage technology to achieve the three-tier architecture of
Web server clusters. This technology not only improves availability, but also
affects the security and performance of the application services being
requested. Benefits of the system include node failover overcome; network
failover overcome; storage limitation overcome and load distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7662</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7662</id><created>2014-11-27</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A. B. M.</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Sadekur</forenames></author></authors><title>Analysis of Topology Based Routing Protocols for Vehicular Ad-Hoc
  Network (VANET)</title><categories>cs.NI</categories><comments>09 pages, available in International Journal of Computer Applications
  (IJCA) Volume 107 December 2014. arXiv admin note: text overlap with
  arXiv:1204.1201 by other authors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Now-a-days vehicles are one of the most important parts of our life. We need
them to cross distances in our everyday life. In this paper we discuss
Vehicular AdHoc Network (VANET) technology that can ensure the maintenance of
traffic rules and regulation. By applying this technology we can save life,
save time, corruption, vehicle security, avoid collision and so on. Vehicular
Ad Hoc Network (VANET) is a part of Mobile Ad Hoc Network (MANET). Every node
or vehicle can move freely and they will communicate each other by wireless
technology in coverage. The main goal of this research is to study the existing
routing protocols for ad-hoc network system and compared between AODV
(Reactive) and DSDV (Proactive). We have studied different types of routing
protocols such as topology based, position based, cluster based, geo-cast based
and broadcast based. We have simulated and compared AODV (Reactive) and DSDV
(Proactive) to find out their efficiency and detect their flaws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7663</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7663</id><created>2014-11-27</created><authors><author><keyname>Schmidt</keyname><forenames>Stephan</forenames></author></authors><title>A Two Stage CVT / Eikonal Convection Mesh Deformation Approach for Large
  Nodal Deformations</title><categories>cs.NA math.NA</categories><msc-class>65N50, 65DXX, 49Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two step mesh deformation approach for large nodal deformations, typically
arising from non-parametric shape optimization, fluid-structure interaction or
computer graphics, is considered. Two major difficulties, collapsed cells and
an undesirable parameterization, are overcome by considering a special form of
ray tracing paired with a centroid Voronoi reparameterization. The ray
direction is computed by solving an Eikonal equation. With respect to the
Hadamard form of the shape derivative, both steps are within the kernel of the
objective and have no negative impact on the minimizer. The paper concludes
with applications in 2D and 3D fluid dynamics and automatic code generation and
manages to solve these problems without any remeshing. The methodology is
available as a FEniCS shape optimization add-on at
http://www.mathematik.uni-wuerzburg.de/~schmidt/femorph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7666</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7666</id><created>2014-11-27</created><authors><author><keyname>Lu</keyname><forenames>Steven</forenames></author></authors><title>No Quantum Brooks' Theorem</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, I introduce quantum graph theory. I also discuss a known lower bound
on the independence numbers and derive from it an upper bound on the chromatic
numbers of quantum graphs. Then, I construct a family of quantum graphs that
can be described as tropical, cyclical, and commutative. I also define a step
logarithm function and express with it the bounds on quantum graph invariants
in closed form. Finally, I obtain an upper bound on the independence numbers
and a lower bound on the chromatic numbers of the constructed quantum graphs
that are similar in form to the existing bounds. I also show that the
constructed family contains graphs of any valence with arbitrarily high
chromatic numbers and conclude by it that quantum graph colorings are quite
different from classical graph colorings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7676</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7676</id><created>2014-11-27</created><updated>2016-02-29</updated><authors><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author></authors><title>Visual Representations: Definint Properties and Deep Approximations</title><categories>cs.CV</categories><comments>UCLA CSD TR140023, Nov. 12, 2014, revised April 13, 2015, November
  13, 2015, February 28, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual representations are defined in terms of minimal sufficient statistics
of visual data, for a class of tasks, that are also invariant to nuisance
variability. Minimal sufficiency guarantees that we can store a representation
in lieu of raw data with smallest complexity and no performance loss on the
task at hand. Invariance guarantees that the statistic is constant with respect
to uninformative transformations of the data. We derive analytical expressions
for such representations and show they are related to feature descriptors
commonly used in computer vision, as well as to convolutional neural networks.
This link highlights the assumptions and approximations tacitly assumed by
these methods and explains empirical practices such as clamping, pooling and
joint normalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7682</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7682</id><created>2014-11-27</created><authors><author><keyname>Omari</keyname><forenames>Mounir</forenames></author><author><keyname>Hassouni</keyname><forenames>Mohammed El</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author><author><keyname>Abdelouahad</keyname><forenames>Abdelkaher Ait</forenames></author></authors><title>On color image quality assessment using natural image statistics</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color distortion can introduce a significant damage in visual quality
perception, however, most of existing reduced-reference quality measures are
designed for grayscale images. In this paper, we consider a basic extension of
well-known image-statistics based quality assessment measures to color images.
In order to evaluate the impact of color information on the measures
efficiency, two color spaces are investigated: RGB and CIELAB. Results of an
extensive evaluation using TID 2013 benchmark demonstrates that significant
improvement can be achieved for a great number of distortion type when the
CIELAB color representation is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7711</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7711</id><created>2014-11-27</created><updated>2015-09-01</updated><authors><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Cascone</keyname><forenames>Carmelo</forenames></author><author><keyname>Nguyen</keyname><forenames>Alessandro Q. T.</forenames></author><author><keyname>Sans&#xf2;</keyname><forenames>Brunilde</forenames></author></authors><title>Detour Planning for Fast and Reliable Failure Recovery in SDN with
  OpenState</title><categories>cs.NI</categories><comments>8 pages, pre-print, Design of Reliable Communication Networks (DRCN),
  2015 11th International Conference on the</comments><doi>10.1109/DRCN.2015.7148981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reliable and scalable mechanism to provide protection against a link or
node failure has additional requirements in the context of SDN and OpenFlow.
Not only it has to minimize the load on the controller, but it must be able to
react even when the controller is unreachable. In this paper we present a
protection scheme based on precomputed backup paths and inspired by MPLS
crankback routing, that guarantees instantaneous recovery times and aims at
zero packet-loss after failure detection, regardless of controller
reachability, even when OpenFlow's &quot;fast-failover&quot; feature cannot be used. The
proposed mechanism is based on OpenState, an OpenFlow extension that allows a
programmer to specify how forwarding rules should autonomously adapt in a
stateful fashion, reducing the need to rely on remote controllers. We present
the scheme as well as two different formulations for the computation of backup
paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7714</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7714</id><created>2014-11-27</created><authors><author><keyname>Leordeanu</keyname><forenames>Marius</forenames></author><author><keyname>Radu</keyname><forenames>Alexandra</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author></authors><title>Features in Concert: Discriminative Feature Selection meets Unsupervised
  Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is an essential problem in computer vision, important for
category learning and recognition. Along with the rapid development of a wide
variety of visual features and classifiers, there is a growing need for
efficient feature selection and combination methods, to construct powerful
classifiers for more complex and higher-level recognition tasks. We propose an
algorithm that efficiently discovers sparse, compact representations of input
features or classifiers, from a vast sea of candidates, with important
optimality properties, low computational cost and excellent accuracy in
practice. Different from boosting, we start with a discriminant linear
classification formulation that encourages sparse solutions. Then we obtain an
equivalent unsupervised clustering problem that jointly discovers ensembles of
diverse features. They are independently valuable but even more powerful when
united in a cluster of classifiers. We evaluate our method on the task of
large-scale recognition in video and show that it significantly outperforms
classical selection approaches, such as AdaBoost and greedy forward-backward
selection, and powerful classifiers such as SVMs, in speed of training and
performance, especially in the case of limited training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7715</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7715</id><created>2014-11-27</created><authors><author><keyname>Rozantsev</keyname><forenames>Artem</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Flying Objects Detection from a Single Moving Camera</title><categories>cs.CV</categories><doi>10.1109/CVPR.2015.7299040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to detect flying objects such as UAVs and aircrafts
when they occupy a small portion of the field of view, possibly moving against
complex backgrounds, and are filmed by a camera that itself moves.
  Solving such a difficult problem requires combining both appearance and
motion cues. To this end we propose a regression-based approach to motion
stabilization of local image patches that allows us to achieve effective
classification on spatio-temporal image cubes and outperform state-of-the-art
techniques.
  As the problem is relatively new, we collected two challenging datasets for
UAVs and Aircrafts, which can be used as benchmarks for flying objects
detection and vision-guided collision avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7716</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7716</id><created>2014-11-27</created><updated>2015-08-31</updated><authors><author><keyname>Sahu</keyname><forenames>Anit Kumar</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author></authors><title>Distributed Sequential Detection for Gaussian Shift-in-Mean Hypothesis
  Testing</title><categories>math.OC cs.SY</categories><comments>37 pages. Submitted for journal publication. Initial submission: Nov.
  2014. Revised: Aug. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of sequential Gaussian shift-in-mean
hypothesis testing in a distributed multi-agent network. A sequential
probability ratio test (SPRT) type algorithm in a distributed framework of the
\emph{consensus}+\emph{innovations} form is proposed, in which the agents
update their decision statistics by simultaneously processing latest
observations (innovations) sensed sequentially over time and information
obtained from neighboring agents (consensus). For each pre-specified set of
type I and type II error probabilities, local decision parameters are derived
which ensure that the algorithm achieves the desired error performance and
terminates in finite time almost surely (a.s.) at each network agent. Large
deviation exponents for the tail probabilities of the agent stopping time
distributions are obtained and it is shown that asymptotically (in the number
of agents or in the high signal-to-noise-ratio regime) these exponents
associated with the distributed algorithm approach that of the optimal
centralized detector. The expected stopping time for the proposed algorithm at
each network agent is evaluated and is benchmarked with respect to the optimal
centralized algorithm. The efficiency of the proposed algorithm in the sense of
the expected stopping times is characterized in terms of network connectivity.
Finally, simulation studies are presented which illustrate and verify the
analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7717</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7717</id><created>2014-11-27</created><updated>2015-01-22</updated><authors><author><keyname>Martens</keyname><forenames>James</forenames></author><author><keyname>Medabalimi</keyname><forenames>Venkatesh</forenames></author></authors><title>On the Expressive Efficiency of Sum Product Networks</title><categories>cs.LG stat.ML</categories><comments>Various minor revisions and corrections throughout</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sum Product Networks (SPNs) are a recently developed class of deep generative
models which compute their associated unnormalized density functions using a
special type of arithmetic circuit. When certain sufficient conditions, called
the decomposability and completeness conditions (or &quot;D&amp;C&quot; conditions), are
imposed on the structure of these circuits, marginal densities and other useful
quantities, which are typically intractable for other deep generative models,
can be computed by what amounts to a single evaluation of the network (which is
a property known as &quot;validity&quot;). However, the effect that the D&amp;C conditions
have on the capabilities of D&amp;C SPNs is not well understood.
  In this work we analyze the D&amp;C conditions, expose the various connections
that D&amp;C SPNs have with multilinear arithmetic circuits, and consider the
question of how well they can capture various distributions as a function of
their size and depth. Among our various contributions is a result which
establishes the existence of a relatively simple distribution with fully
tractable marginal densities which cannot be efficiently captured by D&amp;C SPNs
of any depth, but which can be efficiently captured by various other deep
generative models. We also show that with each additional layer of depth
permitted, the set of distributions which can be efficiently captured by D&amp;C
SPNs grows in size. This kind of &quot;depth hierarchy&quot; property has been widely
conjectured to hold for various deep models, but has never been proven for any
of them. Some of our other contributions include a new characterization of the
D&amp;C conditions as sufficient and necessary ones for a slightly strengthened
notion of validity, and various state-machine characterizations of the types of
computations that can be performed efficiently by D&amp;C SPNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7718</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7718</id><created>2014-11-27</created><updated>2015-07-18</updated><authors><author><keyname>Liu</keyname><forenames>Tongliang</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Classification with Noisy Labels by Importance Reweighting</title><categories>stat.ML cs.LG</categories><doi>10.1109/TPAMI.2015.2456899</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a classification problem in which sample labels are
randomly corrupted. In this scenario, there is an unobservable sample with
noise-free labels. However, before being observed, the true labels are
independently flipped with a probability $\rho\in[0,0.5)$, and the random label
noise can be class-conditional. Here, we address two fundamental problems
raised by this scenario. The first is how to best use the abundant surrogate
loss functions designed for the traditional classification problem when there
is label noise. We prove that any surrogate loss function can be used for
classification with noisy labels by using importance reweighting, with
consistency assurance that the label noise does not ultimately hinder the
search for the optimal classifier of the noise-free sample. The other is the
open problem of how to obtain the noise rate $\rho$. We show that the rate is
upper bounded by the conditional probability $P(y|x)$ of the noisy sample.
Consequently, the rate can be estimated, because the upper bound can be easily
reached in classification problems. Experimental results on synthetic and real
datasets confirm the efficiency of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7719</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7719</id><created>2014-11-27</created><updated>2015-08-10</updated><authors><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author></authors><title>Growing Self-organized Design of Efficient and Robust Complex Networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>10 pages, 14 figures, 3 tables, Proc. of 2014 IEEE 8th Int. Conf. on
  Self-Adaptive and Self-Organizing Systems, pp.50-59.
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7001000&amp;tag=1. IEEE
  Xplore Digital Library 2014</comments><doi>10.1109/SASO.2014.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-organization of efficient and robust networks is important for a
future design of communication or transportation systems, however both
characteristics are incompatible in many real networks. Recently, it has been
found that the robustness of onion-like structure with positive degree-degree
correlations is optimal against intentional attacks. We show that, by
biologically inspired copying, an onion-like network emerges in the incremental
growth with functions of proxy access and reinforced connectivity on a space.
The proposed network consists of the backbone of tree-like structure by
copyings and the periphery by adding shortcut links between low degree nodes to
enhance the connectivity. It has the fine properties of the statistically
self-averaging unlike the conventional duplication-divergence model,
exponential-like degree distribution without overloaded hubs, strong robustness
against both malicious attacks and random failures, and the efficiency with
short paths counted by the number of hops as mediators and by the Euclidean
distances. The adaptivity to heal over and to recover the performance of
networking is also discussed for a change of environment in such disasters or
battlefields on a geographical map. These properties will be useful for a
resilient and scalable infrastructure of network systems even in emergent
situations or poor environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7726</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7726</id><created>2014-11-27</created><authors><author><keyname>Zbieg</keyname><forenames>Anita</forenames></author><author><keyname>Zak</keyname><forenames>Blazej</forenames></author><author><keyname>Jankowski</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Michalski</keyname><forenames>Radoslaw</forenames></author><author><keyname>Ciuberek</keyname><forenames>Sylwia</forenames></author></authors><title>Studying Diffusion of Viral Content at Dyadic Level</title><categories>cs.SI physics.soc-ph</categories><comments>ASONAM 2012, The 2012 IEEE/ACM International Conference on Advances
  in Social Networks Analysis and Mining. IEEE Computer Society, pp. 1291-1297</comments><doi>10.1109/ASONAM.2012.217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of information and viral content, social contagion and influence
are still topics of broad evaluation. As theory explaining the role of
influentials moves slightly to reduce their importance in the propagation of
viral content, authors of the following paper have studied the information
epidemic in a social networking platform in order to confirm recent theoretical
findings in this area. While most of related experiments focus on the level of
individuals, the elementary entities of the following analysis are dyads. The
authors study behavioral motifs that are possible to observe at the dyadic
level. The study shows significant differences between dyads that are more vs
less engaged in the diffusion process. Dyads that fuel the diffusion proccess
are characterized by stronger relationships (higher activity, more common
friends), more active and networked receiving party (higher centrality
measures), and higher authority centrality of person sending a viral message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7727</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7727</id><created>2014-11-27</created><authors><author><keyname>Zak</keyname><forenames>Blazej</forenames></author><author><keyname>Zbieg</keyname><forenames>Anita</forenames></author></authors><title>Model for simulating mechanisms responsible of similarities between
  people connected in networks of social relations</title><categories>cs.SI physics.soc-ph</categories><comments>Working paper, presented and discussed on The 3rd International
  Conference on Social Informatics SocInfo, Singapore Management University,
  Singapore, 06-08.10.2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It the literature have been identified three social mechanisms explaining the
similarity between people connected in the network of social relations
homophily, confounding and social contagion. The article proposes a simple
model for simulating mechanisms responsible for similarity of attitudes in
networks of social relations; along with a measure that is able to indicate
which of the three mechanisms has taken major role in the process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7747</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7747</id><created>2014-11-27</created><updated>2015-04-14</updated><authors><author><keyname>Bhangale</keyname><forenames>Amey</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Varma</keyname><forenames>Girish</forenames></author></authors><title>A Characterization of hard-to-cover CSPs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the study of covering complexity of constraint satisfaction
problems (CSPs) initiated by Guruswami, Hastad and Sudan [SIAM J. Computing,
31(6):1663--1686, 2002] and Dinur and Kol [In Proc. $28$th IEEE Conference on
Computational Complexity, 2013]. The covering number of a CSP instance $\Phi$,
denoted by $\nu(\Phi)$ is the smallest number of assignments to the variables
of $\Phi$, such that each constraint of $\Phi$ is satisfied by at least one of
the assignments. We show the following results regarding how well efficient
algorithms can approximate the covering number of a given CSP instance.
  - Assuming a covering unique games conjecture, introduced by Dinur and Kol,
we show that for every non-odd predicate $P$ over any constant sized alphabet
and every integer $K$, it is NP-hard to distinguish between $P$-CSP instances
(i.e., CSP instances where all the constraints are of type $P$) which are
coverable by a constant number of assignments and those whose covering number
is at least $K$. Previously, Dinur and Kol, using the same covering unique
games conjecture, had shown a similar hardness result for every non-odd
predicate over the Boolean alphabet that supports a pairwise independent
distribution. Our generalization yields a complete characterization of CSPs
over constant sized alphabet $\Sigma$ that are hard to cover since CSP's over
odd predicates are trivially coverable with $|\Sigma|$ assignments.
  - For a large class of predicates that are contained in the $2k$-LIN
predicate, we show that it is quasi-NP-hard to distinguish between instances
which have covering number at most two and covering number at least
$\Omega(\log\log n)$. This generalizes the $4$-LIN result of Dinur and Kol that
states it is quasi-NP-hard to distinguish between $4$-LIN-CSP instances which
have covering number at most two and covering number at least $\Omega(\log
\log\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7753</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7753</id><created>2014-11-28</created><authors><author><keyname>Bajaj</keyname><forenames>Chandrajit</forenames></author><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Eshan</forenames></author><author><keyname>Zuckerman</keyname><forenames>David</forenames></author></authors><title>On Low Discrepancy Samplings in Product Spaces of Motion Groups</title><categories>cs.CG</categories><msc-class>52CXX (Primary) 68Q25, 68W01 (Secondary)</msc-class><acm-class>I.3.5; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministically generating near-uniform point samplings of the motion
groups like SO(3), SE(3) and their n-wise products SO(3)^n, SE(3)^n is
fundamental to numerous applications in computational and data sciences. The
natural measure of sampling quality is discrepancy. In this work, our main goal
is construct low discrepancy deterministic samplings in product spaces of the
motion groups. To this end, we develop a novel strategy (using a two-step
discrepancy construction) that leads to an almost exponential improvement in
size (from the trivial direct product). To the best of our knowledge, this is
the first nontrivial construction for SO(3)^n, SE(3)^n and the hypertorus T^n.
  We also construct new low discrepancy samplings of S^2 and SO(3). The central
component in our construction for SO(3) is an explicit construction of N points
in S^2 with discrepancy \tilde{\O}(1/\sqrt{N}) with respect to convex sets,
matching the bound achieved for the special case of spherical caps in
\cite{ABD_12}. We also generalize the discrepancy of Cartesian product sets
\cite{Chazelle04thediscrepancy} to the discrepancy of local Cartesian product
sets.
  The tools we develop should be useful in generating low discrepancy samplings
of other complicated geometric spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7756</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7756</id><created>2014-11-28</created><authors><author><keyname>Shukla</keyname><forenames>Samiksha</forenames></author><author><keyname>Sadashivappa</keyname><forenames>G.</forenames></author><author><keyname>Mishra</keyname><forenames>Durgesh Kumar</forenames></author></authors><title>Simulation of Collision Resistant Secure Sum Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  secure multi-party computation is widely studied area in computer science. It
is touching all most every aspect of human life. This paper demonstrates
theoretical and experimental results of one of the secure multi-party
computation protocols proposed by Shukla et al. implemented using visual C++.
Data outflow probability is computed by changing parameters. At the end, time
and space complexity is calculated using theoretical and experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7766</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7766</id><created>2014-11-28</created><updated>2015-09-24</updated><authors><author><keyname>Liu</keyname><forenames>Ziwei</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Deep Learning Face Attributes in the Wild</title><categories>cs.CV</categories><comments>To appear in International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting face attributes in the wild is challenging due to complex face
variations. We propose a novel deep learning framework for attribute prediction
in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly
with attribute tags, but pre-trained differently. LNet is pre-trained by
massive general object categories for face localization, while ANet is
pre-trained by massive face identities for attribute prediction. This framework
not only outperforms the state-of-the-art with a large margin, but also reveals
valuable facts on learning face representation.
  (1) It shows how the performances of face localization (LNet) and attribute
prediction (ANet) can be improved by different pre-training strategies.
  (2) It reveals that although the filters of LNet are fine-tuned only with
image-level attribute tags, their response maps over entire images have strong
indication of face locations. This fact enables training LNet for face
localization with only image-level annotations, but without face bounding boxes
or landmarks, which are required by all attribute recognition works.
  (3) It also demonstrates that the high-level hidden neurons of ANet
automatically discover semantic concepts after pre-training with massive face
identities, and such concepts are significantly enriched after fine-tuning with
attribute tags. Each attribute can be well explained with a sparse linear
combination of these concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7783</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7783</id><created>2014-11-28</created><updated>2015-02-02</updated><authors><author><keyname>Valpola</keyname><forenames>Harri</forenames></author></authors><title>From neural PCA to deep unsupervised learning</title><categories>stat.ML cs.LG cs.NE</categories><comments>A revised version of an article that has been accepted for
  publication in Advances in Independent Component Analysis and Learning
  Machines (2015), edited by Ella Bingham, Samuel Kaski, Jorma Laaksonen and
  Jouko Lampinen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network supporting deep unsupervised learning is presented. The network is
an autoencoder with lateral shortcut connections from the encoder to decoder at
each level of the hierarchy. The lateral shortcut connections allow the higher
levels of the hierarchy to focus on abstract invariant features. While standard
autoencoders are analogous to latent variable models with a single layer of
stochastic variables, the proposed network is analogous to hierarchical latent
variables models. Learning combines denoising autoencoder and denoising sources
separation frameworks. Each layer of the network contributes to the cost
function a term which measures the distance of the representations produced by
the encoder and the decoder. Since training signals originate from all levels
of the network, all layers can learn efficiently even in deep networks. The
speedup offered by cost terms from higher levels of the hierarchy and the
ability to learn invariant features are demonstrated in experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7785</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7785</id><created>2014-11-28</created><updated>2015-03-19</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Jovanovic</keyname><forenames>Miodrag</forenames></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames></author></authors><title>Performance laws of large heterogeneous cellular networks</title><categories>cs.NI math.PR</categories><doi>10.1109/WIOPT.2015.7151124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model for heterogeneous cellular networks assuming a space-time
Poisson process of call arrivals, independently marked by data volumes, and
served by different types of base stations (having different transmission
powers) represented by the superposition of independent Poisson processes on
the plane. Each station applies a processor sharing policy to serve users
arriving in its vicinity, modeled by the Voronoi cell perturbed by some random
signal propagation effects (shadowing). Users' peak service rates depend on
their signal-to-interference-and-noise ratios (SINR) with respect to the
serving station. The mutual-dependence of the cells (due to the extra-cell
interference) is captured via some system of cell-load equations impacting the
spatial distribution of the SINR. We use this model to study in a semi-analytic
way (involving only static simulations, with the temporal evolution handled by
the queuing theoretic results) network performance metrics (cell loads, mean
number of users) and the quality of service perceived by the users (mean
throughput) served by different types of base stations. Our goal is to identify
macroscopic laws regarding these performance metrics, involving averaging both
over time and the network geometry. The reveled laws are validated against real
field measurement in an operational network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7798</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7798</id><created>2014-11-28</created><authors><author><keyname>He</keyname><forenames>Ran</forenames></author><author><keyname>Zhang</keyname><forenames>Man</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Ji</keyname><forenames>Ye</forenames></author><author><keyname>Yin</keyname><forenames>Qiyue</forenames></author></authors><title>Cross-Modal Learning via Pairwise Constraints</title><categories>cs.CV</categories><comments>12 pages, 5 figures, 70 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multimedia applications, the text and image components in a web document
form a pairwise constraint that potentially indicates the same semantic
concept. This paper studies cross-modal learning via the pairwise constraint,
and aims to find the common structure hidden in different modalities. We first
propose a compound regularization framework to deal with the pairwise
constraint, which can be used as a general platform for developing cross-modal
algorithms. For unsupervised learning, we propose a cross-modal subspace
clustering method to learn a common structure for different modalities. For
supervised learning, to reduce the semantic gap and the outliers in pairwise
constraints, we propose a cross-modal matching method based on compound ?21
regularization along with an iteratively reweighted algorithm to find the
global optimum. Extensive experiments demonstrate the benefits of joint text
and image modeling with semantically induced pairwise constraints, and show
that the proposed cross-modal methods can further reduce the semantic gap
between different modalities and improve the clustering/retrieval accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7801</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7801</id><created>2014-11-28</created><updated>2015-10-26</updated><authors><author><keyname>Soodhalter</keyname><forenames>Kirk M.</forenames></author></authors><title>Stagnation of block GMRES and its relationship to block FOM</title><categories>math.NA cs.NA</categories><comments>29 pages, 4 figures</comments><msc-class>65F10, 65F50, 65F08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the the convergence behavior of block GMRES and block FOM and
characterize the phenomenon of stagnation in block GMRES. Stagnation is then
related to the behavior of the block FOM method. Following from [Brown, SIAM J.
Sci. Statist. Comput., '91], we generalize the block FOM method to generate
well-defined approximations in the case that block FOM would normally break
down, and these generalized solutions are used in our analysis. Our analysis
covers both the cases of normal iterations as well as block Arnoldi breakdown.
Two toy numerical examples are given to illustrate what we have proven, and we
also apply both block methods to a small application problem to demonstrate the
validity of the analysis in a less non-pathological case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7803</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7803</id><created>2014-11-28</created><authors><author><keyname>Han</keyname><forenames>Aaron L. -F.</forenames></author><author><keyname>Wong</keyname><forenames>Derek F.</forenames></author><author><keyname>Chao</keyname><forenames>Lidia S.</forenames></author></authors><title>Password Cracking and Countermeasures in Computer Security: A Survey</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of internet technologies, social networks, and
other related areas, user authentication becomes more and more important to
protect the data of the users. Password authentication is one of the widely
used methods to achieve authentication for legal users and defense against
intruders. There have been many password cracking methods developed during the
past years, and people have been designing the countermeasures against password
cracking all the time. However, we find that the survey work on the password
cracking research has not been done very much. This paper is mainly to give a
brief review of the password cracking methods, import technologies of password
cracking, and the countermeasures against password cracking that are usually
designed at two stages including the password design stage (e.g. user
education, dynamic password, use of tokens, computer generations) and after the
design (e.g. reactive password checking, proactive password checking, password
encryption, access control). The main objective of this work is offering the
abecedarian IT security professionals and the common audiences with some
knowledge about the computer security and password cracking, and promoting the
development of this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7806</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7806</id><created>2014-11-28</created><authors><author><keyname>Bajer</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Hole&#x148;a</keyname><forenames>Martin</forenames></author></authors><title>Two Gaussian Approaches to Black-Box Optomization</title><categories>cs.NE cs.AI</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Outline of several strategies for using Gaussian processes as surrogate
models for the covariance matrix adaptation evolution strategy (CMA-ES).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7812</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7812</id><created>2014-11-28</created><authors><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Elections with Few Voters: Candidate Control Can Be Easy</title><categories>cs.AI cs.GT cs.MA</categories><comments>52 pages. to appear in the Twenty-Ninth AAAI Conference on
  Artificial Intelligence (AAAI-15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of candidate control in elections with
few voters (that is, we take the number of voters as a parameter). We consider
both the standard scenario of adding and deleting candidates, where one asks if
a given candidate can become a winner (or, in the destructive case, can be
precluded from winning) by adding/deleting some candidates, and a combinatorial
scenario where adding/deleting a candidate automatically means adding/deleting
a whole group of candidates. Our results show that the parameterized complexity
of candidate control (with the number of voters as the parameter) is much more
varied than in the setting with many voters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7816</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7816</id><created>2014-11-28</created><authors><author><keyname>G&#xfc;zeltepe</keyname><forenames>Murat</forenames></author></authors><title>Erratum to &quot;Lattice constellation and codes from quadratic number
  fields&quot; [IEEE Trans. Inform. Theory, vol. 47, No. 4, May. 2001]</title><categories>cs.IT math.IT</categories><msc-class>94B05, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We correct a partial mistake for a metric presented in the article &quot;Lattice
constellation and codes from quadratic number fields&quot; [IEEE Trans. Inform.
Theory, vol. 47, No. 4, May. 2001]. We show that the metric defined in the
article is not true, therefore, this brings about to destroy the encoding and
decoding procedures. Also, we define a proper metric for some codes defined in
the article and show that there exist some $1-$error correcting perfect codes
with respect to this new metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7817</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7817</id><created>2014-11-28</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J.</forenames></author><author><keyname>Ziehe</keyname><forenames>Andreas</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author></authors><title>Learning with Algebraic Invariances, and the Invariant Kernel Trick</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When solving data analysis problems it is important to integrate prior
knowledge and/or structural invariances. This paper contributes by a novel
framework for incorporating algebraic invariance structure into kernels. In
particular, we show that algebraic properties such as sign symmetries in data,
phase independence, scaling etc. can be included easily by essentially
performing the kernel trick twice. We demonstrate the usefulness of our theory
in simulations on selected applications such as sign-invariant spectral
clustering and underdetermined ICA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7819</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7819</id><created>2014-11-28</created><updated>2015-12-04</updated><authors><author><keyname>Bishnu</keyname><forenames>Arijit</forenames></author><author><keyname>Desai</keyname><forenames>Sameer</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Paul</keyname><forenames>Subhabrata</forenames></author></authors><title>Uniformity of point samples in metric spaces using gap ratio</title><categories>cs.CG</categories><comments>31 pages, 10 figures</comments><msc-class>52C99, 68Q25, 68R99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teramoto et al. defined a new measure called the gap ratio that measures the
uniformity of a finite point set sampled from $\cal S$, a bounded subset of
$\mathbb{R}^2$. We generalize this definition of measure over all metric spaces
by appealing to covering and packing radius. The definition of gap ratio needs
only a metric unlike discrepancy, a widely used uniformity measure, that
depends on the notion of a range space and its volume. We also show some
interesting connections of gap ratio to Delaunay triangulation and discrepancy
in the Euclidean plane. The major focus of this work is on solving optimization
related questions about selecting uniform point samples from metric spaces; the
uniformity being measured using gap ratio. We consider discrete spaces like
graph and set of points in the Euclidean space and continuous spaces like the
unit square and path connected spaces. We deduce lower bounds, prove hardness
and approximation hardness results. We show that a general approximation
algorithm framework gives different approximation ratios for different metric
spaces based on the lower bound we deduce. Apart from the above, we show
existence of coresets for sampling uniform points from the Euclidean space --
for both the static and the streaming case. This leads to a $\left( 1+\epsilon
\right)$-approximation algorithm for uniform sampling from the Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7820</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7820</id><created>2014-11-28</created><authors><author><keyname>Nastase</keyname><forenames>Vivi</forenames></author><author><keyname>Fahrni</keyname><forenames>Angela</forenames></author></authors><title>Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic
  Models and Encyclopedic Knowledge</title><categories>cs.CL</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for coarse-grained cross-lingual alignment of comparable
texts: segments consisting of contiguous paragraphs that discuss the same theme
(e.g. history, economy) are aligned based on induced multilingual topics. The
method combines three ideas: a two-level LDA model that filters out words that
do not convey themes, an HMM that models the ordering of themes in the
collection of documents, and language-independent concept annotations to serve
as a cross-language bridge and to strengthen the connection between paragraphs
in the same segment through concept relations. The method is evaluated on
English and French data previously used for monolingual alignment. The results
show state-of-the-art performance in both monolingual and cross-lingual
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7825</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7825</id><created>2014-11-28</created><authors><author><keyname>Balbiani</keyname><forenames>Philippe</forenames></author><author><keyname>Herzig</keyname><forenames>Andreas</forenames></author><author><keyname>Schwarzentruber</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Troquard</keyname><forenames>Nicolas</forenames></author></authors><title>DL-PA and DCL-PC: model checking and satisfiability problem are indeed
  in PSPACE</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the model checking and the satisfiability problem of both
Dynamic Logic of Propositional Assignments DL-PA and Coalition Logic of
Propositional Control and Delegation DCL-PC are in PSPACE. We explain why the
proof of EXPTIME-hardness of the model checking problem of DL-PA presented in
(Balbiani, Herzig, Troquard, 2013) is false. We also explain why the proof of
membership in PSPACE of the model checking problem of DCL-PC given in (van der
Hoek, Walther, Wooldridge, 2010) is wrong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7838</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7838</id><created>2014-11-28</created><updated>2015-12-02</updated><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Fafianie</keyname><forenames>Stefan</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>The Complexity of Finding Effectors</title><categories>cs.DS</categories><comments>28 pages</comments><msc-class>05C85, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-hard EFFECTORS problem on directed graphs is motivated by applications
in network mining, particularly concerning the analysis of probabilistic
information-propagation processes in social networks. In the corresponding
model the arcs carry probabilities and there is a probabilistic diffusion
process activating nodes by neighboring activated nodes with probabilities as
specified by the arcs. The point is to explain a given network activation state
as well as possible by using a minimum number of &quot;effector nodes&quot;; these are
selected before the activation process starts.
  We correct, complement, and extend previous work from the data mining
community by a more thorough computational complexity analysis of EFFECTORS,
identifying both tractable and intractable cases. To this end, we also exploit
a parameterization measuring the &quot;degree of randomness&quot; (the number of &quot;really&quot;
probabilistic arcs) which might prove useful for analyzing other probabilistic
network diffusion problems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7839</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7839</id><created>2014-11-28</created><updated>2015-10-28</updated><authors><author><keyname>Dissegna</keyname><forenames>Stefano</forenames></author><author><keyname>Logozzo</keyname><forenames>Francesco</forenames></author><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author></authors><title>An Abstract Interpretation-based Model of Tracing Just-In-Time
  Compilation</title><categories>cs.PL</categories><comments>To appear in ACM Transactions on Programming Languages and Systems</comments><acm-class>D.2.4; D.3.4; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracing just-in-time compilation is a popular compilation technique for the
efficient implementation of dynamic languages, which is commonly used for
JavaScript, Python and PHP. We provide a formal model of tracing JIT
compilation of programs using abstract interpretation. Hot path detection
corresponds to an abstraction of the trace semantics of the program. The
optimization phase corresponds to a transform of the original program that
preserves its trace semantics up to an observation modeled by some abstraction.
We provide a generic framework to express dynamic optimizations and prove them
correct. We instantiate it to prove the correctness of dynamic type
specialization and constant variable folding. We show that our framework is
more general than the model of tracing compilation introduced by Guo and
Palsberg [2011] based on operational bisimulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7855</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7855</id><created>2014-11-28</created><authors><author><keyname>Mendivil</keyname><forenames>Franklin</forenames></author><author><keyname>Stenflo</keyname><forenames>&#xd6;rjan</forenames></author></authors><title>V-variable image compression</title><categories>cs.CV</categories><comments>15 pages, 22 figures</comments><msc-class>28A80, 68U10, 94A08</msc-class><journal-ref>Fractals, 23, no 02, (2015)</journal-ref><doi>10.1142/S0218348X15500073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  V-variable fractals, where $V$ is a positive integer, are intuitively
fractals with at most $V$ different &quot;forms&quot; or &quot;shapes&quot; at all levels of
magnification. In this paper we describe how V-variable fractals can be used
for the purpose of image compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7864</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7864</id><created>2014-11-28</created><authors><author><keyname>Fruergaard</keyname><forenames>Bjarne &#xd8;rum</forenames></author><author><keyname>Herlau</keyname><forenames>Tue</forenames></author></authors><title>Efficient inference of overlapping communities in complex networks</title><categories>stat.ML cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss two views on extending existing methods for complex network
modeling which we dub the communities first and the networks first view,
respectively. Inspired by the networks first view that we attribute to White,
Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochastic
blockmodel (MNSBM), which seeks to separate the observed network into
subnetworks of different types and where the problem of inferring structure in
each subnetwork becomes easier. We show how this model is specified in a
generative Bayesian framework where parameters can be inferred efficiently
using Gibbs sampling. The result is an effective multiple-membership model
without the drawbacks of introducing complex definitions of &quot;groups&quot; and how
they interact. We demonstrate results on the recovery of planted structure in
synthetic networks and show very encouraging results on link prediction
performances using multiple-networks models on a number of real-world network
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7883</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7883</id><created>2014-11-28</created><updated>2015-04-24</updated><authors><author><keyname>Del Pero</keyname><forenames>Luca</forenames></author><author><keyname>Ricco</keyname><forenames>Susanna</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Articulated motion discovery using pairs of trajectories</title><categories>cs.CV</categories><comments>10 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an unsupervised approach for discovering characteristic motion
patterns in videos of highly articulated objects performing natural, unscripted
behaviors, such as tigers in the wild. We discover consistent patterns in a
bottom-up manner by analyzing the relative displacements of large numbers of
ordered trajectory pairs through time, such that each trajectory is attached to
a different moving part on the object. The pairs of trajectories descriptor
relies entirely on motion and is more discriminative than state-of-the-art
features that employ single trajectories. Our method generates temporal video
intervals, each automatically trimmed to one instance of the discovered
behavior, and clusters them by type (e.g., running, turning head, drinking
water). We present experiments on two datasets: dogs from YouTube-Objects and a
new dataset of National Geographic tiger videos. Results confirm that our
proposed descriptor outperforms existing appearance- and trajectory-based
descriptors (e.g., HOG and DTFs) on both datasets and enables us to segment
unconstrained animal video into intervals containing single behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7889</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7889</id><created>2014-11-26</created><authors><author><keyname>Jafarpour</keyname><forenames>Aliakbar</forenames></author></authors><title>Open-source code for manifold-based 3D rotation recovery of X-ray
  scattering patterns</title><categories>physics.optics cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single particle 3D imaging with ultrashort X-ray laser pulses is based on
collecting and combining the information content of 2D scattering patterns of
an object at different orientations. Typical sample-delivery schemes leave
little or no room for controlling the orientations. As such, the orientation
associated with a given snapshot should be estimated after the experiment. Here
we present an open-source code for the most rigorous technique having been
reported in this context. Some practical issues along with proposed solutions
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7895</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7895</id><created>2014-11-28</created><updated>2015-05-25</updated><authors><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Louail</keyname><forenames>Thomas</forenames></author><author><keyname>Cantu-Ros</keyname><forenames>Oliva G.</forenames></author><author><keyname>Picornell</keyname><forenames>Miguel</forenames></author><author><keyname>Herranz</keyname><forenames>Ricardo</forenames></author><author><keyname>Arias</keyname><forenames>Juan Murillo</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author></authors><title>Influence of sociodemographic characteristics on human mobility</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 11 figures + Supplementary information</comments><journal-ref>Scientific Reports 5:10075 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility has been traditionally studied using surveys that deliver
snapshots of population displacement patterns. The growing accessibility to ICT
information from portable digital media has recently opened the possibility of
exploring human behavior at high spatio-temporal resolutions. Mobile phone
records, geolocated tweets, check-ins from Foursquare or geotagged photos, have
contributed to this purpose at different scales, from cities to countries, in
different world areas. Many previous works lacked, however, details on the
individuals' attributes such as age or gender. In this work, we analyze
credit-card records from Barcelona and Madrid and by examining the geolocated
credit-card transactions of individuals living in the two provinces, we find
that the mobility patterns vary according to gender, age and occupation.
Differences in distance traveled and travel purpose are observed between
younger and older people, but, curiously, either between males and females of
similar age. While mobility displays some generic features, here we show that
sociodemographic characteristics play a relevant role and must be taken into
account for mobility and epidemiological modelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7910</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7910</id><created>2014-11-28</created><authors><author><keyname>Di Sanzo</keyname><forenames>Pierangelo</forenames></author><author><keyname>Quaglia</keyname><forenames>Francesco</forenames></author><author><keyname>Ciciani</keyname><forenames>Bruno</forenames></author><author><keyname>Pellegrini</keyname><forenames>Alessandro</forenames></author><author><keyname>Didona</keyname><forenames>Diego</forenames></author><author><keyname>Romano</keyname><forenames>Paolo</forenames></author><author><keyname>Palmieri</keyname><forenames>Roberto</forenames></author><author><keyname>Peluso</keyname><forenames>Sebastiano</forenames></author></authors><title>A Flexible Framework for Accurate Simulation of Cloud In-Memory Data
  Stores</title><categories>cs.PF cs.DC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-memory (transactional) data stores are recognized as a first-class data
management technology for cloud platforms, thanks to their ability to match the
elasticity requirements imposed by the pay-as-you-go cost model. On the other
hand, defining the well-suited amount of cache servers to be deployed, and the
degree of in-memory replication of slices of data, in order to optimize
reliability/availability and performance tradeoffs, is far from being a trivial
task. Yet, it is an essential aspect of the provisioning process of cloud
platforms, given that it has an impact on how well cloud resources are actually
exploited. To cope with the issue of determining optimized configurations of
cloud in-memory data stores, in this article we present a flexible simulation
framework offering skeleton simulation models that can be easily specialized in
order to capture the dynamics of diverse data grid systems, such as those
related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of
the framework lies in that it integrates simulation and machine-learning
(black-box) techniques, the latter being essentially used to capture the
dynamics of the data-exchange layer (e.g. the message passing layer) across the
cache servers. This is a relevant aspect when considering that the actual
data-transport/networking infrastructure on top of which the data grid is
deployed might be unknown, hence being not feasible to be modeled via white-box
(namely purely simulative) approaches. We also provide an extended experimental
study aimed at validating instances of simulation models supported by our
framework against execution dynamics of real data grid systems deployed on top
of either private or public cloud infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7911</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7911</id><created>2014-11-28</created><authors><author><keyname>Rozantsev</keyname><forenames>Artem</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>On Rendering Synthetic Images for Training an Object Detector</title><categories>cs.CV</categories><doi>10.1016/j.cviu.2014.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach to synthesizing images that are effective for
training object detectors. Starting from a small set of real images, our
algorithm estimates the rendering parameters required to synthesize similar
images given a coarse 3D model of the target object. These parameters can then
be reused to generate an unlimited number of training images of the object of
interest in arbitrary 3D poses, which can then be used to increase
classification performances.
  A key insight of our approach is that the synthetically generated images
should be similar to real images, not in terms of image quality, but rather in
terms of features used during the detector training. We show in the context of
drone, plane, and car detection that using such synthetically generated images
yields significantly better performances than simply perturbing real images or
even synthesizing images in such way that they look very realistic, as is often
done when only limited amounts of training data are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7920</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7920</id><created>2014-11-28</created><updated>2014-12-03</updated><authors><author><keyname>Rodriques</keyname><forenames>Samuel G.</forenames></author></authors><title>Probability Theory without Bayes' Rule</title><categories>math.PR cs.AI math.ST stat.TH</categories><comments>12 pages, no figures</comments><msc-class>60A05 (Primary), 62A01 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the Kolmogorov theory of probability, Bayes' rule allows one to
perform statistical inference by relating conditional probabilities to
unconditional probabilities. As we show here, however, there is a continuous
set of alternative inference rules that yield the same results, and that may
have computational or practical advantages for certain problems. We formulate
generalized axioms for probability theory, according to which the reverse
conditional probability distribution P(B|A) is not specified by the forward
conditional probability distribution P(A|B) and the marginals P(A) and P(B).
Thus, in order to perform statistical inference, one must specify an additional
&quot;inference axiom,&quot; which relates P(B|A) to P(A|B), P(A), and P(B). We show that
when Bayes' rule is chosen as the inference axiom, the axioms are equivalent to
the classical Kolmogorov axioms. We then derive consistency conditions on the
inference axiom, and thereby characterize the set of all possible rules for
inference. The set of &quot;first-order&quot; inference axioms, defined as the set of
axioms in which P(B|A) depends on the first power of P(A|B), is found to be a
1-simplex, with Bayes' rule at one of the extreme points. The other extreme
point, the &quot;inversion rule,&quot; is studied in depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7923</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7923</id><created>2014-11-28</created><authors><author><keyname>Yi</keyname><forenames>Dong</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Liao</keyname><forenames>Shengcai</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Learning Face Representation from Scratch</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pushing by big data and deep convolutional neural network (CNN), the
performance of face recognition is becoming comparable to human. Using private
large scale training datasets, several groups achieve very high performance on
LFW, i.e., 97% to 99%. While there are many open source implementations of CNN,
none of large scale face dataset is publicly available. The current situation
in the field of face recognition is that data is more important than algorithm.
To solve this problem, this paper proposes a semi-automatical way to collect
face images from Internet and builds a large scale dataset containing about
10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database,
we use a 11-layer CNN to learn discriminative representation and obtain
state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will
attract more research groups entering this field and accelerate the development
of face recognition in the wild.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7924</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7924</id><created>2014-11-28</created><authors><author><keyname>Fruergaard</keyname><forenames>Bjarne &#xd8;rum</forenames></author></authors><title>Predicting clicks in online display advertising with latent features and
  side-information</title><categories>stat.ML cs.LG stat.AP</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review a method for click-through rate prediction based on the work of
Menon et al. [11], which combines collaborative filtering and matrix
factorization with a side-information model and fuses the outputs to proper
probabilities in [0,1]. In addition we provide details, both for the modeling
as well as the experimental part, that are not found elsewhere. We rigorously
test the performance on several test data sets from consecutive days in a
click-through rate prediction setup, in a manner which reflects a real-world
pipeline. Our results confirm that performance can be increased using latent
features, albeit the differences in the measures are small but significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7925</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7925</id><created>2014-11-28</created><authors><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author></authors><title>Alignment of Polarized Sets</title><categories>cs.IT math.IT quant-ph</categories><comments>22 pages, 6 figures</comments><msc-class>68P30, 94A15, 81P45</msc-class><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 34, no. 2,
  pages 224-238, 2016</journal-ref><doi>10.1109/JSAC.2015.2504271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ar{\i}kan's polar coding technique is based on the idea of synthesizing $n$
channels from the $n$ instances of the physical channel by a simple linear
encoding transformation. Each synthesized channel corresponds to a particular
input to the encoder. For large $n$, the synthesized channels become either
essentially noiseless or almost perfectly noisy, but in total carry as much
information as the original $n$ channels. Capacity can therefore be achieved by
transmitting messages over the essentially noiseless synthesized channels.
Unfortunately, the set of inputs corresponding to reliable synthesized channels
is poorly understood, in particular how the set depends on the underlying
physical channel. In this work, we present two analytic conditions sufficient
to determine if the reliable inputs corresponding to different discrete
memoryless channels are aligned or not, i.e. if one set is contained in the
other. Understanding the alignment of the polarized sets is important as it is
directly related to universality properties of the induced polar codes, which
are essential in particular for network coding problems. We demonstrate the
performance of our conditions on a few examples for wiretap and broadcast
channels. Finally we show that these conditions imply that the simple quantum
polar coding scheme of Renes et al. [Phys. Rev. Lett. 109, 050504 (2012)]
requires entanglement assistance for general channels, but also show such
assistance to be unnecessary in many cases of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7935</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7935</id><created>2014-11-24</created><authors><author><keyname>Leal-Taix&#xe9;</keyname><forenames>Laura</forenames></author></authors><title>Multiple object tracking with context awareness</title><categories>cs.CV</categories><comments>PhD thesis, Leibniz University Hannover, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple people tracking is a key problem for many applications such as
surveillance, animation or car navigation, and a key input for tasks such as
activity recognition. In crowded environments occlusions and false detections
are common, and although there have been substantial advances in recent years,
tracking is still a challenging task. Tracking is typically divided into two
steps: detection, i.e., locating the pedestrians in the image, and data
association, i.e., linking detections across frames to form complete
trajectories.
  For the data association task, approaches typically aim at developing new,
more complex formulations, which in turn put the focus on the optimization
techniques required to solve them. However, they still utilize very basic
information such as distance between detections. In this thesis, I focus on the
data association task and argue that there is contextual information that has
not been fully exploited yet in the tracking community, mainly social context
and spatial context coming from different views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7942</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7942</id><created>2014-11-28</created><updated>2014-12-12</updated><authors><author><keyname>Polajnar</keyname><forenames>Tamara</forenames></author><author><keyname>Rimell</keyname><forenames>Laura</forenames></author><author><keyname>Clark</keyname><forenames>Stephen</forenames></author></authors><title>Using Sentence Plausibility to Learn the Semantics of Transitive Verbs</title><categories>cs.CL</categories><comments>Full updated paper for NIPS learning semantics workshop, with some
  minor errata fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The functional approach to compositional distributional semantics considers
transitive verbs to be linear maps that transform the distributional vectors
representing nouns into a vector representing a sentence. We conduct an initial
investigation that uses a matrix consisting of the parameters of a logistic
regression classifier trained on a plausibility task as a transitive verb
function. We compare our method to a commonly used corpus-based method for
constructing a verb matrix and find that the plausibility training may be more
effective for disambiguation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7960</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7960</id><created>2014-11-26</created><authors><author><keyname>Tarable</keyname><forenames>Alberto</forenames></author><author><keyname>Nordio</keyname><forenames>Alessandro</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Marsan</keyname><forenames>Marco Ajmone</forenames></author></authors><title>The Importance of Being Earnest in Crowdsourcing Systems</title><categories>cs.HC cs.SI</categories><comments>To appear at Infocom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first systematic investigation of the potential
performance gains for crowdsourcing systems, deriving from available
information at the requester about individual worker earnestness (reputation).
In particular, we first formalize the optimal task assignment problem when
workers' reputation estimates are available, as the maximization of a monotone
(submodular) function subject to Matroid constraints. Then, being the optimal
problem NP-hard, we propose a simple but efficient greedy heuristic task
allocation algorithm. We also propose a simple ``maximum a-posteriori``
decision rule. Finally, we test and compare different solutions, showing that
system performance can greatly benefit from information about workers'
reputation. Our main findings are that: i) even largely inaccurate estimates of
workers' reputation can be effectively exploited in the task assignment to
greatly improve system performance; ii) the performance of the maximum
a-posteriori decision rule quickly degrades as worker reputation estimates
become inaccurate; iii) when workers' reputation estimates are significantly
inaccurate, the best performance can be obtained by combining our proposed task
assignment algorithm with the LRA decision rule introduced in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7964</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7964</id><created>2014-11-28</created><authors><author><keyname>Hassner</keyname><forenames>Tal</forenames></author><author><keyname>Harel</keyname><forenames>Shai</forenames></author><author><keyname>Paz</keyname><forenames>Eran</forenames></author><author><keyname>Enbar</keyname><forenames>Roee</forenames></author></authors><title>Effective Face Frontalization in Unconstrained Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Frontalization&quot; is the process of synthesizing frontal facing views of faces
appearing in single unconstrained photos. Recent reports have suggested that
this process may substantially boost the performance of face recognition
systems. This, by transforming the challenging problem of recognizing faces
viewed from unconstrained viewpoints to the easier problem of recognizing faces
in constrained, forward facing poses. Previous frontalization methods did this
by attempting to approximate 3D facial shapes for each query image. We observe
that 3D face shape estimation from unconstrained photos may be a harder problem
than frontalization and can potentially introduce facial misalignments.
Instead, we explore the simpler approach of using a single, unmodified, 3D
surface as an approximation to the shape of all input faces. We show that this
leads to a straightforward, efficient and easy to implement method for
frontalization. More importantly, it produces aesthetic new frontal views and
is surprisingly effective when used for face recognition and gender estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7973</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7973</id><created>2014-11-28</created><authors><author><keyname>Kormaksson</keyname><forenames>Matthias</forenames></author><author><keyname>Barbosa</keyname><forenames>Luciano</forenames></author><author><keyname>Vieira</keyname><forenames>Marcos R.</forenames></author><author><keyname>Zadrozny</keyname><forenames>Bianca</forenames></author></authors><title>Bus Travel Time Predictions Using Additive Models</title><categories>cs.LG stat.AP</categories><comments>11 pages, this is the technical report supporting the IEEE 2014
  International Conference on Data Mining (ICDM) submission with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many factors can affect the predictability of public bus services such as
traffic, weather and local events. Other aspects, such as day of week or hour
of day, may influence bus travel times as well, either directly or in
conjunction with other variables. However, the exact nature of such
relationships between travel times and predictor variables is, in most
situations, not known. In this paper we develop a framework that allows for
flexible modeling of bus travel times through the use of Additive Models. In
particular, we model travel times as a sum of linear as well as nonlinear terms
that are modeled as smooth functions of predictor variables. The proposed class
of models provides a principled statistical framework that is highly flexible
in terms of model building. The experimental results demonstrate uniformly
superior performance of our best model as compared to previous prediction
methods when applied to a very large GPS data set obtained from buses operating
in the city of Rio de Janeiro.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.7974</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.7974</id><created>2014-11-28</created><updated>2014-12-31</updated><authors><author><keyname>Waugh</keyname><forenames>Kevin</forenames></author><author><keyname>Morrill</keyname><forenames>Dustin</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>Solving Games with Functional Regret Estimation</title><categories>cs.AI cs.GT cs.MA</categories><comments>AAAI Conference on Artificial Intelligence 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel online learning method for minimizing regret in large
extensive-form games. The approach learns a function approximator online to
estimate the regret for choosing a particular action. A no-regret algorithm
uses these estimates in place of the true regrets to define a sequence of
policies.
  We prove the approach sound by providing a bound relating the quality of the
function approximation and regret of the algorithm. A corollary being that the
method is guaranteed to converge to a Nash equilibrium in self-play so long as
the regrets are ultimately realizable by the function approximator. Our
technique can be understood as a principled generalization of existing work on
abstraction in large games; in our work, both the abstraction as well as the
equilibrium are learned during self-play. We demonstrate empirically the method
achieves higher quality strategies than state-of-the-art abstraction techniques
given the same resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.8003</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.8003</id><created>2014-11-28</created><updated>2015-05-29</updated><authors><author><keyname>Sun</keyname><forenames>Ruoyu</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Guaranteed Matrix Completion via Non-convex Factorization</title><categories>cs.LG</categories><comments>76 pages, 6 figures, a linear convergence result added, extended
  discussion on resampling</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix factorization is a popular approach for large-scale matrix completion.
In this approach, the unknown low-rank matrix is expressed as the product of
two much smaller matrices so that the low-rank property is automatically
fulfilled. The resulting optimization problem, even with huge size, can be
solved (to stationary points) very efficiently through standard optimization
algorithms such as alternating minimization and stochastic gradient descent
(SGD). However, due to the non-convexity caused by the factorization model,
there is a limited theoretical understanding of whether these algorithms will
generate a good solution. In this paper, we establish a theoretical guarantee
for the factorization based formulation to correctly recover the underlying
low-rank matrix. In particular, we show that under similar conditions to those
in previous works, many standard optimization algorithms converge to the global
optima of a factorization based formulation, and recover the true low-rank
matrix. A major difference of our work from the existing results is that we do
not need resampling (i.e., using independent samples at each iteration) in
either the algorithm or its analysis. To the best of our knowledge, our result
is the first one that provides exact recovery guarantee for many standard
algorithms such as gradient descent, SGD and block coordinate gradient descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0003</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0003</id><created>2014-11-26</created><authors><author><keyname>Su</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Fan</forenames></author><author><keyname>Yi</keyname><forenames>Li</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>3D-Assisted Image Feature Synthesis for Novel Views of an Object</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparing two images in a view-invariant way has been a challenging problem
in computer vision for a long time, as visual features are not stable under
large view point changes. In this paper, given a single input image of an
object, we synthesize new features for other views of the same object. To
accomplish this, we introduce an aligned set of 3D models in the same class as
the input object image. Each 3D model is represented by a set of views, and we
study the correlation of image patches between different views, seeking what we
call surrogates --- patches in one view whose feature content predicts well the
features of a patch in another view. In particular, for each patch in the novel
desired view, we seek surrogates from the observed view of the given image. For
a given surrogate, we predict that surrogate using linear combination of the
corresponding patches of the 3D model views, learn the coefficients, and then
transfer these coefficients on a per patch basis to synthesize the features of
the patch in the novel view. In this way we can create feature sets for all
views of the latent object, providing us a multi-view representation of the
object. View-invariant object comparisons are achieved simply by computing the
$L^2$ distances between the features of corresponding views. We provide
theoretical and empirical analysis of the feature synthesis process, and
evaluate the proposed view-agnostic distance (VAD) in fine-grained image
retrieval (100 object classes) and classification tasks. Experimental results
show that our synthesized features do enable view-independent comparison
between images and perform significantly better than traditional image features
in this respect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0007</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0007</id><created>2014-11-27</created><updated>2014-12-07</updated><authors><author><keyname>Stegmann</keyname><forenames>Johannes</forenames></author></authors><title>Paradigm shifts. Part I. Collagen. Confirming and complementing the work
  of Henry Small</title><categories>cs.DL cs.IR</categories><comments>6 pages, 1 figure, 3 tables, corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paradigm shift in collagen research during the early 1970s marked by the
discovery of the collagen precursor molecule procollagen was traced using
co-citation analysis and title word frequency determination, confirming
previous work performed by Henry Small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0008</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0008</id><created>2014-11-27</created><authors><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author><author><keyname>Templeman</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Dennis</forenames></author><author><keyname>Crandall</keyname><forenames>David</forenames></author><author><keyname>Kapadia</keyname><forenames>Apu</forenames></author></authors><title>ScreenAvoider: Protecting Computer Screens from Ubiquitous Cameras</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live and work in environments that are inundated with cameras embedded in
devices such as phones, tablets, laptops, and monitors. Newer wearable devices
like Google Glass, Narrative Clip, and Autographer offer the ability to quietly
log our lives with cameras from a `first person' perspective. While capturing
several meaningful and interesting moments, a significant number of images
captured by these wearable cameras can contain computer screens. Given the
potentially sensitive information that is visible on our displays, there is a
need to guard computer screens from undesired photography. People need
protection against photography of their screens, whether by other people's
cameras or their own cameras.
  We present ScreenAvoider, a framework that controls the collection and
disclosure of images with computer screens and their sensitive content.
ScreenAvoider can detect images with computer screens with high accuracy and
can even go so far as to discriminate amongst screen content. We also introduce
a ScreenTag system that aids in the identification of screen content, flagging
images with highly sensitive content such as messaging applications or email
webpages. We evaluate our concept on realistic lifelogging datasets, showing
that ScreenAvoider provides a practical and useful solution that can help users
manage their privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0034</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0034</id><created>2014-11-28</created><authors><author><keyname>Panteleev</keyname><forenames>Pavel</forenames></author></authors><title>Preset Distinguishing Sequences and Diameter of Transformation
  Semigroups</title><categories>cs.FL</categories><comments>13 pages, 3 figures, LATA 2015</comments><msc-class>68Q45 (Primary), 68Q70, 20D60 (Secondary)</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the length $\ell(n,k)$ of a shortest preset distinguishing
sequence (PDS) in the worst case for a $k$-element subset of an $n$-state Mealy
automaton. It was mentioned by Sokolovskii that this problem is closely related
to the problem of finding the maximal subsemigroup diameter
$\ell(\mathbf{T}_n)$ for the full transformation semigroup $\mathbf{T}_n$ of an
$n$-element set. We prove that
$\ell(\mathbf{T}_n)=2^n\exp\{\sqrt{\frac{n}{2}\ln n}(1+ o(1))\}$ as
$n\to\infty$ and, using approach of Sokolovskii, find the asymptotics of
$\log_2 \ell(n,k)$ as $n,k\to\infty$ and $k/n\to a\in (0,1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0035</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0035</id><created>2014-11-26</created><authors><author><keyname>Mahendran</keyname><forenames>Aravindh</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Understanding Deep Image Representations by Inverting Them</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image representations, from SIFT and Bag of Visual Words to Convolutional
Neural Networks (CNNs), are a crucial component of almost any image
understanding system. Nevertheless, our understanding of them remains limited.
In this paper we conduct a direct analysis of the visual information contained
in representations by asking the following question: given an encoding of an
image, to which extent is it possible to reconstruct the image itself? To
answer this question we contribute a general framework to invert
representations. We show that this method can invert representations such as
HOG and SIFT more accurately than recent alternatives while being applicable to
CNNs too. We then use this technique to study the inverse of recent
state-of-the-art CNN image representations for the first time. Among our
findings, we show that several layers in CNNs retain photographically accurate
information about the image, with different degrees of geometric and
photometric invariance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0036</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0036</id><created>2014-11-28</created><updated>2015-04-14</updated><authors><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author></authors><title>Randomized Rounding for the Largest Simplex Problem</title><categories>cs.CG cs.DS math.FA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum volume $j$-simplex problem asks to compute the $j$-dimensional
simplex of maximum volume inside the convex hull of a given set of $n$ points
in $\mathbb{Q}^d$. We give a deterministic approximation algorithm for this
problem which achieves an approximation ratio of $e^{j/2 + o(j)}$. The problem
is known to be $\mathrm{NP}$-hard to approximate within a factor of $c^{j}$ for
some constant $c &gt; 1$. Our algorithm also gives a factor $e^{j + o(j)}$
approximation for the problem of finding the principal $j\times j$ submatrix of
a rank $d$ positive semidefinite matrix with the largest determinant. We
achieve our approximation by rounding solutions to a generalization of the
$D$-optimal design problem, or, equivalently, the dual of an appropriate
smallest enclosing ellipsoid problem. Our arguments give a short and simple
proof of a restricted invertibility principle for determinants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0037</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0037</id><created>2014-11-26</created><authors><author><keyname>Boualem</keyname><forenames>Djehiche</forenames></author><author><keyname>Hamidou</keyname><forenames>Tembine</forenames></author></authors><title>On the Solvability of Risk-Sensitive Linear-Quadratic Mean-Field Games</title><categories>math.OC cs.GT cs.SY math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we formulate and solve a mean-field game described by a linear
stochastic dynamics and a quadratic or exponential-quadratic cost functional
for each generic player. The optimal strategies for the players are given
explicitly using a simple and direct method based on square completion and a
Girsanov-type change of measure. This approach does not use the well-known
solution methods such as the Stochastic Maximum Principle and the Dynamic
Programming Principle with Hamilton-Jacobi-Bellman-Isaacs equation and
Fokker-Planck-Kolmogorov equation. In the risk-neutral linear-quadratic
mean-field game, we show that there is unique best response strategy to the
mean of the state and provide a simple sufficient condition of existence and
uniqueness of mean-field equilibrium. This approach gives a basic insight into
the solution by providing a simple explanation for the additional term in the
robust or risk-sensitive Riccati equation, compared to the risk-neutral Riccati
equation. Sufficient conditions for existence and uniqueness of mean-field
equilibria are obtained when the horizon length and risk-sensitivity index are
small enough. The method is then extended to the linear-quadratic robust
mean-field games under small disturbance, formulated as a minimax mean-field
game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0041</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0041</id><created>2014-11-28</created><updated>2016-01-28</updated><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author></authors><title>FairCache: Introducing Fairness to ICN Caching - Technical Report</title><categories>cs.NI cs.DC cs.GT cs.PF</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-centric networking extensively uses universal in-network caching.
However, developing an efficient and fair collaborative caching algorithm for
selfish caches is still an open question. In addition, the communication
overhead induced by collaboration is especially poorly understood in a general
network setting such as realistic ISP and Autonomous System networks. In this
paper, we address these two problems by modeling the in-network caching problem
as a Nash bargaining game. We show that the game is a convex optimization
problem and further derive the corresponding distributed algorithm. We
analytically investigate the collaboration overhead on general graph
topologies, and theoretically show that collaboration has to be constrained
within a small neighborhood due to its cost growing exponentially. Our proposed
algorithm achieves at least 16% performance gain over its competitors on
different network topologies in the evaluation, and guarantees provable
convergence, Pareto efficiency and proportional fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0055</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0055</id><created>2014-11-28</created><authors><author><keyname>Battagello</keyname><forenames>Vin&#xed;cius A.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Carlos H. C.</forenames></author></authors><title>Analysis of the Effects of Failure and Noise in the Distributed
  Connectivity Maintenance of a Multi-robot System</title><categories>cs.MA</categories><comments>6 pages, 7 figures, published in CINTI 2014</comments><acm-class>B.8.2; C.1.4; C.4; I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To perform cooperative tasks in a decentralized manner, multi-robot systems
are often required to communicate with each other. Therefore, maintaining the
communication graph connectivity is a fundamental issue when roaming a
territory with obstacles. However, when dealing with real-robot systems,
several sources of data corruption can appear in the agent interaction. In this
paper, the effects of failure and noise in the communication between agents are
analyzed upon a connectivity maintenance control strategy. The results show
that the connectivity strategy is resilient to the negative effects of such
disturbances under realistic settings that consider a bandwidth limit for the
control effort. This opens the perspective of applying the connectivity
maintenance strategy in adaptive schemes that consider, for instance,
autonomous adaptation to constraints other than connectivity itself, e.g.
communication efficiency and energy harvesting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0056</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0056</id><created>2014-11-28</created><authors><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author></authors><title>Verifiably Truthful Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is typically expected that if a mechanism is truthful, then the agents
would, indeed, truthfully report their private information. But why would an
agent believe that the mechanism is truthful? We wish to design truthful
mechanisms, whose truthfulness can be verified efficiently (in the
computational sense). Our approach involves three steps: (i) specifying the
structure of mechanisms, (ii) constructing a verification algorithm, and (iii)
measuring the quality of verifiably truthful mechanisms. We demonstrate this
approach using a case study: approximate mechanism design without money for
facility location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0057</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0057</id><created>2014-11-28</created><authors><author><keyname>Drummond</keyname><forenames>Ross</forenames></author><author><keyname>Howey</keyname><forenames>David A.</forenames></author><author><keyname>Duncan</keyname><forenames>Stephen R.</forenames></author></authors><title>Low-Order Mathematical Modelling of Electric Double Layer
  Supercapacitors Using Spectral Methods</title><categories>cs.SY physics.chem-ph</categories><comments>Pre-print submitted to Journal of Power Sources</comments><doi>10.1016/j.jpowsour.2014.11.116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates two physics-based models that simulate the non-linear
partial differential algebraic equations describing an electric double layer
supercapacitor. In one model the linear dependence between electrolyte
concentration and conductivity is accounted for, while in the other model it is
not. A spectral element method is used to discretise the model equations and it
is found that the error convergence rate with respect to the number of elements
is faster compared to a finite difference method. The increased accuracy of the
spectral element approach means that, for a similar level of solution accuracy,
the model simulation computing time is approximately 50% of that of the finite
difference method. This suggests that the spectral element model could be used
for control and state estimation purposes. For a typical supercapacitor
charging profile, the numerical solutions from both models closely match
experimental voltage and current data. However, when the electrolyte is dilute
or where there is a long charging time, a noticeable difference between the
numerical solutions of the two models is observed. Electrical impedance
spectroscopy simulations show that the capacitance of the two models rapidly
decreases when the frequency of the perturbation current exceeds an upper
threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0059</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0059</id><created>2014-11-28</created><authors><author><keyname>Miyajima</keyname><forenames>Kent</forenames></author><author><keyname>Sakuragawa</keyname><forenames>Takashi</forenames></author></authors><title>Continuous and robust clustering coefficients for weighted and directed
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>29 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new clustering coefficients for weighted networks. They are
continuous and robust against edge weight changes. Recently, generalized
clustering coefficients for weighted and directed networks have been proposed.
These generalizations have a common property, that their values are not
continuous. They are sensitive with edge weight changes, especially at zero
weight. With these generalizations, if vanishingly low weights of edges are
truncated to weight zero for some reason, the coefficient value may change
significantly from the original value. It is preferable that small changes of
edge weights cause small changes of coefficient value. We call this property
the continuity of generalized clustering coefficients. Our new coefficients
admit this property. In the past, few studies have focused on the continuity of
generalized clustering coefficients. In experiments, we performed comparative
assessments of existing and our generalizations. In the case of a real world
network dataset (C. Elegans Neural network), after adding random edge weight
errors, though the value of one discontinuous generalization was changed about
436%, the value of proposed one was only changed 0.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0060</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0060</id><created>2014-11-28</created><authors><author><keyname>Rogez</keyname><forenames>Gregory</forenames></author><author><keyname>Supancic</keyname><forenames>James S.</forenames><suffix>III</suffix></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Egocentric Pose Recognition in Four Lines of Code</title><categories>cs.CV</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of estimating the 3D pose of an individual's upper
limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider
pose estimation during everyday interactions with objects. Past work shows that
strong pose+viewpoint priors and depth-based features are crucial for robust
performance. In egocentric views, hands and arms are observable within a well
defined volume in front of the camera. We call this volume an egocentric
workspace. A notable property is that hand appearance correlates with workspace
location. To exploit this correlation, we classify arm+hand configurations in a
global egocentric coordinate frame, rather than a local scanning window. This
greatly simplify the architecture and improves performance. We propose an
efficient pipeline which 1) generates synthetic workspace exemplars for
training using a virtual chest-mounted camera whose intrinsic parameters match
our physical camera, 2) computes perspective-aware depth features on this
entire volume and 3) recognizes discrete arm+hand pose classes through a sparse
multi-class SVM. Our method provides state-of-the-art hand pose recognition
performance from egocentric RGB-D images in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0062</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0062</id><created>2014-11-28</created><authors><author><keyname>Babagholami-Mohamadabadi</keyname><forenames>Behnam</forenames></author><author><keyname>Jourabloo</keyname><forenames>Amin</forenames></author><author><keyname>Zarghami</keyname><forenames>Ali</forenames></author><author><keyname>Kasaei</keyname><forenames>Shohreh</forenames></author></authors><title>A Bayesian Framework for Sparse Representation-Based 3D Human Pose
  Estimation</title><categories>cs.CV</categories><comments>Accepted in IEEE Signal Processing Letters (SPL), 2014</comments><doi>10.1109/LSP.2014.2301726</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian framework for 3D human pose estimation from monocular images based
on sparse representation (SR) is introduced. Our probabilistic approach aims at
simultaneously learning two overcomplete dictionaries (one for the visual input
space and the other for the pose space) with a shared sparse representation.
Existing SR-based pose estimation approaches only offer a point estimation of
the dictionary and the sparse codes. Therefore, they might be unreliable when
the number of training examples is small. Our Bayesian framework estimates a
posterior distribution for the sparse codes and the dictionaries from labeled
training data. Hence, it is robust to overfitting on small-size training data.
Experimental results on various human activities show that the proposed method
is superior to the state of-the-art pose estimation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0065</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0065</id><created>2014-11-28</created><authors><author><keyname>Rogez</keyname><forenames>Gregory</forenames></author><author><keyname>Supancic</keyname><forenames>James S.</forenames><suffix>III</suffix></author><author><keyname>Khademi</keyname><forenames>Maryam</forenames></author><author><keyname>Montiel</keyname><forenames>Jose Maria Martinez</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>3D Hand Pose Detection in Egocentric RGB-D Images</title><categories>cs.CV</categories><comments>14 pages, 15 figures, extended version of the corresponding ECCV
  workshop paper, submitted to International Journal of Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the task of everyday hand pose estimation from egocentric
viewpoints. For this task, we show that depth sensors are particularly
informative for extracting near-field interactions of the camera wearer with
his/her environment. Despite the recent advances in full-body pose estimation
using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D
images is still an unsolved problem. The problem is considerably exacerbated
when analyzing hands performing daily activities from a first-person viewpoint,
due to severe occlusions arising from object manipulations and a limited
field-of-view. Our system addresses these difficulties by exploiting strong
priors over viewpoint and pose in a discriminative tracking-by-detection
framework. Our priors are operationalized through a photorealistic synthetic
model of egocentric scenes, which is used to generate training data for
learning depth-based pose classifiers. We evaluate our approach on an annotated
dataset of real egocentric object manipulation scenes and compare to both
commercial and academic approaches. Our method provides state-of-the-art
performance for both hand detection and pose estimation in egocentric RGB-D
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0069</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0069</id><created>2014-11-28</created><authors><author><keyname>Tian</keyname><forenames>Yonglong</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Pedestrian Detection aided by Deep Learning Semantic Tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning methods have achieved great success in pedestrian detection,
owing to its ability to learn features from raw pixels. However, they mainly
capture middle-level representations, such as pose of pedestrian, but confuse
positive with hard negative samples, which have large ambiguity, e.g. the shape
and appearance of `tree trunk' or `wire pole' are similar to pedestrian in
certain viewpoint. This ambiguity can be distinguished by high-level
representation. To this end, this work jointly optimizes pedestrian detection
with semantic tasks, including pedestrian attributes (e.g. `carrying backpack')
and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than
expensively annotating scene attributes, we transfer attributes information
from existing scene segmentation datasets to the pedestrian dataset, by
proposing a novel deep model to learn high-level features from multiple tasks
and multiple data sources. Since distinct tasks have distinct convergence rates
and data from different datasets have different distributions, a multi-task
objective function is carefully designed to coordinate tasks and reduce
discrepancies among datasets. The importance coefficients of tasks and network
parameters in this objective function can be iteratively estimated. Extensive
evaluations show that the proposed approach outperforms the state-of-the-art on
the challenging Caltech and ETH datasets, where it reduces the miss rates of
previous deep models by 17 and 5.5 percent, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0073</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0073</id><created>2014-11-29</created><updated>2015-04-08</updated><authors><author><keyname>Liu</keyname><forenames>Jingcheng</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>FPTAS for #BIS with Degree Bounds on One Side</title><categories>cs.DS</categories><comments>15 pages, to appear in STOC 2015; Improved presentations from
  previous version;</comments><msc-class>05C70</msc-class><acm-class>F.2.2; G.2.1</acm-class><doi>10.1145/2746539.2746598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the number of independent sets for a bipartite graph (#BIS) plays a
crucial role in the study of approximate counting. It has been conjectured that
there is no fully polynomial-time (randomized) approximation scheme
(FPTAS/FPRAS) for #BIS, and it was proved that the problem for instances with a
maximum degree of $6$ is already as hard as the general problem. In this paper,
we obtain a surprising tractability result for a family of #BIS instances. We
design a very simple deterministic fully polynomial-time approximation scheme
(FPTAS) for #BIS when the maximum degree for one side is no larger than $5$.
There is no restriction for the degrees on the other side, which do not even
have to be bounded by a constant. Previously, FPTAS was only known for
instances with a maximum degree of $5$ for both sides.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0100</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0100</id><created>2014-11-29</created><authors><author><keyname>Mathe</keyname><forenames>Stefan</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised
  Detection in Images</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art visual recognition and detection systems increasingly rely
on large amounts of training data and complex classifiers. Therefore it becomes
increasingly expensive both to manually annotate datasets and to keep running
times at levels acceptable for practical applications. In this paper, we
propose two solutions to address these issues. First, we introduce a weakly
supervised, segmentation-based approach to learn accurate detectors and image
classifiers from weak supervisory signals that provide only approximate
constraints on target localization. We illustrate our system on the problem of
action detection in static images (Pascal VOC Actions 2012), using human visual
search patterns as our training signal. Second, inspired from the
saccade-and-fixate operating principle of the human visual system, we use
reinforcement learning techniques to train efficient search models for
detection. Our sequential method is weakly supervised and general (it does not
require eye movements), finds optimal search strategies for any given detection
confidence function and achieves performance similar to exhaustive sliding
window search at a fraction of its computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0103</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0103</id><created>2014-11-29</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>Inference on the Network Evolution in BitTorrent Mainline DHT</title><categories>cs.NI cs.CY cs.SI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network size is a fundamental statistic for a peer-to-peer system but is
generally considered to contain too little information to be useful. However,
most existing work only considers the metric by itself and does not explore
what features could be extracted from this seem- ingly trivial metric. In this
paper, we show that Fourier transform allows us to extract frequency features
from such time series data, which can further be used to characterize user
behaviors and detect system anoma- lies in a peer-to-peer system automatically
without needing to resort to visual comparisons. By using the proposed
algorithm, our system suc- cessfully discovers and clusters countries of
similar user behavior and captures the anomalies like Sybil attacks and other
real-world events with high accuracy. Our work in this paper highlights the
usefulness of more advanced time series processing techniques in analyzing
network measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0111</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0111</id><created>2014-11-29</created><authors><author><keyname>Omari</keyname><forenames>Mounir</forenames></author><author><keyname>Abdelouahad</keyname><forenames>Abdelkaher Ait</forenames></author><author><keyname>Hassouni</keyname><forenames>Mohammed El</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author></authors><title>Color image quality assessment measure using multivariate generalized
  Gaussian distribution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with color image quality assessment in the reduced-reference
framework based on natural scenes statistics. In this context, we propose to
model the statistics of the steerable pyramid coefficients by a Multivariate
Generalized Gaussian distribution (MGGD). This model allows taking into account
the high correlation between the components of the RGB color space. For each
selected scale and orientation, we extract a parameter matrix from the three
color components subbands. In order to quantify the visual degradation, we use
a closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using
&quot;TID 2008&quot; benchmark, the proposed measure has been compared with the most
influential methods according to the FRTV1 VQEG framework. Results demonstrates
its effectiveness for a great variety of distortion type. Among other benefits
this measure uses only very little information about the original image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0129</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0129</id><created>2014-11-29</created><updated>2016-02-06</updated><authors><author><keyname>von Stengel</keyname><forenames>Bernhard</forenames></author></authors><title>Recursive Inspection Games</title><categories>cs.GT</categories><comments>final version for Mathematics of Operations Research, new Theorem 2</comments><msc-class>91A05</msc-class><journal-ref>Mathematics of Operations Research (2016), Articles in Advance,
  pp. 1-18</journal-ref><doi>10.1287/moor.2015.0762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a sequential inspection game where an inspector uses a limited
number of inspections over a larger number of time periods to detect a
violation (an illegal act) of an inspectee. Compared with earlier models, we
allow varying rewards to the inspectee for successful violations. As one
possible example, the most valuable reward may be the completion of a sequence
of thefts of nuclear material needed to build a nuclear bomb. The inspectee can
observe the inspector, but the inspector can only determine if a violation
happens during a stage where he inspects, which terminates the game; otherwise
the game continues. Under reasonable assumptions for the payoffs, the
inspector's strategy is independent of the number of successful violations.
This allows to apply a recursive description of the game, even though this
normally assumes fully informed players after each stage. The resulting
recursive equation in three variables for the equilibrium payoff of the game,
which generalizes several other known equations of this kind, is solved
explicitly in terms of sums of binomial coefficients. We also extend this
approach to non-zero-sum games and, similar to Maschler (1966), &quot;inspector
leadership&quot; where the inspector commits to (the same) randomized inspection
schedule, but the inspectee acts legally (rather than mixes as in the
simultaneous game) as long as inspections remain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0131</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0131</id><created>2014-11-29</created><authors><author><keyname>Zak</keyname><forenames>Blazej</forenames></author><author><keyname>Zbieg</keyname><forenames>Anita</forenames></author></authors><title>Heuristic for Network Coverage Optimization Applied in Finding
  Organizational Change Agents</title><categories>cs.SI physics.soc-ph</categories><comments>The 2014 IEEE/ACM European Network Intelligence Conference. IEEE
  Computer Society, pp. 118-122</comments><doi>10.1109/ENIC.2014.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authors compare different ways of selecting change agents within network
analysis paradigm and propose a new algorithm of doing so. All methods are
evaluated against network coverage measure that calculates how many network
members can be directly reached by selected nodes. Results from the analysis of
organizational network show that compared to other methods the proposed
algorithm provides better network coverage, at the same time selecting change
agents that are well connected, influential opinion leaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0134</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0134</id><created>2014-11-29</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Classification of digital n-manifolds</title><categories>cs.DM</categories><comments>10 pages, 8 figures, Discrete Applied Mathematics, In Press,
  Available online 16 September 2014. arXiv admin note: text overlap with
  arXiv:1307.1371</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the classification of digital n-manifolds based on the
notion of complexity and homotopy equivalence. We introduce compressed
n-manifolds and study their properties. We show that any n-manifold with p
points is homotopy equivalent to a compressed n-manifold with m points, m&lt;p. We
design an algorithm for the classification of digital n-manifolds of any
dimension n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0143</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0143</id><created>2014-11-29</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Topology preserving representations of compact 2D manifolds by digital
  2-surfaces. Compressed digital models and digital weights of compact 2D
  manifolds. Classification of closed surfaces by digital tools</title><categories>cs.CG cs.DM math.AG</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using digital topology approach, we construct digital models of closed
surfaces as the intersection graphs of LCL covers of the surfaces. It is proved
that digital models of closed surfaces are digital 2-dimensional surfaces
preserving the geometry and topology of their continuous counterparts. In the
framework of the proposed models, we show that for any closed surface there
exists a compressed model of this surface with the minimal number of points.
  Key words: Closed Surface; Digital space; Cover; Graph; Digital model;
Medical imaging;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0152</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0152</id><created>2014-11-29</created><authors><author><keyname>Adelnia</keyname><forenames>Marzieh</forenames></author><author><keyname>Khayyambashi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Consistency of Commitments in Social Web Services</title><categories>cs.SI</categories><comments>5 pages, 1 Figure</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 12, No. 11, pp. 1-5, November 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Service is one of the most important information sharing technologies on
the web and one of the example of service oriented processing. To guarantee
accurate execution of web services operations, they must be accountable with
regulations of the social networks in which they sign up. This operations
implement using controls called 'Commitment'. This paper studies commitments,
then has an overview on existing researches, web service execution method using
commitments and information sharing methods between web services based on
commitments and social networks. A key challenge in this technique is
consistency ensuring in execution time. The aim of this study is presenting an
algorithm for consistency ensuring between commitments. An application designed
for proving correctness of algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0156</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0156</id><created>2014-11-29</created><authors><author><keyname>D&#xe9;fossez</keyname><forenames>Alexandre</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and
  Optimal Sampling Distributions</title><categories>cs.LG math.OC stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the least-squares regression problem and provide a detailed
asymptotic analysis of the performance of averaged constant-step-size
stochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex
case, we provide an asymptotic expansion up to explicit exponentially decaying
terms. Our analysis leads to new insights into stochastic approximation
algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the
generalization error may be divided into a variance term which is decaying as
O(1/n), independently of the step-size $\gamma$, and a bias term that decays as
O(1/$\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of a
good sampling density depends on whether the variance or bias terms dominate.
In particular, when the variance term dominates, optimal sampling densities do
not lead to much gain, while when the bias term dominates, we can choose larger
step-sizes that leads to significant improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0159</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0159</id><created>2014-11-29</created><authors><author><keyname>Cheung</keyname><forenames>Yun Kuen</forenames></author><author><keyname>Cole</keyname><forenames>Richard</forenames></author></authors><title>Amortized Analysis on Asynchronous Gradient Descent</title><categories>math.OC cs.GT</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient descent is an important class of iterative algorithms for minimizing
convex functions. Classically, gradient descent has been a sequential and
synchronous process. Distributed and asynchronous variants of gradient descent
have been studied since the 1980s, and they have been experiencing a resurgence
due to demand from large-scale machine learning problems running on multi-core
processors.
  We provide a version of asynchronous gradient descent (AGD) in which
communication between cores is minimal and for which there is little
synchronization overhead. We also propose a new timing model for its analysis.
With this model, we give the first amortized analysis of AGD on convex
functions. The amortization allows for bad updates (updates that increase the
value of the convex function); in contrast, most prior work makes the strong
assumption that every update must be significantly improving.
  Typically, the step sizes used in AGD are smaller than those used in its
synchronous counterpart. We provide a method to determine the step sizes in AGD
based on the Hessian entries for the convex function. In certain circumstances,
the resulting step sizes are a constant fraction of those used in the
corresponding synchronous algorithm, enabling the overall performance of AGD to
improve linearly with the number of cores.
  We give two applications of our amortized analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0165</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0165</id><created>2014-11-29</created><updated>2015-06-03</updated><authors><author><keyname>Ozyesil</keyname><forenames>Onur</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Robust Camera Location Estimation by Convex Programming</title><categories>cs.CV</categories><comments>10 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $3$D structure recovery from a collection of $2$D images requires the
estimation of the camera locations and orientations, i.e. the camera motion.
For large, irregular collections of images, existing methods for the location
estimation part, which can be formulated as the inverse problem of estimating
$n$ locations $\mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_n$ in
$\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions
$\frac{\mathbf{t}_i - \mathbf{t}_j}{\|\mathbf{t}_i - \mathbf{t}_j\|}$, are
sensitive to outliers in direction measurements. In this paper, we firstly
provide a complete characterization of well-posed instances of the location
estimation problem, by presenting its relation to the existing theory of
parallel rigidity. For robust estimation of camera locations, we introduce a
two-step approach, comprised of a pairwise direction estimation method robust
to outliers in point correspondences between image pairs, and a convex program
to maintain robustness to outlier directions. In the presence of partially
corrupted measurements, we empirically demonstrate that our convex formulation
can even recover the locations exactly. Lastly, we demonstrate the utility of
our formulations through experiments on Internet photo collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0171</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0171</id><created>2014-11-29</created><updated>2015-05-19</updated><authors><author><keyname>Wang</keyname><forenames>Jian-min</forenames></author><author><keyname>Xie</keyname><forenames>Tian-yu</forenames></author><author><keyname>Zhang</keyname><forenames>Hong-fei</forenames></author><author><keyname>Yang</keyname><forenames>Dong-xu</forenames></author><author><keyname>Xie</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>A bias-free quantum random number generation using photon arrival time
  selectively</title><categories>quant-ph cs.CR</categories><journal-ref>IEEE Photonics Journal, Volume:7 , Issue: 2. 2015</journal-ref><doi>10.1109/JPHOT.2015.2402127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a high-quality, bias-free quantum random number generator (QRNG)
using photon arrival time selectively in accordance with the number of photon
detection events within a sampling time interval in attenuated light. It is
well showed in both theoretical analysis and experiments verification that this
random number production method eliminates both bias and correlation perfectly
without more post processing and the random number can clearly pass the
standard randomness tests. We fulfill theoretical analysis and experimental
verification of the method whose rate can reach up to 45Mbps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0180</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0180</id><created>2014-11-30</created><updated>2015-02-05</updated><authors><author><keyname>Kalathil</keyname><forenames>Dileep</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Empirical Q-Value Iteration</title><categories>math.OC cs.LG</categories><comments>21 pages, Submitted to SIAM Journal on Optimization and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new simple and natural algorithm for learning the optimal
$Q$-value function of a discounted-cost Markov Decision Process (MDP) when the
transition kernels are unknown. Unlike the classical learning algorithms for
MDPs, such as $Q$-learning and `actor-critic' algorithms, this algorithm
doesn't depend on a stochastic approximation-based method. We show that our
algorithm, which we call the empirical $Q$-value iteration (EQVI) algorithm,
converges almost surely to the optimal $Q$-value function. To the best of our
knowledge, this is the first algorithm for learning in MDPs that guarantees an
almost sure convergence without using stochastic approximations. We also give a
rate of convergence or a non-aymptotic sample complexity bound, and also show
that an asynchronous (or online) version of the algorithm will also work.
Preliminary experimental results suggest a faster rate of convergence to a ball
park estimate for our algorithm compared to stochastic approximation-based
algorithms. In fact, the asynchronous setting EQVI vastly outperforms the
popular and widely-used Q-learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0193</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0193</id><created>2014-11-30</created><updated>2015-08-10</updated><authors><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Conrado</forenames></author></authors><title>Analysis of Pivot Sampling in Dual-Pivot Quicksort</title><categories>cs.DS math.PR</categories><comments>This article is identical (up to typograhical details) to the
  Algorithmica version available from Springerlink (see DOI). It is an extended
  and improved version of our corresponding article at the AofA 2014 conference
  [arXiv:1403.6602]</comments><doi>10.1007/s00453-015-0041-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new dual-pivot Quicksort by Vladimir Yaroslavskiy - used in Oracle's Java
runtime library since version 7 - features intriguing asymmetries. They make a
basic variant of this algorithm use less comparisons than classic single-pivot
Quicksort. In this paper, we extend the analysis to the case where the two
pivots are chosen as fixed order statistics of a random sample. Surprisingly,
dual-pivot Quicksort then needs more comparisons than a corresponding version
of classic Quicksort, so it is clear that counting comparisons is not
sufficient to explain the running time advantages observed for Yaroslavskiy's
algorithm in practice. Consequently, we take a more holistic approach and give
also the precise leading term of the average number of swaps, the number of
executed Java Bytecode instructions and the number of scanned elements, a new
simple cost measure that approximates I/O costs in the memory hierarchy. We
determine optimal order statistics for each of the cost measures. It turns out
that the asymmetries in Yaroslavskiy's algorithm render pivots with a
systematic skew more efficient than the symmetric choice. Moreover, we finally
have a convincing explanation for the success of Yaroslavskiy's algorithm in
practice: Compared with corresponding versions of classic single-pivot
Quicksort, dual-pivot Quicksort needs significantly less I/Os, both with and
without pivot sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0197</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0197</id><created>2014-11-30</created><updated>2015-02-25</updated><authors><author><keyname>Thuseethan</keyname><forenames>Selvarajah</forenames></author><author><keyname>Achchuthan</keyname><forenames>Sivapalan</forenames></author><author><keyname>Kuhanesan</keyname><forenames>Sinnathamby</forenames></author></authors><title>Usability Evaluation of Learning Management Systems in Sri Lankan
  Universities</title><categories>cs.HC</categories><comments>12 pages, 10 figures, Submitted for Advances in Human Computer
  Interaction</comments><msc-class>65Q06</msc-class><journal-ref>Global Journal of Computer Science and Technology, 15(1) (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As far as Learning Management System is concerned, it offers an integrated
platform for educational materials, distribution and management of learning as
well as accessibility by a range of users including teachers, learners and
content makers especially for distance learning. Usability evaluation is
considered as one approach to assess the efficiency of e-Learning systems. It
is used to evaluate how well technology and tools are working for users. There
are some factors contributing as major reason why LMS is not used effectively
and regularly. Learning Management Systems, as major part of e-Learning
systems, can benefit from usability research to evaluate the LMS ease of use
and satisfaction among its handlers. Many academic institutions worldwide
prefer using their own customized Learning Management Systems; this is the case
with Moodle, an open source Learning Management Systems platform designed and
operated by most of the universities in Sri Lanka. This paper gives an overview
of Learning Management Systems used in Sri Lankan universities, and evaluates
its usability using some pre-defined usability standards. In addition it
measures the effectiveness of Learning Management System by testing the
Learning Management Systems. The findings and result of this study as well as
the testing are discussed and presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0218</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0218</id><created>2014-11-30</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Simple pairs of points in digital spaces. Topology-preserving
  transformations of digital spaces by contracting simple pairs of points</title><categories>cs.DM cs.CV</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformations of digital spaces preserving local and global topology play
an important role in thinning, skeletonization and simplification of digital
images. In the present paper, we introduce and study contractions of simple
pair of points based on the notions of a digital contractible space and
contractible transformations of digital spaces. We show that the contraction of
a simple pair of points preserves local and global topology of a digital space.
Relying on the obtained results, we study properties if digital manifolds. In
particular, we show that a digital n-manifold can be transformed to its
compressed form with the minimal number of points by sequential contractions of
simple pairs.
  Key Words: Graph, digital space, contraction, splitting, simple pair,
homotopy, thinning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0223</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0223</id><created>2014-11-30</created><updated>2015-11-10</updated><authors><author><keyname>Cheng</keyname><forenames>Peng</forenames></author><author><keyname>Lian</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Zhao</forenames></author><author><keyname>Fu</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Han</keyname><forenames>Jinsong</forenames></author><author><keyname>Zhao</keyname><forenames>Jizhong</forenames></author></authors><title>Reliable Diversity-Based Spatial Crowdsourcing by Moving Workers</title><categories>cs.DB</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of mobile devices and the crowdsourcig platforms,
the spatial crowdsourcing has attracted much attention from the database
community, specifically, spatial crowdsourcing refers to sending a
location-based request to workers according to their positions. In this paper,
we consider an important spatial crowdsourcing problem, namely reliable
diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as
taking videos/photos of a landmark or firework shows, and checking whether or
not parking spaces are available) are time-constrained, and workers are moving
towards some directions. Our RDB-SC problem is to assign workers to spatial
tasks such that the completion reliability and the spatial/temporal diversities
of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and
intractable. Thus, we propose three effective approximation approaches,
including greedy, sampling, and divide-and-conquer algorithms. In order to
improve the efficiency, we also design an effective cost-model-based index,
which can dynamically maintain moving workers and spatial tasks with low cost,
and efficiently facilitate the retrieval of RDB-SC answers. Through extensive
experiments, we demonstrate the efficiency and effectiveness of our proposed
approaches over both real and synthetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0233</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0233</id><created>2014-11-30</created><updated>2015-01-21</updated><authors><author><keyname>Choromanska</keyname><forenames>Anna</forenames></author><author><keyname>Henaff</keyname><forenames>Mikael</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Arous</keyname><forenames>G&#xe9;rard Ben</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>The Loss Surfaces of Multilayer Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the connection between the highly non-convex loss function of a
simple model of the fully-connected feed-forward neural network and the
Hamiltonian of the spherical spin-glass model under the assumptions of: i)
variable independence, ii) redundancy in network parametrization, and iii)
uniformity. These assumptions enable us to explain the complexity of the fully
decoupled neural network through the prism of the results from random matrix
theory. We show that for large-size decoupled networks the lowest critical
values of the random loss function form a layered structure and they are
located in a well-defined band lower-bounded by the global minimum. The number
of local minima outside that band diminishes exponentially with the size of the
network. We empirically verify that the mathematical model exhibits similar
behavior as the computer simulations, despite the presence of high dependencies
in real networks. We conjecture that both simulated annealing and SGD converge
to the band of low critical points, and that all critical points found there
are local minima of high quality measured by the test error. This emphasizes a
major difference between large- and small-size networks where for the latter
poor quality local minima have non-zero probability of being recovered.
Finally, we prove that recovering the global minimum becomes harder as the
network size increases and that it is in practice irrelevant as global minimum
often leads to overfitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0251</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0251</id><created>2014-11-30</created><authors><author><keyname>Perrone</keyname><forenames>Daniele</forenames></author><author><keyname>Favaro</keyname><forenames>Paolo</forenames></author></authors><title>A Clearer Picture of Blind Deconvolution</title><categories>cs.CV</categories><comments>Submitted to IEEE Transaction on Pattern Analysis and Machine
  Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution is the problem of recovering a sharp image and a blur
kernel from a noisy blurry image. Recently, there has been a significant effort
on understanding the basic mechanisms to solve blind deconvolution. While this
effort resulted in the deployment of effective algorithms, the theoretical
findings generated contrasting views on why these approaches worked. On the one
hand, one could observe experimentally that alternating energy minimization
algorithms converge to the desired solution. On the other hand, it has been
shown that such alternating minimization algorithms should fail to converge and
one should instead use a so-called Variational Bayes approach. To clarify this
conundrum, recent work showed that a good image and blur prior is instead what
makes a blind deconvolution algorithm work. Unfortunately, this analysis did
not apply to algorithms based on total variation regularization. In this
manuscript, we provide both analysis and experiments to get a clearer picture
of blind deconvolution. Our analysis reveals the very reason why an algorithm
based on total variation works. We also introduce an implementation of this
algorithm and show that, in spite of its extreme simplicity, it is very robust
and achieves a performance comparable to the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0252</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0252</id><created>2014-11-30</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Brown</keyname><forenames>D. Richard</forenames><suffix>III</suffix></author></authors><title>Channel Estimation Techniques for Quantized Distributed Reception in
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Proceedings of the 2014 Asilomar Conference on Signals, Systems &amp;
  Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) could enable the development of cloud
multiple-input multiple-output (MIMO) systems where internet-enabled devices
can work as distributed transmission/reception entities. We expect that spatial
multiplexing with distributed reception using cloud MIMO would be a key factor
of future wireless communication systems. In this paper, we first review
practical receivers for distributed reception of spatially multiplexed transmit
data where the fusion center relies on quantized received signals conveyed from
geographically separated receive nodes. Using the structures of these
receivers, we propose practical channel estimation techniques for the
block-fading scenario. The proposed channel estimation techniques rely on very
simple operations at the received nodes while achieving near-optimal channel
estimation performance as the training length becomes large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0260</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0260</id><created>2014-11-30</created><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Zeinalpour-Yazdi</keyname><forenames>Zolfa</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Outage Performance of Uplink Two-tier Networks Under Backhaul
  Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tier cellular communication networks constitute a promising approach to
expand the coverage of cellular networks and enable them to offer higher data
rates. In this paper, an uplink two-tier communication network is studied, in
which macro users, femto users and femto access points are geometrically
located inside the coverage area of a macro base station according to Poisson
point processes. Each femtocell is assumed to have a fixed backhaul constraint
that puts a limit on the maximum number of femto and macro users it can
service. Under this backhaul constraint, the network adopts a special open
access policy, in which each macro user is either assigned to its closest femto
access point or to the macro base station, depending on the ratio between its
distances from those two. Under this model, upper and lower bounds on the
outage probabilities experienced by users serviced by femto access points are
derived as functions of the distance between the macro base station and the
femto access point serving them. Similarly, upper and lower bounds on the
outage probabilities of the users serviced by the macro base station are
obtained. The bounds in both cases are confirmed via simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0265</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0265</id><created>2014-11-30</created><updated>2015-03-17</updated><authors><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author></authors><title>Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels</title><categories>cs.CV</categories><doi>10.1109/TPAMI.2015.2414422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop an approach to exploiting kernel methods with
manifold-valued data. In many computer vision problems, the data can be
naturally represented as points on a Riemannian manifold. Due to the
non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision
and machine learning algorithms yield inferior results on such data. In this
paper, we define Gaussian radial basis function (RBF)-based positive definite
kernels on manifolds that permit us to embed a given manifold with a
corresponding metric in a high dimensional reproducing kernel Hilbert space.
These kernels make it possible to utilize algorithms developed for linear
spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with
any given metric is not always positive definite, we present a unified
framework for analyzing the positive definiteness of the Gaussian RBF on a
generic metric space. We then use the proposed framework to identify positive
definite kernels on two specific manifolds commonly encountered in computer
vision: the Riemannian manifold of symmetric positive definite matrices and the
Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a
Euclidean space. We show that many popular algorithms designed for Euclidean
spaces, such as support vector machines, discriminant analysis and principal
component analysis can be generalized to Riemannian manifolds with the help of
such positive definite Gaussian kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0271</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0271</id><created>2014-11-30</created><updated>2015-12-21</updated><authors><author><keyname>Cseh</keyname><forenames>&#xc1;gnes</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author></authors><title>Stable marriage and roommates problems with restricted edges: complexity
  and approximability</title><categories>cs.DM cs.DS</categories><comments>conference version appeared at SAGT 2015, 29 pages</comments><msc-class>05C85</msc-class><acm-class>G.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the stable marriage and roommates problems, a set of agents is given, each
of them having a strictly ordered preference list over some or all of the other
agents. A matching is a set of disjoint pairs of mutually accepted agents. If
any two agents mutually prefer each other to their partner, then they block the
matching, otherwise, the matching is said to be stable. In this paper we
investigate the complexity of finding a solution satisfying additional
constraints on restricted pairs of agents. Restricted pairs can be either
forced or forbidden. A stable solution must contain all of the forced pairs,
while it must contain none of the forbidden pairs.
  Dias et al. gave a polynomial-time algorithm to decide whether such a
solution exists in the presence of restricted edges. If the answer is no, one
might look for a solution close to optimal. Since optimality in this context
means that the matching is stable and satisfies all constraints on restricted
pairs, there are two ways of relaxing the constraints by permitting a solution
to: (1) be blocked by some pairs (as few as possible), or (2) violate some
constraints on restricted pairs (again as few as possible).
  Our main theorems prove that for the (bipartite) stable marriage problem,
case (1) leads to NP-hardness and inapproximability results, whilst case (2)
can be solved in polynomial time. For the non-bipartite stable roommates
instances, case (2) yields an NP-hard but (under some cardinality assumptions)
2-approximable problem. In the case of NP-hard problems, we also discuss
polynomially solvable special cases, arising from restrictions on the lengths
of the preference lists, or upper bounds on the numbers of restricted pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0279</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0279</id><created>2014-11-30</created><updated>2015-03-22</updated><authors><author><keyname>Shchesnovich</keyname><forenames>Valery S.</forenames></author></authors><title>Boson-Sampling with non-interacting fermions</title><categories>quant-ph cond-mat.other cs.CC</categories><comments>Minor misprints corrected. 19 pages; 3 figures (one colored)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the conditions under which identical particles in unitary linear
networks behave as the other species, i.e. bosons as fermions and fermions as
bosons. It is found that the Boson-Sampling computer of Aaronson &amp; Arkhipov can
be implemented in an interference experiment with non-interacting fermions in
an appropriately entangled state. Moreover, a scheme is proposed which
simulates the scattershot version of the Boson-Sampling computer by preparing,
on the fly, the required entangled state of fermions from an unentangled one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0291</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0291</id><created>2014-11-30</created><authors><author><keyname>Wibral</keyname><forenames>Michael</forenames></author><author><keyname>Lizier</keyname><forenames>Joseph T.</forenames></author><author><keyname>Priesemann</keyname><forenames>Viola</forenames></author></authors><title>Bits from Biology for Computational Intelligence</title><categories>q-bio.NC cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational intelligence is broadly defined as biologically-inspired
computing. Usually, inspiration is drawn from neural systems. This article
shows how to analyze neural systems using information theory to obtain
constraints that help identify the algorithms run by such systems and the
information they represent. Algorithms and representations identified
information-theoretically may then guide the design of biologically inspired
computing systems (BICS). The material covered includes the necessary
introduction to information theory and the estimation of information theoretic
quantities from neural data. We then show how to analyze the information
encoded in a system about its environment, and also discuss recent
methodological developments on the question of how much information each agent
carries about the environment either uniquely, or redundantly or
synergistically together with others. Last, we introduce the framework of local
information dynamics, where information processing is decomposed into component
processes of information storage, transfer, and modification -- locally in
space and time. We close by discussing example applications of these measures
to neural data and other complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0296</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0296</id><created>2014-11-30</created><authors><author><keyname>Papandreou</keyname><forenames>George</forenames></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames></author><author><keyname>Savalle</keyname><forenames>Pierre-Andr&#xe9;</forenames></author></authors><title>Untangling Local and Global Deformations in Deep Convolutional Networks
  for Image Classification and Sliding Window Detection</title><categories>cs.CV</categories><comments>13 pages, 7 figures, 5 tables. arXiv admin note: substantial text
  overlap with arXiv:1406.2732</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling'
(MP) layers to extract deformation-invariant features, but we argue in favor of
a more refined treatment. First, we introduce epitomic convolution as a
building block alternative to the common convolution-MP cascade of DCNNs; while
having identical complexity to MP, Epitomic Convolution allows for parameter
sharing across different filters, resulting in faster convergence and better
generalization. Second, we introduce a Multiple Instance Learning approach to
explicitly accommodate global translation and scaling when training a DCNN
exclusively with class labels. For this we rely on a `patchwork' data structure
that efficiently lays out all image scales and positions as candidates to a
DCNN. Factoring global and local deformations allows a DCNN to `focus its
resources' on the treatment of non-rigid deformations and yields a substantial
classification accuracy improvement. Third, further pursuing this idea, we
develop an efficient DCNN sliding window object detector that employs explicit
search over position, scale, and aspect ratio. We provide competitive image
classification and localization results on the ImageNet dataset and object
detection results on the Pascal VOC 2007 benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0301</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0301</id><created>2014-11-30</created><authors><author><keyname>Deshpande</keyname><forenames>Ajay</forenames></author></authors><title>Guaranteed sensor coverage with the weighted-$D^2$ sampling</title><categories>cs.SY cs.MA math.OC</categories><comments>Submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the mobile sensor coverage problem formulated as a
continuous locational optimization problem. Cort\`es et al. first proposed a
distributed version of the Lloyd descent algorithm with guaranteed convergence
to a local optima. Since then researchers have studied a number of variations
of the coverage problem. The quality of the final solution with the Lloyd
descent depends on the initial sensor configuration. Inspired by the recent
results on a related $k$-means problem, in this paper we propose the
weighted-$D^2$ sampling to choose the initial sensor configuration and show
that it yields $O(\log k)$-competitive sensor coverage before even applying the
Lloyd descent. Through extensive numerical simulations, we show that the
initial coverage with the weighted-$D^2$ sampling is significantly lower than
that with the uniform random initial sensor configuration. We also show that
the average distance traveled by the sensors to reach the final configuration
through the Lloyd descent is also significantly lower than that with the
uniform random configuration. This also implies considerable savings in the
energy spent by the sensors during motion and faster convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0305</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0305</id><created>2014-11-30</created><authors><author><keyname>Guo</keyname><forenames>Alan</forenames></author><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author></authors><title>List-decoding algorithms for lifted codes</title><categories>cs.IT math.IT</categories><comments>12 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifted Reed-Solomon codes are a natural affine-invariant family of
error-correcting codes which generalize Reed-Muller codes. They were known to
have efficient local-testing and local-decoding algorithms (comparable to the
known algorithms for Reed-Muller codes), but with significantly better rate. We
give efficient algorithms for list-decoding and local list-decoding of lifted
codes. Our algorithms are based on a new technical lemma, which says that
codewords of lifted codes are low degree polynomials when viewed as univariate
polynomials over a big field (even though they may be very high degree when
viewed as multivariate polynomials over a small field).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0307</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0307</id><created>2014-11-30</created><authors><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>Wagner</keyname><forenames>Markus</forenames></author></authors><title>Seeding the Initial Population of Multi-Objective Evolutionary
  Algorithms: A Computational Study</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most experimental studies initialize the population of evolutionary
algorithms with random genotypes. In practice, however, optimizers are
typically seeded with good candidate solutions either previously known or
created according to some problem-specific method. This &quot;seeding&quot; has been
studied extensively for single-objective problems. For multi-objective
problems, however, very little literature is available on the approaches to
seeding and their individual benefits and disadvantages. In this article, we
are trying to narrow this gap via a comprehensive computational study on common
real-valued test functions. We investigate the effect of two seeding techniques
for five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8
objectives. We observe that some functions (e.g., DTLZ4 and the LZ family)
benefit significantly from seeding, while others (e.g., WFG) profit less. The
advantage of seeding also depends on the examined algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0315</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0315</id><created>2014-11-30</created><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author></authors><title>Lifted Probabilistic Inference for Asymmetric Graphical Models</title><categories>cs.AI</categories><comments>To appear in Proceedings of AAAI-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifted probabilistic inference algorithms have been successfully applied to a
large number of symmetric graphical models. Unfortunately, the majority of
real-world graphical models is asymmetric. This is even the case for relational
representations when evidence is given. Therefore, more recent work in the
community moved to making the models symmetric and then applying existing
lifted inference algorithms. However, this approach has two shortcomings.
First, all existing over-symmetric approximations require a relational
representation such as Markov logic networks. Second, the induced symmetries
often change the distribution significantly, making the computed probabilities
highly biased. We present a framework for probabilistic sampling-based
inference that only uses the induced approximate symmetries to propose steps in
a Metropolis-Hastings style Markov chain. The framework, therefore, leads to
improved probability estimates while remaining unbiased. Experiments
demonstrate that the approach outperforms existing MCMC algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0320</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0320</id><created>2014-11-30</created><updated>2015-01-24</updated><authors><author><keyname>Shen</keyname><forenames>Yuping</forenames></author><author><keyname>Zhao</keyname><forenames>Xishun</forenames></author></authors><title>Canonical Logic Programs are Succinctly Incomparable with Propositional
  Formulas</title><categories>cs.LO cs.AI</categories><comments>This is an extended version of a conference paper with the same name
  in KR2014</comments><msc-class>03F20</msc-class><acm-class>I.2.3; I.2.4; F.1.3; F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \emph{Canonical (logic) programs} (CP) refer to normal logic programs
augmented with connective $not\ not$. In this paper we address the question of
whether CP are \emph{succinctly incomparable} with \emph{propositional
formulas} (PF). Our main result shows that the PARITY problem, which can be
polynomially represented in PF but \emph{only} has exponential representations
in CP. In other words, PARITY \emph{separates} PF from CP. Simply speaking,
this means that exponential size blowup is generally inevitable when
translating a set of formulas in PF into an equivalent program in CP (without
introducing new variables). Furthermore, since it has been shown by Lifschitz
and Razborov that there is also a problem that separates CP from PF (assuming
$\mathsf{P}\nsubseteq \mathsf{NC^1/poly}$), it follows that CP and PF are
indeed succinctly incomparable. From the view of the theory of computation, the
above result may also be considered as the separation of two \emph{models of
computation}, i.e., we identify a language in $\mathsf{NC^1/poly}$ which is not
in the set of languages computable by polynomial size CP programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0321</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0321</id><created>2014-11-30</created><updated>2014-12-08</updated><authors><author><keyname>Liu</keyname><forenames>Jiajun</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Sommer</keyname><forenames>Philipp</forenames></author><author><keyname>Shang</keyname><forenames>Shuo</forenames></author><author><keyname>Kusy</keyname><forenames>Brano</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author></authors><title>Bounded Quadrant System: Error-bounded Trajectory Compression on the Go</title><categories>cs.DS cs.DB</categories><comments>International Conference on Data Engineering (ICDE) 2015, 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long-term location tracking, where trajectory compression is commonly used,
has gained high interest for many applications in transport, ecology, and
wearable computing. However, state-of-the-art compression methods involve high
space-time complexity or achieve unsatisfactory compression rate, leading to
rapid exhaustion of memory, computation, storage and energy resources. We
propose a novel online algorithm for error-bounded trajectory compression
called the Bounded Quadrant System (BQS), which compresses trajectories with
extremely small costs in space and time using convex-hulls. In this algorithm,
we build a virtual coordinate system centered at a start point, and establish a
rectangular bounding box as well as two bounding lines in each of its
quadrants. In each quadrant, the points to be assessed are bounded by the
convex-hull formed by the box and lines. Various compression error-bounds are
therefore derived to quickly draw compression decisions without expensive error
computations. In addition, we also propose a light version of the BQS version
that achieves $\mathcal{O}(1)$ complexity in both time and space for processing
each point to suit the most constrained computation environments. Furthermore,
we briefly demonstrate how this algorithm can be naturally extended to the 3-D
case.
  Using empirical GPS traces from flying foxes, cars and simulation, we
demonstrate the effectiveness of our algorithm in significantly reducing the
time and space complexity of trajectory compression, while greatly improving
the compression rates of the state-of-the-art algorithms (up to 47%). We then
show that with this algorithm, the operational time of the target
resource-constrained hardware platform can be prolonged by up to 41%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0325</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0325</id><created>2014-11-30</created><updated>2015-09-20</updated><authors><author><keyname>Arulselvan</keyname><forenames>Ashwin</forenames></author><author><keyname>Cseh</keyname><forenames>&#xc1;gnes</forenames></author><author><keyname>Gro&#xdf;</keyname><forenames>Martin</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author><author><keyname>Matuschke</keyname><forenames>Jannik</forenames></author></authors><title>Many-to-one matchings with lower quotas: Algorithms and complexity</title><categories>cs.DM cs.DS</categories><comments>restricted version has appeared at ISAAC 2015</comments><msc-class>05C85</msc-class><acm-class>G.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural generalization of the maximum weight many-to-one matching
problem. We are given an undirected bipartite graph $G= (A \cup P, E)$ with
weights on the edges in $E$, and with lower and upper quotas on the vertices in
$P$. We seek a maximum weight many-to-one matching satisfying two sets of
constraints: vertices in $A$ are incident to at most one matching edge, while
vertices in $P$ are either unmatched or they are incident to a number of
matching edges between their lower and upper quota. This problem, which we call
maximum weight many-to-one matching with lower and upper quotas (WMLQ), has
applications to the assignment of students to projects within university
courses, where there are constraints on the minimum and maximum numbers of
students that must be assigned to each project.
  In this paper, we provide a comprehensive analysis of the complexity of WMLQ
from the viewpoints of classic polynomial time algorithms, fixed-parameter
tractability, as well as approximability. We draw the line between NP-hard and
polynomially tractable instances in terms of degree and quota constraints and
provide efficient algorithms to solve the tractable ones. We further show that
the problem can be solved in polynomial time for instances with bounded
treewidth; however, the corresponding runtime is exponential in the treewidth
with the maximum upper quota $u_{max}$ as basis, and we prove that this
dependence is necessary unless FPT = W[1]. Finally, we also present an
approximation algorithm for the general case with performance guarantee
$u_{max}+1$, which is asymptotically best possible unless P = NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0327</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0327</id><created>2014-11-30</created><authors><author><keyname>Liu</keyname><forenames>Jiajun</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Khan</keyname><forenames>Saeed</forenames></author><author><keyname>Cameron</keyname><forenames>Mark</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author></authors><title>Multi-scale Population and Mobility Estimation with Geo-tagged Tweets</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>1st International Workshop on Big Data Analytics for Biosecurity
  (BioBAD2015), 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent outbreaks of Ebola and Dengue viruses have again elevated the
significance of the capability to quickly predict disease spread in an emergent
situation. However, existing approaches usually rely heavily on the
time-consuming census processes, or the privacy-sensitive call logs, leading to
their unresponsive nature when facing the abruptly changing dynamics in the
event of an outbreak. In this paper we study the feasibility of using
large-scale Twitter data as a proxy of human mobility to model and predict
disease spread. We report that for Australia, Twitter users' distribution
correlates well the census-based population distribution, and that the Twitter
users' travel patterns appear to loosely follow the gravity law at multiple
scales of geographic distances, i.e. national level, state level and
metropolitan level. The radiation model is also evaluated on this dataset
though it has shown inferior fitness as a result of Australia's sparse
population and large landmass. The outcomes of the study form the cornerstones
for future work towards a model-based, responsive prediction method from
Twitter data for disease spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0340</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0340</id><created>2014-11-30</created><updated>2015-04-14</updated><authors><author><keyname>Wang</keyname><forenames>Yi-Kai</forenames></author></authors><title>MAX-CSP, Graph Cuts and Statistical Physics</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretical models providing mathematical abstractions for several
significant optimization problems in machine learning, combinatorial
optimization, computer vision and statistical physics have intrinsic
similarities. We propose a unified framework to model these computation tasks
where the structures of these optimization problems are encoded by functions
attached on the vertices and edges of a graph. We show that computing MAX 2-CSP
admits polynomial-time approximation scheme (PTAS) on planar graphs, graphs
with bounded local treewidth, $H$-minor-free graphs, geometric graphs with
bounded density and graphs embeddable with bounded number of crossings per
edge. This implies computing MAX-CUT, MAX-DICUT and MAX $k$-CUT admits PTASs on
all these classes of graphs. Our method also gives the first PTAS for computing
the ground state of ferromagnetic Edwards-Anderson model without external
magnetic field on $d$-dimensional lattice graphs. These results are widely
applicable in vision, graphics and machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0348</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0348</id><created>2014-11-30</created><updated>2015-04-13</updated><authors><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author></authors><title>Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless
  SETH is false)</title><categories>cs.CC cs.DS</categories><comments>STOC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The edit distance (a.k.a. the Levenshtein distance) between two strings is
defined as the minimum number of insertions, deletions or substitutions of
symbols needed to transform one string into another. The problem of computing
the edit distance between two strings is a classical computational task, with a
well-known algorithm based on dynamic programming. Unfortunately, all known
algorithms for this problem run in nearly quadratic time.
  In this paper we provide evidence that the near-quadratic running time bounds
known for the problem of computing edit distance might be tight. Specifically,
we show that, if the edit distance can be computed in time O(n^{2-delta}) for
some constant delta&gt;0, then the satisfiability of conjunctive normal form
formulas with N variables and M clauses can be solved in time M^{O(1)}
2^{(1-eps)N} for a constant eps&gt;0. The latter result would violate the Strong
Exponential Time Hypothesis, which postulates that such algorithms do not
exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0349</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0349</id><created>2014-12-01</created><updated>2015-08-26</updated><authors><author><keyname>Liu</keyname><forenames>Wanchun</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Secure Communication with a Wireless-Powered Friendly Jammer</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to use a wireless-powered friendly jammer to enable
secure communication between a source node and destination node, in the
presence of an eavesdropper. We consider a two-phase communication protocol
with fixed-rate transmission. In the first phase, wireless power transfer is
conducted from the source to the jammer. In the second phase, the source
transmits the information-bearing signal under the protection of a jamming
signal sent by the jammer using the harvested energy in the first phase. We
analytically characterize the long-time behavior of the proposed protocol and
derive a closed-form expression for the throughput. We further optimize the
rate parameters for maximizing the throughput subject to a secrecy outage
probability constraint. Our analytical results show that the throughput
performance differs significantly between the single-antenna jammer case and
the multi-antenna jammer case. For instance, as the source transmit power
increases, the throughput quickly reaches an upper bound with single-antenna
jammer, while the throughput grows unbounded with multi-antenna jammer. Our
numerical results also validate the derived analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0356</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0356</id><created>2014-12-01</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>An Algorithmic Separating Hyperplane Theorem and Its Applications</title><categories>cs.CC</categories><comments>27 pages, 14 figures</comments><msc-class>90C25, 90C05, 90C06, 90C46, 90C51, 52B55, 91B02, 68T05, 68T10, 68W25</msc-class><acm-class>G.1.6; I.3.5; G.1.3; I.2; I.5; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first prove a new separating hyperplane theorem characterizing when a pair
of compact convex subsets $K, K'$ of the Euclidean space intersect, and when
they are disjoint. The theorem is distinct from classical separation theorems.
It generalizes the {\it distance duality} proved in our earlier work for
testing the membership of a distinguished point in the convex hull of a finite
point set. Next by utilizing the theorem, we develop a substantially
generalized and stronger version of the {\it Triangle Algorithm} introduced in
the previous work to perform any of the following three tasks: (1) To compute a
pair $(p,p') \in K \times K'$, where either the Euclidean distance $d(p,p')$ is
to within a prescribed tolerance, or the orthogonal bisecting hyperplane of the
line segment $pp'$ separates the two sets; (2) When $K$ and $K'$ are disjoint,
to compute $(p,p') \in K \times K'$ so that $d(p,p')$ approximates $d(K,K')$ to
within a prescribed tolerance; (3) When $K$ and $K'$ are disjoint, to compute a
pair of parallel supporting hyperplanes $H,H'$ so that $d(H,H')$ is to within a
prescribed tolerance of the optimal margin. The worst-case complexity of each
iteration is solving a linear objective over $K$ or $K'$. The resulting
algorithm is a fully polynomial-time approximation scheme for such important
special cases as when $K$ and $K'$ are convex hulls of finite points sets, or
the intersection of a finite number of halfspaces. The results find many
theoretical and practical applications, such as in machine learning,
statistics, linear, quadratic and convex programming. In a forthcoming work we
show how the algorithms can provide a certain approximation to NP-complete
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0364</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0364</id><created>2014-12-01</created><updated>2015-10-18</updated><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Garcia-Molina</keyname><forenames>Hector</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Interactive Data Exploration with Smart Drill-Down</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present {\em smart drill-down}, an operator for interactively exploring a
relational table to discover and summarize &quot;interesting&quot; groups of tuples. Each
group of tuples is described by a {\em rule}. For instance, the rule $(a, b,
\star, 1000)$ tells us that there are a thousand tuples with value $a$ in the
first column and $b$ in the second column (and any value in the third column).
Smart drill-down presents an analyst with a list of rules that together
describe interesting aspects of the table. The analyst can tailor the
definition of interesting, and can interactively apply smart drill-down on an
existing rule to explore that part of the table. We demonstrate that the
underlying optimization problems are {\sc NP-Hard}, and describe an algorithm
for finding the approximately optimal list of rules to display when the user
uses a smart drill-down, and a dynamic sampling scheme for efficiently
interacting with large tables. Finally, we perform experiments on real datasets
on our experimental prototype to demonstrate the usefulness of smart drill-down
and study the performance of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0366</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0366</id><created>2014-12-01</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author><author><keyname>Mumford</keyname><forenames>Philip</forenames></author></authors><title>Node Failure Time and Coverage Loss Time Analysis for Maximum Stability
  Vs Minimum Distance Spanning Tree based Data Gathering in Mobile Sensor
  Networks</title><categories>cs.NI</categories><comments>16 pages, 11 figures</comments><journal-ref>IJCNC, 5(4): 15-30, 2013</journal-ref><doi>10.5121/ijcnc.2013.5402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile sensor network is a wireless network of sensor nodes that move
arbitrarily. In this paper, we explore the use of a maximum stability spanning
tree-based data gathering (Max.Stability-DG) algorithm and a minimum-distance
spanning tree-based data gathering (MST-DG) algorithm for mobile sensor
networks. We analyze the impact of these two algorithms on the node failure
times and the resulting coverage loss due to node failures. Both the
Max.Stability-DG and MST-DG algorithms are based on a greedy strategy of
determining a data gathering tree when one is needed and using that tree as
long as it exists. The Max.Stability-DG algorithm assumes the availability of
the complete knowledge of future topology changes and determines a data
gathering tree whose corresponding spanning tree would exist for the longest
time since the current time instant; whereas, the MST-DG algorithm determines a
data gathering tree whose corresponding spanning tree is the minimum distance
tree at the current time instant. We observe the Max.Stability-DG trees to
incur a longer network lifetime (time of disconnection of the network of live
sensor nodes due to node failures), a larger coverage loss time for a
particular fraction of loss of coverage as well as a lower fraction of coverage
loss at any time. The tradeoff is that the Max.Stability-DG trees incur a lower
node lifetime (the time of first node failure) due to repeated use of a data
gathering tree for a longer time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0393</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0393</id><created>2014-12-01</created><updated>2015-10-26</updated><authors><author><keyname>Soodhalter</keyname><forenames>Kirk M.</forenames></author></authors><title>Block Krylov subspace recycling for shifted systems with unrelated
  right-hand sides</title><categories>math.NA cs.NA</categories><comments>24 pages, 4 figures, 2 tables</comments><msc-class>65F10, 65F50</msc-class><journal-ref>SIAM J. Sci. Comput., 38(1), A302-A324, 2016</journal-ref><doi>10.1137/140998214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Krylov subspace methods for shifted linear systems take advantage of the
invariance of the Krylov subspace under a shift of the matrix. However,
exploiting this fact in the non-Hermitian case introduces restrictions; e.g.,
initial residuals must be collinear and this collinearity must be maintained at
restart. Thus we cannot simultaneously solve shifted systems with unrelated
right-hand sides using this strategy, and all shifted residuals cannot be
simultaneously minimized over a Krylov subspace such that collinearity is
maintained. It has been shown that this renders them generally incompatible
with techniques of subspace recycling [Soodhalter et al. APNUM '14].
  This problem, however, can be overcome. By interpreting a family of shifted
systems as one Sylvester equation, we can take advantage of the known &quot;shift
invariance&quot; of the Krylov subspace generated by the Sylvester operator. Thus we
can simultaneously solve all systems over one block Krylov subspace using FOM
or GMRES type methods, even when they have unrelated right-hand sides. Because
residual collinearity is no longer a requirement at restart, these methods are
fully compatible with subspace recycling techniques. Furthermore, we realize
the benefits of block sparse matrix operations which arise in the context of
high-performance computing applications.
  In this paper, we discuss exploiting this Sylvester equation point of view
which has yielded methods for shifted systems which are compatible with
unrelated right-hand sides. From this, we propose a recycled GMRES method for
simultaneous solution of shifted systems.Numerical experiments demonstrate the
effectiveness of the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0422</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0422</id><created>2014-12-01</created><authors><author><keyname>Demirel</keyname><forenames>Burak</forenames></author><author><keyname>Guvenc</keyname><forenames>Levent</forenames></author></authors><title>Parameter Space Design of Repetitive Controllers for Satisfying a Robust
  Performance Requirement</title><categories>cs.SY</categories><journal-ref>IEEE Transactions on Automatic Control, 55(8): 1893-1899, Aug.
  2010</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A parameter space procedure for designing chosen parameters of a repetitive
controller to satisfy a robust performance criterion is presented. Using this
method, low order robust repetitive controllers can be designed and implemented
for plants that possibly include time delay, poles on the imaginary axis and
discontinuous weights. A design and simulation study based on a high speed
atomic force microscope position control example is utilized to illustrate the
method presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0423</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0423</id><created>2014-12-01</created><authors><author><keyname>Engels</keyname><forenames>Christian</forenames></author></authors><title>Dichotomy Theorems for Homomorphism Polynomials of Graph Classes</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will show dichotomy theorems for the computation of
polynomials corresponding to evaluation of graph homomorphisms in Valiant's
model. We are given a fixed graph $H$ and want to find all graphs, from some
graph class, homomorphic to this $H$. These graphs will be encoded by a family
of polynomials.
  We give dichotomies for the polynomials for cycles, cliques, trees,
outerplanar graphs, planar graphs and graphs of bounded genus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0426</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0426</id><created>2014-12-01</created><authors><author><keyname>Lasseter</keyname><forenames>John H. E.</forenames></author></authors><title>The Interpreter In An Undergraduate Compilers Course</title><categories>cs.CY</categories><comments>Final version to appear in SIGCSE '15</comments><doi>10.1145/2676723.2677314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An undergraduate compilers course poses significant challenges to students,
in both the conceptual richness of the major components and in the programming
effort necessary to implement them. In this paper, I argue that a related
architecture, the interpreter, serves as an effective conceptual framework in
which to teach some of the later stages of the compiler pipeline. This
framework can serve both to unify some of the major concepts that are taught in
a typical undergraduate course and to structure the implementation of a
semester-long compiler project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0436</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0436</id><created>2014-12-01</created><updated>2015-09-07</updated><authors><author><keyname>Torgo</keyname><forenames>Luis</forenames></author></authors><title>An Infra-Structure for Performance Estimation and Experimental
  Comparison of Predictive Models in R</title><categories>cs.MS cs.LG cs.SE stat.CO</categories><comments>Updated to version 1.0.2 of the R package. Added a small section on
  package installation. Made explicit the reference to the R package version
  number within the document</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes an infra-structure provided by the R package
performanceEstimation that allows to estimate the predictive performance of
different approaches (workflows) to predictive tasks. The infra-structure is
generic in the sense that it can be used to estimate the values of any
performance metrics, for any workflow on different predictive tasks, namely,
classification, regression and time series tasks. The package also includes
several standard workflows that allow users to easily set up their experiments
limiting the amount of work and information they need to provide. The overall
goal of the infra-structure provided by our package is to facilitate the task
of estimating the predictive performance of different modeling approaches to
predictive tasks in the R environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0439</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0439</id><created>2014-12-01</created><updated>2014-12-02</updated><authors><author><keyname>Lim</keyname><forenames>Chern Hong</forenames></author><author><keyname>Vats</keyname><forenames>Ekta</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author></authors><title>Fuzzy human motion analysis: A review</title><categories>cs.CV cs.AI</categories><comments>Accepted in Pattern Recognition, first survey paper that discusses
  and reviews fuzzy approaches towards HMA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Motion Analysis (HMA) is currently one of the most popularly active
research domains as such significant research interests are motivated by a
number of real world applications such as video surveillance, sports analysis,
healthcare monitoring and so on. However, most of these real world applications
face high levels of uncertainties that can affect the operations of such
applications. Hence, the fuzzy set theory has been applied and showed great
success in the recent past. In this paper, we aim at reviewing the fuzzy set
oriented approaches for HMA, individuating how the fuzzy set may improve the
HMA, envisaging and delineating the future perspectives. To the best of our
knowledge, there is not found a single survey in the current literature that
has discussed and reviewed fuzzy approaches towards the HMA. For ease of
understanding, we conceptually classify the human motion into three broad
levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0449</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0449</id><created>2014-12-01</created><updated>2014-12-15</updated><authors><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author><author><keyname>Yin</keyname><forenames>Zhangqi</forenames></author></authors><title>The Generation Cost of Bipartite Quantum States under LOCC</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors as similar results have
  been found by Francesco Buscemi and Nilanjana Datta</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a realistic setting of quantum tasks that generate shared
bipartite quantum states. Suppose \alice and \bob are located at different
places and need to produce a target shared quantum state $\rho$. In order to
save quantum communication, they can choose to share a proper smaller quantum
state $\sigma$ first, and then turn $\sigma$ to $\rho$ by performing only local
quantum operations and classical communications (LOCC). We hope $\sigma$ is the
optimal such that the quantum communication needed is as little as possible,
which is called the generation cost of $\rho$. In this paper, for an arbitrary
bipartite $\rho$, we characterize its generation cost completely by proving
that it is exactly equivalent to the logarithm of the Schmidt number of $\rho$.
Similar quantum schemes where classical communication is not allowed have
actually been considered. By comparing the two settings, we are able to look
into the role that classical communication plays in these fundamental tasks,
where we exhibit some instances in which classical communication is not helpful
completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0477</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0477</id><created>2014-12-01</created><updated>2015-04-24</updated><authors><author><keyname>Del Pero</keyname><forenames>Luca</forenames></author><author><keyname>Ricco</keyname><forenames>Susanna</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Recovering Spatiotemporal Correspondence between Deformable Objects by
  Exploiting Consistent Foreground Motion in Video</title><categories>cs.CV</categories><comments>9 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given unstructured videos of deformable objects, we automatically recover
spatiotemporal correspondences to map one object to another (such as animals in
the wild). While traditional methods based on appearance fail in such
challenging conditions, we exploit consistency in object motion between
instances. Our approach discovers pairs of short video intervals where the
object moves in a consistent manner and uses these candidates as seeds for
spatial alignment. We model the spatial correspondence between the point
trajectories on the object in one interval to those in the other using a
time-varying Thin Plate Spline deformation model. On a large dataset of tiger
and horse videos, our method automatically aligns thousands of pairs of frames
to a high accuracy, and outperforms the popular SIFT Flow algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0488</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0488</id><created>2014-12-01</created><authors><author><keyname>Pietzsch</keyname><forenames>Tobias</forenames></author><author><keyname>Saalfeld</keyname><forenames>Stephan</forenames></author><author><keyname>Preibisch</keyname><forenames>Stephan</forenames></author><author><keyname>Tomancak</keyname><forenames>Pavel</forenames></author></authors><title>BigDataViewer: Interactive Visualization and Image Processing for
  Terabyte Data Sets</title><categories>q-bio.QM cs.GR</categories><comments>38 pages, 1 main figure, 27 supplementary figures, under review at
  Nature Methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasingly popular light sheet microscopy techniques generate very
large 3D time-lapse recordings of living biological specimen. The necessity to
make large volumetric datasets available for interactive visualization and
analysis has been widely recognized. However, existing solutions build on
dedicated servers to generate virtual slices that are transferred to the client
applications, practically leading to insufficient frame rates (less than 10
frames per second) for truly interactive experience. An easily accessible open
source solution for interactive arbitrary virtual re-slicing of very large
volumes and time series of volumes has yet been missing. We fill this gap with
BigDataViewer, a Fiji plugin to interactively navigate and visualize large
image sequences from both local and remote data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0494</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0494</id><created>2014-12-01</created><updated>2015-01-30</updated><authors><author><keyname>Bhamre</keyname><forenames>Tejal</forenames></author><author><keyname>Zhang</keyname><forenames>Teng</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Orthogonal Matrix Retrieval in Cryo-Electron Microscopy</title><categories>cs.CV</categories><comments>Modified introduction and summary. Accepted to the IEEE International
  Symposium on Biomedical Imaging</comments><doi>10.1109/ISBI.2015.7164051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In single particle reconstruction (SPR) from cryo-electron microscopy
(cryo-EM), the 3D structure of a molecule needs to be determined from its 2D
projection images taken at unknown viewing directions. Zvi Kam showed already
in 1980 that the autocorrelation function of the 3D molecule over the rotation
group SO(3) can be estimated from 2D projection images whose viewing directions
are uniformly distributed over the sphere. The autocorrelation function
determines the expansion coefficients of the 3D molecule in spherical harmonics
up to an orthogonal matrix of size $(2l+1)\times (2l+1)$ for each
$l=0,1,2,...$. In this paper we show how techniques for solving the phase
retrieval problem in X-ray crystallography can be modified for the cryo-EM
setup for retrieving the missing orthogonal matrices. Specifically, we present
two new approaches that we term Orthogonal Extension and Orthogonal
Replacement, in which the main algorithmic components are the singular value
decomposition and semidefinite programming. We demonstrate the utility of these
approaches through numerical experiments on simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0501</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0501</id><created>2014-12-01</created><updated>2015-01-18</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>SmartPacket: Re-Distributing the Routing Intelligence among Network
  Components in SDNs</title><categories>cs.NI</categories><comments>9 pages, 3 figures, 5 tables. To be presented in SDS 2015, 9-13 March
  2015, Tempe, AZ, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a new region-based, multipath-enabled packet routing is
presented and called SmartPacket Routing. The proposed approach provides
several opportunities to re-distribute the smartness and decision making among
various elements of a network including the packets themselves toward providing
a decentralized solution for SDNs. This would bring efficiency and scalability,
and therefore also lower environmental footprint for the ever-growing networks.
In particular, a region-based representation of the network topology is
proposed which is then used to describe the routing actions along the possible
paths for a packet flow. In addition to a region stack that expresses a partial
or full regional path of a packet, QoS requirements of the packet (or its
associated flow) is considered in the packet header in order to enable possible
QoS-aware routing at region level without requiring a centralized controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0525</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0525</id><created>2014-12-01</created><authors><author><keyname>Hamlet</keyname><forenames>Alan J.</forenames></author><author><keyname>Crane</keyname><forenames>Carl D.</forenames></author></authors><title>Robotic Behavior Prediction Using Hidden Markov Models</title><categories>cs.RO</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many situations in which it would be beneficial for a robot to have
predictive abilities similar to those of rational humans. Some of these
situations include collaborative robots, robots in adversarial situations, and
for dynamic obstacle avoidance. This paper presents an approach to modeling
behaviors of dynamic agents in order to empower robots with the ability to
predict the agent's actions and identify the behavior the agent is executing in
real time. The method of behavior modeling implemented uses hidden Markov
models (HMMs) to model the unobservable states of the dynamic agents. The
background and theory of the behavior modeling is presented. Experimental
results of realistic simulations of a robot predicting the behaviors and
actions of a dynamic agent in a static environment are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0527</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0527</id><created>2014-12-01</created><authors><author><keyname>Autili</keyname><forenames>Marco</forenames></author><author><keyname>Inverardi</keyname><forenames>Paola</forenames></author><author><keyname>Tivoli</keyname><forenames>Massimo</forenames></author></authors><title>Automatic adaptor synthesis for protocol transformation</title><categories>cs.SE</categories><comments>8 pages, Proceedings of the First International Workshop on
  Coordination and Adaptation Techniques for Software Entities (WCAT'04) at
  ECOOP 2004, June 14, 2004, Oslo, Norway</comments><report-no>ISBN: 84-688-6782-9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptation of software components is an important issue in Component Based
Software Engineering (CBSE). Building a system from reusable or
Commercial-Off-The-Shelf (COTS) components introduces a set of issues, mainly
related to compatibility and communication aspects. Components may have
incompatible interaction behavior. Moreover it might be necessary to enhance
the current communication protocol to introduce more sophisticated interactions
among components. We address these problems enhancing our architectural
approach which allows for detection and recovery of integration mismatches by
synthesizing a suitable coordinator. Starting from the specification of the
system to be assembled and from the specification of the needed protocol
enhancements, our framework automatically derives, in a compositional way, the
glue code for the set of components. The synthesized glue code avoids
interaction mismatches and provides a protocol-enhanced version of the composed
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0529</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0529</id><created>2014-12-01</created><authors><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Blanco-Justicia</keyname><forenames>Alberto</forenames></author></authors><title>Group Discounts Compatible with Buyer Privacy</title><categories>cs.CR</categories><comments>Presented at 9th DPM International Workshop on Data Privacy
  Management (DPM 2014, Sep. 10,2014). To appear in workshop proceedings, LNCS,
  Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how group discounts can be offered without forcing buyers to
surrender their anonymity, as long as buyers can use their own computing
devices (e.g. smartphone, tablet or computer) to perform a purchase.
Specifically, we present a protocol for privacy-preserving group discounts. The
protocol allows a group of buyers to prove how many they are without disclosing
their identities. Coupled with an anonymous payment system, this makes group
discounts compatible with buyer privacy (that is, buyer anonymity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0537</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0537</id><created>2014-12-01</created><authors><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Reynier</keyname><forenames>Pierre-Alain</forenames></author></authors><title>On Streaming String Transducers and HDT0L Systems</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Copyless streaming string transducers (copyless SST) have been introduced by
R. Alur and P. Cerny in 2010 as a one-way deterministic automata model to
define transformations of finite strings. Copyless SST extend deterministic
finite state automata with a set of registers in which to store intermediate
output strings, and those registers can be combined and updated all along the
run, in a linear manner, i.e., no register content can be copied on
transitions. It is known that copyless SST capture exactly the class of
MSO-definable string-to-string transformations, as defined by B. Courcelle, and
are equi-expressive to deterministic two-way transducers. They enjoy good
algorithmic properties. Most notably, they have decidable equivalence problem
(in PSpace). In this paper, we show that they still have decidable equivalence
problem even without the copyless restriction. The proof reduces to the HDT0L
sequence equivalence problem, which is known to be decidable. We also show that
this latter problem is as difficult as the SST equivalence problem, modulo
linear time reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0538</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0538</id><created>2014-12-01</created><authors><author><keyname>Langetepe</keyname><forenames>Elmar</forenames></author><author><keyname>Lenerz</keyname><forenames>Andreas</forenames></author><author><keyname>Br&#xfc;ggemann</keyname><forenames>Bernd</forenames></author></authors><title>Strategic deployment in graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conquerors of old (like, e.g., Alexander the Great or Ceasar) had to solve
the following deployment problem. Sufficiently strong units had to be stationed
at locations of strategic importance, and the moving forces had to be strong
enough to advance to the next location. To the best of our knowledge we are the
first to consider the (off-line) graph version of this problem. While being
NP-hard for general graphs, for trees the minimum number of agents and an
optimal deployment can be computed in optimal polynomial time. Moreover, the
optimal solution for the minimum spanning tree of an arbitrary graph G results
in a 2-approximation of the optimal solution for G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0540</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0540</id><created>2014-12-01</created><updated>2015-12-20</updated><authors><author><keyname>Talon</keyname><forenames>Alexandre</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author></authors><title>Completion of the mixed unit interval graphs hierarchy</title><categories>cs.DM math.CO</categories><comments>17 pages, 36 figures (three not numbered). v1 Accepted in the TAMC
  2015 conference. The recognition algorithm is faster in v2. One graph was not
  listed in Theorem 7 of v1 of this paper v3 provides a proposition to
  recognize the mixed unit interval graphs in quadratic time</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the missing class of the hierarchy of mixed unit interval graphs,
generated by the intersection graphs of closed, open and one type of half-open
intervals of the real line. This class lies strictly between unit interval
graphs and mixed unit interval graphs. We give a complete characterization of
this new class, as well as quadratic-time algorithms that recognize graphs from
this class and produce a corresponding interval representation if one exists.
We also mention that the work in arXiv:1405.4247 directly extends to provide a
quadratic-time algorithm to recognize the class of mixed unit interval graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0542</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0542</id><created>2014-11-07</created><authors><author><keyname>Caminati</keyname><forenames>Marco B.</forenames></author><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author><author><keyname>Rowat</keyname><forenames>Colin</forenames></author></authors><title>Budget Imbalance Criteria for Auctions: A Formalized Theorem</title><categories>q-fin.MF cs.GT cs.LO q-fin.EC</categories><comments>6th Podlasie Conference on Mathematics 2014, 11 pages</comments><msc-class>91B26 62P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an original theorem in auction theory: it specifies general
conditions under which the sum of the payments of all bidders is necessarily
not identically zero, and more generally not constant. Moreover, it explicitly
supplies a construction for a finite minimal set of possible bids on which such
a sum is not constant. In particular, this theorem applies to the important
case of a second-price Vickrey auction, where it reduces to a basic result of
which a novel proof is given. To enhance the confidence in this new theorem, it
has been formalized in Isabelle/HOL: the main results and definitions of the
formal proof are re- produced here in common mathematical language, and are
accompanied by an informal discussion about the underlying ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0543</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0543</id><created>2014-12-01</created><authors><author><keyname>Perkins</keyname><forenames>Steven</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Leslie</keyname><forenames>David S.</forenames></author></authors><title>Game-theoretical control with continuous action sets</title><categories>math.OC cs.GT cs.MA stat.ML</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent applications of game-theoretical learning techniques
to the design of distributed control systems, we study a class of control
problems that can be formulated as potential games with continuous action sets,
and we propose an actor-critic reinforcement learning algorithm that provably
converges to equilibrium in this class of problems. The method employed is to
analyse the learning process under study through a mean-field dynamical system
that evolves in an infinite-dimensional function space (the space of
probability distributions over the players' continuous controls). To do so, we
extend the theory of finite-dimensional two-timescale stochastic approximation
to an infinite-dimensional, Banach space setting, and we prove that the
continuous dynamics of the process converge to equilibrium in the case of
potential games. These results combine to give a provably-convergent learning
algorithm in which players do not need to keep track of the controls selected
by the other agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0544</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0544</id><created>2014-12-01</created><authors><author><keyname>Mulansky</keyname><forenames>Mario</forenames></author></authors><title>Optimizing Large-Scale ODE Simulations</title><categories>physics.comp-ph cs.PF nlin.CD</categories><comments>18 pages, 9 figures submitted to SIAM Journal of Scientific Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strategy to speed up Runge-Kutta-based ODE simulations of large
systems with nearest-neighbor coupling. We identify the cache/memory bandwidth
as the crucial performance bottleneck. To reduce the required bandwidth, we
introduce a granularity in the simulation and identify the optimal cluster size
in a performance study. This leads to a considerable performance increase and
transforms the algorithm from bandwidth bound to CPU bound. By additionally
employing SIMD instructions we are able to boost the efficiency even further.
In the end, a total performance increase of up to a factor three is observed
when using cache optimization and SIMD instructions compared to a standard
implementation. All simulation codes are written in C++ and made publicly
available. By using the modern C++ libraries Boost.odeint and Boost.SIMD, these
optimizations can be implemented with minimal programming effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0583</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0583</id><created>2014-11-25</created><updated>2014-12-01</updated><authors><author><keyname>Medus</keyname><forenames>A. D.</forenames></author><author><keyname>Dorso</keyname><forenames>C. O.</forenames></author></authors><title>Vaccination and public trust: a model for the dissemination of
  vaccination behavior with external intervention</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 14 figures, Supp. information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vaccination is widely recognized as the most effective way of immunization
against many infectious diseases. However, unfounded claims about supposed side
effects of some vaccines have contributed to spread concern and fear among
people, thus inducing vaccination refusal. For instance, MMR vaccine coverage
has undergone an important decrease in a large part of Europe and US as a
consequence of erroneously alleged side effects, leading to recent measles
outbreaks. In this work, we propose a general agent-based model to study the
spread of vaccination behavior in social networks, not as an isolated binary
opinion spreading on it, but as part of a process of cultural dissemination in
the spirit of Axelrod's model. We particularly focused on the impact of a small
anti-vaccination movement over an initial population of pro-vaccination social
agents. Additionally, we have considered two classes of edges in the underlying
social network: personal edges able to spread both opinions and diseases; and
the non-personal ones representing interactions mediated by information
technologies, which only allow opinion exchanges. We study the clustering of
unvaccinated agents as a dynamical outcome of the model, together with its
direct relation with the increase of the probability of occurrence and the
final size of measles outbreaks. Finally, we illustrate the mitigating effect
of a public health campaign, represented by an external field, against the
harmful action of anti-vaccination movements. We show that the topological
characteristics of the clusters of unvaccinated agents determine the scopes of
this mitigating effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0587</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0587</id><created>2014-11-27</created><authors><author><keyname>Moinet</keyname><forenames>Antoine</forenames></author><author><keyname>Starnini</keyname><forenames>Michele</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author></authors><title>Burstiness and aging in social temporal networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. Lett. 114, 108701 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.108701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of burstiness in temporal social networks, revealed by a power
law form of the waiting time distribution of consecutive interactions, is
expected to produce aging effects in the corresponding time-integrated network.
Here we propose an analytically tractable model, in which interactions among
the agents are ruled by a renewal process, and that is able to reproduce this
aging behavior. We develop an analytic solution for the topological properties
of the integrated network produced by the model, finding that the time
translation invariance of the degree distribution is broken. We validate our
predictions against numerical simulations, and we check for the presence of
aging effects in a empirical temporal network, ruled by bursty social
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0588</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0588</id><created>2014-12-01</created><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>$\ell_p$ Row Sampling by Lewis Weights</title><categories>cs.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple algorithm to efficiently sample the rows of a matrix while
preserving the p-norms of its product with vectors. Given an $n$-by-$d$ matrix
$\boldsymbol{\mathit{A}}$, we find with high probability and in input sparsity
time an $\boldsymbol{\mathit{A}}'$ consisting of about $d \log{d}$ rescaled
rows of $\boldsymbol{\mathit{A}}$ such that $\| \boldsymbol{\mathit{A}}
\boldsymbol{\mathit{x}} \|_1$ is close to $\| \boldsymbol{\mathit{A}}'
\boldsymbol{\mathit{x}} \|_1$ for all vectors $\boldsymbol{\mathit{x}}$. We
also show similar results for all $\ell_p$ that give nearly optimal sample
bounds in input sparsity time. Our results are based on sampling by &quot;Lewis
weights&quot;, which can be viewed as statistical leverage scores of a reweighted
matrix. We also give an elementary proof of the guarantees of this sampling
process for $\ell_1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0591</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0591</id><created>2014-12-01</created><authors><author><keyname>Aravind</keyname><forenames>G</forenames></author><author><keyname>Gautham</keyname><forenames>Vasan</forenames></author><author><keyname>Kumar</keyname><forenames>T. S. B Gowtham</forenames></author><author><keyname>Naresh</keyname><forenames>Balaji</forenames></author></authors><title>A Control Strategy for an Autonomous Robotic Vacuum Cleaner for Solar
  Panels</title><categories>cs.RO</categories><report-no>TIIC IADC 2014 Team ID: 56.2 CMT ID: 272</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accumulation of dust on the surface of solar panels reduces the amount of
radiation reaching it. This leads to loss in generated electric power and
formation of hotspots which would permanently damage the solar panel. This
project aims at developing an autonomous vacuum cleaning method which can be
used on a regular basis to maximize the lifetime and efficiency of a solar
panel. This system is implemented using two subsystems namely a Robotic Vacuum
Cleaner and a Docking Station. The Robotic Vacuum Cleaner uses a two stage
cleaning process to remove the dust from the solar panel. It is designed to
work on inclined and slippery surfaces. A control strategy is formulated to
navigate the robot in the required path using an appropriate feedback
mechanism. The battery voltage of the robot is determined periodically and if
it goes below a threshold, it returns to the docking station and charges itself
automatically using power drawn from the solar panels. The operation of the
robotic vacuum cleaner has been verified and relevant results are presented.
The DC Charging circuit in the docking station is simulated in Proteus
environment and is implemented in hardware. An economical, robust Robotic
Vacuum Cleaner which can clean arrays of Solar panels (with or without
inclination) interlinked by rails and recharge itself automatically at a
docking station is designed and implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0595</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0595</id><created>2014-12-01</created><authors><author><keyname>Balaji</keyname><forenames>Naresh</forenames></author><author><keyname>Yavuz</keyname><forenames>Esin</forenames></author><author><keyname>Nowotny</keyname><forenames>Thomas</forenames></author></authors><title>Scalability and Optimization Strategies for GPU Enhanced Neural Networks
  (GeNN)</title><categories>cs.DC cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation of spiking neural networks has been traditionally done on
high-performance supercomputers or large-scale clusters. Utilizing the parallel
nature of neural network computation algorithms, GeNN (GPU Enhanced Neural
Network) provides a simulation environment that performs on General Purpose
NVIDIA GPUs with a code generation based approach. GeNN allows the users to
design and simulate neural networks by specifying the populations of neurons at
different stages, their synapse connection densities and the model of
individual neurons. In this report we describe work on how to scale synaptic
weights based on the configuration of the user-defined network to ensure
sufficient spiking and subsequent effective learning. We also discuss
optimization strategies particular to GPU computing: sparse representation of
synapse connections and occupancy based block-size determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0600</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0600</id><created>2014-12-01</created><authors><author><keyname>Rauzy</keyname><forenames>Pablo</forenames></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author></authors><title>Countermeasures Against High-Order Fault-Injection Attacks on CRT-RSA</title><categories>cs.CR</categories><comments>Fault Diagnosis and Tolerance in Cryptography, Sep 2014, Busan, South
  Korea. Eleventh Workshop on Fault Diagnosis and Tolerance in Cryptography
  (FDTC 2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the existing CRT-RSA countermeasures against
fault-injection at-tacks. In an attempt to classify them we get to achieve deep
understanding of how they work. We show that the many countermeasures that we
study (and their variations) actually share a number of common features, but
optimize them in different ways. We also show that there is no conceptual
distinction between test-based and infective countermeasures and how either one
can be transformed into the other. Furthermore, we show that faults on the code
(skipping instructions) can be captured by considering only faults on the data.
These intermediate results allow us to improve the state of the art in several
ways: (a) we fix an existing and that was known to be broken countermeasure
(namely the one from Shamir); (b) we drastically optimize an existing
countermeasure (namely the one from Vigilant) which we reduce to 3 tests
instead of 9 in its original version, and prove that it resists not only one
fault but also an arbitrary number of randomizing faults; (c) we also show how
to upgrade countermeasures to resist any given number of faults: given a
correct first-order countermeasure, we present a way to design a prov-able
high-order countermeasure (for a well-defined and reasonable fault model).
Finally, we pave the way for a generic approach against fault attacks for any
modular arithmetic computations, and thus for the automatic insertion of
countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0607</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0607</id><created>2014-12-01</created><authors><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>How to monitor and mitigate stair-casing in l1 trend filtering</title><categories>math.ST cs.SY stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the estimation of changing trends in time-series using
$\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV)
denoising for detection of step changes in means to detecting changes in
trends, and it relies on a convex optimization problem for which there are very
efficient numerical algorithms. It is known that TV denoising suffers from the
so-called stair-case effect, which leads to detecting false change points. The
objective of this paper is to show that $\ell_1$ trend filtering also suffers
from a certain stair-case problem. The analysis is based on an interpretation
of the dual variables of the optimization problem in the method as integrated
random walk. We discuss consistency conditions for $\ell_1$ trend filtering,
how to monitor their fulfillment, and how to modify the algorithm to avoid the
stair-case false detection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0611</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0611</id><created>2014-12-01</created><authors><author><keyname>Prezioso</keyname><forenames>Mirko</forenames></author><author><keyname>Merrikh-Bayat</keyname><forenames>Farnood</forenames></author><author><keyname>Hoskins</keyname><forenames>Brian</forenames></author><author><keyname>Adam</keyname><forenames>Gina</forenames></author><author><keyname>Likharev</keyname><forenames>Konstantin K.</forenames></author><author><keyname>Strukov</keyname><forenames>Dmitri B.</forenames></author></authors><title>Training and Operation of an Integrated Neuromorphic Network Based on
  Metal-Oxide Memristors</title><categories>cs.ET</categories><comments>21 pages, 12 figures</comments><acm-class>B.7.1; C.1.3</acm-class><journal-ref>Nature, vol. 521, pp. 61-64, 2015</journal-ref><doi>10.1038/nature14441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite all the progress of semiconductor integrated circuit technology, the
extreme complexity of the human cerebral cortex makes the hardware
implementation of neuromorphic networks with a comparable number of devices
exceptionally challenging. One of the most prospective candidates to provide
comparable complexity, while operating much faster and with manageable power
dissipation, are so-called CrossNets based on hybrid CMOS/memristor circuits.
In these circuits, the usual CMOS stack is augmented with one or several
crossbar layers, with adjustable two-terminal memristors at each crosspoint.
Recently, there was a significant progress in improvement of technology of
fabrication of such memristive crossbars and their integration with CMOS
circuits, including first demonstrations of their vertical integration.
Separately, there have been several demonstrations of discrete memristors as
artificial synapses for neuromorphic networks. Very recently such experiments
were extended to crossbar arrays of phase-change memristive devices. The
adjustment of such devices, however, requires an additional transistor at each
crosspoint, and hence the prospects of their scaling are less impressive than
those of metal-oxide memristors, whose nonlinear I-V curves enable
transistor-free operation. Here we report the first experimental implementation
of a transistor-free metal-oxide memristor crossbar with device variability
lowered sufficiently to demonstrate a successful operation of a simple
integrated neural network, a single layer-perceptron. The network could be
taught in situ using a coarse-grain variety of the delta-rule algorithm to
perform the perfect classification of 3x3-pixel black/white images into 3
classes. We believe that this demonstration is an important step towards the
implementation of much larger and more complex memristive neuromorphic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0614</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0614</id><created>2014-12-01</created><authors><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Wang</keyname><forenames>Liming</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Jianbo</forenames></author><author><keyname>Reeves</keyname><forenames>Galen</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Classification and Reconstruction of High-Dimensional Signals from
  Low-Dimensional Noisy Features in the Presence of Side Information</title><categories>cs.IT cs.CV math.IT math.ST stat.ML stat.TH</categories><comments>53 pages, 12 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper offers a characterization of fundamental limits in the
classification and reconstruction of high-dimensional signals from
low-dimensional features, in the presence of side information. In particular,
we consider a scenario where a decoder has access both to noisy linear features
of the signal of interest and to noisy linear features of the side information
signal; while the side information may be in a compressed form, the objective
is recovery or classification of the primary signal, not the side information.
We assume the signal of interest and the side information signal are drawn from
a correlated mixture of distributions/components, where each component
associated with a specific class label follows a Gaussian mixture model (GMM).
By considering bounds to the misclassification probability associated with the
recovery of the underlying class label of the signal of interest, and bounds to
the reconstruction error associated with the recovery of the signal of interest
itself, we then provide sharp sufficient and/or necessary conditions for the
phase transition of these quantities in the low-noise regime. These conditions,
which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv conditions,
are a function of the number of linear features extracted from the signal of
interest, the number of linear features extracted from the side information
signal, and the geometry of these signals and their interplay. Our framework,
which also offers a principled mechanism to integrate side information in
high-dimensional data problems, is also tested in the context of imaging
applications. In particular, we report state-of-the-art results in compressive
hyperspectral imaging applications, where the accompanying side information is
a conventional digital photograph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0617</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0617</id><created>2014-12-01</created><authors><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Visotsky</keyname><forenames>Eugene</forenames></author><author><keyname>Yang</keyname><forenames>Weidong</forenames></author><author><keyname>Moorut</keyname><forenames>Prakash</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Radar In-Band and Out-of-Band Interference into LTE Macro and Small Cell
  Uplinks in the 3.5 GHz Band</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.7334</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  National Telecommunications and Information Administration (NTIA) has
proposed vast exclusions zones between radar and Worldwide Interoperability for
Microwave Access (WiMAX) (WiMAX) systems which are also being considered as
geographic separations between radars and 3.5 GHz Long Term Evolution (LTE)
systems without investigating any changes induced by the distinct nature of LTE
as opposed to WiMAX. This paper performs a detailed system-level analysis of
the interference effects from shipborne radar systems into LTE systems. Even
though the results reveal impacts of radar interference on LTE systems
performance, they provide clear indications of conspicuously narrower exclusion
zones for LTE vis-\`a-vis those for WiMAX and pave the way toward deploying LTE
at 3.5 GHz within the coastline populous areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0620</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0620</id><created>2014-12-01</created><updated>2015-09-01</updated><authors><author><keyname>Aswani</keyname><forenames>Anil</forenames></author></authors><title>Low-Rank Approximation and Completion of Positive Tensors</title><categories>math.ST cs.LG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike the matrix case, computing low-rank approximations of tensors is
NP-hard and numerically ill-posed in general. Even the best rank-1
approximation of a tensor is NP-hard. In this paper, we use convex optimization
to develop polynomial-time algorithms for low-rank approximation and completion
of positive tensors. Our approach is to use algebraic topology to define a new
(numerically well-posed) decomposition for positive tensors, which we show is
equivalent to the standard tensor decomposition in important cases. Though
computing this decomposition is a nonconvex optimization problem, we prove it
can be exactly reformulated as a convex optimization problem. This allows us to
construct polynomial-time randomized algorithms for computing this
decomposition and for solving low-rank tensor approximation problems. Among the
consequences is that best rank-1 approximations of positive tensors can be
computed in polynomial time. Our framework is next extended to the tensor
completion problem, where noisy entries of a tensor are observed and then used
to estimate missing entries. We provide a polynomial-time algorithm that
requires a polynomial (in tensor order) number of measurements, in contrast to
existing approaches that require an exponential number of measurements for
specific cases. These algorithms are extended to exploit sparsity in the tensor
to reduce the number of measurements needed. We conclude by providing a novel
interpretation of statistical regression problems with categorical variables as
tensor completion problems, and numerical examples with synthetic data and data
from a bioengineered metabolic network show the improved performance of our
approach on this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0623</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0623</id><created>2014-12-01</created><updated>2015-04-14</updated><authors><author><keyname>Bell</keyname><forenames>Sean</forenames></author><author><keyname>Upchurch</keyname><forenames>Paul</forenames></author><author><keyname>Snavely</keyname><forenames>Noah</forenames></author><author><keyname>Bala</keyname><forenames>Kavita</forenames></author></authors><title>Material Recognition in the Wild with the Materials in Context Database</title><categories>cs.CV</categories><comments>CVPR 2015. Sean Bell and Paul Upchurch contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing materials in real-world images is a challenging task. Real-world
materials have rich surface texture, geometry, lighting conditions, and
clutter, which combine to make the problem particularly difficult. In this
paper, we introduce a new, large-scale, open dataset of materials in the wild,
the Materials in Context Database (MINC), and combine this dataset with deep
learning to achieve material recognition and segmentation of images in the
wild.
  MINC is an order of magnitude larger than previous material databases, while
being more diverse and well-sampled across its 23 categories. Using MINC, we
train convolutional neural networks (CNNs) for two tasks: classifying materials
from patches, and simultaneous material recognition and segmentation in full
images. For patch-based classification on MINC we found that the best
performing CNN architectures can achieve 85.2% mean class accuracy. We convert
these trained CNN classifiers into an efficient fully convolutional framework
combined with a fully connected conditional random field (CRF) to predict the
material at every pixel in an image, achieving 73.1% mean class accuracy. Our
experiments demonstrate that having a large, well-sampled dataset such as MINC
is crucial for real-world material recognition and segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0624</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0624</id><created>2014-12-01</created><updated>2015-04-26</updated><authors><author><keyname>Stankovic</keyname><forenames>Ljubisa</forenames></author><author><keyname>Dakovic</keyname><forenames>Milos</forenames></author></authors><title>Reconstruction of Randomly Sampled Sparse Signals Using an Adaptive
  Gradient Algorithm</title><categories>cs.IT math.IT</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signals can be recovered from a reduced set of samples by using
compressive sensing algorithms. In common methods the signal is recovered in
the sparse domain. A method for the reconstruction of sparse signal which
reconstructs the remaining missing samples/measurements is recently proposed.
The available samples are fixed, while the missing samples are considered as
minimization variables. Recovery of missing samples/measurements is done using
an adaptive gradient-based algorithm in the time domain. A new criterion for
the parameter adaptation in this algorithm, based on the gradient direction
angles, is proposed. It improves the algorithm computational efficiency. A
theorem for the uniqueness of the recovered signal for given set of missing
samples (reconstruction variables) is presented. The case when available
samples are a random subset of a uniformly or nonuniformly sampled signal is
considered in this paper. A recalculation procedure is used to reconstruct the
nonuniformly sampled signal. The methods are illustrated on statistical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0625</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0625</id><created>2014-12-01</created><authors><author><keyname>Smith</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Ross</keyname><forenames>Neil J.</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Quipper: Concrete Resource Estimation in Quantum Algorithms</title><categories>cs.PL cs.ET quant-ph</categories><comments>Extended abstract for a talk given at QAPL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the rich literature on quantum algorithms, there is a surprisingly
small amount of coverage of their concrete logical design and implementation.
Most resource estimation is done at the level of complexity analysis, but
actual concrete numbers (of quantum gates, qubits, etc.) can differ by orders
of magnitude. The line of work we present here is a formal framework to write,
and reason about, quantum algorithms. Specifically, we designed a language,
Quipper, with scalability in mind, and we are able to report actual resource
counts for seven non-trivial algorithms found in the quantum computer science
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0630</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0630</id><created>2014-12-01</created><authors><author><keyname>Anderson</keyname><forenames>Sean</forenames></author><author><keyname>Barfoot</keyname><forenames>Timothy D.</forenames></author><author><keyname>Tong</keyname><forenames>Chi Hay</forenames></author><author><keyname>S&#xe4;rkk&#xe4;</keyname><forenames>Simo</forenames></author></authors><title>Batch Nonlinear Continuous-Time Trajectory Estimation as Exactly Sparse
  Gaussian Process Regression</title><categories>cs.RO</categories><comments>Submitted to Autonomous Robots on 20 November 2014, manuscript #
  AURO-D-14-00185, 16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit batch state estimation through the lens of Gaussian
process (GP) regression. We consider continuous-discrete estimation problems
wherein a trajectory is viewed as a one-dimensional GP, with time as the
independent variable. Our continuous-time prior can be defined by any
nonlinear, time-varying stochastic differential equation driven by white noise;
this allows the possibility of smoothing our trajectory estimates using a
variety of vehicle dynamics models (e.g., `constant-velocity'). We show that
this class of prior results in an inverse kernel matrix (i.e., covariance
matrix between all pairs of measurement times) that is exactly sparse
(block-tridiagonal) and that this can be exploited to carry out GP regression
(and interpolation) very efficiently. When the prior is based on a linear,
time-varying stochastic differential equation and the measurement model is also
linear, this GP approach is equivalent to classical, discrete-time smoothing
(at the measurement times); when a nonlinearity is present, we iterate over the
whole trajectory to maximize accuracy. We test the approach experimentally on a
simultaneous trajectory estimation and mapping problem using a mobile robot
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0639</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0639</id><created>2014-12-01</created><authors><author><keyname>Rosenbaum</keyname><forenames>David J.</forenames></author></authors><title>Beating the Generator-Enumeration Bound for Solvable-Group Isomorphism</title><categories>cs.DS</categories><comments>22 pages. This is an updated and improved version of the results for
  solvable groups in arXiv:1205.0642</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the isomorphism problem for groups specified by their
multiplication tables. Until recently, the best published bound for the
worst-case was achieved by the n^(log_p n + O(1)) generator-enumeration
algorithm. In previous work with Fabian Wagner, we showed an n^((1 / 2) log_p n
+ O(log n / log log n)) time algorithm for testing isomorphism of p-groups by
building graphs with degree bounded by p + O(1) that represent composition
series for the groups and applying Luks' algorithm for testing isomorphism of
bounded degree graphs.
  In this work, we extend this improvement to the more general class of
solvable groups to obtain an n^((1 / 2) log_p n + O(log n / log log n)) time
algorithm. In the case of solvable groups, the composition factors can be large
which prevents previous methods from outperforming the generator-enumeration
algorithm. Using Hall's theory of Sylow bases, we define a new object that
generalizes the notion of a composition series with small factors but exists
even when the composition factors are large. By constructing graphs that
represent these objects and running Luks' algorithm, we obtain our algorithm
for solvable-group isomorphism. We also extend our algorithm to compute
canonical forms of solvable groups while retaining the same complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0640</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0640</id><created>2014-12-01</created><authors><author><keyname>Hoang</keyname><forenames>Tuan L.</forenames></author><author><keyname>Marian</keyname><forenames>Jaime</forenames></author><author><keyname>Bulatov</keyname><forenames>Vasily V.</forenames></author><author><keyname>Hosemann</keyname><forenames>Peter</forenames></author></authors><title>Computationally-efficient stochastic cluster dynamics method for
  modeling damage accumulation in irradiated materials</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved version of a recently developed stochastic cluster dynamics (SCD)
method {[}Marian, J. and Bulatov, V. V., {\it J. Nucl. Mater.} \textbf{415}
(2014) 84-95{]} is introduced as an alternative to rate theory (RT) methods for
solving coupled ordinary differential equation (ODE) systems for irradiation
damage simulations. SCD circumvents by design the curse of dimensionality of
the variable space that renders traditional ODE-based RT approaches inefficient
when handling complex defect population comprised of multiple (more than two)
defect species. Several improvements introduced here enable efficient and
accurate simulations of irradiated materials up to realistic (high) damage
doses characteristic of next-generation nuclear systems. The first improvement
is a procedure for efficiently updating the defect reaction-network and event
selection in the context of a dynamically expanding reaction-network. Next is a
novel implementation of the $\tau$-leaping method that speeds up SCD
simulations by advancing the state of the reaction network in large time
increments when appropriate. Lastly, a volume rescaling procedure is introduced
to control the computational complexity of the expanding reaction-network
through occasional reductions of the defect population while maintaining
accurate statistics. The enhanced SCD method is then applied to model defect
cluster accumulation in iron thin films subjected to triple ion-beam
($\text{Fe}^{3+}$, $\text{He}^{+}$ and $ $$\text{H\ensuremath{{}^{+}}}$$ $)
irradiations, for which standard RT or spatially-resolved kinetic Monte Carlo
simulations are prohibitively expensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0644</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0644</id><created>2014-12-01</created><authors><author><keyname>Balieiro</keyname><forenames>Andson Marreiros</forenames></author><author><keyname>Dias</keyname><forenames>Kelvin Lopes</forenames></author></authors><title>Mapping of Secondary Virtual Networks onto Wireless Substrate based on
  Cognitive Radio: multi-objective formulation and analysis</title><categories>cs.NI</categories><comments>21 pages, 4 figures, submitted to &quot;Elsevier Computer Networks&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing demand for wireless services with different requirements
in a dense and heterogeneous wireless environment. Handling this complex
ecosystem is becoming a challenging issue, and wireless virtualization emerges
as an efficient solution. Although the inclusion of virtualization in wireless
networks ensures a better use of resources, current approaches adopted for
wireless virtualization can cause an underutilization of resources, since the
resource allocated to a virtual wireless network is not shared with other one.
This problem can be overcome by combining wireless virtualization with the
cognitive radio (CR) technology and dynamic access spectrum (DSA) techniques.
Thus, virtual wireless networks with different access priorities to resources
(e.g., primary and secondary) can be deployed in an overlay form and share the
same substrate wireless network, where the secondary networks use the resources
opportunistically. However, challenges emerge in this new scenario, ranging
from the mapping to the operation of these networks. This paper is the first to
propose the cognitive radio virtual network environment and to formulate the
problem of mapping the secondary virtual networks (SVNs) onto the wireless
substrate network based on CR. To this end, a multi-objective formulation is
designed. Moreover, an analysis of the metrics defined in the problem is
performed in order to give the reader useful assistance in designing schemes to
solve the problem of mapping SVNs onto substrate networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0650</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0650</id><created>2014-11-29</created><updated>2015-04-22</updated><authors><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author></authors><title>A review of &quot;Mem-computing NP-complete problems in polynomial time using
  polynomial resources&quot; (arXiv:1411.4798)</title><categories>cs.ET cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reviewed paper describes an analog device that empirically solves small
instances of the NP-complete Subset Sum Problem (SSP). The authors claim that
this device can solve the SSP in polynomial time using polynomial space, in
principle, and observe no exponential scaling in resource requirements. We
point out that (a) the properties ascribed by the authors to their device are
insufficient to solve NP-complete problems in poly-time, (b) runtime analysis
offered does not cover the spectral measurement step, (c) the overall technique
requires exponentially increasing resources when scaled up because of the
spectral measurement step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0652</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0652</id><created>2014-11-30</created><updated>2015-01-14</updated><authors><author><keyname>Saxena</keyname><forenames>Sanjeev</forenames></author></authors><title>Still Simpler Way of Introducing Interior-Point method for Linear
  Programming</title><categories>cs.DS</categories><comments>added section on initial solution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Programming is now included in Algorithm undergraduate and
postgraduate courses for Computer Science majors. It is possible to teach
interior-point methods directly with just minimal knowledge of Algebra and
Matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0659</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0659</id><created>2014-12-01</created><authors><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author><author><keyname>Gaburov</keyname><forenames>Evghenii</forenames></author><author><keyname>Fujii</keyname><forenames>Michiko S.</forenames></author><author><keyname>Nitadori</keyname><forenames>Keigo</forenames></author><author><keyname>Ishiyama</keyname><forenames>Tomoaki</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>24.77 Pflops on a Gravitational Tree-Code to Simulate the Milky Way
  Galaxy with 18600 GPUs</title><categories>astro-ph.GA astro-ph.IM cs.DC</categories><comments>12 pages, 4 figures, Published in: 'Proceeding SC '14 Proceedings of
  the International Conference for High Performance Computing, Networking,
  Storage and Analysis'. Gordon Bell Prize 2014 finalist</comments><doi>10.1109/SC.2014.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have simulated, for the first time, the long term evolution of the Milky
Way Galaxy using 51 billion particles on the Swiss Piz Daint supercomputer with
our $N$-body gravitational tree-code Bonsai. Herein, we describe the scientific
motivation and numerical algorithms. The Milky Way model was simulated for 6
billion years, during which the bar structure and spiral arms were fully
formed. This improves upon previous simulations by using 1000 times more
particles, and provides a wealth of new data that can be directly compared with
observations. We also report the scalability on both the Swiss Piz Daint and
the US ORNL Titan. On Piz Daint the parallel efficiency of Bonsai was above
95%. The highest performance was achieved with a 242 billion particle Milky Way
model using 18600 GPUs on Titan, thereby reaching a sustained GPU and
application performance of 33.49 Pflops and 24.77 Pflops respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0680</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0680</id><created>2014-12-01</created><authors><author><keyname>Ayremlou</keyname><forenames>Ali</forenames></author><author><keyname>Goldstein</keyname><forenames>Thomas</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse approximations using highly over-complete dictionaries is a
state-of-the-art tool for many imaging applications including denoising,
super-resolution, compressive sensing, light-field analysis, and object
recognition. Unfortunately, the applicability of such methods is severely
hampered by the computational burden of sparse approximation: these algorithms
are linear or super-linear in both the data dimensionality and size of the
dictionary. We propose a framework for learning the hierarchical structure of
over-complete dictionaries that enables fast computation of sparse
representations. Our method builds on tree-based strategies for nearest
neighbor matching, and presents domain-specific enhancements that are highly
efficient for the analysis of image patches. Contrary to most popular methods
for building spatial data structures, out methods rely on shallow, balanced
trees with relatively few layers. We show an extensive array of experiments on
several applications such as image denoising/superresolution, compressive
video/light-field sensing where we practically achieve 100-1000x speedup (with
a less than 1dB loss in accuracy).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0681</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0681</id><created>2014-12-01</created><updated>2015-06-23</updated><authors><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Near Optimal LP Rounding Algorithm for Correlation Clustering on
  Complete and Complete k-partite Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new rounding schemes for the standard linear programming relaxation
of the correlation clustering problem, achieving approximation factors almost
matching the integrality gaps:
  - For complete graphs our appoximation is $2.06 - \varepsilon$ for a fixed
constant $\varepsilon$, which almost matches the previously known integrality
gap of $2$.
  - For complete $k$-partite graphs our approximation is $3$. We also show a
matching integrality gap.
  - For complete graphs with edge weights satisfying triangle inequalities and
probability constraints, our approximation is $1.5$, and we show an integrality
gap of $1.2$.
  Our results improve a long line of work on approximation algorithms for
correlation clustering in complete graphs, previously culminating in a ratio of
$2.5$ for the complete case by Ailon, Charikar and Newman (JACM'08). In the
weighted complete case satisfying triangle inequalities and probability
constraints, the same authors give a $2$-approximation; for the bipartite case,
Ailon, Avigdor-Elgrabli, Liberty and van Zuylen give a $4$-approximation
(SICOMP'12).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0683</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0683</id><created>2014-11-26</created><updated>2015-04-14</updated><authors><author><keyname>Nguyen</keyname><forenames>Hung D.</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Robust Stability Assessment in the Presence of Load Dynamics Uncertainty</title><categories>cs.SY math.DS</categories><comments>16 pages, 20 figures, IEEE Transactions on Power Systems 2015, in
  press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic response of loads has a significant effect on system stability and
directly determines the stability margin of the operating point. Inherent
uncertainty and natural variability of load models make the stability
assessment especially difficult and may compromise the security of the system.
We propose a novel mathematical &quot;robust stability&quot; criterion for the assessment
of small-signal stability of operating points. Whenever the criterion is
satisfied for a given operating point, it provides mathematical guarantees that
the operating point will be stable with respect to small disturbances for any
dynamic response of the loads. The criterion can be naturally used for
identification of operating regions secure from the occurrence of Hopf
bifurcation. Several possible applications of the criterion are discussed, most
importantly the concept of Robust Stability Assessment (RSA) that could be
integrated in dynamic security assessment packages and used in contingency
screening and other planning and operational studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0684</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0684</id><created>2014-11-26</created><authors><author><keyname>Nabavi</keyname><forenames>Seyedbehzad</forenames></author><author><keyname>Chakrabortty</keyname><forenames>Aranya</forenames></author><author><keyname>Khargonekar</keyname><forenames>Pramod P.</forenames></author></authors><title>A Global Identifiability Condition for Consensus Networks with Tree
  Graphs</title><categories>cs.SY math.DS</categories><comments>7 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a sufficient condition that guarantees
identifiability of linear network dynamic systems exhibiting continuous-time
weighted consensus protocols with acyclic structure. Each edge of the
underlying network graph $\mathcal G$ of the system is defined by a constant
parameter, referred to as the weight of the edge, while each node is defined by
a scalar state whose dynamics evolve as the weighted linear combination of its
difference with the states of its neighboring nodes. Following the classical
definitions of identifiability and indistinguishability, we first derive a
condition that ensure the identifiability of the edge weights of $\mathcal G$
in terms of the associated transfer function. Using this characterization, we
propose a sensor placement algorithm that guarantees identifiability of the
edge weights. We describe our results using several illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0691</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0691</id><created>2014-12-01</created><updated>2015-04-12</updated><authors><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Sener</keyname><forenames>Ozan</forenames></author><author><keyname>Jami</keyname><forenames>Aditya</forenames></author><author><keyname>Misra</keyname><forenames>Dipendra K.</forenames></author><author><keyname>Koppula</keyname><forenames>Hema S.</forenames></author></authors><title>RoboBrain: Large-Scale Knowledge Engine for Robots</title><categories>cs.AI cs.RO</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a knowledge engine, which learns and shares
knowledge representations, for robots to carry out a variety of tasks. Building
such an engine brings with it the challenge of dealing with multiple data
modalities including symbols, natural language, haptic senses, robot
trajectories, visual features and many others. The \textit{knowledge} stored in
the engine comes from multiple sources including physical interactions that
robots have while performing tasks (perception, planning and control),
knowledge bases from the Internet and learned representations from several
robotics research groups.
  We discuss various technical aspects and associated challenges such as
modeling the correctness of knowledge, inferring latent information and
formulating different robotic tasks as queries to the knowledge engine. We
describe the system architecture and how it supports different mechanisms for
users and robots to interact with the engine. Finally, we demonstrate its use
in three important research areas: grounding natural language, perception, and
planning, which are the key building blocks for many robotic tasks. This
knowledge engine is a collaborative effort and we call it RoboBrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0696</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0696</id><created>2014-12-01</created><updated>2015-08-27</updated><authors><author><keyname>Gao</keyname><forenames>Shuyang</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Understanding confounding effects in linguistic coordination: an
  information-theoretic approach</title><categories>cs.CL cs.IT cs.SI math.IT physics.data-an</categories><journal-ref>PLoS ONE 10(6): e0130167, 2015</journal-ref><doi>10.1371/journal.pone.0130167</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest an information-theoretic approach for measuring stylistic
coordination in dialogues. The proposed measure has a simple predictive
interpretation and can account for various confounding factors through proper
conditioning. We revisit some of the previous studies that reported strong
signatures of stylistic accommodation, and find that a significant part of the
observed coordination can be attributed to a simple confounding effect - length
coordination. Specifically, longer utterances tend to be followed by longer
responses, which gives rise to spurious correlations in the other stylistic
features. We propose a test to distinguish correlations in length due to
contextual factors (topic of conversation, user verbosity, etc.) and
turn-by-turn coordination. We also suggest a test to identify whether stylistic
coordination persists even after accounting for length coordination and
contextual factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0703</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0703</id><created>2014-12-01</created><authors><author><keyname>Claesson</keyname><forenames>Anders</forenames></author><author><keyname>Tenner</keyname><forenames>Bridget Eileen</forenames></author><author><keyname>Ulfarsson</keyname><forenames>Henning</forenames></author></authors><title>Coincidence among families of mesh patterns</title><categories>math.CO cs.DM</categories><comments>17 pages, 16 figures</comments><msc-class>05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mesh patterns are coincident if they are avoided by the same set of
permutations. In this paper, we provide necessary conditions for this
coincidence, which include having the same set of enclosed diagonals. This
condition is sufficient to prove coincidence of vincular patterns, although it
is not enough to guarantee coincidence of bivincular patterns. In addition, we
provide a generalization of the Shading Lemma (Hilmarsson et al.), a result
that examined when a square could be added to the mesh of a pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0721</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0721</id><created>2014-12-01</created><updated>2014-12-09</updated><authors><author><keyname>Song</keyname><forenames>Shuang</forenames></author><author><keyname>Issac</keyname><forenames>Biju</forenames></author></authors><title>Analysis of WiFi and WiMAX and Wireless Network Coexistence</title><categories>cs.NI</categories><comments>16 pages. ISSN 0974-9322</comments><journal-ref>Journal of Computer Networks and Communications (IJCNC), 6(6),
  63-78 2014</journal-ref><doi>10.5121/ijcnc.2014.6605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks are very popular nowadays. Wireless Local Area Network
(WLAN) that uses the IEEE 802.11 standard and WiMAX (Worldwide Interoperability
for Microwave Access) that uses the IEEE 802.16 standard are networks that we
want to explore. WiMAX has been developed over 10 years, but it is still
unknown to most people. However compared to WLAN, it has many advantages in
transmission speed and coverage area. This paper will introduce these two
technologies and make comparisons between WiMAX and WiFi. In addition, wireless
network coexistence of WLAN and WiMAX will be explored through simulation.
Lastly we want to discuss the future of WiMAX in relation to WiFi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0744</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0744</id><created>2014-12-01</created><updated>2015-05-18</updated><authors><author><keyname>Kolchinsky</keyname><forenames>Artemy</forenames></author><author><keyname>Louren&#xe7;o</keyname><forenames>An&#xe1;lia</forenames></author><author><keyname>Wu</keyname><forenames>Heng-Yi</forenames></author><author><keyname>Li</keyname><forenames>Lang</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from
  the Literature</title><categories>stat.ML cs.IR q-bio.QM</categories><comments>PLOS One (2015)</comments><acm-class>H.2.8; H.3.1; J.3</acm-class><doi>10.1371/journal.pone.0122199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a
subject of intense scientific interest. Biomedical literature mining can aid
DDI research by extracting evidence for large numbers of potential interactions
from published literature and clinical databases. Though DDI is investigated in
domains ranging in scale from intracellular biochemistry to human populations,
literature mining has not been used to extract specific types of experimental
evidence, which are reported differently for distinct experimental goals. We
focus on pharmacokinetic evidence for DDI, essential for identifying causal
mechanisms of putative interactions and as input for further pharmacological
and pharmaco-epidemiology investigations. We used manually curated corpora of
PubMed abstracts and annotated sentences to evaluate the efficacy of literature
mining on two tasks: first, identifying PubMed abstracts containing
pharmacokinetic evidence of DDIs; second, extracting sentences containing such
evidence from abstracts. We implemented a text mining pipeline and evaluated it
using several linear classifiers and a variety of feature transforms. The most
important textual features in the abstract and sentence classification tasks
were analyzed. We also investigated the performance benefits of using features
derived from PubMed metadata fields, various publicly available named entity
recognizers, and pharmacokinetic dictionaries. Several classifiers performed
very well in distinguishing relevant and irrelevant abstracts (reaching
F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,
iAUC~=0.83). We found that word bigram features were important for achieving
optimal classifier performance and that features derived from Medical Subject
Headings (MeSH) terms significantly improved abstract classification. ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0751</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0751</id><created>2014-12-01</created><authors><author><keyname>Wieting</keyname><forenames>John</forenames></author></authors><title>Tiered Clustering to Improve Lexical Entailment</title><categories>cs.CL</categories><comments>Paper for course project for Advanced NLP Spring 2013. 8 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Many tasks in Natural Language Processing involve recognizing lexical
entailment. Two different approaches to this problem have been proposed
recently that are quite different from each other. The first is an asymmetric
similarity measure designed to give high scores when the contexts of the
narrower term in the entailment are a subset of those of the broader term. The
second is a supervised approach where a classifier is learned to predict
entailment given a concatenated latent vector representation of the word. Both
of these approaches are vector space models that use a single context vector as
a representation of the word. In this work, I study the effects of clustering
words into senses and using these multiple context vectors to infer entailment
using extensions of these two algorithms. I find that this approach offers some
improvement to these entailment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0755</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0755</id><created>2014-12-01</created><authors><author><keyname>Baaziz</keyname><forenames>Abdelkader</forenames></author><author><keyname>Quoniam</keyname><forenames>Luc</forenames></author></authors><title>How to use Big Data technologies to optimize operations in Upstream
  Petroleum Industry</title><categories>cs.CY</categories><comments>21st World Petroleum Congress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Big Data is the oil of the new economy&quot; is the most famous citation during
the three last years. It has even been adopted by the World Economic Forum in
2011. In fact, Big Data is like crude! It's valuable, but if unrefined it
cannot be used. It must be broken down, analyzed for it to have value. But what
about Big Data generated by the Petroleum Industry and particularly its
upstream segment?
  Upstream is no stranger to Big Data. Understanding and leveraging data in the
upstream segment enables firms to remain competitive throughout planning,
exploration, delineation, and field development. Oil &amp; Gas Companies conduct
advanced geophysics modeling and simulation to support operations where 2D, 3D
&amp; 4D Seismic generate significant data during exploration phases. They closely
monitor the performance of their operational assets. To do this, they use
thousands sensors in subsurface wells and surface facilities to provide
continuous and real-time monitoring of assets and environmental conditions.
Unfortunately, this information comes in various and increasingly complex
forms, making it a challenge to collect, interpret, and leverage the disparate
data.
  Big Data technologies integrate common and disparate data sets to deliver the
right information at the appropriate time to the correct decision-maker. These
capabilities help firms act on large volumes of data, transforming
decision-making from reactive to proactive and optimizing all phases of
exploration, development and production. Furthermore, Big Data offers multiple
opportunities to ensure safer, more responsible operations. Another invaluable
effect of that would be shared learning.
  The aim of this paper is to explain how to use Big Data technologies to
optimize operations. How can Big Data help experts to decision-making leading
the desired outcomes?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0760</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0760</id><created>2014-12-01</created><updated>2015-10-28</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Fagerberg</keyname><forenames>Rolf</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Competitive Local Routing with Constraints</title><categories>cs.CG</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ vertices in the plane and $S$ a set of non-crossing
line segments between vertices in $P$, called constraints. Two vertices are
visible if the straight line segment connecting them does not properly
intersect any constraints. The constrained $\Theta_m$-graph is constructed by
partitioning the plane around each vertex into $m$ disjoint cones, each with
aperture $\theta = 2 \pi/m$, and adding an edge to the `closest' visible vertex
in each cone. We consider how to route on the constrained $\Theta_6$-graph. We
first show that no deterministic 1-local routing algorithm is
$o(\sqrt{n})$-competitive on all pairs of vertices of the constrained
$\Theta_6$-graph. After that, we show how to route between any two visible
vertices of the constrained $\Theta_6$-graph using only 1-local information.
Our routing algorithm guarantees that the returned path has length at most 2
times the Euclidean distance between the source and destination. Additionally,
we provide a 1-local 18-competitive routing algorithm for visible vertices in
the constrained half-$\Theta_6$-graph, a subgraph of the constrained
$\Theta_6$-graph that is equivalent to the Delaunay graph where the empty
region is an equilateral triangle. To the best of our knowledge, these are the
first local routing algorithms in the constrained setting with guarantees on
the length of the returned path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0765</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0765</id><created>2014-12-01</created><updated>2016-02-02</updated><authors><author><keyname>Thornburg</keyname><forenames>Andrew</forenames></author><author><keyname>Bai</keyname><forenames>Tianyang</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Performance Analysis of mmWave Ad Hoc Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing, December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad hoc networks provide an on-demand, infrastructure-free means to
communicate between soldiers in war zones, aid workers in disaster areas, or
consumers in device-to-device (D2D) applications. Unfortunately, ad hoc
networks are limited by interference due to nearby transmissions.
Millimeter-wave (mmWave) devices offer several potential advantages for ad hoc
networks including reduced interference due to directional antennas and
building blockages, not to mention huge bandwidth channels for large data
rates.. This paper uses a stochastic geometry approach to characterize the
one-way and two-way signal-to-interference ratio distribution of a mmWave ad
hoc network with directional antennas, random blockages, and ALOHA channel
access. The interference-to-noise ratio shows that a fundamental limitation of
an ad hoc network, interference, may still be an issue. The performance of
mmWave ad hoc networks is bounded by the transmission capacity and area
spectral efficiency. The results show that mmWave networks can support much
higher densities and larger spectral efficiencies, even in the presence of
blockage, compared with lower frequency communication for certain link
distances. Due to the increased bandwidth, the rate coverage of mmWave can be
much greater than lower frequency devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0767</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0767</id><created>2014-12-01</created><updated>2015-10-06</updated><authors><author><keyname>Tran</keyname><forenames>Du</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author></authors><title>Learning Spatiotemporal Features with 3D Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple, yet effective approach for spatiotemporal feature
learning using deep 3-dimensional convolutional networks (3D ConvNets) trained
on a large scale supervised video dataset. Our findings are three-fold: 1) 3D
ConvNets are more suitable for spatiotemporal feature learning compared to 2D
ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in
all layers is among the best performing architectures for 3D ConvNets; and 3)
Our learned features, namely C3D (Convolutional 3D), with a simple linear
classifier outperform state-of-the-art methods on 4 different benchmarks and
are comparable with current best methods on the other 2 benchmarks. In
addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset
with only 10 dimensions and also very efficient to compute due to the fast
inference of ConvNets. Finally, they are conceptually very simple and easy to
train and use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0773</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0773</id><created>2014-12-01</created><authors><author><keyname>Zhang</keyname><forenames>Heng</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Expressiveness of Logic Programs under General Stable Model Semantics</title><categories>cs.AI cs.LO</categories><comments>Technical report, an extended version of arXiv:1304.0620</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable model semantics had been recently generalized to non-Herbrand
structures by several works, which provides a unified framework and solid
logical foundations for answer set programming. This paper focuses on the
expressiveness of normal and disjunctive programs under the general stable
model semantics. A translation from disjunctive programs to normal programs is
proposed for infinite structures. Over finite structures, some disjunctive
programs are proved to be intranslatable to normal programs if the arities of
auxiliary predicates and functions are bounded in a certain way. The
equivalence of the expressiveness of normal programs and disjunctive programs
over arbitrary structures is also shown to coincide with that over finite
structures, and coincide with whether NP is closed under complement. Moreover,
to capture the exact expressiveness, some intertranslatability results between
logic program classes and fragments of second-order logic are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0774</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0774</id><created>2014-12-01</created><authors><author><keyname>Mostajabi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Yadollahpour</keyname><forenames>Payman</forenames></author><author><keyname>Shakhnarovich</keyname><forenames>Gregory</forenames></author></authors><title>Feedforward semantic segmentation with zoom-out features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a purely feed-forward architecture for semantic segmentation. We
map small image elements (superpixels) to rich feature representations
extracted from a sequence of nested regions of increasing extent. These regions
are obtained by &quot;zooming out&quot; from the superpixel all the way to scene-level
resolution. This approach exploits statistical structure in the image and in
the label space without setting up explicit structured prediction mechanisms,
and thus avoids complex and expensive inference. Instead superpixels are
classified by a feedforward multilayer network. Our architecture achieves new
state of the art performance in semantic segmentation, obtaining 64.4% average
accuracy on the PASCAL VOC 2012 test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0779</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0779</id><created>2014-12-01</created><updated>2015-11-28</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Shortest Path in a Polygon using Sublinear Space</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\SetX}{\mathsf{X}} \newcommand{\VorX}[1]{\mathcal{V} \pth{#1}}
\newcommand{\Polygon}{\mathsf{P}} \newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}$ We resolve an open problem due
to Tetsuo Asano, showing how to compute the shortest path in a polygon, given
in a read only memory, using sublinear space and subquadratic time.
Specifically, given a simple polygon $\Polygon$ with $n$ vertices in a read
only memory, and additional working memory of size $\Space$, the new algorithm
computes the shortest path (in $\Polygon$) in $O( n^2 /\, \Space )$ expected
time. This requires several new tools, which we believe to be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0781</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0781</id><created>2014-12-01</created><updated>2015-12-15</updated><authors><author><keyname>Zhao</keyname><forenames>Zhizhen</forenames></author><author><keyname>Shkolnisky</keyname><forenames>Yoel</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Fast Steerable Principal Component Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryo-electron microscopy nowadays often requires the analysis of hundreds of
thousands of 2D images as large as a few hundred pixels in each direction. Here
we introduce an algorithm that efficiently and accurately performs principal
component analysis (PCA) for a large set of two-dimensional images, and, for
each image, the set of its uniform rotations in the plane and their
reflections. For a dataset consisting of $n$ images of size $L \times L$
pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, while
existing algorithms take $O(nL^4)$. The new algorithm computes the expansion
coefficients of the images in a Fourier-Bessel basis efficiently using the
non-uniform fast Fourier transform. We compare the accuracy and efficiency of
the new algorithm with traditional PCA and existing algorithms for steerable
PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0784</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0784</id><created>2014-12-02</created><authors><author><keyname>Hamilton</keyname><forenames>Linus</forenames></author></authors><title>Braid is undecidable</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Braid is a 2008 puzzle game centered around the ability to reverse time. We
show that Braid can simulate an arbitrary computation. Our construction makes
no use of Braid's unique time mechanics, and therefore may apply to many other
video games.
  We also show that a plausible &quot;bounded&quot; variant of Braid lies within
2-EXPSPACE. Our proof relies on a technical lemma about Turing machines which
may be of independent interest. Namely, define a braidlike Turing machine to be
a Turing machine that, when it writes to the tape, deletes all data on the tape
to the right of the head. We prove that deciding the behavior of such a machine
lies in EXPSPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0795</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0795</id><created>2014-12-02</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Hu</keyname><forenames>Guangda</forenames></author></authors><title>Sylvester-Gallai for Arrangements of Subspaces</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study arrangements of $k$-dimensional subspaces
$V_1,\ldots,V_n \subset \mathbb{C}^\ell$. Our main result shows that, if every
pair $V_{a},V_b$ of subspaces is contained in a dependent triple (a triple
$V_{a},V_b,V_c$ contained in a $2k$-dimensional space), then the entire
arrangement must be contained in a subspace whose dimension depends only on $k$
(and not on $n$). The theorem holds under the assumption that $V_a \cap V_b =
\{0\}$ for every pair (otherwise it is false). This generalizes the
Sylvester-Gallai theorem (or Kelly's theorem for complex numbers), which proves
the $k=1$ case. Our proof also handles arrangements in which we have many pairs
(instead of all) appearing in dependent triples, generalizing the quantitative
results of Barak et. al. [BDWY-pnas].
  One of the main ingredients in the proof is a strengthening of a Theorem of
Barthe [Bar98] (from the $k=1$ to $k&gt;1$ case) proving the existence of a linear
map that makes the angles between pairs of subspaces large on average. Such a
mapping can be found, unless there is an obstruction in the form of a low
dimensional subspace intersecting many of the spaces in the arrangement (in
which case one can use a different argument to prove the main theorem).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0799</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0799</id><created>2014-12-02</created><authors><author><keyname>Vorel</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Roman</keyname><forenames>Adam</forenames></author></authors><title>Complexity of Road Coloring with Prescribed Reset Words</title><categories>cs.FL</categories><comments>To be presented at LATA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By the Road Coloring Theorem (Trahtman, 2008), the edges of any aperiodic
directed multigraph with a constant out-degree can be colored such that the
resulting automaton admits a reset word. There may also be a need for a
particular reset word to be admitted. For certain words it is NP-complete to
decide whether there is a suitable coloring of a given multigraph. We present a
classification of all words over the binary alphabet that separates such words
from those that make the problem solvable in polynomial time. We show that the
classification becomes different if we consider only strongly connected
multigraphs. In this restricted setting the classification remains incomplete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0801</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0801</id><created>2014-12-02</created><authors><author><keyname>Dasgupta</keyname><forenames>Poorna Banerjee</forenames></author></authors><title>Analytical Comparison of Noise Reduction Filters for Image Restoration
  Using SNR Estimation</title><categories>cs.CV</categories><comments>4 pages,Published with International Journal of Computer Trends and
  Technology (IJCTT), volume 17 number 3, Nov 2014</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V17(3):121-124, Nov 2014. ISSN:2231-2803. www.ijcttjournal.org</journal-ref><doi>10.14445/22312803/IJCTT-V17P123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise removal from images is a part of image restoration in which we try to
reconstruct or recover an image that has been degraded by using apriori
knowledge of the degradation phenomenon. Noises present in images can be of
various types with their characteristic Probability Distribution Functions
(PDF). Noise removal techniques depend on the kind of noise present in the
image rather than on the image itself. This paper explores the effects of
applying noise reduction filters having similar properties on noisy images with
emphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0823</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0823</id><created>2014-12-02</created><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Topological Interference Management with Transmitter Cooperation</title><categories>cs.IT math.IT</categories><comments>46 pages, 10 figures, short version presented at the International
  Symposium on Information Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference networks with no channel state information at the transmitter
(CSIT) except for the knowledge of the connectivity graph have been recently
studied under the topological interference management (TIM) framework. In this
paper, we consider a similar problem with topological knowledge but in a
distributed broadcast channel setting, i.e. a network where transmitter
cooperation is enabled. We show that the topological information can also be
exploited in this case to strictly improve the degrees of freedom (DoF) as long
as the network is not fully connected, which is a reasonable assumption in
practice. Achievability schemes based on selective graph coloring, interference
alignment, and hypergraph covering, are proposed. Together with outer bounds
built upon generator sequence, the concept of compound channel settings, and
the relation to index coding, we characterize the symmetric DoF for so-called
regular networks with constant number of interfering links, and identify the
sufficient and/or necessary conditions for the arbitrary network topologies to
achieve a certain amount of symmetric DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0825</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0825</id><created>2014-12-02</created><authors><author><keyname>Bj&#xf8;rner</keyname><forenames>Nikolaj</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Fioravanti</keyname><forenames>Fabio</forenames><affiliation>University of Chieti-Pescara</affiliation></author><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Senni</keyname><forenames>Valerio</forenames><affiliation>ALES s.r.l.</affiliation></author></authors><title>Proceedings First Workshop on Horn Clauses for Verification and
  Synthesis</title><categories>cs.LO cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; I.2.2;</acm-class><journal-ref>EPTCS 169, 2014</journal-ref><doi>10.4204/EPTCS.169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of HCVS 2014, the First Workshop on Horn
Clauses for Verification and Synthesis which was held on July 17, 2014 in
Vienna, Austria as a satellite event of the Federated Logic Conference (FLoC)
and part of the Vienna Summer of Logic (VSL 2014).
  HCVS 2014 was affiliated to the 26th International Conference on Computer
Aided Verification (CAV 2014) and to the 30th International Conference on Logic
Programming (ICLP 2014).
  Most Program Verification and Synthesis problems of interest can be modeled
directly using Horn clauses and many recent advances in the Constraint/Logic
Programming and Program Verification communities have centered around
efficiently solving problems presented as Horn clauses.
  Since Horn clauses for verification and synthesis have been advocated by
these communities in different times and from different perspectives, the HCVS
workshop was organized to stimulate interaction and a fruitful exchange and
integration of experiences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0826</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0826</id><created>2014-12-02</created><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Tang</keyname><forenames>Zhenmin</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Hashing on Nonlinear Manifolds</title><categories>cs.CV</categories><comments>13 pages. arXiv admin note: text overlap with arXiv:1303.7043</comments><doi>10.1109/TIP.2015.2405340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning based hashing methods have attracted considerable attention due to
their ability to greatly increase the scale at which existing algorithms may
operate. Most of these methods are designed to generate binary codes preserving
the Euclidean similarity in the original space. Manifold learning techniques,
in contrast, are better able to model the intrinsic structure embedded in the
original high-dimensional data. The complexities of these models, and the
problems with out-of-sample data, have previously rendered them unsuitable for
application to large-scale embedding, however. In this work, how to learn
compact binary embeddings on their intrinsic manifolds is considered. In order
to address the above-mentioned difficulties, an efficient, inductive solution
to the out-of-sample data problem, and a process by which non-parametric
manifold learning may be used as the basis of a hashing method is proposed. The
proposed approach thus allows the development of a range of new hashing
techniques exploiting the flexibility of the wide variety of manifold learning
approaches available. It is particularly shown that hashing on the basis of
t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark
datasets, and is very effective for image classification with very short code
lengths. The proposed hashing framework is shown to be easily improved, for
example, by minimizing the quantization error with learned orthogonal
rotations. In addition, a supervised inductive manifold hashing framework is
developed by incorporating the label information, which is shown to greatly
advance the semantic retrieval performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0833</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0833</id><created>2014-12-02</created><authors><author><keyname>Siyari</keyname><forenames>Peyman</forenames></author><author><keyname>Aghaeinia</keyname><forenames>Hassan</forenames></author></authors><title>Fast and Optimal Power Control Games in Multiuser MIMO Networks</title><categories>cs.GT cs.IT math.IT</categories><comments>Submitted to IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the problem of power control in a multiuser MIMO
network, where the optimal linear precoder is employed in each user to achieve
maximum point- to-point information rate. We design a distributed power control
algorithm based on the concept of game theory and contractive functions that
has a couple of advantages over the previous designs (e.g. more uniqueness
probability of Nash equilibria and asynchronous implementation). Despite these
improvements, the sum-rate of the users does not increase because the proposed
algorithm can not lead the power control game to an efficient equilibrium
point. We solve this issue by modifying our algorithm such that the game is led
to the equilibrium that satisfies a particular criterion. This criterion can be
chosen by the designer to achieve a certain optimality among the equilibria.
Furthermore, we propose the inexact method that helps us to boost the
convergence speed of our modified algorithms. Lastly, we show that pricing
algorithms can also be a special case of our modified algorithms. Simulations
show a noticeable improvement in the sum-rate when we modify our proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0839</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0839</id><created>2014-12-02</created><authors><author><keyname>H&#xe9;am</keyname><forenames>P. -C</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC, FEMTO-ST</affiliation></author><author><keyname>Hugot</keyname><forenames>V.</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Kouchnarenko</keyname><forenames>O.</forenames><affiliation>FEMTO-ST, INRIA Nancy - Grand Est / LORIA / LIFC</affiliation></author></authors><title>The Emptiness Problem for Tree Automata with at Least One Disequality
  Constraint is NP-hard</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of tree automata with equality and disequality constraints was
introduced in 2007 by Filiot, Talbot and Tison. In this paper we show that if
there is at least one disequality constraint, the emptiness problem is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0845</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0845</id><created>2014-12-02</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Vittorio</forenames></author></authors><title>On the Robustness of the Approximate Price of Anarchy in Generalized
  Congestion Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main results shown through Roughgarden's notions of smooth games
and robust price of anarchy is that, for any sum-bounded utilitarian social
function, the worst-case price of anarchy of coarse correlated equilibria
coincides with that of pure Nash equilibria in the class of weighted congestion
games with non-negative and non-decreasing latency functions and that such a
value can always be derived through the, so called, smoothness argument. We
significantly extend this result by proving that, for a variety of (even
non-sum-bounded) utilitarian and egalitarian social functions and for a broad
generalization of the class of weighted congestion games with non-negative (and
possibly decreasing) latency functions, the worst-case price of anarchy of
$\epsilon$-approximate coarse correlated equilibria still coincides with that
of $\epsilon$-approximate pure Nash equilibria, for any $\epsilon\geq 0$. As a
byproduct of our proof, it also follows that such a value can always be
determined by making use of the primal-dual method we introduced in a previous
work. It is important to note that our scenario of investigation is beyond the
scope of application of the robust price of anarchy (for as it is currently
defined), so that our result seems unlikely to be alternatively proved via the
smoothness framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0854</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0854</id><created>2014-12-02</created><authors><author><keyname>Hassan</keyname><forenames>Thomas</forenames><affiliation>Le2i</affiliation></author><author><keyname>Peixoto</keyname><forenames>Rafael</forenames><affiliation>Le2i</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Bertaux</keyname><forenames>Aurlie</forenames><affiliation>Le2i</affiliation></author><author><keyname>Silva</keyname><forenames>Nuno</forenames></author></authors><title>Semantic HMC for Big Data Analysis</title><categories>cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing Big Data can help corporations to im-prove their efficiency. In
this work we present a new vision to derive Value from Big Data using a
Semantic Hierarchical Multi-label Classification called Semantic HMC based in a
non-supervised Ontology learning process. We also proposea Semantic HMC
process, using scalable Machine-Learning techniques and Rule-based reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0864</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0864</id><created>2014-12-02</created><updated>2014-12-04</updated><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>On the Induced Matching Problem in Hamiltonian Bipartite Graphs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the parameterized complexity and inapproximability of
the {\sc Induced Matching} problem in hamiltonian bipartite graphs. We show
that, given a hamiltonian cycle in a hamiltonian bipartite graph, the problem
is W[1]-hard and cannot be solved in time $n^{o(k^{\frac{1}{2}})}$ unless
W[1]=FPT, where $n$ is the number of vertices in the graph. In addition, we
show that unless NP=P, the maximum induced matching in a hamiltonian graph
cannot be approximated within a ratio of $n^{1-\epsilon}$, where $n$ is the
number of vertices in the graph. For a bipartite hamiltonian graph in $n$
vertices, it is NP-hard to approximate its maximum induced matching based on a
hamiltonian cycle of the graph within a ratio of $n^{\frac{1}{4}-\epsilon}$,
where $n$ is the number of vertices in the graph and $\epsilon$ is any positive
constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0879</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0879</id><created>2014-12-02</created><authors><author><keyname>Gallagher</keyname><forenames>Sean</forenames></author><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames></author><author><keyname>Shalaby</keyname><forenames>Walid</forenames></author><author><keyname>Avadhani</keyname><forenames>Adarsh</forenames></author></authors><title>Watsonsim: Overview of a Question Answering Engine</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of the project is to design and run a system similar to Watson,
designed to answer Jeopardy questions. In the course of a semester, we
developed an open source question answering system using the Indri, Lucene,
Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among
additional modules. By the end of the semester, we achieved 18% accuracy on
Jeopardy questions, and work has not stopped since then.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0880</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0880</id><created>2014-12-02</created><authors><author><keyname>Casetti</keyname><forenames>Claudio</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla-Fabiana</forenames></author><author><keyname>Pelle</keyname><forenames>Luciano Curto</forenames></author><author><keyname>Del Valle</keyname><forenames>Carolina</forenames></author><author><keyname>Duan</keyname><forenames>Yufeng</forenames></author><author><keyname>Giaccone</keyname><forenames>Paolo</forenames></author></authors><title>Content-centric Routing in Wi-Fi Direct Multi-group Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The added value of Device-to-Device (D2D) communication amounts to an
efficient content discovery mechanism that enables users to steer their
requests toward the node most likely to satisfy them. In this paper, we address
the implementation of content-centric routing in a D2D architecture for Android
devices based on WiFi Direct, a protocol recently standardised by the Wi-Fi
Alliance. After discussing the creation of multiple D2D groups, we introduce
novel paradigms featuring intra- and inter-group bidirectional communication.
We then present the primitives involved in content advertising and requesting
among members of the multi-group network. Finally, we evaluate the performance
of our architecture in a real testbed involving Android devices in different
group configurations. We also compare the results against the ones achievable
exploiting Bluetooth technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0883</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0883</id><created>2014-12-02</created><authors><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Samarasinghe</keyname><forenames>Tharaka</forenames></author><author><keyname>Evans</keyname><forenames>Jamie S.</forenames></author></authors><title>Optimizing user selection schemes in vector broadcast channels</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the ergodic downlink sum-rate performance of a
system consisting of a set of heterogeneous users. We study three user
selection schemes to group near-orthogonal users for simultaneous transmission.
The first scheme is a random selection policy that achieves fairness, but does
not exploit multi-user diversity. The second scheme is a greedy selection
policy that fully exploits multi-user diversity, but does not achieve fairness,
and the third scheme achieves fairness while partially exploiting multi-user
diversity. We also consider two beamforming methods for data transmission,
namely, maximum-ratio transmission and zero-forcing beamforming. In all
scheduling schemes studied in the paper, there is a key parameter that controls
the degrees of orthogonality of channel directions between co-scheduled users.
We focus on optimally setting this parameter for each scheduling scheme such
that the ergodic downlink sum-rate is maximized. To this end, we derive
analytical expressions for the ergodic downlink sum-rate considering each
scheduling scheme. Numerical results are also presented to provide further
insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0885</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0885</id><created>2014-12-02</created><updated>2015-09-21</updated><authors><author><keyname>Ngo</keyname><forenames>Van Chan</forenames><affiliation>ESPRESSO</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>ESTASYS</affiliation></author><author><keyname>Quilbeuf</keyname><forenames>Jean</forenames><affiliation>ESTASYS</affiliation></author></authors><title>Dynamic Verification of SystemC with Statistical Model Checking</title><categories>cs.SE cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many embedded and real-time systems have a inherent probabilistic behaviour
(sensors data, unreliable hardware,...). In that context, it is crucial to
evaluate system properties such as &quot;the probability that a particular hardware
fails&quot;. Such properties can be evaluated by using probabilistic model checking.
However, this technique fails on models representing realistic embedded and
real-time systems because of the state space explosion. To overcome this
problem, we propose a verification framework based on Statistical Model
Checking. Our framework is able to evaluate probabilistic and temporal
properties on large systems modelled in SystemC, a standard system-level
modelling language. It is fully implemented as an extension of the Plasma-lab
statistical model checker. We illustrate our approach on a multi-lift system
case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0904</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0904</id><created>2014-12-02</created><authors><author><keyname>Duan</keyname><forenames>Nan</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author></authors><title>Finding Semi-Analytic Solutions of Power System Differential-Algebraic
  Equations for Fast Transient Stability Simulation</title><categories>math.DS cs.NA cs.SC cs.SY math.CA</categories><comments>Submitted to IEEE Transactions on Power Systems</comments><msc-class>93D99, 65P99</msc-class><acm-class>G.1.0; G.1.1; G.1.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studies the semi-analytic solution (SAS) of a power system's
differential-algebraic equation. A SAS is a closed-form function of symbolic
variables including time, the initial state and the parameters on system
operating conditions, and hence able to directly give trajectories on system
state variables, which are accurate for at least a certain time window. A
two-stage SAS-based approach for fast transient stability simulation is
proposed, which offline derives the SAS by the Adomian Decomposition Method and
online evaluates the SAS for each of sequential time windows until making up a
desired simulation period. When applied to fault simulation, the new approach
employs numerical integration only for the fault-on period to determine the
post-disturbance initial state of the SAS. The paper further analyzes the
maximum length of a time window for a SAS to keep its accuracy, and
accordingly, introduces a divergence indicator for adaptive time windows. The
proposed SAS-based new approach is validated on the IEEE 10-machine, 39-bus
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0914</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0914</id><created>2014-12-02</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>On Channel Inseparability and the DoF Region of MIMO Multi-way Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>to appear in the ITG Conference on Systems, Communications and Coding
  (SCC), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex multi-way relaying is a potential solution for supporting high
data rates in future Internet-of-Things (IoT) and 5G networks. Thus, in this
paper the full-duplex MIMO multi-way channel consisting of 3 users (Y-channel)
with $M$ antennas each and a common relay node with $N$ antennas is studied.
Each user wants to exchange messages with all the other users via the relay. A
transmission strategy is proposed based on channel diagonalization that
decomposes the channel into parallel sub-channels, and physical-layer network
coding over these sub-channels. It is shown that the proposed strategy achieves
the optimal DoF region of the channel if $N\leq M$. Furthermore, the proposed
strategy that requires joint encoding over multiple sub-channels is compared to
another strategy that encodes over each sub-channel separately. It turns out
that coding jointly over sub-channels is necessary for an optimal transmission
strategy. This shows that the MIMO Y-channel is inseparable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0929</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0929</id><created>2014-12-02</created><updated>2015-02-04</updated><authors><author><keyname>Bellalta</keyname><forenames>B.</forenames></author><author><keyname>Checco</keyname><forenames>A.</forenames></author><author><keyname>Zocca</keyname><forenames>A.</forenames></author><author><keyname>Barcelo</keyname><forenames>J.</forenames></author></authors><title>On the Interactions between Multiple Overlapping WLANs using Channel
  Bonding</title><categories>cs.NI</categories><comments>in IEEE Transactions on Vehicular Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next-generation WLANs will support the use of wider channels, which is known
as channel bonding, to achieve higher throughput. However, because both the
channel center frequency and the channel width are autonomously selected by
each WLAN, the use of wider channels may also increase the competition with
other WLANs operating in the same area for the limited available spectrum, thus
causing the opposite effect. In this paper, we analyse the interactions between
a group of neighboring WLANs that use channel bonding and evaluate the impact
of those interactions on the achievable throughput. A Continuous Time Markov
Network (CTMN) model that is able to capture the coupled operation of a group
of overlapping WLANs is introduced and validated. The results show that the use
of channel bonding can provide significant performance gains even in scenarios
with high densities of WLANs, though it may also cause unfair situations in
which some WLANs cannot access the channel, while others receive most of the
transmission opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0954</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0954</id><created>2014-12-02</created><authors><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Optimized Packet Scheduling in Multiview Video Navigation Systems</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiview video systems, multiple cameras generally acquire the same scene
from different perspectives, such that users have the possibility to select
their preferred viewpoint. This results in large amounts of highly redundant
data, which needs to be properly handled during encoding and transmission over
resource-constrained channels. In this work, we study coding and transmission
strategies in multicamera systems, where correlated sources send data through a
bottleneck channel to a central server, which eventually transmits views to
different interactive users. We propose a dynamic correlation-aware packet
scheduling optimization under delay, bandwidth, and interactivity constraints.
The optimization relies both on a novel rate-distortion model, which captures
the importance of each view in the 3D scene reconstruction, and on an objective
function that optimizes resources based on a client navigation model. The
latter takes into account the distortion experienced by interactive clients as
well as the distortion variations that might be observed by clients during
multiview navigation. We solve the scheduling problem with a novel
trellis-based solution, which permits to formally decompose the multivariate
optimization problem thereby significantly reducing the computation complexity.
Simulation results show the gain of the proposed algorithm compared to baseline
scheduling policies. More in details, we show the gain offered by our dynamic
scheduling policy compared to static camera allocation strategies and to
schemes with constant coding strategies. Finally, we show that the best
scheduling policy consistently adapts to the most likely user navigation path
and that it minimizes distortion variations that can be very disturbing for
users in traditional navigation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0961</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0961</id><created>2014-11-30</created><authors><author><keyname>Chen</keyname><forenames>Jingshu</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Duflot</keyname><forenames>Marie</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Merz</keyname><forenames>Stephan</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Analyzing Conflict Freedom For Multi-threaded Programs With Time
  Annotations</title><categories>cs.PL</categories><comments>http://journal.ub.tu-berlin.de/eceasst/article/view/978</comments><proxy>ccsd</proxy><journal-ref>Electronic Communications of the EASST, 2014, Automated
  Verification of Critical Systems 2014, 70, pp.14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Avoiding access conflicts is a major challenge in the design of
multi-threaded programs. In the context of real-time systems, the absence of
conflicts can be guaranteed by ensuring that no two potentially conflicting
accesses are ever scheduled concurrently.In this paper, we analyze programs
that carry time annotations specifying the time for executing each statement.
We propose a technique for verifying that a multi-threaded program with time
annotations is free of access conflicts. In particular, we generate constraints
that reflect the possible schedules for executing the program and the required
properties. We then invoke an SMT solver in order to verify that no execution
gives rise to concurrent conflicting accesses. Otherwise, we obtain a trace
that exhibits the access conflict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0962</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0962</id><created>2014-12-02</created><updated>2014-12-02</updated><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author></authors><title>Batched Point Location in SINR Diagrams via Algebraic Tools</title><categories>cs.CG</categories><comments>fixed a bib error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SINR model for the quality of wireless connections has been the subject
of extensive recent study. It attempts to predict whether a particular
transmitter is heard at a specific location, in a setting consisting of $n$
simultaneous transmitters and background noise. The SINR model gives rise to a
natural geometric object, the SINR diagram, which partitions the space into $n$
regions where each of the transmitters can be heard and the remaining space
where no transmitter can be heard.
  Efficient point location in the SINR diagram, i.e., being able to build a
data structure that facilitates determining, for a query point, whether any
transmitter is heard there, and if so, which one, has been recently
investigated in several papers. These planar data structures are constructed in
time at least quadratic in $n$ and support logarithmic-time approximate
queries. Moreover, the performance of some of the proposed structures depends
strongly not only on the number $n$ of transmitters and on the approximation
parameter $\varepsilon$, but also on some geometric parameters that cannot be
bounded a priori as a function of $n$ or $\varepsilon$.
  We address the question of batched point location queries, i.e., answering
many queries simultaneously. Specifically, in one dimension, we can answer $n$
queries exactly in amortized polylogarithmic time per query, while in the plane
we can do it approximately.
  We also show how to answer $n^2$ queries exactly in amortized polylogarithmic
time per query, assuming the queries are located on a possibly non-uniform $n
\times n$ grid.
  All these results can handle arbitrary power assignments to the transmitters.
Moreover, the amortized query time in these results depends only on $n$ and
$\varepsilon$.
  These results demonstrate the (so far underutilized) power of combining
algebraic tools with those of computational geometry and other fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0963</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0963</id><created>2014-12-02</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Dropshipping - a revolutionary form movement of goods in the global
  economic crisis</title><categories>cs.CY</categories><comments>14 pages, in Russian</comments><journal-ref>Management and marketing in the innovation economy, Ed.
  A.I.Kovalev. Sankt-Peterburg: Info-Da, 2011. P. 172-185. ISBN
  978-5-94652-371-4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper analyzes the socio-economic impacts of dropshipping - a new form of
organization of sales, is rapidly gaining the Russian market. Dropshipping
opens up tremendous prospects not only for ordinary people, but also for the
Russian manufacturers, which if used properly can dropshipping opportunities
with minimal direct access to the world market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0967</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0967</id><created>2014-12-02</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Ord&#xf3;&#xf1;ez</keyname><forenames>Alberto</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author></authors><title>Queries on LZ-Bounded Encodings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a data structure that stores a string $S$ in space similar to
that of its Lempel-Ziv encoding and efficiently supports access, rank and
select queries. These queries are fundamental for implementing succinct and
compressed data structures, such as compressed trees and graphs. We show that
our data structure can be built in a scalable manner and is both small and fast
in practice compared to other data structures supporting such queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0969</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0969</id><created>2014-12-02</created><authors><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author><author><keyname>Yazdanbod</keyname><forenames>Sadra</forenames></author></authors><title>Settling Some Open Problems on 2-Player Symmetric Nash Equilibria</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, researchers have studied the complexity of several decision
versions of Nash equilibrium in (symmetric) two-player games (bimatrix games).
To the best of our knowledge, the last remaining open problem of this sort is
the following; it was stated by Papadimitriou in 2007: find a non-symmetric
Nash equilibrium (NE) in a symmetric game. We show that this problem is
NP-complete and the problem of counting the number of non-symmetric NE in a
symmetric game is #P-complete.
  In 2005, Kannan and Theobald defined the &quot;rank of a bimatrix game&quot;
represented by matrices (A, B) to be rank(A+B) and asked whether a NE can be
computed in rank 1 games in polynomial time. Observe that the rank 0 case is
precisely the zero sum case, for which a polynomial time algorithm follows from
von Neumann's reduction of such games to linear programming. In 2011, Adsul et.
al. obtained an algorithm for rank 1 games; however, it does not solve the case
of symmetric rank 1 games. We resolve this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0975</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0975</id><created>2014-12-02</created><authors><author><keyname>Gonze</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author><author><keyname>Trahtman</keyname><forenames>A. N.</forenames></author></authors><title>A Note on a Recent Attempt to Improve the Pin-Frankl Bound</title><categories>cs.FL</categories><comments>Short note presenting a counterexample and the resulting open
  question</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a counterexample to a lemma used in a recent tentative improvement
of the the Pin-Frankl bound for synchronizing automata. This example naturally
leads us to formulate an open question, whose answer could fix the line of
proof, and improve the bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0980</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0980</id><created>2014-12-02</created><updated>2015-08-31</updated><authors><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Scholz</keyname><forenames>Volkher B.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Approximate Degradable Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>v2: 21 pages, 2 figures, improved bounds on the capacity for
  approximate degradable channels based on [arXiv:1507.07775], an author added</comments><msc-class>81P45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Degradable quantum channels are an important class of completely positive
trace-preserving maps. Among other properties, they offer a single-letter
formula for the quantum and the private classical capacity and are
characterized by the fact that the complementary channel can be obtained from
the channel by applying a degrading map. In this work we introduce the concept
of approximate degradable channels, which satisfy this condition up to some
finite $\varepsilon\geq 0$. That is, there exists a degrading map which upon
composition with the channel is $\varepsilon$-close in the diamond norm to the
complementary channel. We show that for any fixed channel the smallest such
$\varepsilon$ can be efficiently determined via a semidefinite program.
Moreover, these approximate degradable channels also approximately inherit all
other properties of degradable channels. As an application, we derive improved
upper bounds to the quantum and private classical capacity for certain channels
of interest in quantum communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0981</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0981</id><created>2014-12-02</created><authors><author><keyname>Vostokin</keyname><forenames>Sergey</forenames></author></authors><title>Templet: a Markup Language for Concurrent Programming</title><categories>cs.PL cs.DC</categories><comments>13 pages</comments><msc-class>68N15</msc-class><acm-class>D.3.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new approach to the description of a network of
interacting processes in a traditional programming language. Special
programming languages or extensions to sequential languages are usually
designed to express the semantics of concurrent execution. Using libraries in
C++, Java, C#, and other languages is more practical way of concurrent
programming. However, this method leads to an increase in workload of a manual
coding. Besides, stock compilers can not detect semantic errors related to the
programming model in such libraries. The new markup language and a special
technique of automatic programming based on the marked code can solve these
problems. The article provides a detailed specification of the markup language
without discussing its implementation details. The language is used for
programming of current and prospective multi-core and many-core systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.0985</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.0985</id><created>2014-12-02</created><updated>2015-02-11</updated><authors><author><keyname>And&#xe9;n</keyname><forenames>Joakim</forenames></author><author><keyname>Katsevich</keyname><forenames>Eugene</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Covariance estimation using conjugate gradient for 3D classification in
  Cryo-EM</title><categories>cs.CV</categories><doi>10.1109/ISBI.2015.7163849</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying structural variability in noisy projections of biological
macromolecules is a central problem in Cryo-EM. In this work, we build on a
previous method for estimating the covariance matrix of the three-dimensional
structure present in the molecules being imaged. Our proposed method allows for
incorporation of contrast transfer function and non-uniform distribution of
viewing angles, making it more suitable for real-world data. We evaluate its
performance on a synthetic dataset and an experimental dataset obtained by
imaging a 70S ribosome complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1001</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1001</id><created>2014-12-02</created><updated>2015-12-04</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Liao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author></authors><title>Optimization Algorithms for Computational Geometry</title><categories>cs.CG cs.DS math.OC</categories><comments>This version 2 has two more algorithms and improved running times
  over version 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two fundamental problems in computational geometry: finding the
maximum inscribed ball (MaxIB) inside a bounded polyhedron defined by $m$
hyperplanes in a $d$-dimensional space, and finding the minimum enclosing ball
(MinEB) of a set of $n$ points in a $d$-dimensional space. We translate both
these geometric problems into optimization problems and apply first-order
methods for smooth and saddle-point optimization to obtain simpler, faster
nearly-linear-time algorithms.
  For MaxIB, the best known running time is $\tilde{O}(m d \alpha^3 /
\varepsilon^3)$ [XSX06], where $\alpha \geq 1$ is the aspect ratio of the
polyhedron. We obtain two new algorithms for MaxIB: one runs in $\tilde{O}(m d
\alpha / \varepsilon)$ time using smooth optimization, and the other runs in
$\tilde{O}(md + m \sqrt{d} \alpha / \varepsilon)$ time using saddle-point
optimization.
  For MinEB, the best known running time is $\tilde{O}(n d /
\sqrt{\varepsilon})$ [SVZ11]. We obtain two new algorithms for MinEB: one runs
in $\tilde{O}(n d / \sqrt{\varepsilon})$ time using smooth optimization, and
the other runs in $\tilde{O}(nd + n \sqrt{d} / \sqrt{\varepsilon})$ time using
saddle-point optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1013</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1013</id><created>2014-12-02</created><authors><author><keyname>Calderone</keyname><forenames>Alberto</forenames></author></authors><title>Assembling biological boolean networks using manually curated databases
  and prediction algorithms</title><categories>q-bio.MN cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the large quantity of information available, thorough researches in
various biological databases are still needed in order to reconstruct and
understand the steps that lead to known or new phenomena. By using
protein-protein interaction networks and algorithms to extract relevant
interconnections among proteins of interest, it is possible to assemble
subnetworks from global interactomes. Using these extracted networks it is
possible to use algorithms to predict signal directions while activation and
inhibition effects can be predicted using RNA interference screenings. The
result of this approach is the automatic generation of boolean networks. This
way of modelling dynamical systems allows the discovery of steady states and
the prediction of stimuli response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1023</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1023</id><created>2014-12-02</created><authors><author><keyname>Luo</keyname><forenames>Yi</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharmalingam</forenames></author><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author></authors><title>Degrees-of-Freedom Regions for $K$-User MISO Time-Correlated Broadcast
  Channel</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the achievable degrees-of-freedom (DoF) regions of
the $K$-user multiple-input-single-output (MISO) time correlated broadcast
channel (BC). The time correlation induces knowledge of the current channel
state information at transmitter (CSIT) with an estimation error $P^{-\alpha}$,
where $P$ is the signal-to-noise ratio (SNR). We consider the following two
scenarios: $(i)$ $K$-user with $K$-antenna base station (BS) and $(ii)$
$3$-user with $2$-antenna BS. In case of symmetric DoF tuples, where all the
users obtain the same DoF, we derive the total DoF equal to
$\frac{K(1-\alpha)}{1+\frac{1}{2}+\cdots+\frac{1}{K}}+K\alpha$ for the first
scenario and $\frac{3+\alpha}{2}$ for the second one. In particular, we provide
the achievability schemes for these two DoF tuples. Nevertheless, we also
consider the asymmetric case where one of the users is guaranteed {\it one}
DoF, and provide the achievability scheme. Notably, the consistency of the
proposed DoF regions with an already published outer bound , as well as with
the Maddah-Ali-Tse (MAT), which assumes only perfect delayed CSIT, and the ZF
beamforming schemes (perfect current CSIT) consents to the optimality of the
proposed achievability schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1024</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1024</id><created>2014-12-02</created><authors><author><keyname>Ciotti</keyname><forenames>Valerio</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Capocci</keyname><forenames>Andrea</forenames></author><author><keyname>Colaiori</keyname><forenames>Francesca</forenames></author><author><keyname>Panzarasa</keyname><forenames>Pietro</forenames></author></authors><title>Degree correlations in signed social networks</title><categories>physics.soc-ph cs.SI</categories><report-no>PHYSA15755</report-no><journal-ref>Physica A: Statistical Mechanics and its Applications (2015), pp.
  25-39</journal-ref><doi>10.1016/j.physa.2014.11.062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate degree correlations in two online social networks where users
are connected through different types of links. We find that, while subnetworks
in which links have a positive connotation, such as endorsement and trust, are
characterized by assortative mixing by degree, networks in which links have a
negative connotation, such as disapproval and distrust, are characterized by
disassortative patterns. We introduce a class of simple theoretical models to
analyze the interplay between network topology and the superimposed structure
based on the sign of links. Results uncover the conditions that underpin the
emergence of the patterns observed in the data, namely the assortativity of
positive subnetworks and the disassortativity of negative ones. We discuss the
implications of our study for the analysis of signed complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1027</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1027</id><created>2014-12-02</created><authors><author><keyname>Botchkarev</keyname><forenames>Alexei</forenames></author><author><keyname>Finnigan</keyname><forenames>Patrick</forenames></author></authors><title>Complexity in the Context of Systems Approach to Project Management</title><categories>cs.SE</categories><journal-ref>Organisational Project Management, 2015, 2(1): 15-34</journal-ref><doi>10.5130/opm.v2i1.4272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complexity is an inherent attribute of any project. The purpose of defining
and documenting complexity is to have an early warning tool allowing a project
team to focus on certain areas and aspects of the project in order to prevent
and alleviate future risks and issues caused by this complexity. The main
contribution of this paper is to present a systematic view of complexity in
project management by identifying its key attributes and classifying complexity
by these attributes. A &quot;complexity taxonomy&quot;, based on a survey of the existing
complexity literature, is developed and discussed including the product,
project, and external environment dimensions. We show how complexity types are
described through simple real life examples and business cases. Then we develop
a framework (tool) for applying the notion of complexity as an early warning
tool for a project manager in order to timely foresee future risks and
problems. The paper is intended for researchers in complexity, project
management, information systems, technology solutions and business management,
and also for information specialists, project managers, program managers,
financial staff and technology directors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1039</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1039</id><created>2014-12-02</created><authors><author><keyname>Atalay</keyname><forenames>F. Betul</forenames></author><author><keyname>Friedler</keyname><forenames>Sorelle A.</forenames></author><author><keyname>Xu</keyname><forenames>Dianna</forenames></author></authors><title>Convex Hull for Probabilistic Points</title><categories>cs.CG</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an O(n log n) time algorithm for the convex hull problem on
points with locations determined by a continuous probability distribution. Such
probabilistic points are common in applied settings such as location-based
services using machine learning determined locations. Our algorithm finds the
convex hull of such points to precision within some expected correctness
determined by a user-given confidence value p. In order to precisely explain
how correct the resulting structure is, we introduce a new certificate error
model for calculating and understanding approximate geometric error based on
the fundamental properties of a geometric structure. We show that this new
error model implies correctness under a robust statistical error model, in
which each point lies within the hull with probability at least p, for the
convex hull problem.
  We additionally show that the error model that we introduce, which is based
on certificates of the type used in kinetic data structures (KDS), allows for a
translation of the KDS framework to this probabilistic points setting. We thus
introduce a framework for calculating and maintaining geometric structures on
moving objects, even when the precise location of those objects is known only
probabilistically. Our framework maintains the geometric structures to within
approximate correctness based on our new certificate error model. We show that,
under our translation, any existing KDS is approximately correct and is
efficient or close to efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1042</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1042</id><created>2014-12-01</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>A CSP implementation of the bigraph embedding problem</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crucial problem for many results and tools about bigraphs and bigraphical
reactive systems is bigraph embedding. An embedding is more informative than a
bigraph matching, since it keeps track of the correspondence between the
various components of the redex (guest) within the agent (host). In this paper,
we present an algorithm for computing embeddings based on a reduction to a
constraint satisfaction problem. This algorithm, that we prove to be sound and
complete, has been successfully implemented in LibBig, a library for
manipulating bigraphical reactive systems. This library can be used for
implementing a wide range of tools, and it can be adapted to various extensions
of bigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1044</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1044</id><created>2014-12-01</created><updated>2015-08-04</updated><authors><author><keyname>Casares</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Problem Theory</title><categories>cs.AI</categories><comments>43 pages</comments><msc-class>68T20 (Primary), 68T05 (Secondary)</msc-class><acm-class>I.2.8; I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Turing machine, as it was presented by Turing himself, models the
calculations done by a person. This means that we can compute whatever any
Turing machine can compute, and therefore we are Turing complete. The question
addressed here is why, Why are we Turing complete? Being Turing complete also
means that somehow our brain implements the function that a universal Turing
machine implements. The point is that evolution achieved Turing completeness,
and then the explanation should be evolutionary, but our explanation is
mathematical. The trick is to introduce a mathematical theory of problems,
under the basic assumption that solving more problems provides more survival
opportunities. So we build a problem theory by fusing set and computing
theories. Then we construct a series of resolvers, where each resolver is
defined by its computing capacity, that exhibits the following property: all
problems solved by a resolver are also solved by the next resolver in the
series if certain condition is satisfied. The last of the conditions is to be
Turing complete. This series defines a resolvers hierarchy that could be seen
as a framework for the evolution of cognition. Then the answer to our question
would be: to solve most problems. By the way, the problem theory defines
adaptation, perception, and learning, and it shows that there are just three
ways to resolve any problem: routine, trial, and analogy. And, most
importantly, this theory demonstrates how problems can be used to found
mathematics and computing on biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1056</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1056</id><created>2014-12-01</created><updated>2014-12-19</updated><authors><author><keyname>Fang</keyname><forenames>Song</forenames></author></authors><title>Three Laws of Multivariable Feedback Systems, Extended Spectral Flatness
  (Extended Wiener Entropy), 'Uncertainty Principles' in Variance Minimization,
  and Performance Limitations in Minimum Variance Estimation/Filtering</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>This paper has been withdrawn by the author due to personal reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, three laws are obtained for multiple-input multiple-output
feedback systems, which are in entropy domain, frequency domain, and time
domain, respectively. The system setup is that with causal plants and causal
controllers. Those laws characterize the performance limitations of such
systems imposed by the feedback mechanism. Some new notions are proposed to
facilitate the analysis: negentropy rate, extended spectral flatness (extended
Wiener entropy), Gaussianity-whiteness measure (joint Shannon-Wiener entropy),
etc. Two approaches are adopted: the integrated approach and the divided
approach. And 'uncertainty principles' are found in minimum variance control.
Besides, performance limitations in minimum variance estimation and filtering
are obtained. In the end, the special case of linear time-invariant feedback
systems is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1058</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1058</id><created>2014-12-01</created><updated>2015-03-26</updated><authors><author><keyname>Johnson</keyname><forenames>Rie</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Effective Use of Word Order for Text Categorization with Convolutional
  Neural Networks</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) is a neural network that can make use of
the internal structure of data such as the 2D structure of image data. This
paper studies CNN on text categorization to exploit the 1D structure (namely,
word order) of text data for accurate prediction. Instead of using
low-dimensional word vectors as input as is often done, we directly apply CNN
to high-dimensional text data, which leads to directly learning embedding of
small text regions for use in classification. In addition to a straightforward
adaptation of CNN from image to text, a simple but new variation which employs
bag-of-word conversion in the convolution layer is proposed. An extension to
combine multiple convolution layers is also explored for higher accuracy. The
experiments demonstrate the effectiveness of our approach in comparison with
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1060</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1060</id><created>2014-12-02</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Gopi</keyname><forenames>Sivakanth</forenames></author></authors><title>On the number of rich lines in truly high dimensional sets</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a new upper bound on the number of $r$-rich lines (lines with at
least $r$ points) in a `truly' $d$-dimensional configuration of points
$v_1,\ldots,v_n \in \mathbb{C}^d$. More formally, we show that, if the number
of $r$-rich lines is significantly larger than $n^2/r^d$ then there must exist
a large subset of the points contained in a hyperplane. We conjecture that the
factor $r^d$ can be replaced with a tight $r^{d+1}$. If true, this would
generalize the classic Szemer\'edi-Trotter theorem which gives a bound of
$n^2/r^3$ on the number of $r$-rich lines in a planar configuration. This
conjecture was shown to hold in $\mathbb{R}^3$ in the seminal work of Guth and
Katz \cite{GK10} and was also recently proved over $\mathbb{R}^4$ (under some
additional restrictions) \cite{SS14}. For the special case of arithmetic
progressions ($r$ collinear points that are evenly distanced) we give a bound
that is tight up to low order terms, showing that a $d$-dimensional grid
achieves the largest number of $r$-term progressions.
  The main ingredient in the proof is a new method to find a low degree
polynomial that vanishes on many of the rich lines. Unlike previous
applications of the polynomial method, we do not find this polynomial by
interpolation. The starting observation is that the degree $r-2$ Veronese
embedding takes $r$-collinear points to $r$ linearly dependent images. Hence,
each collinear $r$-tuple of points, gives us a dependent $r$-tuple of images.
We then use the design-matrix method of \cite{BDWY12} to convert these 'local'
linear dependencies into a global one, showing that all the images lie in a
hyperplane. This then translates into a low degree polynomial vanishing on the
original set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1069</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1069</id><created>2014-12-02</created><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Approximate Lifted Inference with Probabilistic Databases</title><categories>cs.DB cs.AI</categories><comments>12 pages, 5 figures, pre-print for a paper appearing in VLDB 2015.
  arXiv admin note: text overlap with arXiv:1310.6257</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach for approximate evaluation of #P-hard
queries with probabilistic databases. In our approach, every query is evaluated
entirely in the database engine by evaluating a fixed number of query plans,
each providing an upper bound on the true probability, then taking their
minimum. We provide an algorithm that takes into account important schema
information to enumerate only the minimal necessary plans among all possible
plans. Importantly, this algorithm is a strict generalization of all known
results of PTIME self-join-free conjunctive queries: A query is safe if and
only if our algorithm returns one single plan. We also apply three relational
query optimization techniques to evaluate all minimal safe plans very fast. We
give a detailed experimental evaluation of our approach and, in the process,
provide a new way of thinking about the value of probabilistic methods over
non-probabilistic methods for ranking query answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1074</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1074</id><created>2014-12-02</created><authors><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author><author><keyname>Gigu&#xe8;re</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Sagatovich</keyname><forenames>Vladana</forenames></author><author><keyname>D&#xe9;raspe</keyname><forenames>Maxime</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Corbeil</keyname><forenames>Jacques</forenames></author></authors><title>Learning interpretable models of phenotypes from whole genome sequences
  with the Set Covering Machine</title><categories>q-bio.GN cs.CE cs.LG stat.ML</categories><comments>Presented at Machine Learning in Computational Biology 2014,
  Montr\'eal, Qu\'ebec, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increased affordability of whole genome sequencing has motivated its use
for phenotypic studies. We address the problem of learning interpretable models
for discrete phenotypes from whole genomes. We propose a general approach that
relies on the Set Covering Machine and a k-mer representation of the genomes.
We show results for the problem of predicting the resistance of Pseudomonas
Aeruginosa, an important human pathogen, against 4 antibiotics. Our results
demonstrate that extremely sparse models which are biologically relevant can be
learnt using this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1114</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1114</id><created>2014-12-02</created><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>Simm</keyname><forenames>Jaak</forenames></author><author><keyname>Popovic</keyname><forenames>Dusan</forenames></author><author><keyname>Moreau</keyname><forenames>Yves</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Easy Hyperparameter Search Using Optunity</title><categories>cs.LG</categories><comments>5 pages, 1 figure</comments><acm-class>G.4; I.2.5; I.2.6; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optunity is a free software package dedicated to hyperparameter optimization.
It contains various types of solvers, ranging from undirected methods to direct
search, particle swarm and evolutionary optimization. The design focuses on
ease of use, flexibility, code clarity and interoperability with existing
software in all machine learning environments. Optunity is written in Python
and contains interfaces to environments such as R and MATLAB. Optunity uses a
BSD license and is freely available online at http://www.optunity.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1123</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1123</id><created>2014-12-02</created><updated>2015-04-23</updated><authors><author><keyname>Bertasius</keyname><forenames>Gedas</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour
  Detection</title><categories>cs.CV</categories><comments>Accepted to CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contour detection has been a fundamental component in many image segmentation
and object detection systems. Most previous work utilizes low-level features
such as texture or saliency to detect contours and then use them as cues for a
higher-level task such as object detection. However, we claim that recognizing
objects and predicting contours are two mutually related tasks. Contrary to
traditional approaches, we show that we can invert the commonly established
pipeline: instead of detecting contours with low-level cues for a higher-level
recognition task, we exploit object-related features as high-level cues for
contour detection.
  We achieve this goal by means of a multi-scale deep network that consists of
five convolutional layers and a bifurcated fully-connected sub-network. The
section from the input layer to the fifth convolutional layer is fixed and
directly lifted from a pre-trained network optimized over a large-scale object
classification task. This section of the network is applied to four different
scales of the image input. These four parallel and identical streams are then
attached to a bifurcated sub-network consisting of two independently-trained
branches. One branch learns to predict the contour likelihood (with a
classification objective) whereas the other branch is trained to learn the
fraction of human labelers agreeing about the contour presence at a given point
(with a regression criterion).
  We show that without any feature engineering our multi-scale deep learning
approach achieves state-of-the-art results in contour detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1124</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1124</id><created>2014-12-02</created><authors><author><keyname>Itsykson</keyname><forenames>Dmitry</forenames></author><author><keyname>Malova</keyname><forenames>Anna</forenames></author><author><keyname>Oparin</keyname><forenames>Vsevolod</forenames></author><author><keyname>Sokolov</keyname><forenames>Dmitry</forenames></author></authors><title>Tree-like resolution complexity of two planar problems</title><categories>cs.CC</categories><msc-class>68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two CSP problems: the first CSP encodes 2D Sperner's lemma for
the standard triangulation of the right triangle on $n^2$ small triangles; the
second CSP encodes the fact that it is impossible to match cells of $n \times
n$ square to arrows (two horizontal, two vertical and four diagonal) such that
arrows in two cells with a common edge differ by at most $45^\circ$, and all
arrows on the boundary of the square do not look outside (this fact is a
corollary of the Brower's fixed point theorem). We prove that the tree-like
resolution complexities of these CSPs are $2^{\Theta(n)}$. For Sperner's lemma
our result implies $\Omega(n)$ lower bound on the number of request to colors
of vertices that is enough to make in order to find a trichromatic triangle;
this lower bound was originally proved by Crescenzi and Silvestri.
  CSP based on Sperner's lemma is related with the $\rm PPAD$-complete problem.
We show that CSP corresponding to arrows is also related with a $\rm
PPAD$-complete problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1127</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1127</id><created>2014-12-02</created><authors><author><keyname>Lashgar</keyname><forenames>Ahmad</forenames></author><author><keyname>Majidi</keyname><forenames>Alireza</forenames></author><author><keyname>Baniasadi</keyname><forenames>Amirali</forenames></author></authors><title>IPMACC: Open Source OpenACC to CUDA/OpenCL Translator</title><categories>cs.PL cs.DC</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce IPMACC, a framework for translating OpenACC
applications to CUDA or OpenCL. IPMACC is composed of set of translators
translating OpenACC for C applications to CUDA or OpenCL. The framework uses
the system compiler (e.g. nvcc) for generating final accelerator's binary. The
framework can be used for extending the OpenACC API, executing OpenACC
applications, or obtaining CUDA or OpenCL code which is equivalent to OpenACC
code. We verify correctness of our framework under several benchmarks included
from Rodinia Benchmark Suit and CUDA SDK. We also compare the performance of
CUDA version of the benchmarks to OpenACC version which is compiled by our
framework. By comparing CUDA and OpenACC versions, we discuss the limitations
of OpenACC in achieving a performance near to highly-optimized CUDA version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1130</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1130</id><created>2014-12-02</created><authors><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author><author><keyname>Rosenbaum</keyname><forenames>Will</forenames></author></authors><title>It's Not Easy Being Three: The Approximability of Three-Dimensional
  Stable Matching Problems</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1976, Knuth asked if the stable marriage problem (SMP) can be generalized
to marriages consisting of 3 genders. In 1988, Alkan showed that the natural
generalization of SMP to 3 genders ($3$GSM) need not admit a stable marriage.
Three years later, Ng and Hirschberg proved that it is NP-complete to determine
if given preferences admit a stable marriage. They further prove an analogous
result for the $3$ person stable assignment ($3$PSA) problem.
  In light of Ng and Hirschberg's NP-hardness result for $3$GSM and $3$PSA, we
initiate the study of approximate versions of these problems. In particular, we
describe two optimization variants of $3$GSM and $3$PSA: maximally stable
marriage/matching (MSM) and maximum stable submarriage/submatching (MSS). We
show that both variants are NP-hard to approximate within some fixed constant
factor. Conversely, we describe a simple polynomial time algorithm which
computes constant factor approximations for the maximally stable marriage and
matching problems. Thus both variants of MSM are APX-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1135</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1135</id><created>2014-12-02</created><authors><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Pathak</keyname><forenames>Deepak</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Detector Discovery in the Wild: Joint Multiple Instance and
  Representation Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop methods for detector learning which exploit joint training over
both weak and strong labels and which transfer learned perceptual
representations from strongly-labeled auxiliary tasks. Previous methods for
weak-label learning often learn detector models independently using latent
variable optimization, but fail to share deep representation knowledge across
classes and usually require strong initialization. Other previous methods
transfer deep representations from domains with strong labels to those with
only weak labels, but do not optimize over individual latent boxes, and thus
may miss specific salient structures for a particular category. We propose a
model that subsumes these previous approaches, and simultaneously trains a
representation and detectors for categories with either weak or strong labels
present. We provide a novel formulation of a joint multiple instance learning
method that includes examples from classification-style data when available,
and also performs domain transfer learning to improve the underlying detector
representation. Our model outperforms known methods on ImageNet-200 detection
with weak labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1138</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1138</id><created>2014-12-02</created><authors><author><keyname>Fulcher</keyname><forenames>B. D.</forenames></author><author><keyname>Georgieva</keyname><forenames>A. E.</forenames></author><author><keyname>Redman</keyname><forenames>C. W. G.</forenames></author><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author></authors><title>Highly comparative fetal heart rate analysis</title><categories>cs.LG cs.AI q-bio.QM</categories><comments>7 pages, 4 figures</comments><journal-ref>Fulcher, B. D., Georgieva, A., Redman, C. W., &amp; Jones, N. S.
  (2012). Highly comparative fetal heart rate analysis (pp. 3135-3138).
  Presented at the 34th Annual International Conference of the IEEE EMBS, San
  Diego, CA, USA</journal-ref><doi>10.1109/EMBC.2012.6346629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A database of fetal heart rate (FHR) time series measured from 7221 patients
during labor is analyzed with the aim of learning the types of features of
these recordings that are informative of low cord pH. Our 'highly comparative'
analysis involves extracting over 9000 time-series analysis features from each
FHR time series, including measures of autocorrelation, entropy, distribution,
and various model fits. This diverse collection of features was developed in
previous work, and is publicly available. We describe five features that most
accurately classify a balanced training set of 59 'low pH' and 59 'normal pH'
FHR recordings. We then describe five of the features with the strongest linear
correlation to cord pH across the full dataset of FHR time series. The features
identified in this work may be used as part of a system for guiding
intervention during labor in future. This work successfully demonstrates the
utility of comparing across a large, interdisciplinary literature on
time-series analysis to automatically contribute new scientific results for
specific biomedical signal processing challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1140</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1140</id><created>2014-12-02</created><authors><author><keyname>Park</keyname><forenames>Dong-hyeon</forenames></author><author><keyname>Bagaria</keyname><forenames>Akhil</forenames></author><author><keyname>Hannan</keyname><forenames>Fabiha</forenames></author><author><keyname>Storm</keyname><forenames>Eric</forenames></author><author><keyname>Spjut</keyname><forenames>Josef</forenames></author></authors><title>Sphynx: A Shared Instruction Cache Exporatory Study</title><categories>cs.AR</categories><comments>4 pages, 6 figures</comments><acm-class>B.3.2; C.1.2; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sphynx project was an exploratory study to discover what might be done to
improve the heavy replication of in- structions in independent instruction
caches for a massively parallel machine where a single program is executing
across all of the cores. While a machine with only many cores (fewer than 50)
might not have any issues replicating the instructions for each core, as we
approach the era where thousands of cores can be placed on one chip, the
overhead of instruction replication may become unacceptably large. We believe
that a large amount of sharing should be possible when the ma- chine is
configured for all of the threads to issue from the same set of instructions.
We propose a technique that allows sharing an instruction cache among a number
of independent processor cores to allow for inter-thread sharing and reuse of
instruction memory. While we do not have test cases to demonstrate the
potential magnitude of performance gains that could be achieved, the potential
for sharing reduces the die area required for instruction storage on chip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1141</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1141</id><created>2014-11-30</created><authors><author><keyname>Otoom</keyname><forenames>Ahmed</forenames></author><author><keyname>Atoum</keyname><forenames>Issa</forenames></author></authors><title>An Implementation Framework (IF) for the National Information Assurance
  and Cyber Security Strategy (NIACSS) of Jordan</title><categories>cs.SE</categories><comments>7 pages,8 figures, The International Arab Journal of Information
  Technology, Vol. 10, No. 4, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an implementation framework that lays out the ground for
a coherent, systematic, and comprehensive approach to implement the National
Information Assurance and Cyber Security Strategy (NIACSS) of Jordan. The
Framework 1). Suggests a methodology to analyze the NIACSS, 2). Illustrates how
the NIACSS analysis can be utilized to design strategic moves and develop an
appropriate functional structure, and 3). proposes a set of adaptable strategic
controls that govern the NIACSS implementation and allow achieving excellence,
innovation, efficiency, and quality.The framework, if adopted, is expected to
harvest several advantages within the following areas: information security
implementation management, control and guidance, efforts consolidation,
resource utilization, productive collaboration, and completeness. The framework
is flexible and expandable; therefore, it can be generalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1143</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1143</id><created>2014-12-02</created><updated>2015-07-22</updated><authors><author><keyname>Anari</keyname><forenames>Nima</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author></authors><title>The Kadison-Singer Problem for Strongly Rayleigh Measures and
  Applications to Asymmetric TSP</title><categories>cs.DS math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marcus, Spielman, and Srivastava in their seminal work \cite{MSS13} resolved
the Kadison-Singer conjecture by proving that for any set of finitely supported
independently distributed random vectors $v_1,\dots, v_n$ which have &quot;small&quot;
expected squared norm and are in isotropic position (in expectation), there is
a positive probability that the sum $\sum v_i v_i^\intercal$ has small spectral
norm. Their proof crucially employs real stability of polynomials which is the
natural generalization of real-rootedness to multivariate polynomials.
  Strongly Rayleigh distributions are families of probability distributions
whose generating polynomials are real stable \cite{BBL09}. As independent
distributions are just special cases of strongly Rayleigh measures, it is a
natural question to see if the main theorem of \cite{MSS13} can be extended to
families of vectors assigned to the elements of a strongly Rayleigh
distribution.
  In this paper we answer this question affirmatively; we show that for any
homogeneous strongly Rayleigh distribution where the marginal probabilities are
upper bounded by $\epsilon_1$ and any isotropic set of vectors assigned to the
underlying elements whose norms are at most $\sqrt{\epsilon_2}$, there is a set
in the support of the distribution such that the spectral norm of the sum of
the natural quadratic forms of the vectors assigned to the elements of the set
is at most $O(\epsilon_1+\epsilon_2)$. We employ our theorem to provide a
sufficient condition for the existence of spectrally thin trees. This, together
with a recent work of the authors \cite{AO14}, provides an improved upper bound
on the integrality gap of the natural LP relaxation of the Asymmetric Traveling
Salesman Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1145</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1145</id><created>2014-12-02</created><updated>2015-02-05</updated><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author></authors><title>Matrix Multiplication, Trilinear Decompositions, APA Algorithms, and
  Summation</title><categories>cs.DS</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix multiplication (hereafter we use the acronym MM) is among the most
fundamental operations of modern computations. The efficiency of its
performance depends on various factors, in particular vectorization, data
movement and arithmetic complexity of the computations, but here we focus just
on the study of the arithmetic cost and the impact of this study on other areas
of modern computing. In the early 1970s it was expected that the
straightforward cubic time algorithm for MM will soon be accelerated to enable
MM in nearly quadratic arithmetic time, with some far fetched implications.
While pursuing this goal the mainstream research had its focus on the decrease
of the classical exponent 3 of the complexity of MM towards its lower bound 2,
disregarding the growth of the input size required to support this decrease.
Eventually, surprising combinations of novel ideas and sophisticated techniques
enabled the decrease of the exponent to its benchmark value of about 2.38, but
the supporting MM algorithms improved the straightforward one only for the
inputs of immense sizes. Meanwhile, the communication complexity, rather than
the arithmetic complexity, has become the bottleneck of computations in linear
algebra. This development may seem to undermine the value of the past and
future research aimed at the decrease of the arithmetic cost of MM, but we feel
that the study should be reassessed rather than closed and forgotten. We review
the old and new work in this area in the present day context, recall some major
techniques introduced in the study of MM, discuss their impact on the modern
theory and practice of computations for MM and beyond MM, and link one of these
techniques to some simple algorithms for inner product and summation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1151</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1151</id><created>2014-12-02</created><authors><author><keyname>De Angelis</keyname><forenames>Emanuele</forenames></author><author><keyname>Fioravanti</keyname><forenames>Fabio</forenames></author><author><keyname>Navas</keyname><forenames>Jorge A.</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author></authors><title>Verification of Programs by Combining Iterated Specialization with
  Interpolation</title><categories>cs.LO cs.SE</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; I.2.2</acm-class><journal-ref>EPTCS 169, 2014, pp. 3-18</journal-ref><doi>10.4204/EPTCS.169.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a verification technique for program safety that combines Iterated
Specialization and Interpolating Horn Clause Solving. Our new method composes
together these two techniques in a modular way by exploiting the common Horn
Clause representation of the verification problem. The Iterated Specialization
verifier transforms an initial set of verification conditions by using
unfold/fold equivalence preserving transformation rules. During transformation,
program invariants are discovered by applying widening operators. Then the
output set of specialized verification conditions is analyzed by an
Interpolating Horn Clause solver, hence adding the effect of interpolation to
the effect of widening. The specialization and interpolation phases can be
iterated, and also combined with other transformations that change the
direction of propagation of the constraints (forward from the program
preconditions or backward from the error conditions). We have implemented our
verification technique by integrating the VeriMAP verifier with the FTCLP Horn
Clause solver, based on Iterated Specialization and Interpolation,
respectively. Our experimental results show that the integrated verifier
improves the precision of each of the individual components run separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1152</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1152</id><created>2014-12-02</created><authors><author><keyname>Garoche</keyname><forenames>Pierre-Loic</forenames><affiliation>Onera, The French Aerospace Lab</affiliation></author><author><keyname>Gurfinkel</keyname><forenames>Arie</forenames><affiliation>SEI / CMU</affiliation></author><author><keyname>Kahsai</keyname><forenames>Temesghen</forenames><affiliation>NASA Ames / CMU</affiliation></author></authors><title>Synthesizing Modular Invariants for Synchronous Code</title><categories>cs.LO</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 169, 2014, pp. 19-30</journal-ref><doi>10.4204/EPTCS.169.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore different techniques to synthesize modular
invariants for synchronous code encoded as Horn clauses. Modular invariants are
a set of formulas that characterizes the validity of predicates. They are very
useful for different aspects of analysis, synthesis, testing and program
transformation. We describe two techniques to generate modular invariants for
code written in the synchronous dataflow language Lustre. The first technique
directly encodes the synchronous code in a modular fashion. While in the second
technique, we synthesize modular invariants starting from a monolithic
invariant. Both techniques, take advantage of analysis techniques based on
property-directed reachability. We also describe a technique to minimize the
synthesized invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1153</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1153</id><created>2014-12-02</created><authors><author><keyname>Hojjat</keyname><forenames>Hossein</forenames><affiliation>Cornell University, USA</affiliation></author><author><keyname>R&#xfc;mmer</keyname><forenames>Philipp</forenames><affiliation>Uppsala University, Sweden</affiliation></author><author><keyname>Subotic</keyname><forenames>Pavle</forenames><affiliation>Uppsala University, Sweden</affiliation></author><author><keyname>Yi</keyname><forenames>Wang</forenames><affiliation>Uppsala University, Sweden</affiliation></author></authors><title>Horn Clauses for Communicating Timed Systems</title><categories>cs.LO cs.SE cs.SY</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 169, 2014, pp. 39-52</journal-ref><doi>10.4204/EPTCS.169.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Languages based on the theory of timed automata are a well established
approach for modelling and analysing real-time systems, with many applications
both in industrial and academic context. Model checking for timed automata has
been studied extensively during the last two decades; however, even now
industrial-grade model checkers are available only for few timed automata
dialects (in particular Uppaal timed automata), exhibit limited scalability for
systems with large discrete state space, or cannot handle parametrised systems.
We explore the use of Horn constraints and off-the-shelf model checkers for
analysis of networks of timed automata. The resulting analysis method is fully
symbolic and applicable to systems with large or infinite discrete state space,
and can be extended to include various language features, for instance
Uppaal-style communication/broadcast channels and BIP-style interactions, and
systems with infinite parallelism. Experiments demonstrate the feasibility of
the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1154</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1154</id><created>2014-12-02</created><authors><author><keyname>Kafle</keyname><forenames>Bishoksan</forenames></author><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author></authors><title>Convex polyhedral abstractions, specialisation and property-based
  predicate splitting in Horn clause verification</title><categories>cs.LO</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 169, 2014, pp. 53-67</journal-ref><doi>10.4204/EPTCS.169.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to constrained Horn clause (CHC) verification
combining three techniques: abstract interpretation over a domain of convex
polyhedra, specialisation of the constraints in CHCs using abstract
interpretation of query-answer transformed clauses, and refinement by splitting
predicates. The purpose of the work is to investigate how analysis and
transformation tools developed for constraint logic programs (CLP) can be
applied to the Horn clause verification problem. Abstract interpretation over
convex polyhedra is capable of deriving sophisticated invariants and when used
in conjunction with specialisation for propagating constraints it can
frequently solve challenging verification problems. This is a contribution in
itself, but refinement is needed when it fails, and the question of how to
refine convex polyhedral analyses has not been studied much. We present a
refinement technique based on interpolants derived from a counterexample trace;
these are used to drive a property-based specialisation that splits predicates,
leading in turn to more precise convex polyhedral analyses. The process of
specialisation, analysis and splitting can be repeated, in a manner similar to
the CEGAR and iterative specialisation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1156</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1156</id><created>2014-12-02</created><authors><author><keyname>Perotti</keyname><forenames>Alan</forenames><affiliation>University of Turin, Italy</affiliation></author><author><keyname>Boella</keyname><forenames>Guido</forenames><affiliation>University of Turin, Italy</affiliation></author><author><keyname>Garcez</keyname><forenames>Artur d'Avila</forenames><affiliation>City University London, UK</affiliation></author></authors><title>Runtime Verification Through Forward Chaining</title><categories>cs.LO</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 169, 2014, pp. 68-81</journal-ref><doi>10.4204/EPTCS.169.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel rule-based approach for Runtime Verification
of FLTL properties over finite but expanding traces. Our system exploits Horn
clauses in implication form and relies on a forward chaining-based monitoring
algorithm. This approach avoids the branching structure and exponential
complexity typical of tableaux-based formulations, creating monitors with a
single state and a fixed number of rules. This allows for a fast and scalable
tool for Runtime Verification: we present the technical details together with a
working implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1180</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1180</id><created>2014-12-02</created><authors><author><keyname>Lee</keyname><forenames>Joonseok</forenames><affiliation>Bob</affiliation></author><author><keyname>I.</keyname><forenames>R.</forenames><affiliation>Bob</affiliation></author><author><keyname>McKay</keyname></author></authors><title>Optimizing a Personalized Multigram Cellphone Keypad</title><categories>cs.HC</categories><comments>18 pages (including header pages), 3 figures, 8 tables</comments><report-no>TRSNUSC:2014:001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current layouts for alphabetic input on mobile phone keypads are very
inefficient. We propose a genetic algorithm (GA) to find a suitable keypad
layout for each user, based on their personal text history. It incorporates
codes for frequent multigrams, which may be directly input. This greatly
reduces the average number of strokes required for typing. We optimize for
two-handed use, the left thumb covering the leftmost rows and vice versa. The
GA mini- mizes the number of strokes, consecutive use of the same key, and
consecutive use of the same hand. Using these criteria, the algorithm
re-arranges the 26 alphabetic characters, plus 14 additional multigrams, on the
10-key pad. We demonstrate that this arrangement can generate a more effective
layout, especially for SMS-style messages. Substantial savings are verified by
both computational analysis and human evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1185</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1185</id><created>2014-12-02</created><authors><author><keyname>Morgan</keyname><forenames>Jonathan Scott</forenames></author><author><keyname>Barjasteh</keyname><forenames>Iman</forenames></author><author><keyname>Lampe</keyname><forenames>Cliff</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>The Entropy of Attention and Popularity in YouTube Videos</title><categories>cs.SI cs.CY physics.soc-ph</categories><acm-class>C.4; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast majority of YouTube videos never become popular, languishing in
obscurity with few views, no likes, and no comments. We use information
theoretical measures based on entropy to examine how time series distributions
of common measures of popularity in videos from YouTube's &quot;Trending videos&quot; and
&quot;Most recent&quot; video feeds relate to the theoretical concept of attention. While
most of the videos in the &quot;Most recent&quot; feed are never popular, some 20% of
them have distributions of attention metrics and measures of entropy that are
similar to distributions for &quot;Trending videos&quot;. We analyze how the 20% of &quot;Most
recent&quot; videos that become somewhat popular differ from the 80% that do not,
then compare these popular &quot;Most recent&quot; videos to different subsets of
&quot;Trending videos&quot; to try to characterize and compare the attention each
receives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1188</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1188</id><created>2014-12-02</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Elder</keyname><forenames>Murray</forenames></author><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author><author><keyname>Tillmann</keyname><forenames>Stephan</forenames></author></authors><title>2-manifold recognition is in logspace</title><categories>math.GT cs.CC</categories><comments>11 pages, 2 figures</comments><msc-class>57M99, 68Q15</msc-class><acm-class>F.2.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the homeomorphism problem for 2-manifolds can be decided in
logspace. The proof relies on Reingold's logspace solution to the undirected
$s,t$-connectivity problem in graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1189</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1189</id><created>2014-12-02</created><authors><author><keyname>Bonato</keyname><forenames>Anthony</forenames></author><author><keyname>Lozier</keyname><forenames>Marc</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>P&#xe9;rez-Gim&#xe9;nez</keyname><forenames>Xavier</forenames></author><author><keyname>Pra&#x142;at</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>The domination number of on-line social networks and random geometric
  graphs</title><categories>cs.SI cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the domination number for on-line social networks, both in a
stochastic network model, and for real-world, networked data. Asymptotic
sublinear bounds are rigorously derived for the domination number of graphs
generated by the memoryless geometric protean random graph model. We establish
sublinear bounds for the domination number of graphs in the Facebook 100 data
set, and these bounds are well-correlated with those predicted by the
stochastic model. In addition, we derive the asymptotic value of the domination
number in classical random geometric graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1193</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1193</id><created>2014-12-03</created><updated>2015-09-30</updated><authors><author><keyname>Martens</keyname><forenames>James</forenames></author></authors><title>New insights and perspectives on the natural gradient method</title><categories>cs.LG stat.ML</categories><comments>New title and abstract. Added multiple sections, including a proper
  introduction/outline and one on convergence speed. Many other revisions
  throughout</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural gradient descent is an optimization method traditionally motivated
from the perspective of information geometry, and works well for many
applications as an alternative to stochastic gradient descent. In this paper we
critically analyze this method and its properties, and show how it can be
viewed as a type of approximate 2nd-order optimization method, where the Fisher
information matrix used to compute the natural gradient direction can be viewed
as an approximation of the Hessian. This perspective turns out to have
significant implications for how to design a practical and robust version of
the method. Among our various other contributions is a thorough analysis of the
convergence speed of natural gradient descent and more general stochastic
methods, a critical examination of the oft-used &quot;empirical&quot; approximation of
the Fisher matrix, and an analysis of the (approximate) parameterization
invariance property possessed by the method, which we show still holds for
certain other choices of the curvature matrix, but notably not the Hessian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1194</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1194</id><created>2014-12-03</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Laganiere</keyname><forenames>Robert</forenames></author><author><keyname>Petriu</keyname><forenames>Emil</forenames></author></authors><title>Gradient Boundary Histograms for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a high efficient local spatiotemporal descriptor,
called gradient boundary histograms (GBH). The proposed GBH descriptor is built
on simple spatio-temporal gradients, which are fast to compute. We demonstrate
that it can better represent local structure and motion than other
gradient-based descriptors, and significantly outperforms them on large
realistic datasets. A comprehensive evaluation shows that the recognition
accuracy is preserved while the spatial resolution is greatly reduced, which
yields both high efficiency and low memory usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1205</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1205</id><created>2014-12-03</created><updated>2014-12-25</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>A Simple Homotopy Proximal Mapping for Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel yet simple homotopy proximal mapping
algorithm for compressive sensing. The algorithm adopts a simple proximal
mapping for $\ell_1$ norm regularization at each iteration and gradually
reduces the regularization parameter of the $\ell_1$ norm. We prove a global
linear convergence for the proposed homotopy proximal mapping (HPM) algorithm
for solving compressive sensing under three different settings (i) sparse
signal recovery under noiseless measurements, (ii) sparse signal recovery under
noise measurements, and (iii) nearly-sparse signal recovery under sub-gaussian
noise measurements. In particular, we show that when the measurement matrix
satisfies Restricted Isometric Properties (RIP), our theoretical results in
settings (i) and (ii) almost recover the best condition on the RIP constants
for compressive sensing. In addition, in setting (iii), our results for sparse
signal recovery are better than the previous results, and furthermore our
analysis explicitly exhibits that more observations lead to not only more
accurate recovery but also faster convergence. Compared with previous studies
on linear convergence for sparse signal recovery, our algorithm is simple and
efficient, and our results are better and provide more insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1215</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1215</id><created>2014-12-03</created><authors><author><keyname>Pignot</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>SAMM</affiliation></author><author><keyname>Piton</keyname><forenames>Odile</forenames><affiliation>SAMM</affiliation></author></authors><title>Mary Astell's words in A Serious Proposal to the Ladies (part I), a
  lexicographic inquiry with NooJ</title><categories>cs.CL</categories><comments>Zoe Gavriilidou, Elina Chadjipapa, Lena Papadopoulou, Max
  Silberztein. Nooj 2010 International Conference and Workshop, May 2010,
  Komotini, Greece. University of Thrace, Proceedings of the Nooj 2010
  International Conference and Workshop, pp.232-244,
  http://synmorphose.compulaw.gr/joomlatools-files/docman-files/Zoe-Gav\_BOOK\_7.pdf</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the following article we elected to study with NooJ the lexis of a 17 th
century text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,
part I, published in 1694. We first focused on the semantics to see how Astell
builds her vindication of the female sex, which words she uses to sensitise
women to their alienated condition and promote their education. Then we studied
the morphology of the lexemes (which is different from contemporary English)
used by the author, thanks to the NooJ tools we have devised for this purpose.
NooJ has great functionalities for lexicographic work. Its commands and graphs
prove to be most efficient in the spotting of archaic words or variants in
spelling. Introduction In our previous articles, we have studied the
singularities of 17 th century English within the framework of a diachronic
analysis thanks to syntactical and morphological graphs and thanks to the
dictionaries we have compiled from a corpus that may be expanded overtime. Our
early work was based on a limited corpus of English travel literature to Greece
in the 17 th century. This article deals with a late seventeenth century text
written by a woman philosopher and essayist, Mary Astell (1666--1731),
considered as one of the first English feminists. Astell wrote her essay at a
time in English history when women were &quot;the weaker vessel&quot; and their main
business in life was to charm and please men by their looks and submissiveness.
In this essay we will see how NooJ can help us analyse Astell's rhetoric (what
point of view does she adopt, does she speak in her own name, in the name of
all women, what is her representation of men and women and their relationships
in the text, what are the goals of education?). Then we will turn our attention
to the morphology of words in the text and use NooJ commands and graphs to
carry out a lexicographic inquiry into Astell's lexemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1216</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1216</id><created>2014-12-03</created><authors><author><keyname>Heidsieck</keyname><forenames>Alexandra</forenames></author></authors><title>Simple Two-Dimensional Object Tracking based on a Graph Algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visual observation and tracking of cells and other micrometer-sized
objects has many different biomedical applications. The automation of those
tasks based on computer methods helps in the evaluation of such measurements.
In this work, we present a general purpose algorithm that excels at evaluating
deterministic behavior of micrometer-sized objects. Our concrete application is
the tracking of fast moving objects over large distances along deterministic
trajectories in a microscopic video. Thereby, we are able to determine
characteristic properties of the objects. For this purpose, we use a set of
basic algorithms, including blob recognition, feature-based shape recognition
and a graph algorithm, and combined them in a novel way. An evaluation of the
algorithms performance shows a high accuracy in the recognition of objects as
well as of complete trajectories. Moreover, a direct comparison to a similar
algorithm shows superior recognition rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1219</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1219</id><created>2014-12-03</created><authors><author><keyname>Deschaud</keyname><forenames>Jean-Emmanuel</forenames><affiliation>CAOR</affiliation></author><author><keyname>Brun</keyname><forenames>Xavier</forenames><affiliation>CAOR</affiliation></author><author><keyname>Goulette</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>CAOR</affiliation></author></authors><title>Colorisation et texturation temps r\'eel d'environnements urbains par
  syst\`eme mobile avec scanner laser et cam\'era fish-eye</title><categories>cs.RO cs.CV</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Revue Francaise de Photogrammetrie et de Teledetection, Revue
  Francaise de Photogrammetrie et de Teledetection, 2010, pp.29-37</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here a real time mobile mapping system mounted on a vehicle. The
terrestrial acquisition system is based on a geolocation system and two
sensors, namely, a laser scanner and a camera with a fish-eye lens. We produce
3D colored points cloud and textured models of the environment. Once the system
has been calibrated, the data acquisition and processing are done &quot;on the way&quot;.
This article mainly presents our methods of colorization of point cloud,
triangulation and texture mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1221</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1221</id><created>2014-12-03</created><authors><author><keyname>Kang</keyname><forenames>Daeseong</forenames></author><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Mahmud</keyname><forenames>Zulkarnine</forenames></author></authors><title>Sequential Operations in LogicWeb</title><categories>cs.PL</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential tasks cannot be effectively handled in logic programming based on
classical logic or linear logic. This limitation can be addressed by using a
fragment of Japaridze'sSequential tasks cannot be effectively handled in logic
programming based on classical logic or linear logic. This limitation can be
addressed by using a fragment of Japaridze's computability logic. We propose
\seqweb, an extension to LogicWeb with sequential goal formulas. SeqWeb extends
the LogicWeb by allowing goals of the form $G\seqand G$ and $G\seqor G$ where
$G$ is a goal. These goals allow us to specify both sequential-conjunctive and
sequential-disjunctive tasks. computability logic. We propose \seqweb, an
extension to LogicWeb with sequential goal formulas. SeqWeb extends the
LogicWeb by allowing goals of the form $G\seqand G$ and $G\seqor G$ where $G$
is a goal. These goals allow us to specify both sequential-conjunctive and
sequential-disjunctive tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1226</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1226</id><created>2014-12-03</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>Christian</forenames></author></authors><title>Mathematical foundation of Information Field Dynamics (revised version)</title><categories>math.DS cs.IT math.IT</categories><comments>Master's thesis, Math Dept, Univ M\&quot;unchen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Field Dynamics (IFD) by Torsten En{\ss}lin provides a tool to
construct simulation schemes for data vectors $d(T)$ from measurements $d(0)$
which describe certain features of a physical process (signal), without any
concrete assumptions about the subgrid structure of the problem. In this work,
the measure theoretical fundament and the necessary probabilistic framework for
IFD are introduced first. The notation for the rest of the work is established
and essential properties are developed. Afterwards, the general setting for IFD
is described, consisting of the signal with its evolution equation and its
linear connection to the data by a response operator. The developed environment
is then used, to step by step imbed the physical language, used in Torsten
En{\ss}lin's work, into a mathematical framework. Also a general approach for
the approximation of an evolution equation is given, serving as a base to
describe the construction of a simulation scheme with IFD. Simulation errors in
the various steps are pointed out, to give an idea where inaccuracies come in
and which steps therefore lead to the necessity of many degrees of freedom of
the signal and of many time steps within the simulation. In the end, IFD is
illustrated in an example scenario. A matrix relation for the update steps of
IFD is derived. It allows to simulate a data vector $d(T)$ which averages a
Klein-Gordon field with one dimension in space and periodic over $[0,2\pi)$.
Also a non-iterative equation for the direct computation of $d(T)$ from $d(0)$
is constructed. This is reached by the fact that the original problem converts
into an ordinary differential equation for the data, if the simulation time
steps get infinitesimally small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1227</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1227</id><created>2014-12-03</created><authors><author><keyname>Villemaud</keyname><forenames>Guillaume</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Hutu</keyname><forenames>Florin</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Risset</keyname><forenames>Tanguy</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Enjeux et propositions sur les architectures RF pour l'homme connect\'e
  \`a la soci\'et\'e num\'erique</title><categories>cs.NI</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Journ{\'e}es Scientifiques URSI 2014, Mar 2014, Paris, France</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an overview of the challenges of increasing development
of wireless links to enable a more consistent and transparent interconnection
between people and the digital world. These issues are in the domain of high
performance architectures for conventional applications of wireless Internet,
but also in the field of sensor networks and connected objects. Beyond the
constraints of the various applications push to develop architectures with high
digital capabilities like software defined radio. In each of these categories,
examples of approaches proposed by INRIA Socrate team are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1229</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1229</id><created>2014-12-03</created><updated>2015-09-21</updated><authors><author><keyname>Pan</keyname><forenames>Feng</forenames></author></authors><title>SAT is a problem with exponential complexity measured by negentropy</title><categories>cs.CC</categories><comments>This paper has been withdrawn by the author due to a crucial naive
  error</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the reason why entropy reduction (negentropy) can be used to
measure the complexity of any computation was first elaborated both in the
aspect of mathematics and informational physics. In the same time the
equivalence of computation and information was clearly stated. Then the
complexities of three specific problems: logical compare, sorting and SAT, were
analyzed and measured. The result showed SAT was a problem with exponential
complexity which naturally leads to the conclusion that no efficient algorithm
exists to solve it. That's to say: NP!=P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1241</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1241</id><created>2014-12-03</created><authors><author><keyname>Fukuda</keyname><forenames>Komei</forenames></author><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author><author><keyname>Szedl&#xe1;k</keyname><forenames>May</forenames></author></authors><title>Combinatorial Redundancy Detection</title><categories>cs.CG cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting and removing redundant constraints is fundamental in
optimization. We focus on the case of linear programs (LPs) in dictionary form,
given by $n$ equality constraints in $n+d$ variables, where the variables are
constrained to be nonnegative. A variable $x_r$ is called redundant, if after
removing $x_r \geq 0$ the LP still has the same feasible region. The time
needed to solve such an LP is denoted by $LP(n,d)$.
  It is easy to see that solving $n+d$ LPs of the above size is sufficient to
detect all redundancies. The currently fastest practical method is the one by
Clarkson: it solves $n+d$ linear programs, but each of them has at most $s$
variables, where $s$ is the number of nonredundant constraints.
  In the first part we show that knowing all of the finitely many dictionaries
of the LP is sufficient for the purpose of redundancy detection. A dictionary
is a matrix that can be thought of as an enriched encoding of a vertex in the
LP. Moreover - and this is the combinatorial aspect - it is enough to know only
the signs of the entries, the actual values do not matter. Concretely we show
that for any variable $x_r$ one can find a dictionary, such that its sign
pattern is either a redundancy or nonredundancy certificate for $x_r$.
  In the second part we show that considering only the sign patterns of the
dictionary, there is an output sensitive algorithm of running time
$\mathcal{O}(d \cdot (n+d) \cdot s^{d-1} \cdot LP(s,d) + d \cdot s^{d} \cdot
LP(n,d))$ to detect all redundancies. In the case where all constraints are in
general position, the running time is $\mathcal{O}(s \cdot LP(n,d) + (n+d)
\cdot LP(s,d))$, which is essentially the running time of the Clarkson method.
Our algorithm extends naturally to a more general setting of arrangements of
oriented topological hyperplane arrangements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1251</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1251</id><created>2014-12-03</created><authors><author><keyname>Toumi</keyname><forenames>Tarek</forenames></author><author><keyname>Zidani</keyname><forenames>Abdelmadjid</forenames></author></authors><title>From Human-Computer Interaction to Human-Robot Social Interaction</title><categories>cs.RO cs.HC cs.SY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 1, No 1, 2014 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human-Robot Social Interaction became one of active research fields in which
researchers from different areas propose solutions and directives leading
robots to improve their interactions with humans. In this paper we propose to
introduce works in both human robot interaction and human computer interaction
and to make a bridge between them, i.e. to integrate emotions and capabilities
concepts of the robot in human computer model to become adequate for human
robot interaction and discuss challenges related to the proposed model. Finally
an illustration through real case of this model will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1254</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1254</id><created>2014-12-03</created><updated>2015-07-09</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Landau</keyname><forenames>Gad M.</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Longest Common Extensions in Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The longest common extension (LCE) of two indices in a string is the length
of the longest identical substrings starting at these two indices. The LCE
problem asks to preprocess a string into a compact data structure that supports
fast LCE queries. In this paper we generalize the LCE problem to trees and
suggest a few applications of LCE in trees to tries and XML databases. Given a
labeled and rooted tree $T$ of size $n$, the goal is to preprocess $T$ into a
compact data structure that support the following LCE queries between subpaths
and subtrees in $T$. Let $v_1$, $v_2$, $w_1$, and $w_2$ be nodes of $T$ such
that $w_1$ and $w_2$ are descendants of $v_1$ and $v_2$ respectively.
\begin{itemize} \item $\LCEPP(v_1, w_1, v_2, w_2)$: (path-path $\LCE$) return
the longest common prefix of the paths $v_1 \leadsto w_1$ and $v_2 \leadsto
w_2$. \item $\LCEPT(v_1, w_1, v_2)$: (path-tree $\LCE$) return maximal
path-path LCE of the path $v_1 \leadsto w_1$ and any path from $v_2$ to a
descendant leaf. \item $\LCETT(v_1, v_2)$: (tree-tree $\LCE$) return a maximal
path-path LCE of any pair of paths from $v_1$ and $v_2$ to descendant leaves.
\end{itemize} We present the first non-trivial bounds for supporting these
queries. For $\LCEPP$ queries, we present a linear-space solution with
$O(\log^{*} n)$ query time. For $\LCEPT$ queries, we present a linear-space
solution with $O((\log\log n)^{2})$ query time, and complement this with a
lower bound showing that any path-tree LCE structure of size $O(n \polylog(n))$
must necessarily use $\Omega(\log\log n)$ time to answer queries. For $\LCETT$
queries, we present a time-space trade-off, that given any parameter $\tau$, $1
\leq \tau \leq n$, leads to an $O(n\tau)$ space and $O(n/\tau)$ query-time
solution. This is complemented with a reduction to the the set intersection
problem implying that a fast linear space solution is not likely to exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1257</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1257</id><created>2014-12-03</created><updated>2015-12-10</updated><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Markin</keyname><forenames>Nadya</forenames></author></authors><title>Fast-Decodable Space-Time Codes for the $N$-Relay and Multiple-Access
  MIMO Channel</title><categories>cs.IT math.IT math.NT</categories><comments>IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2015.2496254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the first general constructions of fast-decodable, more
specifically (conditionally) $g$-group decodable, space-time block codes for
the Nonorthogonal Amplify and Forward (NAF) Multiple-Input Multiple-Output
(MIMO) relay channel under the half-duplex constraint are proposed. In this
scenario, the source and the intermediate relays used for data amplification
are allowed to employ multiple antennas for data transmission and reception.
The worst-case decoding complexity of the obtained codes is reduced by up to
$75%$. In addition to being fast-decodable, the proposed codes achieve
full-diversity and have nonvanishing determinants, which has been shown to be
useful for achieving the optimal Diversity-Multiplexing Tradeoff (DMT) of the
NAF channel.
  Further, it is shown that the same techniques as in the cooperative scenario
can be utilized to achieve fast-decodability for $K$-user MIMO Multiple-Access
Channel (MAC) space-time block codes. The resulting codes in addition exhibit
the conditional nonvanishing determinant property which, for its part, has been
shown to be useful for achieving the optimal MAC-DMT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1261</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1261</id><created>2014-12-03</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>On the Complexity of Various Parameterizations of Common Induced
  Subgraph Isomorphism</title><categories>cs.DS</categories><comments>Accepted in IWOCA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum Common Induced Subgraph (henceforth MCIS) is among the most studied
classical NP-hard problems. MCIS remains NP-hard on many graph classes
including bipartite graphs, planar graphs and $k$-trees. Little is known,
however, about the parameterized complexity of the problem. When parameterized
by the vertex cover number of the input graphs, the problem was recently shown
to be fixed-parameter tractable. Capitalizing on this result, we show that the
problem does not have a polynomial kernel when parameterized by vertex cover
unless $NP \subseteq \mathsf{coNP}/poly$. We also show that \mccis (MCCIS),
which is a variant where the solution must be connected, is also
fixed-parameter tractable when parameterized by the vertex cover number of
input graphs. Both problems are shown to be W[1]-complete on bipartite graphs
and graphs of girth five and, unless P = NP, they do not belong to the class XP
when parameterized by a bound on the size of the minimum feedback vertex sets
of the input graphs, that is solving them in polynomial time is very unlikely
when this parameter is a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1265</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1265</id><created>2014-12-03</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Deeply learned face representations are sparse, selective, and robust</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper designs a high-performance deep convolutional network (DeepID2+)
for face recognition. It is learned with the identification-verification
supervisory signal. By increasing the dimension of hidden representations and
adding supervision to early convolutional layers, DeepID2+ achieves new
state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical
studies, we have discovered three properties of its deep neural activations
critical for the high performance: sparsity, selectiveness and robustness. (1)
It is observed that neural activations are moderately sparse. Moderate sparsity
maximizes the discriminative power of the deep net as well as the distance
between images. It is surprising that DeepID2+ still can achieve high
recognition accuracy even after the neural responses are binarized. (2) Its
neurons in higher layers are highly selective to identities and
identity-related attributes. We can identify different subsets of neurons which
are either constantly excited or inhibited when different identities or
attributes are present. Although DeepID2+ is not taught to distinguish
attributes during training, it has implicitly learned such high-level concepts.
(3) It is much more robust to occlusions, although occlusion patterns are not
included in the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1267</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1267</id><created>2014-12-03</created><authors><author><keyname>Morsi</keyname><forenames>Rania</forenames></author><author><keyname>Michalopoulos</keyname><forenames>Diomidis S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>On-Off Transmission Policy for Wireless Powered Communication with
  Energy Storage</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, 1 Table, accepted for conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an energy harvesting (EH) node which harvests
energy from a radio frequency (RF) signal broadcasted by an access point (AP)
in the downlink (DL). The node stores the harvested energy in an energy buffer
and uses the stored energy to transmit data to the AP in the uplink (UL). We
consider a simple transmission policy, which accounts for the fact that, in
practice, the EH node may not have knowledge of the EH profile nor of the UL
channel state information. In particular, in each time slot, the EH node
transmits with either a constant desired power or remains silent if not enough
energy is available in its energy buffer. For this simple policy, we use the
theory of discrete-time continuous-state Markov chains to analyze the limiting
distribution of the stored energy for finite- and infinite-size energy buffers.
Moreover, we take into account imperfections of the energy buffer and the
circuit power consumption of the EH node. For a Rayleigh fading DL channel, we
provide the limiting distribution of the energy buffer content in closed form.
In addition, we analyze the average error rate and the outage probability of a
Rayleigh faded UL channel and show that the diversity order is not affected by
the finite capacity of the energy buffer. Our results reveal that, for medium
to high signal-to-noise ratio (SNRs), the optimal target transmit power of the
EH node is less than the average harvested power and increases with the
capacity of the energy buffer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1271</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1271</id><created>2014-12-03</created><updated>2015-01-28</updated><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Deep Distributed Random Samplings for Supervised Learning: An
  Alternative to Random Forests?</title><categories>cs.LG stat.ML</categories><comments>This paper has been withdrawn by the author. The idea is wrong and is
  no longer to be posed on site. The paper will no longer be updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine
learning as a coding and dimensionality reduction problem, and further proposed
a simple unsupervised dimensionality reduction method, entitled deep
distributed random samplings (DDRS). In this paper, we further extend it to
supervised learning incrementally. The key idea here is to incorporate label
information into the coding process by reformulating that each center in DDRS
has multiple output units indicating which class the center belongs to. The
supervised learning method seems somewhat similar with random forests
(\cite{breiman2001random}), here we emphasize their differences as follows. (i)
Each layer of our method considers the relationship between part of the data
points in training data with all training data points, while random forests
focus on building each decision tree on only part of training data points
independently. (ii) Our method builds gradually-narrowed network by sampling
less and less data points, while random forests builds gradually-narrowed
network by merging subclasses. (iii) Our method is trained more straightforward
from bottom layer to top layer, while random forests build each tree from top
layer to bottom layer by splitting. (iv) Our method encodes output targets
implicitly in sparse codes, while random forests encode output targets by
remembering the class attributes of the activated nodes. Therefore, our method
is a simpler, more straightforward, and maybe a better alternative choice,
though both methods use two very basic elements---randomization and nearest
neighbor optimization---as the core. This preprint is used to protect the
incremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). Full
empirical evaluation will be announced carefully later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1283</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1283</id><created>2014-12-03</created><updated>2015-04-02</updated><authors><author><keyname>Dai</keyname><forenames>Jifeng</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Convolutional Feature Masking for Joint Object and Stuff Segmentation</title><categories>cs.CV</categories><comments>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topic of semantic segmentation has witnessed considerable progress due to
the powerful features learned by convolutional neural networks (CNNs). The
current leading approaches for semantic segmentation exploit shape information
by extracting CNN features from masked image regions. This strategy introduces
artificial boundaries on the images and may impact the quality of the extracted
features. Besides, the operations on the raw image domain require to compute
thousands of networks on a single image, which is time-consuming. In this
paper, we propose to exploit shape information via masking convolutional
features. The proposal segments (e.g., super-pixels) are treated as masks on
the convolutional feature maps. The CNN features of segments are directly
masked out from these maps and used to train classifiers for recognition. We
further propose a joint method to handle objects and &quot;stuff&quot; (e.g., grass, sky,
water) in the same framework. State-of-the-art results are demonstrated on
benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling
computational speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1285</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1285</id><created>2014-12-03</created><authors><author><keyname>Frank</keyname><forenames>Steven A.</forenames></author></authors><title>The inductive theory of natural selection</title><categories>q-bio.PE cs.NE physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of natural selection has two forms. Deductive theory describes how
populations change over time. One starts with an initial population and some
rules for change. From those assumptions, one calculates the future state of
the population. Deductive theory predicts how populations adapt to
environmental challenge. Inductive theory describes the causes of change in
populations. One starts with a given amount of change. One then assigns
different parts of the total change to particular causes. Inductive theory
analyzes alternative causal models for how populations have adapted to
environmental challenge. This chapter emphasizes the inductive analysis of
cause.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1297</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1297</id><created>2014-12-03</created><authors><author><keyname>Ferro</keyname><forenames>Mariza</forenames></author><author><keyname>Mury</keyname><forenames>Antonio R.</forenames></author><author><keyname>Manfroi</keyname><forenames>Laion F.</forenames></author><author><keyname>Schlze</keyname><forenames>Bruno</forenames></author></authors><title>High Performance Computing Evaluation A methodology based on Scientific
  Application Requirements</title><categories>cs.DC</categories><comments>29 pages, 2 tables and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Performance Distributed Computing is essential to boost scientific
progress in many areas of science and to efficiently deploy a number of complex
scientific applications. These applications have different characteristics that
require distinct computational resources too. In this work we propose a
systematic performance evaluation methodology. The focus of our methodology
begins on scientific application characteristics, and then considers how these
characteristics interact with the problem size, with the programming language
and finally with a specific computational architecture. The computational
experiments developed highlight this model of evaluation and indicate that
optimal performance is found when we evaluate a combination of application
class, program language, problem size and architecture model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1318</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1318</id><created>2014-12-03</created><authors><author><keyname>Bhattacharya</keyname><forenames>Sayan</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author></authors><title>Deterministic Fully Dynamic Data Structures for Vertex Cover and
  Matching</title><categories>cs.DS</categories><comments>An extended abstract of this paper will appear in SODA' 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first deterministic data structures for maintaining
approximate minimum vertex cover and maximum matching in a fully dynamic graph
$G = (V,E)$, with $|V| = n$ and $|E| =m$, in $o(\sqrt{m}\,)$ time per update.
In particular, for minimum vertex cover we provide deterministic data
structures for maintaining a $(2+\eps)$ approximation in $O(\log n/\eps^2)$
amortized time per update.
  For maximum matching, we show how to maintain a $(3+\eps)$ approximation in
$O(\min(\sqrt{n}/\epsilon, m^{1/3}/\eps^2))$ {\em amortized} time per update,
and a $(4+\eps)$ approximation in $O(m^{1/3}/\eps^2)$ {\em worst-case} time per
update. Our data structure for fully dynamic minimum vertex cover is
essentially near-optimal and settles an open problem by Onak and Rubinfeld from
STOC' 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1330</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1330</id><created>2014-12-03</created><authors><author><keyname>Barreau</keyname><forenames>Jean-Baptiste</forenames><affiliation>CReAAH, INRIA - IRISA, INSA Rennes</affiliation></author><author><keyname>Nicolas</keyname><forenames>Th&#xe9;ophane</forenames><affiliation>INRAP</affiliation></author><author><keyname>Bruniaux</keyname><forenames>G</forenames><affiliation>CReAAH</affiliation></author><author><keyname>Petit</keyname><forenames>E</forenames><affiliation>CReAAH</affiliation></author><author><keyname>Petit</keyname><forenames>Q</forenames><affiliation>CReAAH, UR1</affiliation></author><author><keyname>Bernard</keyname><forenames>Y</forenames><affiliation>CReAAH, UR1</affiliation></author><author><keyname>Gaugne</keyname><forenames>Ronan</forenames><affiliation>UR1</affiliation></author><author><keyname>Gouranton</keyname><forenames>Val&#xe9;rie</forenames><affiliation>INRIA - IRISA, INSA Rennes</affiliation></author></authors><title>Ceramics Fragments Digitization by Photogrammetry, Reconstructions and
  Applications</title><categories>cs.GR</categories><comments>International Conference on Culturage Heritage, EuroMed, 2014, Nov
  2014, Lemessos, Cyprus</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an application of photogrammetry on ceramic fragments
from two excavation sites located north-west of France. The restitution by
photogrammetry of these different fragments allowed reconstructions of the
potteries in their original state or at least to get to as close as possible.
We used the 3D reconstructions to compute some metrics and to generate a
presentation support by using a 3D printer. This work is based on affordable
tools and illustrates how 3D technologies can be quite easily integrated in
archaeology process with limited financial resources. 1. INTRODUCTION Today,
photogrammetry and 3D modelling are an integral part of the methods used in
archeology and heritage management. They provide answers to scientific needs in
the fields of conservation, preservation, restoration and mediation of
architectural, archaeological and cultural heritage [2] [6] [7] [9].
Photogrammetry on ceramic fragments was one of the first applications
contemporary of the development of this technique applied in the archaeological
community [3]. More recently and due to its democratization, it was applied
more generally to artifacts [5]. Finally joined today by the rise of 3D
printing [8] [10], it can restore fragmented artifacts [1] [12]. These examples
target one or several particular objects and use different types of equipment
that can be expensive. These aspects can put off uninitiated archaeologists. So
it would be appropriate to see if these techniques could be generalized to a
whole class of geometrically simple and common artifacts, such as ceramics.
From these observations, associated to ceramics specialists with fragments of
broken ceramics, we aimed at arranging different tools and methods, including
photogrammetry, to explore opportunities for a cheap and attainable
reconstruction methodology and its possible applications. Our first objective
was to establish a protocol for scanning fragments with photogrammetry, and for
reconstruction of original ceramics. We used the digital reconstitutions of the
ceramics we got following our process to calculate some metrics and to design
and 3D print a display for the remaining fragments of one pottery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1340</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1340</id><created>2014-12-03</created><authors><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>B&#xfc;ttner</keyname><forenames>Martin</forenames></author><author><keyname>Leistedt</keyname><forenames>Boris</forenames></author><author><keyname>Peiris</keyname><forenames>Hiranya V.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>On spin scale-discretised wavelets on the sphere for the analysis of CMB
  polarisation</title><categories>astro-ph.IM astro-ph.CO cs.IT math.IT</categories><comments>4 pages, Proceedings IAU Symposium No. 306, 2014 (A. F. Heavens,
  J.-L. Starck, A. Krone-Martins eds.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new spin wavelet transform on the sphere is proposed to analyse the
polarisation of the cosmic microwave background (CMB), a spin $\pm 2$ signal
observed on the celestial sphere. The scalar directional scale-discretised
wavelet transform on the sphere is extended to analyse signals of arbitrary
spin. The resulting spin scale-discretised wavelet transform probes the
directional intensity of spin signals. A procedure is presented using this new
spin wavelet transform to recover E- and B-mode signals from partial-sky
observations of CMB polarisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1342</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1342</id><created>2014-12-03</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>A perspective on the advancement of natural language processing tasks
  via topological analysis of complex networks</title><categories>cs.CL</categories><journal-ref>Physics of Life Reviews, v. 11, p. 641-643, 2014</journal-ref><doi>10.1016/j.plrev.2014.07.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comment on &quot;Approaching human language with complex networks&quot; by Cong and Liu
(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1353</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1353</id><created>2014-12-03</created><authors><author><keyname>Pentina</keyname><forenames>Anastasia</forenames></author><author><keyname>Sharmanska</keyname><forenames>Viktoriia</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Curriculum Learning of Multiple Tasks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing information between multiple tasks enables algorithms to achieve good
generalization performance even from small amounts of training data. However,
in a realistic scenario of multi-task learning not all tasks are equally
related to each other, hence it could be advantageous to transfer information
only between the most related tasks. In this work we propose an approach that
processes multiple tasks in a sequence with sharing between subsequent tasks
instead of solving all tasks jointly. Subsequently, we address the question of
curriculum learning of tasks, i.e. finding the best order of tasks to be
learned. Our approach is based on a generalization bound criterion for choosing
the task order that optimizes the average expected classification performance
over all tasks. Our experimental results show that learning multiple related
tasks sequentially can be more effective than learning them jointly, the order
in which tasks are being solved affects the overall performance, and that our
model is able to automatically discover the favourable order of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1372</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1372</id><created>2014-12-03</created><authors><author><keyname>Syrj&#xe4;l&#xe4;</keyname><forenames>Ville</forenames></author><author><keyname>Yamamoto</keyname><forenames>Koji</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Analysis and Design Specifications for Full-Duplex Radio Transceivers
  under RF Oscillator Phase-Noise with Arbitrary Spectral Shape</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the effects of oscillator phase-noise with arbitrary spectral
characteristics on self-interference cancellation capability of a full-duplex
radio transceiver are addressed, and design considerations are given for
oscillator designers for optimized PLL design for full-duplex radio
application. The paper first gives a full-duplex transceiver model that
inherently mitigates most of the phase-noise effect from the self-interference
signal. The remaining effect of the phase noise is then analysed. Closed-form
solutions for the self-interference power are then derived. In the simulations
part, a practical phase-locked loop type oscillator is used, which is based on
the arbitrary mask phase-noise model. Analytical derivations are verified with
the simulations, and the self-interference cancellation performance is
thoroughly studied with various parameters. Design considerations are finally
given for oscillator design for full-duplex radio transceivers, with the help
of tangible parameters of the phase-locked loop type oscillators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1393</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1393</id><created>2014-12-03</created><authors><author><keyname>Antoniotti</keyname><forenames>Marco</forenames></author></authors><title>CLAZY: Lazy Calling for Common Lisp</title><categories>cs.PL</categories><comments>A version of this note was presented at the 1st European Lisp
  Symposium 2008, Bordeaux, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document contains a description of a Common Lisp extension that allows a
programmer to write functional programs that use &quot;normal order&quot; evaluation, as
in &quot;non-strict&quot; languages like Haskell. The extension is relatively
straightforward, and it appears to be the first one such that is integrated in
the overall Common Lisp framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1395</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1395</id><created>2014-12-03</created><updated>2015-11-11</updated><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Gringoli</keyname><forenames>Francesco</forenames></author></authors><title>A High Efficiency MAC Protocol for WLANs: Providing Fairness in Dense
  Scenarios</title><categories>cs.NI</categories><comments>This work has been submitted to the IEEE/ACM Transactions on
  Networking journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Collisions are a main cause of throughput degradation in WLANs. The current
contention mechanism used in IEEE 802.11 networks is called Carrier Sense
Multiple Access with Collision Avoidance (CSMA/CA). It uses a Binary
Exponential Backoff (BEB) technique to randomise each contender attempt of
transmitting, effectively reducing the collision probability. Nevertheless,
CSMA/CA relies on a random backoff that while effective and fully
decentralised, in principle is unable to completely eliminate collisions,
therefore degrading the network throughput as more contenders attempt to share
the channel.
  To overcome these situations, Carrier Sense Multiple Access with Enhanced
Collision Avoidance (CSMA/ECA) is able to create a collision-free schedule in a
fully decentralised manner using a deterministic backoff after successful
transmissions. Hysteresis and Fair Share are two extensions of CSMA/ECA to
support a large number of contenders in a collision-free schedule. CSMA/ECA
offers better throughput than CSMA/CA and short-term throughput fairness. This
work describes CSMA/ECA and its extensions. Additionally, it provides the first
evaluation results of CSMA/ECA with non-saturated traffic, channel errors, and
its performance when coexisting with CSMA/CA nodes. Furthermore, it describes
the effects of imperfect clocks over CSMA/ECA and present a mechanism to
leverage the impact of channel errors and the addition/withdrawal of nodes over
collision-free schedules. Finally, experimental results on throughput and lost
frames from a CSMA/ECA implementation using commercial hardware and open-source
firmware are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1398</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1398</id><created>2014-12-03</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kumar</keyname><forenames>Nirman</forenames></author><author><keyname>Mount</keyname><forenames>David M.</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>Space Exploration via Proximity Search</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate what computational tasks can be performed on a point set in
$\Re^d$, if we are only given black-box access to it via nearest-neighbor
search. This is a reasonable assumption if the underlying point set is either
provided implicitly, or it is stored in a data structure that can answer such
queries. In particular, we show the following: (A) One can compute an
approximate bi-criteria $k$-center clustering of the point set, and more
generally compute a greedy permutation of the point set. (B) One can decide if
a query point is (approximately) inside the convex-hull of the point set.
  We also investigate the problem of clustering the given point set, such that
meaningful proximity queries can be carried out on the centers of the clusters,
instead of the whole point set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1401</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1401</id><created>2014-10-28</created><authors><author><keyname>Jun</keyname><forenames>Kyung-Taek</forenames></author></authors><title>Throat Finding Algorithms based on Throat Types</title><categories>cs.CG cs.GR</categories><comments>23 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three-dimensional geometry and connectivity of pore space determines the
flow of single-phase incompressible flow. Herein I report on new throat finding
algorithms that contribute to finding the exact flow-relevant geometrical
properties of the void space, including high porosity samples of X2B images,
three-dimensional synchrotron X-ray computed microtomographic images, and
amounting to over 20% porosity. These new algorithms use the modified medial
axis that comes from the 3DMA-Rock software package. To find accurate throats,
we classify three major throat types: mostly planar and simply connected type,
non-planar and simply connected type, and non-planar and non-simply connected
type. For each type, we make at least one algorithm to find the throats. Here I
introduce an example that has a non-planar and simply connected throat, and my
solution indicated by one of my algorithms. My five algorithms each calculate
the throat for each path. It selects one of them, which has the smallest inner
area. New algorithms find accurate throats at least 98% among 12 high porosity
samples (over 20%). Also, I introduce a new length calculation in the digitized
image. The new calculation uses three mathematical concepts: i)
differentiability, ii) implicit function theorem, iii) line integral. The
result can convert the discrete boundary of the XMCT image to the real
boundary. When the real boundary has an arc shape, the new calculation has less
than 1% relative error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1402</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1402</id><created>2014-12-02</created><authors><author><keyname>Hahanov</keyname><forenames>Vladimir</forenames></author><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Chumachenko</keyname><forenames>Svetlana</forenames></author><author><keyname>Litvinova</keyname><forenames>Eugenia</forenames></author></authors><title>Qubit Data Structures for Analyzing Computing Systems</title><categories>cs.OH</categories><comments>9 pages,4 figures, Proceeding of the Third International Conference
  on Data Mining &amp; Knowledge Management Process (CDKP 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qubit models and methods for improving the performance of software and
hardware for analyzing digital devices through increasing the dimension of the
data structures and memory are proposed. The basic concepts, terminology and
definitions necessary for the implementation of quantum computing when
analyzing virtual computers are introduced. The investigation results
concerning design and modeling computer systems in a cyberspace based on the
use of two-component structure &lt;memory - transactions&gt; are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1419</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1419</id><created>2014-12-03</created><authors><author><keyname>Allen</keyname><forenames>John</forenames></author><author><keyname>Scott</keyname><forenames>David</forenames></author><author><keyname>Illingworth</keyname><forenames>Malcolm</forenames></author><author><keyname>Dobrzelecki</keyname><forenames>Bartek</forenames></author><author><keyname>Virdee</keyname><forenames>Davy</forenames></author><author><keyname>Thorn</keyname><forenames>Steve</forenames></author><author><keyname>Knott</keyname><forenames>Sara</forenames></author></authors><title>CloudQTL: Evolving a Bioinformatics Application to the Cloud</title><categories>cs.DC</categories><comments>12 pages, 3 figures, EGI conference Madrid 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A timeline is presented which shows the stages involved in converting a
bioinformatics software application from a set of standalone algorithms through
to a simple web based tool then to a web based portal harnessing Grid
technologies and on to its latest inception as a Cloud based bioinformatics web
tool. The nature of the software is discussed together with a description of
its development at various stages including a detailed account of the Cloud
service. An outline of user results is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1424</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1424</id><created>2014-12-03</created><authors><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Cosley</keyname><forenames>Dan</forenames></author></authors><title>Studying and Modeling the Connection between People's Preferences and
  Content Sharing</title><categories>cs.HC cs.SI</categories><comments>CSCW 2015</comments><acm-class>H.1.2; H.3.3</acm-class><doi>10.1145/2675133.2675151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People regularly share items using online social media. However, people's
decisions around sharing---who shares what to whom and why---are not well
understood. We present a user study involving 87 pairs of Facebook users to
understand how people make their sharing decisions. We find that even when
sharing to a specific individual, people's own preference for an item
(individuation) dominates over the recipient's preferences (altruism). People's
open-ended responses about how they share, however, indicate that they do try
to personalize shares based on the recipient. To explain these contrasting
results, we propose a novel process model of sharing that takes into account
people's preferences and the salience of an item. We also present encouraging
results for a sharing prediction model that incorporates both the senders' and
the recipients' preferences. These results suggest improvements to both
algorithms that support sharing in social media and to information diffusion
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1441</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1441</id><created>2014-12-03</created><updated>2015-12-08</updated><authors><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Reed</keyname><forenames>Scott</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Ioffe</keyname><forenames>Sergey</forenames></author></authors><title>Scalable, High-Quality Object Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current high-quality object detection approaches use the scheme of
salience-based object proposal methods followed by post-classification using
deep convolutional features. This spurred recent research in improving object
proposal methods. However, domain agnostic proposal generation has the
principal drawback that the proposals come unranked or with very weak ranking,
making it hard to trade-off quality for running time. This raises the more
fundamental question of whether high-quality proposal generation requires
careful engineering or can be derived just from data alone. We demonstrate that
learning-based proposal methods can effectively match the performance of
hand-engineered methods while allowing for very efficient runtime-quality
trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox)
approach, we substantially advance the state-of-the-art on the ILSVRC 2014
detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAP
for an ensemble of two models. MSC-Multibox significantly improves the proposal
quality over its predecessor MultiBox~method: AP increases from $0.42$ to
$0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improved
bounding-box recall compared to Multiscale Combinatorial Grouping with less
proposals on the Microsoft-COCO data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1442</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1442</id><created>2014-12-03</created><authors><author><keyname>Collins</keyname><forenames>Maxwell D.</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>Memory Bounded Deep Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the use of sparsity-inducing regularizers during
training of Convolution Neural Networks (CNNs). These regularizers encourage
that fewer connections in the convolution and fully connected layers take
non-zero values and in effect result in sparse connectivity between hidden
units in the deep network. This in turn reduces the memory and runtime cost
involved in deploying the learned CNNs. We show that training with such
regularization can still be performed using stochastic gradient descent
implying that it can be used easily in existing codebases. Experimental
evaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows that
our regularizers can result in dramatic reductions in memory requirements. For
instance, when applied on AlexNet, our method can reduce the memory consumption
by a factor of four with minimal loss in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1443</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1443</id><created>2014-12-03</created><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Structure learning of antiferromagnetic Ising models</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>15 pages. NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the computational complexity of learning the
graph structure underlying a discrete undirected graphical model from i.i.d.
samples. We first observe that the notoriously difficult problem of learning
parities with noise can be captured as a special case of learning graphical
models. This leads to an unconditional computational lower bound of $\Omega
(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree
$d$, for the class of so-called statistical algorithms recently introduced by
Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime
required to exhaustively search over neighborhoods cannot be significantly
improved without restricting the class of models.
  Aside from structural assumptions on the graph such as it being a tree,
hypertree, tree-like, etc., many recent papers on structure learning assume
that the model has the correlation decay property. Indeed, focusing on
ferromagnetic Ising models, Bento and Montanari (2009) showed that all known
low-complexity algorithms fail to learn simple graphs when the interaction
strength exceeds a number related to the correlation decay threshold. Our
second set of results gives a class of repelling (antiferromagnetic) models
that have the opposite behavior: very strong interaction allows efficient
learning in time $O(p^2)$. We provide an algorithm whose performance
interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the
repulsion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1454</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1454</id><created>2014-12-03</created><updated>2015-06-26</updated><authors><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author><author><keyname>Pelemans</keyname><forenames>Joris</forenames></author><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author></authors><title>Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability
  Estimation</title><categories>cs.LG cs.CL</categories><report-no>Google Research Publication Id: 43222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel family of language model (LM) estimation techniques named
Sparse Non-negative Matrix (SNM) estimation. A first set of experiments
empirically evaluating it on the One Billion Word Benchmark shows that SNM
$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)
models. When using skip-gram features the models are able to match the
state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling
techniques yields the best known result on the benchmark. The computational
advantages of SNM over both maximum entropy and RNN LM estimation are probably
its main strength, promising an approach that has the same flexibility in
combining arbitrary features effectively and yet should scale to very large
amounts of data as gracefully as $n$-gram LMs do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1455</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1455</id><created>2014-12-03</created><updated>2015-05-12</updated><authors><author><keyname>Ben-Artzi</keyname><forenames>Gil</forenames></author><author><keyname>Werman</keyname><forenames>Michael</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author></authors><title>Event Retrieval Using Motion Barcodes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple and effective method for retrieval of videos showing a
specific event, even when the videos of that event were captured from
significantly different viewpoints. Appearance-based methods fail in such
cases, as appearances change with large changes of viewpoints.
  Our method is based on a pixel-based feature, &quot;motion barcode&quot;, which records
the existence/non-existence of motion as a function of time. While appearance,
motion magnitude, and motion direction can vary greatly between disparate
viewpoints, the existence of motion is viewpoint invariant. Based on the motion
barcode, a similarity measure is developed for videos of the same event taken
from very different viewpoints. This measure is robust to occlusions common
under different viewpoints, and can be computed efficiently.
  Event retrieval is demonstrated using challenging videos from stationary and
hand held cameras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1462</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1462</id><created>2014-12-03</created><updated>2015-08-24</updated><authors><author><keyname>Aslay</keyname><forenames>Cigdem</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Goyal</keyname><forenames>Amit</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>Viral Marketing Meets Social Advertising: Ad Allocation with Minimum
  Regret</title><categories>cs.SI</categories><comments>RRC-sets generation process in Section 5.2 is enhanced and made more
  explicit</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of allocating ads to users through the
viral-marketing lens. Advertisers approach the host with a budget in return for
the marketing campaign service provided by the host. We show that allocation
that takes into account the propensity of ads for viral propagation can achieve
significantly better performance. However, uncontrolled virality could be
undesirable for the host as it creates room for exploitation by the
advertisers: hoping to tap uncontrolled virality, an advertiser might declare a
lower budget for its marketing campaign, aiming at the same large outcome with
a smaller cost.
  This creates a challenging trade-off: on the one hand, the host aims at
leveraging virality and the network effect to improve advertising efficacy,
while on the other hand the host wants to avoid giving away free service due to
uncontrolled virality. We formalize this as the problem of ad allocation with
minimum regret, which we show is NP-hard and inapproximable w.r.t. any factor.
However, we devise an algorithm that provides approximation guarantees w.r.t.
the total budget of all advertisers. We develop a scalable version of our
approximation algorithm, which we extensively test on four real-world data
sets, confirming that our algorithm delivers high quality solutions, is
scalable, and significantly outperforms several natural baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1463</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1463</id><created>2014-12-03</created><updated>2014-12-03</updated><authors><author><keyname>Gigu&#xe8;re</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Rolland</keyname><forenames>Am&#xe9;lie</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author></authors><title>On the String Kernel Pre-Image Problem with Applications in Drug
  Discovery</title><categories>cs.LG cs.CE</categories><comments>Peer-reviewed and accepted for presentation at Machine Learning in
  Computational Biology 2014, Montr\'eal, Qu\'ebec, Canada</comments><acm-class>I.2.6; K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pre-image problem has to be solved during inference by most structured
output predictors. For string kernels, this problem corresponds to finding the
string associated to a given input. An algorithm capable of solving or finding
good approximations to this problem would have many applications in
computational biology and other fields. This work uses a recent result on
combinatorial optimization of linear predictors based on string kernels to
develop, for the pre-image, a low complexity upper bound valid for many string
kernels. This upper bound is used with success in a branch and bound searching
algorithm. Applications and results in the discovery of druggable peptides are
presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1468</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1468</id><created>2014-12-03</created><updated>2015-06-18</updated><authors><author><keyname>Yu</keyname><forenames>Chung-Kai</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Information-Sharing over Adaptive Networks with Self-interested Agents</title><categories>cs.MA</categories><comments>19 pages, 8 figures. To appear in IEEE Transactions on Signal and
  Information Processing over Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the behavior of multi-agent networks where information-sharing is
subject to a positive communications cost over the edges linking the agents. We
consider a general mean-square-error formulation where all agents are
interested in estimating the same target vector. We first show that, in the
absence of any incentives to cooperate, the optimal strategy for the agents is
to behave in a selfish manner with each agent seeking the optimal solution
independently of the other agents. Pareto inefficiency arises as a result of
the fact that agents are not using historical data to predict the behavior of
their neighbors and to know whether they will reciprocate and participate in
sharing information. Motivated by this observation, we develop a reputation
protocol to summarize the opponent's past actions into a reputation score,
which can then be used to form a belief about the opponent's subsequent
actions. The reputation protocol entices agents to cooperate and turns their
optimal strategy into an action-choosing strategy that enhances the overall
social benefit of the network. In particular, we show that when the
communications cost becomes large, the expected social benefit of the proposed
protocol outperforms the social benefit that is obtained by cooperative agents
that always share data. We perform a detailed mean-square-error analysis of the
evolution of the network over three domains: far field, near-field, and
middle-field, and show that the network behavior is stable for sufficiently
small step-sizes. The various theoretical results are illustrated by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1470</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1470</id><created>2014-12-03</created><updated>2015-09-28</updated><authors><author><keyname>Chehreghani</keyname><forenames>Mostafa Haghir</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Mining Rooted Ordered Trees under Subtree Homeomorphism</title><categories>cs.DB cs.DS</categories><comments>This paper is accepted in the Data Mining and Knowledge Discovery
  journal
  (http://www.springer.com/computer/database+management+%26+information+retrieval/journal/10618)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining frequent tree patterns has many applications in different areas such
as XML data, bioinformatics and World Wide Web. The crucial step in frequent
pattern mining is frequency counting, which involves a matching operator to
find occurrences (instances) of a tree pattern in a given collection of trees.
A widely used matching operator for tree-structured data is subtree
homeomorphism, where an edge in the tree pattern is mapped onto an
ancestor-descendant relationship in the given tree. Tree patterns that are
frequent under subtree homeomorphism are usually called embedded patterns. In
this paper, we present an efficient algorithm for subtree homeomorphism with
application to frequent pattern mining. We propose a compact data-structure,
called occ, which stores only information about the rightmost paths of
occurrences and hence can encode and represent several occurrences of a tree
pattern. We then define efficient join operations on the occ data-structure,
which help us count occurrences of tree patterns according to occurrences of
their proper subtrees. Based on the proposed subtree homeomorphism method, we
develop an effective pattern mining algorithm, called TPMiner. We evaluate the
efficiency of TPMiner on several real-world and synthetic datasets. Our
extensive experiments confirm that TPMiner always outperforms well-known
existing algorithms, and in several cases the improvement with respect to
existing algorithms is significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1505</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1505</id><created>2014-12-03</created><updated>2015-06-01</updated><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Gribkoff</keyname><forenames>Eric</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Symmetric Weighted First-Order Model Counting</title><categories>cs.DB cs.AI cs.CC cs.LO</categories><comments>To appear at PODS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FO Model Counting problem (FOMC) is the following: given a sentence
$\Phi$ in FO and a number $n$, compute the number of models of $\Phi$ over a
domain of size $n$; the Weighted variant (WFOMC) generalizes the problem by
associating a weight to each tuple and defining the weight of a model to be the
product of weights of its tuples. In this paper we study the complexity of the
symmetric WFOMC, where all tuples of a given relation have the same weight. Our
motivation comes from an important application, inference in Knowledge Bases
with soft constraints, like Markov Logic Networks, but the problem is also of
independent theoretical interest. We study both the data complexity, and the
combined complexity of FOMC and WFOMC. For the data complexity we prove the
existence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the
existence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also
prove that all $\gamma$-acyclic queries have polynomial time data complexity.
For the combined complexity, we prove that, for every fragment FO$^{k}$, $k\geq
2$, the combined complexity of FOMC (or WFOMC) is #P-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1506</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1506</id><created>2014-12-03</created><authors><author><keyname>Djaroudib</keyname><forenames>Khamsa</forenames></author><author><keyname>Ahmed</keyname><forenames>Abdelmalik Taleb</forenames></author><author><keyname>Zidani</keyname><forenames>Abdelmadjid</forenames></author></authors><title>Textural Approach for Mass Abnormality Segmentation in Mammographic
  Images</title><categories>cs.CV</categories><comments>07 pages, 11 figures, 1 tableau, 07 equations, 34 references. appears
  in IJCSI International Journal of Computer Science Issues november 2013</comments><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mass abnormality segmentation is a vital step for the medical diagnostic
process and is attracting more and more the interest of many research groups.
Currently, most of the works achieved in this area have used the Gray Level
Co-occurrence Matrix (GLCM) as texture features with a region-based approach.
These features come in previous phase for segmentation stage or are using as
inputs to classification stage. The work discussed in this paper attempts to
experiment the GLCM method under a contour-based approach. Besides, we
experiment the proposed approach on various tissues densities to bring more
significant results. At this end, we explored some challenging breast images
from BIRADS medical Data Base. Our first experimentations showed promising
results with regard to the edges mass segmentation methods. This paper
discusses first the main works achieved in this area. Sections 2 and 3 present
materials and our methodology. The main results are showed and evaluated before
concluding our paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1520</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1520</id><created>2014-12-03</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Lim</keyname><forenames>Fabian</forenames></author></authors><title>The Single-Uniprior Index-Coding Problem: The Single-Sender Case and The
  Multi-Sender Extension</title><categories>cs.IT math.IT</categories><comments>Submitted for journal publication. Parts of the material in this
  paper was presented at the IEEE International Symposium on Information
  Theory, Istanbul, Turkey, July 7-12, 2013, and at the IEEE International
  Conference on Communications, Ottawa, Canada, June 10-15, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Index coding studies multiterminal source-coding problems where a set of
receivers are required to decode multiple (different) messages from a common
broadcast, and they each know some messages a priori. In this paper, we
consider a class of index-coding problems, which we term single uniprior, where
at the receiver end, each receiver knows one of the messages a priori. At the
broadcasting end, we consider a generalized setting where they are multiple
senders, each sender only knows a subset of messages, and all senders are
required to collectively transmit the index code. The aim is to find the
minimum number of total coded bits the senders need to transmit. For the
single-sender setup, we propose a pruning algorithm to find the optimal (i.e.,
minimum) index codelength, and establish that linear index codes are optimal.
For the multi-sender setup, the pruning technique is used in conjunction with a
proposed appending technique to give a lower bound to the optimal index
codelength. An upper bound is derived based on cyclic codes. While the two
bounds do not match in general, for the special case where no two senders know
any message bit in common, the bounds match, giving the optimal index
codelength. The results are derived based on graph theory, and are expressed in
terms of strongly connected components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1523</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1523</id><created>2014-12-03</created><updated>2015-12-06</updated><authors><author><keyname>Ying</keyname><forenames>Bicheng</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Information Exchange and Learning Dynamics over Weakly-Connected
  Adaptive Networks</title><categories>cs.MA cs.IT cs.LG math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper examines the learning mechanism of adaptive agents over
weakly-connected graphs and reveals an interesting behavior on how information
flows through such topologies. The results clarify how asymmetries in the
exchange of data can mask local information at certain agents and make them
totally dependent on other agents. A leader-follower relationship develops with
the performance of some agents being fully determined by the performance of
other agents that are outside their domain of influence. This scenario can
arise, for example, due to intruder attacks by malicious agents or as the
result of failures by some critical links. The findings in this work help
explain why strong-connectivity of the network topology, adaptation of the
combination weights, and clustering of agents are important ingredients to
equalize the learning abilities of all agents against such disturbances. The
results also clarify how weak-connectivity can be helpful in reducing the
effect of outlier data on learning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1526</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1526</id><created>2014-12-03</created><updated>2015-11-24</updated><authors><author><keyname>Chen</keyname><forenames>Xianjie</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Parsing Occluded People by Flexible Compositions</title><categories>cs.CV</categories><comments>CVPR 15 Camera Ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to parsing humans when there is significant
occlusion. We model humans using a graphical model which has a tree structure
building on recent work [32, 6] and exploit the connectivity prior that, even
in presence of occlusion, the visible nodes form a connected subtree of the
graphical model. We call each connected subtree a flexible composition of
object parts. This involves a novel method for learning occlusion cues. During
inference we need to search over a mixture of different flexible models. By
exploiting part sharing, we show that this inference can be done extremely
efficiently requiring only twice as many computations as searching for the
entire object (i.e., not modeling occlusion). We evaluate our model on the
standard benchmarked &quot;We Are Family&quot; Stickmen dataset and obtain significant
performance improvements over the best alternative algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1538</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1538</id><created>2014-12-03</created><authors><author><keyname>Aldroubi</keyname><forenames>Akram</forenames></author><author><keyname>Krishtal</keyname><forenames>Ilya</forenames></author></authors><title>Krylov Subspace Methods in Dynamical Sampling</title><categories>cs.IT math.IT</categories><comments>12 pages, 2 figures</comments><msc-class>94A20, 94A12, 42C15, 15A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $B$ be an unknown linear evolution process on $\mathbb C^d\simeq
l^2(\mathbb Z_d)$ driving an unknown initial state $x$ and producing the states
$\{B^\ell x, \ell = 0,1,\ldots\}$ at different time levels. The problem under
consideration in this paper is to find as much information as possible about
$B$ and $x$ from the measurements $Y=\{x(i)$, $Bx(i)$, $\dots$,
$B^{\ell_i}x(i): i \in \Omega\subset \mathbb Z^d\}$. If $B$ is a &quot;low-pass&quot;
convolution operator, we show that we can recover both $B$ and $x$, almost
surely, as long as we double the amount of temporal samples needed in
\cite{ADK13} to recover the signal propagated by a known operator $B$. For a
general operator $B$, we can recover parts or even all of its spectrum from
$Y$. As a special case of our method, we derive the centuries old Prony's
method \cite{BDVMC08, P795, PP13} which recovers a vector with an $s$-sparse
Fourier transform from $2s$ of its consecutive components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1543</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1543</id><created>2014-12-03</created><authors><author><keyname>Giannopoulou</keyname><forenames>Archontia C.</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author></authors><title>New Geometric Representations and Domination Problems on Tolerance and
  Multitolerance Graphs</title><categories>cs.CC math.CO</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tolerance graphs model interval relations in such a way that intervals can
tolerate a certain amount of overlap without being in conflict. In one of the
most natural generalizations of tolerance graphs with direct applications in
the comparison of DNA sequences from different organisms, namely
\emph{multitolerance} graphs, two tolerances are allowed for each interval -
one from the left and one from the right side. Several efficient algorithms for
optimization problems that are NP-hard in general graphs have been designed for
tolerance and multitolerance graphs. In spite of this progress, the complexity
status of some fundamental algorithmic problems on tolerance and multitolerance
graphs, such as the \emph{dominating set} problem, remained unresolved until
now, three decades after the introduction of tolerance graphs. In this article
we introduce two new geometric representations for tolerance and multitolerance
graphs, given by points and line segments in the plane. Apart from being
important on their own, these new representations prove to be a powerful tool
for deriving both hardness results and polynomial time algorithms. Using them,
we surprisingly prove that the dominating set problem can be solved in
polynomial time on tolerance graphs and that it is APX-hard on multitolerance
graphs, solving thus a longstanding open problem. This problem is the first one
that has been discovered with a different complexity status in these two graph
classes. Furthermore we present an algorithm that solves the independent
dominating set problem on multitolerance graphs in polynomial time, thus
demonstrating the potential of this new representation for further exploitation
via sweep line algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1547</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1547</id><created>2014-12-03</created><authors><author><keyname>Bagchi</keyname><forenames>Bhaskar</forenames></author><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Datta</keyname><forenames>Basudeb</forenames></author><author><keyname>Singh</keyname><forenames>Nitin</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Efficient algorithms to decide tightness</title><categories>cs.CG math.CO</categories><comments>18 pages, 3 figures</comments><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tightness is a generalisation of the notion of convexity: a space is tight if
and only if it is &quot;as convex as possible&quot;, given its topological constraints.
For a simplicial complex, deciding tightness has a straightforward exponential
time algorithm, but efficient methods to decide tightness are only known in the
trivial setting of triangulated surfaces.
  In this article, we present a new polynomial time procedure to decide
tightness for triangulations of $3$-manifolds -- a problem which previously was
thought to be hard. Furthermore, we describe an algorithm to decide general
tightness in the case of $4$-dimensional combinatorial manifolds which is fixed
parameter tractable in the treewidth of the $1$-skeletons of their vertex
links, and we present an algorithm to decide $\mathbb{F}_2$-tightness for weak
pseudomanifolds $M$ of arbitrary but fixed dimension which is fixed parameter
tractable in the treewidth of the dual graph of $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1565</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1565</id><created>2014-12-04</created><authors><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author></authors><title>Recovery Analysis for Weighted $\ell_1$-Minimization Using a Null Space
  Property</title><categories>cs.IT math.IT</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the recovery of sparse signals from underdetermined linear
measurements when a potentially erroneous support estimate is available. Our
results are twofold. First, we derive necessary and sufficient conditions for
signal recovery from compressively sampled measurements using weighted
$\ell_1$-norm minimization. These conditions, which depend on the choice of
weights as well as the size and accuracy of the support estimate, are on the
null space of the measurement matrix. They can guarantee recovery even when
standard $\ell_1$ minimization fails. Second, we derive bounds on the number of
Gaussian measurements for these conditions to be satisfied, i.e., for weighted
$\ell_1$ minimization to successfully recover all sparse signals whose support
has been estimated sufficiently accurately. Our bounds show that weighted
$\ell_1$ minimization requires significantly fewer measurements than standard
$\ell_1$ minimization when the support estimate is relatively accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1574</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1574</id><created>2014-12-04</created><authors><author><keyname>Zhao</keyname><forenames>Liming</forenames></author><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Xiao</keyname><forenames>Jun</forenames></author><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Zhuang</keyname><forenames>Yueting</forenames></author></authors><title>Metric Learning Driven Multi-Task Structured Output Optimization for
  Robust Keypoint Tracking</title><categories>cs.CV cs.LG</categories><comments>Accepted by AAAI-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important and challenging problem in computer vision and graphics,
keypoint-based object tracking is typically formulated in a spatio-temporal
statistical learning framework. However, most existing keypoint trackers are
incapable of effectively modeling and balancing the following three aspects in
a simultaneous manner: temporal model coherence across frames, spatial model
consistency within frames, and discriminative feature construction. To address
this issue, we propose a robust keypoint tracker based on spatio-temporal
multi-task structured output optimization driven by discriminative metric
learning. Consequently, temporal model coherence is characterized by multi-task
structured keypoint model learning over several adjacent frames, while spatial
model consistency is modeled by solving a geometric verification based
structured learning problem. Discriminative feature construction is enabled by
metric learning to ensure the intra-class compactness and inter-class
separability. Finally, the above three modules are simultaneously optimized in
a joint learning scheme. Experimental results have demonstrated the
effectiveness of our tracker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1576</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1576</id><created>2014-12-04</created><authors><author><keyname>Yuan</keyname><forenames>Jinhui</forenames></author><author><keyname>Gao</keyname><forenames>Fei</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Wei</keyname><forenames>Jinliang</forenames></author><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author><author><keyname>Ma</keyname><forenames>Wei-Ying</forenames></author></authors><title>LightLDA: Big Topic Models on Modest Compute Clusters</title><categories>stat.ML cs.DC cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When building large-scale machine learning (ML) programs, such as big topic
models or deep neural nets, one usually assumes such tasks can only be
attempted with industrial-sized clusters with thousands of nodes, which are out
of reach for most practitioners or academic researchers. We consider this
challenge in the context of topic modeling on web-scale corpora, and show that
with a modest cluster of as few as 8 machines, we can train a topic model with
1 million topics and a 1-million-word vocabulary (for a total of 1 trillion
parameters), on a document collection with 200 billion tokens -- a scale not
yet reported even with thousands of machines. Our major contributions include:
1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whose
running cost is (surprisingly) agnostic of model size, and empirically
converges nearly an order of magnitude faster than current state-of-the-art
Gibbs samplers; 2) a structure-aware model-parallel scheme, which leverages
dependencies within the topic model, yielding a sampling strategy that is
frugal on machine memory and network communication; 3) a differential
data-structure for model storage, which uses separate data structures for high-
and low-frequency words to allow extremely large models to fit in memory, while
maintaining high inference speed; and 4) a bounded asynchronous data-parallel
scheme, which allows efficient distributed processing of massive data via a
parameter server. Our distribution strategy is an instance of the
model-and-data-parallel programming model underlying the Petuum framework for
general distributed ML, and was implemented on top of the Petuum open-source
system. We provide experimental evidence showing how this development puts
massive models within reach on a small cluster while still enjoying
proportional time cost reductions with increasing cluster size, in comparison
with alternative options.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1587</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1587</id><created>2014-12-04</created><updated>2015-04-11</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author></authors><title>The entropic barrier: a simple and optimal universal self-concordant
  barrier</title><categories>math.OC cs.IT cs.LG math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the Cram\'er transform of the uniform measure on a convex body
in $\mathbb{R}^n$ is a $(1+o(1)) n$-self-concordant barrier, improving a
seminal result of Nesterov and Nemirovski. This gives the first explicit
construction of a universal barrier for convex bodies with optimal
self-concordance parameter. The proof is based on basic geometry of log-concave
distributions, and elementary duality in exponential families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1591</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1591</id><created>2014-12-04</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Searching and Indexing Genomic Databases via Kernelization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid advance of DNA sequencing technologies has yielded databases of
thousands of genomes. To search and index these databases effectively, it is
important that we take advantage of the similarity between those genomes.
Several authors have recently suggested searching or indexing only one
reference genome and the parts of the other genomes where they differ. In this
paper we survey the twenty-year history of this idea and discuss its relation
to kernelization in parameterized complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1602</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1602</id><created>2014-12-04</created><authors><author><keyname>Chorowski</keyname><forenames>Jan</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>End-to-end Continuous Speech Recognition using Attention-based Recurrent
  NN: First Results</title><categories>cs.NE cs.LG stat.ML</categories><comments>As accepted to: Deep Learning and Representation Learning Workshop,
  NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We replace the Hidden Markov Model (HMM) which is traditionally used in in
continuous speech recognition with a bi-directional recurrent neural network
encoder coupled to a recurrent neural network decoder that directly emits a
stream of phonemes. The alignment between the input and output sequences is
established using an attention mechanism: the decoder emits each symbol based
on a context created with a subset of input symbols elected by the attention
mechanism. We report initial results demonstrating that this new approach
achieves phoneme error rates that are comparable to the state-of-the-art
HMM-based decoders, on the TIMIT dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1619</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1619</id><created>2014-12-04</created><updated>2015-07-17</updated><authors><author><keyname>Kuzborskij</keyname><forenames>Ilja</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>Fast Rates by Transferring from Auxiliary Hypotheses</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the learning setting where, in addition to the
training set, the learner receives a collection of auxiliary hypotheses
originating from other tasks. We focus on a broad class of ERM-based linear
algorithms that can be instantiated with any non-negative smooth loss function
and any strongly convex regularizer. We establish generalization and excess
risk bounds, showing that, if the algorithm is fed with a good combination of
source hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$
instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if the
source hypotheses combination is a misfit for the target task, we recover the
usual learning rate. As a byproduct of our study, we also prove a new bound on
the Rademacher complexity of the smooth loss class under weaker assumptions
compared to previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1623</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1623</id><created>2014-12-04</created><authors><author><keyname>Mainka</keyname><forenames>Christian</forenames></author><author><keyname>Mladenov</keyname><forenames>Vladislav</forenames></author><author><keyname>Schwenk</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Do not trust me: Using malicious IdPs for analyzing and attacking Single
  Sign-On</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single Sign-On (SSO) systems simplify login procedures by using an an
Identity Provider (IdP) to issue authentication tokens which can be consumed by
Service Providers (SPs). Traditionally, IdPs are modeled as trusted third
parties. This is reasonable for SSO systems like Kerberos, MS Passport and
SAML, where each SP explicitely specifies which IdP he trusts. However, in open
systems like OpenID and OpenID Connect, each user may set up his own IdP, and a
discovery phase is added to the protocol flow. Thus it is easy for an attacker
to set up its own IdP. In this paper we use a novel approach for analyzing SSO
authentication schemes by introducing a malicious IdP. With this approach we
evaluate one of the most popular and widely deployed SSO protocols - OpenID. We
found four novel attack classes on OpenID, which were not covered by previous
research, and show their applicability to real-life implementations. As a
result, we were able to compromise 11 out of 16 existing OpenID implementations
like Sourceforge, Drupal and ownCloud. We automated discovery of these attacks
in a open source tool OpenID Attacker, which additionally allows fine-granular
testing of all parameters in OpenID implementations. Our research helps to
better understand the message flow in the OpenID protocol, trust assumptions in
the different components of the system, and implementation issues in OpenID
components. It is applicable to other SSO systems like OpenID Connect and SAML.
All OpenID implementations have been informed about their vulnerabilities and
we supported them in fixing the issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1628</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1628</id><created>2014-12-04</created><updated>2014-12-19</updated><authors><author><keyname>Yoo</keyname><forenames>Donggeun</forenames></author><author><keyname>Park</keyname><forenames>Sunggyun</forenames></author><author><keyname>Lee</keyname><forenames>Joon-Young</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Fisher Kernel for Deep Neural Activations</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to image representation based on low-level local descriptors, deep
neural activations of Convolutional Neural Networks (CNNs) are richer in
mid-level representation, but poorer in geometric invariance properties. In
this paper, we present a straightforward framework for better image
representation by combining the two approaches. To take advantages of both
representations, we propose an efficient method to extract a fair amount of
multi-scale dense local activations from a pre-trained CNN. We then aggregate
the activations by Fisher kernel framework, which has been modified with a
simple scale-wise normalization essential to make it suitable for CNN
activations. Replacing the direct use of a single activation vector with our
representation demonstrates significant performance improvements: +17.76 (Acc.)
on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that
our proposal can be used as a primary image representation for better
performances in visual recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1632</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1632</id><created>2014-12-04</created><authors><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author><author><keyname>Pulman</keyname><forenames>Stephen</forenames></author></authors><title>Deep Learning for Answer Sentence Selection</title><categories>cs.CL</categories><comments>9 pages, accepted by NIPS deep learning workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer sentence selection is the task of identifying sentences that contain
the answer to a given question. This is an important problem in its own right
as well as in the larger context of open domain question answering. We propose
a novel approach to solving this task via means of distributed representations,
and learn to match questions with answers by considering their semantic
encoding. This contrasts prior work on this task, which typically relies on
classifiers with large numbers of hand-crafted syntactic and semantic features
and various external resources. Our approach does not require any feature
engineering nor does it involve specialist linguistic data, making this model
easily applicable to a wide range of domains and languages. Experimental
results on a standard benchmark dataset from TREC demonstrate that---despite
its simplicity---our model matches state of the art performance on the answer
sentence selection task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1639</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1639</id><created>2014-12-04</created><updated>2015-03-02</updated><authors><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Jaberi</keyname><forenames>Raed</forenames></author></authors><title>On testing single connectedness in directed graphs and some related
  problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a directed graph with $n$ vertices and $m$ edges. The graph
$G$ is called singly-connected if for each pair of vertices $v,w \in V$ there
is at most one simple path from $v$ to $w$ in $G$. Buchsbaum and Carlisle
(1993) gave an algorithm for testing whether $G$ is singly-connected in
$O(n^{2})$ time. In this paper we describe a refined version of this algorithm
with running time $O(s\cdot t+m)$, where $s$ and $t$ are the number of sources
and sinks, respectively, in the reduced graph $G^{r}$ obtained by first
contracting each strongly connected component of $G$ into one vertex and then
eliminating vertices of indegree or outdegree $1$ by a contraction operation.
Moreover, we show that the problem of finding a minimum cardinality edge subset
$C\subseteq E$ (respectively, vertex subset $F\subseteq V$) whose removal from
$G$ leaves a singly-connected graph is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1641</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1641</id><created>2014-12-04</created><updated>2015-06-09</updated><authors><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Thomazo</keyname><forenames>Micha&#xeb;l</forenames></author></authors><title>On $k$-piecewise testability (preliminary report)</title><categories>cs.FL</categories><comments>Full version of the paper accepted for DLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a non-negative integer $k$, a language is $k$-piecewise test\-able
($k$-PT) if it is a finite boolean combination of languages of the form
$\Sigma^* a_1 \Sigma^* \cdots \Sigma^* a_n \Sigma^*$ for $a_i\in\Sigma$ and
$0\le n \le k$. We study the following problem: Given a DFA recognizing a
piecewise testable language, decide whether the language is $k$-PT. We provide
a complexity bound and a detailed analysis for small $k$'s. The result can be
used to find the minimal $k$ for which the language is $k$-PT. We show that the
upper bound on $k$ given by the depth of the minimal DFA can be exponentially
bigger than the minimal possible $k$, and provide a tight upper bound on the
depth of the minimal DFA recognizing a $k$-PT language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1652</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1652</id><created>2014-12-04</created><authors><author><keyname>Smiljkovikj</keyname><forenames>Katerina</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Efficiency Analysis of Decoupled Downlink and Uplink Access in
  Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>6 pages; 6 figures; Submitted to the IEEE International Conference on
  Communications (ICC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes two-tier heterogeneous cellular network with decoupled
downlink and uplink access. The basic performance benefits of uplink/downlink
decoupling have been recently introduced. Here we provide a more elaborate
treatment of the decoupling mechanism by analyzing spectral and energy
efficiency of the system, as the joint improvement of these two features is
crucial for the upcoming 5G systems. Contrary to the common assumption of a
homogeneous user domain, we analyze two-tier user domain, whose transmit powers
depend purely on the association process. The derived joint association
probabilities and the distributions of the distance to the serving base station
give deeper insight into the fundamentals of a system with decoupled access.
The rigorous theoretical analysis shows that decoupling of downlink and uplink
with two-level uplink power adaptation improves both, spectral and energy
efficiency of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1665</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1665</id><created>2014-12-04</created><updated>2015-06-09</updated><authors><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Seo</keyname><forenames>Junyeong</forenames></author></authors><title>Randomly-Directional Beamforming in Millimeter-Wave Multi-User MISO
  Downlink</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, randomly-directional beamforming (RDB) is considered for
millimeter-wave (mmwave) multi-user (MU) multiple-input single-output (MISO)
downlink systems. By using asymptotic techniques, the performance of RDB and
the MU gain in mm-wave MISO are analyzed based on the uniform random
line-of-sight (UR-LoS) channel model suitable for highly directional mm-wave
radio propagation channels. It is shown that there exists a transition point on
the number of users relative to the number of antenna elements for non-trivial
performance of the RDB scheme, and furthermore sum rate scaling arbitrarily
close to linear scaling with respect to the number of antenna elements can be
achieved under the UR-LoS channel model by opportunistic random beamforming
with proper user scheduling if the number of users increases linearly with
respect to the number of antenna elements. The provided results yield insights
into the most effective beamforming and scheduling choices for mm-wave MU-MISO
in various operating conditions. Simulation results validate our analysis based
on asymptotic techniques for finite cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1671</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1671</id><created>2014-12-04</created><updated>2014-12-10</updated><authors><author><keyname>Thorne</keyname><forenames>Camilo</forenames></author></authors><title>Chases and Bag-Set Certain Answers</title><categories>cs.DB</categories><comments>4pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the chase technique is powerful enough to capture
the bag-set semantics of conjunctive queries over IDBs and IDs and TGDs. In
addition, we argue that in such cases it provides efficient (LogSpace) query
evaluation algorithms and that, moreover, it can serve as a basis for
evaluating some restricted classes of aggregate queries under incomplete
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1679</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1679</id><created>2014-12-04</created><authors><author><keyname>Gurciullo</keyname><forenames>Stefano</forenames></author></authors><title>Stess-testing the system: Financial shock contagion in the realm of
  uncertainty</title><categories>q-fin.RM cs.SI physics.soc-ph q-fin.GN</categories><comments>50 pages, Paper presented as part of the coursework for the PhD
  Transfer Viva</comments><msc-class>91B82, 91B74, 91G99</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work proposes an augmented variant of DebtRank with uncertainty
intervals as a method to investigate and assess systemic risk in financial
networks, in a context of incomplete data. The algorithm is tested against a
default contagion algorithm on three ensembles of networks with increasing
density, estimated from real-world banking data related to the largest 227 EU15
financial institutions indexed in a stock market. Results suggest that DebtRank
is capable of capturing increasing rates of systemic risk in a more sensitive
and continuous way, thereby acting as an early-warning signal. The paper
proposes three policy instruments based on this approach: the monitoring of
systemic risk over time by applying the augmented DebtRank on time snapshots of
interbank networks, a stress-testing framework able to test the systemic
importance of financial institutions on different shock scenarios, and the
evaluation of distribution of systemic losses in currency value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1680</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1680</id><created>2014-12-04</created><updated>2015-04-07</updated><authors><author><keyname>Buchet</keyname><forenames>Micka&#xeb;l</forenames></author><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Fan</keyname><forenames>Fengtao</forenames></author><author><keyname>Oudot</keyname><forenames>Steve Y.</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Topological analysis of scalar fields with outliers</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given a real-valued function $f$ defined over a manifold $M$ embedded in
$\mathbb{R}^d$, we are interested in recovering structural information about
$f$ from the sole information of its values on a finite sample $P$. Existing
methods provide approximation to the persistence diagram of $f$ when geometric
noise and functional noise are bounded. However, they fail in the presence of
aberrant values, also called outliers, both in theory and practice.
  We propose a new algorithm that deals with outliers. We handle aberrant
functional values with a method inspired from the k-nearest neighbors
regression and the local median filtering, while the geometric outliers are
handled using the distance to a measure. Combined with topological results on
nested filtrations, our algorithm performs robust topological analysis of
scalar fields in a wider range of noise models than handled by current methods.
We provide theoretical guarantees and experimental results on the quality of
our approximation of the sampled scalar field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1683</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1683</id><created>2014-12-04</created><authors><author><keyname>Anagnostopoulos</keyname><forenames>Evangelos</forenames></author><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Psarros</keyname><forenames>Ioannis</forenames></author></authors><title>Low-quality dimension reduction and high-dimensional Approximate Nearest
  Neighbor</title><categories>cs.CG</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approximate nearest neighbor problem ($\epsilon$-ANN) in Euclidean
settings is a fundamental question, which has been addressed by two main
approaches: Data-dependent space partitioning techniques perform well when the
dimension is relatively low, but are affected by the curse of dimensionality.
On the other hand, locality sensitive hashing has polynomial dependence in the
dimension, sublinear query time with an exponent inversely proportional to the
error factor $\epsilon$, and subquadratic space requirement.
  We generalize the Johnson-Lindenstrauss lemma to define &quot;low-quality&quot;
mappings to a Euclidean space of significantly lower dimension, such that they
satisfy a requirement weaker than approximately preserving all distances or
even preserving the nearest neighbor. This mapping guarantees, with arbitrarily
high probability, that an approximate nearest neighbor lies among the $k$
approximate nearest neighbors in the projected space. This leads to a
randomized tree based data structure that avoids the curse of dimensionality
for $\epsilon$-ANN. Our algorithm, given $n$ points in dimension $d$, achieves
space usage in $O(dn)$, preprocessing time in $O(dn\log n)$, and query time in
$O(d n^{\rho}\log n)$, where $\rho$ is proportional to $1-{1}/{\ln \ln n}$, for
fixed $\epsilon \in (0,1)$. It employs a data structure, such as BBD-trees,
that efficiently finds $k$ approximate nearest neighbors. The dimension
reduction is larger if one assumes that pointsets possess some structure,
namely bounded expansion rate. We implement our method and present experimental
results in up to 500 dimensions and $10^5$ points, which show that the
practical performance is better than predicted by the theoretical analysis. In
addition, we compare our approach with E2LSH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1687</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1687</id><created>2014-11-20</created><authors><author><keyname>Smirnov</keyname><forenames>A. V.</forenames></author></authors><title>The Approximate Bilinear Algorithm of Length 46 for Multiplication of 4
  x 4 Matrices</title><categories>math.NA cs.NA</categories><comments>PDF, 8 pages</comments><msc-class>65F30 (Primary), 15A99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the arbitrary precision approximate (APA) bilinear algorithm of
length 46 for multiplication of 4 x 4 and 4 x 4 matrices. The algorithm has
polynomial order 3 and 352 nonzero coefficients from total 2208.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1695</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1695</id><created>2014-12-04</created><updated>2014-12-11</updated><authors><author><keyname>Hurley</keyname><forenames>Ted</forenames></author></authors><title>Convolutional codes from unit schemes</title><categories>math.RA cs.DM cs.IT math.IT</categories><msc-class>94B10, 11T71, 16S99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic methods for the construction, design and analysis of series of
convolutional codes are developed. The methods use row or block structures of
matrices. New codes and infinite series of codes are presented. Codes with
specific properties are designed and analysed and examples of various types are
given. Free distances and lower bounds on free distances are proved
algebraically and these can be at or near the maximum free distances attainable
for their types. Methods for designing series of LDPC (low density parity
check) convolutional codes are derived. Design methods for self-dual and
dual-containing convolutional codes are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1710</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1710</id><created>2014-12-04</created><authors><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Convolutional Neural Networks at Constrained Time Cost</title><categories>cs.CV</categories><comments>8-page technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though recent advanced convolutional neural networks (CNNs) have been
improving the image recognition accuracy, the models are getting more complex
and time-consuming. For real-world applications in industrial and commercial
scenarios, engineers and developers are often faced with the requirement of
constrained time budget. In this paper, we investigate the accuracy of CNNs
under constrained time cost. Under this constraint, the designs of the network
architectures should exhibit as trade-offs among the factors like depth,
numbers of filters, filter sizes, etc. With a series of controlled comparisons,
we progressively modify a baseline model while preserving its time complexity.
This is also helpful for understanding the importance of the factors in network
designs. We present an architecture that achieves very competitive accuracy in
the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than
&quot;AlexNet&quot; (16.0% top-5 error, 10-view test).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1712</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1712</id><created>2014-12-04</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Application of Smart Antenna Technologies in Simultaneous Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous wireless information and power transfer (SWIPT) is a promising
solution to increase the lifetime of wireless nodes and hence alleviate the
energy bottleneck of energy constrained wireless networks. As an alternative to
conventional energy harvesting techniques, SWIPT relies on the use of radio
frequency signals, and is expected to bring some fundamental changes to the
design of wireless communication networks. This article focuses on the
application of advanced smart antenna technologies, including multiple-input
multiple-output and relaying techniques, to SWIPT. These smart antenna
technologies have the potential to significantly improve the energy efficiency
and also the spectral efficiency of SWIPT. Different network topologies with
single and multiple users are investigated, along with some promising solutions
to achieve a favorable trade-off between system performance and complexity. A
detailed discussion of future research challenges for the design of SWIPT
systems is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1717</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1717</id><created>2014-12-04</created><authors><author><keyname>Cleveland</keyname><forenames>Joseph</forenames></author><author><keyname>Dzugan</keyname><forenames>Jeffrey</forenames></author><author><keyname>Hauenstein</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Haywood</keyname><forenames>Ian</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author><author><keyname>Morse</keyname><forenames>Anthony</forenames></author><author><keyname>Robol</keyname><forenames>Leonardo</forenames></author><author><keyname>Schlenk</keyname><forenames>Taylor</forenames></author></authors><title>Certified counting of roots of random univariate polynomials</title><categories>math.NA cs.NA hep-th nlin.CD</categories><comments>28 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A challenging problem in computational mathematics is to compute roots of a
high-degree univariate random polynomial. We combine an efficient
multiprecision implementation for solving high-degree random polynomials with
two certification methods, namely Smale's $\alpha$-theory and one based on
Gerschgorin's theorem, for showing that a given numerical approximation is in
the quadratic convergence region of Newton's method of some exact solution.
With this combination, we can certifiably count the number of real roots of
random polynomials. We quantify the difference between the two certification
procedures and list the salient features of both of them. After benchmarking on
random polynomials where the coefficients are drawn from the Gaussian
distribution, we obtain novel experimental results for the Cauchy distribution
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1723</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1723</id><created>2014-12-04</created><updated>2015-01-14</updated><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author></authors><title>Communication complexity and the reality of the wave-function</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Mod. Phys. Lett. A, 30, 1530001 (2015)</journal-ref><doi>10.1142/S0217732315300013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this review, we discuss a relation between quantum communication
complexity and a long-standing debate in quantum foundation concerning the
interpretation of the quantum state. Is the quantum state a physical element of
reality as originally interpreted by Schrodinger? Or is it an abstract
mathematical object containing statistical information about the outcome of
measurements as interpreted by Born? Although these questions sound
philosophical and pointless, they can be made precise in the framework of what
we call classical theories of quantum processes, which are a reword of quantum
phenomena in the language of classical probability theory. In 2012, Pusey,
Barrett and Rudolph (PBR) proved, under an assumption of preparation
independence, a theorem supporting the original interpretation of Schrodinger
in the classical framework. Recently, we showed that these questions are
related to a practical problem in quantum communication complexity, namely,
quantifying the minimal amount of classical communication required in the
classical simulation of a two-party quantum communication process. In
particular, we argued that the statement of the PBR theorem can be proved if
the classical communication cost of simulating the communication of n qubits
grows more than exponentially in 'n'. Our argument is based on an assumption
that we call probability equipartition property. This property is somehow
weaker than the preparation independence property used in the PBR theorem, as
the former can be justified by the latter and the asymptotic equipartition
property of independent stochastic sources. The equipartition property is a
general and natural hypothesis that can be assumed even if the preparation
independence hypothesis is dropped. In this review, we further develop our
argument into the form of a theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1732</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1732</id><created>2014-12-04</created><updated>2015-05-14</updated><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Ma</keyname><forenames>Jianhua</forenames></author><author><keyname>Lu</keyname><forenames>Hongbing</forenames></author><author><keyname>Liang</keyname><forenames>Zhengrong</forenames></author></authors><title>Statistical models and regularization strategies in statistical image
  reconstruction of low-dose X-ray CT: a survey</title><categories>physics.med-ph cs.CV</categories><comments>This paper has been withdrawn because the authors have differnt
  opinions on some contents of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical image reconstruction (SIR) methods have shown potential to
substantially improve the image quality of low-dose X-ray computed tomography
(CT) as compared to the conventional filtered back-projection (FBP) method for
various clinical tasks. According to the maximum a posterior (MAP) estimation,
the SIR methods can be typically formulated by an objective function consisting
of two terms: (1) data-fidelity (or equivalently, data-fitting or
data-mismatch) term modeling the statistics of projection measurements, and (2)
regularization (or equivalently, prior or penalty) term reflecting prior
knowledge or expectation on the characteristics of the image to be
reconstructed. Existing SIR methods for low-dose CT can be divided into two
groups: (1) those that use calibrated transmitted photon counts (before
log-transform) with penalized maximum likelihood (pML) criterion, and (2) those
that use calibrated line-integrals (after log-transform) with penalized
weighted least-squares (PWLS) criterion. Accurate statistical modeling of the
projection measurements is a prerequisite for SIR, while the regularization
term in the objective function also plays a critical role for successful image
reconstruction. This paper reviews several statistical models on CT projection
measurements and various regularization strategies incorporating prior
knowledge or expected properties of the image to be reconstructed, which
together formulate the objective function of the SIR methods for low-dose X-ray
CT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1736</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1736</id><created>2014-12-04</created><authors><author><keyname>Boykett</keyname><forenames>Tim</forenames></author><author><keyname>Wendt</keyname><forenames>Gerhard</forenames></author></authors><title>J2 Radical in Automata Nearrings</title><categories>cs.FL math.RA</categories><comments>Published</comments><journal-ref>International Journal of Foundations of Computer Science Vol. 25,
  No. 05, pp. 585-595 (2014)</journal-ref><doi>10.1142/S0129054114500233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking at the automata defined over a group alphabet as a nearring, we see
that they are a highly complicated structure. As with ring theory, one method
to deal with complexity is to look at semisimplicity modulo radical structures.
We find some bounds on the Jacobson 2-radical and show that in certain groups,
this radical can be explicitly found and the semisimple image determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1740</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1740</id><created>2014-12-04</created><updated>2015-05-23</updated><authors><author><keyname>Kusner</keyname><forenames>Matt J.</forenames></author><author><keyname>Kolkin</keyname><forenames>Nicholas I.</forenames></author><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author></authors><title>Image Data Compression for Covariance and Histogram Descriptors</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covariance and histogram image descriptors provide an effective way to
capture information about images. Both excel when used in combination with
special purpose distance metrics. For covariance descriptors these metrics
measure the distance along the non-Euclidean Riemannian manifold of symmetric
positive definite matrices. For histogram descriptors the Earth Mover's
distance measures the optimal transport between two histograms. Although more
precise, these distance metrics are very expensive to compute, making them
impractical in many applications, even for data sets of only a few thousand
examples. In this paper we present two methods to compress the size of
covariance and histogram datasets with only marginal increases in test error
for k-nearest neighbor classification. Specifically, we show that we can reduce
data sets to 16% and in some cases as little as 2% of their original size,
while approximately matching the test error of kNN classification on the full
training set. In fact, because the compressed set is learned in a supervised
fashion, it sometimes even outperforms the full data set, while requiring only
a fraction of the space and drastically reducing test-time computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1741</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1741</id><created>2014-12-04</created><updated>2015-06-29</updated><authors><author><keyname>Memeti</keyname><forenames>Suejb</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author></authors><title>PaREM: A Novel Approach for Parallel Regular Expression Matching</title><categories>cs.FL cs.DC</categories><comments>CSE-2014, Dec. 19th - 21st, 2014, Chengdu, Sichuan, China</comments><doi>10.1109/CSE.2014.146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular expression matching is essential for many applications, such as
finding patterns in text, exploring substrings in large DNA sequences, or
lexical analysis. However, sequential regular expression matching may be
time-prohibitive for large problem sizes. In this paper, we describe a novel
algorithm for parallel regular expression matching via deterministic finite
automata. Furthermore, we present our tool PaREM that accepts regular
expressions and finite automata as input and automatically generates the
corresponding code for our algorithm that is amenable for parallel execution on
shared-memory systems. We evaluate our parallel algorithm empirically by
comparing it with a commonly used algorithm for sequential regular expression
matching. Experiments on a dual-socket shared-memory system with 24 physical
cores show speed-ups of up to 21x for 48 threads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1763</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1763</id><created>2014-12-04</created><authors><author><keyname>Huang</keyname><forenames>Zengfeng</forenames></author><author><keyname>Tai</keyname><forenames>Wai Ming</forenames></author><author><keyname>Yi</keyname><forenames>Ke</forenames></author></authors><title>Tracking the Frequency Moments at All Times</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional requirement for a randomized streaming algorithm is just {\em
one-shot}, i.e., algorithm should be correct (within the stated $\eps$-error
bound) at the end of the stream. In this paper, we study the {\em tracking}
problem, where the output should be correct at all times. The standard approach
for solving the tracking problem is to run $O(\log m)$ independent instances of
the one-shot algorithm and apply the union bound to all $m$ time instances. In
this paper, we study if this standard approach can be improved, for the
classical frequency moment problem. We show that for the $F_p$ problem for any
$1 &lt; p \le 2$, we actually only need $O(\log \log m + \log n)$ copies to
achieve the tracking guarantee in the cash register model, where $n$ is the
universe size. Meanwhile, we present a lower bound of $\Omega(\log m \log\log
m)$ bits for all linear sketches achieving this guarantee. This shows that our
upper bound is tight when $n=(\log m)^{O(1)}$. We also present an
$\Omega(\log^2 m)$ lower bound in the turnstile model, showing that the
standard approach by using the union bound is essentially optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1769</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1769</id><created>2014-12-04</created><authors><author><keyname>Balko</keyname><forenames>Martin</forenames></author><author><keyname>Jel&#xed;nek</keyname><forenames>V&#xed;t</forenames></author><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>On the Beer index of convexity and its variants</title><categories>math.MG cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a subset of $\mathbb{R}^d$ with finite positive Lebesgue measure.
The Beer index of convexity $\operatorname{b}(S)$ of $S$ is the probability
that two points of $S$ chosen uniformly independently at random see each other
in $S$. The convexity ratio $\operatorname{c}(S)$ of $S$ is the Lebesgue
measure of the largest convex subset of $S$ divided by the Lebesgue measure of
$S$. We investigate the relationship between these two natural measures of
convexity of $S$.
  We show that every set $S\subseteq\mathbb{R}^2$ with simply connected
components satisfies $\operatorname{b}(S)\leq\alpha\operatorname{c}(S)$ for an
absolute constant $\alpha$, provided $\operatorname{b}(S)$ is defined. This
implies an affirmative answer to the conjecture of Cabello et al. asserting
that this estimate holds for simple polygons.
  We also consider higher-order generalizations of $\operatorname{b}(S)$. For
$1\leq k\leq d$, the $k$-index of convexity $\operatorname{b}_k(S)$ of
$S\subseteq\mathbb{R}^d$ is the probability that the convex hull of a
$(k+1)$-tuple of points chosen uniformly independently at random from $S$ is
contained in $S$. We show that for every $d\geq 2$ there is a constant
$\beta(d)&gt;0$ such that every set $S\subseteq\mathbb{R}^d$ satisfies
$\operatorname{b}_d(S)\leq\beta\operatorname{c}(S)$, provided
$\operatorname{b}_d(S)$ exists. We provide an almost matching lower bound by
showing that there is a constant $\gamma(d)&gt;0$ such that for every
$\varepsilon\in(0,1]$ there is a set $S\subseteq\mathbb{R}^d$ of Lebesgue
measure one satisfying $\operatorname{c}(S)\leq\varepsilon$ and
$\operatorname{b}_d(S)\geq\gamma\frac{\varepsilon}{\log_2{1/\varepsilon}}\geq\gamma\frac{\operatorname{c}(S)}{\log_2{1/\operatorname{c}(S)}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1772</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1772</id><created>2014-12-04</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Heart Rate Monitoring as an Easy Way to Increase Engagement in
  Human-Agent Interaction</title><categories>cs.HC cs.CY</categories><comments>PhyCS - International Conference on Physiological Computing Systems,
  Feb 2015, Angers, France. SCITEPRESS, \&amp;lt;http://www.phycs.org/\&amp;gt</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physiological sensors are gaining the attention of manufacturers and users.
As denoted by devices such as smartwatches or the newly released Kinect 2 --
which can covertly measure heartbeats -- or by the popularity of smartphone
apps that track heart rate during fitness activities. Soon, physiological
monitoring could become widely accessible and transparent to users. We
demonstrate how one could take advantage of this situation to increase users'
engagement and enhance user experience in human-agent interaction. We created
an experimental protocol involving embodied agents -- &quot;virtual avatars&quot;. Those
agents were displayed alongside a beating heart. We compared a condition in
which this feedback was simply duplicating the heart rates of users to another
condition in which it was set to an average heart rate. Results suggest a
superior social presence of agents when they display feedback similar to users'
internal state. This physiological &quot;similarity-attraction&quot; effect may lead,
with little effort, to a better acceptance of agents and robots by the general
public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1780</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1780</id><created>2014-12-04</created><authors><author><keyname>Aubert</keyname><forenames>Olivier</forenames></author><author><keyname>Jaeger</keyname><forenames>Joscha</forenames></author></authors><title>Annotating Video with Open Educational Resources in a Flipped Classroom
  Scenario</title><categories>cs.CY cs.MM</categories><comments>OCWC Global Conference, Apr 2014, Llubljana, Slovenia</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wealth of Open Educational Resources is now available, and beyond the first
and evident problem of finding them, the issue of articulating a set of
resources is arising. When using audiovisual resources, among different
possibilities, annotating a video resource with additional resources linked to
specific fragments can constitute one of the articulation modalities.
Annotating a video is a complex task, and in a pedagogical context,
intermediary activities should be proposed in order to mitigate this
complexity. In this paper, we describe a tool dedicated to supporting video
annotation activities. It aims at improving learner engagement, by having
students be more active when watching videos by offering a progressive
annotation process, first guided by providing predefined resources, then more
freely, to accompany users in the practice of annotating videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1787</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1787</id><created>2014-12-04</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Devanny</keyname><forenames>William E.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>ERGMs are Hard</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of the exponential random graph
model (ERGM) commonly used in social network analysis. This model represents a
probability distribution on graphs by setting the log-likelihood of generating
a graph to be a weighted sum of feature counts. These log-likelihoods must be
exponentiated and then normalized to produce probabilities, and the normalizing
constant is called the \emph{partition function}. We show that the problem of
computing the partition function is $\mathsf{\#P}$-hard, and inapproximable in
polynomial time to within an exponential ratio, assuming $\mathsf{P} \neq
\mathsf{NP}$. Furthermore, there is no randomized polynomial time algorithm for
generating random graphs whose distribution is within total variation distance
$1-o(1)$ of a given ERGM. Our proofs use standard feature types based on the
sociological theories of assortative mixing and triadic closure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1788</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1788</id><created>2014-12-04</created><authors><author><keyname>Yanez</keyname><forenames>Felipe</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Primal-Dual Algorithms for Non-negative Matrix Factorization with the
  Kullback-Leibler Divergence</title><categories>cs.LG math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) approximates a given matrix as a
product of two non-negative matrices. Multiplicative algorithms deliver
reliable results, but they show slow convergence for high-dimensional data and
may be stuck away from local minima. Gradient descent methods have better
behavior, but only apply to smooth losses such as the least-squares loss. In
this article, we propose a first-order primal-dual algorithm for non-negative
decomposition problems (where one factor is fixed) with the KL divergence,
based on the Chambolle-Pock algorithm. All required computations may be
obtained in closed form and we provide an efficient heuristic way to select
step-sizes. By using alternating optimization, our algorithm readily extends to
NMF and, on synthetic examples, face recognition or music source separation
datasets, it is either faster than existing algorithms, or leads to improved
local optima, or both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1790</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1790</id><created>2014-12-04</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Gervais</keyname><forenames>Renaud</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Fleck</keyname><forenames>St&#xe9;phanie</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Hachet</keyname><forenames>Martin</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Teegi: Tangible EEG Interface</title><categories>cs.HC</categories><comments>to appear in UIST-ACM User Interface Software and Technology
  Symposium, Oct 2014, Honolulu, United States</comments><proxy>ccsd</proxy><doi>10.1145/2642918.2647368</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that
enables novice users to get to know more about something as complex as brain
signals, in an easy, en- gaging and informative way. To this end, we have
designed a new system based on a unique combination of spatial aug- mented
reality, tangible interaction and real-time neurotech- nologies. With Teegi, a
user can visualize and analyze his or her own brain activity in real-time, on a
tangible character that can be easily manipulated, and with which it is
possible to interact. An exploration study has shown that interacting with
Teegi seems to be easy, motivating, reliable and infor- mative. Overall, this
suggests that Teegi is a promising and relevant training and mediation tool for
the general public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1792</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1792</id><created>2014-12-04</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Beyond the Euler characteristic: Approximating the genus of general
  graphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the Euler genus of a graph is a fundamental problem in graph theory
and topology. It has been shown to be NP-hard by [Thomassen '89] and a
linear-time fixed-parameter algorithm has been obtained by [Mohar '99]. Despite
extensive study, the approximability of the Euler genus remains wide open.
While the existence of an $O(1)$-approximation is not ruled out, the currently
best-known upper bound is a trivial $O(n/g)$-approximation that follows from
bounds on the Euler characteristic.
  In this paper, we give the first non-trivial approximation algorithm for this
problem. Specifically, we present a polynomial-time algorithm which given a
graph $G$ of Euler genus $g$ outputs an embedding of $G$ into a surface of
Euler genus $g^{O(1)}$. Combined with the above $O(n/g)$-approximation, our
result also implies a $O(n^{1-\alpha})$-approximation, for some universal
constant $\alpha&gt;0$.
  Our approximation algorithm also has implications for the design of
algorithms on graphs of small genus. Several of these algorithms require that
an embedding of the graph into a surface of small genus is given as part of the
input. Our result implies that many of these algorithms can be implemented even
when the embedding of the input graph is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1793</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1793</id><created>2014-12-04</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>VC-dimension and Erd\H{o}s-P\'osa property</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a graph. A $k$-neighborhood in $G$ is a set of vertices
consisting of all the vertices at distance at most $k$ from some vertex of $G$.
The hypergraph on vertex set $V$ which edge set consists of all the
$k$-neighborhoods of $G$ for all $k$ is the neighborhood hypergraph of $G$. Our
goal in this paper is to investigate the complexity of a graph in terms of its
neighborhoods. Precisely, we define the distance VC-dimension of a graph $G$ as
the maximum taken over all induced subgraphs $G'$ of $G$ of the VC-dimension of
the neighborhood hypergraph of $G'$. For a class of graphs, having bounded
distance VC-dimension both generalizes minor closed classes and graphs with
bounded clique-width.
  Our motivation is a result of Chepoi, Estellon and Vax\`es asserting that
every planar graph of diameter $2\ell$ can be covered by a bounded number of
balls of radius $\ell$. In fact, they obtained the existence of a function $f$
such that every set $\cal F$ of balls of radius $\ell$ in a planar graph admits
a hitting set of size $f(\nu)$ where $\nu$ is the maximum number of pairwise
disjoint elements of $\cal F$.
  Our goal is to generalize the proof of Chepoi, Estellon and Vax\`es with the
unique assumption of bounded distance VC-dimension of neighborhoods. In other
words, the set of balls of fixed radius in a graph with bounded distance
VC-dimension has the Erd\H{o}s-P\'osa property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1798</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1798</id><created>2014-12-04</created><authors><author><keyname>Nassif</keyname><forenames>Roula</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Ferrari</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Multitask diffusion adaptation over asynchronous networks</title><categories>cs.SY cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multitask diffusion LMS is an efficient strategy to simultaneously infer,
in a collaborative manner, multiple parameter vectors. Existing works on
multitask problems assume that all agents respond to data synchronously. In
several applications, agents may not be able to act synchronously because
networks can be subject to several sources of uncertainties such as changing
topology, random link failures, or agents turning on and off for energy
conservation. In this work, we describe a model for the solution of multitask
problems over asynchronous networks and carry out a detailed mean and
mean-square error analysis. Results show that sufficiently small step-sizes can
still ensure both stability and performance. Simulations and illustrative
examples are provided to verify the theoretical findings. The framework is
applied to a particular application involving spectral sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1820</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1820</id><created>2014-12-03</created><authors><author><keyname>Gillick</keyname><forenames>Dan</forenames></author><author><keyname>Lazic</keyname><forenames>Nevena</forenames></author><author><keyname>Ganchev</keyname><forenames>Kuzman</forenames></author><author><keyname>Kirchner</keyname><forenames>Jesse</forenames></author><author><keyname>Huynh</keyname><forenames>David</forenames></author></authors><title>Context-Dependent Fine-Grained Entity Type Tagging</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity type tagging is the task of assigning category labels to each mention
of an entity in a document. While standard systems focus on a small set of
types, recent work (Ling and Weld, 2012) suggests that using a large
fine-grained label set can lead to dramatic improvements in downstream tasks.
In the absence of labeled training data, existing fine-grained tagging systems
obtain examples automatically, using resolved entities and their types
extracted from a knowledge base. However, since the appropriate type often
depends on context (e.g. Washington could be tagged either as city or
government), this procedure can result in spurious labels, leading to poorer
generalization. We propose the task of context-dependent fine type tagging,
where the set of acceptable labels for a mention is restricted to only those
deducible from the local context (e.g. sentence or document). We introduce new
resources for this task: 11,304 mentions annotated with their context-dependent
fine types, and we provide baseline experimental results on this data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1840</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1840</id><created>2014-12-04</created><authors><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Huijse</keyname><forenames>Pablo</forenames></author><author><keyname>Estevez</keyname><forenames>Pablo A.</forenames></author><author><keyname>Zegers</keyname><forenames>Pablo</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>A Novel, Fully Automated Pipeline for Period Estimation in the EROS 2
  Data Set</title><categories>astro-ph.IM astro-ph.SR cs.IT math.IT</categories><journal-ref>The Astrophysical Journal Supplement Series, Volume 216, Number 2,
  2015</journal-ref><doi>10.1088/0067-0049/216/2/25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method to discriminate periodic from non-periodic
irregularly sampled lightcurves. We introduce a periodic kernel and maximize a
similarity measure derived from information theory to estimate the periods and
a discriminator factor. We tested the method on a dataset containing 100,000
synthetic periodic and non-periodic lightcurves with various periods,
amplitudes and shapes generated using a multivariate generative model. We
correctly identified periodic and non-periodic lightcurves with a completeness
of 90% and a precision of 95%, for lightcurves with a signal-to-noise ratio
(SNR) larger than 0.5. We characterize the efficiency and reliability of the
model using these synthetic lightcurves and applied the method on the EROS-2
dataset. A crucial consideration is the speed at which the method can be
executed. Using hierarchical search and some simplification on the parameter
search we were able to analyze 32.8 million lightcurves in 18 hours on a
cluster of GPGPUs. Using the sensitivity analysis on the synthetic dataset, we
infer that 0.42% in the LMC and 0.61% in the SMC of the sources show periodic
behavior. The training set, the catalogs and source code are all available in
http://timemachine.iic.harvard.edu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1841</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1841</id><created>2014-12-02</created><updated>2015-05-12</updated><authors><author><keyname>Tupper</keyname><forenames>P. F.</forenames></author></authors><title>Exemplar Dynamics and Sound Merger in Language</title><categories>cs.CL math.DS nlin.AO</categories><comments>23 pages. Considerably abbreviated version of this work appeared
  (without mathematical details) in the Proceedings of the 36th Annual
  Conference of the Cognitive Science Society (2014)</comments><msc-class>91F20, 70F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model of phonological contrast in natural language.
Specifically, the model describes the maintenance of contrast between different
words in a language, and the elimination of such contrast when sounds in the
words merge. An example of such a contrast is that provided by the two vowel
sounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' in
most dialects of English. We model language users' knowledge of the
pronunciation of a word as consisting of collections of labeled exemplars
stored in memory. Each exemplar is a detailed memory of a particular utterance
of the word in question. In our model an exemplar is represented by one or two
phonetic variables along with a weight indicating how strong the memory of the
utterance is. Starting from an exemplar-level model we derive
integro-differential equations for the evolution of exemplar density fields in
phonetic space. Using these latter equations we investigate under what
conditions two sounds merge, thus eliminating the contrast. Our main conclusion
is that for the preservation of phonological contrast, it is necessary that
anomalous utterances of a given word are discarded, and not merely stored in
memory as an exemplar of another word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1842</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1842</id><created>2014-12-04</created><authors><author><keyname>Jaderberg</keyname><forenames>Max</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Reading Text in the Wild with Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present an end-to-end system for text spotting -- localising
and recognising text in natural scene images -- and text based image retrieval.
This system is based on a region proposal mechanism for detection and deep
convolutional neural networks for recognition. Our pipeline uses a novel
combination of complementary proposal generation techniques to ensure high
recall, and a fast subsequent filtering stage for improving precision. For the
recognition and ranking of proposals, we train very large convolutional neural
networks to perform word recognition on the whole proposal region at the same
time, departing from the character classifier based systems of the past. These
networks are trained solely on data produced by a synthetic text generation
engine, requiring no human labelled data.
  Analysing the stages of our pipeline, we show state-of-the-art performance
throughout. We perform rigorous experiments across a number of standard
end-to-end text spotting benchmarks and text-based image retrieval datasets,
showing a large improvement over all previous methods. Finally, we demonstrate
a real-world application of our text spotting system to allow thousands of
hours of news footage to be instantly searchable via a text query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1859</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1859</id><created>2014-12-04</created><authors><author><keyname>Elahi</keyname><forenames>Tariq</forenames></author><author><keyname>Murdoch</keyname><forenames>Steven J.</forenames></author><author><keyname>Goldberg</keyname><forenames>Ian</forenames></author></authors><title>Censorship Resistance: Let a Thousand Flowers Bloom?</title><categories>cs.CR cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that one of the most important decisions in designing and
deploying censorship resistance systems is whether one set of system options
should be selected (the best), or whether there should be several sets of good
ones. We model the problem of choosing these options as a cat-and-mouse game
and show that the best strategy depends on the value the censor associates with
total system censorship versus partial, and the tolerance of false positives.
If the censor has a low tolerance to false positives then choosing one
censorship resistance system is best. Otherwise choosing several systems is the
better choice, but the way traffic should be distributed over the systems
depends on the tolerance of the censor to false negatives. We demonstrate that
establishing the censor's utility function is critical to discovering the best
strategy for censorship resistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1862</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1862</id><created>2014-12-04</created><updated>2015-05-20</updated><authors><author><keyname>Egr&#xe9;</keyname><forenames>Paul</forenames></author><author><keyname>Marty</keyname><forenames>Paul</forenames></author><author><keyname>Renne</keyname><forenames>Bryan</forenames></author></authors><title>Knowledge, Justification, and Reason-Based Belief</title><categories>cs.LO cs.AI</categories><comments>v3 edits acknowledgments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is knowledge definable as justified true belief (&quot;JTB&quot;)? We argue that one
can legitimately answer positively or negatively, depending on how the notion
of justification is understood. To facilitate our argument, we introduce a
simple propositional logic of reason-based belief. We show that this logic is
sufficiently flexible to accommodate various useful features, including
quantification over reasons. We use our framework to contrast two notions of
JTB: one internalist, the other externalist. We argue that Gettier cases
essentially challenge the internalist notion but not the externalist one. In
particular, we may equate knowledge and JTB if the latter is grounded in what
we call &quot;adequate&quot; reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1866</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1866</id><created>2014-12-04</created><updated>2015-06-18</updated><authors><author><keyname>Kerr</keyname><forenames>Catherine</forenames></author><author><keyname>Hoare</keyname><forenames>Terri</forenames></author><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>Carroll</keyname><forenames>Paula</forenames></author></authors><title>Integer Programming Ensemble of Classifiers for Temporal Relations</title><categories>cs.CL cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extraction of events and understanding related temporal expression among them
is a major challenge in natural language processing. In longer texts,
processing on sentence-by-sentence or expression-by-expression basis often
fails, in part due to the disregard for the consistency of the processed data.
We present an ensemble method, which reconciles the output of multiple
classifiers for temporal expressions, subject to consistency constraints across
the whole text. The use of integer programming to enforce the consistency
constraints globally improves upon the best published results from the
TempEval-3 Challenge considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1870</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1870</id><created>2014-12-04</created><updated>2015-10-15</updated><authors><author><keyname>Li</keyname><forenames>Yunpeng</forenames></author></authors><title>A New Single-Source Shortest Path Algorithm for Nonnegative Weight Graph</title><categories>cs.DS cs.DM math.MG</categories><comments>11 pages,The algorithm is not good enough to improve the existing
  work</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The single-source shortest path problem is a classical problem in the
research field of graph algorithm. In this paper, a new single-source shortest
path algorithm for nonnegative weight graph is proposed. The algorithm can
compress multi-round Fibonacci heap operations to one round to save running
time relative to Dijkstra's algorithm using Fibonacci heap. The time complexity
of the algorithm is also O(m+nlogn) in the worst case, where m is the number of
edges and n is the number of nodes. However, the bound can be linear in some
case, for example, when edge weights of a graph are all the same and the hop
count of the longest shortest path is much less than n.Based on the theoretical
analyses, we demonstrate that the algorithm is faster than Dijkstra's algorithm
using Fibonacci heap in average situation when n is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1871</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1871</id><created>2014-12-04</created><authors><author><keyname>Herscovich</keyname><forenames>Estanislao</forenames></author></authors><title>A higher homotopic extension of persistent (co)homology</title><categories>math.AT cs.CG cs.CV math.KT</categories><comments>Any comment(s) would be highly appreciated</comments><msc-class>16E45, 16W70, 18G55, 55U10, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our objective in this article is to show a possibly interesting structure of
homotopic nature appearing in persistent (co)homology. Assuming that the
filtration of the (say) simplicial set embedded in a finite dimensional vector
space induces a multiplicative filtration (which would not be a so harsh
hypothesis in our setting) on the dg algebra given by the complex of simplicial
cochains, we may use a result by T. Kadeishvili to get a unique (up to
noncanonical equivalence) A_infinity-algebra structure on the complete
persistent cohomology of the filtered simplicial (or topological) set. We then
provide a construction of a (pseudo)metric on the set of all (generalized)
barcodes (that is, of all cohomological degrees) enriched with the
A_infinity-algebra structure stated before, refining the usual bottleneck
metric, and which is also independent of the particular A_infinity-algebra
structure chosen (among those equivalent to each other). We think that this
distance might deserve some attention for topological data analysis, for it in
particular can recognize different linking or foldings patterns, as in the
Borromean rings. As an aside, we give a simple proof of a result relating the
barcode structure between persistent homology and cohomology. This result was
observed in a recent article by V. de Silva, D. Morozov and M.
Vejdemo-Johansson under some restricted assumptions, which we do not suppose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1885</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1885</id><created>2014-12-04</created><updated>2014-12-28</updated><authors><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author></authors><title>Decomposition of Big Tensors With Low Multilinear Rank</title><categories>cs.NA cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor decompositions are promising tools for big data analytics as they
bring multiple modes and aspects of data to a unified framework, which allows
us to discover complex internal structures and correlations of data.
Unfortunately most existing approaches are not designed to meet the major
challenges posed by big data analytics. This paper attempts to improve the
scalability of tensor decompositions and provides two contributions: A flexible
and fast algorithm for the CP decomposition (FFCP) of tensors based on their
Tucker compression; A distributed randomized Tucker decomposition approach for
arbitrarily big tensors but with relatively low multilinear rank. These two
algorithms can deal with huge tensors, even if they are dense. Extensive
simulations provide empirical evidence of the validity and efficiency of the
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1888</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1888</id><created>2014-12-04</created><authors><author><keyname>Rafi</keyname><forenames>Muhammad</forenames></author><author><keyname>Amin</keyname><forenames>Farnaz</forenames></author><author><keyname>Shaikh</keyname><forenames>Mohammad Shahid</forenames></author></authors><title>Document clustering using graph based document representation with
  constraints</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document clustering is an unsupervised approach in which a large collection
of documents (corpus) is subdivided into smaller, meaningful, identifiable, and
verifiable sub-groups (clusters). Meaningful representation of documents and
implicitly identifying the patterns, on which this separation is performed, is
the challenging part of document clustering. We have proposed a document
clustering technique using graph based document representation with
constraints. A graph data structure can easily capture the non-linear
relationships of nodes, document contains various feature terms that can be
non-linearly connected hence a graph can easily represents this information.
Constrains, are explicit conditions for document clustering where background
knowledge is use to set the direction for Linking or Not-Linking a set of
documents for a target clusters, thus guiding the clustering process. We deemed
clustering is an ill-define problem, there can be many clustering results.
Background knowledge can be used to drive the clustering algorithm in the right
direction. We have proposed three different types of constraints, Instance
level, corpus level and cluster level constraints. A new algorithm Constrained
HAC is also proposed which will incorporate Instance level constraints as prior
knowledge; it will guide the clustering process leading to better results.
Extensive set of experiments have been performed on both synthetic and standard
document clustering datasets, results are compared on standard clustering
measures like: purity, entropy and F-measure. Results clearly establish that
our proposed approach leads to improvement in cluster quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1897</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1897</id><created>2014-12-05</created><updated>2015-04-02</updated><authors><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author></authors><title>Deep Neural Networks are Easily Fooled: High Confidence Predictions for
  Unrecognizable Images</title><categories>cs.CV cs.AI cs.NE</categories><comments>To appear at CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call &quot;fooling
images&quot; (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1898</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1898</id><created>2014-12-05</created><updated>2015-03-24</updated><authors><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Joint Rate and SINR Coverage Analysis for Decoupled Uplink-Downlink
  Biased Cell Associations in HetNets</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load balancing by proactively offloading users onto small and otherwise
lightly-loaded cells is critical for tapping the potential of dense
heterogeneous cellular networks (HCNs). Offloading has mostly been studied for
the downlink, where it is generally assumed that a user offloaded to a small
cell will communicate with it on the uplink as well. The impact of coupled
downlink-uplink offloading is not well understood. Uplink power control and
spatial interference correlation further complicate the mathematical analysis
as compared to the downlink. We propose an accurate and tractable model to
characterize the uplink SINR and rate distribution in a multi-tier HCN as a
function of the association rules and power control parameters. Joint
uplink-downlink rate coverage is also characterized. Using the developed
analysis, it is shown that the optimal degree of channel inversion (for uplink
power control) increases with load imbalance in the network. In sharp contrast
to the downlink, minimum path loss association is shown to be optimal for
uplink rate. Moreover, with minimum path loss association and full channel
inversion, uplink SIR is shown to be invariant of infrastructure density. It is
further shown that a decoupled association---employing differing association
strategies for uplink and downlink---leads to significant improvement in joint
uplink-downlink rate coverage over the standard coupled association in HCNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1908</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1908</id><created>2014-12-05</created><authors><author><keyname>Zhao</keyname><forenames>Rui</forenames></author><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author></authors><title>Person Re-identification by Saliency Learning</title><categories>cs.CV</categories><comments>This manuscript has 14 pages with 25 figures, and a preliminary
  version was published in ICCV 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Human eyes can recognize person identities based on small salient regions,
i.e. human saliency is distinctive and reliable in pedestrian matching across
disjoint camera views. However, such valuable information is often hidden when
computing similarities of pedestrian images with existing approaches. Inspired
by our user study result of human perception on human saliency, we propose a
novel perspective for person re-identification based on learning human saliency
and matching saliency distribution. The proposed saliency learning and matching
framework consists of four steps: (1) To handle misalignment caused by drastic
viewpoint change and pose variations, we apply adjacency constrained patch
matching to build dense correspondence between image pairs. (2) We propose two
alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a
saliency score for each image patch, through which distinctive features stand
out without using identity labels in the training procedure. (3) saliency
matching is proposed based on patch matching. Matching patches with
inconsistent saliency brings penalty, and images of the same identity are
recognized by minimizing the saliency matching cost. (4) Furthermore, saliency
matching is tightly integrated with patch matching in a unified structural
RankSVM learning framework. The effectiveness of our approach is validated on
the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the
state-of-the-art person re-identification methods on both datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1913</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1913</id><created>2014-12-05</created><authors><author><keyname>Mungle</keyname><forenames>Santosh</forenames></author></authors><title>A Portfolio Approach to Algorithm Selection for Discrete Time-Cost
  Trade-off Problem</title><categories>cs.AI</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been widely known that performance of algorithms for NP-Hard problems
varies from instance to instance. This phenomenon has been observed, when we
comprehensively studied multi-objective evolutionary algorithms (MOEAs) on a
six benchmark instances of discrete time-cost trade-off problem (DTCTP).
Instead of using single algorithm to solve DTCTP, we use a portfolio approach
that takes multiple algorithms as its constituent. In this paper, we proposed
portfolio comprising of four MOEAs, Non-dominated sorting genetic algorithm 2
(NSGA 2), the strength Pareto EA 2 (SPEA 2), Pareto archive evolutionary
strategy (PAES) and Niched Pareto Genetic Algorithm 2 (NPGA 2) to solve DTCTP.
The result shows that the portfolio approach is computationally fast and
qualitatively superior than its constituent algorithms for all benchmark
instances. Moreover, portfolio approach provides an insight in selecting the
best algorithm for all instances of DTCTP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1929</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1929</id><created>2014-12-05</created><updated>2015-01-28</updated><authors><author><keyname>D&#xfc;hrkop</keyname><forenames>Kai</forenames></author><author><keyname>B&#xf6;cker</keyname><forenames>Sebastian</forenames></author></authors><title>Fragmentation trees reloaded</title><categories>q-bio.QM cs.CE</categories><comments>different dataset</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metabolites, small molecules that are involved in cellular reactions, provide
a direct functional signature of cellular state. Untargeted metabolomics
experiments usually relies on tandem mass spectrometry to identify the
thousands of compounds in a biological sample. Today, the vast majority of
metabolites remain unknown. Fragmentation trees have become a powerful tool for
the interpretation of tandem mass spectrometry data of small molecules. These
trees are found by combinatorial optimization, and aim at explaining the
experimental data via fragmentation cascades. To obtain biochemically
meaningful results requires an elaborate optimization function. We present a
new scoring for computing fragmentation trees, transforming the combinatorial
optimization into a maximum a posteriori estimator. We demonstrate the
superiority of the new scoring for two tasks: Both for the de novo
identification of molecular formulas of unknown compounds, and for searching a
database for structurally similar compounds, our methods performs significantly
better than the previous scoring, as well as other methods for this task. Our
method can expedite the workflow for untargeted metabolomics, allowing
researchers to investigate unknowns using automated computational methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1945</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1945</id><created>2014-12-05</created><updated>2015-03-11</updated><authors><author><keyname>Sastry</keyname><forenames>Aditya A. V.</forenames></author></authors><title>Background Modelling using Octree Color Quantization</title><categories>cs.CV</categories><msc-class>65D19</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  By assuming that the most frequently occuring color in a video or a region of
a video I propose a new algorithm for detecting foreground objects in a video.
The process of detecting the foreground objects is complicated because of the
fact that there may be swaying trees, objects of the background being moved
around or lighting changes in the video. To deal with such complexities many
have come up with solutions which heavily rely on expensive floating point
operations. In this paper I used a data structure called Octree which is
implemented only using binary operations. Traditionally octrees were used for
color quantization but here in this paper I used it as a data structure to
store the most frequently occuring colors in a video as well. For each of the
starting few video frames, I constructed a Octree using all the colors of that
frame. Next I pruned all the trees by removing nodes below a certain height and
gave the leaf nodes a color which is dependant on the topological path from
that node to its parent. Hence any two leaf nodes in two different octrees with
the same topological path from themselves to the root will represent the same
color. Next I merged all these individual trees into a single tree retaining
only those nodes whose topological path to itself from the root is most common
among all the trees. The colors represented by the leaf nodes in the resultant
tree will be the most frequently occuring colors in the starting video frames
of the video. Hence any color of an incomming frame that is not close to any of
the colors represented by the leaf node of the merged tree can be regarded as
belonging to a foreground object.
  As an Octree is constructed using only binary operations, it is very fast
compared to other leading algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1947</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1947</id><created>2014-12-05</created><authors><author><keyname>Sastry</keyname><forenames>Aditya AV</forenames></author><author><keyname>Netti</keyname><forenames>Kalyan</forenames></author></authors><title>A parallel sampling based clustering</title><categories>cs.LG</categories><msc-class>68Q32</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The problem of automatically clustering data is an age old problem. People
have created numerous algorithms to tackle this problem. The execution time of
any of this algorithm grows with the number of input points and the number of
cluster centers required. To reduce the number of input points we could average
the points locally and use the means or the local centers as the input for
clustering. However since the required number of local centers is very high,
running the clustering algorithm on the entire dataset to obtain these
representational points is very time consuming. To remedy this problem, in this
paper we are proposing two subclustering schemes where by we subdivide the
dataset into smaller sets and run the clustering algorithm on the smaller
datasets to obtain the required number of datapoints to run our clustering
algorithm with. As we are subdividing the given dataset, we could run
clustering algorithm on each smaller piece of the dataset in parallel. We found
that both parallel and serial execution of this method to be much faster than
the original clustering algorithm and error in running the clustering algorithm
on a reduced set to be very less.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1952</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1952</id><created>2014-12-05</created><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Opportunistic Routing for the Vehicular Energy Network</title><categories>cs.SY</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular energy network (VEN) is a vehicular network which can transport
energy over a large geographical area by means of electric vehicles (EVs). In
the near future, an abundance of EVs, plentiful generation of the renewables,
and mature wireless energy transfer and vehicular communication technologies
will expedite the realization of VEN. To transmit energy from a source to a
destination, we need to establish energy paths, which are composed of segments
of vehicular routes, while satisfying various design objectives. In this paper,
we develop a method to construct all energy paths for a particular energy
source-destination pair, followed by some analytical results of the method. We
describe how to utilize the energy paths to develop optimization models for
different design goals and propose two solutions. We also develop a heuristic
for the power loss minimization problem. We compare the performance of the
three solution methods with artificial and real-world traffic networks and
provide a comprehensive comparison in terms of solution quality, computation
time, solvable problem size, and applicability. This paper lays the foundations
of VEN routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1957</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1957</id><created>2014-12-05</created><authors><author><keyname>Ravindran</keyname><forenames>Swarna Kamlam</forenames></author><author><keyname>Mittal</keyname><forenames>Anurag</forenames></author></authors><title>CoMIC: Good features for detection and matching at object boundaries</title><categories>cs.CV</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature or interest points typically use information aggregation in 2D
patches which does not remain stable at object boundaries when there is object
motion against a significantly varying background. Level or iso-intensity
curves are much more stable under such conditions, especially the longer ones.
In this paper, we identify stable portions on long iso-curves and detect
corners on them. Further, the iso-curve associated with a corner is used to
discard portions from the background and improve matching. Such CoMIC (Corners
on Maximally-stable Iso-intensity Curves) points yield superior results at the
object boundary regions compared to state-of-the-art detectors while performing
comparably at the interior regions as well. This is illustrated in exhaustive
matching experiments for both boundary and non-boundary regions in applications
such as stereo and point tracking for structure from motion in video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1961</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1961</id><created>2014-12-05</created><authors><author><keyname>Schwartz</keyname><forenames>Benjamin</forenames></author><author><keyname>N&#xe4;gele</keyname><forenames>Ludwig</forenames></author><author><keyname>Angerer</keyname><forenames>Andreas</forenames></author><author><keyname>MacDonald</keyname><forenames>Bruce A.</forenames></author></authors><title>Towards a graphical language for quadrotor missions</title><categories>cs.RO cs.PL</categories><comments>Presented at DSLRob 2014 (arXiv:cs/1411.7148)</comments><report-no>DSLRob/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for defining Unmanned Aerial Vehicle (UAV)
missions on a high level. Current methods for UAV mission specification are
evaluated and their deficiencies are analyzed. From these findings, a new
graphical specification language for UAV missions is proposed, which is
targeted towards typical UAV users from various domains rather than computer
science experts. The research is ongoing, but a first prototype is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1964</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1964</id><created>2014-12-05</created><authors><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Simplified Erasure/List Decoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of erasure/list decoding using certain classes of
simplified decoders. Specifically, we assume a class of erasure/list decoders,
such that a codeword is in the list if its likelihood is larger than a
threshold. This class of decoders both approximates the optimal decoder of
Forney, and also includes the following simplified subclasses of decoding
rules: The first is a function of the output vector only, but not the codebook
(which is most suitable for high rates), and the second is a scaled version of
the maximum likelihood decoder (which is most suitable for low rates). We
provide single-letter expressions for the exact random coding exponents of any
decoder in these classes, operating over a discrete memoryless channel. For
each class of decoders, we find the optimal decoder within the class, in the
sense that it maximizes the erasure/list exponent, under a given constraint on
the error exponent. We establish the optimality of the simplified decoders of
the first and second kind for low and high rates, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1990</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1990</id><created>2014-12-05</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Baras</keyname><forenames>John. S.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Emergent Behaviors over Signed Random Dynamical Networks:
  Relative-State-Flipping Model</title><categories>cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.5488</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study asymptotic dynamical patterns that emerge among a set of nodes
interacting in a dynamically evolving signed random network, where positive
links carry out standard consensus and negative links induce relative-state
flipping. A sequence of deterministic signed graphs define potential node
interactions that take place independently. Each node receives a positive
recommendation consistent with the standard consensus algorithm from its
positive neighbors, and a negative recommendation defined by relative-state
flipping from its negative neighbors. After receiving these recommendations,
each node puts a deterministic weight to each recommendation, and then encodes
these weighted recommendations in its state update through stochastic
attentions defined by two Bernoulli random variables. We establish a number of
conditions regarding almost sure convergence and divergence of the node states.
We also propose a condition for almost sure state clustering for essentially
weakly balanced graphs, with the help of several martingale convergence lemmas.
Some fundamental differences on the impact of the deterministic weights and
stochastic attentions to the node state evolution are highlighted between the
current relative-state-flipping model and the state-flipping model considered
in Altafini 2013 and Shi et al. 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.1993</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.1993</id><created>2014-12-05</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author></authors><title>On the Optimality of Simple Schedules for Networks with Multiple
  Half-Duplex Relays</title><categories>cs.IT math.IT</categories><comments>This paper is an extension of arXiv:1410.7174. Submitted to IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies networks with N half-duplex relays assisting the
communication between a source and a destination. In ISIT'12 Brahma,
\&quot;{O}zg\&quot;{u}r and Fragouli conjectured that in Gaussian half-duplex diamond
networks (i.e., without a direct link between the source and the destination,
and with N non-interfering relays) an approximately optimal relay scheduling
policy (i.e., achieving the cut-set upper bound to within a constant gap) has
at most N+1 active states (i.e., at most N+1 out of the $2^N$ possible relay
listen-transmit states have a strictly positive probability). Such relay
scheduling policies were referred to as simple. In ITW'13 we conjectured that
simple approximately optimal relay scheduling policies exist for any Gaussian
half-duplex multi-relay network irrespectively of the topology. This paper
formally proves this more general version of the conjecture and shows it holds
beyond Gaussian noise networks. In particular, for any memoryless half-duplex
N-relay network with independent noises and for which independent inputs are
approximately optimal in the cut-set upper bound, an approximately optimal
simple relay scheduling policy exists. A convergent iterative polynomial-time
algorithm, which alternates between minimizing a submodular function and
maximizing a linear program, is proposed to find the approximately optimal
simple relay schedule. As an example, for N-relay Gaussian networks with
independent noises, where each node in equipped with multiple antennas and
where each antenna can be configured to listen or transmit irrespectively of
the others, the existence of an approximately optimal simple relay scheduling
policy with at most N+1 active states is proved. Through a line-network example
it is also shown that independently switching the antennas at each relay can
provide a strictly larger multiplexing gain compared to using the antennas for
the same purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2005</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2005</id><created>2014-12-05</created><authors><author><keyname>Vila</keyname><forenames>Jeremy</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>Adaptive Damping and Mean Removal for the Generalized Approximate
  Message Passing Algorithm</title><categories>cs.IT math.IT</categories><journal-ref>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
  International Conference on Year: 2015 Pages: 2021 - 2025</journal-ref><doi>10.1109/ICASSP.2015.7178325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized approximate message passing (GAMP) algorithm is an efficient
method of MAP or approximate-MMSE estimation of $x$ observed from a noisy
version of the transform coefficients $z = Ax$. In fact, for large zero-mean
i.i.d sub-Gaussian $A$, GAMP is characterized by a state evolution whose fixed
points, when unique, are optimal. For generic $A$, however, GAMP may diverge.
In this paper, we propose adaptive damping and mean-removal strategies that aim
to prevent divergence. Numerical results demonstrate significantly enhanced
robustness to non-zero-mean, rank-deficient, column-correlated, and
ill-conditioned $A$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2007</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2007</id><created>2014-12-05</created><updated>2015-03-18</updated><authors><author><keyname>Jean</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On Using Very Large Target Vocabulary for Neural Machine Translation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural machine translation, a recently proposed approach to machine
translation based purely on neural networks, has shown promising results
compared to the existing approaches such as phrase-based statistical machine
translation. Despite its recent success, neural machine translation has its
limitation in handling a larger vocabulary, as training complexity as well as
decoding complexity increase proportionally to the number of target words. In
this paper, we propose a method that allows us to use a very large target
vocabulary without increasing training complexity, based on importance
sampling. We show that decoding can be efficiently done even with the model
having a very large target vocabulary by selecting only a small subset of the
whole target vocabulary. The models trained by the proposed approach are
empirically found to outperform the baseline models with a small vocabulary as
well as the LSTM-based neural machine translation models. Furthermore, when we
use the ensemble of a few models with very large target vocabularies, we
achieve the state-of-the-art translation performance (measured by BLEU) on the
English-&gt;German translation and almost as high performance as state-of-the-art
English-&gt;French translation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2013</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2013</id><created>2014-12-05</created><authors><author><keyname>Gkounis</keyname><forenames>Dimitrios</forenames><affiliation>Foundation for Research and Technology - Hellas</affiliation></author><author><keyname>Kotronis</keyname><forenames>Vasileios</forenames><affiliation>ETH Zurich, Switzerland</affiliation></author><author><keyname>Dimitropoulos</keyname><forenames>Xenofontas</forenames><affiliation>Foundation for Research and Technology - Hellas</affiliation></author></authors><title>Towards Defeating the Crossfire Attack using SDN</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose online traffic engineering as a novel approach to
detect and mitigate an emerging class of stealthy Denial of Service (DoS)
link-flooding attacks. Our approach exploits the Software Defined Networking
(SDN) paradigm, which renders the management of network traffic more flexible
through centralised flow-level control and monitoring. We implement a full
prototype of our solution on an emulated SDN environment using OpenFlow to
interface with the network devices. We further discuss useful insights gained
from our preliminary experiments as well as a number of open research questions
which constitute work in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2032</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2032</id><created>2014-12-04</created><updated>2015-05-27</updated><authors><author><keyname>Malinowski</keyname><forenames>M. J.</forenames></author><author><keyname>Matsinos</keyname><forenames>E.</forenames></author><author><keyname>Roth</keyname><forenames>S.</forenames></author></authors><title>On using the Microsoft Kinect$^{\rm TM}$ sensors in the analysis of
  human motion</title><categories>physics.med-ph cs.CV cs.RO</categories><comments>29 pages, 1 table, 4 figures. Versions 1 and 2 of the study have been
  retracted due to the use of a non-representative database; save for one
  sentence at the end of the abstract of the current version, versions 3 and 4
  are identical</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper aims at providing the theoretical background required for
investigating the use of the Microsoft Kinect$^{\rm TM}$ (`Kinect', for short)
sensors (original and upgraded) in the analysis of human motion. Our
methodology is developed in such a way that its application be easily adaptable
to comparative studies of other systems used in capturing human-motion data.
Our future plans include the application of this methodology to two situations:
first, in a comparative study of the performance of the two Kinect sensors;
second, in pursuing their validation on the basis of comparisons with a
marker-based system (MBS). One important feature in our approach is the
transformation of the MBS output into Kinect-output format, thus enabling the
analysis of the measurements, obtained from different systems, with the same
software application, i.e., the one we use in the analysis of Kinect-captured
data; one example of such a transformation, for one popular marker-placement
scheme (`Plug-in Gait'), is detailed. We propose that the similarity of the
output, obtained from the different systems, be assessed on the basis of the
comparison of a number of waveforms, representing the variation within the gait
cycle of quantities which are commonly used in the modelling of the human
motion. The data acquisition may involve commercially-available treadmills and
a number of velocity settings: for instance, walking-motion data may be
acquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommend
that particular attention be called to systematic effects associated with the
subject's knee and lower leg, as well as to the ability of the Kinect sensors
in reliably capturing the details in the asymmetry of the motion for the left
and right parts of the human body. The previous versions of the study have been
withdrawn due to the use of a non-representative database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2062</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2062</id><created>2014-12-05</created><authors><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author></authors><title>Dynamic Models of Reputation and Competition in Job-Market Matching</title><categories>cs.GT</categories><doi>10.1145/2688073.2688091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental decision faced by a firm hiring employees - and a familiar one
to anyone who has dealt with the academic job market, for example - is deciding
what caliber of candidates to pursue. Should the firm try to increase its
reputation by making offers to higher-quality candidates, despite the risk that
the candidates might reject the offers and leave the firm empty-handed? Or
should it concentrate on weaker candidates who are more likely to accept the
offer? The question acquires an added level of complexity once we take into
account the effect one hiring cycle has on the next: hiring better employees in
the current cycle increases the firm's reputation, which in turn increases its
attractiveness for higher-quality candidates in the next hiring cycle. These
considerations introduce an interesting temporal dynamic aspect to the rich
line of research on matching models for job markets, in which long-range
planning and evolving reputational effects enter into the strategic decisions
made by competing firms.
  We develop a model based on two competing firms to try capturing as cleanly
as possible the elements that we believe constitute the strategic tension at
the core of the problem: the trade-off between short-term recruiting success
and long-range reputation-building; the inefficiency that results from
underemployment of people who are not ranked highest; and the influence of
earlier accidental outcomes on long-term reputations.
  Our model exhibits all these phenomena in a stylized setting, governed by a
parameter q that captures the difference in strength between the two top
candidates in each hiring cycle. We show that when q is relatively low the
efficiency of the job market is improved by long-range reputational effects,
but when q is relatively high, taking future reputations into account can
sometimes reduce the efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2064</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2064</id><created>2014-12-05</created><authors><author><keyname>Miranda</keyname><forenames>F&#xe9;lix A.</forenames></author><author><keyname>Casta&#xf1;os</keyname><forenames>Fernando</forenames></author></authors><title>Robust Output Regulation of Linear Passive Systems with Multivalued
  Upper Semicontinuous Controls</title><categories>cs.SY math.DS</categories><comments>10 pages, 9 figures</comments><msc-class>93C10, 93D09, 34A60, 65K15, 90C33</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of multivalued controls derived from a special maximal monotone
operator are studied in this note. Starting with a strictly passive linear
system (with possible parametric uncertainty and external disturbances) a
multivalued control law is derived, ensuring regulation of the output to a
desired value. The methodology used falls in a passivity-based control context,
where we study how the multivalued control affects the dissipation equation of
the closed-loop system, from which we derive its robustness properties.
Finally, some numerical examples together with implementation issues are
presented to support the main result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2066</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2066</id><created>2014-12-05</created><updated>2014-12-09</updated><authors><author><keyname>Wang</keyname><forenames>Shaofei</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless C.</forenames></author></authors><title>Learning Multi-target Tracking with Quadratic Object Interactions</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a model for multi-target tracking based on associating
collections of candidate detections across frames of a video. In order to model
pairwise interactions between different tracks, such as suppression of
overlapping tracks and contextual cues about co-occurence of different objects,
we augment a standard min-cost flow objective with quadratic terms between
detection variables. We learn the parameters of this model using structured
prediction and a loss function which approximates the multi-target tracking
accuracy. We evaluate two different approaches to finding an optimal set of
tracks under model objective based on an LP relaxation and a novel greedy
extension to dynamic programming that handles pairwise interactions. We find
the greedy algorithm achieves equivalent performance to the LP relaxation while
being 2-7x faster than a commercial solver. The resulting model with learned
parameters outperforms existing methods across several categories on the KITTI
tracking benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2067</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2067</id><created>2014-11-20</created><authors><author><keyname>May</keyname><forenames>Victor</forenames></author><author><keyname>Keller</keyname><forenames>Yosi</forenames></author><author><keyname>Sharon</keyname><forenames>Nir</forenames></author><author><keyname>Shkolnisky</keyname><forenames>Yoel</forenames></author></authors><title>An algorithm for improving Non-Local Means operators via low-rank
  approximation</title><categories>cs.CV math.GM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for improving a Non Local Means operator by computing its
low-rank approximation. The low-rank operator is constructed by applying a
filter to the spectrum of the original Non Local Means operator. This results
in an operator which is less sensitive to noise while preserving important
properties of the original operator. The method is efficiently implemented
based on Chebyshev polynomials and is demonstrated on the application of
natural images denoising. For this application, we provide a comprehensive
comparison of our method with leading denoising methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2070</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2070</id><created>2014-12-05</created><authors><author><keyname>Rodrigues</keyname><forenames>Jo&#xe3;o G. P.</forenames></author><author><keyname>Aguiar</keyname><forenames>Ana</forenames></author><author><keyname>Barros</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>SenseMyCity: Crowdsourcing an Urban Sensor</title><categories>cs.CY</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People treat smartphones as a second skin, having them around nearly 24/7 and
constantly interacting with them. Although smartphones are used mainly for
personal communication, social networking and web browsing, they have many
connectivity capabilities, and are at the same time equipped with a wide range
of embedded sensors. Additionally, bluetooth connectivity can be leveraged to
collect data from external sensors, greatly extending the sensing capabilities.
However, massive data-gathering using smartphones still poses many
architectural challenges, such as limited battery and processing power, and
possibly connectivity costs.
  This article describes SenseMyCity (SMC), an Internet of Things mobile urban
sensor that is extensible and fully configurable. The platform consists of an
app, a backoffice and a frontoffice. The SMC app can collect data from embedded
sensors, like GPS, wifi, accelerometer, magnetometer, etc, as well as from
external bluetooth sensors, ranging from On-Board Diagnostics gathering data
from vehicles, to wearable cardiac sensors. Adding support for new internal or
external sensors is straightforward due to the modular architecture. Data
transmission to our servers can occur either on-demand or in real-time, while
keeping costs down by only using the configured type of Internet connectivity.
We discuss our experience implementing the platform and using it to make
longitudinal studies with many users. Further, we present results on bandwidth
utilization and energy consumption for different sensors and sampling rates.
Finally, we show two use cases: mapping fuel consumption and user stress
extracted from cardiac sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2078</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2078</id><created>2014-12-03</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author></authors><title>The myth of the Digital Earth between fragmentation and wholeness</title><categories>cs.CY</categories><comments>Web: http://wi.mobilities.ca/myth-of-the-digital-earth</comments><journal-ref>Journal of Mobile Media. 08.02 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Daring predictions of the proximate future can establish shared discursive
frameworks, mobilize capital, and steer complex processes. Among the prophetic
visions that encouraged and accompanied the development of new communication
technologies was the &quot;Digital Earth,&quot; described in a 1998 speech by Al Gore as
a high-resolution representation of the planet to share and analyze detailed
information about its state. This article traces a genealogy of the Digital
Earth as a techno-scientific myth, locating it in a constellation of media
futures, arguing that a common subtext of these envisionments consists of a
dream of wholeness, an afflatus to overcome perceived fragmentation among
humans, and between humans and the Earth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2087</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2087</id><created>2014-12-05</created><authors><author><keyname>Li</keyname><forenames>Yingzhe</forenames></author><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Statistical Modeling and Probabilistic Analysis of Cellular Networks
  with Determinantal Point Processes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the Poisson point process (PPP) has been widely used to model base
station (BS) locations in cellular networks, it is an idealized model that
neglects the spatial correlation among BSs. The present paper proposes the use
of determinantal point process (DPP) to take into account these correlations;
in particular the repulsiveness among macro base station locations. DPPs are
demonstrated to be analytically tractable by leveraging several unique
computational properties. Specifically, we show that the empty space function,
the nearest neighbor function, the mean interference and the
signal-to-interference ratio (SIR) distribution have explicit analytical
representations and can be numerically evaluated for cellular networks with DPP
configured BSs. In addition, the modeling accuracy of DPPs is investigated by
fitting three DPP models to real BS location data sets from two major U.S.
cities. Using hypothesis testing for various performance metrics of interest,
we show that these fitted DPPs are significantly more accurate than popular
choices such as the PPP and the perturbed hexagonal grid model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2105</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2105</id><created>2014-12-05</created><updated>2015-02-15</updated><authors><author><keyname>Ramos</keyname><forenames>Arthur</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Anjolina G.</forenames></author></authors><title>Sequences of Rewrites: A Categorical Interpretation</title><categories>cs.LO math.CT</categories><comments>13 pages, submitted to a scientific conference (WoLLIC 2015);
  corrected typos; Moved part of Section 2.4 to the appendix; corrected small
  issues in Section 3 (typos and some compositions order), results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Martin-L\&quot;of's Intensional Type Theory, identity type is a heavily used
and studied concept. The reason for that is the fact that it's responsible for
the recently discovered connection between Type Theory and Homotopy Theory. The
main problem with identity types, as originally formulated, is that they are
complex to understand and use. Using that fact as motivation, a much simpler
formulation for the identity type was proposed by Queiroz &amp; Gabbay (1994) and
further developed by de Queiroz &amp; de Oliveira (2013). In this formulation, an
element of an identity type is seen as a sequence of rewrites (or computational
paths). Together with the logical rules of this new entity, there exists a
system of reduction rules between sequence of rewrites called LND_{EQS}-RWS.
This system is constructed using the labelled natural deduction (i.e. Prawitz'
Natural Deduction plus derivations-as-terms) and is responsible for
establishing how a sequence of rewrites can be rewritten, resulting in a new
sequence of rewrites. In this context, we propose a categorical interpretation
for this new entity, using the types as objects and the rules of rewrites as
morphisms. Moreover, we show that our interpretation is in accordance with some
known results, like that types have a groupoidal structure. We also interpret
more complicated structures, like the one formed by a rewrite of a sequence of
rewrites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2106</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2106</id><created>2014-12-05</created><authors><author><keyname>Kot&#x142;owski</keyname><forenames>Wojciech</forenames></author></authors><title>Consistent optimization of AMS by logistic loss minimization</title><categories>cs.LG</categories><comments>9 pages, HEPML workshop at NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we theoretically justify an approach popular among
participants of the Higgs Boson Machine Learning Challenge to optimize
approximate median significance (AMS). The approach is based on the following
two-stage procedure. First, a real-valued function is learned by minimizing a
surrogate loss for binary classification, such as logistic loss, on the
training sample. Then, a threshold is tuned on a separate validation sample, by
direct optimization of AMS. We show that the regret of the resulting
(thresholded) classifier measured with respect to the squared AMS, is
upperbounded by the regret of the underlying real-valued function measured with
respect to the logistic loss. Hence, we prove that minimizing logistic
surrogate is a consistent method of optimizing AMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2109</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2109</id><created>2014-12-05</created><updated>2015-03-18</updated><authors><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Korhonen</keyname><forenames>Janne H.</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Algebrisation in Distributed Graph Algorithms: Fast Matrix
  Multiplication in the Congested Clique</title><categories>cs.DC cs.DS</categories><comments>This paper has been withdrawn by the authors. This paper has been
  superseded by arXiv:1503.04963 (merged from arXiv:1412.2109 and
  arXiv:1412.2667)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While algebrisation constitutes a powerful technique in the design and
analysis of centralised algorithms, to date there have been hardly any
applications of algebraic techniques in the context of distributed graph
algorithms. This work is a case study that demonstrates the potential of
algebrisation in the distributed context. We will focus on distributed graph
algorithms in the congested clique model; the graph problems that we will
consider include, e.g., the triangle detection problem and the all-pairs
shortest path problem (APSP). There is plenty of prior work on combinatorial
algorithms in the congested clique model: for example, Dolev et al. (DISC 2012)
gave an algorithm for triangle detection with a running time of $\tilde
O(n^{1/3})$, and Nanongkai (STOC 2014) gave an approximation algorithm for APSP
with a running time of $\tilde O(n^{1/2})$. In this work, we will use algebraic
techniques -- in particular, algorithms based on fast matrix multiplication --
to solve both triangle detection and the unweighted APSP in time
$O(n^{0.15715})$; for weighted APSP, we give a $(1+o(1))$-approximation with
this running time, as well as an exact $\tilde O(n^{1/3})$ solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2113</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2113</id><created>2014-12-05</created><updated>2015-04-07</updated><authors><author><keyname>Gunasekar</keyname><forenames>Suriya</forenames></author><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Yin</keyname><forenames>Dawei</forenames></author><author><keyname>Chang</keyname><forenames>Yi</forenames></author></authors><title>Consistent Collective Matrix Completion under Joint Low Rank Structure</title><categories>stat.ML cs.LG</categories><comments>19 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the collective matrix completion problem of jointly recovering a
collection of matrices with shared structure from partial (and potentially
noisy) observations. To ensure well--posedness of the problem, we impose a
joint low rank structure, wherein each component matrix is low rank and the
latent space of the low rank factors corresponding to each entity is shared
across the entire collection. We first develop a rigorous algebra for
representing and manipulating collective--matrix structure, and identify
sufficient conditions for consistent estimation of collective matrices. We then
propose a tractable convex estimator for solving the collective matrix
completion problem, and provide the first non--trivial theoretical guarantees
for consistency of collective matrix completion. We show that under reasonable
assumptions stated in Section 3.1, with high probability, the proposed
estimator exactly recovers the true matrices whenever sample complexity
requirements dictated by Theorem 1 are met. The sample complexity requirement
derived in the paper are optimum up to logarithmic factors, and significantly
improve upon the requirements obtained by trivial extensions of standard matrix
completion. Finally, we propose a scalable approximate algorithm to solve the
proposed convex program, and corroborate our results through simulated
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2114</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2114</id><created>2014-12-01</created><authors><author><keyname>Ohira</keyname><forenames>Toru</forenames></author></authors><title>Chases and Escapes, and Optimization Problems</title><categories>cs.AI</categories><comments>3 pages, 4 figures. To appear in the Proceedings of the International
  Symposium on Artificial Life and Robotics (AROB20th), Beppu, Oita Japan,
  January 21-23, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for solving combinatorial optimization problem by
utilizing the mechanism of chases and escapes, which has a long history in
mathematics. In addition to the well-used steepest descent and neighboring
search, we perform a chase and escape game on the &quot;landscape&quot; of the cost
function. We have created a concrete algorithm for the Traveling Salesman
Problem. Our preliminary test indicates a possibility that this new fusion of
chases and escapes problem into combinatorial optimization search is fruitful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2115</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2115</id><created>2014-12-04</created><authors><author><keyname>Ashton</keyname><forenames>Abi J.</forenames></author><author><keyname>Russo</keyname><forenames>Pedro</forenames></author><author><keyname>Heenatigala</keyname><forenames>Thilina</forenames></author></authors><title>Crowdfunding Astronomy Outreach Projects: Lessons Learned from the UNAWE
  Crowdfunding Campaign</title><categories>cs.CY astro-ph.IM physics.ed-ph physics.soc-ph</categories><comments>Published - Communicating Astronomy with the Public journal #16 (4
  pages) (2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In recent years, crowdfunding has become a popular method of funding new
technology or entertainment products, or artistic projects. The idea is that
people or projects ask for many small donations from individuals who support
the proposed work, rather than a large amount from a single source.
Crowdfunding is usually done via an online portal or platform which handles the
financial transactions involved. The Universe Awareness (UNAWE) programme
decided to undertake a Kickstarter crowdfunding campaign centring on the
resource Universe in a Box2. In this article we present the lessons learned and
best practices from that campaign.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2118</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2118</id><created>2014-12-05</created><updated>2014-12-30</updated><authors><author><keyname>Bonelli</keyname><forenames>Eduardo</forenames></author><author><keyname>Kesner</keyname><forenames>Delia</forenames></author><author><keyname>Lombardi</keyname><forenames>Carlos</forenames></author><author><keyname>Rios</keyname><forenames>Alejandro</forenames></author></authors><title>An abstract normalisation result with applications to non-sequential
  calculi</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study normalisation of multistep strategies, strategies that reduce a set
of redexes at a time, focussing on the notion of necessary sets, those which
contain at least one redex that cannot be avoided in order to reach a normal
form. This is particularly appealing in the setting of non-sequential rewrite
systems, in which terms that are not in normal form may not have any
\emph{needed} redex. We first prove a normalisation theorem for abstract
rewrite systems or ARS, a general rewriting framework encompassing many
rewriting systems developed by P-A.Mellies. The theorem states that multistep
strategies reducing so called necessary and non-gripping sets of redexes at a
time are normalising in any ARS. Gripping refers to an abstract property
reflecting the behavior of higher-order substitution. We then apply this result
to the particular case of the Pure Pattern Calculus, a calculus of patterns and
to the lambda-calculus with parallel-or.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2122</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2122</id><created>2014-11-25</created><updated>2015-01-19</updated><authors><author><keyname>Ponce-L&#xf3;pez</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>Escalera</keyname><forenames>Sergio</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Marc</forenames></author><author><keyname>Jan&#xe9;s</keyname><forenames>Oriol</forenames></author><author><keyname>Bar&#xf3;</keyname><forenames>Xavier</forenames></author></authors><title>Non-Verbal Communication Analysis in Victim-Offender Mediations</title><categories>cs.HC cs.AI cs.CY</categories><comments>Please, find the supplementary video material at:
  http://sunai.uoc.edu/~vponcel/video/VOMSessionSample.mp4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a non-invasive ambient intelligence framework for
the semi-automatic analysis of non-verbal communication applied to the
restorative justice field. In particular, we propose the use of computer vision
and social signal processing technologies in real scenarios of Victim-Offender
Mediations, applying feature extraction techniques to multi-modal
audio-RGB-depth data. We compute a set of behavioral indicators that define
communicative cues from the fields of psychology and observational methodology.
We test our methodology on data captured in real world Victim-Offender
Mediation sessions in Catalonia in collaboration with the regional government.
We define the ground truth based on expert opinions when annotating the
observed social responses. Using different state-of-the-art binary
classification approaches, our system achieves recognition accuracies of 86%
when predicting satisfaction, and 79% when predicting both agreement and
receptivity. Applying a regression strategy, we obtain a mean deviation for the
predictions between 0.5 and 0.7 in the range [1-5] for the computed social
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2123</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2123</id><created>2014-12-05</created><authors><author><keyname>Hwang</keyname><forenames>Dawsen</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengyuan</forenames></author></authors><title>Distributed Multi-Depot Routing without Communications</title><categories>cs.DC cs.DS</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider and formulate a class of distributed multi-depot routing
problems, where servers are to visit a set of requests, with the aim of
minimizing the total distance travelled by all servers. These problems fall
into two categories: distributed offline routing problems where all the
requests that need to be visited are known from the start; distributed online
routing problems where the requests come to be known incrementally. A critical
and novel feature of our formulations is that communications are not allowed
among the servers, hence posing an interesting and challenging question: what
performance can be achieved in comparison to the best possible solution
obtained from an omniscience planner with perfect communication capabilities?
The worst-case (over all possible request-set instances) performance metrics
are given by the approximation ratio (offline case) and the competitive ratio
(online case).
  Our first result indicates that the online and offline problems are
effectively equivalent: for the same request-set instance, the approximation
ratio and the competitive ratio differ by at most an additive factor of 2,
irrespective of the release dates in the online case. Therefore, we can
restrict our attention to the offline problem. For the offline problem, we show
that the approximation ratio given by the Voronoi partition is m (the number of
servers). For two classes of depot configurations, when the depots form a line
and when the ratios between the distances of pairs of depots are upper bounded
by a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes
with sublinear approximation ratios O(log m) and {\Theta}(f(m)) respectively.
We also discuss several interesting open problems in our formulations: in
particular, how our initial results (on the two deliberately chosen classes of
depots) shape our conjecture on the open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2144</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2144</id><created>2014-12-05</created><authors><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Nowzari</keyname><forenames>Cameron</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Data-Driven Allocation of Vaccines for Controlling Epidemic Outbreaks</title><categories>math.OC cs.SI</categories><comments>Submitted to the IEEE Transactions on Network Science and Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a mathematical framework, based on conic geometric programming, to
control a susceptible-infected-susceptible viral spreading process taking place
in a directed contact network with unknown contact rates. We assume that we
have access to time series data describing the evolution of the spreading
process observed by a collection of sensor nodes over a finite time interval.
We propose a data-driven robust convex optimization framework to find the
optimal allocation of protection resources (e.g., vaccines and/or antidotes) to
eradicate the viral spread at the fastest possible rate. In contrast to current
network identification heuristics, in which a single network is identified to
explain the observed data, we use available data to define an uncertainty set
containing all networks that are coherent with empirical observations. Our
characterization of this uncertainty set of networks is tractable in the
context of conic geometric programming, recently proposed by Chandrasekaran and
Shah, which allows us to efficiently find the optimal allocation of resources
to control the worst-case spread that can take place in the uncertainty set of
networks. We illustrate our approach in a transportation network from which we
collect partial data about the dynamics of a hypothetical epidemic outbreak
over a finite period of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2154</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2154</id><created>2014-12-03</created><updated>2015-04-21</updated><authors><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Liu</keyname><forenames>Jiajun</forenames></author><author><keyname>AbouJaoude</keyname><forenames>Maurice</forenames></author><author><keyname>Cameron</keyname><forenames>Mark</forenames></author><author><keyname>Newth</keyname><forenames>David</forenames></author></authors><title>Understanding Human Mobility from Twitter</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 6 Figures</comments><journal-ref>PLoS ONE 10(7): e0131469 (2015)</journal-ref><doi>10.1371/journal.pone.0131469</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding human mobility is crucial for a broad range of applications
from disease prediction to communication networks. Most efforts on studying
human mobility have so far used private and low resolution data, such as call
data records. Here, we propose Twitter as a proxy for human mobility, as it
relies on publicly available data and provides high resolution positioning when
users opt to geotag their tweets with their current location. We analyse a
Twitter dataset with more than six million geotagged tweets posted in
Australia, and we demonstrate that Twitter can be a reliable source for
studying human mobility patterns. Our analysis shows that geotagged tweets can
capture rich features of human mobility, such as the diversity of movement
orbits among individuals and of movements within and between cities. We also
find that short and long-distance movers both spend most of their time in large
metropolitan areas, in contrast with intermediate-distance movers movements,
reflecting the impact of different modes of travel. Our study provides solid
evidence that Twitter can indeed be a useful proxy for tracking and predicting
human movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2155</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2155</id><created>2014-12-04</created><updated>2015-11-29</updated><authors><author><keyname>Khor</keyname><forenames>Susan</forenames></author></authors><title>Protein residue networks from a local search perspective</title><categories>q-bio.MN cs.CE cs.SI</categories><comments>v5 has 74 pages, a correction in section 2.1, an expansion of
  Appendix B and the addition of Appendix H. Only materials and results related
  to sections 3.1 to 3.6 have been published in the journal article which has
  the same title as this manuscript. Materials and results from section 3.7 and
  section 4 are each expanded in other manuscripts, Journal of Complex Networks
  (2015)</comments><doi>10.1093/comnet/cnv014</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We examined protein residue networks (PRNs) from a local search perspective
to understand why PRNs are highly clustered when having short paths is
important for protein functionality. We found that by adopting a local search
perspective, this conflict between form and function is resolved as increased
clustering actually helps to reduce path length in PRNs. Further, the paths
found via our EDS local search algorithm are more congruent with the
characteristics of intra-protein communication. EDS identifies a subset of PRN
edges called short-cuts that are distinct, have high usage, impacts EDS path
length, diversity and stretch, and are dominated by short-range contacts. The
short-cuts form a network (SCN) that increases in size and transitivity as a
protein folds. The structure of a SCN supports its function and formation, and
the function of a SCN influences its formation. Several significant differences
in terms of SCN structure, function and formation is found between successful
and unsuccessful MD trajectories. By connecting the static and the dynamic
aspects of PRNs, the protein folding process becomes a problem of graph
formation with the purpose of forming suitable pathways within proteins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2158</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2158</id><created>2014-12-05</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author><author><keyname>Skelton</keyname><forenames>Gordon</forenames></author></authors><title>A Two-layer Architecture of Mobile Sinks and Static Sensors</title><categories>cs.NI</categories><comments>11 pages, 1 figure</comments><journal-ref>Proceedings of the 15th International Conference on Advanced
  Computing and Communication, pp. 249-254, Guwahathi, India, December 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a two-layer mobile sink and static sensor network (MSSSN)
architecture for large scale wireless sensor networks. The top layer is a
mobile ad hoc network of resource-rich sink nodes while the bottom layer is a
network of static resource-constrained sensor nodes. The MSSSN architecture can
be implemented at a lower cost with the currently available IEEE 802.11 devices
that only use a single halfduplex transceiver. Each sink node is assigned a
particular region to monitor and collect data. A sink node moves to the
vicinity of the sensor nodes (within a few hops) to collect data. The collected
data is exchanged with peer mobile sinks. Thus, the MSSSN architecture provides
scalability, extends sensor lifetime by letting them operate with limited
transmission range and provides connectivity between isolated regions of sensor
nodes. In order to provide fault tolerance, more than one mobile sink could be
collecting data from a given region or a mobile sink could collect data from
more than one region. In the later half of the paper, we discuss several open
research issues that need to be addressed while implementing the MSSSN
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2161</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2161</id><created>2014-12-05</created><authors><author><keyname>Babatunji</keyname><forenames>Omoniwa</forenames></author></authors><title>Considering Fading Effects for Vertical Handover in Heterogenous
  Wireless Networks</title><categories>cs.NI</categories><comments>This work is the Thesis of Omoniwa Babatunji submitted to COMSATS
  Institute of Information Technology as partial fulfillment of the requirement
  for the award of MS in Computer Engineering</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Over the years, vertical handover has attracted the interest of numerous
researchers. Despite the attractive benefits of integrating different wireless
platforms, mobile users are confronted with the issue of detrimental handover.
As a mobile node (MN) moves within a heterogeneous environment, satisfactory
quality of service (QoS) is desired by ensuring efficient vertical handover.
This demands not only the efficient execution of vertical handover, but also
optimized pre-handover decisions, such as: handover necessity estimation (HNE),
handover triggering condition estimation (HTCE) and handover target selection
(HTS). The existing works on HNE and HTCE optimization considered the coverage
region of a point of attachment to be circular, ignoring the fading effect.
This paper considers the effect of shadow fading and used extensive geometric
and probability analysis in modelling the coverage area of a WLAN cell. Thus,
presents a realistic and novel model with an attempt to ensure optimal handover
as a mobile node (MN) traverses a heterogeneous wireless environment.
Monte-Carlo simulations were carried out to show the behavior of the proposed
models. Results were validated by comparing the proposed models with existing
works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2168</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2168</id><created>2014-12-05</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author><author><keyname>Skelton</keyname><forenames>Gordon</forenames></author></authors><title>An Energy Efficient Risk Notification Message Dissemination Protocol for
  Vehicular Ad hoc Networks</title><categories>cs.NI</categories><comments>15 pages, 15 figures</comments><journal-ref>IAENG International Journal of Computer Science, vol. 37, no. 1,
  pp. 1 - 10, March 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the design and development of an energy-efficient Risk
Notification Message Dissemination Protocol (RNMDP) for vehicular ad hoc
networks (VANETs). RNMDP propagates Risk Notification Messages (RNMs) from
their location of origin (called the Risk Zone) to vehicles approaching the
Risk Zone. RNMDP assumes each node is aware of its current location in the
network. The protocol works as follows: A RNM is broadcast in the neighborhood
of the Risk Zone. A node receiving the RNM from another node waits for a
Rebroadcast-Wait-Time before deciding to rebroadcast the message. The
Rebroadcast-Wait-Time for a node is modeled based on the ratio of the distance
between the node and the immediate sender of the RNM and the direction of
movement of the node. Priority for rebroadcast is given for nodes farthest away
from the sender and traveling towards the Risk Zone. Nodes that are traveling
in lanes in direction away from the Risk Zone are also considered for
rebroadcast, albeit with a larger Rebroadcast-Wait-Time. During the
Rebroadcast-Wait-Time, if a node hears the same RNM again rebroadcast in the
neighborhood, then the node stops from further broadcasting the message. If a
node does not hear the RNM in its neighborhood during the RebroadcastWait-Time,
the node broadcasts the message in its neighborhood. A RNM is considered to
have been delivered to all the vehicles in the road, if the message reaches the
Target Zone. The performance of RNMDP has been compared with that of the
commonly used flooding strategy through extensive simulations conducted for
highway networks with different number of lanes and lane density. Simulation
results indicate that with a slightly larger delay (i.e., no more than 35% of
the delay incurred for flooding), RNMDP can achieve the same message delivery
ratio attained by flooding, but at a relatively much lower energy loss compared
to flooding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2170</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2170</id><created>2014-11-27</created><updated>2015-03-29</updated><authors><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Opinion control in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>7 figures, 1 table</comments><journal-ref>New Journal of Physics 17, 033031 (2015)</journal-ref><doi>10.1088/1367-2630/17/3/033031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many instances of election, the electorate appears to be a composite of
partisan and independent voters. Given that partisans are not likely to convert
to a different party, a main goal for a party could be to mobilize independent
voters toward the party with the help of strong leadership, mass media,
partisans, and effects of peer-to-peer influence. Based on the exact solution
of the classical voter model dynamics in the presence of perfectly partisan
voters (i.e., zealots), we propose a computational method to maximize the share
of the party in a social network of independent voters by pinning control
strategy. The party, corresponding to the controller or zealots, optimizes the
nodes to be controlled given the information about the connectivity of
independent voters and the set of nodes that the opponent party controls. We
show that controlling hubs is generally a good strategy, whereas the optimized
strategy is even better. The superiority of the optimized strategy is
particularly eminent when the independent voters are connected as directed
rather than undirected networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2176</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2176</id><created>2014-12-05</created><authors><author><keyname>Arevalo</keyname><forenames>L.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>R.</forenames></author></authors><title>Multi-Branch Lattice-Reduction SIC for Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>7 figures, ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new detection technique for multiuser
multiple-input multiple-output (MU-MIMO) systems. The proposed scheme combines
a lattice reduction (LR) transformation, which makes the channel matrix nearly
orthogonal, and then employs a multi-branch (MB) technique with successive
interference cancellation (SIC). A single LR transformation is required for the
receive filters of all branches in the scheme, which proposes a different
ordering for each branch and generates a list of detection candidates. The best
vector of estimated symbols is chosen according to the maximum likelihood (ML)
selection criterion. Simulation results show that the proposed detection
structure has a near-optimal performance while the computational complexity is
much lower than that of the ML detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2179</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2179</id><created>2014-12-05</created><updated>2014-12-09</updated><authors><author><keyname>Xu</keyname><forenames>Yi</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author><author><keyname>Su</keyname><forenames>Xin</forenames></author></authors><title>Interference Alignment Improves the Capacity of OFDM Systems</title><categories>cs.IT math.IT</categories><doi>10.1109/TVT.2015.2402191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user Orthogonal Frequency Division Multiplexing (OFDM) and Multiple
Output Multiple Output (MIMO) have been widely adopted to enhance the system
throughput and combat the detrimental effects of wireless channels. Recently,
interference alignment was proposed to exploit interference to enable
concurrent transmissions of multiple signals. In this paper, we investigate how
to combine these techniques to further enhance the system throughput. We first
reveal the unique characteristics and challenges brought about by using
interference alignment in diagonal channels. We then derive a performance bound
for the multi-user (MIMO) OFDM/interference alignment system under practical
constraints, and show how to achieve this bound with a decomposition approach.
The superior performance of the proposed scheme is validated with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2181</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2181</id><created>2014-12-05</created><authors><author><keyname>Babatunji</keyname><forenames>Omoniwa</forenames></author><author><keyname>Hussain</keyname><forenames>Riaz</forenames></author></authors><title>Dwell Time Prediction Model for Minimizing Unnecessary Handovers in
  Heterogenous Wireless Networks, Considering Amoebic Shaped Coverage Region</title><categories>cs.NI</categories><comments>Paper Submitted to Springer Journal of Mobile Networks and
  Applications on 26th September, 2014. Currently under review</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Over the years, vertical handover necessity estimation has attracted the
interest of numerous researchers. Despite the attractive benefits of
integrating different wireless platforms, mobile users are confronted with the
issue of detrimental handover. This paper used extensive geometric and
probability analysis in modelling the coverage area of a WLAN cell. Thus,
presents a realistic and novel model with an attempt to minimize unnecessary
handover and handover failure of a mobile node (MN) traversing the WLAN cell
from a third generation (3G) network. The dwell time is estimated along with
the threshold values to ensure an optimal handover decision by the MN, while
the probability of unnecessary handover and handover failure are kept within
tolerable bounds. Monte-Carlo simulations were carried out to show the behavior
of the proposed model. Results were validated by comparing this model with
existing models for unnecessary handover minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2186</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2186</id><created>2014-12-05</created><authors><author><keyname>Owda</keyname><forenames>Hasan M. H.</forenames></author><author><keyname>Omoniwa</keyname><forenames>Babatunji</forenames></author><author><keyname>Shahid</keyname><forenames>Ahmad R.</forenames></author><author><keyname>Ziauddin</keyname><forenames>Sheikh</forenames></author></authors><title>Using Artificial Neural Network Techniques for Prediction of Electric
  Energy Consumption</title><categories>cs.NE cs.AI</categories><comments>10 pages, 5 figures, Journal</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Due to imprecision and uncertainties in predicting real world problems,
artificial neural network (ANN) techniques have become increasingly useful for
modeling and optimization. This paper presents an artificial neural network
approach for forecasting electric energy consumption. For effective planning
and operation of power systems, optimal forecasting tools are needed for energy
operators to maximize profit and also to provide maximum satisfaction to energy
consumers. Monthly data for electric energy consumed in the Gaza strip was
collected from year 1994 to 2013. Data was trained and the proposed model was
validated using 2-Fold and K-Fold cross validation techniques. The model has
been tested with actual energy consumption data and yields satisfactory
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2188</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2188</id><created>2014-12-05</created><updated>2015-11-30</updated><authors><author><keyname>Takeichi</keyname><forenames>Yuki</forenames></author><author><keyname>Sasahara</keyname><forenames>Kazutoshi</forenames></author><author><keyname>Suzuki</keyname><forenames>Reji</forenames></author><author><keyname>Arita</keyname><forenames>Takaya</forenames></author></authors><title>Concurrent Bursty Behavior of Social Sensors in Sporting Events</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 8 figures</comments><journal-ref>PLoS ONE 2015, 10(12): e0144646</journal-ref><doi>10.1371/journal.pone.0144646</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of social media expands our ability to transmit information and
connect with others instantly, which enables us to behave as &quot;social sensors.&quot;
Here, we studied concurrent bursty behavior of Twitter users during major
sporting events to determine their function as social sensors. We show that the
degree of concurrent bursts in tweets (posts) and retweets (re-posts) works as
a strong indicator of winning or losing a game. More specifically, our simple
tweet analysis of Japanese professional baseball games in 2013 revealed that
social sensors can immediately react to positive and negative events through
bursts of tweets, but that positive events are more likely to induce a
subsequent burst of retweets. We also show that these findings hold true across
cultures by analyzing tweets related to Major League Baseball games in 2015.
Furthermore, we demonstrate active interactions among social sensors by
constructing retweet networks during a baseball game. The resulting networks
commonly exhibited user clusters depending on the baseball team, with a
scale-free connectedness that is indicative of a substantial difference in user
popularity as an information source. While previous studies have mainly focused
on bursts of tweets as a simple indicator of a real-world event, the temporal
correlation between tweets and retweets implies unique aspects of social
sensors, offering new insights into human behavior in a highly connected world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2190</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2190</id><created>2014-12-05</created><authors><author><keyname>Lin</keyname><forenames>Yuansheng</forenames></author><author><keyname>Li</keyname><forenames>Daqing</forenames></author><author><keyname>Kang</keyname><forenames>Rui</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Robustness of networks with topologies of dependency links</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness of complex networks with dependencies has been studied in
recent years. However, previous studies focused on the robustness of networks
composed of dependency links without network topology. In this study, we will
analyze the percolation properties of a realistic network model where
dependency links follow certain network topology. We perform the theoretical
analysis and numerical simulations to show the critical effects of topology of
dependency links on robustness of complex networks. For Erd\&quot;os-R\'enyi (ER)
connectivity network, we find that the system with dependency of RR topology is
more vulnerable than system with dependency of ER topology. And RR-RR (i.e.
random-regular (RR) network with dependency of RR topology) disintegrates in an
abrupt transition. In particular, we find that the system of RR-ER shows
different types of phase transitions. For system of different combinations, the
type of percolation depends on the interaction between connectivity topology
and dependency topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2192</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2192</id><created>2014-12-05</created><authors><author><keyname>Seroussi</keyname><forenames>Gadiel</forenames></author><author><keyname>Weinberger</keyname><forenames>Marcelo J.</forenames></author></authors><title>Optimal algorithms for universal random number generation from finite
  memory sources</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Trans. Info. Theory, Vol. 61, No. 3, 1277-1297, March 2015</journal-ref><doi>10.1109/TIT.2014.2386860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study random number generators (RNGs), both in the fixed to
variable-length (FVR) and the variable to fixed-length (VFR) regimes, in a
universal setting in which the input is a finite memory source of arbitrary
order and unknown parameters, with arbitrary input and output (finite) alphabet
sizes. Applying the method of types, we characterize essentially unique optimal
universal RNGs that maximize the expected output (respectively, minimize the
expected input) length in the FVR (respectively, VFR) case. For the FVR case,
the RNG studied is a generalization of Elias's scheme, while in the VFR case
the general scheme is new. We precisely characterize, up to an additive
constant, the corresponding expected lengths, which include second-order terms
similar to those encountered in universal data compression and universal
simulation. Furthermore, in the FVR case, we consider also a &quot;twice-universal&quot;
setting, in which the Markov order k of the input source is also unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2196</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2196</id><created>2014-12-05</created><authors><author><keyname>Zhang</keyname><forenames>Hongyang</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author></authors><title>Relations among Some Low Rank Subspace Recovery Models</title><categories>cs.LG math.OC</categories><comments>Submitted to Neural Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering intrinsic low dimensional subspaces from data distributed on them
is a key preprocessing step to many applications. In recent years, there has
been a lot of work that models subspace recovery as low rank minimization
problems. We find that some representative models, such as Robust Principal
Component Analysis (R-PCA), Robust Low Rank Representation (R-LRR), and Robust
Latent Low Rank Representation (R-LatLRR), are actually deeply connected. More
specifically, we discover that once a solution to one of the models is
obtained, we can obtain the solutions to other models in closed-form
formulations. Since R-PCA is the simplest, our discovery makes it the center of
low rank subspace recovery models. Our work has two important implications.
First, R-PCA has a solid theoretical foundation. Under certain conditions, we
could find better solutions to these low rank models at overwhelming
probabilities, although these models are non-convex. Second, we can obtain
significantly faster algorithms for these models by solving R-PCA first. The
computation cost can be further cut by applying low complexity randomized
algorithms, e.g., our novel $\ell_{2,1}$ filtering algorithm, to R-PCA.
Experiments verify the advantages of our algorithms over other state-of-the-art
ones that are based on the alternating direction method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2197</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2197</id><created>2014-12-05</created><updated>2015-06-01</updated><authors><author><keyname>Cao</keyname><forenames>Liangliang</forenames></author><author><keyname>Wang</keyname><forenames>Chang</forenames></author></authors><title>Practice in Synonym Extraction at Large Scale</title><categories>cs.CL</categories><comments>This paper has been withdrawn by the author since the experimental
  results are not good enough</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synonym extraction is an important task in natural language processing and
often used as a submodule in query expansion, question answering and other
applications. Automatic synonym extractor is highly preferred for large scale
applications. Previous studies in synonym extraction are most limited to small
scale datasets. In this paper, we build a large dataset with 3.4 million
synonym/non-synonym pairs to capture the challenges in real world scenarios. We
proposed (1) a new cost function to accommodate the unbalanced learning
problem, and (2) a feature learning based deep neural network to model the
complicated relationships in synonym pairs. We compare several different
approaches based on SVMs and neural networks, and find out a novel feature
learning based neural network outperforms the methods with hand-assigned
features. Specifically, the best performance of our model surpasses the SVM
baseline with a significant 97\% relative improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2204</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2204</id><created>2014-12-06</created><authors><author><keyname>Detti</keyname><forenames>Andrea</forenames></author><author><keyname>Pisa</keyname><forenames>Claudio</forenames></author><author><keyname>Blefari-Melazzi</keyname><forenames>Nicola</forenames></author></authors><title>Multipath forwarding strategies in Information Centric Networks with
  AIMD congestion control</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A content can be replicated in more than one node, in Information Centric
Networks (ICNs). Thus, more than one path can be followed to reach the same
content, and it is necessary to decide the interface(s) to be selected in every
network node to forward content requests towards such multiple content
containers. A multipath forwarding strategy defines how to perform this choice.
In this paper we propose a general analytical model to evaluate the effect of
multipath forwarding strategies on the performance of an ICN content delivery,
whose congestion control follows a receiver driven, loss-based AIMD scheme. We
use the model to understand the behavior of ICN multipath forwarding strategies
proposed in the literature so far, and to devise and evaluate a novel strategy.
The considered multipath forwarding strategies are also evaluated in a
realistic network setting, by using the PlanetLab testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2210</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2210</id><created>2014-12-06</created><updated>2015-01-27</updated><authors><author><keyname>Gubbi</keyname><forenames>Sagar Venkatesh</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>Risk Estimation Without Using Stein's Lemma -- Application to Image
  Denoising</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of image denoising in additive white noise without
placing restrictive assumptions on its statistical distribution. In the recent
literature, specific noise distributions have been considered and
correspondingly, optimal denoising techniques have been developed. One of the
successful approaches for denoising relies on the notion of unbiased risk
estimation, which enables one to obtain a useful substitute for the mean-square
error. For the case of additive white Gaussian noise contamination, the risk
estimation procedure relies on Stein's lemma. Sophisticated wavelet-based
denoising techniques, which are essentially nonlinear, have been developed with
the help of the lemma. We show that, for linear, shift-invariant denoisers, it
is possible to obtain unbiased risk estimates of the mean-square error without
using Stein's lemma. An interesting consequence of this development is that the
unbiased risk estimator becomes agnostic to the statistical distribution of the
noise. As a proof of principle, we show how the new methodology can be used to
optimize the parameters of a simple Gaussian smoother. By locally adapting the
parameters of the Gaussian smoother, we obtain a shift-variant smoother, which
has a denoising performance (quantified by the improvement in peak
signal-to-noise ratio (PSNR)) that is competitive to far more sophisticated
methods reported in the literature. The proposed solution exhibits considerable
parallelism, which we exploit in a Graphics Processing Unit (GPU)
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2219</identifier>
 <datestamp>2014-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2219</id><created>2014-12-06</created><authors><author><keyname>Ghilezan</keyname><forenames>S.</forenames><affiliation>LIP</affiliation></author><author><keyname>Ivetic</keyname><forenames>J.</forenames><affiliation>LIP</affiliation></author><author><keyname>Lescanne</keyname><forenames>P.</forenames><affiliation>LIP</affiliation></author><author><keyname>Likavec</keyname><forenames>S.</forenames></author></authors><title>Resource control and intersection types: an intrinsic connection</title><categories>cs.LO math.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.2283</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the $\lambda$ -calculus, a $\lambda$-calculus
enriched with resource control. Explicit control of resources is enabled by the
presence of erasure and duplication operators, which correspond to thinning and
con-traction rules in the type assignment system. We introduce directly the
class of $\lambda$ -terms and we provide a new treatment of substitution by its
decompo-sition into atomic steps. We propose an intersection type assignment
system for $\lambda$ -calculus which makes a clear correspondence between three
roles of variables and three kinds of intersection types. Finally, we provide
the characterisation of strong normalisation in $\lambda$ -calculus by means of
an in-tersection type assignment system. This process uses typeability of
normal forms, redex subject expansion and reducibility method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2220</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2220</id><created>2014-12-06</created><authors><author><keyname>Vali</keyname><forenames>Zahra</forenames></author><author><keyname>Hashemi</keyname><forenames>Massoud Reza</forenames></author><author><keyname>Moghim</keyname><forenames>Neda</forenames></author></authors><title>An Adaptive Load Balancing to Provide Quality of Service</title><categories>cs.NI</categories><doi>10.7321/jscse.v3.n7.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to utilize network resources and provide a more reliable delivery of
information, multipath routing algorithms are used with a higher priority than
single path routing. But providing the quality of service (QoS) requirements
while load balancing is done among multiple paths is still a challenge. In this
paper a new load balancing algorithm is proposed by using an explicit endpoint
admission control (EEAC) to achieve a dynamic load balancing algorithm, with
the ability to guarantee the end-to-end QoS for a variety of service classes.
The simulation results for non deterministic network conditions show that the
proposed algorithm increases the utilization of network resources and also
decreases the end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2221</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2221</id><created>2014-12-06</created><updated>2015-01-05</updated><authors><author><keyname>Barany</keyname><forenames>Vince</forenames></author><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Kimelfeld</keyname><forenames>Benny</forenames></author><author><keyname>Olteanu</keyname><forenames>Dan</forenames></author><author><keyname>Vagena</keyname><forenames>Zografoula</forenames></author></authors><title>Declarative Statistical Modeling with Datalog</title><categories>cs.DB cs.AI cs.PL</categories><comments>14 pages, 4 figures</comments><acm-class>F.1.2; G.3; H.2.3; H.2.4; H.2.8; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formalisms for specifying statistical models, such as
probabilistic-programming languages, typically consist of two components: a
specification of a stochastic process (the prior), and a specification of
observations that restrict the probability space to a conditional subspace (the
posterior). Use cases of such formalisms include the development of algorithms
in machine learning and artificial intelligence. We propose and investigate a
declarative framework for specifying statistical models on top of a database,
through an appropriate extension of Datalog. By virtue of extending Datalog,
our framework offers a natural integration with the database, and has a robust
declarative semantics. Our Datalog extension provides convenient mechanisms to
include numerical probability functions; in particular, conclusions of rules
may contain values drawn from such functions. The semantics of a program is a
probability distribution over the possible outcomes of the input database with
respect to the program; these outcomes are minimal solutions with respect to a
related program with existentially quantified variables in conclusions.
Observations are naturally incorporated by means of integrity constraints over
the extensional and intensional relations. We focus on programs that use
discrete numerical distributions, but even then the space of possible outcomes
may be uncountable (as a solution can be infinite). We define a probability
measure over possible outcomes by applying the known concept of cylinder sets
to a probabilistic chase procedure. We show that the resulting semantics is
robust under different chases. We also identify conditions guaranteeing that
all possible outcomes are finite (and then the probability space is discrete).
We argue that the framework we propose retains the purely declarative nature of
Datalog, and allows for natural specifications of statistical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2226</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2226</id><created>2014-12-06</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Possible and Necessary Allocations via Sequential Mechanisms</title><categories>cs.AI cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple mechanism for allocating indivisible resources is sequential
allocation in which agents take turns to pick items. We focus on possible and
necessary allocation problems, checking whether allocations of a given form
occur in some or all mechanisms for several commonly used classes of sequential
allocation mechanisms. In particular, we consider whether a given agent
receives a given item, a set of items, or a subset of items for five natural
classes of sequential allocation mechanisms: balanced, recursively balanced,
balanced alternating, strictly alternating and all policies. We identify
characterizations of allocations produced balanced, recursively balanced,
balanced alternating policies and strictly alternating policies respectively,
which extend the well-known characterization by Brams and King [2005] for
policies without restrictions. In addition, we examine the computational
complexity of possible and necessary allocation problems for these classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2227</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2227</id><created>2014-12-06</created><updated>2015-05-10</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Hellmann-Feynman connection for the relative Fisher information</title><categories>cond-mat.stat-mech cs.IT math-ph math.IT math.MP physics.data-an</categories><comments>29 pages, 2 figures. Article to Appear in Annals of Physics. R. C.
  Venkatesan, A. Plastino, Hellmann-Feynman connection for the relative Fisher
  information, Annals of Physics (2015),
  http://dx.doi.org/10.1016/j.aop.2015.04.021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $(i)$ reciprocity relations for the relative Fisher information (RFI,
hereafter) and $(ii)$ a generalized RFI-Euler theorem, are self-consistently
derived from the Hellmann-Feynman theorem. These new reciprocity relations
generalize the RFI-Euler theorem and constitute the basis for building up a
mathematical Legendre transform structure (LTS, hereafter), akin to that of
thermodynamics, that underlies the RFI scenario. This demonstrates the
possibility of translating the entire mathematical structure of thermodynamics
into a RFI-based theoretical framework. Virial theorems play a prominent role
in this endeavor, as a Schr\&quot;odinger-like equation can be associated to the
RFI. Lagrange multipliers are determined invoking the RFI-LTS link and the
quantum mechanical virial theorem. An appropriate ansatz allows for the
inference of probability density functions (pdf's, hereafter) and
energy-eigenvalues of the above mentioned Schr\&quot;odinger-like equation. The
energy-eigenvalues obtained here via inference are benchmarked against
established theoretical and numerical results. A principled theoretical basis
to reconstruct the RFI-framework from the FIM framework is established.
Numerical examples for exemplary cases are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2231</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2231</id><created>2014-12-06</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Zhu</keyname><forenames>Changbo</forenames></author><author><keyname>Xu</keyname><forenames>Chunyan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Generalized Singular Value Thresholding</title><categories>cs.CV cs.LG cs.NA math.NA</categories><comments>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the Generalized Singular Value Thresholding (GSVT) operator
${\Prox}_{g}^{\bm{\sigma}}(\cdot)$, \begin{equation*}
  {\Prox}_{g}^{\bm{\sigma}}(\B)=\arg\min\limits_{\X}\sum_{i=1}^{m}g(\sigma_{i}(\X))
+ \frac{1}{2}||\X-\B||_{F}^{2}, \end{equation*} associated with a nonconvex
function $g$ defined on the singular values of $\X$. We prove that GSVT can be
obtained by performing the proximal operator of $g$ (denoted as
$\Prox_g(\cdot)$) on the singular values since $\Prox_g(\cdot)$ is monotone
when $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (many
popular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0&lt;p&lt;1$, of
$\ell_0$-norm are special cases), a general solver to find $\Prox_g(b)$ is
proposed for any $b\geq0$. GSVT greatly generalizes the known Singular Value
Thresholding (SVT) which is a basic subroutine in many convex low rank
minimization methods. We are able to solve the nonconvex low rank minimization
problem by using GSVT in place of SVT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2235</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2235</id><created>2014-12-06</created><updated>2015-02-16</updated><authors><author><keyname>Sato</keyname><forenames>Masahiro</forenames></author></authors><title>An Intuitionistic Set-theoretical Model of the Extended Calculus of
  Constructions</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Werner's set-theoretical model is one of the most intuitive models of ECC. It
combines a functional view of predicative universes with a collapsed view of
the impredicative sort Prop. However this model of Prop is so coarse that the
principle of excluded middle holds. In this paper, we interpret Prop into a
topological space (a special case of Heyting algebra) to make it more
intuitionistic without sacrificing simplicity. We prove soundness and show some
applications of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2247</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2247</id><created>2014-12-06</created><authors><author><keyname>Sanches</keyname><forenames>Pedro</forenames></author><author><keyname>Svee</keyname><forenames>Eric-Oluf</forenames></author><author><keyname>Bylund</keyname><forenames>Markus</forenames></author><author><keyname>Hirsch</keyname><forenames>Benjamin</forenames></author><author><keyname>Boman</keyname><forenames>Magnus</forenames></author></authors><title>Knowing Your Population: Privacy-Sensitive Mining of Massive Data</title><categories>cs.CY</categories><journal-ref>Network and Communication Technologies 2, no. 1 (2013): p34</journal-ref><doi>10.5539/nct.v2n1p34</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Location and mobility patterns of individuals are important to environmental
planning, societal resilience, public health, and a host of commercial
applications. Mining telecommunication traffic and transactions data for such
purposes is controversial, in particular raising issues of privacy. However,
our hypothesis is that privacy-sensitive uses are possible and often beneficial
enough to warrant considerable research and development efforts. Our work
contends that peoples behavior can yield patterns of both significant
commercial, and research, value. For such purposes, methods and algorithms for
mining telecommunication data to extract commonly used routes and locations,
articulated through time-geographical constructs, are described in a case study
within the area of transportation planning and analysis. From the outset, these
were designed to balance the privacy of subscribers and the added value of
mobility patterns derived from their mobile communication traffic and
transactions data. Our work directly contrasts the current, commonly held
notion that value can only be added to services by directly monitoring the
behavior of individuals, such as in current attempts at location-based
services. We position our work within relevant legal frameworks for privacy and
data protection, and show that our methods comply with such requirements and
also follow best-practices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2257</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2257</id><created>2014-12-06</created><authors><author><keyname>Schmidt</keyname><forenames>Florian</forenames></author><author><keyname>Ceriotti</keyname><forenames>Matteo</forenames></author><author><keyname>Hauser</keyname><forenames>Niklas</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>HotBox: Testing Temperature Effects in Sensor Networks</title><categories>cs.NI</categories><comments>18 pages, 6 figures, published as technical report of the Department
  of Computer Science of RWTH Aachen University</comments><report-no>AIB-2014-14</report-no><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-power wireless networks, especially in outside deployments, are exposed
to a wide range of temperatures. The detrimental effect of high temperatures on
communication quality is well known. To investigate these influences under
controlled conditions, we present HotBox, a solution with the following
properties: (1) It allows exposition of sensor motes to a wide range of
temperatures with a high degree of accuracy. (2) It supports specifying exact
spatial orientation of motes which, if not ensured, interferes with repeatable
experiment setups. (3) It is reasonably easy to assemble by following the
information (code, PCB schematics, hardware list and crafting instructions)
available online, facilitating further use of the platforms by other
researchers. After presenting HotBox, we will show its performance and prove
its feasibility as evaluation platform by conducting several experiments. These
experiments additionally provide further insight into the influence of
temperature effects on communication performance in low-power wireless
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2261</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2261</id><created>2014-12-06</created><authors><author><keyname>Bekrar</keyname><forenames>Marwa</forenames></author></authors><title>Protection de la vie priv\'ee \`a base d'agents dans un syst\`eme
  d'e-learning</title><categories>cs.CR cs.CY</categories><comments>M\'emoire de fin d'\'etudes, dipl\^ome d'Ing\'enieur d'Etat en
  Informatique, Ecole Nationale Superieure d'Informatique, Algeria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The e-learning systems are designed to provide an easy and constant access to
educational resources online. Indeed, E-learning systems have capacity to adapt
content and learning process according to the learner profile. Adaptation
techniques using advanced behavioral analysis mechanisms, called &quot;Learner
Modeling&quot; or &quot;Profiling&quot;. The latter require continuous tracking of the
activities of the learner to identify gaps and strengths in order to tailor
content to their specific needs or advise and accompany him during his
apprenticeship. However, the disadvantage of these systems is that they cause
learners' discouragement, for learners, alone with his screen loses its
motivation to improve. Adding social extension to learning, to avoid isolation
of learners and boost support and interaction between members of the learning
community, was able to increase learner's motivation. However, the tools to
facilitate social interactions integrated to E-learning platforms can be used
for purposes other than learning. These needs, which can be educational,
professional or personal, create a mixture of data from the private life and
public life of learners. With the integration of these tools for e-learning
systems and the growth of the amount of personal data stored in the databases
of these latter, protecting the privacy of students becomes a major concern.
Indeed, the exchange of profiles between e-learning systems is done without the
permission of their owners. Furthermore, the profiling behavior analysis
currently represents a very cost-effective way to generate profits by selling
these profiles advertising companies. Today, the right to privacy is threatened
from all sides. In addition to the threat from pirates, the source of the most
dangerous threats is that from service providers online that users devote a
blind trust. Control and centralized data storage and access privileges that
have suppliers are responsible for the threat. Our work is limited to the
protection of personal data in e-learning systems. We try to answer the
question: How can we design a system that protects the privacy of users against
threats from the provider while benefiting from all the services, including
analysis of behavior? In the absence of solutions that take into account the
protection and respect of privacy in e-learning systems that integrate social
learning tools, we designed our own solution. Our &quot;ApprAide&quot; system uses a set
of protocols based on security techniques to protect users' privacy. In
addition, our system incorporates tools that promote social interactions as a
social learning network, a chat tool and a virtual table. Our solution allows
the use of adaptation techniques and profiling to assist learners. Keywords:
Social learning, privacy, security, e-learning, agents
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2268</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2268</id><created>2014-12-06</created><authors><author><keyname>Wang</keyname><forenames>Feiran</forenames></author><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Energy-Efficient Resource Allocation for Device-to-Device Underlay
  Communication</title><categories>cs.NI</categories><comments>IEEE Transactions on Wireless Communications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Device-to-device (D2D) communication underlaying cellular networks is
expected to bring significant benefits for utilizing resources, improving user
throughput and extending battery life of user equipments. However, the
allocation of radio and power resources to D2D communication needs elaborate
coordination, as D2D communication can cause interference to cellular
communication. In this paper, we study joint channel and power allocation to
improve the energy efficiency of user equipments. To solve the problem
efficiently, we introduce an iterative combinatorial auction algorithm, where
the D2D users are considered as bidders that compete for channel resources, and
the cellular network is treated as the auctioneer. We also analyze important
properties of D2D underlay communication, and present numerical simulations to
verify the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2269</identifier>
 <datestamp>2014-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2269</id><created>2014-12-06</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Predicting Node Degree Centrality with the Node Prominence Profile</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1409.0285</comments><journal-ref>Nature Scientific Reports 4, Article number: 7236, 2014</journal-ref><doi>10.1038/srep07236</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Centrality of a node measures its relative importance within a network. There
are a number of applications of centrality, including inferring the influence
or success of an individual in a social network, and the resulting social
network dynamics. While we can compute the centrality of any node in a given
network snapshot, a number of applications are also interested in knowing the
potential importance of an individual in the future. However, current
centrality is not necessarily an effective predictor of future centrality.
While there are different measures of centrality, we focus on degree centrality
in this paper. We develop a method that reconciles preferential attachment and
triadic closure to capture a node's prominence profile. We show that the
proposed node prominence profile method is an effective predictor of degree
centrality. Notably, our analysis reveals that individuals in the early stage
of evolution display a distinctive and robust signature in degree centrality
trend, adequately predicted by their prominence profile. We evaluate our work
across four real-world social networks. Our findings have important
implications for the applications that require prediction of a node's future
degree centrality, as well as the study of social network dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2276</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2276</id><created>2014-12-06</created><authors><author><keyname>Teukolsky</keyname><forenames>Saul A.</forenames></author></authors><title>Short note on the mass matrix for Gauss-Lobatto grid points</title><categories>math.NA cs.NA physics.comp-ph</categories><doi>10.1016/j.jcp.2014.12.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mass matrix for Gauss-Lobatto grid points is usually approximated by
Gauss-Lobatto quadrature because this leads to a diagonal matrix that is easy
to invert. The exact mass matrix and its inverse are full. We show that the
exact mass matrix \emph{and} its inverse differ from the approximate diagonal
ones by a simple rank-1 update (outer product). They can thus be applied to an
arbitrary vector in $O(N)$ operations instead of $O(N^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2281</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2281</id><created>2014-12-06</created><updated>2015-10-15</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Syntactic Complexity of Suffix-Free Languages</title><categories>cs.FL</categories><comments>22 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve an open problem concerning syntactic complexity: We prove that the
cardinality of the syntactic semigroup of a suffix-free language with $n$ left
quotients (that is, with state complexity $n$) is at most $(n-1)^{n-2}+n-2$ for
$n\ge 6$. Since this bound is known to be reachable, this settles the problem.
We also reduce the alphabet of the witness languages reaching this bound to
five letters instead of $n+2$, and show that it cannot be any smaller. Finally,
we prove that the transition semigroup of a minimal deterministic automaton
accepting a witness language is unique for each $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2287</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2287</id><created>2014-12-06</created><updated>2015-01-16</updated><authors><author><keyname>L&#xf3;pez-Gonz&#xe1;lez</keyname><forenames>Juan C.</forenames></author><author><keyname>Rueda-Toicen</keyname><forenames>Antonio</forenames></author></authors><title>Search of Complex Binary Cellular Automata Using Behavioral Metrics</title><categories>cs.FL nlin.CG</categories><comments>23 pages</comments><msc-class>68Q80</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose the characterization of binary cellular automata using a set of
behavioral metrics that are applied to the minimal Boolean form of a cellular
automaton's transition function. These behavioral metrics are formulated to
satisfy heuristic criteria derived from elementary cellular automata. Behaviors
characterized through these metrics are growth, decrease, chaoticity, and
stability. From these metrics, two measures of global behavior are calculated:
1) a static measure that considers all possible input patterns and counts the
occurrence of the proposed metrics in the truth table of the minimal Boolean
form of the automaton; 2) a dynamic measure, corresponding to the mean of the
behavioral metrics in $n$ executions of the automaton, starting from $n$ random
initial states. We use these measures to characterize a cellular automaton and
guide a genetic search algorithm, which selects cellular automata similar to
the Game of Life. Using this method, we found an extensive set of complex
binary cellular automata with interesting properties, including
self-replication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2290</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2290</id><created>2014-12-06</created><authors><author><keyname>Reppas</keyname><forenames>Andreas I.</forenames></author><author><keyname>Spiliotis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Siettos</keyname><forenames>Constantinos I.</forenames></author></authors><title>Tuning the average path length of complex networks and its influence to
  the emergent dynamics of the majority-rule model</title><categories>cs.SI physics.soc-ph</categories><comments>10 figures</comments><journal-ref>Mathematics and Computers in Simulation, 109, 186-196 (2015)</journal-ref><doi>10.1016/j.matcom.2014.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how appropriate rewiring with the aid of Metropolis Monte Carlo
computational experiments can be exploited to create network topologies
possessing prescribed values of the average path length (APL) while keeping the
same connectivity degree and clustering coefficient distributions. Using the
proposed rewiring rules we illustrate how the emergent dynamics of the
celebrated majority-rule model are shaped by the distinct impact of the APL
attesting the need for developing efficient algorithms for tuning such network
characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2291</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2291</id><created>2014-12-06</created><updated>2015-08-20</updated><authors><author><keyname>Usevich</keyname><forenames>Konstantin</forenames></author><author><keyname>Markovsky</keyname><forenames>Ivan</forenames></author></authors><title>Adjusted least squares fitting of algebraic hypersurfaces</title><categories>stat.CO cs.CG cs.CV math.NA</categories><comments>30 pages, 10 figures</comments><msc-class>15A22, 15B05, 33C45, 62H12, 65D10, 65F15, 68U05</msc-class><doi>10.1016/j.laa.2015.07.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fitting a set of points in Euclidean space by an
algebraic hypersurface. We assume that points on a true hypersurface, described
by a polynomial equation, are corrupted by zero mean independent Gaussian
noise, and we estimate the coefficients of the true polynomial equation. The
adjusted least squares estimator accounts for the bias present in the ordinary
least squares estimator. The adjusted least squares estimator is based on
constructing a quasi-Hankel matrix, which is a bias-corrected matrix of
moments. For the case of unknown noise variance, the estimator is defined as a
solution of a polynomial eigenvalue problem. In this paper, we present new
results on invariance properties of the adjusted least squares estimator and an
improved algorithm for computing the estimator for an arbitrary set of
monomials in the polynomial equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2300</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2300</id><created>2014-12-06</created><authors><author><keyname>Andrews</keyname><forenames>Aaron M.</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Minimizing the Aggregate Movements for Interval Coverage</title><categories>cs.CG cs.DS</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an interval coverage problem. Given $n$ intervals of the same
length on a line $L$ and a line segment $B$ on $L$, we want to move the
intervals along $L$ such that every point of $B$ is covered by at least one
interval and the sum of the moving distances of all intervals is minimized. As
a basic geometry problem, it has applications in mobile sensor barrier coverage
in wireless sensor networks. The previous work solved the problem in $O(n^2)$
time. In this paper, by discovering many interesting observations and
developing new algorithmic techniques, we present an $O(n\log n)$ time
algorithm. We also show an $\Omega(n\log n)$ time lower bound for this problem,
which implies the optimality of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2302</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2302</id><created>2014-12-06</created><updated>2015-04-06</updated><authors><author><keyname>Ding</keyname><forenames>Weiguang</forenames></author><author><keyname>Wang</keyname><forenames>Ruoyan</forenames></author><author><keyname>Mao</keyname><forenames>Fei</forenames></author><author><keyname>Taylor</keyname><forenames>Graham</forenames></author></authors><title>Theano-based Large-Scale Visual Recognition with Multiple GPUs</title><categories>cs.LG</categories><comments>ICLR 2015 workshop camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)
implementation and its naive data parallelism on multiple GPUs. Our performance
on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)
run on 1 GPU. To the best of our knowledge, this is the first open-source
Python-based AlexNet implementation to-date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2304</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2304</id><created>2014-12-06</created><updated>2014-12-14</updated><authors><author><keyname>Dymchenko</keyname><forenames>Sergii</forenames></author><author><keyname>Mykhailova</keyname><forenames>Mariia</forenames></author></authors><title>Declaratively solving tricky Google Code Jam problems with Prolog-based
  ECLiPSe CLP system</title><categories>cs.PL</categories><comments>6 pages. This is a full version of a paper accepted at SAC 2015</comments><acm-class>D.3.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate several examples of solving challenging
algorithmic problems from the Google Code Jam programming contest with the
Prolog-based ECLiPSe system using declarative techniques like constraint logic
programming and linear (integer) programming. These problems were designed to
be solved by inventing clever algorithms and efficiently implementing them in a
conventional imperative programming language, but we present relatively simple
declarative programs in ECLiPSe that are fast enough to find answers within the
time limit imposed by the contest rules. We claim that declarative programming
with ECLiPSe is better suited for solving certain common kinds of programming
problems offered in Google Code Jam than imperative programming. We show this
by comparing the mental steps required to come up with both kinds of solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2306</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2306</id><created>2014-12-06</created><updated>2015-04-14</updated><authors><author><keyname>Karpathy</keyname><forenames>Andrej</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Deep Visual-Semantic Alignments for Generating Image Descriptions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model that generates natural language descriptions of images and
their regions. Our approach leverages datasets of images and their sentence
descriptions to learn about the inter-modal correspondences between language
and visual data. Our alignment model is based on a novel combination of
Convolutional Neural Networks over image regions, bidirectional Recurrent
Neural Networks over sentences, and a structured objective that aligns the two
modalities through a multimodal embedding. We then describe a Multimodal
Recurrent Neural Network architecture that uses the inferred alignments to
learn to generate novel descriptions of image regions. We demonstrate that our
alignment model produces state of the art results in retrieval experiments on
Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated
descriptions significantly outperform retrieval baselines on both full images
and on a new dataset of region-level annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2309</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2309</id><created>2014-12-06</created><updated>2015-06-04</updated><authors><author><keyname>Chalupka</keyname><forenames>Krzysztof</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author><author><keyname>Eberhardt</keyname><forenames>Frederick</forenames></author></authors><title>Visual Causal Feature Learning</title><categories>stat.ML cs.AI cs.CV cs.LG</categories><comments>Accepted at UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a rigorous definition of the visual cause of a behavior that is
broadly applicable to the visually driven behavior in humans, animals, neurons,
robots and other perceiving systems. Our framework generalizes standard
accounts of causal learning to settings in which the causal variables need to
be constructed from micro-variables. We prove the Causal Coarsening Theorem,
which allows us to gain causal knowledge from observational data with minimal
experimental effort. The theorem provides a connection to standard inference
techniques in machine learning that identify features of an image that
correlate with, but may not cause, the target behavior. Finally, we propose an
active learning scheme to learn a manipulator function that performs optimal
manipulations on the image to automatically identify the visual cause of a
target behavior. We illustrate our inference and learning algorithms in
experiments based on both synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2314</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2314</id><created>2014-12-06</created><updated>2015-03-21</updated><authors><author><keyname>Waggoner</keyname><forenames>Bo</forenames></author></authors><title>$\ell_p$ Testing and Learning of Discrete Distributions</title><categories>cs.DS cs.LG math.ST stat.TH</categories><comments>This is the full version of the paper appearing at ITCS 2015. Two
  columns. 24 pages, of which 14 appendix</comments><acm-class>F.2.0; G.3</acm-class><doi>10.1145/2688073.2688095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic problems of testing uniformity of and learning a discrete
distribution, given access to independent samples from it, are examined under
general $\ell_p$ metrics. The intuitions and results often contrast with the
classic $\ell_1$ case. For $p &gt; 1$, we can learn and test with a number of
samples that is independent of the support size of the distribution: With an
$\ell_p$ tolerance $\epsilon$, $O(\max\{ \sqrt{1/\epsilon^q}, 1/\epsilon^2 \})$
samples suffice for testing uniformity and $O(\max\{ 1/\epsilon^q,
1/\epsilon^2\})$ samples suffice for learning, where $q=p/(p-1)$ is the
conjugate of $p$. As this parallels the intuition that $O(\sqrt{n})$ and $O(n)$
samples suffice for the $\ell_1$ case, it seems that $1/\epsilon^q$ acts as an
upper bound on the &quot;apparent&quot; support size.
  For some $\ell_p$ metrics, uniformity testing becomes easier over larger
supports: a 6-sided die requires fewer trials to test for fairness than a
2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,
this inverse dependence on support size holds if and only if $p &gt; \frac{4}{3}$.
The uniformity testing algorithm simply thresholds the number of &quot;collisions&quot;
or &quot;coincidences&quot; and has an optimal sample complexity up to constant factors
for all $1 \leq p \leq 2$. Another algorithm gives order-optimal sample
complexity for $\ell_{\infty}$ uniformity testing. Meanwhile, the most natural
learning algorithm is shown to have order-optimal sample complexity for all
$\ell_p$ metrics.
  The author thanks Cl\'{e}ment Canonne for discussions and contributions to
this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2316</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2316</id><created>2014-12-06</created><authors><author><keyname>Korki</keyname><forenames>Mehdi</forenames></author><author><keyname>Zhang</keyname><forenames>Jingxin</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author></authors><title>Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals</title><categories>stat.ML cs.IT math.IT</categories><comments>13 pages, 7 figures, Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA)
for reconstructing block-sparse signals with unknown block structures. Unlike
the existing algorithms for block sparse signal recovery which assume the
cluster structure of the nonzero elements of the unknown signal to be
independent and identically distributed (i.i.d.), we use a more realistic
Bernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d.
block-sparse signals commonly encountered in practice. The Block-IBA
iteratively estimates the amplitudes and positions of the block-sparse signal
using the steepest-ascent based Expectation-Maximization (EM), and optimally
selects the nonzero elements of the block-sparse signal by adaptive
thresholding. The global convergence of Block-IBA is analyzed and proved, and
the effectiveness of Block-IBA is demonstrated by numerical experiments and
simulations on synthetic and real-life data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2321</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2321</id><created>2014-12-07</created><authors><author><keyname>Honnappa</keyname><forenames>Harsha</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Ward</keyname><forenames>Amy R.</forenames></author></authors><title>On Transitory Queueing</title><categories>math.PR cs.PF</categories><comments>Under review (and revision), Math. of Operations Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework and develop a theory of transitory queueing models.
These are models that are not only non-stationary and time-varying but also
have other features such as the queueing system operates over finite time, or
only a finite population arrives. Such models are relevant in many real-world
settings, from queues at post-offces, DMV, concert halls and stadia to
out-patient departments at hospitals. We develop fluid and diffusion limits for
a large class of transitory queueing models. We then introduce three specific
models that fit within this framework, namely, the Delta(i)/GI/1 model, the
conditioned G/GI/1 model, and an arrival model of scheduled traffic with epoch
uncertainty. We show that asymptotically these models are distributionally
equivalent, i.e., they have the same fluid and diffusion limits. We note that
our framework provides the first ever way of analyzing the standard G/GI/1
model when we condition on the number of arrivals. In obtaining these results,
we provide generalizations and extensions of the Glivenko-Cantelli and Donskers
Theorem for empirical processes with triangular arrays. Our analysis uses the
population acceleration technique that we introduce and develop. This may be
useful in analysis of other non-stationary and non-ergodic queuing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2324</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2324</id><created>2014-12-07</created><updated>2015-12-02</updated><authors><author><keyname>Faleiro</keyname><forenames>Jose M.</forenames></author><author><keyname>Abadi</keyname><forenames>Daniel J.</forenames></author></authors><title>Rethinking serializable multiversion concurrency control</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-versioned database systems have the potential to significantly increase
the amount of concurrency in transaction processing because they can avoid
read-write conflicts. Unfortunately, the increase in concurrency usually comes
at the cost of transaction serializability. If a database user requests full
serializability, modern multi-versioned systems significantly constrain
read-write concurrency among conflicting transactions and employ expensive
synchronization patterns in their design. In main-memory multi-core settings,
these additional constraints are so burdensome that multi-versioned systems are
often significantly outperformed by single-version systems.
  We propose Bohm, a new concurrency control protocol for main-memory
multi-versioned database systems. Bohm guarantees serializable execution while
ensuring that reads never block writes. In addition, Bohm does not require
reads to perform any book-keeping whatsoever, thereby avoiding the overhead of
tracking reads via contended writes to shared memory. This leads to excellent
scalability and performance in multi-core settings. Bohm has all the above
characteristics without performing validation based concurrency control.
Instead, it is pessimistic, and is therefore not prone to excessive aborts in
the presence of contention. An experimental evaluation shows that Bohm performs
well in both high contention and low contention settings, and is able to
dramatically outperform state-of-the-art multi-versioned systems despite
maintaining the full set of serializability guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2326</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2326</id><created>2014-12-07</created><authors><author><keyname>Wu</keyname><forenames>Jiqiang</forenames></author><author><keyname>Zhou</keyname><forenames>Yipeng</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author><author><keyname>Hua</keyname><forenames>Youwei</forenames></author><author><keyname>Zhu</keyname><forenames>Zirong</forenames></author></authors><title>Modeling Dynamics of Online Video Popularity</title><categories>cs.NI cs.MM cs.SI</categories><comments>9 pages, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Internet video delivery systems serve millions of videos to tens of
millions of users on daily basis, via Video-on-Demand and live streaming. Video
popularity evolves over time. It represents the workload, as welll as business
value, of the video to the overall system. The ability to predict video
popularity is very helpful for improving service quality and operating
efficiency. Previous studies adopted simple models for video popularity, or
directly adopted patterns from measurement studies. In this paper, we develop a
stochastic fluid model that tries to capture two hidden processes that give
rise to different patterns of a given video's popularity evolution: the
information spreading process, and the user reaction process. Specifically,
these processes model how the video is recommended to the user, the videos
inherent attractiveness, and users reaction rate, and yield specific popularity
evolution patterns. We then validate our model by matching the predictions of
the model with observed patterns from our collaborator, a large content
provider in China. This model thus gives us the insight to explain the common
and different video popularity evolution patterns and why.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2328</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2328</id><created>2014-12-07</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Taimoor</forenames></author><author><keyname>Serpanos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shrobe</keyname><forenames>Howard</forenames></author></authors><title>On the Behavioural Formalization of the Cognitive Middleware AWDRAT</title><categories>cs.AI cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our ongoing work and initial results towards the (behavioral)
correctness analysis of the cognitive middleware AWDRAT. Since, the (provable)
behavioral correctness of a software system is a fundamental pre-requisite of
the system's security. Therefore, the goal of the work is to first formalize
the behavioral semantics of the middleware as a pre-requisite for our proof of
the behavioral correctness. However, in this paper, we focus only on the core
and critical component of the middleware, i.e. Execution Monitor which is a
part of the module &quot;Architectural Differencer&quot; of AWDRAT. The role of the
execution monitor is to identify inconsistencies between runtime observations
of the target system and predictions of the specification System Architectural
Model of the system. As a starting point we have defined the formal
(denotational) semantics of the observations (runtime events) and predictions
(executable specifications as of System Architectural Model); then based on the
aforementioned formal semantices, we have formalized the behavior of the
&quot;Execution Monitor&quot; of the middleware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2332</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2332</id><created>2014-12-07</created><updated>2015-03-31</updated><authors><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Civili</keyname><forenames>Cristina</forenames></author><author><keyname>Sherkhonov</keyname><forenames>Evgeny</forenames></author><author><keyname>Tan</keyname><forenames>Wang-Chiew</forenames></author></authors><title>High-Level Why-Not Explanations using Ontologies</title><categories>cs.DB</categories><comments>in PODS 2015</comments><acm-class>H.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel foundational framework for why-not explanations, that is,
explanations for why a tuple is missing from a query result. Our why-not
explanations leverage concepts from an ontology to provide high-level and
meaningful reasons for why a tuple is missing from the result of a query. A key
algorithmic problem in our framework is that of computing a most-general
explanation for a why-not question, relative to an ontology, which can either
be provided by the user, or it may be automatically derived from the data
and/or schema. We study the complexity of this problem and associated problems,
and present concrete algorithms for computing why-not explanations. In the case
where an external ontology is provided, we first show that the problem of
deciding the existence of an explanation to a why-not question is NP-complete
in general. However, the problem is solvable in polynomial time for queries of
bounded arity, provided that the ontology is specified in a suitable language,
such as a member of the DL-Lite family of description logics, which allows for
efficient concept subsumption checking. Furthermore, we show that a
most-general explanation can be computed in polynomial time in this case. In
addition, we propose a method for deriving a suitable (virtual) ontology from a
database and/or a data workspace schema, and we present an algorithm for
computing a most-general explanation to a why-not question, relative to such
ontologies. This algorithm runs in polynomial-time in the case when concepts
are defined in a selection-free language, or if the underlying schema is fixed.
Finally, we also study the problem of computing short most-general
explanations, and we briefly discuss alternative definitions of what it means
to be an explanation, and to be most general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2333</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2333</id><created>2014-12-07</created><authors><author><keyname>Pemmaraju</keyname><forenames>Sriram V.</forenames></author><author><keyname>Sardeshmukh</keyname><forenames>Vivek B.</forenames></author></authors><title>Minimum-weight Spanning Tree Construction in $O(\log \log \log n)$
  Rounds on the Congested Clique</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the \textit{minimum spanning tree (MST)} problem in the
Congested Clique model and presents an algorithm that runs in $O(\log \log \log
n)$ rounds, with high probability. Prior to this, the fastest MST algorithm in
this model was a deterministic algorithm due to Lotker et al.~(SIAM J on Comp,
2005) from about a decade ago. A key step along the way to designing this MST
algorithm is a \textit{connectivity verification} algorithm that not only runs
in $O(\log \log \log n)$ rounds with high probability, but also has low message
complexity. This allows the fast computation of an MST by running multiple
instances of the connectivity verification algorithm in parallel. These results
depend on a new edge-sampling theorem, developed in the paper, that says that
if each edge $e = \{u, v\}$ is sampled independently with probability $c \log^2
n/\min\{\mbox{degree}(u), \mbox{degree}(v)\}$ (for a large enough constant $c$)
then all cuts of size at least $n$ are approximated in the sampled graph. This
sampling theorem is inspired by series of papers on graph sparsification via
random edge sampling due to Karger~(STOC 1994), Bencz\'ur and Karger~(STOC
1996, arxiv 2002), and Fung et al.~(STOC 2011). The edge sampling techniques in
these papers use probabilities that are functions of edge-connectivity or a
related measure called edge-strength. For the purposes of this paper, these
edge-connectivity measures seem too costly to compute and the main technical
contribution of this paper is to show that degree-based edge-sampling suffices
to approximate large cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2335</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2335</id><created>2014-12-07</created><authors><author><keyname>Shatalov</keyname><forenames>Vladimir</forenames></author></authors><title>School of the future: smartphones as a laboratory in pocket of each
  student</title><categories>cs.CY cs.NI</categories><comments>4 pages, in Ukrainian, 2 figures. The essays in Ukrainian was
  submitted for the first step of the contest &quot;Samsung towards knowledge&quot;,
  November 2014 (http://www.samsung.com/ua/education/), and then it was
  rejected by the jury</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea to unite student smartphones in a scalable network and use the
network to collect data of personal environmental sensors and health indicators
is proposed. Access to the sensors, which are available in every smartphone
will provide the appropriate software. Such a monitoring at the global level
would reveal the impact of the electromagnetic radiation, environmental
pollution and weather factors on human health. Participation in these
measurements increases the educational and social activity of students. [in
Ukrainian]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2338</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2338</id><created>2014-12-07</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>An Algorithm to Determine Energy-aware Maximal Leaf Nodes Data Gathering
  Tree for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>14 pages, 5 figures, Journal of Theoretical and Applied Information
  Technology, Vol. 15, No. 1, May 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an Energy-aware Maximal Leaf Nodes Data Gathering (EMLN-DG)
algorithm for periodic data collection and transmission in wireless sensor
networks. For each round of data gathering, an EMLN-DG tree spanning the entire
sensor network is formed based on the residual energy level available at the
nodes and the number of uncovered neighbors of a node during tree formation.
Only nodes that have a relatively larger number of neighbors as well as a
higher energy level are included as intermediate nodes in the EMLN-DG tree. By
maximizing the number of leaf nodes in a DG tree and considering the energy
level available at the nodes while forming the tree, we reduce energy
consumption per round as well as balance the energy level across all the nodes
in the network. This contributes to a significantly larger network lifetime,
measured as the number of rounds before the first node failure due to
exhaustion of battery charge. Performance comparison studies with the
well-known data gathering algorithms such as LEACH and PEGASIS illustrate that
EMLN-DG can help to sustain the network for a significantly larger number of
rounds and at the same time incur a lower, or if not comparable, energy loss,
delay and energy loss*delay per round of data gathering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2341</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2341</id><created>2014-12-07</created><authors><author><keyname>Desai</keyname><forenames>Madhav</forenames></author><author><keyname>Sule</keyname><forenames>Virendra</forenames></author></authors><title>Generalized cofactors and decomposition of Boolean satisfiability
  problems</title><categories>cs.DS</categories><comments>13 pages</comments><msc-class>03G05, 06E30, 94C10</msc-class><acm-class>I.1.2; F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach for decomposing Boolean satisfiability problems while
extending recent results of \cite{sul2} on solving Boolean systems of
equations. Developments in \cite{sul2} were aimed at the expansion of functions
$f$ in orthonormal (ON) sets of base functions as a generalization of the
Boole-Shannon expansion and the derivation of the consistency condition for the
equation $f=0$ in terms of the expansion co-efficients. In this paper, we
further extend the Boole-Shannon expansion over an arbitrary set of base
functions and derive the consistency condition for $f=1$. The generalization of
the Boole-Shannon formula presented in this paper is in terms of
\emph{cofactors} as co-efficients with respect to a set of CNFs called a
\emph{base} which appear in a given Boolean CNF formula itself. This approach
results in a novel parallel algorithm for decomposition of a CNF formula and
computation of all satisfying assignments when they exist by using the given
data set of CNFs itself as the base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2342</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2342</id><created>2014-12-07</created><authors><author><keyname>Shouno</keyname><forenames>Hayaru</forenames></author></authors><title>Bayesian Image Restoration for Poisson Corrupted Image using a Latent
  Variational Method with Gaussian MRF</title><categories>cs.CV</categories><comments>9 pages, 6 figures, The of this manuscript is submitting to the
  Information Processing Society of Japan(IPSJ), Transactions on Mathematical
  Modeling and its Applications (TOM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We treat an image restoration problem with a Poisson noise chan- nel using a
Bayesian framework. The Poisson randomness might be appeared in observation of
low contrast object in the field of imaging. The noise observation is often
hard to treat in a theo- retical analysis. In our formulation, we interpret the
observation through the Poisson noise channel as a likelihood, and evaluate the
bound of it with a Gaussian function using a latent variable method. We then
introduce a Gaussian Markov random field (GMRF) as the prior for the Bayesian
approach, and derive the posterior as a Gaussian distribution. The latent
parameters in the likelihood and the hyperparameter in the GMRF prior could be
treated as hid- den parameters, so that, we propose an algorithm to infer them
in the expectation maximization (EM) framework using loopy belief
propagation(LBP). We confirm the ability of our algorithm in the computer
simulation, and compare it with the results of other im- age restoration
frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2347</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2347</id><created>2014-12-07</created><authors><author><keyname>Dubach</keyname><forenames>Christophe</forenames></author><author><keyname>Fursin</keyname><forenames>Grigori</forenames></author></authors><title>Proceedings of the 5th International Workshop on Adaptive Self-tuning
  Computing Systems 2015 (ADAPT'15)</title><categories>cs.PF</categories><report-no>ADAPT/2015/00</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the proceedings of the 5th International Workshop on Adaptive
Self-tuning Computing Systems 2015 (ADAPT'15).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2352</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2352</id><created>2014-12-07</created><authors><author><keyname>Kolumb&#xe1;n</keyname><forenames>S&#xe1;ndor</forenames></author><author><keyname>Vajk</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Schoukens</keyname><forenames>Johan</forenames></author></authors><title>Perturbed Datasets Methods for Hypothesis Testing and Structure of
  Corresponding Confidence Sets</title><categories>cs.SY</categories><journal-ref>Automatica, vol. 51, pages 326 - 331, 2015</journal-ref><doi>10.1016/j.automatica.2014.10.083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypothesis testing methods that do not rely on exact distribution assumptions
have been emerging lately. The method of sign-perturbed sums (SPS) is capable
of characterizing confidence regions with exact confidence levels for linear
regression and linear dynamical systems parameter estimation problems if the
noise distribution is symmetric. This paper describes a general family of
hypothesis testing methods that have an exact user chosen confidence level
based on finite sample count and without relying on an assumed noise
distribution. It is shown that the SPS method belongs to this family and we
provide another hypothesis test for the case where the symmetry assumption is
replaced with exchangeability. In the case of linear regression problems it is
shown that the confidence regions are connected, bounded and possibly
non-convex sets in both cases. To highlight the importance of understanding the
structure of confidence regions corresponding to such hypothesis tests it is
shown that confidence sets for linear dynamical systems parameter estimates
generated using the SPS method can have non-connected parts, which have far
reaching consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2356</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2356</id><created>2014-12-07</created><authors><author><keyname>Vasan</keyname><forenames>Gautham</forenames></author><author><keyname>Singh</keyname><forenames>Arun Kumar</forenames></author><author><keyname>Krishna</keyname><forenames>Madhava</forenames></author></authors><title>Model Predictive Control for Micro Aerial Vehicle Systems (MAV) Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for path-following for quadcopter trajectories
in real time. Non-Linear Guidance Logic is used to find the intercepts of the
subsequent destination. Trajectory tracking is implemented by formulating the
trajectory of the quadcopter using its jerk, in discrete time, and then solving
a convex optimization problem on each decoupled axis. Based on the maximum
possible thrust and angular rates of the quadcopter, feasibility constraints
for the quadcopter have been derived. In this report we describe the design and
implementation of explicit MPC controllers where the controllers were executed
on a computer using sparse solvers to control the vehicle in hovering flight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2378</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2378</id><created>2014-12-07</created><authors><author><keyname>Bollegala</keyname><forenames>Danushka</forenames></author><author><keyname>Maehara</keyname><forenames>Takanori</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Learning Word Representations from Relational Graphs</title><categories>cs.CL</categories><comments>AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attributes of words and relations between two words are central to numerous
tasks in Artificial Intelligence such as knowledge representation, similarity
measurement, and analogy detection. Often when two words share one or more
attributes in common, they are connected by some semantic relations. On the
other hand, if there are numerous semantic relations between two words, we can
expect some of the attributes of one of the words to be inherited by the other.
Motivated by this close connection between attributes and relations, given a
relational graph in which words are inter- connected via numerous semantic
relations, we propose a method to learn a latent representation for the
individual words. The proposed method considers not only the co-occurrences of
words as done by existing approaches for word representation learning, but also
the semantic relations in which two words co-occur. To evaluate the accuracy of
the word representations learnt using the proposed method, we use the learnt
word representations to solve semantic word analogy problems. Our experimental
results show that it is possible to learn better word representations by using
semantic semantics between words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2379</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2379</id><created>2014-12-07</created><updated>2015-05-04</updated><authors><author><keyname>Stout</keyname><forenames>Quentin F.</forenames></author></authors><title>An Algorithm for $L_\infty$ Approximation by Step Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is given for determining an optimal $b$-step approximation of
weighted data, where the error is measured with respect to the $L_\infty$ norm.
For data presorted by the independent variable the algorithm takes $\Theta(n +
\log n \cdot b(1+\log n/b))$ time and $\Theta(n)$ space. This is $\Theta(n \log
n)$ in the worst case and $\Theta(n)$ when $b = O(n/\log n \log\log n)$. A
minor change determines an optimal reduced isotonic regression in the same time
and space bounds, and the algorithm also solves the $k$-center problem for
1-dimensional weighted data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2385</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2385</id><created>2014-12-07</created><authors><author><keyname>Kamaev</keyname><forenames>Valery</forenames></author><author><keyname>Finogeev</keyname><forenames>Alexey</forenames></author><author><keyname>Finogeev</keyname><forenames>Anton</forenames></author><author><keyname>Shevchenko</keyname><forenames>Sergey</forenames></author></authors><title>Knowledge Discovery in the SCADA Databases Used for the Municipal Power
  Supply System</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This scientific paper delves into the problems related to the develop-ment of
intellectual data analysis system that could support decision making to manage
municipal power supply services. The management problems of mu-nicipal power
supply system have been specified taking into consideration modern tendencies
shown by new technologies that allow for an increase in the energy efficiency.
The analysis findings of the system problems related to the integrated
computer-aided control of the power supply for the city have been given. The
consideration was given to the hierarchy-level management decom-position model.
The objective task targeted at an increase in the energy effi-ciency to
minimize expenditures and energy losses during the generation and
transportation of energy carriers to the Consumer, the optimization of power
consumption at the prescribed level of the reliability of pipelines and
networks and the satisfaction of Consumers has been defined. To optimize the
support of the decision making a new approach to the monitoring of engineering
systems and technological processes related to the energy consumption and
transporta-tion using the technologies of geospatial analysis and Knowledge
Discovery in databases (KDD) has been proposed. The data acquisition for
analytical prob-lems is realized in the wireless heterogeneous medium, which
includes soft-touch VPN segments of ZigBee technology realizing the 6LoWPAN
standard over the IEEE 802.15.4 standard and also the segments of the networks
of cellu-lar communications. JBoss Application Server is used as a server-based
plat-form for the operation of the tools used for the retrieval of data
collected from sensor nodes, PLC and energy consumption record devices. The KDD
tools are developed using Java Enterprise Edition platform and Spring and ORM
Hiber-nate technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2387</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2387</id><created>2014-12-07</created><authors><author><keyname>Botvinkin</keyname><forenames>Pavel Viktorovich</forenames></author><author><keyname>Kamaev</keyname><forenames>Valery Anatolevich</forenames></author><author><keyname>Nefedova</keyname><forenames>Irina Sergeevna</forenames></author><author><keyname>Finogeev</keyname><forenames>Aleksey Germanovich</forenames></author><author><keyname>Finogeev</keyname><forenames>Egor Alekseevich</forenames></author></authors><title>Analysis, classification and detection methods of attacks via wireless
  sensor networks in SCADA systems</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Effectiveness of information security of automated process control systems,
as well as of SCADA, depends on data transmissions protection technologies
applied on transport environments components. This article investigates the
problem of detecting attacks on WSN (wireless sensor networks) of SCADA
systems. As the result of analytical studies the authors developed the detailed
classification of external attacks on sensor networks and brought the detailed
description of attacking impacts on components of SCADA systems in accordance
with selected directions of attacks. Reviewed the methods of intrusion
detection in wireless sensor networks of SCADA systems and functions of WIDS
(wireless intrusion detection systems). Noticed the role of anthropogenic
factors in internal security threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2391</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2391</id><created>2014-12-07</created><updated>2015-12-09</updated><authors><author><keyname>Kiskani</keyname><forenames>Mohsen Karimzadeh</forenames></author><author><keyname>Sadjadpour</keyname><forenames>Hamid R.</forenames></author></authors><title>Multihop Caching-Aided Coded Multicasting for the Next Generation of
  Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation of cellular networks deploying wireless distributed
femtocaching infrastructure proposed by Golrezaei et. al. are studied. By
taking advantage of multihop communications in each cell, the number of
required femtocaching helpers is significantly reduced. This reduction of
femtocaches is achieved by using the underutilized storage and communication
capabilities in the User Terminals (UTs), which results in reducing the
deployment costs of distributed femtocaches. A multihop index coding technique
is proposed to code the cached contents in helpers to achieve order optimal
capacity gains. This can serve as an efficient content delivery algorithm for
the solution provided by Golrezaei et. al. As an example, we consider a
wireless cellular system in which contents have a popularity distribution. It
has been shown that if the contents follow a high content reuse popularity
distribution, our approach can replace many unicast communication with
multicast communication. We will prove that simple linear index codes found by
heuristics based on graph coloring algorithms can achieve order optimal
capacity under Zipfian content popularity distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2404</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2404</id><created>2014-12-07</created><updated>2015-05-31</updated><authors><author><keyname>Arpit</keyname><forenames>Devansh</forenames></author><author><keyname>Nwogu</keyname><forenames>Ifeoma</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Dimensionality Reduction with Subspace Structure Preservation</title><categories>cs.LG stat.ML</categories><comments>Published in NIPS 2014; v2: minor updates to the algorithm and added
  a few lines addressing application to large-scale/high-dimensional data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling data as being sampled from a union of independent subspaces has been
widely applied to a number of real world applications. However, dimensionality
reduction approaches that theoretically preserve this independence assumption
have not been well studied. Our key contribution is to show that $2K$
projection vectors are sufficient for the independence preservation of any $K$
class data sampled from a union of independent subspaces. It is this
non-trivial observation that we use for designing our dimensionality reduction
technique. In this paper, we propose a novel dimensionality reduction algorithm
that theoretically preserves this structure for a given dataset. We support our
theoretical analysis with empirical results on both synthetic and real world
data achieving \textit{state-of-the-art} results compared to popular
dimensionality reduction techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2416</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2416</id><created>2014-12-07</created><updated>2014-12-09</updated><authors><author><keyname>Stegmann</keyname><forenames>Johannes</forenames></author></authors><title>Paradigm shifts. Part II. Reverse Transcriptase. Analysis of reference
  stability and word frequencies</title><categories>cs.DL cs.IR</categories><comments>10 pages, 7 tables, 1 figure, corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reverse transcription paradigm shift in RNA tumor virus research marked
by the discovery of the reverse transcriptase in 1970 was traced using
co-citation and title word frequency analysis. It is shown that this event is
associated with a break in citation patterns and the occurrence of previously
unknown technical terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2417</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2417</id><created>2014-12-07</created><updated>2015-02-16</updated><authors><author><keyname>Zhang</keyname><forenames>Hantian</forenames></author></authors><title>Non-smooth Approach for Contact Dynamics and Impulse-based Control of
  Frictional Furuta Pendulum</title><categories>cs.SY cs.CE physics.class-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, a non-penetrated and physically consistent non-smooth
numerical approach has been proposed, by employing the Prox formulation and
Moreau's mid-point time-stepping rule, for the contact dynamics with coupled
and decoupled constraints. Under this circumstance, the robust impulse-based
control has been successfully implemented and validated on the motion system of
controlled frictional oscillator. Further improvement has been achieved by
utilizing shooting method in the impulse estimating process instead of robust
estimation. This non-smooth numerical technique has been applied to the
under-actuated friction-coupled mulit-body system, by means of an
implementation on the controlled frictional Furuta pendulum. The specifically
designed impulse-based controller has successfully solved the problem of
stabilization of the inverted frictional Furuta pendulum, which is suffered
from the stiction effect of friction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2424</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2424</id><created>2014-12-07</created><updated>2015-02-25</updated><authors><author><keyname>Arablouei</keyname><forenames>Reza</forenames></author><author><keyname>Do&#x11f;an&#xe7;ay</keyname><forenames>Kutluy&#x131;l</forenames></author><author><keyname>Werner</keyname><forenames>Stefan</forenames></author></authors><title>On the Mean-Square Performance of the Constrained LMS Algorithm</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The so-called constrained least mean-square algorithm is one of the most
commonly used linear-equality-constrained adaptive filtering algorithms. Its
main advantages are adaptability and relative simplicity. In order to gain
analytical insights into the performance of this algorithm, we examine its
mean-square performance and derive theoretical expressions for its transient
and steady-state mean-square deviation. Our methodology is inspired by the
principle of energy conservation in adaptive filters. Simulation results
corroborate the accuracy of the derived formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2432</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2432</id><created>2014-12-07</created><updated>2015-06-17</updated><authors><author><keyname>Meeds</keyname><forenames>Edward</forenames></author><author><keyname>Hendriks</keyname><forenames>Remco</forenames></author><author><keyname>Faraby</keyname><forenames>Said Al</forenames></author><author><keyname>Bruntink</keyname><forenames>Magiel</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>MLitB: Machine Learning in the Browser</title><categories>cs.DC cs.LG stat.ML</categories><comments>Revised for PeerJ Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With few exceptions, the field of Machine Learning (ML) research has largely
ignored the browser as a computational engine. Beyond an educational resource
for ML, the browser has vast potential to not only improve the state-of-the-art
in ML research, but also, inexpensively and on a massive scale, to bring
sophisticated ML learning and prediction to the public at large. This paper
introduces MLitB, a prototype ML framework written entirely in JavaScript,
capable of performing large-scale distributed computing with heterogeneous
classes of devices. The development of MLitB has been driven by several
underlying objectives whose aim is to make ML learning and usage ubiquitous (by
using ubiquitous compute devices), cheap and effortlessly distributed, and
collaborative. This is achieved by allowing every internet capable device to
run training algorithms and predictive models with no software installation and
by saving models in universally readable formats. Our prototype library is
capable of training deep neural networks with synchronized, distributed
stochastic gradient descent. MLitB offers several important opportunities for
novel ML research, including: development of distributed learning algorithms,
advancement of web GPU algorithms, novel field and mobile applications, privacy
preserving computing, and green grid-computing. MLitB is available as open
source software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2433</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2433</id><created>2014-12-07</created><updated>2015-06-23</updated><authors><author><keyname>Nagy</keyname><forenames>Marcin</forenames></author><author><keyname>Bui</keyname><forenames>Thanh</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author><author><keyname>Ott</keyname><forenames>Joerg</forenames></author><author><keyname>Sadeghi</keyname><forenames>Ahmad-Reza</forenames></author></authors><title>How Far Removed Are You? Scalable Privacy-Preserving Estimation of
  Social Path Length with Social PaL</title><categories>cs.CR cs.SI</categories><comments>A preliminary version of this paper appears in ACM WiSec 2015. This
  is the full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social relationships are a natural basis on which humans make trust
decisions. Online Social Networks (OSNs) are increasingly often used to let
users base trust decisions on the existence and the strength of social
relationships. While most OSNs allow users to discover the length of the social
path to other users, they do so in a centralized way, thus requiring them to
rely on the service provider and reveal their interest in each other. This
paper presents Social PaL, a system supporting the privacy-preserving discovery
of arbitrary-length social paths between any two social network users. We
overcome the bootstrapping problem encountered in all related prior work,
demonstrating that Social PaL allows its users to find all paths of length two
and to discover a significant fraction of longer paths, even when only a small
fraction of OSN users is in the Social PaL system - e.g., discovering 70% of
all paths with only 40% of the users. We implement Social PaL using a scalable
server-side architecture and a modular Android client library, allowing
developers to seamlessly integrate it into their apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2437</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2437</id><created>2014-12-07</created><updated>2015-10-15</updated><authors><author><keyname>Li</keyname><forenames>Yunpeng</forenames><affiliation>Southeast University</affiliation></author></authors><title>A New Exact Algorithm for Traveling Salesman Problem with Time
  Complexity Interval (O(n^4), O(n^3*2^n))</title><categories>cs.DS cs.DM</categories><comments>22 pages; Another new faster algorithm is proposed in section 7 of
  the updated version. arXiv admin note: text overlap with arXiv:1412.1870</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traveling salesman problem is a NP-hard problem. Until now, researchers have
not found a polynomial time algorithm for traveling salesman problem. Among the
existing algorithms, dynamic programming algorithm can solve the problem in
time O(n^2*2^n) where n is the number of nodes in the graph. The branch-and-cut
algorithm has been applied to solve the problem with a large number of nodes.
However, branch-and-cut algorithm also has an exponential worst-case running
time.
  In this paper, a new exact algorithm for traveling salesman problem is
proposed. The algorithm can be used to solve an arbitrary instance of traveling
salesman problem in real life and the time complexity interval of the algorithm
is (O(n^4), O(n^3*2^n)). It means that for some instances, the algorithm can
find the optimal solution in polynomial time although the algorithm also has an
exponential worst-case running time. In other words, the algorithm tells us
that not all the instances of traveling salesman problem need exponential time
to compute the optimal solution. The algorithm of this paper can not only
assist us to solve traveling salesman problem better, but also can assist us to
deepen the comprehension of the relationship between NP-complete and P.
Therefore, it is considerable in the further research on traveling salesman
problem and NP-hard problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2442</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2442</id><created>2014-12-07</created><authors><author><keyname>Kaadan</keyname><forenames>M. Yahia</forenames></author><author><keyname>Kaadan</keyname><forenames>Asaad</forenames></author></authors><title>Rediscovering the Alphabet - On the Innate Universal Grammar</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal Grammar (UG) theory has been one of the most important research
topics in linguistics since introduced five decades ago. UG specifies the
restricted set of languages learnable by human brain, and thus, many
researchers believe in its biological roots. Numerous empirical studies of
neurobiological and cognitive functions of the human brain, and of many natural
languages, have been conducted to unveil some aspects of UG. This, however,
resulted in different and sometimes contradicting theories that do not indicate
a universally unique grammar. In this research, we tackle the UG problem from
an entirely different perspective. We search for the Unique Universal Grammar
(UUG) that facilitates communication and knowledge transfer, the sole purpose
of a language. We formulate this UG and show that it is unique, intrinsic, and
cosmic, rather than humanistic. Initial analysis on a widespread natural
language already showed some positive results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2444</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2444</id><created>2014-12-07</created><updated>2014-12-09</updated><authors><author><keyname>Kundu</keyname><forenames>Raka</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Lenka</keyname><forenames>Prasanna</forenames></author></authors><title>An Approach for Reducing Outliers of Non Local Means Image Denoising
  Filter</title><categories>cs.CV</categories><comments>The paper presents an improvement in denoising algorithm for
  ultrasound images using the filter non-local means. The paper is accepted in
  MedImage2014 (IISc Bangalore). The research was supported by Centre of
  Excellence in Systems Biology and Biomedical Engineering (TEQIP PHASE-II),
  University of Calcutta and National Institute for the Orthopaedically
  Handicapped, Kolkata, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an adaptive approach for non local means (NLM) image filtering
termed as non local adaptive clipped means (NLACM), which reduces the effect of
outliers and improves the denoising quality as compared to traditional NLM.
Common method to neglect outliers from a data population is computation of mean
in a range defined by mean and standard deviation. In NLACM we perform the
median within the defined range based on statistical estimation of the
neighbourhood region of a pixel to be denoised. As parameters of the range are
independent of any additional input and is based on local intensity values,
hence the approach is adaptive. Experimental results for NLACM show better
estimation of true intensity from noisy neighbourhood observation as compared
to NLM at high noise levels. We have verified the technique for speckle noise
reduction and we have tested it on ultrasound (US) image of lumbar spine. These
ultrasound images act as guidance for injection therapy for treatment of lumbar
radiculopathy. We believe that the proposed approach for image denoising is
first of its kind and its efficiency can be well justified as it shows better
performance in image restoration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2449</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2449</id><created>2014-12-08</created><authors><author><keyname>Munishwar</keyname><forenames>Vikram</forenames></author><author><keyname>Kolar</keyname><forenames>Vinay</forenames></author><author><keyname>Jayachandran</keyname><forenames>Praveen</forenames></author><author><keyname>Kokku</keyname><forenames>Ravi</forenames></author></authors><title>RTChoke: Efficient Real-Time Traffic Chokepoint Detection and Monitoring</title><categories>cs.NI</categories><comments>11 pages, Tech rep of the paper accepted at COMSNETS 2015</comments><acm-class>C.2.4; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel efficient adaptive sensing and monitoring solution for a
system of mobile sensing devices that support traffic monitoring applications.
We make a key observation that much of the variance in commute times arises at
a few congestion hotspots, and a reliable estimate of congestion can be
obtained by selectively monitoring congestion just at these hotspots. We design
a smartphone application and a backend system that automatically identifies and
monitors congestion hotspots. The solution has low resource footprint in terms
of both battery usage on the sensing devices and the network bytes used for
uploading data. When a user is not inside any hotspot zone, adaptive sampling
conserves battery power and reduces network usage, while ensuring that any new
hotspots can be effectively identified. Our results show that our application
consumes 40- 80% less energy than a periodic sampling system for different
routes in our experiments, with similar accuracy of congestion information. The
system can be used for a variety of applications such as automatic congestion
alerts to users approaching hotspots, reliable end-to-end commute time
estimates and effective alternate route suggestions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2455</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2455</id><created>2014-12-08</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author></authors><title>Location Verification Systems for VANETs in Rician Fading Channels</title><categories>cs.NI cs.CR cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose and examine Location Verification Systems (LVSs) for
Vehicular Ad Hoc Networks (VANETs) in the realistic setting of Rician fading
channels. In our LVSs, a single authorized Base Station (BS) equipped with
multiple antennas aims to detect a malicious vehicle that is spoofing its
claimed location. We first determine the optimal attack strategy of the
malicious vehicle, which in turn allows us to analyze the optimal LVS
performance as a function of the Rician $K$-factor of the channel between the
BS and a legitimate vehicle. Our analysis also allows us to formally prove that
the LVS performance limit is independent of the properties of the channel
between the BS and the malicious vehicle, provided the malicious vehicle's
antenna number is above a specified value. We also investigate how tracking
information on a vehicle quantitatively improves the detection performance of
an LVS, showing how optimal performance is obtained under the assumption of the
tracking length being randomly selected. The work presented here can be readily
extended to multiple BS scenarios, and therefore forms the foundation for all
optimal location authentication schemes within the context of Rician fading
channels. Our study closes important gaps in the current understanding of LVS
performance within the context of VANETs, and will be of practical value to
certificate revocation schemes within IEEE 1609.2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2457</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2457</id><created>2014-12-08</created><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author></authors><title>Weighted Polynomial Approximations: Limits for Learning and
  Pseudorandomness</title><categories>cs.CC cs.LG</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial approximations to boolean functions have led to many positive
results in computer science. In particular, polynomial approximations to the
sign function underly algorithms for agnostically learning halfspaces, as well
as pseudorandom generators for halfspaces. In this work, we investigate the
limits of these techniques by proving inapproximability results for the sign
function.
  Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput.
2008) shows that halfspaces can be learned with respect to log-concave
distributions on $\mathbb{R}^n$ in the challenging agnostic learning model. The
power of this algorithm relies on the fact that under log-concave
distributions, halfspaces can be approximated arbitrarily well by low-degree
polynomials. We ask whether this technique can be extended beyond log-concave
distributions, and establish a negative result. We show that polynomials of any
degree cannot approximate the sign function to within arbitrarily low error for
a large class of non-log-concave distributions on the real line, including
those with densities proportional to $\exp(-|x|^{0.99})$.
  Secondly, we investigate the derandomization of Chernoff-type concentration
inequalities. Chernoff-type tail bounds on sums of independent random variables
have pervasive applications in theoretical computer science. Schmidt et al.
(SIAM J. Discrete Math. 1995) showed that these inequalities can be established
for sums of random variables with only $O(\log(1/\delta))$-wise independence,
for a tail probability of $\delta$. We show that their results are tight up to
constant factors.
  These results rely on techniques from weighted approximation theory, which
studies how well functions on the real line can be approximated by polynomials
under various distributions. We believe that these techniques will have further
applications in other areas of computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2458</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2458</id><created>2014-12-08</created><authors><author><keyname>Breu</keyname><forenames>Ruth</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Huber</keyname><forenames>Franz</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schwerin</keyname><forenames>Wolfgang</forenames></author></authors><title>Systems, Views and Models of UML</title><categories>cs.SE</categories><comments>17 pages, 4 figures; The Unified Modeling Language, Technical Aspects
  and Applications Martin Schader, Axel Korthaus (eds.) Physica Verlag,
  Heidelberg. 1998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show by using the example of UML, how a software engineering
method can benefit from an integrative mathematical foundation. The
mathematical foundation is given by a mathematical system model. This model
provides the basis both for integrating the various description techniques of
UML and for implementing methodical support. After describing the basic
concepts of the system model, we give a short overview of the UML description
techniques. Then we show how they fit into the system model framework and
sketch an approach to structure the UML development process such that it
provides methodological guidance for developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2460</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2460</id><created>2014-12-08</created><updated>2014-12-11</updated><authors><author><keyname>Krishnamurthy</keyname><forenames>Supriya</forenames></author><author><keyname>Sumedha</keyname></author></authors><title>An alternate view of complexity in k-SAT problems</title><categories>cond-mat.stat-mech cs.CC</categories><comments>5 pages, 3 figures, typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The satisfiability threshold for constraint satisfaction problems is that
value of the ratio of constraints (or clauses) to variables, above which the
probability that a random instance of the problem has a solution is zero in the
large system limit. Two different approaches to obtaining this threshold have
been discussed in the literature - using first or second-moment methods which
give rigorous bounds or using the non-rigorous but powerful replica-symmetry
breaking (RSB) approach, which gives very accurate predictions on random
graphs. In this paper, we lay out a different route to obtaining this threshold
on a Bethe lattice. We need make no assumptions about the solution-space
structure, a key assumption in the RSB approach. Despite this, our expressions
and threshold values exactly match the best predictions of the cavity method
under the 1-RSB assumption. Our method hence provides alternate interpretations
as well as motivations for the key equations in the RSB approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2461</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2461</id><created>2014-12-08</created><authors><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Klein</keyname><forenames>Cornel</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Enhancing the SysLab System Model with State</title><categories>cs.SE</categories><comments>46 pages, 7 figures; Technical Report TUM-I9631, TU Munich, 1996.
  arXiv admin note: text overlap with arXiv:1411.6027</comments><report-no>TUM-I9631</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, the SYSLAB model is complemented in different ways: State-box
models are provided through timed port automata, for which an operational and a
corresponding denotational semantics are given. Composition is defined for
components modeled in the state-box view as well as for components modeled in
the black-box view. This composition is well-defined for networks of infinitely
many components. To show the applicability of the model, several examples are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2463</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2463</id><created>2014-12-08</created><authors><author><keyname>Saha</keyname><forenames>Amal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Applicability of DUKPT Key Management Scheme to Cloud Wallet and other
  Mobile Payments</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After discussing the concept of DUKPT based symmetric encryption key
management (e.g., for 3DES) and definition of cloud or remote wallet, the paper
analyses applicability of DUKPT to different use cases like mobile banking, NFC
payment using EMV contactless card and mobile based EMV card emulation, web
browser based transaction and cloud or remote wallet.
  Cloud wallet is an emerging payment method and is gaining momentum very fast.
Anticipating that the wallet product managers and security specialists may face
these questions from different stakeholders, the authors have addressed
applicability of DUKPT to cloud wallet use case quite elaborately. As per
knowledge of the authors, this topic has been analysed and discussed for the
first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2470</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2470</id><created>2014-12-08</created><authors><author><keyname>Balaji</keyname><forenames>Nikhil</forenames></author><author><keyname>Datta</keyname><forenames>Samir</forenames></author></authors><title>Bounded Treewidth and Space-Efficient Linear Algebra</title><categories>cs.CC</categories><comments>Replaces http://arxiv.org/abs/1312.7468</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a recent result of Elberfeld, Jakoby and Tantau showing that
$\mathsf{MSO}$ properties are Logspace computable on graphs of bounded
tree-width, we consider the complexity of computing the determinant of the
adjacency matrix of a bounded tree-width graph and as our main result prove
that it is in Logspace. It is important to notice that the determinant is
neither an $\mathsf{MSO}$-property nor counts the number of solutions of an
$\mathsf{MSO}$-predicate. This technique yields Logspace algorithms for
counting the number of spanning arborescences and directed Euler tours in
bounded tree-width digraphs.
  We demonstrate some linear algebraic applications of the determinant
algorithm by describing Logspace procedures for the characteristic polynomial,
the powers of a weighted bounded tree-width graph and feasibility of a system
of linear equations where the underlying bipartite graph has bounded
tree-width.
  Finally, we complement our upper bounds by proving $\mathsf{L}$-hardness of
the problems of computing the determinant, and of powering a bounded tree-width
matrix. We also show the $\mathsf{GapL}$-hardness of Iterated Matrix
Multiplication where each matrix has bounded tree-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2477</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2477</id><created>2014-12-08</created><updated>2014-12-18</updated><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Duan</keyname><forenames>Huiping</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author></authors><title>Super-Resolution Compressed Sensing: A Generalized Iterative Reweighted
  L2 Approach</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1401.4312</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional compressed sensing theory assumes signals have sparse
representations in a known, finite dictionary. Nevertheless, in many practical
applications such as direction-of-arrival (DOA) estimation and line spectral
estimation, the sparsifying dictionary is usually characterized by a set of
unknown parameters in a continuous domain. To apply the conventional compressed
sensing technique to such applications, the continuous parameter space has to
be discretized to a finite set of grid points, based on which a &quot;presumed
dictionary&quot; is constructed for sparse signal recovery. Discretization, however,
inevitably incurs errors since the true parameters do not necessarily lie on
the discretized grid. This error, also referred to as grid mismatch, may lead
to deteriorated recovery performance or even recovery failure. To address this
issue, in this paper, we propose a generalized iterative reweighted L2 method
which jointly estimates the sparse signals and the unknown parameters
associated with the true dictionary. The proposed algorithm is developed by
iteratively decreasing a surrogate function majorizing a given objective
function, resulting in a gradual and interweaved iterative process to refine
the unknown parameters and the sparse signal. A simple yet effective scheme is
developed for adaptively updating the regularization parameter that controls
the tradeoff between the sparsity of the solution and the data fitting error.
Extension of the proposed algorithm to the multiple measurement vector scenario
is also considered. Numerical results show that the proposed algorithm achieves
a super-resolution accuracy and presents superiority over other existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2485</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2485</id><created>2014-12-08</created><authors><author><keyname>Nathan</keyname><forenames>Vikram</forenames></author><author><keyname>Raghvendra</keyname><forenames>Sharath</forenames></author></authors><title>Accurate Streaming Support Vector Machines</title><categories>cs.LG</categories><comments>2 figures, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widely-used tool for binary classification is the Support Vector Machine
(SVM), a supervised learning technique that finds the &quot;maximum margin&quot; linear
separator between the two classes. While SVMs have been well studied in the
batch (offline) setting, there is considerably less work on the streaming
(online) setting, which requires only a single pass over the data using
sub-linear space. Existing streaming algorithms are not yet competitive with
the batch implementation. In this paper, we use the formulation of the SVM as a
minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based
off of the blurred ball cover originally proposed by Agarwal and Sharathkumar.
Our implementation consistently outperforms existing streaming SVM approaches
and provides higher accuracies than libSVM on several datasets, thus making it
competitive with the standard SVM batch implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2486</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2486</id><created>2014-12-08</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>Optimization models of natural communication</title><categories>physics.soc-ph cs.CL physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of information theoretic models of communication was introduced more
than a decade ago to explain the origins of Zipf's law for word frequencies.
The family is a based on a combination of two information theoretic principles:
maximization of mutual information between forms and meanings and minimization
of form entropy. The family also sheds light on the origins of three other
patterns: the principle of contrast, a related a vocabulary learning bias and
the meaning-frequency law. Here two important components of the family, namely
the information theoretic principles and the energy function that combines them
linearly, are reviewed from the perspective of psycholinguistics, language
learning, information theory and synergetic linguistics. The minimization of
this linear function resembles a sort of agnostic information theoretic model
selection that might be tuned by self-organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2487</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2487</id><created>2014-12-08</created><updated>2016-02-10</updated><authors><author><keyname>Blythe</keyname><forenames>Richard A.</forenames></author><author><keyname>Smith</keyname><forenames>Andrew D. M.</forenames></author><author><keyname>Smith</keyname><forenames>Kenny</forenames></author></authors><title>Word learning under infinite uncertainty</title><categories>physics.soc-ph cs.CL</categories><comments>30 pages, 4 figures, contains considerable extra discussion and
  relaxation of original model assumptions. Version to appear in Cognition</comments><journal-ref>Cognition (2016) v151 pp18-27</journal-ref><doi>10.1016/j.cognition.2016.02.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language learners must learn the meanings of many thousands of words, despite
those words occurring in complex environments in which infinitely many meanings
might be inferred by the learner as a word's true meaning. This problem of
infinite referential uncertainty is often attributed to Willard Van Orman
Quine. We provide a mathematical formalisation of an ideal cross-situational
learner attempting to learn under infinite referential uncertainty, and
identify conditions under which word learning is possible. As Quine's
intuitions suggest, learning under infinite uncertainty is in fact possible,
provided that learners have some means of ranking candidate word meanings in
terms of their plausibility; furthermore, our analysis shows that this ranking
could in fact be exceedingly weak, implying that constraints which allow
learners to infer the plausibility of candidate word meanings could themselves
be weak. This approach lifts the burden of explanation from `smart' word
learning constraints in learners, and suggests a programme of research into
weak, unreliable, probabilistic constraints on the inference of word meaning in
real word learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2494</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2494</id><created>2014-12-08</created><authors><author><keyname>Hu</keyname><forenames>Yan</forenames></author><author><keyname>Bai</keyname><forenames>Guohua</forenames></author></authors><title>A systematic literature review of cloud computing in eHealth</title><categories>cs.CY</categories><journal-ref>Health Informatics: An International Journal, ISSN:2319-2046,
  Vol.3 No.4, Novermber 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing in eHealth is an emerging area for only few years. There
needs to identify the state of the art and pinpoint challenges and possible
directions for researchers and applications developers. Based on this need, we
have conducted a systematic review of cloud computing in eHealth. We searched
ACM Digital Library, IEEE Xplore, Inspec, ISI Web of Science and Springer as
well as relevant open-access journals for relevant articles. A total of 237
studies were first searched, of which 44 papers met the Include Criteria. The
studies identified three types of studied areas about cloud computing in
eHealth, namely (1) cloud-based eHealth framework design (n=13); (2)
applications of cloud computing (n=17); and (3) security or privacy control
mechanisms of healthcare data in the cloud (n=14). Most of the studies in the
review were about designs and concept-proof. Only very few studies have
evaluated their research in the real world, which may indicate that the
application of cloud computing in eHealth is still very immature. However, our
presented review could pinpoint that a hybrid cloud platform with mixed access
control and security protection mechanisms will be a main research area for
developing citizen centred home-based healthcare applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2495</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2495</id><created>2014-12-08</created><authors><author><keyname>Bhatia</keyname><forenames>Priyanka</forenames></author><author><keyname>Sumbaly</keyname><forenames>Ronak</forenames></author></authors><title>Framework for Wireless Network Security using Quantum Cryptography</title><categories>cs.CR</categories><comments>17 pages, 11 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.6, November 2014</journal-ref><doi>10.5121/ijcnc.2014.6604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data that is transient over an unsecured wireless network is always
susceptible to being intercepted by anyone within the range of the wireless
signal. Hence providing secure communication to keep the user information and
devices safe when connected wirelessly has become one of the major concerns.
Quantum cryptography provides a solution towards absolute communication
security over the network by encoding information as polarized photons, which
can be sent through the air. This paper explores on the aspect of application
of quantum cryptography in wireless networks. In this paper we present a
methodology for integrating quantum cryptography and security of IEEE 802.11
wireless networks in terms of distribution of the encryption keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2497</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2497</id><created>2014-12-08</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Cohen</keyname><forenames>Johanne</forenames></author><author><keyname>Rabie</keyname><forenames>Mika&#xeb;l</forenames></author></authors><title>Homonym Population Protocols, or Providing a Small Space of Computation
  Using a Few Identifiers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols have been introduced by Angluin et al. as a model in
which n passively mobile anonymous finite-state agents stably compute a
predicate on the multiset of their inputs via interactions by pairs. The model
has been extended by Guerraoui and Ruppert to yield the community protocol
models where agents have unique identifiers but may only store a finite number
of the identifiers they already heard about. The population protocol models can
only compute semi-linear predicates, whereas in the community protocol model
the whole community of agents provides collectively the power of a Turing
machine with a O(n log n) space. We consider variations on the above models and
we obtain a whole landscape that covers and extends already known results: By
considering the case of homonyms, that is to say the case when several agents
may share the same identifier, we provide a hierarchy that goes from the case
of no identifier (i.e. a single one for all, i.e. the population protocol
model) to the case of unique identifiers (i.e. community protocol model). We
obtain in particular that any Turing Machine on space O(logO(1) n) can be
simulated with at least O(logO(1) n) identifiers, a result filling a gap left
open in all previous studies. Our results also extend and revisit in particular
the hierarchy provided by Chatzigiannakis et al. on population protocols
carrying Turing Machines on limited space, solving the problem of the gap left
by this work between per-agent space o(log log n) (proved to be equivalent to
population protocols) and O(log n) (proved to be equivalent to Turing
machines).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2502</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2502</id><created>2014-12-08</created><authors><author><keyname>Thanh</keyname><forenames>Cao Thai Phuong</forenames></author><author><keyname>Nam</keyname><forenames>Ha Hai</forenames></author><author><keyname>Hung</keyname><forenames>Tran Cong</forenames></author></authors><title>Solving bandwidth-guaranteed routing problem using routing data</title><categories>cs.NI</categories><comments>published in The International Journal of Computer Networks &amp;
  Communications (IJCNC), vol. 6, no. 6</comments><doi>10.5121/ijcnc.2014.6607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a traffic engineering routing algorithm that aims to
accept as many routing demands as possible on the condition that a certain
amount of bandwidth resource is reserved for each accepted demand. The novel
idea is to select routes based on not only network states but also information
derived from routing data such as probabilities of the ingress egress pairs and
usage frequencies of the links. Experiments with respect to acceptance ratio
and computation time have been conducted against various test sets. Results
indicate that the proposed algorithm has better performance than the existing
popular algorithms including Minimum Interference Routing Algorithm (MIRA) and
Random Race based Algorithm for Traffic Engineering (RRATE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2511</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2511</id><created>2014-12-08</created><authors><author><keyname>da Cruz</keyname><forenames>Ederval Pablo Ferreira</forenames></author><author><keyname>Gottardo</keyname><forenames>Luis Eduardo</forenames></author><author><keyname>Rossini</keyname><forenames>Franciele Pereira</forenames></author><author><keyname>Oliveira</keyname><forenames>Vinicius de Souza</forenames></author><author><keyname>Pereira</keyname><forenames>Lucas Cellim</forenames></author></authors><title>Evaluating Feasibility of Using Wireless Sensor Networks in a Coffee
  Crop Through Simulation of AODV, AOMDV, DSDV and Their Variants with 802.15.4
  Mac Protocol</title><categories>cs.NI</categories><journal-ref>nternational Journal of Computer Networks &amp; Communications (IJCNC)
  Vol.6, No.6, November 2014</journal-ref><doi>10.5121/ijcnc.2014.6602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wireless Sensor Networks is a network formed with sensors that have
characteristics to sensor an area to extract a specific metric, depending of
the application. We would like to analyse the feasibility to use sensors in a
coffee crop. In this work we are evaluating routing protocols using real
dimensions and characteristics of a coffee crop. We evaluate, through
simulation, AODV, DSDV and AOMDV and two variants known in this work as AODVMOD
and AOMDVMOD with 802.15.4 MAC Protocol. For this comparison, we defined three
performance metrics: Packet Delivery Ratio (PDR), End -to-End Delay and Average
Energy Consumption. Simulation results show that AOMDVMOD overall, outperforms
others routing protocols evaluated, showing that is possible to use WSN in a
real coffee crop environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2526</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2526</id><created>2014-12-08</created><authors><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author></authors><title>Exploiting Packing Components in General-Purpose Integer Programming
  Solvers</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of packing boxes into a large box is often a part of a larger
problem. For example in furniture supply chain applications, one needs to
decide what trucks to use to transport furniture between production sites and
distribution centers and stores, such that the furniture fits inside. Such
problems are often formulated and sometimes solved using general-purpose
integer programming solvers.
  This chapter studies the problem of identifying a compact formulation of the
multi-dimensional packing component in a general instance of integer linear
programming, reformulating it using the discretisation of
Allen--Burke--Marecek, and and solving the extended reformulation. Results on
instances of up to 10000000 boxes are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2543</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2543</id><created>2014-12-08</created><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author></authors><title>Asymptotically Optimal Matching of Multiple Sequences to Source
  Distributions and Training Sequences</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2014.2374157</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a finite set of sources, each producing i.i.d. observations that
follow a unique probability distribution on a finite alphabet. We study the
problem of matching a finite set of observed sequences to the set of sources
under the constraint that the observed sequences are produced by distinct
sources. In general, the number of sequences $N$ may be different from the
number of sources $M$, and only some $K \leq \min\{M,N\}$ of the observed
sequences may be produced by a source from the set of sources of interest. We
consider two versions of the problem -- one in which the probability laws of
the sources are known, and another in which the probability laws of the sources
are unspecified but one training sequence from each of the sources is
available. We show that both these problems can be solved using a sequence of
tests that are allowed to produce &quot;no-match&quot; decisions. The tests ensure
exponential decay of the probabilities of incorrect matching as the sequence
lengths increase, and minimize the &quot;no-match&quot; decisions. Both tests can be
implemented using variants of the minimum weight matching algorithm applied to
a weighted bipartite graph. We also compare the performances obtained by using
these tests with those obtained by using tests that do not take into account
the constraint that the sequences are produced by distinct sources. For the
version of the problem in which the probability laws of the sources are known,
we compute the rejection exponents and error exponents of the tests and show
that tests that make use of the constraint have better exponents than tests
that do not make use of this information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2544</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2544</id><created>2014-12-08</created><updated>2016-02-18</updated><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Multi-Player Diffusion Games on Graph Classes</title><categories>cs.GT</categories><comments>Extended version of the TAMC 2015 conference version now discussing
  hypercube results</comments><msc-class>91A06, 91A10, 91A40, 91A43, 91A50, 91A24</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study competitive diffusion games on graphs introduced by Alon et al. [1]
to model the spread of influence in social networks. Extending results of
Roshanbin [8] for two players, we investigate the existence of pure Nash
equilibria for at least three players on different classes of graphs including
paths, cycles, grid graphs and hypercubes; as a main contribution, we answer an
open question proving that there is no Nash equilibrium for three players on (m
x n) grids with min(m, n) &gt;= 5. Further, extending results of Etesami and Basar
[3] for two players, we prove the existence of pure Nash equilibria for four
players on every d-dimensional hypercube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2546</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2546</id><created>2014-12-08</created><authors><author><keyname>Dobslaw</keyname><forenames>Felix</forenames></author><author><keyname>Zhang</keyname><forenames>Tingting</forenames></author><author><keyname>Gidlund</keyname><forenames>Mikael</forenames></author></authors><title>End-to-End Reliability-aware Scheduling for Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSN) are gaining popularity as a flexible and
economical alternative to field-bus installations for monitoring and control
applications. For mission-critical applications, communication networks must
provide end-to-end reliability guarantees, posing substantial challenges for
WSN. Reliability can be improved by redundancy, and is often addressed on the
MAC layer by re-submission of lost packets, usually applying slotted
scheduling. Recently, researchers have proposed a strategy to optimally improve
the reliability of a given schedule by repeating the most rewarding slots in a
schedule incrementally until a deadline. This Incrementer can be used with most
scheduling algorithms but has scalability issues which narrows its usability to
offline calculations of schedules, for networks that are rather static. In this
paper, we introduce SchedEx, a generic heuristic scheduling algorithm extension
which guarantees a user-defined end-to-end reliability. SchedEx produces
competitive schedules to the existing approach, and it does that consistently
more than an order of magnitude faster. The harsher the end-to-end reliability
demand of the network, the better SchedEx performs compared to the Incrementer.
We further show that SchedEx has a more evenly distributed improvement impact
on the scheduling algorithms, whereas the Incrementer favors schedules created
by certain scheduling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2559</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2559</id><created>2014-12-08</created><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>On the complexity of the vector connectivity problem</title><categories>cs.DM cs.CC math.CO</categories><comments>14 pages</comments><msc-class>05C40, 05C85, 90C27, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a relaxation of the Vector Domination problem called Vector
Connectivity (VecCon). Given a graph $G$ with a requirement $r(v)$ for each
vertex $v$, VecCon asks for a minimum cardinality set $S$ of vertices such that
every vertex $v\in V\setminus S$ is connected to $S$ via $r(v)$ disjoint paths.
In the paper introducing the problem, Boros et al. [Networks, 2014] gave
polynomial-time solutions for VecCon in trees, cographs, and split graphs, and
showed that the problem can be approximated in polynomial time on $n$-vertex
graphs to within a factor of $\log n+2$, leaving open the question of whether
the problem is NP-hard on general graphs. We show that VecCon is APX-hard in
general graphs, and NP-hard in planar bipartite graphs and in planar line
graphs. We also generalize the polynomial result for trees by solving the
problem for block graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2562</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2562</id><created>2014-12-08</created><authors><author><keyname>Delos</keyname><forenames>Vincent</forenames><affiliation>I2M</affiliation></author><author><keyname>Teissandier</keyname><forenames>Denis</forenames><affiliation>I2M</affiliation></author></authors><title>Minkowski sum of HV-polytopes in Rn</title><categories>cs.CG cs.MS physics.class-ph</categories><comments>4th Annual International Conference on Computational Mathematics,
  Computational Geometry and Statistics, Jan 2015, Singapore, Singapore</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minkowski sums cover a wide range of applications in many different fields
like algebra, morphing, robotics, mechanical CAD/CAM systems ... This paper
deals with sums of polytopes in a n dimensional space provided that both
H-representation and V-representation are available i.e. the polytopes are
described by both their half-spaces and vertices. The first method uses the
polytope normal fans and relies on the ability to intersect dual polyhedral
cones. Then we introduce another way of considering Minkowski sums of polytopes
based on the primal polyhedral cones attached to each vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2564</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2564</id><created>2014-12-08</created><updated>2015-06-16</updated><authors><author><keyname>Delos</keyname><forenames>Vincent</forenames><affiliation>I2M</affiliation></author><author><keyname>Teissandier</keyname><forenames>Denis</forenames><affiliation>I2M</affiliation></author></authors><title>Minkowski Sum of Polytopes Defined by Their Vertices</title><categories>cs.CG cs.MS physics.class-ph</categories><proxy>ccsd</proxy><journal-ref>Journal of Applied Mathematics and Physics (JAMP), Scientific
  Research Publishing, 2015, 3 (1), pp.62-67</journal-ref><doi>10.4236/jamp.2015.31008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minkowski sums are of theoretical interest and have applications in fields
related to industrial backgrounds. In this paper we focus on the specific case
of summing polytopes as we want to solve the tolerance analysis problem
described in [1]. Our approach is based on the use of linear programming and is
solvable in polynomial time. The algorithm we developed can be implemented and
parallelized in a very easy way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2566</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2566</id><created>2014-12-08</created><updated>2015-06-28</updated><authors><author><keyname>Kala</keyname><forenames>Srikant Manas</forenames></author><author><keyname>Reddy</keyname><forenames>M. Pavan Kumar</forenames></author><author><keyname>Musham</keyname><forenames>Ranadheer</forenames></author><author><keyname>Tamma</keyname><forenames>Bheemarjuna Reddy</forenames></author></authors><title>Interference Mitigation In Wireless Mesh Networks Through Radio
  Co-location Aware Conflict Graphs</title><categories>cs.NI</categories><comments>The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11276-015-1002-4</comments><journal-ref>Wireless Networks, February 2016, Volume 22, Issue 2, pp 679-702</journal-ref><doi>10.1007/s11276-015-1002-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Mesh Networks (WMNs) have evolved into a wireless communication
technology of immense interest. But technological advancements in WMNs have
inadvertently spawned a plethora of network performance bottlenecks, caused
primarily by the rise in prevalent interference. Conflict Graphs are
indispensable tools used to theoretically represent and estimate the
interference in wireless networks. We propose a generic algorithm to generate
conflict graphs which is independent of the underlying interference model.
Further, we propose the notion of radio co-location interference, which is
caused and experienced by spatially co-located radios in multi-radio
multi-channel (MRMC) WMNs. We experimentally validate the concept, and propose
a new all-encompassing algorithm to create a radio co-location aware conflict
graph. Our novel conflict graph generation algorithm is demonstrated to be
significantly superior and more efficient than the conventional approach,
through theoretical interference estimates and comprehensive experiments. The
results of an extensive set of ns-3 simulations run on the IEEE 802.11g
platform strongly indicate that the radio co-location aware conflict graphs are
a marked improvement over their conventional counterparts. We also question the
use of total interference degree as a reliable metric to predict the
performance of a Channel Assignment scheme in a given WMN deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2570</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2570</id><created>2014-12-08</created><authors><author><keyname>Brunisholz</keyname><forenames>Pierre</forenames><affiliation>Grenoble INP</affiliation></author><author><keyname>Minier</keyname><forenames>Marine</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon</affiliation></author><author><keyname>Valois</keyname><forenames>Fabrice</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>The Gain of Network Coding in Wireless Sensor Networking</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks have some well known features such as low battery
consumption, changing topology awareness, open environment, non reliable radio
links, etc.In this paper, we investigate the benefits of Network Coding
Wireless Sensor networking, especially resiliency.One of our main concern is
the resiliency in Wireless Sensor Networks.We have seen that resiliency could
be described as a multi dimensional metric
\cite{5478822,erdene2011enhancing,6423640} taking parameters such as Average
Delivery Ratio, Delay Efficiency, Energy Efficiency, Average Throughput and
Delivery Fairness into account.Resiliency can then be graphically represented
as a kiviat diagram created by the previous weighted parameters.In order to
introduce these metrics, previous works have been leaded on the Random Gradient
Based Routing, which proved good resiliency in malicious environment.We look
for seeing the improvements in term of resiliency, when adding network coding
in the Random Gradient Based Routing with malicious nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2588</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2588</id><created>2014-12-08</created><authors><author><keyname>Vinay</keyname><forenames>S</forenames></author><author><keyname>Aithal</keyname><forenames>Shridhar</forenames></author><author><keyname>Adiga</keyname><forenames>Sudhakara</forenames></author></authors><title>Integrating goals after prioritization and evaluation-A Goal-oriented
  requirements engineering method</title><categories>cs.SE</categories><comments>IJSEA 2014</comments><doi>10.5121/ijsea.2014.5604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision support system in Requirements engineering plays an important role
in software development life cycle. The relationship between functional and
non-functional requirements often plays a crucial role in resolving conflicts
or arriving at decisions in requirements engineering phase. Goal-Oriented
Requirements Engineering (GORE) methods make a good attempt of addressing these
aspects which are helpful in decision support. We propose a GORE method -
Integrating goals after prioritization and evaluation (IGAPE). The method is
semi-formal in nature thereby ensuring active stakeholder participation. In
this paper we elaborate the various steps of IGAPE method. The output of IGAPE
is then given as input to a decision support system which makes use of Analytic
Hierarchy Process (AHP) and Technique for Order of Preference by Similarity to
Ideal Solution (TOPSIS). Integration of IGAPE with AHP and TOPSIS will clearly
provide a rationale for various decisions which are arrived at during the
requirements engineering phase. The method is illustrated for an e-commerce
application and is validated by expert analysis approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2595</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2595</id><created>2014-11-22</created><authors><author><keyname>Decuyper</keyname><forenames>Adeline</forenames></author><author><keyname>Rutherford</keyname><forenames>Alex</forenames></author><author><keyname>Wadhwa</keyname><forenames>Amit</forenames></author><author><keyname>Bauer</keyname><forenames>Jean-Martin</forenames></author><author><keyname>Krings</keyname><forenames>Gautier</forenames></author><author><keyname>Gutierrez</keyname><forenames>Thoralf</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author><author><keyname>Luengo-Oroz</keyname><forenames>Miguel A.</forenames></author></authors><title>Estimating Food Consumption and Poverty Indices with Mobile Phone Data</title><categories>cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown the value of mobile phone data to tackle problems
related to economic development and humanitarian action. In this research, we
assess the suitability of indicators derived from mobile phone data as a proxy
for food security indicators. We compare the measures extracted from call
detail records and airtime credit purchases to the results of a nationwide
household survey conducted at the same time. Results show high correlations (&gt;
.8) between mobile phone data derived indicators and several relevant food
security variables such as expenditure on food or vegetable consumption. This
correspondence suggests that, in the future, proxies derived from mobile phone
data could be used to provide valuable up-to-date operational information on
food security throughout low and middle income countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2600</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2600</id><created>2014-12-08</created><updated>2015-03-30</updated><authors><author><keyname>Ye</keyname><forenames>Zakaria</forenames></author><author><keyname>EL-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Jimenez</keyname><forenames>Tania</forenames></author><author><keyname>Xu</keyname><forenames>Yuedong</forenames></author></authors><title>Computing Quality of Experience of Video Streaming in Network with
  Long-Range-Dependent Traffic</title><categories>cs.NI</categories><comments>10 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We take an analytical approach to study the Quality of user Experience (QoE)
for video streaming applications. Our propose is to characterize buffer
starvations for streaming video with Long-Range-Dependent (LRD) input traffic.
Specifically we develop a new analytical framework to investigate Quality of
user Experience (QoE) for streaming by considering a Markov Modulated Fluid
Model (MMFM) that accurately approximates the Long Range Dependence (LRD)
nature of network traffic. We drive the close-form expressions for calculating
the distribution of starvation as well as start-up delay using partial
differential equations (PDEs) and solve them using the Laplace Transform. We
illustrate the results with the cases of the two-state Markov Modulated Fluid
Model that is commonly used in multimedia applications. We compare our
analytical model with simulation results using ns-3 under various operating
parameters. We further adopt the model to analyze the effect of bitrate
switching on the starvation probability and start-up delay. Finally, we apply
our analysis results to optimize the objective quality of experience (QoE) of
media streaming realizing the tradeoff among different metrics incorporating
user preferences on buffering ratio, startup delay and perceived quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2601</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2601</id><created>2014-12-08</created><updated>2015-03-05</updated><authors><author><keyname>Rabbany</keyname><forenames>Reihaneh</forenames></author><author><keyname>Za&#xef;ane</keyname><forenames>Osmar R.</forenames></author></authors><title>Generalization of Clustering Agreements and Distances for Overlapping
  Clusters and Network Communities</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Data Mining and Knowledge Discovery: Volume 29, Issue 5 (2015)</journal-ref><doi>10.1007/s10618-015-0426-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A measure of distance between two clusterings has important applications,
including clustering validation and ensemble clustering. Generally, such
distance measure provides navigation through the space of possible clusterings.
Mostly used in cluster validation, a normalized clustering distance, a.k.a.
agreement measure, compares a given clustering result against the ground-truth
clustering. Clustering agreement measures are often classified into two
families of pair-counting and information theoretic measures, with the
widely-used representatives of Adjusted Rand Index (ARI) and Normalized Mutual
Information (NMI), respectively. This paper sheds light on the relation between
these two families through a generalization. It further presents an alternative
algebraic formulation for these agreement measures which incorporates an
intuitive clustering distance, which is defined based on the analogous between
cluster overlaps and co-memberships of nodes in clusters. Unlike the original
measures, it is easily extendable for different cases, including overlapping
clusters and clusters of inter-related data for complex networks. These two
extensions are, in particular, important in the context of finding clusters in
social and information networks, a.k.a communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2604</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2604</id><created>2014-12-08</created><updated>2015-05-05</updated><authors><author><keyname>Gkioxari</keyname><forenames>Georgia</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Actions and Attributes from Wholes and Parts</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the importance of parts for the tasks of action and attribute
classification. We develop a part-based approach by leveraging convolutional
network features inspired by recent advances in computer vision. Our part
detectors are a deep version of poselets and capture parts of the human body
under a distinct set of poses. For the tasks of action and attribute
classification, we train holistic convolutional neural networks and show that
adding parts leads to top-performing results for both tasks. In addition, we
demonstrate the effectiveness of our approach when we replace an oracle person
detector, as is the default in the current evaluation protocol for both tasks,
with a state-of-the-art person detection system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2620</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2620</id><created>2014-12-08</created><updated>2016-02-16</updated><authors><author><keyname>Leifert</keyname><forenames>G.</forenames><affiliation>University of Rostock</affiliation></author><author><keyname>Strau&#xdf;</keyname><forenames>T.</forenames><affiliation>University of Rostock</affiliation></author><author><keyname>Gr&#xfc;ning</keyname><forenames>T.</forenames><affiliation>University of Rostock</affiliation></author><author><keyname>Labahn</keyname><forenames>R.</forenames><affiliation>University of Rostock</affiliation></author></authors><title>Cells in Multidimensional Recurrent Neural Networks</title><categories>cs.AI cs.NE</categories><msc-class>68T10, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transcription of handwritten text on images is one task in machine
learning and one solution to solve it is using multi-dimensional recurrent
neural networks (MDRNN) with connectionist temporal classification (CTC). The
RNNs can contain special units, the long short-term memory (LSTM) cells. They
are able to learn long term dependencies but they get unstable when the
dimension is chosen greater than one. We defined some useful and necessary
properties for the one-dimensional LSTM cell and extend them in the
multi-dimensional case. Thereby we introduce several new cells with better
stability. We present a method to design cells using the theory of linear shift
invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT
and Rimes database, where we can improve the recognition rate compared to the
LSTM cell. So each application where the LSTM cells in MDRNNs are used could be
improved by substituting them by the new developed cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2624</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2624</id><created>2014-12-08</created><updated>2015-08-18</updated><authors><author><keyname>Bensmail</keyname><forenames>Julien</forenames><affiliation>LIP</affiliation></author><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames><affiliation>LIP</affiliation></author><author><keyname>Valicov</keyname><forenames>Petru</forenames><affiliation>LIF</affiliation></author></authors><title>Strong edge-coloring of $(3, \Delta)$-bipartite graphs</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong edge-coloring of a graph $G$ is an assignment of colors to edges
such that every color class induces a matching. We here focus on bipartite
graphs whose one part is of maximum degree at most $3$ and the other part is of
maximum degree $\Delta$. For every such graph, we prove that a strong
$4\Delta$-edge-coloring can always be obtained. Together with a result of
Steger and Yu, this result confirms a conjecture of Faudree, Gy\'arf\'as,
Schelp and Tuza for this class of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2643</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2643</id><created>2014-12-08</created><updated>2015-06-11</updated><authors><author><keyname>Beggs</keyname><forenames>Edwin</forenames></author><author><keyname>Tucker</keyname><forenames>John V.</forenames></author></authors><title>Analogue-digital systems with modes of physical behaviour</title><categories>cs.SE cs.LO cs.SY</categories><comments>32 pages. Ver 2: Updated with additional material and figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex environments, processes and systems may exhibit several distinct
modes of physical behaviour or operation. Thus, for their design, a set of
mathematical models may be needed, each model having its own domain of
application and representing a particular mode of behaviour or operation of
physical reality. The models may be of disparate kinds and, furthermore, not
all physical modes may have a reliable model. What is a mode of behaviour? How
do we specify algorithms and software that monitor or govern a complex physical
situation with many modes? How do we specify a portfolio of modes, and the
computational problem of transitioning from using one mode to another mode as
physical modes change? We propose a general definition of an analogue-digital
system with modes. We show how any diverse set of modes, with or without
models, can be bound together, and how the transitions between modes/models can
be determined, by constructing a data type based upon a simplicial complex. We
illustrate the ideas of physical modes and our theory by reflecting on simple
examples, including driverless racing cars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2651</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2651</id><created>2014-12-08</created><updated>2015-03-04</updated><authors><author><keyname>Satpathi</keyname><forenames>Siddhartha</forenames></author><author><keyname>Nagda</keyname><forenames>Rushil</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Optimal Offline and Competitive Online Strategies for
  Transmitter-Receiver Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>This paper is an extended version of the one to appear in Proc. IEEE
  ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A joint transmitter-receiver energy harvesting model is considered, where
both the transmitter and receiver are powered by (renewable) energy harvesting
source. Given a fixed number of bits, the problem is to find the optimal
transmission power profile at the transmitter and ON-OFF profile at the
receiver to minimize the transmission time. With infinite capacity at both the
transmitter and receiver, optimal offline and optimal online policies are
derived. The optimal online policy is shown to be two-competitive in the
arbitrary input case. With finite battery capacities at both ends, only random
energy arrival sequence with given distribution are considered, for which an
online policy with bounded expected competitive ratio is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2662</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2662</id><created>2014-12-08</created><updated>2016-02-13</updated><authors><author><keyname>Zakablukov</keyname><forenames>Dmitry V.</forenames></author></authors><title>On Gate Complexity of Reversible Circuits Consisting of NOT, CNOT and
  2-CNOT Gates</title><categories>cs.ET</categories><comments>In Russian, 18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the gate complexity of reversible circuits consisting of
NOT, CNOT and 2-CNOT gates. The Shannon gate complexity function $L(n, q)$ for
a reversible circuit, implementing a Boolean transformation $f\colon \mathbb
Z_2^n \to \mathbb Z_2^n$, is defined as a function of $n$ and the number of
additional inputs $q$. The general lower bound $L(n,q) \geq
\frac{2^n(n-2)}{3\log_2(n+q)} - \frac{n}{3}$ for the gate complexity of a
reversible circuit is proved. An upper bound $L(n,0) \leqslant
3n2^{n+4}(1+o(1)) \mathop / \log_2n$ for the gate complexity of a reversible
circuit without additional inputs is proved. An upper bound $L(n,q_0) \lesssim
2^n$ for the gate complexity of a reversible circuit with $q_0 \sim
n2^{n-o(n)}$ additional inputs is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2667</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2667</id><created>2014-12-08</created><updated>2015-03-22</updated><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Paz</keyname><forenames>Ami</forenames></author></authors><title>Computing Exact Distances in the Congested Clique</title><categories>cs.DC cs.DS</categories><comments>This paper has been withdrawn by the authors. This paper has been
  superseded by arXiv:1503.04963 (merged from arXiv:1412.2109 and
  arXiv:1412.2667)</comments><acm-class>D.1.3; F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives simple distributed algorithms for the fundamental problem of
computing graph distances in the Congested Clique model. One of the main
components of our algorithms is fast matrix multiplication, for which we show
an $O(n^{1/3})$-round algorithm when the multiplication needs to be performed
over a semi-ring, and an $O(n^{0.157})$-round algorithm when the computation
can be performed over a field. We propose to denote by $\kappa$ the exponent of
matrix multiplication in this model, which gives $\kappa &lt; 0.157$.
  We show how to compute all-pairs-shortest-paths (APSP) in $O(n^{1/3}\log{n})$
rounds in weighted graphs of $n$ nodes, implying also the computation of the
graph diameter $D$. In unweighted graphs, APSP can be computed in
$O(\min\{n^{1/3}\log{D},n^{\kappa} D\})$ rounds, and the diameter can be
computed in $O(n^{\kappa}\log{D})$ rounds. Furthermore, we show how to compute
the girth of a graph in $O(n^{1/3})$ rounds, and provide triangle detection and
4-cycle detection algorithms that complete in $O(n^{\kappa})$ rounds.
  All our algorithms are deterministic. Our triangle detection and 4-cycle
detection algorithms improve upon the previously best known algorithms in this
model, and refute a conjecture that $\tilde \Omega (n^{1/3})$ rounds are
required for detecting triangles by any deterministic oblivious algorithm. Our
distance computation algorithms are exact, and improve upon the previously best
known $\tilde O(n^{1/2})$ algorithm of Nanongkai [STOC 2014] for computing a
$(2+o(1))$-approximation of APSP.
  Finally, we give lower bounds that match the above for natural families of
algorithms. For the Congested Clique Broadcast model, we derive unconditioned
lower bounds for matrix multiplication and APSP. The matrix multiplication
algorithms and lower bounds are adapted from parallel computations, which is a
connection of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2669</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2669</id><created>2014-12-05</created><updated>2015-06-02</updated><authors><author><keyname>Biswas</keyname><forenames>Sampurna</forenames></author><author><keyname>Poddar</keyname><forenames>Sunrita</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author><author><keyname>Mudumbai</keyname><forenames>Raghuraman</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Two step recovery of jointly sparse and low-rank matrices: theoretical
  guarantees</title><categories>stat.ML cs.IT math.IT</categories><comments>4 pages, 4 figures, ISBI 2015 conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a two step algorithm with theoretical guarantees to recover a
jointly sparse and low-rank matrix from undersampled measurements of its
columns. The algorithm first estimates the row subspace of the matrix using a
set of common measurements of the columns. In the second step, the subspace
aware recovery of the matrix is solved using a simple least square algorithm.
The results are verified in the context of recovering CINE data from
undersampled measurements; we obtain good recovery when the sampling conditions
are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2672</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2672</id><created>2014-12-08</created><authors><author><keyname>Gao</keyname><forenames>Tao</forenames></author><author><keyname>Harari</keyname><forenames>Daniel</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua</forenames></author><author><keyname>Ullman</keyname><forenames>Shimon</forenames></author></authors><title>When Computer Vision Gazes at Cognition</title><categories>cs.AI cs.CV</categories><comments>Tao Gao and Daniel Harari contributed equally to this work</comments><report-no>CBMM Memo No. 025, MIT</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint attention is a core, early-developing form of social interaction. It is
based on our ability to discriminate the third party objects that other people
are looking at. While it has been shown that people can accurately determine
whether another person is looking directly at them versus away, little is known
about human ability to discriminate a third person gaze directed towards
objects that are further away, especially in unconstraint cases where the
looker can move her head and eyes freely. In this paper we address this
question by jointly exploring human psychophysics and a cognitively motivated
computer vision model, which can detect the 3D direction of gaze from 2D face
images. The synthesis of behavioral study and computer vision yields several
interesting discoveries. (1) Human accuracy of discriminating targets
8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gaze
task; (2) The ability to interpret gaze of different lookers vary dramatically;
(3) This variance can be captured by the computational model; (4) Human
outperforms the current model significantly. These results collectively show
that the acuity of human joint attention is indeed highly impressive, given the
computational challenge of the natural looking task. Moreover, the gap between
human and model performance, as well as the variability of gaze interpretation
across different lookers, require further understanding of the underlying
mechanisms utilized by humans for this challenging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2673</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2673</id><created>2014-12-08</created><updated>2015-01-28</updated><authors><author><keyname>Guazzone</keyname><forenames>Marco</forenames></author></authors><title>Mining the Workload of Real Grid Computing Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the mid 1990s, grid computing systems have emerged as an analogy for
making computing power as pervasive an easily accessible as an electric power
grid. Since then, grid computing systems have been shown to be able to provide
very large amounts of storage and computing power to mainly support the
scientific and engineering research on a wide geographic scale. Understanding
the workload characteristics incoming to such systems is a milestone for the
design and the tuning of effective resource management strategies. This is
accomplished through the workload characterization, where workload
characteristics are analyzed and a possibly realistic model for those is
obtained. In this paper, we study the workload of some real grid systems by
using a data mining approach to build a workload model for job interarrival
time and runtime, and a Bayesian approach to capture user correlations and
usage patterns. The final model is then validated against the workload coming
from a real grid system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2675</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2675</id><created>2014-12-08</created><updated>2015-10-19</updated><authors><author><keyname>Fan</keyname><forenames>Yaru</forenames></author><author><keyname>Wang</keyname><forenames>Yilun</forenames></author><author><keyname>Huang</keyname><forenames>Tingzhu</forenames></author></authors><title>Enhanced joint sparsity via Iterative Support Detection</title><categories>cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:1008.4348 by other authors</comments><msc-class>90C26</msc-class><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint sparsity has attracted considerable attention in recent years in many
fields including sparse signal recovery in compressed sensing (CS), statistics,
and machine learning. Traditional convex models suffer from the suboptimal
performance though enjoying tractable computation. In this paper, we propose a
new non-convex joint sparsity model, and develop a corresponding multi-stage
adaptive convex relaxation algorithm. This method extends the idea of iterative
support detection (ISD) from the single vector estimation to the multi-vector
estimation by considering the joint sparsity prior. We provide some preliminary
theoretical analysis including convergence analysis and a sufficient recovery
condition. Numerical experiments from both compressive sensing and feature
learning show the better performance of the proposed method in comparison with
several state-of-the-art alternatives. Moreover, we demonstrate that the
extension of ISD from the single vector to multi-vector estimation is not
trivial. In particular, while ISD does not work well for reconstructing the
signal channel sparse Bernoulli signal, it does achieve significantly improved
performance when recovering the multi-channel sparse Bernoulli signal thanks to
its ability of natural incorporation of the joint sparsity structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2684</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2684</id><created>2014-12-08</created><updated>2015-05-18</updated><authors><author><keyname>Aldea</keyname><forenames>Victor Stefan</forenames></author><author><keyname>Ahmad</keyname><forenames>M. O.</forenames></author><author><keyname>Lynch</keyname><forenames>W. E.</forenames></author></authors><title>HyperSpectral classification with adaptively weighted L1-norm
  regularization and spatial postprocessing</title><categories>math.OC cs.CV</categories><comments>v1: 9 pages, to be submitted to ICIP 2015. v2: Updated formatting of
  entire paper. Results in section 4 have been improved for added clarity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse regression methods have been proven effective in a wide range of
signal processing problems such as image compression, speech coding, channel
equalization, linear regression and classification. In this paper we develop a
new method of hyperspectral image classification based on the sparse unmixing
algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is
introduced. To further enhance class separability, the algorithm is kernelized
using a RBF kernel and the final results are improved by a combination of
spatial pre and post-processing operations. We show that our method is
competitive with state of the art algorithms such as SVM-CK, KLR-CK, KSOMP and
KSSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2689</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2689</id><created>2014-12-08</created><authors><author><keyname>Aajli</keyname><forenames>Ali</forenames></author><author><keyname>Afdel</keyname><forenames>Karim</forenames></author></authors><title>A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic</title><categories>cs.CY cs.AI cs.LG</categories><comments>58-66</comments><journal-ref>Journal of Engineering Research and Applications,ISSN : 2248-9622,
  Vol. 4, Issue 10( Part - 3), October 2014, pp.58-66</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In recent years, adaptive learning systems rely increasingly on learning
hierarchy to customize the educational logic developed in their courses. Most
approaches do not consider that the relationships of prerequisites between the
skills are fuzzy relationships. In this article, we describe a new approach of
a practical application of fuzzy logic techniques to the construction of
learning hierarchies. For this, we use a learning hierarchy predefined by one
or more experts of a specific field. However, the relationships of
prerequisites between the skills in the learning hierarchy are not definitive
and they are fuzzy relationships. Indeed, we measure relevance degree of all
relationships existing in this learning hierarchy and we try to answer to the
following question: Is the relationships of prerequisites predefined in initial
learning hierarchy are correctly established or not?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2690</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2690</id><created>2014-12-08</created><updated>2016-01-26</updated><authors><author><keyname>Barnett</keyname><forenames>Nix</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Computational Mechanics of Input-Output Processes: Structured
  transformations and the $\epsilon$-transducer</title><categories>cond-mat.stat-mech cs.IT math.DS math.IT</categories><comments>30 pages, 19 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/et1.htm; Updated to conform to
  published version plus additional corrections and updates</comments><doi>10.1007/s10955-015-1327-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational mechanics quantifies structure in a stochastic process via its
causal states, leading to the process's minimal, optimal predictor---the
$\epsilon$-machine. We extend computational mechanics to communication channels
between two processes, obtaining an analogous optimal model---the
$\epsilon$-transducer---of the stochastic mapping between them. Here, we lay
the foundation of a structural analysis of communication channels, treating
joint processes and processes with input. The result is a principled structural
analysis of mechanisms that support information flow between processes. It is
the first in a series on the structural information theory of memoryful
channels, channel composition, and allied conditional information measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2693</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2693</id><created>2014-12-08</created><updated>2015-04-28</updated><authors><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Provable Methods for Training Neural Networks with Sparse Connectivity</title><categories>cs.LG cs.NE stat.ML</categories><comments>Accepted for presentation at Neural Information Processing
  Systems(NIPS) 2014 Deep Learning workshop and Accepted as a workshop
  contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide novel guaranteed approaches for training feedforward neural
networks with sparse connectivity. We leverage on the techniques developed
previously for learning linear networks and show that they can also be
effectively adopted to learn non-linear networks. We operate on the moments
involving label and the score function of the input, and show that their
factorization provably yields the weight matrix of the first layer of a deep
network under mild conditions. In practice, the output of our method can be
employed as effective initializers for gradient descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2695</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2695</id><created>2014-12-08</created><authors><author><keyname>Ouahi</keyname><forenames>H.</forenames></author><author><keyname>Hajji</keyname><forenames>M. El</forenames></author><author><keyname>Afdel</keyname><forenames>K.</forenames></author></authors><title>Secure and Image Retrieval based on Multipurpose Watermarking for
  Mammography Images Database</title><categories>cs.CR</categories><comments>28-31</comments><journal-ref>International Journal of Computer Applications,Volume 90,No
  14,March,2014</journal-ref><doi>10.5120/15789-4536</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the cancerology domain, we were brought to make periodic mammography
images to monitor tumor patients. Oracle Database Management system (DBMS) is a
solution to manage these images with patient's data recorder. Knowing the large
size of medical images of mammograms, the Oracle DBMS saves these images
outside the Oracle database using external LOBs. The link between these images
and Oracle is done through the BFILE. At this level, two problems are raised:
the first problem is that access to these images can become impossible because
the link is likely to be broken. The second problem is security, the fact that
the images are saved outside the Oracle database, they do not benefit from its
powerful security. The protection of the integrity and confidentiality of data
and patient images are a necessity defended by laws and they must be preserved
against any unauthorized access, alteration or destruction. In this paper, we
propose the method of reversible watermarking technique based on the difference
expansion to resolve these two problems and explore its use in search and
retrieval strategy of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2697</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2697</id><created>2014-12-08</created><authors><author><keyname>Abdelouahad</keyname><forenames>Abdelkaher Ait</forenames></author><author><keyname>Hassouni</keyname><forenames>Mohammed El</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames></author></authors><title>Image quality assessment measure based on natural image statistics in
  the Tetrolet domain</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a reduced reference (RR) image quality measure based on
natural image statistics modeling. For this purpose, Tetrolet transform is used
since it provides a convenient way to capture local geometric structures. This
transform is applied to both reference and distorted images. Then, Gaussian
Scale Mixture (GSM) is proposed to model subbands in order to take account
statistical dependencies between tetrolet coefficients. In order to quantify
the visual degradation, a measure based on Kullback Leibler Divergence (KLD) is
provided. The proposed measure was tested on the Cornell VCL A-57 dataset and
compared with other measures according to FR-TV1 VQEG framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2700</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2700</id><created>2014-12-05</created><updated>2015-06-02</updated><authors><author><keyname>Biswas</keyname><forenames>Sampurna</forenames></author><author><keyname>Poddar</keyname><forenames>Sunrita</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author><author><keyname>Mudumbai</keyname><forenames>Raghuraman</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Subspace based low rank and joint sparse matrix recovery</title><categories>cs.NA cs.CV</categories><comments>5 pages, 5 figures, Asilomar 2014 conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recovery of a low rank and jointly sparse matrix from under
sampled measurements of its columns. This problem is highly relevant in the
recovery of dynamic MRI data with high spatio-temporal resolution, where each
column of the matrix corresponds to a frame in the image time series; the
matrix is highly low-rank since the frames are highly correlated. Similarly the
non-zero locations of the matrix in appropriate transform/frame domains (e.g.
wavelet, gradient) are roughly the same in different frame. The superset of the
support can be safely assumed to be jointly sparse. Unlike the classical
multiple measurement vector (MMV) setup that measures all the snapshots using
the same matrix, we consider each snapshot to be measured using a different
measurement matrix. We show that this approach reduces the total number of
measurements, especially when the rank of the matrix is much smaller than than
its sparsity. Our experiments in the context of dynamic imaging shows that this
approach is very useful in realizing free breathing cardiac MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2711</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2711</id><created>2014-12-08</created><authors><author><keyname>Geng</keyname><forenames>Chunhua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>On the Optimality of Treating Interference as Noise: Compound
  Interference Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a K-user Gaussian interference channel, it has been shown by Geng et al.
that if for each user the desired signal strength is no less than the sum of
the strengths of the strongest interference from this user and the strongest
interference to this user (all values in dB scale), then power control and
treating interference as noise (TIN) is optimal from the perspective of
generalized degrees of freedom (GDoF) and achieves the entire channel capacity
region to within a constant gap. In this work, we generalize the optimality of
TIN to compound networks. We show that for a K-user compound Gaussian
interference channel, if in every possible state for each receiver, the channel
always satisfies the TIN-optimality condition identified by Geng et al., then
the GDoF region of the compound channel is the intersection of the GDoF regions
of all possible network realizations, which is achievable by power control and
TIN. Furthermore, we demonstrate that for a general K-user compound
interference channel, regardless of the number of states of each receiver, we
can always construct a counterpart K-user regular interference channel that has
the same TIN region as the original compound channel. The regular interference
channel has only one state for each receiver, which may be different from all
of the original states. Solving the GDoF-based power control problem for the
compound channel is equivalent to solving the same problem in its regular
counterpart. Exploring the power control problem further we develop a
centralized power control scheme for K-user compound interference channels, to
achieve all the Pareto optimal GDoF tuples. Finally, based on this scheme, we
devise an iterative power control algorithm which requires at most K updates to
obtain the globally optimal power allocation for any feasible GDoF tuple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2716</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2716</id><created>2014-12-08</created><authors><author><keyname>Citron</keyname><forenames>Daniel T.</forenames></author><author><keyname>Ginsparg</keyname><forenames>Paul</forenames></author></authors><title>Patterns of Text Reuse in a Scientific Corpus</title><categories>cs.DL physics.soc-ph</categories><comments>6 pages, plus 10 pages of supplementary material. To appear in PNAS
  (online 8 Dec 2014)</comments><doi>10.1073/pnas.1415135111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the incidence of text &quot;reuse&quot; by researchers, via a systematic
pairwise comparison of the text content of all articles deposited to arXiv.org
from 1991--2012. We measure the global frequencies of three classes of text
reuse, and measure how chronic text reuse is distributed among authors in the
dataset. We infer a baseline for accepted practice, perhaps surprisingly
permissive compared with other societal contexts, and a clearly delineated set
of aberrant authors. We find a negative correlation between the amount of
reused text in an article and its influence, as measured by subsequent
citations. Finally, we consider the distribution of countries of origin of
articles containing large amounts of reused text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2721</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2721</id><created>2014-12-08</created><authors><author><keyname>Stolbova</keyname><forenames>Irina</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Fault Induced Delayed Voltage Recovery in a Long Inhomogeneous Power
  Distribution Feeder</title><categories>physics.soc-ph cs.SY physics.data-an</categories><comments>10 pages, 10 figures</comments><report-no>LANL LA-UR-14-23192</report-no><journal-ref>Phys. Rev. E 91, 022812 (2015)</journal-ref><doi>10.1103/PhysRevE.91.022812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the dynamics of a distribution circuit loaded with many induction
motor and subjected to sudden changes in voltage at the beginning of the
circuit. As opposed to earlier work \cite{13DCB}, the motors are disordered,
i.e. the mechanical torque applied to the motors varies in a random manner
along the circuit. In spite of the disorder, many of the qualitative features
of a homogenous circuit persist, e.g. long-range motor-motor interactions
mediated by circuit voltage and electrical power flows result in coexistence of
the spatially-extended and propagating normal and stalled phases. We also
observed a new phenomenon absent in the case without inhomogeneity/disorder.
Specifically, transition front between the normal and stalled phases becomes
somewhat random, even when the front is moving very slowly or is even
stationary. Motors within the blurred domain appears in a normal or stalled
state depending on the local configuration of the disorder. We quantify effects
of the disorder and discuss statistics of distribution dynamics, e.g. the front
position and width, total active/reactive consumption of the feeder and maximum
clearing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2723</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2723</id><created>2014-12-08</created><authors><author><keyname>Tang</keyname><forenames>Jiliang</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Aggarwal</keyname><forenames>Charu</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Negative Link Prediction in Social Media</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signed network analysis has attracted increasing attention in recent years.
This is in part because research on signed network analysis suggests that
negative links have added value in the analytical process. A major impediment
in their effective use is that most social media sites do not enable users to
specify them explicitly. In other words, a gap exists between the importance of
negative links and their availability in real data sets. Therefore, it is
natural to explore whether one can predict negative links automatically from
the commonly available social network data. In this paper, we investigate the
novel problem of negative link prediction with only positive links and
content-centric interactions in social media. We make a number of important
observations about negative links, and propose a principled framework NeLP,
which can exploit positive links and content-centric interactions to predict
negative links. Our experimental results on real-world social networks
demonstrate that the proposed NeLP framework can accurately predict negative
links with positive links and content-centric interactions. Our detailed
experiments also illustrate the relative importance of various factors to the
effectiveness of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2773</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2773</id><created>2014-12-08</created><authors><author><keyname>Li</keyname><forenames>Shang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Cooperative Change Detection for Online Power Quality Monitoring</title><categories>cs.SY</categories><journal-ref>IEEE Trans. Info. Forensics and Security, 11(1), pp.86-99, 2015</journal-ref><doi>10.1109/TIFS.2015.2477796</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the real-time power quality monitoring in power grid
systems. The goal is to detect the occurrence of disturbances in the nominal
sinusoidal voltage/current signal as quickly as possible such that protection
measures can be taken in time. Based on an autoregressive (AR) model for the
disturbance, we propose a generalized local likelihood ratio (GLLR) detector
which processes meter readings sequentially and alarms as soon as the test
statistic exceeds a prescribed threshold. The proposed detector not only reacts
to a wide range of disturbances, but also achieves lower detection delay
compared to the conventional block processing method. Then we further propose
to deploy multiple meters to monitor the power signal cooperatively. The
distributed meters communicate wirelessly to a central meter, where the data
fusion and detection are performed. In light of the limited bandwidth of
wireless channels, we develop a level-triggered sampling scheme, where each
meter transmits only one-bit each time asynchronously. The proposed multi-meter
scheme features substantially low communication overhead, while its performance
is close to that of the ideal case where distributed meter readings are
perfectly available at the central meter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2782</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2782</id><created>2014-12-08</created><updated>2015-01-30</updated><authors><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>A streamlined difference ring theory: Indefinite nested sums, the
  alternating sign and the parameterized telescoping problem</title><categories>cs.SC</categories><comments>Some typos are removed</comments><journal-ref>Proceedings of the 16th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '14), pp. 26-33. IEEE,
  2014</journal-ref><doi>10.1109/SYNASC.2014.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algebraic framework to represent indefinite nested sums over
hypergeometric expressions in difference rings. In order to accomplish this
task, parts of Karr's difference field theory have been extended to a ring
theory in which also the alternating sign can be expressed. The underlying
machinery relies on algorithms that compute all solutions of a given
parameterized telescoping equation. As a consequence, we can solve the
telescoping and creative telescoping problem in such difference rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2787</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2787</id><created>2014-12-08</created><updated>2014-12-10</updated><authors><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Uchoa</keyname><forenames>Eduardo</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>A Robust and Scalable Algorithm for the Steiner Problem in Graphs</title><categories>cs.DS</categories><comments>33 pages. Presented at the 11th DIMACS Implementation Challenge on
  Steiner Tree Problems</comments><acm-class>G.1.6; G.2.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an effective heuristic for the Steiner Problem in Graphs. Its main
elements are a multistart algorithm coupled with aggressive combination of
elite solutions, both leveraging recently-proposed fast local searches. We also
propose a fast implementation of a well-known dual ascent algorithm that not
only makes our heuristics more robust (by quickly dealing with easier cases),
but can also be used as a building block of an exact (branch-and-bound)
algorithm that is quite effective for some inputs. On all graph classes we
consider, our heuristic is competitive with (and sometimes more effective than)
any previous approach with similar running times. It is also scalable: with
long runs, we could improve or match the best published results for most open
instances in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2799</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2799</id><created>2014-12-08</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Fan</keyname><forenames>Pingzhi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Impact of User Pairing on 5G Non-Orthogonal Multiple Access</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-orthogonal multiple access (NOMA) represents a paradigm shift from
conventional orthogonal multiple access (MA) concepts, and has been recognized
as one of the key enabling technologies for 5G systems. In this paper, the
impact of user pairing on the performance of two NOMA systems, NOMA with fixed
power allocation (F-NOMA) and cognitive radio inspired NOMA (CR-NOMA), is
characterized. For FNOMA, both analytical and numerical results are provided to
demonstrate that F-NOMA can offer a larger sum rate than orthogonal MA, and the
performance gain of F-NOMA over conventional MA can be further enlarged by
selecting users whose channel conditions are more distinctive. For CR-NOMA, the
quality of service (QoS) for users with the poorer channel condition can be
guaranteed since the transmit power allocated to other users is constrained
following the concept of cognitive radio networks. Because of this constraint,
CR-NOMA has different behavior compared to F-NOMA. For example, for the user
with the best channel condition, CR-NOMA prefers to pair it with the user with
the second best channel condition, whereas the user with the worst channel
condition is preferred by F-NOMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2812</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2812</id><created>2014-12-08</created><authors><author><keyname>Titov</keyname><forenames>Ivan</forenames></author><author><keyname>Khoddam</keyname><forenames>Ehsan</forenames></author></authors><title>Unsupervised Induction of Semantic Roles within a Reconstruction-Error
  Minimization Framework</title><categories>cs.CL cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to unsupervised estimation of feature-rich
semantic role labeling models. Our model consists of two components: (1) an
encoding component: a semantic role labeling model which predicts roles given a
rich set of syntactic and lexical features; (2) a reconstruction component: a
tensor factorization model which relies on roles to predict argument fillers.
When the components are estimated jointly to minimize errors in argument
reconstruction, the induced roles largely correspond to roles defined in
annotated resources. Our method performs on par with most accurate role
induction methods on English and German, even though, unlike these previous
approaches, we do not incorporate any prior linguistic knowledge about the
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2813</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2813</id><created>2014-12-08</created><updated>2016-02-29</updated><authors><author><keyname>Zhao</keyname><forenames>Ningning</forenames></author><author><keyname>Basarab</keyname><forenames>Adrian</forenames></author><author><keyname>Kouame</keyname><forenames>Denis</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Joint Segmentation and Deconvolution of Ultrasound Images Using a
  Hierarchical Bayesian Model based on Generalized Gaussian Priors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a joint segmentation and deconvolution Bayesian method
for medical ultrasound (US) images. Contrary to piecewise homogeneous images,
US images exhibit heavy characteristic speckle patterns correlated with the
tissue structures. The generalized Gaussian distribution (GGD) has been shown
to be one of the most relevant distributions for characterizing the speckle in
US images. Thus, we propose a GGD-Potts model defined by a label map coupling
US image segmentation and deconvolution. The Bayesian estimators of the unknown
model parameters, including the US image, the label map and all the
hyperparameters are difficult to be expressed in closed form. Thus, we
investigate a Gibbs sampler to generate samples distributed according to the
posterior of interest. These generated samples are finally used to compute the
Bayesian estimators of the unknown parameters. The performance of the proposed
Bayesian model is compared with existing approaches via several experiments
conducted on realistic synthetic data and in vivo US images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2817</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2817</id><created>2014-12-08</created><authors><author><keyname>Gholami</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion Estimation Over Cooperative Multi-Agent Networks With Missing
  Data</title><categories>math.ST cs.SY stat.TH</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many fields, and especially in the medical and social sciences and in
recommender systems, data are gathered through clinical studies or targeted
surveys. Participants are generally reluctant to respond to all questions in a
survey or they may lack information to respond adequately to the questions. The
data collected from these studies tend to lead to linear regression models
where the regression vectors are only known partially: some of their entries
are either missing completely or replaced randomly by noisy values. In this
work, we examine how a connected network of agents, with each one of them
subjected to a stream of data with incomplete regression information, can
cooperate with each other through local interactions to estimate the underlying
model parameters in the presence of missing data. We explain how to adjust the
distributed diffusion through (de)regularization in order to eliminate the bias
introduced by the incomplete model. We also propose a technique to recursively
estimate the (de)regularization parameter and examine the performance of the
resulting strategy. We illustrate the results by considering two applications:
one dealing with a mental health survey and the other dealing with a household
consumption survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2821</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2821</id><created>2014-12-08</created><authors><author><keyname>Wang</keyname><forenames>Xiuli</forenames></author></authors><title>Zipf's Law and the Frequency of Characters or Words of Oracles</title><categories>cs.CL math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article discusses the frequency of characters of Oracle,concluding that
the frequency and the rank of a word or character is fit to Zipf-Mandelboit Law
or Zipf's law with three parameters,and figuring out the parameters based on
the frequency,and pointing out that what some researchers of Oracle call the
assembling on the two ends is just a description by their impression about the
Oracle data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2824</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2824</id><created>2014-12-08</created><authors><author><keyname>Narayanan</keyname><forenames>Vignesh</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Mendoza</keyname><forenames>Nathaniel</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Plan or not: Remote Human-robot Teaming with Incomplete Task Information</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human-robot interaction can be divided into two categories based on the
physical distance between the human and robot: remote and proximal. In proximal
interaction, the human and robot often engage in close coordination; in remote
interaction, the human and robot are less coupled due to communication
constraints. As a result, providing automation for the robot in remote
interaction becomes more important. Thus far, human factor studies on
automation in remote human-robot interaction have been restricted to various
forms of supervision, in which the robot is essentially being used as a smart
mobile manipulation platform with sensing capabilities. In this paper, we
investigate the incorporation of general planning capability into the robot to
facilitate peer-to-peer human-robot teaming, in which the human and robot are
viewed as teammates that are physically separated. The human and robot share
the same global goal and collaborate to achieve it. Note that humans may feel
uncomfortable at such robot autonomy, which can potentially reduce teaming
performance. One important difference between peer-to-peer teaming and
supervised teaming is that an autonomous robot in peer-to-peer teaming can
achieve the goal alone when the task information is completely specified.
However, incompleteness often exists, which implies information asymmetry.
While information asymmetry can be desirable sometimes, it may also lead to the
robot choosing improper actions that negatively influence the teaming
performance. We aim to investigate the various trade-offs, e.g., mental
workload and situation awareness, between these two types of remote human-robot
teaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2844</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2844</id><created>2014-12-08</created><authors><author><keyname>Hardwick</keyname><forenames>Janis</forenames></author><author><keyname>Stout</keyname><forenames>Quentin F.</forenames></author></authors><title>Optimal Reduced Isotonic Regression</title><categories>stat.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isotonic regression is a shape-constrained nonparametric regression in which
the regression is an increasing step function. For $n$ data points, the number
of steps in the isotonic regression may be as large as $n$. As a result,
standard isotonic regression has been criticized as overfitting the data or
making the representation too complicated. So-called &quot;reduced&quot; isotonic
regression constrains the outcome to be a specified number of steps $b$, $b
\leq n$. However, because the previous algorithms for finding the reduced $L_2$
regression took $\Theta(n+bm^2)$ time, where $m$ is the number of steps of the
unconstrained isotonic regression, researchers felt that the algorithms were
too slow and instead used approximations. Other researchers had results that
were approximations because they used a greedy top-down approach. Here we give
an algorithm to find an exact solution in $\Theta(n+bm)$ time, and a simpler
algorithm taking $\Theta(n+b m \log m)$ time. These algorithms also determine
optimal $k$-means clustering of weighted 1-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2845</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2845</id><created>2014-12-08</created><authors><author><keyname>Saha</keyname><forenames>Amal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Survey of Strong Authentication Approaches for Mobile Proximity and
  Remote Wallet Applications - Challenges and Evolution</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wallet may be described as container application used for configuring,
accessing and analysing data from underlying payment application(s). There are
two dominant types of digital wallet applications, proximity wallet and remote
wallet. In the payment industry, one often hears about authentication approach
for proximity or remote wallets or the underlying payment applications
separately, but there is no such approach, as per our knowledge, for combined
wallet, the holder application. While Secure Element (SE) controlled by the
mobile network operator (i.e., SIM card) may ensure strong authentication, it
introduces strong dependencies among business partners in payments and hence is
not getting fraction. Embedded SE in the form of trusted execution environment
[3, 4, 5] or trusted computing [24] may address this issue in future. But such
devices tend to be a bit expensive and are not abundant in the market.
Meanwhile, for many years, context based authentication involving device
fingerprinting and other contextual information for conditional multi-factor
authentication, would prevail and would remain as the most dominant and strong
authentication mechanism for mobile devices from various vendors in different
capability and price ranges. EMVCo payment token standard published in 2014
tries to address security of wallet based payment in a general way. The authors
believe that it is quite likely that EMVCo payment token implementations would
evolve in course of time in such a way that token service providers would start
insisting on device fingerprinting as strong means of authentication before
issuing one-time-use payment token. This paper talks about challenges of
existing authentication mechanisms used in payment and wallet applications, and
their evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2855</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2855</id><created>2014-12-08</created><updated>2016-02-25</updated><authors><author><keyname>Chauhan</keyname><forenames>Jagmohan</forenames></author><author><keyname>Asghar</keyname><forenames>Hassan Jameel</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Mahanti</keyname><forenames>Anirban</forenames></author></authors><title>Gesture-based Continuous Authentication for Wearable Devices: the Google
  Glass Case</title><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the feasibility of touch gesture behavioural biometrics for implicit
authentication of users on a smartglass (Google Glass) by proposing a
continuous authentication system using two classifiers: SVM with RBF kernel,
and a new classifier based on Chebyshev's concentration inequality. Based on
data collected from 30 volunteers, we show that such authentication is feasible
both in terms of classification accuracy and computational load on
smartglasses. We achieve a classification accuracy of up to 99% with only 75
training samples using behavioural biometric data from four different types of
touch gestures. To show that our system can be generalized, we test its
performance on touch data from smartphones and found the accuracy to be similar
to smartglasses. Finally, our experiments on the permanence of gestures show
that the negative impact of changing user behaviour with time on classification
accuracy can be best alleviated by periodically replacing older training
samples with new randomly chosen samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2857</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2857</id><created>2014-12-09</created><authors><author><keyname>Kuriakose</keyname><forenames>Jeril</forenames></author><author><keyname>V.</keyname><forenames>Amruth</forenames></author><author><keyname>G.</keyname><forenames>Sandesh A.</forenames></author><author><keyname>Naveenbabu</keyname><forenames>Jampu Venkata</forenames></author><author><keyname>Shahid</keyname><forenames>Mohammed</forenames></author><author><keyname>Shetty</keyname><forenames>Ashish</forenames></author></authors><title>Analysis of Maximum Likelihood and Mahalanobis Distance for Identifying
  Cheating Anchor Nodes</title><categories>cs.CR</categories><comments>10 pages, 13 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malicious anchor nodes will constantly hinder genuine and appropriate
localization. Discovering the malicious or vulnerable anchor node is an
essential problem in wireless sensor networks (WSNs). In wireless sensor
networks, anchor nodes are the nodes that know its current location.
Neighboring nodes or non-anchor nodes calculate its location (or its location
reference) with the help of anchor nodes. Ingenuous localization is not
possible in the presence of a cheating anchor node or a cheating node.
Nowadays, its a challenging task to identify the cheating anchor node or
cheating node in a network. Even after finding out the location of the cheating
anchor node, there is no assurance, that the identified node is legitimate or
not. This paper aims to localize the cheating anchor nodes using trilateration
algorithm and later associate it with maximum likelihood expectation technique
(MLE), and Mahalanobis distance to obtain maximum accuracy in identifying
malicious or cheating anchor nodes during localization. We were able to attain
a considerable reduction in the error achieved during localization. For
implementation purpose we simulated our scheme using ns-3 network simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2859</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2859</id><created>2014-12-09</created><authors><author><keyname>Marzen</keyname><forenames>Sarah</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Circumventing the Curse of Dimensionality in Prediction: Causal
  Rate-Distortion for Infinite-Order Markov Processes</title><categories>cond-mat.stat-mech cs.LG nlin.CD q-bio.NC stat.ML</categories><comments>25 pages, 14 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/cn.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictive rate-distortion analysis suffers from the curse of dimensionality:
clustering arbitrarily long pasts to retain information about arbitrarily long
futures requires resources that typically grow exponentially with length. The
challenge is compounded for infinite-order Markov processes, since conditioning
on finite sequences cannot capture all of their past dependencies. Spectral
arguments show that algorithms which cluster finite-length sequences fail
dramatically when the underlying process has long-range temporal correlations
and can fail even for processes generated by finite-memory hidden Markov
models. We circumvent the curse of dimensionality in rate-distortion analysis
of infinite-order processes by casting predictive rate-distortion objective
functions in terms of the forward- and reverse-time causal states of
computational mechanics. Examples demonstrate that the resulting causal
rate-distortion theory substantially improves current predictive
rate-distortion analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2863</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2863</id><created>2014-12-09</created><updated>2014-12-11</updated><authors><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Score Function Features for Discriminative Learning: Matrix and Tensor
  Framework</title><categories>cs.LG stat.ML</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature learning forms the cornerstone for tackling challenging learning
problems in domains such as speech, computer vision and natural language
processing. In this paper, we consider a novel class of matrix and
tensor-valued features, which can be pre-trained using unlabeled samples. We
present efficient algorithms for extracting discriminative information, given
these pre-trained features and labeled samples for any related task. Our class
of features are based on higher-order score functions, which capture local
variations in the probability density function of the input. We establish a
theoretical framework to characterize the nature of discriminative information
that can be extracted from score-function features, when used in conjunction
with labeled samples. We employ efficient spectral decomposition algorithms (on
matrices and tensors) for extracting discriminative components. The advantage
of employing tensor-valued features is that we can extract richer
discriminative information in the form of an overcomplete representations.
Thus, we present a novel framework for employing generative models of the input
for discriminative learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2873</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2873</id><created>2014-12-09</created><authors><author><keyname>Stainvas</keyname><forenames>Inna</forenames></author><author><keyname>Manevitch</keyname><forenames>Alexandra</forenames></author><author><keyname>Leichter</keyname><forenames>Isaac</forenames></author></authors><title>Cancer Detection with Multiple Radiologists via Soft Multiple Instance
  Logistic Regression and $L_1$ Regularization</title><categories>cs.CV</categories><comments>20 pages, report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the multiple annotation problem in medical application
of cancer detection in digital images. The main assumption is that though
images are labeled by many experts, the number of images read by the same
expert is not large. Thus differing with the existing work on modeling each
expert and ground truth simultaneously, the multi annotation information is
used in a soft manner. The multiple labels from different experts are used to
estimate the probability of the findings to be marked as malignant. The
learning algorithm minimizes the Kullback Leibler (KL) divergence between the
modeled probabilities and desired ones constraining the model to be compact.
The probabilities are modeled by logit regression and multiple instance
learning concept is used by us.
  Experiments on a real-life computer aided diagnosis (CAD) problem for CXR CAD
lung cancer detection demonstrate that the proposed algorithm leads to similar
results as learning with a binary RVMMIL classifier or a mixture of binary
RVMMIL models per annotator. However, this model achieves a smaller complexity
and is more preferable in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2877</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2877</id><created>2014-12-09</created><updated>2014-12-14</updated><authors><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Autonomous Load Disaggregation Approach based on Active Power
  Measurements</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the help of smart metering valuable information of the appliance usage
can be retrieved. In detail, non-intrusive load monitoring (NILM), also called
load disaggregation, tries to identify appliances in the power draw of an
household. In this paper an unsupervised load disaggregation approach is
proposed that works without a priori knowledge about appliances. The proposed
algorithm works autonomously in real time. The number of used appliances and
the corresponding appliance models are learned in operation and are
progressively updated. The proposed algorithm is considering each useful and
suitable detected power state. The algorithm tries to detect power states
corresponding to on/off appliances as well as to multi-state appliances based
on active power measurements in 1s resolution. We evaluated the novel
introduced load disaggregation approach on real world data by testing the
possibility to disaggregate energy demand on appliance level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2880</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2880</id><created>2014-12-09</created><authors><author><keyname>Marfisi-Schottman</keyname><forenames>Iza</forenames><affiliation>LIG</affiliation></author><author><keyname>George</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIG</affiliation></author><author><keyname>Tarpin-Bernard</keyname><forenames>Franck</forenames><affiliation>LIG</affiliation></author></authors><title>Evaluating Learning Games during their Conception</title><categories>cs.CY cs.HC</categories><comments>European Conference on Game Based Learning, ECGBL, Oct 2014, Berlin,
  Germany</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Games (LGs) are educational environments based on a playful approach
to learning. Their use has proven to be promising in many domains, but is at
present restricted by the time consuming and costly nature of the developing
process. In this paper, we propose a set of quality indicators that can help
the conception team to evaluate the quality of their LG during the designing
process, and before it is developed. By doing so, the designers can identify
and repair problems in the early phases of the conception and therefore reduce
the alteration phases, that occur after testing the LG's prototype. These
quality indicators have been validated by 6 LG experts that used them to assess
the quality of 24 LGs in the process of being designed. They have also proven
to be useful as design guidelines for novice LG designers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2888</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2888</id><created>2014-12-09</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Error Floor Analysis of Coded Slotted ALOHA over Packet Erasure Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/LCOMM.2014.2385073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for the analysis of the error floor of coded slotted
ALOHA (CSA) for finite frame lengths over the packet erasure channel. The error
floor is caused by stopping sets in the corresponding bipartite graph, whose
enumeration is, in general, not a trivial problem. We therefore identify the
most dominant stopping sets for the distributions of practical interest. The
derived analytical expressions allow us to accurately predict the error floor
at low to moderate channel loads and characterize the unequal error protection
inherent in CSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2897</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2897</id><created>2014-12-09</created><authors><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Nasir</keyname><forenames>Adnan</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Guizani</keyname><forenames>Mohsen</forenames></author></authors><title>Trading Wireless Information and Power Transfer: Relay Selection to
  Minimize the Outage Probability</title><categories>cs.IT math.IT</categories><comments>IEEE GlobalSiP, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the outage probability minimization problem for a multiple
relay network with energy harvesting constraints. The relays are hybrid nodes
used for simultaneous wireless information and power transfer from the source
radio frequency (RF) signals. There is a trade-off associated with the amount
of time a relay node is used for energy and information transfer. Large
intervals of information transfer implies little time for energy harvesting
from RF signals and thus, high probability of outage events. We propose relay
selection schemes for a cooperative system with a fixed number of RF powered
relays. We address both causal and non-causal channel state information cases
at the relay--destination link and evaluate the trade-off associated with
information/power transfer in the context of minimization of outage
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2901</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2901</id><created>2014-12-09</created><authors><author><keyname>Nicolay</keyname><forenames>Robin</forenames></author></authors><title>Semantic Enhancement of Lecture Material</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's lectures are often talks following a straight line of slides. In many
lectures the process of content teaching is not as efficient as it could be.
Technologies, such as smart-phones and wireless communication, enable a new
level of interaction between lecturer, content and audience. We describe how
current lecture material can be semantically enhanced, to interactively assist
the audience during and after a lecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2905</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2905</id><created>2014-12-09</created><updated>2015-02-24</updated><authors><author><keyname>Carapelle</keyname><forenames>Claudia</forenames></author><author><keyname>Feng</keyname><forenames>Shiguang</forenames></author><author><keyname>Kartzow</keyname><forenames>Alexander</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author></authors><title>Satisfiability of ECTL* with tree constraints</title><categories>cs.LO cs.FL math.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, we have shown that satisfiability for $\mathsf{ECTL}^*$ with
constraints over $\mathbb{Z}$ is decidable using a new technique. This approach
reduces the satisfiability problem of $\mathsf{ECTL}^*$ with constraints over
some structure A (or class of structures) to the problem whether A has a
certain model theoretic property that we called EHD (for &quot;existence of
homomorphisms is decidable&quot;). Here we apply this approach to concrete domains
that are tree-like and obtain several results. We show that satisfiability of
$\mathsf{ECTL}^*$ with constraints is decidable over (i) semi-linear orders
(i.e., tree-like structures where branches form arbitrary linear orders), (ii)
ordinal trees (semi-linear orders where the branches form ordinals), and (iii)
infinitely branching trees of height h for each fixed $h\in \mathbb{N}$. We
prove that all these classes of structures have the property EHD. In contrast,
we introduce Ehrenfeucht-Fraisse-games for $\mathsf{WMSO}+\mathsf{B}$ (weak
$\mathsf{MSO}$ with the bounding quantifier) and use them to show that the
infinite (order) tree does not have property EHD. As a consequence, a different
approach has to be taken in order to settle the question whether satisfiability
of $\mathsf{ECTL}^*$ (or even $\mathsf{LTL}$) with constraints over the
infinite (order) tree is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2908</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2908</id><created>2014-12-09</created><authors><author><keyname>Sadqi</keyname><forenames>Yassine</forenames></author><author><keyname>Asimi</keyname><forenames>Ahmed</forenames></author><author><keyname>Asimi</keyname><forenames>Younes</forenames></author></authors><title>A Cryptographic Mutual Authentication Scheme for Web Applications</title><categories>cs.CR</categories><comments>15 pages, 2 figures,International Journal</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA) Vol.6, No.6, November 2014</journal-ref><doi>10.5121/ijnsa.2014.6601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of current web authentication is built on username/password.
Unfortunately, password replacement offers more security, but it is difficult
to use and expensive to deploy. In this paper, we propose a new mutual
authentication scheme called StrongAuth which preserves most password
authentication advantages and simultaneously improves security using
cryptographic primitives. Our scheme not only offers webmasters a clear
framework which to build secure user authentication, but it also provides
almost the same conventional user experience. Security analysis shows that the
proposed scheme fulfills the required user authentication security benefits,
and can resist various possible attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2929</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2929</id><created>2014-12-09</created><authors><author><keyname>Yang</keyname><forenames>Yao-Hsiang</forenames></author><author><keyname>Chen</keyname><forenames>Lu-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Chieh-Chih</forenames></author><author><keyname>Chen</keyname><forenames>Chu-Song</forenames></author></authors><title>Bayesian Fisher's Discriminant for Functional Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian framework of Gaussian process in order to extend
Fisher's discriminant to classify functional data such as spectra and images.
The probability structure for our extended Fisher's discriminant is explicitly
formulated, and we utilize the smoothness assumptions of functional data as
prior probabilities. Existing methods which directly employ the smoothness
assumption of functional data can be shown as special cases within this
framework given corresponding priors while their estimates of the unknowns are
one-step approximations to the proposed MAP estimates. Empirical results on
various simulation studies and different real applications show that the
proposed method significantly outperforms the other Fisher's discriminant
methods for functional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2934</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2934</id><created>2014-12-09</created><updated>2015-08-23</updated><authors><author><keyname>Dhuli</keyname><forenames>Sateeshkrishna</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Network Criticality Analysis for Finite Sized Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 pages, 13 figures. arXiv admin note: text overlap with
  arXiv:1411.4577 We want to withdraw this paper due to wrong simulation
  results, incomplete analysis and some grammatical mistakes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topology of a sensor network changes very frequently due to node failures
because of power constraints or physical destruction. Robustness to topology
changes is one of the important design factors of wireless sensor networks
which makes them suitable to military, communications, health and surveillance
applications. Network criticality is a measure which capture the properties of
network robustness to environmental changes. In this work, we derived the
analytical formulas for network criticality, normalized network criticality and
studied the network robustness for r-nearest neighbor cycle and r-nearest
neighbor torus networks. Further, we compared our analytical results with
simulation results and studied the effect of number of nodes, nearest neighbors
and network dimension on the network criticality and normalized network
criticality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2950</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2950</id><created>2014-12-09</created><authors><author><keyname>Onsori</keyname><forenames>Salman</forenames></author><author><keyname>Safaei</keyname><forenames>Farshad</forenames></author></authors><title>Performance Enhancement of Routers in Networks-on-Chip Using Dynamic
  Virtual Channels Allocation</title><categories>cs.AR</categories><comments>14 pages, 18 figures, Computer Applications: An International Journal
  (CAIJ), Vol.1, No.2, November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a new router architecture to improve the performance of
dynamic allocation of virtual channels. The proposed router is designed to
reduce the hardware complexity and to improve power and area consumption,
simultaneously. In the new structure of the proposed router, all of the
controlling components have been implemented sequentially inside the allocator
router modules. This optimizes communications between the controlling
components and eliminates the most of hardware overloads of modular
communications. Eliminating additional communications also reduces the hardware
complexity. In order to show the validity of the proposed design in real
hardware resources, the proposed router has been implemented onto a
Field-Programmable Gate Array (FPGA). Since the implementation of a
Network-on-Chip (NoC) requires certain amount of area on the chip, the
suggested approach is also able to reduce the demand of hardware resources. In
this method, the internal memory of the FPGA is used for implementing control
units. This memory is faster and can be used with specific patterns. The use of
the FPGA memory saves the hardware resources and allows the implementation of
NoC based FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2954</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2954</id><created>2014-12-09</created><updated>2015-06-19</updated><authors><author><keyname>Vempala</keyname><forenames>Santosh S.</forenames></author><author><keyname>Xiao</keyname><forenames>Ying</forenames></author></authors><title>Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample
  Complexity</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple, general technique for reducing the sample complexity of
matrix and tensor decomposition algorithms applied to distributions. We use the
technique to give a polynomial-time algorithm for standard ICA with sample
complexity nearly linear in the dimension, thereby improving substantially on
previous bounds. The analysis is based on properties of random polynomials,
namely the spacings of an ensemble of polynomials. Our technique also applies
to other applications of tensor decompositions, including spherical Gaussian
mixture models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2958</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2958</id><created>2014-12-09</created><authors><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author><author><keyname>Fuchs</keyname><forenames>Benedikt</forenames></author></authors><title>Physical forces between humans and how humans attract and repel each
  other based on their social interactions in an online world</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 7 figures</comments><doi>10.1371/journal.pone.0133185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical interactions between particles are the result of the exchange of
gauge bosons. Human interactions are mediated by the exchange of messages,
goods, money, promises, hostilities, etc. While in the physical world
interactions and their associated forces have immediate dynamical consequences
(Newton's law) the situation is not clear for human interactions. Here we study
the acceleration between humans who interact through the exchange of messages,
goods and hostilities in a massive multiplayer online game. For this game we
have complete information about all interactions (exchange events) between
about 1/2 million players, and about their trajectories (positions) in a metric
space of the game universe at any point in time. We derive the interaction
potentials for communication, trade and attacks and show that they are harmonic
in nature. Individuals who exchange messages and trade goods generally attract
each other and start to separate immediately after exchange events stop. The
interaction potential for attacks mirrors the usual &quot;hit-and-run&quot; tactics of
aggressive players. By measuring interaction intensities as a function of
distance, velocity and acceleration, we show that &quot;forces&quot; between players are
directly related to the number of exchange events. The power-law of the
likelihood for interactions vs. distance is in accordance with previous real
world empirical work. We show that the obtained potentials can be understood
with a simple model assuming an exchange-driven force in combination with a
distance dependent exchange rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2961</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2961</id><created>2014-12-09</created><authors><author><keyname>Greifenberg</keyname><forenames>Timo</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Integrating Heterogeneous Building and Periphery Data Models at the
  District Level: The NIM Approach</title><categories>cs.SE</categories><comments>8 pages, 4 figures</comments><journal-ref>Proceedings of the 10th European Conference on Product and Process
  Modelling (ECPPM 2014) ECPPM 2014 - eWork and eBusiness in Architecture,
  Engineering and Construction, pages 821-828, Vienna, Austria, September 2014.
  CRC Press</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating existing heterogeneous data models for buildings, neighbourhoods
and periphery devices into a common data model that can be used by all
participants, such as users, services or sensors is a cumbersome task. Usually
new extended standards emerge or ontologies are used to define mappings between
concrete data models. Within the COOPERaTE project a neighbourhood information
model (NIM) has been developed to address interoperability and allow for
various kinds of data to be stored and exchanged. The implementation of the NIM
follows a meta model based approach, allowing for runtime extension and for
easily integrating heterogeneous data models via a mapping DSL and code
generation of adaptation components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2962</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2962</id><created>2014-12-09</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Multi-Platform Generative Development of Component &amp; Connector Systems
  using Model and Code Libraries</title><categories>cs.SE</categories><comments>10 pages, 4 figures, 1 listing</comments><journal-ref>ModComp Workshop 2014 - 1st International Workshop on Model-Driven
  Engineering for Component-Based Systems, Valencia, Spain, Volume 1281 of CEUR
  Workshop Proceedings, Eds.: F. Ciccozzi, M. Tivoli, J. Carlson, CEUR-WS.org,
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Component-based software engineering aims to reduce software development
effort by reusing established components as building blocks of complex systems.
Defining components in general-purpose programming languages restricts their
reuse to platforms supporting these languages and complicates component
composition with implementation details. The vision of model-driven engineering
is to reduce the gap between developer intention and implementation details by
lifting abstract models to primary development artifacts and systematically
transforming these into executable systems. For sufficiently complex systems
the transformation from abstract models to platform-specific implementations
requires augmentation with platform-specific components. We propose a
model-driven mechanism to transform platform-independent logical component &amp;
connector architectures into platform-specific implementations combining model
and code libraries. This mechanism allows to postpone commitment to a specific
platform and thus increases reuse of software architectures and components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2963</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2963</id><created>2014-12-09</created><authors><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>The Influence of the Generator's License on Generated Artifacts</title><categories>cs.SE</categories><comments>10 pages, 2 figures, 1 table. Proceedings of the 1st Workshop on Open
  Source Software for Model Driven Engineering (OSS4MDE 2014) co-located with
  17th International Conference on Model Driven Engineering Languages and
  Systems (MODELS 2014)</comments><acm-class>K.5.1; K.5.m; D.2.9; D.3.4</acm-class><journal-ref>Proceedings of the 1st Workshop on Open Source Software for Model
  Driven Engineering (OSS4MDE 2014) co-located with MODELS 2014, pages 20-30,
  Valencia, Spain, September, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open sourcing modelling tools and generators becomes more and more important
as open source software as a whole becomes more important. We evaluate the
impact open source licenses of code generators have on the intellectual
property (IP) of generated artifacts comparing the most common open source
licenses by categories found in literature. Restrictively licensed generators
do have effects on the IP and therefore on the usability of the artifacts they
produce. We then how how this effects can be shaped to the needs of the
licensor and the licensee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2977</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2977</id><created>2014-12-09</created><authors><author><keyname>Miguel</keyname><forenames>Jose P.</forenames></author><author><keyname>Mauricio</keyname><forenames>David</forenames></author><author><keyname>Rodriguez</keyname><forenames>Glen</forenames></author></authors><title>A Review of Software Quality Models for the Evaluation of Software
  Products</title><categories>cs.SE</categories><comments>22 pages, 18 figures, International Journal of Software Engineering &amp;
  Applications (IJSEA), Vol.5, No.6, November 2014</comments><doi>10.5121/ijsea.2014.5603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actually, software products are increasing in a fast way and are used in
almost all activities of human life. Consequently measuring and evaluating the
quality of a software product has become a critical task for many companies.
Several models have been proposed to help diverse types of users with quality
issues. The development of techniques for building software has influenced the
creation of models to assess the quality. Since 2000 the construction of
software started to depend on generated or manufactured components and gave
rise to new challenges for assessing quality. These components introduce new
concepts such as configurability, reusability, availability, better quality and
lower cost. Consequently the models are classified in basic models which were
developed until 2000, and those based on components called tailored quality
models. The purpose of this article is to describe the main models with their
strengths and point out some deficiencies. In this work, we conclude that in
the present age, aspects of communications play an important factor in the
quality of software
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2985</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2985</id><created>2014-12-09</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Cause, Responsibility, and Blame: oA Structural-Model Approach</title><categories>cs.AI</categories><comments>To appear, Law, Probability, and Risk</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A definition of causality introduced by Halpern and Pearl, which uses
structural equations, is reviewed. A more refined definition is then
considered, which takes into account issues of normality and typicality, which
are well known to affect causal ascriptions. Causality is typically an
all-or-nothing notion: either A is a cause of B or it is not. An extension of
the definition of causality to capture notions of degree of responsibility and
degree of blame, due to Chockler and Halpern, is reviewed. For example, if
someone wins an election 11-0, then each person who votes for him is less
responsible for the victory than if he had won 6-5. Degree of blame takes into
account an agent's epistemic state. Roughly speaking, the degree of blame of A
for B is the expected degree of responsibility of A for B, taken over the
epistemic state of an agent. Finally, the structural-equations definition of
causality is compared to Wright's NESS test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2993</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2993</id><created>2014-12-09</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Algorithmic Rationality: Game Theory with Costly Computation</title><categories>cs.GT</categories><comments>To appear, Journal of Economic Theory. Has significant overlap with
  &quot;Game Theory with Costly Computation&quot; (arXiv:0809.0024), which is a
  preliminary version of portions of this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general game-theoretic framework for reasoning about strategic
agents performing possibly costly computation. In this framework, many
traditional game-theoretic results (such as the existence of a Nash
equilibrium) no longer hold. Nevertheless, we can use the framework to provide
psychologically appealing explanations of observed behavior in well-studied
games (such as finitely repeated prisoner's dilemma and rock-paper-scissors).
Furthermore, we provide natural conditions on games sufficient to guarantee
that equilibria exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.2994</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.2994</id><created>2014-12-09</created><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>Brigatti</keyname><forenames>Edgardo</forenames></author></authors><title>Discontinuous phase transition in an open-ended Naming Game</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>13 pages, 6 figures, to appear in JSTAT</comments><journal-ref>J. Stat. Mech. P01019 (2015)</journal-ref><doi>10.1088/1742-5468/2015/01/P01019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study on a 2-dimensional square lattice a recent version of
the Naming Game, an agent-based model used for describing the emergence of
linguistic structures. The system is open-ended and agents can invent new words
all along the evolution of the game, picking them up from a pool characterised
by a Gaussian distribution with standard deviation $\sigma$. The model displays
a nonequilibrium phase transition at a critical point $\sigma_{c}\approx 25.6$,
which separates an absorbing consensus state from an active fragmented state
where agents continuously exchange different words. The finite-size scaling
analysis of our simulations strongly suggests that the phase transition is
discontinuous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3009</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3009</id><created>2014-12-09</created><authors><author><keyname>Sachin</keyname><forenames>Narkhede</forenames></author><author><keyname>Shah</keyname><forenames>Deven</forenames></author><author><keyname>Khairnar</keyname><forenames>Vaishali</forenames></author><author><keyname>Kadu</keyname><forenames>Sujata</forenames></author></authors><title>Brain Tumor Detection Based on Bilateral Symmetry Information</title><categories>cs.CV</categories><comments>06 pages,02 figures,06 graphs. arXiv admin note: text overlap with
  arXiv:1403.6002</comments><journal-ref>ISSN:2248-9622 Vol.4,Issue 6(Version 3),June 2014,pp.98-103</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in computing technology have allowed researchers across many fields
of endeavor to collect and maintain vast amounts of observational statistical
data such as clinical data,biological patient data,data regarding access of web
sites,financial data,and the like.Brain Magnetic Resonance
Imaging(MRI)segmentation is a complex problem in the field of medical imaging
despite various presented methods.MR image of human brain can be divided into
several sub regions especially soft tissues such as gray matter,white matter
and cerebrospinal fluid.Although edge information is the main clue in image
segmentation,it can not get a better result in analysis the content of images
without combining other information.The segmentation of brain tissue in the
magnetic resonance imaging(MRI)is very important for detecting the existence
and outlines of tumors.In this paper,an algorithm about segmentation based on
the symmetry character of brain MRI image is presented.Our goal is to detect
the position and boundary of tumors automatically.Experiments were conducted on
real pictures,and the results show that the algorithm is flexible and
convenient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3010</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3010</id><created>2014-12-09</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Anisotropy-based optimal filtering in linear discrete time invariant
  systems</title><categories>cs.SY math.OC math.PR</categories><comments>15 pages, 2 figures. This paper is a slightly edited version of the
  research report: I.Vladimirov, &quot;Anisotropy-based optimal filtering in linear
  discrete time invariant systems&quot;, Centre for Applied Dynamical Systems,
  Mathematical Analysis and Probability, The University of Queensland,
  Brisbane, Australia, CADSMAP Research Report 01-03, November 2001</comments><msc-class>93C55, 93E11, 93E20, 60G10, 60G15, 60G35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a problem of robust filtering for a
finite-dimensional linear discrete time invariant system with two output
signals, one of which is directly observed while the other has to be estimated.
The system is assumed to be driven by a random disturbance produced from the
Gaussian white noise sequence by an unknown shaping filter. The worst-case
performance of an estimator is quantified by the maximum ratio of the
root-mean-square (RMS) value of the estimation error to that of the disturbance
over stationary Gaussian disturbances whose mean anisotropy is bounded from
above by a given parameter $a \ge 0$. The mean anisotropy is a combined entropy
theoretic measure of temporal colouredness and spatial &quot;nonroundness&quot; of a
signal. We construct an $a$-anisotropic estimator which minimizes the
worst-case error-to-noise RMS ratio. The estimator retains the general
structure of the Kalman filter, though with modified state-space matrices.
Computing the latter is reduced to solving a set of two coupled algebraic
Riccati equations and an equation involving the determinant of a matrix. In two
limiting cases, where $a = 0$ or $a \to +\infty$, the $a$-anisotropic estimator
leads to the standard steady-state Kalman filter or the $H_{\infty}$-optimal
estimator, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3016</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3016</id><created>2014-12-09</created><updated>2015-02-27</updated><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Smyth</keyname><forenames>W. F.</forenames></author></authors><title>Computing Covers Using Prefix Tables</title><categories>cs.DS</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An \emph{indeterminate string} $x = x[1..n]$ on an alphabet $\Sigma$ is a
sequence of nonempty subsets of $\Sigma$; $x$ is said to be \emph{regular} if
every subset is of size one. A proper substring $u$ of regular $x$ is said to
be a \emph{cover} of $x$ iff for every $i \in 1..n$, an occurrence of $u$ in
$x$ includes $x[i]$. The \emph{cover array} $\gamma = \gamma[1..n]$ of $x$ is
an integer array such that $\gamma[i]$ is the longest cover of $x[1..i]$.
Fifteen years ago a complex, though nevertheless linear-time, algorithm was
proposed to compute the cover array of regular $x$ based on prior computation
of the border array of $x$. In this paper we first describe a linear-time
algorithm to compute the cover array of regular string $x$ based on the prefix
table of $x$. We then extend this result to indeterminate strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3022</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3022</id><created>2014-12-09</created><authors><author><keyname>Scouarnec</keyname><forenames>Nicolas Le</forenames></author></authors><title>Fast Product-Matrix Regenerating Codes</title><categories>cs.DC cs.IT cs.PF math.IT</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems support failures of individual devices by the use
of replication or erasure correcting codes. While erasure correcting codes
offer a better storage efficiency than replication for similar fault tolerance,
they incur higher CPU consumption, higher network consumption and higher disk
I/Os. To address these issues, codes specific to storage systems have been
designed. Their main feature is the ability to repair a single lost disk
efficiently. In this paper, we focus on one such class of codes that minimize
network consumption during repair, namely regenerating codes. We implement the
original Product-Matrix Regenerating codes as well as a new optimization we
propose and show that the resulting optimized codes allow achieving 790 MB/s
for encoding in typical settings. Reported speeds are significantly higher than
previous studies, highlighting that regenerating codes can be used with little
CPU penalty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3023</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3023</id><created>2014-12-09</created><updated>2014-12-18</updated><authors><author><keyname>Barbero</keyname><forenames>F.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Sheng</keyname><forenames>B.</forenames></author></authors><title>Parameterized and Approximation Algorithms for the Load Coloring Problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $c, k$ be two positive integers and let $G=(V,E)$ be a graph. The
$(c,k)$-Load Coloring Problem (denoted $(c,k)$-LCP) asks whether there is a
$c$-coloring $\varphi: V \rightarrow [c]$ such that for every $i \in [c]$,
there are at least $k$ edges with both endvertices colored $i$. Gutin and Jones
(IPL 2014) studied this problem with $c=2$. They showed $(2,k)$-LCP to be fixed
parameter tractable (FPT) with parameter $k$ by obtaining a kernel with at most
$7k$ vertices. In this paper, we extend the study to any fixed $c$ by giving
both a linear-vertex and a linear-edge kernel. In the particular case of $c=2$,
we obtain a kernel with less than $4k$ vertices and less than $8k$ edges. These
results imply that for any fixed $c\ge 2$, $(c,k)$-LCP is FPT and that the
optimization version of $(c,k)$-LCP (where $k$ is to be maximized) has an
approximation algorithm with a constant ratio for any fixed $c\ge 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3038</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3038</id><created>2014-12-09</created><authors><author><keyname>Pan</keyname><forenames>Yunpeng</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos A.</forenames></author><author><keyname>Kontitsis</keyname><forenames>Michail</forenames></author></authors><title>Model-based Path Integral Stochastic Control: A Bayesian Nonparametric
  Approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, sampling-based stochastic optimal control (SOC)
frameworks have shown impressive performances in reinforcement learning (RL)
with applications in robotics. However, such approaches require a large amount
of samples from many interactions with the physical systems. To improve
learning efficiency, we present a novel model-based and data-driven SOC
framework based on path integral formulation and Gaussian processes (GPs). The
proposed approach learns explicit and time-varying optimal controls
autonomously from limited sampled data. Based on this framework, we propose an
iterative control scheme with improved applicability in higher-dimensional and
more complex control tasks. We demonstrate the effectiveness and efficiency of
the proposed framework using two nontrivial examples. Compared to
state-of-the-art RL methods, the proposed framework features superior control
learning efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3040</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3040</id><created>2014-12-09</created><authors><author><keyname>Kim</keyname><forenames>Albert</forenames></author><author><keyname>Blais</keyname><forenames>Eric</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Madden</keyname><forenames>Sam</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Rapid Sampling for Visualizations with Ordering Guarantees</title><categories>cs.DB</categories><comments>Tech Report. 17 pages. Condensed version to appear in VLDB Vol. 8 No.
  5</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Visualizations are frequently used as a means to understand trends and gather
insights from datasets, but often take a long time to generate. In this paper,
we focus on the problem of rapidly generating approximate visualizations while
preserving crucial visual proper- ties of interest to analysts. Our primary
focus will be on sampling algorithms that preserve the visual property of
ordering; our techniques will also apply to some other visual properties. For
instance, our algorithms can be used to generate an approximate visualization
of a bar chart very rapidly, where the comparisons between any two bars are
correct. We formally show that our sampling algorithms are generally applicable
and provably optimal in theory, in that they do not take more samples than
necessary to generate the visualizations with ordering guarantees. They also
work well in practice, correctly ordering output groups while taking orders of
magnitude fewer samples and much less time than conventional sampling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3046</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3046</id><created>2014-12-09</created><updated>2016-01-12</updated><authors><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Provable Tensor Methods for Learning Mixtures of Generalized Linear
  Models</title><categories>cs.LG stat.ML</categories><comments>To appear in Proceeding of AI and Statistics (AISTATS) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning mixtures of generalized linear models
(GLM) which arise in classification and regression problems. Typical learning
approaches such as expectation maximization (EM) or variational Bayes can get
stuck in spurious local optima. In contrast, we present a tensor decomposition
method which is guaranteed to correctly recover the parameters. The key insight
is to employ certain feature transformations of the input, which depend on the
input generative model. Specifically, we employ score function tensors of the
input and compute their cross-correlation with the response variable. We
establish that the decomposition of this tensor consistently recovers the
parameters, under mild non-degeneracy conditions. We demonstrate that the
computational and sample complexity of our method is a low order polynomial of
the input and the latent dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3056</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3056</id><created>2014-12-08</created><authors><author><keyname>Qaseem</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Govardhan</keyname><forenames>A.</forenames></author></authors><title>Phishing Detection in IMs using Domain Ontology and CBA - An innovative
  Rule Generation Approach</title><categories>cs.CR cs.AI cs.CY cs.IR</categories><comments>20 pages, 4 figures</comments><msc-class>68Q99</msc-class><doi>10.5121/ijist.2014.4601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User ignorance towards the use of communication services like Instant
Messengers, emails, websites, social networks etc. is becoming the biggest
advantage for phishers. It is required to create technical awareness in users
by educating them to create a phishing detection application which would
generate phishing alerts for the user so that phishing messages are not
ignored. The lack of basic security features to detect and prevent phishing has
had a profound effect on the IM clients, as they lose their faith in e-banking
and e-commerce transactions, which will have a disastrous impact on the
corporate and banking sectors and businesses which rely heavily on the
internet. Very little research contributions were available in for phishing
detection in Instant messengers. A context based, dynamic and intelligent
phishing detection methodology in IMs is proposed, to analyze and detect
phishing in Instant Messages with relevance to domain ontology (OBIE) and
utilizes the Classification based on Association (CBA) for generating phishing
rules and alerting the victims. A PDS Monitoring system algorithm is used to
identify the phishing activity during exchange of messages in IMs, with high
ratio of precision and recall. The results have shown improvement by the
increased percentage of precision and recall when compared to the existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3076</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3076</id><created>2014-12-09</created><authors><author><keyname>Aleksandrowicz</keyname><forenames>Gadi</forenames></author><author><keyname>Chockler</keyname><forenames>Hana</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Ivrii</keyname><forenames>Alexander</forenames></author></authors><title>The Computational Complexity of Structure-Based Causality</title><categories>cs.AI</categories><comments>Appears in AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Halpern and Pearl introduced a definition of actual causality; Eiter and
Lukasiewicz showed that computing whether X=x is a cause of Y=y is NP-complete
in binary models (where all variables can take on only two values) and\
Sigma_2^P-complete in general models. In the final version of their paper,
Halpern and Pearl slightly modified the definition of actual cause, in order to
deal with problems pointed by Hopkins and Pearl. As we show, this modification
has a nontrivial impact on the complexity of computing actual cause. To
characterize the complexity, a new family D_k^P, k= 1, 2, 3, ..., of complexity
classes is introduced, which generalizes the class DP introduced by
Papadimitriou and Yannakakis (DP is just D_1^P). %joe2 %We show that the
complexity of computing causality is $\D_2$-complete %under the new definition.
Chockler and Halpern \citeyear{CH04} extended the We show that the complexity
of computing causality under the updated definition is $D_2^P$-complete.
  Chockler and Halpern extended the definition of causality by introducing
notions of responsibility and blame. The complexity of determining the degree
of responsibility and blame using the original definition of causality was
completely characterized. Again, we show that changing the definition of
causality affects the complexity, and completely characterize it using the
updated definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3078</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3078</id><created>2014-12-09</created><authors><author><keyname>Ng</keyname><forenames>Jun Wei</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process
  Regression</title><categories>stat.ML cs.AI cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a practical and scalable Gaussian process model for large-scale
nonlinear probabilistic regression. Our mixture-of-experts model is
conceptually simple and hierarchically recombines computations for an overall
approximation of a full Gaussian process. Closed-form and distributed
computations allow for efficient and massive parallelisation while keeping the
memory consumption small. Given sufficient computing resources, our model can
handle arbitrarily large data sets, without explicit sparse approximations. We
provide strong experimental evidence that our model can be applied to large
data sets of sizes far beyond millions. Hence, our model has the potential to
lay the foundation for general large-scale Gaussian process research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3079</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3079</id><created>2014-12-09</created><authors><author><keyname>Bozhanov</keyname><forenames>Bozhidar</forenames></author></authors><title>Computoser - rule-based, probability-driven algorithmic music
  composition</title><categories>cs.AI cs.SD</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the Computoser hybrid probability/rule based algorithm
for music composition (http://computoser.com) and provides a reference
implementation. It addresses the issues of unpleasantness and lack of variation
exhibited by many existing approaches by combining the two methods (basing the
parameters of the rules on data obtained from preliminary analysis).
  A sample of 500+ musical pieces was analyzed to derive probabilities for
musical characteristics and events (e.g. scale, tempo, intervals). The
algorithm was constructed to produce musical pieces using the derived
probabilities combined with a large set of composition rules, which were
obtained and structured after studying established composition practices.
Generated pieces were published on the Computoser website where evaluation was
performed by listeners. The feedback was positive (58.4% approval), asserting
the merits of the undertaken approach.
  The paper compares this hybrid approach to other approaches to algorithmic
composition and presents a survey of the pleasantness of the resulting music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3084</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3084</id><created>2014-12-09</created><authors><author><keyname>Alexis</keyname><forenames>Michel</forenames></author><author><keyname>Shurbert</keyname><forenames>Davis</forenames></author><author><keyname>Dunn</keyname><forenames>Charles</forenames></author><author><keyname>Nordstrom</keyname><forenames>Jennifer</forenames></author></authors><title>Clique-Relaxed Competitive Graph Coloring</title><categories>math.CO cs.DS</categories><comments>11 pages, 4 figures, Willamette Valley REU-RET Consortium for
  Mathematics Research, Linfield College, Summer 2014</comments><msc-class>97K30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a variation of the graph coloring game, as studied in [2]. In
the original coloring game, two players, Alice and Bob, alternate coloring
vertices on a graph with legal colors from a fixed color set, where a color
{\alpha} is legal for a vertex if said vertex has no neighbors colored
{\alpha}. Other variations of the game change this definition of a legal color.
For a fixed color set, Alice wins the game if all vertices are colored when the
game ends, while Bob wins if there is a point in the game in which a vertex
cannot be assigned a legal color. The least number of colors needed for Alice
to have a winning strategy on a graph G is called the game chromatic number of
G, and is denoted \c{hi}g(G). A well studied variation is the d-relaxed
coloring game [5] in which a legal coloring of a graph G is defined as any
assignment of colors to V (G) such that the subgraph of G induced by any color
class has maximum degree d. We focus on the k-clique-relaxed n-coloring game. A
k-clique-relaxed n-coloring of a graph G is an n-coloring in which the subgraph
of G induced by any color class has maximum clique size k or less. In other
words, a k-clique-relaxed n-coloring of G is an assignment of n colors to V (G)
in which there are no monochromatic (k + 1)-cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3095</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3095</id><created>2014-12-09</created><authors><author><keyname>Elffers</keyname><forenames>Jan</forenames></author><author><keyname>de Weerdt</keyname><forenames>Mathijs</forenames></author></authors><title>Scheduling with two non-unit task lengths is NP-complete</title><categories>cs.CC</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the non-preemptive task scheduling problem with release times and
deadlines on a single machine parameterized by the set of task lengths the
tasks can have. The identical task lengths case is known to be solvable in
polynomial time. We prove that the problem with two task lengths is
NP-complete, except for the case in which the short jobs have unit task length,
which was already known to be efficiently solvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3098</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3098</id><created>2014-12-09</created><authors><author><keyname>Keshavarz</keyname><forenames>Hengameh</forenames></author><author><keyname>Mazumdar</keyname><forenames>Ravi R.</forenames></author><author><keyname>Roy</keyname><forenames>Rahul</forenames></author><author><keyname>Zoghalchi</keyname><forenames>Farshid</forenames></author></authors><title>On the number of active links in random wireless networks</title><categories>cs.IT math.IT</categories><msc-class>Primary 94A40, Secondary 60G60, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents results on the typical number of simultaneous
point-to-point transmissions above a minimum rate that can be sustained in a
network with $n$ transmitter-receiver node pairs when all transmitting nodes
can potentially interfere with all receivers. In particular we obtain a scaling
law when the fading gains are independent Rayleigh distributed random variables
and the transmitters over different realizations are located at the points of a
stationary Poisson field in the plane. We show that asymptotically with
probability approaching 1, the number of simultaneous transmissions (links that
can transmit at greater than a minimum rate) is of the order of
$O(n^{\frac{1}{4}})$. These asymptotic results are confirmed from simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3100</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3100</id><created>2014-12-09</created><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author></authors><title>Semi-Supervised Learning with Heterophily</title><categories>cs.LG cs.DB</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel linear semi-supervised learning formulation that is
derived from a solid probabilistic framework: belief propagation. We show that
our formulation generalizes a number of label propagation algorithms described
in the literature by allowing them to propagate generalized assumptions about
influences between classes of neighboring nodes. We call this formulation
Semi-Supervised Learning with Heterophily (SSL-H). We also show how the
affinity matrix can be learned from observed data with a simple convex
optimization framework that is inspired by locally linear embedding. We call
this approach Linear Heterophily Estimation (LHE). Experiments on synthetic
data show that both approaches combined can learn heterophily of a graph with
1M nodes, 10M edges and few labels in under 1min, and give better labeling
accuracies than a baseline method in the case of small fraction of explicitly
labeled nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3102</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3102</id><created>2014-12-09</created><authors><author><keyname>Dhuli</keyname><forenames>Sateeshkrishna</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Analysis of Average Travel Time for Stateless Opportunistic Routing
  Techniques</title><categories>cs.NI</categories><comments>6 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1412.2934</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless network applications, such as, searching, routing, self
stabilization and query processing can be modeled as random walks on graphs.
Stateless Opportunistic routing technique is a robust distributed routing
technique based on random walk approach, where nodes transfer the packets to
one of their direct neighbors uniformly, until the packets reach their
destinations. Simplicity in execution, fault tolerance, low overhead and
robustness to topology changes made it more suitable to wireless sensor
networks scenarios. But the main disadvantage of stateless opportunistic
routing is estimating and studying the effect of network parameters on the
packet latency. In this work, we derived the analytical expressions for mean
latency or average packet travel time for r-nearest neighbor cycle, r-nearest
neighbor torus networks. Further, we derived the generalized expression for
mean latency for m-dimensional r- nearest neighbor torus networks and studied
the effect of number of nodes, nearest neighbors and network dimension on
average packet travel time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3103</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3103</id><created>2014-12-09</created><authors><author><keyname>Chakrabarti</keyname><forenames>Aniket</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author></authors><title>Sequential Hypothesis Tests for Adaptive Locality Sensitive Hashing</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All pairs similarity search is a problem where a set of data objects is given
and the task is to find all pairs of objects that have similarity above a
certain threshold for a given similarity measure-of-interest. When the number
of points or dimensionality is high, standard solutions fail to scale
gracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and
its Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some
extent and provides substantial speedup over traditional index based
approaches. BayesLSH is used for pruning the candidate space and computation of
approximate similarity, whereas BayesLSHLite can only prune the candidates, but
similarity needs to be computed exactly on the original data. Thus where ever
the explicit data representation is available and exact similarity computation
is not too expensive, BayesLSHLite can be used to aggressively prune candidates
and provide substantial speedup without losing too much on quality. However,
the loss in quality is higher in the BayesLSH variant, where explicit data
representation is not available, rather only a hash sketch is available and
similarity has to be estimated approximately. In this work we revisit the LSH
problem from a Frequentist setting and formulate sequential tests for composite
hypothesis (similarity greater than or less than threshold) that can be
leveraged by such LSH algorithms for adaptively pruning candidates
aggressively. We propose a vanilla sequential probability ration test (SPRT)
approach based on this idea and two novel variants. We extend these variants to
the case where approximate similarity needs to be computed using fixed-width
sequential confidence interval generation technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3121</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3121</id><created>2014-12-09</created><updated>2016-02-18</updated><authors><author><keyname>Moon</keyname><forenames>Seungwhan</forenames></author><author><keyname>Kim</keyname><forenames>Suyoun</forenames></author><author><keyname>Wang</keyname><forenames>Haohan</forenames></author></authors><title>Multimodal Transfer Deep Learning with Applications in Audio-Visual
  Recognition</title><categories>cs.NE cs.LG</categories><comments>6 pages, MMML workshop at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a transfer deep learning (TDL) framework that can transfer the
knowledge obtained from a single-modal neural network to a network with a
different modality. Specifically, we show that we can leverage speech data to
fine-tune the network trained for video recognition, given an initial set of
audio-video parallel dataset within the same semantics. Our approach first
learns the analogy-preserving embeddings between the abstract representations
learned from intermediate layers of each network, allowing for semantics-level
transfer between the source and target modalities. We then apply our neural
network operation that fine-tunes the target network with the additional
knowledge transferred from the source network, while keeping the topology of
the target network unchanged. While we present an audio-visual recognition task
as an application of our approach, our framework is flexible and thus can work
with any multimodal dataset, or with any already-existing deep networks that
share the common underlying semantics. In this work in progress report, we aim
to provide comprehensive results of different configurations of the proposed
approach on two widely used audio-visual datasets, and we discuss potential
applications of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3124</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3124</id><created>2014-12-09</created><authors><author><keyname>Shao</keyname><forenames>Shuai</forenames></author><author><keyname>Huang</keyname><forenames>Xuqing</forenames></author><author><keyname>Stanley</keyname><forenames>H Eugene</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Percolation of localized attack on complex networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><doi>10.1088/1367-2630/17/2/023049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness of complex networks against node failure and malicious attack
has been of interest for decades, while most of the research has focused on
random attack or hub-targeted attack. In many real-world scenarios, however,
attacks are neither random nor hub-targeted, but localized, where a group of
neighboring nodes in a network are attacked and fail. In this paper we develop
a percolation framework to analytically and numerically study the robustness of
complex networks against such localized attack. In particular, we investigate
this robustness in Erd\H{o}s-R\'{e}nyi networks, random-regular networks, and
scale-free networks. Our results provide insight into how to better protect
networks, enhance cybersecurity, and facilitate the design of more robust
infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3125</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3125</id><created>2014-12-03</created><updated>2014-12-10</updated><authors><author><keyname>Cramer</keyname><forenames>Catherine</forenames></author><author><keyname>Sheetz</keyname><forenames>Lori</forenames></author><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Trunfio</keyname><forenames>Paul</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Uzzo</keyname><forenames>Stephen</forenames></author></authors><title>NetSci High: Bringing Network Science Research to High Schools</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 pages, 1 figure; accepted for publication in the proceedings of
  CompleNet 2015: The 6th International Workshop on Complex Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present NetSci High, our NSF-funded educational outreach program that
connects high school students who are underrepresented in STEM (Science
Technology Engineering and Mathematics), and their teachers, with regional
university research labs and provides them with the opportunity to work with
researchers and graduate students on team-based, year-long network science
research projects, culminating in a formal presentation at a network science
conference. This short paper reports the content and materials that we have
developed to date, including lesson plans and tools for introducing high school
students and teachers to network science; empirical evaluation data on the
effect of participation on students' motivation and interest in pursuing STEM
careers; the application of professional development materials for teachers
that are intended to encourage them to use network science concepts in their
lesson plans and curriculum; promoting district-level interest and engagement;
best practices gained from our experiences; and the future goals for this
project and its subsequent outgrowth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3128</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3128</id><created>2014-12-09</created><updated>2015-02-28</updated><authors><author><keyname>Redmon</keyname><forenames>Joseph</forenames></author><author><keyname>Angelova</keyname><forenames>Anelia</forenames></author></authors><title>Real-Time Grasp Detection Using Convolutional Neural Networks</title><categories>cs.RO cs.CV</categories><comments>Accepted to ICRA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an accurate, real-time approach to robotic grasp detection based
on convolutional neural networks. Our network performs single-stage regression
to graspable bounding boxes without using standard sliding window or region
proposal techniques. The model outperforms state-of-the-art approaches by 14
percentage points and runs at 13 frames per second on a GPU. Our network can
simultaneously perform classification so that in a single step it recognizes
the object and finds a good grasp rectangle. A modification to this model
predicts multiple grasps per object by using a locally constrained prediction
mechanism. The locally constrained model performs significantly better,
especially on objects that can be grasped in a variety of ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3131</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3131</id><created>2014-12-08</created><authors><author><keyname>Aajli</keyname><forenames>Ali</forenames></author><author><keyname>Afdel</keyname><forenames>Karim</forenames></author></authors><title>A tool for implementation of a domain model based on fuzzy relationships</title><categories>cs.CY cs.AI</categories><comments>61-65</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The domain model is one of the important components used by adaptive learning
systems to automatically generate customized courses for the learners. In this
paper our contribution is to propose a new tool for implementation of a domain
model based on fuzzy relationships among concepts. This tool allows the experts
and teachers to find the best parameters in order to adapt the learners's
differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3136</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3136</id><created>2014-12-09</created><authors><author><keyname>Sheff</keyname><forenames>Isaac C.</forenames></author><author><keyname>van Renesse</keyname><forenames>Robbert</forenames></author><author><keyname>Myers</keyname><forenames>Andrew C.</forenames></author></authors><title>Distributed Protocols and Heterogeneous Trust: Technical Report</title><categories>cs.DC cs.CR cs.DB cs.PL</categories><comments>This is the technical report of a submission for EuroSys 2015. 26
  Pages 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness of distributed systems is usually phrased in terms of the
number of failures of certain types that they can withstand. However, these
failure models are too crude to describe the different kinds of trust and
expectations of participants in the modern world of complex, integrated systems
extending across different owners, networks, and administrative domains. Modern
systems often exist in an environment of heterogeneous trust, in which
different participants may have different opinions about the trustworthiness of
other nodes, and a single participant may consider other nodes to differ in
their trustworthiness. We explore how to construct distributed protocols that
meet the requirements of all participants, even in heterogeneous trust
environments. The key to our approach is using lattice-based information flow
to analyse and prove protocol properties. To demonstrate this approach, we show
how two earlier distributed algorithms can be generalized to work in the
presence of heterogeneous trust: first, Heterogeneous Fast Consensus, an
adaptation of the earlier Bosco Fast Consensus protocol; and second, Nysiad, an
algorithm for converting crash-tolerant protocols to be Byzantine-tolerant.
Through simulations, we show that customizing a protocol to a heterogeneous
trust configuration yields performance improvements over the conventional
protocol designed for homogeneous trust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3137</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3137</id><created>2014-12-05</created><authors><author><keyname>Karam</keyname><forenames>Naouel</forenames></author><author><keyname>Ramakrishna</keyname><forenames>Shashishekar</forenames></author><author><keyname>Paschke</keyname><forenames>Adrian</forenames></author></authors><title>Rule reasoning for legal norm validation of FSTP facts</title><categories>cs.AI</categories><comments>1st International workshop on Artificial Intelligence and IP Law,
  AIIP- Jurix 2012- Amsterdam</comments><acm-class>K.6.3; D.2.5; F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Non-obviousness or inventive step is a general requirement for patentability
in most patent law systems. An invention should be at an adequate distance
beyond its prior art in order to be patented. This short paper provides an
overview on a methodology proposed for legal norm validation of FSTP facts
using rule reasoning approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3138</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3138</id><created>2014-12-08</created><updated>2015-01-15</updated><authors><author><keyname>Zhou</keyname><forenames>Yichao</forenames></author><author><keyname>Wu</keyname><forenames>Yuexin</forenames></author><author><keyname>Zeng</keyname><forenames>Jianyang</forenames></author></authors><title>Computational Protein Design Using AND/OR Branch-and-Bound Search</title><categories>cs.AI cs.CE cs.DS</categories><comments>RECOMB 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of the global minimum energy conformation (GMEC) is an
important and challenging topic in structure-based computational protein
design. In this paper, we propose a new protein design algorithm based on the
AND/OR branch-and-bound (AOBB) search, which is a variant of the traditional
branch-and-bound search algorithm, to solve this combinatorial optimization
problem. By integrating with a powerful heuristic function, AOBB is able to
fully exploit the graph structure of the underlying residue interaction network
of a backbone template to significantly accelerate the design process. Tests on
real protein data show that our new protein design algorithm is able to solve
many prob- lems that were previously unsolvable by the traditional exact search
algorithms, and for the problems that can be solved with traditional provable
algorithms, our new method can provide a large speedup by several orders of
magnitude while still guaranteeing to find the global minimum energy
conformation (GMEC) solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3139</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3139</id><created>2014-12-09</created><updated>2015-02-05</updated><authors><author><keyname>Buono</keyname><forenames>C.</forenames></author><author><keyname>Braunstein</keyname><forenames>L. A.</forenames></author></authors><title>Immunization strategy for epidemic spreading on multilayer networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>8 pages, 2 figures</comments><journal-ref>2015 EPL 109 26001</journal-ref><doi>10.1209/0295-5075/109/26001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world complex systems, individuals have many kind of
interactions among them, suggesting that it is necessary to consider a layered
structure framework to model systems such as social interactions. This
structure can be captured by multilayer networks and can have major effects on
the spreading of process that occurs over them, such as epidemics. In this
Letter we study a targeted immunization strategy for epidemic spreading over a
multilayer network. We apply the strategy in one of the layers and study its
effect in all layers of the network disregarding degree-degree correlation
among layers. We found that the targeted strategy is not as efficient as in
isolated networks, due to the fact that in order to stop the spreading of the
disease it is necessary to immunize more than the 80 % of the individuals.
However, the size of the epidemic is drastically reduced in the layer where the
immunization strategy is applied compared to the case with no mitigation
strategy. Thus, the immunization strategy has a major effect on the layer were
it is applied, but does not efficiently protect the individuals of other
layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3159</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3159</id><created>2014-12-09</created><authors><author><keyname>&#xc1;lvarez</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Diego</keyname><forenames>Ferran</forenames></author><author><keyname>Serrat</keyname><forenames>Joan</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Antonio M.</forenames></author></authors><title>Road Detection via On--line Label Transfer</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision-based road detection is an essential functionality for supporting
advanced driver assistance systems (ADAS) such as road following and vehicle
and pedestrian detection. The major challenges of road detection are dealing
with shadows and lighting variations and the presence of other objects in the
scene. Current road detection algorithms characterize road areas at pixel level
and group pixels accordingly. However, these algorithms fail in presence of
strong shadows and lighting variations. Therefore, we propose a road detection
algorithm based on video alignment. The key idea of the algorithm is to exploit
the similarities occurred when a vehicle follows the same trajectory more than
once. In this way, road areas are learned in a first ride and then, this road
knowledge is used to infer areas depicting drivable road surfaces in subsequent
rides. Two different experiments are conducted to validate the proposal on
different video sequences taken at different scenarios and different daytime.
The former aims to perform on-line road detection. The latter aims to perform
off-line road detection and is applied to automatically generate the
ground-truth necessary to validate road detection algorithms. Qualitative and
quantitative evaluations prove that the proposed algorithm is a valid road
detection approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3161</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3161</id><created>2014-12-09</created><authors><author><keyname>Wang</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Chen</keyname><forenames>Guobin</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author></authors><title>Object-centric Sampling for Fine-grained Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to go beyond the state-of-the-art deep convolutional
neural network (CNN) by incorporating the information from object detection,
focusing on dealing with fine-grained image classification. Unfortunately, CNN
suffers from over-fiting when it is trained on existing fine-grained image
classification benchmarks, which typically only consist of less than a few tens
of thousands training images. Therefore, we first construct a large-scale
fine-grained car recognition dataset that consists of 333 car classes with more
than 150 thousand training images. With this large-scale dataset, we are able
to build a strong baseline for CNN with top-1 classification accuracy of 81.6%.
One major challenge in fine-grained image classification is that many classes
are very similar to each other while having large within-class variation. One
contributing factor to the within-class variation is cluttered image
background. However, the existing CNN training takes uniform window sampling
over the image, acting as blind on the location of the object of interest. In
contrast, this paper proposes an \emph{object-centric sampling} (OCS) scheme
that samples image windows based on the object location information. The
challenge in using the location information lies in how to design powerful
object detector and how to handle the imperfectness of detection results. To
that end, we design a saliency-aware object detection approach specific for the
setting of fine-grained image classification, and the uncertainty of detection
results are naturally handled in our OCS scheme. Our framework is demonstrated
to be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the
large-scale fine-grained car classification dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3164</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3164</id><created>2014-12-09</created><updated>2016-01-25</updated><authors><author><keyname>Ro&#x15f;ie</keyname><forenames>R&#x103;zvan</forenames></author></authors><title>On quantum preimage attacks</title><categories>cs.CR quant-ph</categories><comments>Witdrawn by author - Inappropriate format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a preimage attack against cryptographic hash functions based on
the speedup enabled by quantum computing. Preimage resistance is a fundamental
property cryptographic hash functions must possess. The motivation behind this
work relies in the lack of conventional attacks against newly introduced hash
schemes such as the recently elected SHA-3 standard. The proposed algorithm
consists of two parts: a classical one running in O(log |S|), where S
represents the searched space, and a quantum part that contains the bulk of the
Deutsch-Jozsa circuit. The mixed approach we follow makes use of the quantum
parallelism concept to check the existence of an argument (preimage) for a
given hash value (image) in the preestablished search space. For this purpose,
we explain how a non-unitary measurement gate can be used to determine if S
contains the target value. Our method is entirely theoretical and is based on
the assumptions that a hash function can be implemented by a quantum computer
and the key measurement gate we describe is physically realizable. Finally, we
present how the algorithm finds a solution on S.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3187</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3187</id><created>2014-12-09</created><updated>2015-07-22</updated><authors><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Dehghani</keyname><forenames>Sina</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Seddighin</keyname><forenames>Saeed</forenames></author></authors><title>Revenue Maximization for Selling Multiple Correlated Items</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of selling $n$ items to a single buyer with an additive
valuation function. We consider the valuation of the items to be correlated,
i.e., desirabilities of the buyer for the items are not drawn independently.
Ideally, the goal is to design a mechanism to maximize the revenue. However, it
has been shown that a revenue optimal mechanism might be very complicated and
as a result inapplicable to real-world auctions. Therefore, our focus is on
designing a simple mechanism that achieves a constant fraction of the optimal
revenue. Babaioff et al. propose a simple mechanism that achieves a constant
fraction of the optimal revenue for independent setting with a single additive
buyer. However, they leave the following problem as an open question: &quot;Is there
a simple, approximately optimal mechanism for a single additive buyer whose
value for $n$ items is sampled from a common base-value distribution?&quot;
  Babaioff et al. show a constant approximation factor of the optimal revenue
can be achieved by either selling the items separately or as a whole bundle in
the independent setting. We show a similar result for the correlated setting
when the desirabilities of the buyer are drawn from a common base-value
distribution. It is worth mentioning that the core decomposition lemma which is
mainly the heart of the proofs for efficiency of the mechanisms does not hold
for correlated settings. Therefore we propose a modified version of this lemma
which is applicable to the correlated settings as well. Although we apply this
technique to show the proposed mechanism can guarantee a constant fraction of
the optimal revenue in a very weak correlation, this method alone can not
directly show the efficiency of the mechanism in stronger correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3188</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3188</id><created>2014-12-09</created><authors><author><keyname>Blake</keyname><forenames>Sam</forenames></author><author><keyname>Tirkel</keyname><forenames>Andrew</forenames></author></authors><title>Array Orthogonality in Higher Dimensions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the array orthogonality property for perfect autocorrelation
sequences to $n$-dimensional arrays. The generalized array orthogonality
property is used to derive a number of $n$-dimensional perfect array
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3191</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3191</id><created>2014-12-09</created><updated>2014-12-13</updated><authors><author><keyname>Liu</keyname><forenames>I-Ting</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Bhiksha</forenames></author></authors><title>Bach in 2014: Music Composition with Recurrent Neural Network</title><categories>cs.AI cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a framework for computer music composition that uses resilient
propagation (RProp) and long short term memory (LSTM) recurrent neural network.
In this paper, we show that LSTM network learns the structure and
characteristics of music pieces properly by demonstrating its ability to
recreate music. We also show that predicting existing music using RProp
outperforms Back propagation through time (BPTT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3195</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3195</id><created>2014-12-09</created><authors><author><keyname>Kenter</keyname><forenames>Franklin H. J.</forenames></author></authors><title>A Linear Cheeger Inequality using Eigenvector Norms</title><categories>math.CO cs.DM</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cheeger constant, $h_G$, is a measure of expansion within a graph. The
classical Cheeger Inequality states: $\lambda_{1}/2 \le h_G \le \sqrt{2
\lambda_{1}}$ where $\lambda_1$ is the first nontrivial eigenvalue of the
normalized Laplacian matrix. Hence, $h_G$ is tightly controlled by $\lambda_1$
to within a quadratic factor.
  We give an alternative Cheeger Inequality where we consider the $\infty$-norm
of the corresponding eigenvector in addition to $\lambda_1$. This inequality
controls $h_G$ to within a linear factor of $\lambda_1$ thereby providing an
improvement to the previous quadratic bounds. An additional advantage of our
result is that while the original Cheeger constant makes it clear that $h_G \to
0$ as $\lambda_1 \to 0$, our result shows that $h_G \to 1/2$ as $\lambda_1 \to
1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3199</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3199</id><created>2014-12-09</created><authors><author><keyname>Chakravorty</keyname><forenames>Jhelum</forenames></author><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author></authors><title>Distortion-transmission trade-off in real-time transmission of Markov
  sources</title><categories>cs.IT math.IT</categories><acm-class>H.1.1; I.2.8</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The problem of optimal real-time transmission of a Markov source under
constraints on the expected number of transmissions is considered, both for the
discounted and long term average cases. This setup is motivated by applications
where transmission is sporadic and the cost of switching on the radio and
transmitting is significantly more important than the size of the transmitted
data packet. For this model, we characterize the distortion-transmission
function, i.e., the minimum expected distortion that can be achieved when the
expected number of transmissions is less than or equal to a particular value.
In particular, we show that the distortion-transmission function is a piecewise
linear, convex, and decreasing function. We also give an explicit
characterization of each vertex of the piecewise linear function.
  To prove the results, the optimization problem is cast as a decentralized
constrained stochastic control problem. We first consider the Lagrange
relaxation of the constrained problem and identify the structure of optimal
transmission and estimation strategies. In particular, we show that the optimal
transmission is of a threshold type. Using these structural results, we obtain
dynamic programs for the Lagrange relaxations. We identify the performance of
an arbitrary threshold-type transmission strategy and use the idea of
calibration from multi-armed bandits to determine the optimal transmission
strategy for the Lagrange relaxation. Finally, we show that the optimal
strategy for the constrained setup is a randomized strategy that randomizes
between two deterministic strategies that differ only at one state. By
evaluating the performance of these strategies, we determine the shape of the
distortion-transmission function. These results are illustrated using an
example of transmitting a birth-death Markov source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3204</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3204</id><created>2014-12-10</created><authors><author><keyname>Melchionna</keyname><forenames>Andrew</forenames></author><author><keyname>Caloca</keyname><forenames>Jesus</forenames></author><author><keyname>Squires</keyname><forenames>Shane</forenames></author><author><keyname>Antonsen</keyname><forenames>Thomas M.</forenames></author><author><keyname>Ott</keyname><forenames>Edward</forenames></author><author><keyname>Girvan</keyname><forenames>Michelle</forenames></author></authors><title>The Impact of Imperfect Information on Network Attack</title><categories>physics.soc-ph cs.SI</categories><doi>10.1103/PhysRevE.91.032807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the effectiveness of network attack when the attacker has
imperfect information about the network. For Erd\H{o}s-R\'enyi networks, we
observe that dynamical importance and betweenness centrality-based attacks are
surprisingly robust to the presence of a moderate amount of imperfect
information and are more effective compared with simpler degree-based attacks
even at moderate levels of network information error. In contrast, for
scale-free networks the effectiveness of attack is much less degraded by a
moderate level of information error. Furthermore, in the Erd\H{o}os-R\'enyi
case the effectiveness of network attack is much more degraded by missing links
as compared with the same number of false links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3212</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3212</id><created>2014-12-10</created><updated>2014-12-16</updated><authors><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Tran</keyname><forenames>Gia Khanh</forenames></author><author><keyname>Shimodaira</keyname><forenames>Hidekazu</forenames></author><author><keyname>Nanba</keyname><forenames>Shinobu</forenames></author><author><keyname>Sakurai</keyname><forenames>Toshiaki</forenames></author><author><keyname>Takinami</keyname><forenames>Koji</forenames></author><author><keyname>Siaud</keyname><forenames>Isabelle</forenames></author><author><keyname>Strinati</keyname><forenames>Emilio Calvanese</forenames></author><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Karls</keyname><forenames>Ingolf</forenames></author><author><keyname>Arefi</keyname><forenames>Reza</forenames></author><author><keyname>Haustein</keyname><forenames>Thomas</forenames></author></authors><title>Millimeter-wave Evolution for 5G Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>17 pages, 12 figures, accepted to be published in IEICE Transactions
  on Communications. (Mar. 2015)</comments><journal-ref>IEICE Trans. Commun., Vol. E98-B, No. 3, Mar. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triggered by the explosion of mobile traffic, 5G (5th Generation) cellular
network requires evolution to increase the system rate 1000 times higher than
the current systems in 10 years. Motivated by this common problem, there are
several studies to integrate mm-wave access into current cellular networks as
multi-band heterogeneous networks to exploit the ultra-wideband aspect of the
mm-wave band. The authors of this paper have proposed comprehensive
architecture of cellular networks with mm-wave access, where mm-wave small cell
basestations and a conventional macro basestation are connected to
Centralized-RAN (C-RAN) to effectively operate the system by enabling power
efficient seamless handover as well as centralized resource control including
dynamic cell structuring to match the limited coverage of mm-wave access with
high traffic user locations via user-plane/control-plane splitting. In this
paper, to prove the effectiveness of the proposed 5G cellular networks with
mm-wave access, system level simulation is conducted by introducing an expected
future traffic model, a measurement based mm-wave propagation model, and a
centralized cell association algorithm by exploiting the C-RAN architecture.
The numerical results show the effectiveness of the proposed network to realize
1000 times higher system rate than the current network in 10 years which is not
achieved by the small cells using commonly considered 3.5 GHz band.
Furthermore, the paper also gives latest status of mm-wave devices and
regulations to show the feasibility of using mm-wave in the 5G systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3224</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3224</id><created>2014-12-10</created><authors><author><keyname>Zhaoyu</keyname><forenames>Dong</forenames></author><author><keyname>Bing</keyname><forenames>Gao</forenames></author><author><keyname>Yinliang</keyname><forenames>Zhao</forenames></author><author><keyname>Shaolong</keyname><forenames>Song</forenames></author><author><keyname>Yanning</keyname><forenames>Du</forenames></author></authors><title>Prophet: A Speculative Multi-threading Execution Model with
  Architectural Support Based on CMP</title><categories>cs.AR</categories><comments>9 pages</comments><acm-class>C.1.4</acm-class><doi>10.1109/EmbeddedCom-ScalCom.2009.128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speculative multi-threading (SpMT) has been proposed as a perspective method
to exploit Chip Multiprocessors (CMP) hardware potential. It is a thread level
speculation (TLS) model mainly depending on software and hardware co-design.
This paper researches speculative thread-level parallelism of general purpose
programs and a speculative multi-threading execution model called Prophet is
presented. The architectural support for Prophet execution model is designed
based on CMP. In Prophet the inter-thread data dependency are predicted by
pre-computation slice (p-slice) to reduce RAW violation. Prophet
multi-versioning Cache system along with thread state control mechanism in
architectural support are utilized for buffering the speculative data, and a
snooping bus based cache coherence protocol is used to detect data dependence
violation. The simulation-based evaluation shows that the Prophet system could
achieve significant speedup for general-purpose programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3233</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3233</id><created>2014-12-10</created><authors><author><keyname>Mayr</keyname><forenames>Christian</forenames></author><author><keyname>Partzsch</keyname><forenames>Johannes</forenames></author><author><keyname>Noack</keyname><forenames>Marko</forenames></author><author><keyname>H&#xe4;nzsche</keyname><forenames>Stefan</forenames></author><author><keyname>Scholze</keyname><forenames>Stefan</forenames></author><author><keyname>H&#xf6;ppner</keyname><forenames>Sebastian</forenames></author><author><keyname>Ellguth</keyname><forenames>Georg</forenames></author><author><keyname>Sch&#xfc;ffny</keyname><forenames>Rene</forenames></author></authors><title>A Biological-Realtime Neuromorphic System in 28 nm CMOS using
  Low-Leakage Switched Capacitor Circuits</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A switched-capacitor (SC) neuromorphic system for closed-loop neural coupling
in 28 nm CMOS is presented, occupying 600 um by 600 um. It offers 128 input
channels (i.e. presynaptic terminals), 8192 synapses and 64 output channels
(i.e. neurons). Biologically realistic neuron and synapse dynam- ics are
achieved via a faithful translation of the behavioural equations to SC
circuits. As leakage currents significantly affect circuit behaviour at this
technology node, dedicated compensation techniques are employed to achieve
biological-realtime operation, with faithful reproduction of time constants of
several 100 ms at room temperature. Power draw of the overall system is 1.9 mW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3238</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3238</id><created>2014-12-10</created><updated>2015-04-03</updated><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Loop-Back Interference Suppression for OFDM Signals via Sampled-Data
  Control</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>10th Asian Control Conference 2015 (ASCC 2015), 2015; 4 pages, 10
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider the problem of loop-back interference
suppression for orthogonal frequency division multiplexing (OFDM) signals in
amplify-and-forward single-frequency full-duplex relay stations. The loop-back
interference makes the system a closed-loop system, and hence it is important
not only to suppress the interference but also to stabilize the system. For
this purpose, we propose sampled-data $H^{\infty}$ design of digital filters
that ensure the stability of the system and suppress the continuous-time effect
of interference at the same time. Simulation results are shown to illustrate
the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3243</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3243</id><created>2014-12-10</created><authors><author><keyname>Noack</keyname><forenames>Marko</forenames></author><author><keyname>Partzsch</keyname><forenames>Johannes</forenames></author><author><keyname>Mayr</keyname><forenames>Christian</forenames></author><author><keyname>H&#xe4;nzsche</keyname><forenames>Stefan</forenames></author><author><keyname>Scholze</keyname><forenames>Stefan</forenames></author><author><keyname>H&#xf6;ppner</keyname><forenames>Sebastian</forenames></author><author><keyname>Ellguth</keyname><forenames>Georg</forenames></author><author><keyname>Sch&#xfc;ffny</keyname><forenames>Rene</forenames></author></authors><title>Switched-Capacitor Realization of Presynaptic Short-Term-Plasticity and
  Stop-Learning Synapses in 28 nm CMOS</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synaptic dynamics, such as long- and short-term plasticity, play an important
role in the complexity and biological realism achievable when running neural
networks on a neuromorphic IC. For example, they endow the IC with an ability
to adapt and learn from its environment. In order to achieve the mil- lisecond
to second time constants required for these synaptic dynamics, analog
subthreshold circuits are usually employed. However, due to process variation
and leakage problems, it is almost impossible to port these types of circuits
to modern sub-100nm technologies. In contrast, we present a neuromor- phic
system in a 28 nm CMOS process that employs switched capacitor (SC) circuits to
implement 128 short term plasticity presynapses as well as 8192 stop-learning
synapses. The neuromorphic system consumes an area of 0.36 mm2 and runs at a
power consumption of 1.9 mW. The circuit makes use of a technique for
minimizing leakage effects allowing for real-time operation with time constants
up to sev- eral seconds. Since we rely on SC techniques for all calculations,
the system is composed of only generic mixed-signal building blocks. These
generic building blocks make the system easy to port between technologies and
the large digital circuit part inherent in an SC system benefits fully from
technology scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3246</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3246</id><created>2014-12-10</created><updated>2015-06-13</updated><authors><author><keyname>Pich</keyname><forenames>J&#xe1;n</forenames><affiliation>Charles University in Prague</affiliation></author></authors><title>Logical strength of complexity theory and a formalization of the PCP
  theorem in bounded arithmetic</title><categories>math.LO cs.CC</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:8) 2015</journal-ref><doi>10.2168/LMCS-11(2:8)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several known formalizations of theorems from computational
complexity in bounded arithmetic and formalize the PCP theorem in the theory
PV1 (no formalization of this theorem was known). This includes a formalization
of the existence and of some properties of the (n,d,{\lambda})-graphs in PV1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3247</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3247</id><created>2014-12-10</created><authors><author><keyname>Wirkus</keyname><forenames>Malte</forenames></author></authors><title>Towards Robot-independent Manipulation Behavior Description</title><categories>cs.RO</categories><comments>Presented at DSLRob 2014 (arXiv:1411.7148)</comments><report-no>DSLRob/2014/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a workflow to design and control robot manipulation
behavior. To remain independent from particular robot hardware and an explicit
area of application, an embedded domain specific language (eDSL) is used to
describe the particular robot and a controller network that drives the robot.
We make use of a) a component-based software framework, b) model-based
algorithms for motion- and sensor processing representations, c) an abstract
model of the control system, and d) a plan management software, to describe a
sequence of software component networks that generate the desired robot
behavior.
  As first results, we present an eDSL for the description of a robotic system
composed of mechatronic subsystems, and for the creation of a multi-stage
control network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3260</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3260</id><created>2014-12-10</created><updated>2015-03-04</updated><authors><author><keyname>Argento</keyname><forenames>Luciano</forenames></author><author><keyname>Furfaro</keyname><forenames>Angelo</forenames></author></authors><title>A multi-protocol framework for the development of collaborative virtual
  environments</title><categories>cs.SE cs.CY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative virtual environments (CVEs) are used for collaboration and
interaction of possibly many participants that may be spread over large
distances. Both commercial and freely available CVEs exist today. Currently,
CVEs are used already in a variety of different fields: gaming, business,
education, social communication, and cooperative development. In this paper, a
general framework is proposed for the development of a cooperative environment
which is able to exploit a multi protocol network infrastructure. The framework
offers support to concerns such as communication security and inter-protocol
interoperability and let software engineers to focus on the specific business
of the CVE under development. To show the framework effectiveness we consider,
as a case of study, the design of a reusable software layer for the development
of distributed card games built on top of it. This layer is, in turn, used for
the implementation of a specific card game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3262</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3262</id><created>2014-12-10</created><updated>2016-02-04</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Robust Recovery of Stream of Pulses using Convex Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering the delays and amplitudes of a
weighted superposition of pulses. This problem is motivated by a variety of
applications such as ultrasound and radar. We show that for univariate and
bivariate stream of pulses, one can recover the delays and weights to any
desired accuracy by solving a tractable convex optimization problem, provided
that a pulse-dependent separation condition is satisfied. The main result of
this paper states that the recovery is robust to additive noise or model
mismatch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3271</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3271</id><created>2014-12-10</created><authors><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author><author><keyname>Payet</keyname><forenames>Etienne</forenames></author></authors><title>A Second-Order Formulation of Non-Termination</title><categories>cs.LO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the termination/non-termination property of a class of loops.
Such loops are commonly used abstractions of real program pieces. Second-order
logic is a convenient language to express non-termination. Of course, such
property is generally undecidable. However, by restricting the language to
known decidable cases, we exhibit new classes of loops, the non-termination of
which is decidable. We present a bunch of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3276</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3276</id><created>2014-12-10</created><authors><author><keyname>Androulakis</keyname><forenames>Emmanouil G.</forenames></author><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Generalised Entropy MDPs and Minimax Regret</title><categories>cs.LG stat.ML</categories><comments>7 pages, NIPS workshop &quot;From bad models to good policies&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian methods suffer from the problem of how to specify prior beliefs. One
interesting idea is to consider worst-case priors. This requires solving a
stochastic zero-sum game. In this paper, we extend well-known results from
bandit theory in order to discover minimax-Bayes policies and discuss when they
are practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3279</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3279</id><created>2014-12-10</created><authors><author><keyname>Euzenat</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>The category of networks of ontologies</title><categories>cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic web has led to the deployment of ontologies on the web connected
through various relations and, in particular, alignments of their vocabularies.
There exists several semantics for alignments which make difficult
interoperation between different interpretation of networks of ontologies. Here
we present an abstraction of these semantics which allows for defining the
notions of closure and consistency for networks of ontologies independently
from the precise semantics. We also show that networks of ontologies with
specific notions of morphisms define categories of networks of ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3280</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3280</id><created>2014-12-10</created><updated>2016-02-10</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Sparse Sampling in Helical Cone-Beam CT Perfect Reconstruction
  Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current paper we consider the Helical Cone Beam CT. This scanning
method exposes the patient to large quantities of radiation and results in very
large amounts of data being collected and stored. Both these facts are prime
motivators for the development of an efficient, reduced rate, sampling pattern.
We calculate bounds on the support in the frequency domain of the collected
data and use these to suggest an efficient sampling pattern. A reduction of up
to a factor of 2 in sampling rate is suggested. Indeed, we show that
reconstruction quality is not affected by this reduction of sampling rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3282</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3282</id><created>2014-12-10</created><updated>2015-01-07</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Super-resolution on the Sphere using Convex Optimization</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2399861</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering an ensemble of Diracs on a
sphere from its low resolution measurements. The Diracs can be located at any
location on the sphere, not necessarily on a grid. We show that under a
separation condition, one can recover the ensemble with high precision by a
three-stage algorithm, which consists of solving a semi-definite program, root
finding and least-square fitting. The algorithm's computation time depends
solely on the number of measurements, and not on the required solution
accuracy. We also show that in the special case of non-negative ensembles, a
sparsity condition is sufficient for recovery. Furthermore, in the discrete
setting, we estimate the recovery error in the presence of noise as a function
of the noise level and the super-resolution factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3284</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3284</id><created>2014-12-10</created><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Exact recovery of Dirac ensembles from the projection onto spaces of
  spherical harmonics</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of recovering an ensemble of Diracs on
the sphere from its projection onto spaces of spherical harmonics. We show that
under an appropriate separation condition on the unknown locations of the
Diracs, the ensemble can be recovered through Total Variation norm
minimization. The proof of the uniqueness of the solution uses the method of
`dual' interpolating polynomials and is based on [8], where the theory was
developed for trigonometric polynomials. We also show that in the special case
of non-negative ensembles, a sparsity condition is sufficient for exact
recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3289</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3289</id><created>2014-12-10</created><authors><author><keyname>Akpannah</keyname><forenames>Inibehe Emmanuel</forenames></author></authors><title>Optimization of Software Quality Using Management and Technical Review
  Techniques</title><categories>cs.SE</categories><comments>5 Pages and 1 figure</comments><journal-ref>Optimization of Software Quality using Management and Technical
  Review Techniques- International Journal of Computer Trends and Technology by
  IJCTTJournal Volume-17 Number-6 Year of Publication : 2014 Authors :Inibehe
  Emmanuel Akpannah</journal-ref><doi>10.14445/22312803/IJCTT-V17P154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing the quality of software is a function of the degree of reviews
made during the early life of a software development process. Reviews detect
errors and potential errors early in the software development process. The
errors detected during the early life cycle of software are least expensive to
correct. Efficient involvement in software inspections and technical reviews,
help developers improve their own skills, thereby mitigating the occurrence of
errors in the later stage of software development process. The ideas gathered
on this paper point that a properly implemented program of technical and
management reviews drastically reduces the time as well as the cost required
for testing, debugging, and reworking, and dramatically improves the quality of
the resulting product. This paper, Optimization of Software Quality using
management and technical Review Techniques, provides its readers with the
opportunity to learn about and experience using this indispensable software
quality tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3290</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3290</id><created>2014-12-10</created><updated>2015-05-23</updated><authors><author><keyname>Imbach</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Pouget</keyname><forenames>Marc</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Numeric certified algorithm for the topology of resultant and
  discriminant curves</title><categories>cs.CG cs.DS cs.SC math.AG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal C$ be a real plane algebraic curve defined by the resultant of
two polynomials (resp. by the discriminant of a polynomial). Geometrically such
a curve is the projection of the intersection of the surfaces
$P(x,y,z)=Q(x,y,z)=0$ (resp. $P(x,y,z)=\frac{\partial P}{\partial
z}(x,y,z)=0$), and generically its singularities are nodes (resp. nodes and
ordinary cusps). State-of-the-art numerical algorithms compute the topology of
smooth curves but usually fail to certify the topology of singular ones. The
main challenge is to find practical numerical criteria that guarantee the
existence and the uniqueness of a singularity inside a given box $B$, while
ensuring that $B$ does not contain any closed loop of $\mathcal{C}$. We solve
this problem by first providing a square deflation system, based on
subresultants, that can be used to certify numerically whether $B$ contains a
unique singularity $p$ or not. Then we introduce a numeric adaptive separation
criterion based on interval arithmetic to ensure that the topology of $\mathcal
C$ in $B$ is homeomorphic to the local topology at $p$. Our algorithms are
implemented and experiments show their efficiency compared to state-of-the-art
symbolic or homotopic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3325</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3325</id><created>2014-12-09</created><authors><author><keyname>Henze</keyname><forenames>Martin</forenames></author><author><keyname>Hermerschmidt</keyname><forenames>Lars</forenames></author><author><keyname>Kerpen</keyname><forenames>Daniel</forenames></author><author><keyname>H&#xe4;u&#xdf;ling</keyname><forenames>Roger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>User-driven Privacy Enforcement for Cloud-based Services in the Internet
  of Things</title><categories>cs.SE</categories><comments>6 pages, 2 figures, 1 listing. The 2nd International Conference on
  Future Internet of Things and Cloud (FiCloud-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things devices are envisioned to penetrate essentially all
aspects of life, including homes and urbanspaces, in use cases such as health
care, assisted living, and smart cities. One often proposed solution for
dealing with the massive amount of data collected by these devices and offering
services on top of them is the federation of the Internet of Things and cloud
computing. However, user acceptance of such systems is a critical factor that
hinders the adoption of this promising approach due to severe privacy concerns.
We present UPECSI, an approach for user-driven privacy enforcement for
cloud-based services in the Internet of Things to address this critical factor.
UPECSI enables enforcement of all privacy requirements of the user once her
sensitive data leaves the border of her network, provides a novel approach for
the integration of privacy functionality into the development process of
cloud-based services, and offers the user an adaptable and transparent
configuration of her privacy requirements. Hence, UPECSI demonstrates an
approach for realizing user-accepted cloud services in the Internet of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3328</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3328</id><created>2014-12-10</created><updated>2015-12-29</updated><authors><author><keyname>Iscen</keyname><forenames>Ahmet</forenames></author><author><keyname>Furon</keyname><forenames>Teddy</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author><author><keyname>J&#xe9;gou</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Memory vectors for similarity search in high-dimensional spaces</title><categories>cs.CV cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an indexing architecture to store and search in a database of
high-dimensional vectors. This architecture is composed of several memory
units, each of which summarizes a fraction of the database by a single
representative vector.The potential similarity of the query to one of the
vectors stored in the memory unit is gauged by a simple correlation with the
memory unit's representative vector. This representative optimizes the test of
the following hypothesis: the query is independent from any vector in the
memory unit vs. the query is a simple perturbation of one of the stored
vectors. Compared to exhaustive search, our approach finds the most similar
database vectors significantly faster without a noticeable reduction in search
quality. Interestingly, the reduction of complexity is provably better in
high-dimensional spaces. We empirically demonstrate its practical interest in a
large-scale image search scenario with off-the-shelf state-of-the-art
descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3333</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3333</id><created>2014-12-10</created><authors><author><keyname>Prefect</keyname><forenames>Frod</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>Empirical Algorithmics: draw your own conclusions</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1207.4616</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an empirical comparisons of algorithms we might compare run times over a
set of benchmark problems to decide which one is fastest, i.e. an algorithmic
horse race. Ideally we would like to download source code for the algorithms,
compile and then run on our machine. Sometimes code isn't available to download
and sometimes resource isn't available to implement all the algorithms we want
to study. To get round this, published results are rescaled, a technique
endorsed by DIMACS, and those rescaled results included in a new study. This
technique is frequently used when presenting new algorithms for the maximum
clique problem. We demonstrate that this is unsafe, and that if carelessly used
may allow us to draw conflicting conclusions from our empirical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3334</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3334</id><created>2014-12-10</created><authors><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author><author><keyname>Saitoh</keyname><forenames>Toshiki</forenames></author><author><keyname>Satoh</keyname><forenames>Hisayuki</forenames></author><author><keyname>Suzuki</keyname><forenames>Akira</forenames></author><author><keyname>Uchizawa</keyname><forenames>Kei</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author><author><keyname>Yamanaka</keyname><forenames>Katsuhisa</forenames></author><author><keyname>Zhou</keyname><forenames>Xiao</forenames></author></authors><title>Computational Complexity of Competitive Diffusion on (Un)weighted Graphs</title><categories>cs.CC cs.DS cs.GT cs.SI</categories><comments>34 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an undirected graph modeling a social network, where the vertices
represent users, and the edges do connections among them. In the competitive
diffusion game, each of a number of players chooses a vertex as a seed to
propagate his/her opinion, and then it spreads along the edges in the graphs.
The objective of every player is to maximize the number of vertices the opinion
infects. In this paper, we investigate a computational problem of asking
whether a pure Nash equilibrium exists in the competitive diffusion game on
unweighed and weighted graphs, and present several negative and positive
results. We first prove that the problem is W[1]-hard when parameterized by the
number of players even for unweighted graphs. We also show that the problem is
NP-hard even for series-parallel graphs with positive integer weights, and is
NP-hard even for forests with arbitrary integer weights. Furthermore, we show
that the problem for forest of paths with arbitrary weights is solvable in
pseudo-polynomial time; and it is solvable in quadratic time if a given graph
is unweighted. We also prove that the problem for chain, cochain, and threshold
graphs with arbitrary integer weights is solvable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3335</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3335</id><created>2014-12-10</created><authors><author><keyname>Karmarkar</keyname><forenames>Narendra</forenames></author></authors><title>Towards a Broader View of Theory of Computing</title><categories>cs.NA cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beginning with the projectively invariant method for linear programming,
interior point methods have led to powerful algorithms for many difficult
computing problems, in combinatorial optimization, logic, number theory and
non-convex optimization. Algorithms for convex optimization benefitted from
many pre-established ideas from classical mathematics, but non-convex problems
require new concepts. Lecture series I am presenting at the conference on
Foundations of Computational Mathematics, 2014, outlines some of these
concepts{computational models based on the concept of the continuum, algorithms
invariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on
co-ordinate representation, extended proof systems for more efficient
certificates of optimality, extensions of Grassmanns extension theory,
efficient evaluation methods for the effect of exponential number of
constraints, theory of connected sets based on graded connectivity, theory of
curved spaces adapted to the problem data, and concept of relatively algebraic
sets in curved space. Since this conference does not have a proceedings, the
purpose of this article is to provide the material being presented at the
conference in more widely accessible form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3336</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3336</id><created>2014-12-10</created><authors><author><keyname>Zanette</keyname><forenames>Dami&#xe1;n H.</forenames></author></authors><title>Statistical Patterns in Written Language</title><categories>cs.CL</categories><comments>First published in the public domain: Sept. 2012
  (http://fisica.cab.cnea.gov.ar/estadistica/zanette/miscellanea.html)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Quantitative linguistics has been allowed, in the last few decades, within
the admittedly blurry boundaries of the field of complex systems. A growing
host of applied mathematicians and statistical physicists devote their efforts
to disclose regularities, correlations, patterns, and structural properties of
language streams, using techniques borrowed from statistics and information
theory. Overall, results can still be categorized as modest, but the prospects
are promising: medium- and long-range features in the organization of human
language -which are beyond the scope of traditional linguistics- have already
emerged from this kind of analysis and continue to be reported, contributing a
new perspective to our understanding of this most complex communication system.
This short book is intended to review some of these recent contributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3347</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3347</id><created>2014-12-10</created><authors><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Stein</keyname><forenames>Yannik</forenames></author></authors><title>Computational Aspects of the Colorful Caratheodory Theorem</title><categories>cs.CG</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P_1,...,P_{d+1} \subset \mathbb{R}^d$ be $d$-dimensional point sets such
that the convex hull of each $P_i$ contains the origin. We call the sets $P_i$
color classes, and we think of the points in $P_i$ as having color $i$. A
colorful choice is a set with at most one point of each color. The colorful
Caratheodory theorem guarantees the existence of a colorful choice whose convex
hull contains the origin. So far, the computational complexity of finding such
a colorful choice is unknown.
  We approach this problem from two directions. First, we consider
approximation algorithms: an $m$-colorful choice is a set that contains at most
$m$ points from each color class. We show that for any constant $\varepsilon &gt;
0$, an $\lceil \varepsilon(d+1)\rceil$-colorful choice containing the origin in
its convex hull can be found in polynomial time. This notion of approximation
has not been studied before, and it is motivated through the applications of
the colorful Caratheodory theorem in the literature. In the second part, we
present a natural generalization of the colorful Caratheodory problem: in the
Nearest Colorful Polytope problem (NCP), we are given sets $P_1,...,P_n \subset
\mathbb{R}^d$ that do not necessarily contain the origin in their convex hulls.
The goal is to find a colorful choice whose convex hull minimizes the distance
to the origin. We show that computing local optima for the NCP problem is
PLS-complete, while computing a global optimum is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3350</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3350</id><created>2014-12-10</created><updated>2015-05-27</updated><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author></authors><title>A counterexample to the pseudo 2-factor isomorphic graph conjecture</title><categories>math.CO cs.DM</categories><comments>8 pages, added some extra information in Discrete Applied Mathematics
  (2015)</comments><doi>10.1016/j.dam.2015.04.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is pseudo 2-factor isomorphic if the parity of the number of
cycles in a 2-factor is the same for all 2-factors of $G$. Abreu et al.
conjectured that $K_{3,3}$, the Heawood graph and the Pappus graph are the only
essentially 4-edge-connected pseudo 2-factor isomorphic cubic bipartite graphs
(Abreu et al., Journal of Combinatorial Theory, Series B, 2008, Conjecture
3.6).
  Using a computer search we show that this conjecture is false by constructing
a counterexample with 30 vertices. We also show that this is the only
counterexample up to at least 40 vertices.
  A graph $G$ is 2-factor hamiltonian if all 2-factors of $G$ are hamiltonian
cycles. Funk et al. conjectured that every 2-factor hamiltonian cubic bipartite
graph can be obtained from $K_{3,3}$ and the Heawood graph by applying repeated
star products (Funk et al., Journal of Combinatorial Theory, Series B, 2003,
Conjecture 3.2). We verify that this conjecture holds up to at least 40
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3352</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3352</id><created>2014-12-08</created><authors><author><keyname>Pourali</keyname><forenames>Neda</forenames></author></authors><title>Web image annotation by diffusion maps manifold learning algorithm</title><categories>cs.CV cs.IR cs.LG</categories><comments>11 pages, 8 figures</comments><msc-class>68T10</msc-class><doi>10.5121/ijfcst.2014.4606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic image annotation is one of the most challenging problems in machine
vision areas. The goal of this task is to predict number of keywords
automatically for images captured in real data. Many methods are based on
visual features in order to calculate similarities between image samples. But
the computation cost of these approaches is very high. These methods require
many training samples to be stored in memory. To lessen this burden, a number
of techniques have been developed to reduce the number of features in a
dataset. Manifold learning is a popular approach to nonlinear dimensionality
reduction. In this paper, we investigate Diffusion maps manifold learning
method for web image auto-annotation task. Diffusion maps manifold learning
method is used to reduce the dimension of some visual features. Extensive
experiments and analysis on NUS-WIDE-LITE web image dataset with different
visual features show how this manifold learning dimensionality reduction method
can be applied effectively to image annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3359</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3359</id><created>2014-12-10</created><updated>2015-04-17</updated><authors><author><keyname>Duan</keyname><forenames>Qi</forenames></author><author><keyname>Jafarian</keyname><forenames>Haadi</forenames></author><author><keyname>Al-Shaer</keyname><forenames>Ehab</forenames></author><author><keyname>Xu</keyname><forenames>Jinhui</forenames></author></authors><title>On DDoS Attack Related Minimum Cut Problems</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study two important extensions of the classical minimum cut
problem, called {\em Connectivity Preserving Minimum Cut (CPMC)} problem and
{\em Threshold Minimum Cut (TMC)} problem, which have important applications in
large-scale DDoS attacks. In CPMC problem, a minimum cut is sought to separate
a of source from a destination node and meanwhile preserve the connectivity
between the source and its partner node(s). The CPMC problem also has important
applications in many other areas such as emergency responding, image
processing, pattern recognition, and medical sciences. In TMC problem, a
minimum cut is sought to isolate a target node from a threshold number of
partner nodes. TMC problem is an important special case of network inhibition
problem and has important applications in network security. We show that the
general CPMC problem cannot be approximated within $logn$ unless $NP=P$ has
quasi-polynomial algorithms. We also show that a special case of two group CPMC
problem in planar graphs can be solved in polynomial time. The corollary of
this result is that the network diversion problem in planar graphs is in $P$, a
previously open problem. We show that the threshold minimum node cut (TMNC)
problem can be approximated within ratio $O(\sqrt{n})$ and the threshold
minimum edge cut problem (TMEC) can be approximated within ratio
$O(\log^2{n})$. \emph{We also answer another long standing open problem: the
hardness of the network inhibition problem and network interdiction problem. We
show that both of them cannot be approximated within any constant ratio. unless
$NP \nsubseteq \cap_{\delta&gt;0} BPTIME(2^{n^{\delta}})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3367</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3367</id><created>2014-12-10</created><authors><author><keyname>Rajan</keyname><forenames>R. Arokia Paul</forenames></author><author><keyname>Francis</keyname><forenames>F. Sagayaraj</forenames></author></authors><title>Experimenting with Request Assignment Simulator (RAS)</title><categories>cs.DC</categories><comments>November 2014, IJCSE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no existence of dedicated simulators on the Internet that studies
the impact of load balancing principles of the cloud architectures. Request
Assignment Simulator (RAS) is a customizable, visual tool that helps to
understand the request assignment to the resources based on the load balancing
principles. We have designed this simulator to fit into Infrastructure as a
Service (IaaS) cloud model. In this paper, we present a working manual useful
for the conduct of experiment with RAS. The objective of this paper is to
instill the user to understand the pertinent parameters in the cloud, their
metrics, load balancing principles, and their impact on the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3369</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3369</id><created>2014-12-10</created><authors><author><keyname>Ahmed</keyname><forenames>Faruk</forenames></author><author><keyname>Tarlow</keyname><forenames>Daniel</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>Candidate Constrained CRFs for Loss-Aware Structured Prediction</title><categories>cs.CV</categories><comments>20 pages including Supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When evaluating computer vision systems, we are often concerned with
performance on a task-specific evaluation measure such as the
Intersection-Over-Union score used in the PASCAL VOC image segmentation
challenge. Ideally, our systems would be tuned specifically to these evaluation
measures. However, despite much work on loss-aware structured prediction, top
performing systems do not use these techniques. In this work, we seek to
address this problem, incorporating loss-aware prediction in a manner that is
amenable to the approaches taken by top performing systems. Our main idea is to
simultaneously leverage two systems: a highly tuned pipeline system as is found
on top of leaderboards, and a traditional CRF. We show how to combine high
quality candidate solutions from the pipeline with the probabilistic approach
of the CRF that is amenable to loss-aware prediction. The result is that we can
use loss-aware prediction methodology to improve performance of the highly
tuned pipeline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3374</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3374</id><created>2014-12-10</created><authors><author><keyname>Landi</keyname><forenames>Claudia</forenames></author></authors><title>The rank invariant stability via interleavings</title><categories>cs.CG</categories><msc-class>13P20, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lower bound for the interleaving distance on persistence vector spaces is
given in terms of rank invariants. This offers an alternative proof of the
stability of rank invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3377</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3377</id><created>2014-12-10</created><authors><author><keyname>Murphy</keyname><forenames>Niall</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>Uniformity is weaker than semi-uniformity for some membrane systems</title><categories>cs.CC</categories><comments>28 pages, 1 figure</comments><acm-class>F.1.1; F.1.3</acm-class><journal-ref>Fundamenta Informaticae, 134(1-2):129-152. 2014</journal-ref><doi>10.3233/FI-2014-1095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate computing models that are presented as families of finite
computing devices with a uniformity condition on the entire family. Examples of
such models include Boolean circuits, membrane systems, DNA computers, chemical
reaction networks and tile assembly systems, and there are many others.
However, in such models there are actually two distinct kinds of uniformity
condition. The first is the most common and well-understood, where each input
length is mapped to a single computing device (e.g. a Boolean circuit) that
computes on the finite set of inputs of that length. The second, called
semi-uniformity, is where each input is mapped to a computing device for that
input (e.g. a circuit with the input encoded as constants). The former notion
is well-known and used in Boolean circuit complexity, while the latter notion
is frequently found in literature on nature-inspired computation from the past
20 years or so.
  Are these two notions distinct? For many models it has been found that these
notions are in fact the same, in the sense that the choice of uniformity or
semi-uniformity leads to characterisations of the same complexity classes. In
other related work, we showed that these notions are actually distinct for
certain classes of Boolean circuits. Here, we give analogous results for
membrane systems by showing that certain classes of uniform membrane systems
are strictly weaker than the analogous semi-uniform classes. This solves a
known open problem in the theory of membrane systems. We then go on to present
results towards characterising the power of these semi-uniform and uniform
membrane models in terms of NL and languages reducible to the unary languages
in NL, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3397</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3397</id><created>2014-12-10</created><updated>2015-05-03</updated><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Ran</forenames></author><author><keyname>Srihari</keyname><forenames>Sargur</forenames></author></authors><title>Sequential Labeling with online Deep Learning</title><categories>cs.LG</categories><comments>9 pages, 1 figure</comments><msc-class>68T10</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Deep learning has attracted great attention recently and yielded the state of
the art performance in dimension reduction and classification problems.
However, it cannot effectively handle the structured output prediction, e.g.
sequential labeling. In this paper, we propose a deep learning structure, which
can learn discriminative features for sequential labeling problems. More
specifically, we add the inter-relationship between labels in our deep learning
structure, in order to incorporate the context information from the sequential
data. Thus, our model is more powerful than linear Conditional Random Fields
(CRFs) because the objective function learns latent non-linear features so that
target labeling can be better predicted. We pretrain the deep structure with
stacked restricted Boltzmann machines (RBMs) for feature learning and optimize
our objective function with online learning algorithm, a mixture of perceptron
training and stochastic gradient descent. We test our model on different
challenge tasks, and show that our model outperforms significantly over the
completive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3405</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3405</id><created>2014-12-10</created><updated>2015-04-29</updated><authors><author><keyname>Wu</keyname><forenames>Zhihao</forenames></author><author><keyname>Menichetti</keyname><forenames>Giulia</forenames></author><author><keyname>Rahmede</keyname><forenames>Christoph</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Emergent Complex Network Geometry</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>(24 pages, 7 figures, 1 table)</comments><report-no>KA-TP-34-2014</report-no><journal-ref>Nature Scientific Reports 5, 10073 (2015)</journal-ref><doi>10.1038/srep10073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are mathematical structures that are universally used to describe a
large variety of complex systems such as the brain or the Internet.
Characterizing the geometrical properties of these networks has become
increasingly relevant for routing problems, inference and data mining. In real
growing networks, topological, structural and geometrical properties emerge
spontaneously from their dynamical rules. Nevertheless we still miss a model in
which networks develop an emergent complex geometry. Here we show that a single
two parameter network model, the growing geometrical network, can generate
complex network geometries with non-trivial distribution of curvatures,
combining exponential growth and small-world properties with finite spectral
dimensionality. In one limit, the non-equilibrium dynamical rules of these
networks can generate scale-free networks with clustering and communities, in
another limit planar random geometries with non-trivial modularity. Finally we
find that these properties of the geometrical growing networks are present in a
large set of real networks describing biological, social and technological
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3409</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3409</id><created>2014-12-10</created><updated>2015-01-27</updated><authors><author><keyname>Clark</keyname><forenames>Christopher</forenames></author><author><keyname>Storkey</keyname><forenames>Amos</forenames></author></authors><title>Teaching Deep Convolutional Neural Networks to Play Go</title><categories>cs.AI cs.LG cs.NE</categories><comments>9 pages, 8 figures, 5 tables. Corrected typos, minor adjustment to
  table format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mastering the game of Go has remained a long standing challenge to the field
of AI. Modern computer Go systems rely on processing millions of possible
future positions to play well, but intuitively a stronger and more 'humanlike'
way to play the game would be to rely on pattern recognition abilities rather
then brute force computation. Following this sentiment, we train deep
convolutional neural networks to play Go by training them to predict the moves
made by expert Go players. To solve this problem we introduce a number of novel
techniques, including a method of tying weights in the network to 'hard code'
symmetries that are expect to exist in the target function, and demonstrate in
an ablation study they considerably improve performance. Our final networks are
able to achieve move prediction accuracies of 41.1% and 44.4% on two different
Go datasets, surpassing previous state of the art on this task by significant
margins. Additionally, while previous move prediction programs have not yielded
strong Go playing programs, we show that the networks trained in this work
acquired high levels of skill. Our convolutional neural networks can
consistently defeat the well known Go program GNU Go, indicating it is state of
the art among programs that do not use Monte Carlo Tree Search. It is also able
to win some games against state of the art Go playing program Fuego while using
a fraction of the play time. This success at playing Go indicates high level
principles of the game were learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3411</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3411</id><created>2014-12-10</created><authors><author><keyname>Shelton</keyname><forenames>Jacquelyn A.</forenames></author><author><keyname>Gasthaus</keyname><forenames>Jan</forenames></author><author><keyname>Dai</keyname><forenames>Zhenwen</forenames></author><author><keyname>Luecke</keyname><forenames>Joerg</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author></authors><title>GP-select: Accelerating EM using adaptive subspace preselection</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonparametric procedure to achieve fast inference in generative
graphical models when the number of latent states is very large. The approach
is based on iterative latent variable preselection, where we alternate between
learning a 'selection function' to reveal the relevant latent variables, and
use this to obtain a compact approximation of the posterior distribution for
EM; this can make inference possible where the number of possible latent states
is e.g. exponential in the number of latent variables, whereas an exact
approach would be computationally unfeasible. We learn the selection function
entirely from the observed data and current EM state via Gaussian process
regression: this is by contrast with earlier approaches, where selections were
hand-designed for each problem setting. We show our approach to perform as well
as these bespoke selection functions on a wide variety of inference problems:
in particular, for the challenging case of a hierarchical model for object
localization with occlusion, we achieve results that match a customized
state-of-the-art selection method, at a far lower computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3414</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3414</id><created>2014-12-10</created><updated>2015-07-17</updated><authors><author><keyname>Feigenbaum</keyname><forenames>Itai</forenames></author><author><keyname>Sethuraman</keyname><forenames>Jay</forenames></author></authors><title>Strategyproof Mechanisms for One-Dimensional Hybrid and Obnoxious
  Facility Location</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a strategic variant of the facility location problem. We would
like to locate a facility on a closed interval. There are n agents located on
that interval, divided into two types: type 1 agents, who wish for the facility
to be as far from them as possible, and type 2 agents, who wish for the
facility to be as close to them as possible. Our goal is to maximize a form of
aggregated social benefit: maxisum- the sum of the agents' utilities, or the
egalitarian objective- the minimal agent utility. The strategic aspect of the
problem is that the agents' locations are not known to us, but rather reported
to us by the agents- an agent might misreport his location in an attempt to
move the facility away from or towards to his true location. We therefore
require the facility-locating mechanism to be strategyproof, namely that
reporting truthfully is a dominant strategy for each agent. As simply
maximizing the social benefit is generally not strategyproof, our goal is to
design strategyproof mechanisms with good approximation ratios.
  For the maxisum objective, in the deterministic setting, we provide a
best-possible 3- approximate strategyproof mechanism; in the randomized
setting, we provide a 23/13- approximate strategyproof mechanism and a lower
bound of \frac{2}{\sqrt{3}}. For the egalitarian objective, we provide a lower
bound of 3/2 in the randomized setting, and show that no bounded approximation
ratio is attainable in the deterministic setting. To obtain our deterministic
lower bounds, we characterize all deterministic strategyproof mechanisms when
all agents are of type 1. Finally, we consider a generalized model that allows
an agent to control more than one location, and provide best-possible 3- and
3/2- approximate strategyproof mechanisms for maxisum, in the deterministic and
randomized settings respectively, when only type 1 agents are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3421</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3421</id><created>2014-12-10</created><updated>2015-06-12</updated><authors><author><keyname>Iglesias</keyname><forenames>Juan Eugenio</forenames></author><author><keyname>Sabuncu</keyname><forenames>Mert Rory</forenames></author></authors><title>Multi-Atlas Segmentation of Biomedical Images: A Survey</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-atlas segmentation (MAS), first introduced and popularized by the
pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh,
Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert
and Hammers (2006), is becoming one of the most widely-used and successful
image segmentation techniques in biomedical applications. By manipulating and
utilizing the entire dataset of &quot;atlases&quot; (training images that have been
previously labeled, e.g., manually by an expert), rather than some model-based
average representation, MAS has the flexibility to better capture anatomical
variation, thus offering superior segmentation accuracy. This benefit, however,
typically comes at a high computational cost. Recent advancements in computer
hardware and image processing software have been instrumental in addressing
this challenge and facilitated the wide adoption of MAS. Today, MAS has come a
long way and the approach includes a wide array of sophisticated algorithms
that employ ideas from machine learning, probabilistic modeling, optimization,
and computer vision, among other fields. This paper presents a survey of
published MAS algorithms and studies that have applied these methods to various
biomedical problems. In writing this survey, we have three distinct aims. Our
primary goal is to document how MAS was originally conceived, later evolved,
and now relates to alternative methods. Second, this paper is intended to be a
detailed reference of past research activity in MAS, which now spans over a
decade (2003 - 2014) and entails novel methodological developments and
application-specific solutions. Finally, our goal is to also present a
perspective on the future of MAS, which, we believe, will be one of the
dominant approaches in biomedical image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3445</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3445</id><created>2014-12-10</created><updated>2014-12-12</updated><authors><author><keyname>Holzer</keyname><forenames>Stephan</forenames></author><author><keyname>Pinsker</keyname><forenames>Nathan</forenames></author></authors><title>Approximation of Distances and Shortest Paths in the Broadcast Congest
  Clique</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the broadcast version of the CONGEST CLIQUE model of distributed
computing. In this model, in each round, any node in a network of size $n$ can
send the same message (i.e. broadcast a message) of limited size to every other
node in the network. Nanongkai presented in [STOC'14] a randomized
$(2+o(1))$-approximation algorithm to compute all pairs shortest paths (APSP)
in time $\tilde{O}(\sqrt{n})$ on weighted graphs, where we use the convention
that $\tilde{\Omega}(f(n))$ is essentially $\Omega(f(n)/$polylog$f(n))$ and
$\tilde{O}(f(n))$ is essentially $O(f(n) $polylog$f(n))$. We complement this
result by proving that any randomized $(2-o(1))$-approximation of APSP and
$(2-o(1))$-approximation of the diameter of a graph takes $\tilde\Omega(n)$
time in the worst case. This demonstrates that getting a negligible improvement
in the approximation factor requires significantly more time. Furthermore this
bound implies that already computing a $(2-o(1))$-approximation of all pairs
shortest paths is among the hardest graph-problems in the broadcast-version of
the CONGEST CLIQUE model and contrasts a recent $(1+o(1))$-approximation for
APSP that runs in time $O(n^{0.15715})$ in the unicast version of the CONGEST
CLIQUE model. On the positive side we provide a deterministic version of
Nanongkai's $(2+o(1))$-approximation algorithm for APSP. To do so we present a
fast deterministic construction of small hitting sets. We also show how to
replace another randomized part within Nanongkai's algorithm with a
deterministic source-detection algorithm designed for the CONGEST model
presented by Lenzen and Peleg at PODC'13.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3448</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3448</id><created>2014-12-10</created><updated>2015-11-11</updated><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Lu</keyname><forenames>Xuanxuan</forenames></author></authors><title>MIMO Beamforming Design towards Maximizing Mutual Information in
  Wireless Sensor Network</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers joint beamformer design towards maximizing the mutual
information in a coherent wireless sensor network with noisy observation and
multiple antennae. Leveraging the weighted minimum mean square error and block
coordinate ascent (BCA) framework, we propose two new and efficient methods:
batch-mode BCA and cyclic multi-block BCA. The existing batch-mode approaches
require stringent conditions such as diagonal channel matrices and positive
definite second-order matrices, and are therefore inapplicable to our problem.
Our match-mode BCA overcomes the previous limitations via a general
second-order cone programming formation, and exhibits a strong convergence
property which we have rigorously proven. The existing multi-block approaches
rely on numerical solvers to handle the subproblems and some render good
performance only at high signal-to-noise ratios. Exploiting the convexity of
the trust-region subproblem for the convex case, our multi-block BCA
significantly reduces the complexity and enhances the previous results by
providing an analytical expression for the energy-preserving optimal solution.
Analysis and simulations confirm the advantages of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3474</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3474</id><created>2014-12-10</created><authors><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Deep Domain Confusion: Maximizing for Domain Invariance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias on a standard
benchmark. Fine-tuning deep models in a new domain can require a significant
amount of data, which for many applications is simply not available. We propose
a new CNN architecture which introduces an adaptation layer and an additional
domain confusion loss, to learn a representation that is both semantically
meaningful and domain invariant. We additionally show that a domain confusion
metric can be used for model selection to determine the dimension of an
adaptation layer and the best position for the layer in the CNN architecture.
Our proposed adaptation method offers empirical performance which exceeds
previously published results on a standard benchmark visual domain adaptation
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3480</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3480</id><created>2014-12-10</created><updated>2015-09-25</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Logic programming beyond Prolog</title><categories>cs.PL cs.LO</categories><comments>19 pages, 5 figures</comments><report-no>DCS-355-IR</report-no><acm-class>D.1.6; F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A logic program is an executable specification. For example, merge sort in
pure Prolog is a logical formula, yet shows creditable performance on long
linked lists. But such executable specifications are a compromise: the logic is
distorted by algorithmic considerations, yet only indirectly executable via an
abstract machine. This paper introduces relational programming, a method that
solves the difficulty with logic programming by a separation of concerns. It
requires three texts: (1) the axioms, a logical formula that specifies the
problem and is not compromised by algorithmic considerations, (2) the theorem,
a logical formula that expresses the idea of the algorithm and follows from the
axioms, and (3) the code, a transcription of the theorem to a procedural
language. Correctness of the code relies on the logical relationship of the
theorem with the axioms and relies on an accurate transcription of the theorem
to the procedural language. Sorting is an example where relational programming
has the advantage of a higher degree of abstractness: the data to be sorted can
be any data type in C++ (the procedural language we use in our examples) that
satisfies the axioms of linear order, while the pure-Prolog version is limited
to data structures in the form of linked cells. We show another advantage of
relational programs: they have a model-theoretic and fixpoint semantics
equivalent to each other and analogous to those of pure Prolog programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3489</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3489</id><created>2014-12-10</created><updated>2015-05-21</updated><authors><author><keyname>Wiebe</keyname><forenames>Nathan</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Quantum Deep Learning</title><categories>quant-ph cs.LG cs.NE</categories><comments>34 pages, many figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, deep learning has had a profound impact on machine learning
and artificial intelligence. At the same time, algorithms for quantum computers
have been shown to efficiently solve some problems that are intractable on
conventional, classical computers. We show that quantum computing not only
reduces the time required to train a deep restricted Boltzmann machine, but
also provides a richer and more comprehensive framework for deep learning than
classical computing and leads to significant improvements in the optimization
of the underlying objective function. Our quantum methods also permit efficient
training of full Boltzmann machines and multi-layer, fully connected models and
do not have well known classical counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3493</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3493</id><created>2014-12-10</created><authors><author><keyname>Brewster</keyname><forenames>Richard C.</forenames></author><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author></authors><title>Mixing Homomorphisms, Recolourings, and Extending Circular Precolourings</title><categories>math.CO cs.DM</categories><comments>29 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work brings together ideas of mixing graph colourings, discrete
homotopy, and precolouring extension. A particular focus is circular
colourings. We prove that all the $(k,q)$-colourings of a graph $G$ can be
obtained by successively recolouring a single vertex provided $k/q\geq 2col(G)$
along the lines of Cereceda, van den Heuvel and Johnson's result for
$k$-colourings. We give various bounds for such mixing results and discuss
their sharpness, including cases where the bounds for circular and classical
colourings coincide. As a corollary, we obtain an Albertson-type extension
theorem for $(k,q)$-precolourings of circular cliques. Such a result was first
conjectured by Albertson and West. General results on homomorphism mixing are
presented, including a characterization of graphs $G$ for which the
endomorphism monoid can be generated through the mixing process. As in similar
work of Brightwell and Winkler, the concept of dismantlability plays a key
role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3504</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3504</id><created>2014-12-10</created><authors><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author></authors><title>Integrating formal methods into traditional practices for software
  development: an overview</title><categories>cs.SE</categories><comments>Encuentro Nacional de Ciencias de la Computacion: Investigacion y
  Aplicacion de la Ingenieria de Software. Oaxaca, Mexico. ISBN
  978-0-9908236-0-5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows an overview of a research project for integrating formal
methods in popular practices for software development in Mexico. The article
shows only the main results from the survey about methods and practices and an
overview of the initial proposal of practices applying lightweight formal
methods to requirements specification and software modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3506</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3506</id><created>2014-12-10</created><updated>2014-12-17</updated><authors><author><keyname>Alvarez</keyname><forenames>Jose M.</forenames></author><author><keyname>Gevers</keyname><forenames>Theo</forenames></author><author><keyname>Lopez</keyname><forenames>Antonio M.</forenames></author></authors><title>Road Detection by One-Class Color Classification: Dataset and
  Experiments</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting traversable road areas ahead a moving vehicle is a key process for
modern autonomous driving systems. A common approach to road detection consists
of exploiting color features to classify pixels as road or background. These
algorithms reduce the effect of lighting variations and weather conditions by
exploiting the discriminant/invariant properties of different color
representations. Furthermore, the lack of labeled datasets has motivated the
development of algorithms performing on single images based on the assumption
that the bottom part of the image belongs to the road surface.
  In this paper, we first introduce a dataset of road images taken at different
times and in different scenarios using an onboard camera. Then, we devise a
simple online algorithm and conduct an exhaustive evaluation of different
classifiers and the effect of using different color representation to
characterize pixels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3507</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3507</id><created>2014-12-10</created><authors><author><keyname>Azar</keyname><forenames>Yossi</forenames></author><author><keyname>Cohen</keyname><forenames>Ilan Reuven</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>Online Covering with Convex Objectives and Applications</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithmic framework for minimizing general convex objectives
(that are differentiable and monotone non-decreasing) over a set of covering
constraints that arrive online. This substantially extends previous work on
online covering for linear objectives (Alon {\em et al.}, STOC 2003) and online
covering with offline packing constraints (Azar {\em et al.}, SODA 2013). To
the best of our knowledge, this is the first result in online optimization for
generic non-linear objectives; special cases of such objectives have previously
been considered, particularly for energy minimization.
  As a specific problem in this genre, we consider the unrelated machine
scheduling problem with startup costs and arbitrary $\ell_p$ norms on machine
loads (including the surprisingly non-trivial $\ell_1$ norm representing total
machine load). This problem was studied earlier for the makespan norm in both
the offline (Khuller~{\em et al.}, SODA 2010; Li and Khuller, SODA 2011) and
online settings (Azar {\em et al.}, SODA 2013). We adapt the two-phase approach
of obtaining a fractional solution and then rounding it online (used
successfully to many linear objectives) to the non-linear objective. The
fractional algorithm uses ideas from our general framework that we described
above (but does not fit the framework exactly because of non-positive entries
in the constraint matrix). The rounding algorithm uses ideas from offline
rounding of LPs with non-linear objectives (Azar and Epstein, STOC 2005; Kumar
{\em et al.}, FOCS 2005). Our competitive ratio is tight up to a logarithmic
factor. Finally, for the important special case of total load ($\ell_1$ norm),
we give a different rounding algorithm that obtains a better competitive ratio
than the generic rounding algorithm for $\ell_p$ norms. We show that this
competitive ratio is asymptotically tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3508</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3508</id><created>2014-12-10</created><updated>2015-03-03</updated><authors><author><keyname>Sulzbach</keyname><forenames>Henning</forenames></author></authors><title>On martingale tail sums for the path length in random trees</title><categories>math.PR cs.DS</categories><comments>Results generalized to broader tree model; convergence of moments in
  the CLT</comments><msc-class>Primary 60F15, 68P05, secondary 60F05, 60G42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a martingale $(X_n)$ converging almost surely to a random variable $X$,
the sequence $(X_n - X)$ is called martingale tail sum. Recently, Neininger
[Random Structures Algorithms, 46 (2015), 346-361] proved a central limit
theorem for the martingale tail sum of R{\'e}gnier's martingale for the path
length in random binary search trees. Gr{\&quot;u}bel and Kabluchko [2014, preprint,
arXiv 1410.0469] gave an alternative proof also conjecturing a corresponding
law of the iterated logarithm. We prove the central limit theorem with
convergence of higher moments and the law of the iterated logarithm for a
family of trees containing binary search trees, recursive trees and
plane-oriented recursive trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3510</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3510</id><created>2014-12-10</created><authors><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Kluger</keyname><forenames>Yuval</forenames></author><author><keyname>Tygert</keyname><forenames>Mark</forenames></author></authors><title>An implementation of a randomized algorithm for principal component
  analysis</title><categories>stat.CO cs.MS</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed intense development of randomized methods for
low-rank approximation. These methods target principal component analysis (PCA)
and the calculation of truncated singular value decompositions (SVD). The
present paper presents an essentially black-box, fool-proof implementation for
Mathworks' MATLAB, a popular software platform for numerical computation. As
illustrated via several tests, the randomized algorithms for low-rank
approximation outperform or at least match the classical techniques (such as
Lanczos iterations) in basically all respects: accuracy, computational
efficiency (both speed and memory usage), ease-of-use, parallelizability, and
reliability. However, the classical procedures remain the methods of choice for
estimating spectral norms, and are far superior for calculating the least
singular values and corresponding singular vectors (or singular subspaces).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3512</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3512</id><created>2014-12-10</created><authors><author><keyname>Vajnovszki</keyname><forenames>Vincent</forenames></author></authors><title>The equidistribution of some length three vincular patterns on
  $S_n(132)$</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2012 B\'ona showed the rather surprising fact that the cumulative number
of occurrences of the classical patterns $231$ and $213$ are the same on the
set of permutations avoiding $132$, beside the pattern based statistics $231$
and $213$ do not have the same distribution on this set. Here we show that if
it is required for the symbols playing the role of $1$ and $3$ in the
occurrences of $231$ and $213$ to be adjacent, then the obtained statistics are
equidistributed on the set of $132$-avoiding permutations. Actually, expressed
in terms of vincular patterns, we prove the following more general results: the
statistics based on the patterns $b-ca$, $b-ac$ and $ba-c$, together with other
statistics, have the same joint distribution on $S_n(132)$, and so do the
patterns $bc-a$ and $c-ab$; and up to trivial transformations, these statistics
are the only based on length three proper (not classical nor adjacent) vincular
patterns which are equidistributed on a set of permutations avoiding a
classical length three pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3518</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3518</id><created>2014-12-10</created><updated>2015-08-03</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Appropriate Causal Models and the Stability of Causation</title><categories>cs.AI</categories><comments>A preliminary version of this paper appears in the Proceedings of the
  Fourteenth International Conference on Principles of Knowledge Representation
  and Reasoning (KR 2014)}, 2014. To appear, Review of Symbolic Logic</comments><doi>10.1017/S1755020315000246</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal models defined in terms of structural equations have proved to be
quite a powerful way of representing knowledge regarding causality. However, a
number of authors have given examples that seem to show that the Halpern-Pearl
(HP) definition of causality gives intuitively unreasonable answers. Here it is
shown that, for each of these examples, we can give two stories consistent with
the description in the example, such that intuitions regarding causality are
quite different for each story. By adding additional variables, we can
disambiguate the stories. Moreover, in the resulting causal models, the HP
definition of causality gives the intuitively correct answer. It is also shown
that, by adding extra variables, a modification to the original HP definition
made to deal with an example of Hopkins and Pearl may not be necessary. Given
how much can be done by adding extra variables, there might be a concern that
the notion of causality is somewhat unstable. Can adding extra variables in a
&quot;conservative&quot; way (i.e., maintaining all the relations between the variables
in the original model) cause the answer to the question &quot;Is X=x a cause of Y=y&quot;
to alternate between &quot;yes&quot; and &quot;no&quot;? It is shown that we can have such
alternation infinitely often, but if we take normality into consideration, we
cannot. Indeed, under appropriate normality assumptions. adding an extra
variable can change the answer from &quot;yes&quot; to &quot;no&quot;, but after that, it cannot
cannot change back to &quot;yes&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3529</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3529</id><created>2014-12-10</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinrich</forenames></author></authors><title>Towards Logical Architecture and Formal Analysis of Dependencies Between
  Services</title><categories>cs.SE</categories><comments>Preprint, The 2014 Asia-Pacific Services Computing Conference (APSCC
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal approach to modelling and analysis of data and
control flow dependencies between services within remotely deployed distributed
systems of services. Our work aims at elaborating for a concrete system, which
parts of the system (or system model) are necessary to check a given property.
The approach allows services decomposition oriented towards efficient checking
of system properties as well as analysis of dependencies within a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3538</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3538</id><created>2014-12-10</created><authors><author><keyname>Attasena</keyname><forenames>Varunya</forenames><affiliation>ERIC</affiliation></author><author><keyname>Harbi</keyname><forenames>Nouria</forenames><affiliation>ERIC</affiliation></author><author><keyname>Darmont</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>ERIC</affiliation></author></authors><title>fVSS: A New Secure and Cost-Efficient Scheme for Cloud Data Warehouses</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>ACM. 17th International Workshop on Data Warehousing and OLAP
  (DOLAP 2014), Nov 2014, Shangai, China. pp.81-90, Proceedings of the 17th
  International Workshop on Data Warehousing and OLAP</journal-ref><doi>10.1145/2666158.2666173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud business intelligence is an increasingly popular choice to deliver
decision support capabilities via elastic, pay-per-use resources. However, data
security issues are one of the top concerns when dealing with sensitive data.
In this pa-per, we propose a novel approach for securing cloud data warehouses
by flexible verifiable secret sharing, fVSS. Secret sharing encrypts and
distributes data over several cloud ser-vice providers, thus enforcing data
privacy and availability. fVSS addresses four shortcomings in existing secret
sharing-based approaches. First, it allows refreshing the data ware-house when
some service providers fail. Second, it allows on-line analysis processing.
Third, it enforces data integrity with the help of both inner and outer
signatures. Fourth, it helps users control the cost of cloud warehousing by
balanc-ing the load among service providers with respect to their pricing
policies. To illustrate fVSS' efficiency, we thoroughly compare it with
existing secret sharing-based approaches with respect to security features,
querying power and data storage and computing costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3555</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3555</id><created>2014-12-11</created><authors><author><keyname>Chung</keyname><forenames>Junyoung</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Cho</keyname><forenames>KyungHyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling</title><categories>cs.NE cs.LG</categories><comments>Presented in NIPS 2014 Deep Learning and Representation Learning
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3570</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3570</id><created>2014-12-11</created><updated>2016-01-29</updated><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author></authors><title>Bounded-degree factors of lacunary multivariate polynomials</title><categories>cs.SC cs.CC cs.DS</categories><comments>31 pages; Long version of arXiv:1401.4720 with simplified proofs</comments><journal-ref>Journal of Symbolic Computation 75, pages 171-192, 2016</journal-ref><doi>10.1016/j.jsc.2015.11.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new method for computing bounded-degree factors
of lacunary multivariate polynomials. In particular for polynomials over number
fields, we give a new algorithm that takes as input a multivariate polynomial f
in lacunary representation and a degree bound d and computes the irreducible
factors of degree at most d of f in time polynomial in the lacunary size of f
and in d. Our algorithm, which is valid for any field of zero characteristic,
is based on a new gap theorem that enables reducing the problem to several
instances of (a) the univariate case and (b) low-degree multivariate
factorization.
  The reduction algorithms we propose are elementary in that they only
manipulate the exponent vectors of the input polynomial. The proof of
correctness and the complexity bounds rely on the Newton polytope of the
polynomial, where the underlying valued field consists of Puiseux series in a
single variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3579</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3579</id><created>2014-12-11</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Personal Multi-threading</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-threading allows agents to pursue a heterogeneous collection of tasks
in an orderly manner. The view of multi-threading that emerges from thread
algebra is applied to the case where a single agent, who may be human,
maintains a hierarchical multithread as an architecture of its own activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3588</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3588</id><created>2014-12-11</created><updated>2014-12-14</updated><authors><author><keyname>Khan</keyname><forenames>Muhammad Taimoor</forenames></author><author><keyname>Serpanos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shrobe</keyname><forenames>Howard</forenames></author></authors><title>On the Formal Semantics of the Cognitive Middleware AWDRAT</title><categories>cs.PL cs.LO</categories><comments>Technical report (submitted) to CSAIL, MIT, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this work is two fold: on one hand we want to formalize the
behavior of critical components of the self generating and adapting cognitive
middleware AWDRAT such that the formalism not only helps to understand the
semantics and technical details of the middleware but also opens an opportunity
to extend the middleware to support other complex application domains of
cybersecurity; on the other hand, the formalism serves as a pre-requisite for
our proof of the behavioral correctness of the critical components to ensure
the safety of the middleware itself. However, here we focus only on the core
and critical component of the middleware, i.e. Execution Monitor which is a
part of the module &quot;Architectural Differencer&quot; of AWDRAT. The role of the
execution monitor is to identify inconsistencies between runtime observations
of the target system and predictions of the System Architectural Model.
Therefore, to achieve this goal, we first define the formal (denotational)
semantics of the observations (runtime events) and predictions (executable
specifications as of System Architectural Model); then based on the
aforementioned formal semantices, we formalize the behavior of the &quot;Execution
Monitor&quot; of the middleware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3594</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3594</id><created>2014-12-11</created><updated>2015-06-20</updated><authors><author><keyname>Hiltunen</keyname><forenames>Sonja</forenames></author><author><keyname>Loubaton</keyname><forenames>Philippe</forenames></author><author><keyname>Chevalier</keyname><forenames>Pascal</forenames></author></authors><title>Large system analysis of a GLRT for detection with large sensor arrays
  in temporally white noise</title><categories>cs.IT math.IT</categories><comments>18 pages, 9 figures</comments><msc-class>60G35</msc-class><doi>10.1109/TSP.2015.2452220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the behaviour of a classical multi-antenna GLRT test
that allows to detect the presence of a known signal corrupted by a multi-path
propagation channel and by an additive white Gaussian noise with unknown
spatial covariance matrix. The paper is focused on the case where the number of
sensors M is large, and of the same order of magnitude as the sample size N, a
context which is modeled by the large system asymptotic regime M goes to
infinity, N goes to infinity in such a way that M/N goes to c for c in (0,
infinity). The purpose of this paper is to study the behaviour of a GLRT test
statistics in this regime, and to show that the corresponding theoretical
analysis allows to accurately predict the performance of the test when M and N
are of the same order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3596</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3596</id><created>2014-12-11</created><updated>2015-04-27</updated><authors><author><keyname>Poleg</keyname><forenames>Yair</forenames></author><author><keyname>Halperin</keyname><forenames>Tavi</forenames></author><author><keyname>Arora</keyname><forenames>Chetan</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author></authors><title>EgoSampling: Fast-Forward and Stereo for Egocentric Videos</title><categories>cs.CV cs.MM</categories><comments>in IEEE CVPR 2015, Boston, MA, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While egocentric cameras like GoPro are gaining popularity, the videos they
capture are long, boring, and difficult to watch from start to end. Fast
forwarding (i.e. frame sampling) is a natural choice for faster video browsing.
However, this accentuates the shake caused by natural head motion, making the
fast forwarded video useless.
  We propose EgoSampling, an adaptive frame sampling that gives more stable
fast forwarded videos. Adaptive frame sampling is formulated as energy
minimization, whose optimal solution can be found in polynomial time.
  In addition, egocentric video taken while walking suffers from the left-right
movement of the head as the body weight shifts from one leg to another. We turn
this drawback into a feature: Stereo video can be created by sampling the
frames from the left most and right most head positions of each step, forming
approximate stereo-pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3610</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3610</id><created>2014-12-11</created><authors><author><keyname>Davtyan</keyname><forenames>Narine N.</forenames></author><author><keyname>Kamalian</keyname><forenames>Rafayel R.</forenames></author></authors><title>A necessary and sufficient condition for a graph $G$, which satisfies
  the equality $\mu_{21}(G)=|V(G)|$</title><categories>cs.DM</categories><msc-class>05C15, 05C78</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A necessary and sufficient condition is found for a graph $G$, which
satisfies the equality $\mu_{21}(G)=|V(G)|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3613</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3613</id><created>2014-12-11</created><updated>2015-10-15</updated><authors><author><keyname>Xenaki</keyname><forenames>Spyridoula D.</forenames></author><author><keyname>Koutroumbas</keyname><forenames>Konstantinos D.</forenames></author><author><keyname>Rontogiannis</keyname><forenames>Athanasios A.</forenames></author></authors><title>A Novel Adaptive Possibilistic Clustering Algorithm</title><categories>cs.CV</categories><doi>10.1109/TFUZZ.2015.2486806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel possibilistic c-means clustering algorithm, called
Adaptive Possibilistic c-means, is presented. Its main feature is that {\it
all} its parameters, after their initialization, are properly adapted during
its execution. Provided that the algorithm starts with a reasonable
overestimate of the number of physical clusters formed by the data, it is
capable, in principle, to unravel them (a long-standing issue in the clustering
literature). This is due to the fully adaptive nature of the proposed algorithm
that enables the removal of the clusters that gradually become obsolete. In
addition, the adaptation of all its parameters increases the flexibility of the
algorithm in following the variations in the formation of the clusters that
occur from iteration to iteration. Theoretical results that are indicative of
the convergence behavior of the algorithm are also provided. Finally, extensive
simulation results on both synthetic and real data highlight the effectiveness
of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3624</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3624</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Uddin</keyname><forenames>Muhammad Shahin</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Dynamic Channel Allocation for Class-Based QoS Provisioning and Call
  Admission in Visible Light Communication</title><categories>cs.NI cs.PF</categories><journal-ref>The Arabian Journal for Science and Engineering. vol. 39, no. 2,
  pp. 1007-1016, Feb. 2014</journal-ref><doi>10.1007/s13369-013-0680-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provisioning of quality of service (QoS) is a key issue in visible light
communication (VLC) system as well as in other wireless communication systems.
Due to the fact that QoS requirements are not as strict for all traffic types,
more calls of higher priority traffic classes can be accommodated by blocking
some more calls of lower priority traffic classes. Diverse types of high data
rate traffic are supported by existing wireless communication systems while the
resource is limited. Hence, priority based resource allocation can ensure the
service quality for the calls of important traffic class. The fixed guard
channels to prioritize any class of calls always reduce the channel
utilization. In this paper we propose a priority based dynamic channel
reservation scheme for higher priority calls that does not reduce the channel
utilization significantly. The number of reserved channels for each of the
individual traffic classes is calculated using real-time observation of the
call arrival rates of all the traffic classes. The features of the scheme allow
reduction of the call blocking probability of higher priority calls along with
the increase of the channel utilization. The proposed Markov Chain model is
expected to be very much effective for the queuing analysis especially for the
priority scheme of any number of traffic classes. The numerical results show
that the proposed scheme is able to attain reasonable call blocking probability
of higher priority calls without sacrificing channel utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3625</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3625</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Class-Based Service Connectivity using Multi-Level Bandwidth Adaptation
  in Multimedia Wireless Networks</title><categories>cs.NI cs.MM cs.PF</categories><comments>Journal paper</comments><journal-ref>Wireless Personal Communications, vol. 77, no 4, pp. 2735-2745,
  August 2014</journal-ref><doi>10.1007/s11277-014-1665-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the fact that quality of service requirements are not very strict for
all traffic types, more calls of higher priority can be accommodated by
reducing some bandwidth allocation for the bandwidth adaptive calls. The
bandwidth adaptation to accept a higher priority call is more than that of a
lower priority call. Therefore, the multi-level bandwidth adaptation technique
improves the overall forced call termination probability as well as provides
priority of the traffic classes in terms of call blocking probability without
reducing the bandwidth utilization. We propose a novel bandwidth adaptation
model that releases multi-level of bandwidth from the existing multimedia
traffic calls. The amount of released bandwidth is decided based on the
priority of the requesting traffic calls and the number of existing bandwidth
adaptive calls. This prioritization of traffic classes does not reduce the
bandwidth utilization. Moreover, our scheme reduces the overall forced call
termination probability significantly. The proposed scheme is modeled using the
Markov Chain. The numerical results show that the proposed scheme is able to
provide negligible handover call dropping probability as well as significantly
reduced new call blocking probability of higher priority calls without
increasing the overall forced call termination probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3628</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3628</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Nguyen</keyname><forenames>Tuan</forenames></author><author><keyname>Kim</keyname><forenames>Young-Il</forenames></author><author><keyname>Ryu</keyname><forenames>Won</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Radio Resource Allocation for Scalable Video Services over Wireless
  Cellular Networks</title><categories>cs.MM cs.NI</categories><journal-ref>Wireless Personal Communications, vol. 74, no 3, pp. 1061-1079,
  Feb. 2014</journal-ref><doi>10.1007/s11277-013-1344-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good quality video services always require higher bandwidth. Hence, to
provide the video services e.g., multicast/broadcast services (MBS) and unicast
services along with the existing voice, internet, and other background traffic
services over the wireless cellular networks, it is required to efficiently
manage the wireless resources in order to reduce the overall forced call
termination probability, to maximize the overall service quality, and to
maximize the revenue. Fixed bandwidth allocation for the MBS sessions either
reduces the quality of the MBS videos and bandwidth utilization or increases
the overall forced call termination probability and of course the handover call
dropping probability as well. Scalable Video Coding (SVC) technique allows the
variable bit rate allocation for the video services. In this paper, we propose
a bandwidth allocation scheme that efficiently allocates bandwidth among the
MBS sessions and the non-MBS traffic calls (e.g., voice, unicast, internet, and
other background traffic). The proposed scheme reduces the bandwidth allocation
for the MBS sessions during the congested traffic condition only to accommodate
more calls in the system. Instead of allocating fixed bandwidths for the BMS
sessions and the non-MBS traffic, our scheme allocates variable bandwidths for
them. However, the minimum quality of the videos is guaranteed by allocating
minimum bandwidth for them. Using the mathematical and numerical analyses, we
show that the proposed scheme maximizes the bandwidth utilization and
significantly reduces the overall forced call termination probability as well
as the handover call dropping probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3630</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3630</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author></authors><title>Call Admission Control based on Adaptive Bandwidth Allocation for
  Wireless Networks</title><categories>cs.MM cs.NI</categories><journal-ref>IEEE/KICS Journal of Communications and Networks (JCN). vol. 15,
  no. 1, pp. 15-24, Feb. 2013</journal-ref><doi>10.1109/JCN.2013.000005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provisioning of Quality of Service (QoS) is a key issue in any multi-media
system. However, in wireless systems, supporting QoS requirements of different
traffic types is more challenging due to the need to minimize two performance
metrics - the probability of dropping a handover call and the probability of
blocking a new call. Since QoS requirements are not as stringent for
non-real-time traffic types, as opposed to real-time traffic, more calls can be
accommodated by releasing some bandwidth from the already admitted
non-real-time traffic calls. If we require that such a released bandwidth to
accept a handover call ought to be larger than the bandwidth to accept a new
call, then the resulting probability of dropping a handover call will be
smaller than the probability of blocking a new call. In this paper we propose
an efficient Call Admission Control (CAC) that relies on adaptive multi-level
bandwidth-allocation scheme for non-real-time calls. The scheme allows
reduction of the call dropping probability along with increase of the bandwidth
utilization. The numerical results show that the proposed scheme is capable of
attaining negligible handover call dropping probability without sacrificing
bandwidth utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3633</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3633</id><created>2014-12-11</created><updated>2015-05-04</updated><authors><author><keyname>Triska</keyname><forenames>Jan</forenames></author><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Logic of temporal attribute implications</title><categories>cs.LO cs.AI cs.DB</categories><msc-class>68T27, 68T30, 03B44</msc-class><acm-class>H.2.8; H.3.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study logic for reasoning with if-then formulas describing dependencies
between attributes of objects which are observed in consecutive points in time.
We introduce semantic entailment of the formulas, show its fixed-point
characterization, investigate closure properties of model classes, present an
axiomatization and prove its completeness, and investigate alternative
axiomatizations and normalized proofs. We investigate decidability and
complexity issues of the logic and prove that the entailment problem is NP-hard
and belongs to EXPSPACE. We show that by restricting to predictive formulas,
the entailment problem is decidable in pseudo-linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3635</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3635</id><created>2014-12-11</created><authors><author><keyname>Schuld</keyname><forenames>Maria</forenames></author><author><keyname>Sinayskiy</keyname><forenames>Ilya</forenames></author><author><keyname>Petruccione</keyname><forenames>Francesco</forenames></author></authors><title>Simulating a perceptron on a quantum computer</title><categories>quant-ph cs.LG cs.NE</categories><comments>11 pages, 6 figures, accepted by Physics Letters A</comments><journal-ref>Physics Letters A, 379, pp. 660-663 (2015)</journal-ref><doi>10.1016/j.physleta.2014.11.061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perceptrons are the basic computational unit of artificial neural networks,
as they model the activation mechanism of an output neuron due to incoming
signals from its neighbours. As linear classifiers, they play an important role
in the foundations of machine learning. In the context of the emerging field of
quantum machine learning, several attempts have been made to develop a
corresponding unit using quantum information theory. Based on the quantum phase
estimation algorithm, this paper introduces a quantum perceptron model
imitating the step-activation function of a classical perceptron. This scheme
requires resources in $\mathcal{O}(n)$ (where $n$ is the size of the input) and
promises efficient applications for more complex structures such as trainable
quantum neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3637</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3637</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Handover Management in Highly Dense Femtocellular Networks</title><categories>cs.NI</categories><journal-ref>EURASIP Journal on Wireless Communications and Networking, pp.
  1-21, Jan. 2013</journal-ref><doi>10.1186/1687-1499-2013-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For dense femtocells, intelligent integrated femtocell/macrocell network
architecture, a neighbor cell list with a minimum number of femtocells,
effective call admission control (CAC), and handover processes with proper
signaling are the open research issues. An appropriate traffic model for the
integrated femtocell/macrocell network is also not yet developed. In this
paper, we present the major issue of mobility management for the integrated
femtocell/macrocell network. We propose a novel algorithm to create a neighbor
cell list with a minimum, but appropriate, number of cells for handover. We
also propose detailed handover procedures and a novel traffic model for the
integrated femtocell/macrocell network. The proposed CAC effectively handles
various calls. The numerical and simulation results show the importance of the
integrated femtocell/macrocell network and the performance improvement of the
proposed schemes. Our proposed schemes for dense femtocells will be very
effective for those in research and industry to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3639</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3639</id><created>2014-12-11</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author></authors><title>Cost-Effective Frequency Planning for Capacity Enhancement of
  Femtocellular Networks</title><categories>cs.NI</categories><journal-ref>Wireless Personal Communications, vol. 60, no. 1, pp. 83-104,
  Sept. 2011</journal-ref><doi>10.1007/s11277-011-0258-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocellular networks will co-exist with macrocellular networks, mitigation
of the interference between these two network types is a key challenge for
successful integration of these two technologies. In particular, there are
several interference mechanisms between the femtocellular and the macrocellular
networks, and the effects of the resulting interference depend on the density
of femtocells and the overlaid macrocells in a particular coverage area. While
improper interference management can cause a significant reduction in the
system capacity and can increase the outage probability, effective and
efficient frequency allocation among femtocells and macrocells can result in a
successful co-existence of these two technologies. Furthermore, highly dense
femtocellular deployments the ultimate goal of the femtocellular technology
will require significant degree of self-organization in lieu of manual
configuration. In this paper, we present various femtocellular network
deployment scenarios, and we propose a number of frequency-allocation schemes
to mitigate the interference and to increases the spectral efficiency of the
integrated network. These schemes include: shared frequency band, dedicated
frequency band, sub-frequency band, static frequency-reuse, and dynamic
frequency-reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3644</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3644</id><created>2014-12-11</created><updated>2015-03-19</updated><authors><author><keyname>Feng</keyname><forenames>Shiguang</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Quaas</keyname><forenames>Karin</forenames></author></authors><title>Path-Checking for MTL and TPTL over Data Words</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise complexity results are derived for the model checking problems for
MTL (metric temporal logic) and TPTL (timed propositional temporal logic) on
(in)finite data words and deterministic one-counter machines. Depending on the
number of register variables and the encoding of numbers in constraints (unary
or binary), the complexity is either P-complete or PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3664</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3664</id><created>2014-12-11</created><updated>2015-01-26</updated><authors><author><keyname>Angiulli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Argento</keyname><forenames>Luciano</forenames></author><author><keyname>Furfaro</keyname><forenames>Angelo</forenames></author></authors><title>PCkAD: an unsupervised intrusion detection technique exploiting within
  payload n-gram location distribution</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signature-based and protocol-based intrusion detection systems (IDS) are
employed as means to reveal content-based network attacks. Such systems have
proven to be effective in identifying known intrusion attempts and exploits but
they fail to recognize new types of attacks or carefully crafted variants of
well known ones. This paper presents the design and the development of an
anomaly-based IDS technique which is able to detect content-based attacks
carried out over application level protocols, like HTTP and FTP. In order to
identify anomalous packets, the payload is split up in chunks of equal length
and the n-gram technique is used to learn which byte sequences usually appear
in each chunk. The devised technique builds a different model for each pair of
protocol of interest and packet length, in terms of number of chunks, and use
them to classify the incoming traffic. Models are build by means of an
unsupervised approach. Experimental results witness that the technique achieves
an excellent accuracy with a very low false positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3670</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3670</id><created>2014-12-09</created><authors><author><keyname>Bhave</keyname><forenames>Devendra</forenames></author><author><keyname>Jha</keyname><forenames>Sagar</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Bounded-Rate Multi-Mode Systems Based Motion Planning</title><categories>cs.LO</categories><comments>14 pages, 12 figures, HSCC - 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded-rate multi-mode systems are hybrid systems that can switch among a
finite set of modes. Its dynamics is specified by a finite number of
real-valued variables with mode-dependent rates that can vary within given
bounded sets. Given an arbitrary piecewise linear trajectory, we study the
problem of following the trajectory with arbitrary precision, using motion
primitives given as bounded-rate multi-mode systems. We give an algorithm to
solve the problem and show that the problem is co-NP complete. We further prove
that the problem can be solved in polynomial time for multi-mode systems with
fixed dimension. We study the problem with dwell-time requirement and show the
decidability of the problem under certain positivity restriction on the rate
vectors. Finally, we show that introducing structure to the multi-mode systems
leads to undecidability, even when using only a single clock variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3684</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3684</id><created>2014-12-10</created><authors><author><keyname>Goyal</keyname><forenames>Soren</forenames></author><author><keyname>Benjamin</keyname><forenames>Paul</forenames></author></authors><title>Object Recognition Using Deep Neural Networks: A Survey</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of objects using Deep Neural Networks is an active area of
research and many breakthroughs have been made in the last few years. The paper
attempts to indicate how far this field has progressed. The paper briefly
describes the history of research in Neural Networks and describe several of
the recent advances in this field. The performances of recently developed
Neural Network Algorithm over benchmark datasets have been tabulated. Finally,
some the applications of this field have been provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3687</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3687</id><created>2014-12-09</created><authors><author><keyname>Deleuze</keyname><forenames>Gilles</forenames><affiliation>CRAN</affiliation></author><author><keyname>Brinzei</keyname><forenames>Nicolae</forenames><affiliation>CRAN</affiliation></author><author><keyname>Villaume</keyname><forenames>Nicolas</forenames></author></authors><title>Modelling common cause failures of large digital I&amp;C systems with
  coloured Petri nets</title><categories>cs.SE cs.PF</categories><comments>in French, 19\`eme Congr\`es de Ma\^itrise des Risques et S\^uret\'e
  de Fonctionnement, Lambda-Mu'2014, Oct 2014, Dijon, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is the representation of Common Cause Failures
(CCF) in large digital systems. The system under study is representative of a
control system of a nuclear plant. The model for CCF is the generalized Atwood
model. It can represent independent failures, CCF non-lethal for some system
elements and CCF lethal to all. The Atwood model was modified to &quot;direct&quot;
non-lethal DCC on certain parts of the system and take into account the
different possible origins of DCC. Maintenance and repairs are taken into
account in the model that is thus dynamic. The main evaluation results are
probabilistic, the considered indicator is the probability of failure on demand
(PFD). A comparison is made between the estimator of the PFD taking into
account all the failures and the estimator taking into account only the
detected failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3688</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3688</id><created>2014-12-11</created><updated>2014-12-17</updated><authors><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author></authors><title>Run-Length Encoded Nondeterministic KMP and Suffix Automata</title><categories>cs.FL cs.DS</categories><comments>Various fixes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel bit-parallel representation, based on the run-length
encoding, of the nondeterministic KMP and suffix automata for a string $P$ with
at least two distinct symbols. Our method is targeted to the case of long
strings over small alphabets and complements the method of Cantone et al.
(2012), which is effective for long strings over large alphabets. Our encoding
requires $O((\sigma + m)\lceil \rho / w\rceil)$ space and allows one to
simulate the automata on a string in time $O(\lceil \rho / w\rceil)$ per
transition, where $\sigma$ is the alphabet size, $m$ is the length of $P$,
$\rho$ is the length of the run-length encoding of $P$ and $w$ is the machine
word size in bits. The input string can be given in either unencoded or
run-length encoded form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3690</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3690</id><created>2014-12-08</created><authors><author><keyname>Marinho</keyname><forenames>Marcelo</forenames></author><author><keyname>Sampaio</keyname><forenames>Suzana</forenames></author><author><keyname>Lima</keyname><forenames>Telma</forenames></author><author><keyname>Moura</keyname><forenames>Hermano</forenames></author></authors><title>A Systematic Review of Uncertainties in Software Project Management</title><categories>cs.SE</categories><comments>21 pages. arXiv admin note: substantial text overlap with
  arXiv:1411.1920</comments><acm-class>D.2.9</acm-class><journal-ref>International Journal of Software Engineering and Applications V5
  - N6 pag1 2014</journal-ref><doi>10.5121/ijsea.2014.5601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is no secret that many projects fail, regardless of the business sector,
software projects are notoriously disaster victims, not necessarily because of
technological failure, but more often due to their uncertainties. The threats
identified by uncertainty in day-to-day of a project are real and immediate and
the stakes in a project are often high. This paper presents a systematic review
about software project management uncertainties. It helps to identify the
difficulties and the actions that can minimize the uncertainties effects in the
projects and how managers and teams can prepare themselves for the challenges
of their projects scenario, with the aim of contributing to the improvement of
project management in organizations as well as contributing to project success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3692</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3692</id><created>2014-12-06</created><authors><author><keyname>Melnik</keyname><forenames>S. S.</forenames></author><author><keyname>Usatenko</keyname><forenames>O. V.</forenames></author></authors><title>Entropy and long-range correlations in random symbolic sequences</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT physics.data-an</categories><comments>9 pages, 3 fifures. arXiv admin note: substantial text overlap with
  arXiv:1411.2761</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to develop an estimate for the entropy of random
long-range correlated symbolic sequences with elements belonging to a finite
alphabet. As a plausible model, we use the high-order additive stationary
ergodic Markov chain. Supposing that the correlations between random elements
of the chain are weak we express the differential entropy of the sequence by
means of the symbolic pair correlation function. We also examine an algorithm
for estimating the differential entropy of finite symbolic sequences. We show
that the entropy contains two contributions, the correlation and fluctuation
ones. The obtained analytical results are used for numerical evaluation of the
entropy of written English texts and DNA nucleotide sequences. The developed
theory opens the way for constructing a more consistent and sophisticated
approach to describe the systems with strong short- and weak long-range
correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3695</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3695</id><created>2014-12-10</created><authors><author><keyname>Sefidari</keyname><forenames>Maria</forenames></author><author><keyname>Ortega</keyname><forenames>Felipe</forenames></author></authors><title>Evaluating arbitration and conflict resolution mechanisms in the Spanish
  Wikipedia</title><categories>cs.CY</categories><comments>4 pages, 4 tables</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In open collaborative projects like Wikipedia, interactions among users can
produce tension and misunderstandings. Complex disputes require more
sophisticated mechanisms of conflict resolution. In this paper, we examine the
case of the Spanish Wikipedia and its Arbitration Committee, known as CRC, over
its two years of activity. We postulate that the high percentage of rejections
of cases presented by non-administrators, the lack of diversity inside the
committee (composed only by administrators), and the high number of cases
involving administrators played a central role in its eventual downfall. We
conclude that mechanisms that fail to acknowledge the ecosystem they are part
of cannot succceed. Therefore, further research is needed to determine if
granting more decision-making power to non-administrators may lead to more
effective conflict resolution mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3696</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3696</id><created>2014-12-11</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>Covering Problems for Partial Words and for Indeterminate Strings</title><categories>cs.DS</categories><comments>full version (simplified and corrected); preliminary version appeared
  at ISAAC 2014; 14 pages, 4 figures</comments><msc-class>68W32 (Primary), 68Q25 (Secondary)</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a shortest solid cover of an
indeterminate string. An indeterminate string may contain non-solid symbols,
each of which specifies a subset of the alphabet that could be present at the
corresponding position. We also consider covering partial words, which are a
special case of indeterminate strings where each non-solid symbol is a don't
care symbol. We prove that indeterminate string covering problem and partial
word covering problem are NP-complete for binary alphabet and show that both
problems are fixed-parameter tractable with respect to $k$, the number of
non-solid symbols. For the indeterminate string covering problem we obtain a
$2^{O(k \log k)} + n k^{O(1)}$-time algorithm. For the partial word covering
problem we obtain a $2^{O(\sqrt{k}\log k)} + nk^{O(1)}$-time algorithm. We
prove that, unless the Exponential Time Hypothesis is false, no
$2^{o(\sqrt{k})} n^{O(1)}$-time solution exists for either problem, which shows
that our algorithm for this case is close to optimal. We also present an
algorithm for both problems which is feasible in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3697</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3697</id><created>2014-12-10</created><authors><author><keyname>Fiasconaro</keyname><forenames>A.</forenames></author><author><keyname>Tumminello</keyname><forenames>M.</forenames></author><author><keyname>Nicosia</keyname><forenames>V.</forenames></author><author><keyname>Latora</keyname><forenames>V.</forenames></author><author><keyname>Mantegna</keyname><forenames>R. N.</forenames></author></authors><title>Hybrid recommendation methods in complex networks</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>9 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose here two new recommendation methods, based on the appropriate
normalization of already existing similarity measures, and on the convex
combination of the recommendation scores derived from similarity between users
and between objects. We validate the proposed measures on three relevant data
sets, and we compare their performance with several recommendation systems
recently proposed in the literature. We show that the proposed similarity
measures allow to attain an improvement of performances of up to 20\% with
respect to existing non-parametric methods, and that the accuracy of a
recommendation can vary widely from one specific bipartite network to another,
which suggests that a careful choice of the most suitable method is highly
relevant for an effective recommendation on a given system. Finally, we studied
how an increasing presence of random links in the network affects the
recommendation scores, and we found that one of the two recommendation
algorithms introduced here can systematically outperform the others in noisy
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3701</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3701</id><created>2014-12-11</created><updated>2015-10-29</updated><authors><author><keyname>Klein</keyname><forenames>Felix</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>How Much Lookahead is Needed to Win Infinite Games?</title><categories>cs.GT cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay games are two-player games of infinite duration in which one player may
delay her moves to obtain a lookahead on her opponent's moves. For
omega-regular winning conditions, it is known that such games can be solved in
doubly-exponential time and that doubly-exponential lookahead is sufficient.
  We improve upon both results by giving an exponential time algorithm and an
exponential upper bound on the necessary lookahead. This is complemented by
showing EXPTIME-hardness of the solution problem and by tight exponential lower
bounds on the lookahead. Both lower bounds already hold for safety conditions.
Furthermore, solving delay games with reachability conditions is shown to be
PSPACE complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3705</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3705</id><created>2014-12-11</created><updated>2015-01-25</updated><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>A Topic Modeling Approach to Ranking</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a topic modeling approach to the prediction of preferences in
pairwise comparisons. We develop a new generative model for pairwise
comparisons that accounts for multiple shared latent rankings that are
prevalent in a population of users. This new model also captures inconsistent
user behavior in a natural way. We show how the estimation of latent rankings
in the new generative model can be formally reduced to the estimation of topics
in a statistically equivalent topic modeling problem. We leverage recent
advances in the topic modeling literature to develop an algorithm that can
learn shared latent rankings with provable consistency as well as sample and
computational complexity guarantees. We demonstrate that the new approach is
empirically competitive with the current state-of-the-art approaches in
predicting preferences on some semi-synthetic and real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3708</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3708</id><created>2014-12-11</created><updated>2015-04-08</updated><authors><author><keyname>Goessling</keyname><forenames>Marc</forenames></author><author><keyname>Amit</keyname><forenames>Yali</forenames></author></authors><title>Compact Part-Based Image Representations</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning compact, interpretable image representations is a very natural task
which has not been solved satisfactorily even for simple classes of binary
images. In this paper, we review various ways of composing parts (or experts)
for binary data and argue that competitive forms of interaction are best suited
to learn low-dimensional representations. We propose a new composition rule
which discourages parts from focusing on similar structures and which penalizes
opposing votes strongly so that abstaining from voting becomes more attractive.
We also introduce a novel sequential initialization procedure based on a
process of oversimplification and correction. Experiments show that with our
approach very intuitive models can be learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3709</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3709</id><created>2014-12-11</created><updated>2015-04-14</updated><authors><author><keyname>Gonzalez-Garcia</keyname><forenames>Abel</forenames></author><author><keyname>Vezhnevets</keyname><forenames>Alexander</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>An active search strategy for efficient object class detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object class detectors typically apply a window classifier to all the windows
in a large set, either in a sliding window manner or using object proposals. In
this paper, we develop an active search strategy that sequentially chooses the
next window to evaluate based on all the information gathered before. This
results in a substantial reduction in the number of classifier evaluations and
in a more elegant approach in general. Our search strategy is guided by two
forces. First, we exploit context as the statistical relation between the
appearance of a window and its location relative to the object, as observed in
the training set. This enables to jump across distant regions in the image
(e.g. observing a sky region suggests that cars might be far below) and is done
efficiently in a Random Forest framework. Second, we exploit the score of the
classifier to attract the search to promising areas surrounding a highly scored
window, and to keep away from areas near low scored ones. Our search strategy
can be applied on top of any classifier as it treats it as a black-box. In
experiments with R-CNN on the challenging SUN2012 dataset, our method matches
the detection accuracy of evaluating all windows independently, while
evaluating 9x fewer windows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3714</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3714</id><created>2014-12-11</created><updated>2014-12-12</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author></authors><title>Feature Weight Tuning for Recursive Neural Networks</title><categories>cs.NE cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform &quot;weight tuning&quot; for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3717</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3717</id><created>2014-11-18</created><updated>2015-04-10</updated><authors><author><keyname>Efremova</keyname><forenames>Natalia</forenames></author><author><keyname>Tarasenko</keyname><forenames>Sergey</forenames></author></authors><title>Unsupervised Neural Architecture for Saliency Detection: Extended
  Version</title><categories>cs.CV cs.NE</categories><comments>10 pages, 26 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel neural network architecture for visual saliency
detections, which utilizes neurophysiologically plausible mechanisms for
extraction of salient regions. The model has been significantly inspired by
recent findings from neurophysiology and aimed to simulate the bottom-up
processes of human selective attention. Two types of features were analyzed:
color and direction of maximum variance. The mechanism we employ for processing
those features is PCA, implemented by means of normalized Hebbian learning and
the waves of spikes. To evaluate performance of our model we have conducted
psychological experiment. Comparison of simulation results with those of
experiment indicates good performance of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3721</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3721</id><created>2014-12-11</created><authors><author><keyname>Saharoy</keyname><forenames>Debjyoti</forenames></author><author><keyname>Sen</keyname><forenames>Sandeep</forenames></author></authors><title>Approximation Algorithms for Budget Constrained Network Upgradeable
  Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study budget constrained network upgradeable problems. We are given an
undirected edge weighted graph $G=(V,E)$ where the weight an edge $e \in E$ can
be upgraded for a cost $c(e)$. Given a budget $B$ for improvement, the goal is
to find a subset of edges to be upgraded so that the resulting network is
optimum for $B$. The results obtained in this paper include the following.
  Maximum Weight Constrained Spanning Tree
  We present a randomized algorithm for the problem of weight upgradeable
budget constrained maximum spanning tree on a general graph. This returns a
spanning tree $\mathcal{T}^{'}$ which is feasible within the budget $B$, such
that $\Pr [ l(\mathcal{T}^{'}) \geq (1-\epsilon)\text{OPT}\text{ , }
c(\mathcal{T}^{'} ) \leq B] \ge 1-\frac{1}{n}$ (where $l$ and $c$ denote the
length and cost of the tree respectively), for any fixed $\epsilon &gt;0$, in time
polynomial in $|V|=n$, $|E|=m$. Our results extend to the minimization version
also. Previously Krumke et. al. \cite{krumke} presented a$(1+\frac{1}{\gamma},
1+ \gamma)$ bicriteria approximation algorithm for any fixed $\gamma &gt;0$ for
this problem in general graphs for a more general cost upgrade function. The
result in this paper improves their 0/1 cost upgrade model.
  Longest Path in a DAG We consider the problem of weight improvable longest
path in a $n$ vertex DAG and give a $O(n^3)$ algorithm for the problem when
there is a bound on the number of improvements allowed. We also give a
$(1-\epsilon)$-approximation which runs in $O(\frac{n^4}{\epsilon})$ time for
the budget constrained version. Similar results can be achieved also for the
problem of shortest paths in a DAG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3726</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3726</id><created>2014-12-11</created><updated>2014-12-12</updated><authors><author><keyname>Parsai</keyname><forenames>Ali</forenames></author><author><keyname>Soetens</keyname><forenames>Quinten David</forenames></author><author><keyname>Murgia</keyname><forenames>Alessandro</forenames></author><author><keyname>Demeyer</keyname><forenames>Serge</forenames></author></authors><title>Considering Polymorphism in Change-Based Test Suite Reduction</title><categories>cs.SE</categories><comments>The final publication is available at link.springer.com</comments><journal-ref>Lecture Notes in Business Information Processing, 2014, vol. 199,
  Agile Methods. Large-Scale Development, Refactoring, Testing, and Estimation,
  pp 166-181, Springer International Publishing</journal-ref><doi>10.1007/978-3-319-14358-3_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of continuous integration, algorithms for
selecting the minimal test-suite to cover a given set of changes are in order.
This paper reports on how polymorphism can handle false negatives in a previous
algorithm which uses method-level changes in the base-code to deduce which
tests need to be rerun. We compare the approach with and without polymorphism
on two distinct cases ---PMD and CruiseControl--- and discovered an interesting
trade-off: incorporating polymorphism results in more relevant tests to be
included in the test suite (hence improves accuracy), however comes at the cost
of a larger test suite (hence increases the time to run the minimal
test-suite).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3729</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3729</id><created>2014-12-10</created><authors><author><keyname>Payet</keyname><forenames>Etienne</forenames></author><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author></authors><title>Non-termination of Dalvik bytecode via compilation to CLP</title><categories>cs.PL</categories><comments>5 pages, presented at the 13th International Workshop on Termination
  (WST) 2013</comments><acm-class>D.2.4; F.3.1; F.3.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a set of rules for compiling a Dalvik bytecode program into a
logic program with array constraints. Non-termination of the resulting program
entails that of the original one, hence the techniques we have presented before
for proving non-termination of constraint logic programs can be used for
proving non-termination of Dalvik programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3731</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3731</id><created>2014-12-11</created><updated>2015-01-06</updated><authors><author><keyname>Soh</keyname><forenames>Yong Sheng</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author></authors><title>High-Dimensional Change-Point Estimation: Combining Filtering with
  Convex Optimization</title><categories>math.ST cs.IT math.IT math.OC stat.TH</categories><comments>27 pages, 4 figures, minor typo in Theorem 3.1 corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider change-point estimation in a sequence of high-dimensional signals
given noisy observations. Classical approaches to this problem such as the
filtered derivative method are useful for sequences of scalar-valued signals,
but they have undesirable scaling behavior in the high-dimensional setting.
However, many high-dimensional signals encountered in practice frequently
possess latent low-dimensional structure. Motivated by this observation, we
propose a technique for high-dimensional change-point estimation that combines
the filtered derivative approach from previous work with convex optimization
methods based on atomic norm regularization, which are useful for exploiting
structure in high-dimensional data. Our algorithm is applicable in online
settings as it operates on small portions of the sequence of observations at a
time, and it is well-suited to the high-dimensional setting both in terms of
computational scalability and of statistical efficiency. The main result of
this paper shows that our method performs change-point estimation reliably as
long as the product of the smallest-sized change (the Euclidean-norm-squared of
the difference between signals at a change-point) and the smallest distance
between change-points (number of time instances) is larger than a Gaussian
width parameter that characterizes the low-dimensional complexity of the
underlying signal sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3750</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3750</id><created>2014-12-11</created><updated>2016-01-07</updated><authors><author><keyname>Debattista</keyname><forenames>Jeremy</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Luzzu - A Framework for Linked Data Quality Assessment</title><categories>cs.DB cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing adoption and growth of the Linked Open Data cloud [9],
with RDFa, Microformats and other ways of embedding data into ordinary Web
pages, and with initiatives such as schema.org, the Web is currently being
complemented with a Web of Data. Thus, the Web of Data shares many
characteristics with the original Web of Documents, which also varies in
quality. This heterogeneity makes it challenging to determine the quality of
the data published on the Web and to subsequently make this information
explicit to data consumers. The main contribution of this article is LUZZU, a
quality assessment framework for Linked Open Data. Apart from providing quality
metadata and quality problem reports that can be used for data cleaning, LUZZU
is extensible: third party metrics can be easily plugged-in the framework. The
framework does not rely on SPARQL endpoints, and is thus free of all the
problems that come with them, such as query timeouts. Another advantage over
SPARQL based qual- ity assessment frameworks is that metrics implemented in
LUZZU can have more complex functionality than triple matching. Using the
framework, we performed a quality assessment of a number of statistical linked
datasets that are available on the LOD cloud. For this evaluation, 25 metrics
from ten different dimensions were implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3751</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3751</id><created>2014-12-11</created><authors><author><keyname>Bandi</keyname><forenames>Rama Krishna</forenames></author><author><keyname>Bhaintwal</keyname><forenames>Maheshanand</forenames></author></authors><title>Negacyclic codes over Z4+uZ4</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study negacyclic codes of odd length and of length $2^k$
over the ring $R=\mathbb{Z}_4+u\mathbb{Z}_4$, $u^2=0$. We give the complete
structure of negacyclic codes for both the cases. We have obtained a minimal
spanning set for negacyclic codes of odd lengths over $R$. A necessary and
sufficient condition for negacyclic codes of odd lengths to be free is
presented. We have determined the cardinality of negacyclic codes in each case.
We have obtained the structure of the duals of negacyclic codes of length $2^k$
over $R$ and also characterized self-dual negacyclic codes of length $2^k$ over
$R$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3756</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3756</id><created>2014-12-11</created><updated>2015-07-15</updated><authors><author><keyname>Feldman</keyname><forenames>Michael</forenames></author><author><keyname>Friedler</keyname><forenames>Sorelle</forenames></author><author><keyname>Moeller</keyname><forenames>John</forenames></author><author><keyname>Scheidegger</keyname><forenames>Carlos</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Certifying and removing disparate impact</title><categories>stat.ML cs.CY</categories><comments>Extended version of paper accepted at 2015 ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What does it mean for an algorithm to be biased? In U.S. law, unintentional
bias is encoded via disparate impact, which occurs when a selection process has
widely different outcomes for different groups, even as it appears to be
neutral. This legal determination hinges on a definition of a protected class
(ethnicity, gender, religious practice) and an explicit description of the
process.
  When the process is implemented using computers, determining disparate impact
(and hence bias) is harder. It might not be possible to disclose the process.
In addition, even if the process is open, it might be hard to elucidate in a
legal setting how the algorithm makes its decisions. Instead of requiring
access to the algorithm, we propose making inferences based on the data the
algorithm uses.
  We make four contributions to this problem. First, we link the legal notion
of disparate impact to a measure of classification accuracy that while known,
has received relatively little attention. Second, we propose a test for
disparate impact based on analyzing the information leakage of the protected
class from the other data attributes. Third, we describe methods by which data
might be made unbiased. Finally, we present empirical evidence supporting the
effectiveness of our test for disparate impact and our approach for both
masking bias and preserving relevant information in the data. Interestingly,
our approach resembles some actual selection practices that have recently
received legal scrutiny.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3768</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3768</id><created>2014-12-11</created><authors><author><keyname>Brennen</keyname><forenames>Andrea</forenames></author><author><keyname>Danico</keyname><forenames>David</forenames></author><author><keyname>Harnasch</keyname><forenames>Raul</forenames></author><author><keyname>Hunter</keyname><forenames>Maureen</forenames></author><author><keyname>Larkin</keyname><forenames>Richard</forenames></author><author><keyname>Mineweaser</keyname><forenames>Jeremy</forenames></author><author><keyname>Nam</keyname><forenames>Kevin</forenames></author><author><keyname>O'Gwynn</keyname><forenames>David</forenames></author><author><keyname>Phan</keyname><forenames>Harry</forenames></author><author><keyname>Schulz</keyname><forenames>Alexia</forenames></author><author><keyname>Snyder</keyname><forenames>Michael</forenames></author><author><keyname>Staheli</keyname><forenames>Diane</forenames></author><author><keyname>Yu</keyname><forenames>Tamara</forenames></author></authors><title>A novel display for situational awareness at a network operations center</title><categories>cs.HC</categories><comments>Received honorable mention in VAST 2013 Challenge, appears in VAST
  2013 Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As modern industry shifts toward significant globalization, robust and
adaptable network capability is increasingly vital to the success of business
enterprises. Large quantities of information must be distilled and presented in
a single integrated picture in order to maintain the health, security and
performance of global networks. We present a design for a network situational
awareness display that visually aggregates large quantities of data, identifies
problems in a network, assesses their impact on critical company mission areas
and clarifies the utilization of resources. This display facilitates the
prioritization of network problems as they arise by explicitly depicting how
problems interrelate. It also serves to coordinate mitigation strategies with
members of a team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3773</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3773</id><created>2014-12-11</created><updated>2015-12-24</updated><authors><author><keyname>Mooij</keyname><forenames>Joris M.</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Zscheischler</keyname><forenames>Jakob</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Distinguishing cause from effect using observational data: methods and
  benchmarks</title><categories>cs.LG cs.AI stat.ML stat.OT</categories><comments>101 pages, second revision submitted to Journal of Machine Learning
  Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery of causal relationships from purely observational data is a
fundamental problem in science. The most elementary form of such a causal
discovery problem is to decide whether X causes Y or, alternatively, Y causes
X, given joint observations of two variables X, Y. An example is to decide
whether altitude causes temperature, or vice versa, given only joint
measurements of both variables. Even under the simplifying assumptions of no
confounding, no feedback loops, and no selection bias, such bivariate causal
discovery problems are challenging. Nevertheless, several approaches for
addressing those problems have been proposed in recent years. We review two
families of such methods: Additive Noise Methods (ANM) and Information
Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs
that consists of data for 100 different cause-effect pairs selected from 37
datasets from various domains (e.g., meteorology, biology, medicine,
engineering, economy, etc.) and motivate our decisions regarding the &quot;ground
truth&quot; causal directions of all pairs. We evaluate the performance of several
bivariate causal discovery methods on these real-world benchmark data and in
addition on artificially simulated data. Our empirical results on real-world
data indicate that certain methods are indeed able to distinguish cause from
effect using only purely observational data, although more benchmark data would
be needed to obtain statistically significant conclusions. One of the best
performing methods overall is the additive-noise method originally proposed by
Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of
0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of
this work we prove the consistency of that method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3781</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3781</id><created>2014-12-11</created><authors><author><keyname>Pemantle</keyname><forenames>Robin</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author><author><keyname>Rivin</keyname><forenames>Igor</forenames></author></authors><title>Four Random Permutations Conjugated by an Adversary Generate $S_n$ with
  High Probability</title><categories>math.PR cs.SC math-ph math.MP</categories><comments>19pages, 1 figure</comments><msc-class>60C05, 12Y05, 68W20, 68W30, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a conjecture dating back to a 1978 paper of D.R.\
Musser~\cite{musserirred}, namely that four random permutations in the
symmetric group $\mathcal{S}_n$ generate a transitive subgroup with probability
$p_n &gt; \epsilon$ for some $\epsilon &gt; 0$ independent of $n$, even when an
adversary is allowed to conjugate each of the four by a possibly different
element of $\S_n$ (in other words, the cycle types already guarantee generation
of $\mathcal{S}_n$). This is closely related to the following random set model.
A random set $M \subseteq \mathbb{Z}^+$ is generated by including each $n \geq
1$ independently with probability $1/n$. The sumset $\text{sumset}(M)$ is
formed. Then at most four independent copies of $\text{sumset}(M)$ are needed
before their mutual intersection is no longer infinite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3788</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3788</id><created>2014-12-11</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Zhang</keyname><forenames>Kecheng</forenames></author><author><keyname>Jiang</keyname><forenames>Jiamo</forenames></author><author><keyname>Wang</keyname><forenames>Jiaheng</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>Energy-Efficient Resource Assignment and Power Allocation in
  Heterogeneous Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures, accepted by IEEE TVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking full advantages of both heterogeneous networks (HetNets) and cloud
access radio access networks (CRANs), heterogeneous cloud radio access networks
(H-CRANs) are presented to enhance both the spectral and energy efficiencies,
where remote radio heads (RRHs) are mainly used to provide high data rates for
users with high quality of service (QoS) requirements, while the high power
node (HPN) is deployed to guarantee the seamless coverage and serve users with
low QoS requirements. To mitigate the inter-tier interference and improve EE
performances in H-CRANs, characterizing user association with RRH/HPN is
considered in this paper, and the traditional soft fractional frequency reuse
(S-FFR) is enhanced. Based on the RRH/HPN association constraint and the
enhanced S-FFR, an energy-efficient optimization problem with the resource
assignment and power allocation for the orthogonal frequency division multiple
access (OFDMA) based H-CRANs is formulated as a non-convex objective function.
To deal with the non-convexity, an equivalent convex feasibility problem is
reformulated, and closedform expressions for the energy-efficient resource
allocation solution to jointly allocate the resource block and transmit power
are derived by the Lagrange dual decomposition method. Simulation results
confirm that the H-CRAN architecture and the corresponding resource allocation
solution can enhance the energy efficiency significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3795</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3795</id><created>2014-12-11</created><updated>2014-12-15</updated><authors><author><keyname>Krotov</keyname><forenames>Denis S.</forenames></author><author><keyname>Sotnikova</keyname><forenames>Evgeniya V.</forenames></author></authors><title>Embedding in $q$-ary $1$-perfect codes and partitions</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>7 pp</comments><msc-class>05Bxx</msc-class><journal-ref>Discrete Math. 338(11) 2015, 1856-1859</journal-ref><doi>10.1016/j.disc.2015.04.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every $1$-error-correcting code over a finite field can be
embedded in a $1$-perfect code of some larger length. Embedding in this context
means that the original code is a subcode of the resulting $1$-perfect code and
can be obtained from it by repeated shortening. Further, we generalize the
results to partitions: every partition of the Hamming space into
$1$-error-correcting codes can be embedded in a partition of a space of some
larger dimension into $1$-perfect codes. For the partitions, the embedding
length is close to the theoretical bound for the general case and optimal for
the binary case. Keywords: error-correcting code, $1$-perfect code, $1$-perfect
partition, embedding
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3802</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3802</id><created>2014-12-11</created><authors><author><keyname>Rubens</keyname><forenames>Neil</forenames></author></authors><title>Turing Test for the Internet of Things</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How smart is your kettle? How smart are things in your kitchen, your house,
your neighborhood, on the internet? With the advent of Internet of Things, and
the move of making devices `smart' by utilizing AI, a natural question arrises,
how can we evaluate the progress. The standard way of evaluating AI is through
the Turing Test. While Turing Test was designed for AI; the device that it was
tailored to was a computer. Applying the test to variety of devices that
constitute Internet of Things poses a number of challenges which could be
addressed through a number of adaptations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3829</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3829</id><created>2014-12-11</created><updated>2016-01-11</updated><authors><author><keyname>Dizdar</keyname><forenames>Onur</forenames></author><author><keyname>Ar&#x131;kan</keyname><forenames>Erdal</forenames></author></authors><title>A High-Throughput Energy-Efficient Implementation of
  Successive-Cancellation Decoder for Polar Codes Using Combinational Logic</title><categories>cs.AR</categories><comments>12 pages, 10 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a high-throughput energy-efficient Successive
Cancellation (SC) decoder architecture for polar codes based on combinational
logic. The proposed combinational architecture operates at relatively low clock
frequencies compared to sequential circuits, but takes advantage of the high
degree of parallelism inherent in such architectures to provide a favorable
tradeoff between throughput and energy efficiency at short to medium block
lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder
that combines the advantageous aspects of the combinational decoder with the
low-complexity nature of sequential-logic decoders. Performance characteristics
on ASIC and FPGA are presented with a detailed power consumption analysis for
combinational decoders. Finally, the paper presents an analysis of the
complexity and delay of combinational decoders, and of the throughput gains
obtained by hybrid-logic decoders with respect to purely synchronous
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3841</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3841</id><created>2014-12-11</created><updated>2015-10-18</updated><authors><author><keyname>Gospodarczyk</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Wo&#x17a;ny</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Merging of B\'ezier curves with box constraints</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach to the problem of merging of
B\'ezier curves with respect to the $L_2$-norm. We give illustrative examples
to show that the solution of the conventional merging problem may not be
suitable for further modification and applications. As in the case of the
degree reduction problem, we apply the so-called restricted area approach --
proposed recently in (P. Gospodarczyk, Computer-Aided Design 62 (2015),
143--151) -- to avoid certain defects and make the resulting curve more useful.
A method of solving the new problem is based on box-constrained quadratic
programming approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3852</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3852</id><created>2014-12-11</created><updated>2015-11-25</updated><authors><author><keyname>Pontecorvi</keyname><forenames>Matteo</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>Fully Dynamic All Pairs All Shortest Paths</title><categories>cs.DS</categories><comments>This revision incorporates changes that improve the presentation.
  There is no change to the main result. An extended abstract of this paper
  will appear in Proc. ISAAC 2015, in a paper by the authors entitled &quot;Fully
  Dynamic Betweenness Centrality''</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the all pairs all shortest paths (APASP) problem, which maintains
all of the multiple shortest paths for every vertex pair in a directed graph
G=(V,E) with a positive real weight on each edge. We present a fully dynamic
algorithm for this problem in which an update supports either weight increases
or weight decreases on a subset of edges incident to a vertex. Our algorithm
runs in amortized O(\vstar^2 \cdot \log^3 n) time per update, where n = |V|,
and \vstar bounds the number of edges that lie on shortest paths through any
single vertex. Our APASP algorithm leads to the same amortized bound for the
fully dynamic computation of betweenness centrality (BC), which is a parameter
widely used in the analysis of large complex networks.
  Our method is a generalization and a variant of the fully dynamic algorithm
of Demetrescu and Italiano [DI04] for unique shortest path, and it builds on
very recent work on decremental APASP [NPR14]. Our algorithm matches the fully
dynamic amortized bound in [DI04] for graphs with unique shortest paths, though
our method, and especially its analysis, are different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3855</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3855</id><created>2014-12-11</created><authors><author><keyname>Kenter</keyname><forenames>Franklin H. J.</forenames></author></authors><title>Necessary Spectral Conditions for Coloring Hypergraphs</title><categories>math.CO cs.DM</categories><comments>7 pages</comments><journal-ref>J. Combinatorial Computing and Machine Computing, 88 (2014), pp.
  73-84</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hoffman proved that for a simple graph $G$, the chromatic number $\chi(G)$
obeys $\chi(G) \le 1 - \frac{\lambda_1}{\lambda_{n}}$ where $\lambda_1$ and
$\lambda_n$ are the maximal and minimal eigenvalues of the adjacency matrix of
$G$ respectively. Lov\'asz later showed that $\chi(G) \le 1 -
\frac{\lambda_1}{\lambda_{n}}$ for any (perhaps negatively) weighted adjacency
matrix.
  In this paper, we give a probabilistic proof of Lov\'asz's theorem, then
extend the technique to derive generalizations of Hoffman's theorem when
allowed a certain proportion of edge-conflicts. Using this result, we show that
if a 3-uniform hypergraph is 2-colorable, then $\bar d \le
-\frac{3}{2}\lambda_{\min}$ where $\bar d$ is the average degree and
$\lambda_{\min}$ is the minimal eigenvalue of the underlying graph. We
generalize this further for $k$-uniform hypergraphs, for the cases $k=4$ and
$5$, by considering several variants of the underlying graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3861</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3861</id><created>2014-12-11</created><authors><author><keyname>Miranda</keyname><forenames>F&#xe9;lix A.</forenames></author><author><keyname>Casta&#xf1;os</keyname><forenames>Fernando</forenames></author><author><keyname>Poznyak</keyname><forenames>Alexander</forenames></author></authors><title>Min-max piecewise constant optimal control for multi-model linear
  systems</title><categories>cs.SY math.DS math.OC</categories><comments>20 pages, 8 figures. Submitted to IMA Journal of Mathematical Control
  and Applications</comments><msc-class>49J15, 49N05, 93C55, 49K35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work addresses a finite-horizon linear-quadratic optimal control
problem for uncertain systems driven by piecewise constant controls. The
precise values of the system parameters are unknown, but assumed to belong to a
finite set (i.e., there exist only finitely many possible models for the
plant). Uncertainty is dealt with using a min-max approach (i.e., we seek the
best control for the worst possible plant). The optimal control is derived
using a multi-model version of Lagrange's multipliers method, which specifies
the control in terms of a discrete-time Riccati equation and an optimization
problem over a simplex. A numerical algorithm for computing the optimal control
is proposed and tested by simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3869</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3869</id><created>2014-12-11</created><authors><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Milo</keyname><forenames>Tova</forenames></author><author><keyname>Roy</keyname><forenames>Sudeepa</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Answering Conjunctive Queries with Inequalities</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the complexity of answering conjunctive queries (CQ)
with inequalities). In particular, we are interested in comparing the
complexity of the query with and without inequalities. The main contribution of
our work is a novel combinatorial technique that enables us to use any
Select-Project-Join query plan for a given CQ without inequalities in answering
the CQ with inequalities, with an additional factor in running time that only
depends on the query. The key idea is to define a new projection operator,
which keeps a small representation (independent of the size of the database) of
the set of input tuples that map to each tuple in the output of the projection;
this representation is used to evaluate all the inequalities in the query.
Second, we generalize a result by Papadimitriou-Yannakakis [17] and give an
alternative algorithm based on the color-coding technique [4] to evaluate a CQ
with inequalities by using an algorithm for the CQ without inequalities. Third,
we investigate the structure of the query graph, inequality graph, and the
augmented query graph with inequalities, and show that even if the query and
the inequality graphs have bounded treewidth, the augmented graph not only can
have an unbounded treewidth but can also be NP-hard to evaluate. Further, we
illustrate classes of queries and inequalities where the augmented graphs have
unbounded treewidth, but the CQ with inequalities can be evaluated in
poly-time. Finally, we give necessary properties and sufficient properties that
allow a class of CQs to have poly-time combined complexity with respect to any
inequality pattern. We also illustrate classes of queries where our
query-plan-based technique outperforms the alternative approaches discussed in
the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3885</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3885</id><created>2014-12-11</created><authors><author><keyname>Hulovatyy</keyname><forenames>Yuriy</forenames></author><author><keyname>Chen</keyname><forenames>Huili</forenames></author><author><keyname>Milenkovic</keyname><forenames>Tijana</forenames></author></authors><title>Exploring the structure and function of temporal networks with dynamic
  graphlets</title><categories>cs.SI physics.soc-ph q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing amount of available temporal real-world network data, an
important question is how to efficiently study these data. One can simply model
a temporal network as either a single aggregate static network, or as a series
of time-specific snapshots, each of which is an aggregate static network over
the corresponding time window. The advantage of modeling the temporal data in
these two ways is that one can use existing well established methods for static
network analysis to study the resulting aggregate network(s). Here, we develop
a novel approach for studying temporal network data more explicitly. We base
our methodology on the well established notion of graphlets (subgraphs), which
have been successfully used in numerous contexts in static network research.
Here, we take the notion of static graphlets to the next level and develop new
theory needed to allow for graphlet-based analysis of temporal networks. Our
new notion of dynamic graphlets is quite different than existing approaches for
dynamic network analysis that are based on temporal motifs (statistically
significant subgraphs). Namely, these approaches suffer from many limitations.
For example, they can only deal with subgraph structures of limited complexity.
Also, their major drawback is that their results heavily depend on the choice
of a null network model that is required to evaluate the significance of a
subgraph. However, choosing an appropriate null network model is a non-trivial
task. Our dynamic graphlet approach overcomes the limitations of the existing
temporal motif-based approaches. At the same time, when we thoroughly evaluate
the ability of our new approach to characterize the structure and function of
an entire temporal network or of individual nodes, we find that the dynamic
graphlet approach outperforms the static graphlet approach, which indicates
that accounting for temporal information helps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3898</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3898</id><created>2014-12-12</created><authors><author><keyname>Yu</keyname><forenames>Lu</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhang</keyname><forenames>Zike</forenames></author></authors><title>ILCR: Item-based Latent Factors for Sparse Collaborative Retrieval</title><categories>cs.IR</categories><comments>10 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions between search and recommendation have recently attracted
significant attention, and several studies have shown that many potential
applications involve with a joint problem of producing recommendations to users
with respect to a given query, termed $Collaborative$ $Retrieval$ (CR).
Successful algorithms designed for CR should be potentially flexible at dealing
with the sparsity challenges since the setup of collaborative retrieval
associates with a given $query$ $\times$ $user$ $\times$ $item$ tensor instead
of traditional $user$ $\times$ $item$ matrix. Recently, several works are
proposed to study CR task from users' perspective. In this paper, we aim to
sufficiently explore the sophisticated relationship of each $query$ $\times$
$user$ $\times$ $item$ triple from items' perspective. By integrating
item-based collaborative information for this joint task, we present an
alternative factorized model that could better evaluate the ranks of those
items with sparse information for the given query-user pair. In addition, we
suggest to employ a recently proposed scalable ranking learning algorithm,
namely BPR, to optimize the state-of-the-art approach, $Latent$ $Collaborative$
$Retrieval$ model, instead of the original learning algorithm. The experimental
results on two real-world datasets, (i.e. \emph{Last.fm}, \emph{Yelp}),
demonstrate the efficiency and effectiveness of our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3906</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3906</id><created>2014-12-12</created><authors><author><keyname>Damschen</keyname><forenames>Marvin</forenames></author><author><keyname>Plessl</keyname><forenames>Christian</forenames></author></authors><title>Easy-to-Use On-the-Fly Binary Program Acceleration on Many-Cores</title><categories>cs.DC cs.PF</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Binary Acceleration At Runtime (BAAR), an easy-to-use
on-the-fly binary acceleration mechanism which aims to tackle the problem of
enabling existent software to automatically utilize accelerators at runtime.
BAAR is based on the LLVM Compiler Infrastructure and has a client-server
architecture. The client runs the program to be accelerated in an environment
which allows program analysis and profiling. Program parts which are identified
as suitable for the available accelerator are exported and sent to the server.
The server optimizes these program parts for the accelerator and provides RPC
execution for the client. The client transforms its program to utilize
accelerated execution on the server for offloaded program parts.
  We evaluate our work with a proof-of-concept implementation of BAAR that uses
an Intel Xeon Phi 5110P as the acceleration target and performs automatic
offloading, parallelization and vectorization of suitable program parts. The
practicality of BAAR for real-world examples is shown based on a study of
stencil codes. Our results show a speedup of up to 4x without any
developer-provided hints and 5.77x with hints over the same code compiled with
the Intel Compiler at optimization level O2 and running on an Intel Xeon
E5-2670 machine. Based on our insights gained during implementation and
evaluation we outline future directions of research, e.g., offloading more
fine-granular program parts than functions, a more sophisticated communication
mechanism or introducing on-stack-replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3908</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3908</id><created>2014-12-12</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Hermann</keyname><forenames>Alice</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>ICube</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Belief revision in the propositional closure of a qualitative algebra</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>14th International Conference on Principles of Knowledge
  Representation and Reasoning, Jul 2014, Vienne, Austria. AAAI Press, pp.4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief revision is an operation that aims at modifying old be-liefs so that
they become consistent with new ones. The issue of belief revision has been
studied in various formalisms, in particular, in qualitative algebras (QAs) in
which the result is a disjunction of belief bases that is not necessarily
repre-sentable in a QA. This motivates the study of belief revision in
formalisms extending QAs, namely, their propositional clo-sures: in such a
closure, the result of belief revision belongs to the formalism. Moreover, this
makes it possible to define a contraction operator thanks to the Harper
identity. Belief revision in the propositional closure of QAs is studied, an
al-gorithm for a family of revision operators is designed, and an open-source
implementation is made freely available on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3910</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3910</id><created>2014-12-12</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Du</keyname><forenames>Yuxian</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Local structure entropy of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying influential nodes in the complex networks is of theoretical and
practical significance. There are many methods are proposed to identify the
influential nodes in the complex networks. In this paper, a local structure
entropy which is based on the degree centrality and the statistical mechanics
is proposed to identifying the influential nodes in the complex network.
  In the definition of the local structure entropy, each node has a local
network, the local structure entropy of each node is equal to the structure
entropy of the local network. The main idea in the local structure entropy is
try to use the influence of the local network to replace the node's influence
on the whole network.
  The influential nodes which are identified by the local structure entropy are
the intermediate nodes in the network. The intermediate nodes which connect
those nodes with a big value of degree.
  We use the $Susceptible-Infective$ (SI) model to evaluate the performance of
the influential nodes which are identified by the local structure entropy. In
the SI model the nodes use as the source of infection. According to the SI
model, the bigger the percentage of the infective nodes in the network the
important the node to the whole networks. The simulation on four real networks
show that the proposed method is efficacious and rationality to identify the
influential nodes in the complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3914</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3914</id><created>2014-12-12</created><authors><author><keyname>Rott</keyname><forenames>Tamar</forenames></author><author><keyname>Shriki</keyname><forenames>Dorin</forenames></author><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author></authors><title>Edge Preserving Multi-Modal Registration Based On Gradient Intensity
  Self-Similarity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration is a challenging task in the world of medical imaging.
Particularly, accurate edge registration plays a central role in a variety of
clinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)
demonstrates state of the art alignment, based on the image self-similarity.
However, this method appears to be less accurate regarding edge registration.
In this work, we propose a new registration method, incorporating gradient
intensity and MIND self-similarity metric. Experimental results show the
superiority of this method in edge registration tasks, while preserving the
original MIND performance for other image features and textures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3919</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3919</id><created>2014-12-12</created><authors><author><keyname>Abraham</keyname><forenames>Alexandre</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Pedregosa</keyname><forenames>Fabian</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Eickenberg</keyname><forenames>Michael</forenames><affiliation>LNAO, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Gervais</keyname><forenames>Philippe</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Muller</keyname><forenames>Andreas</forenames><affiliation>NEUROSPIN, LTCI</affiliation></author><author><keyname>Kossaifi</keyname><forenames>Jean</forenames><affiliation>NEUROSPIN, LTCI</affiliation></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames><affiliation>NEUROSPIN, LTCI</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Varoquaux</keyname><forenames>G&#xe4;el</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France, LNAO</affiliation></author></authors><title>Machine Learning for Neuroimaging with Scikit-Learn</title><categories>cs.LG cs.CV stat.ML</categories><comments>Frontiers in neuroscience, Frontiers Research Foundation, 2013, pp.15</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical machine learning methods are increasingly used for neuroimaging
data analysis. Their main virtue is their ability to model high-dimensional
datasets, e.g. multivariate analysis of activation images or resting-state time
series. Supervised learning is typically used in decoding or encoding settings
to relate brain images to behavioral or clinical observations, while
unsupervised learning can uncover hidden structures in sets of images (e.g.
resting state functional MRI) or find sub-populations in large cohorts. By
considering different functional neuroimaging applications, we illustrate how
scikit-learn, a Python machine learning library, can be used to perform some
key analysis steps. Scikit-learn contains a very large set of statistical
learning algorithms, both supervised and unsupervised, and its application to
neuroimaging data provides a versatile tool to study the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3922</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3922</id><created>2014-12-12</created><authors><author><keyname>Dutta</keyname><forenames>Kunal</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author></authors><title>Size sensitive packing number for Hamming cube and its consequences</title><categories>cs.DM cs.CG cs.LG math.CO</categories><comments>At the time of submission, we have become aware of a similar packing
  result proven simultaneously by Ezra. However, we note that our proof of the
  main packing lemma is quite different from hers. Also, the focus of our paper
  is on discrepancy bounds and sampling complexity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a size-sensitive version of Haussler's Packing
lemma~\cite{Haussler92spherepacking} for set-systems with bounded primal
shatter dimension, which have an additional {\em size-sensitive property}. This
answers a question asked by Ezra~\cite{Ezra-sizesendisc-soda-14}. We also
partially address another point raised by Ezra regarding overcounting of sets
in her chaining procedure. As a consequence of these improvements, we get an
improvement on the size-sensitive discrepancy bounds for set systems with the
above property. Improved bounds on the discrepancy for these special set
systems also imply an improvement in the sizes of {\em relative $(\varepsilon,
\delta)$-approximations} and $(\nu, \alpha)$-samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3925</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3925</id><created>2014-12-12</created><authors><author><keyname>Abraham</keyname><forenames>Alexandre</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Dohmatob</keyname><forenames>Elvis</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Samaras</keyname><forenames>Dimitris</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Varoquaux</keyname><forenames>Gael</forenames><affiliation>NEUROSPIN, INRIA Saclay - Ile de France</affiliation></author></authors><title>Region segmentation for sparse decompositions: better brain
  parcellations from rest fMRI</title><categories>q-bio.NC cs.CV</categories><proxy>ccsd</proxy><journal-ref>Sparsity Techniques in Medical Imaging, Sep 2014, Boston, United
  States. pp.8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional Magnetic Resonance Images acquired during resting-state provide
information about the functional organization of the brain through measuring
correlations between brain areas. Independent components analysis is the
reference approach to estimate spatial components from weakly structured data
such as brain signal time courses; each of these components may be referred to
as a brain network and the whole set of components can be conceptualized as a
brain functional atlas. Recently, new methods using a sparsity prior have
emerged to deal with low signal-to-noise ratio data. However, even when using
sophisticated priors, the results may not be very sparse and most often do not
separate the spatial components into brain regions. This work presents
post-processing techniques that automatically sparsify brain maps and separate
regions properly using geometric operations, and compares these techniques
according to faithfulness to data and stability metrics. In particular, among
threshold-based approaches, hysteresis thresholding and random walker
segmentation, the latter improves significantly the stability of both dense and
sparse models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3947</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3947</id><created>2014-12-12</created><authors><author><keyname>Reshytko</keyname><forenames>Alexander</forenames></author></authors><title>The OCDF diagram. A metamodel for object-oriented systems visual design</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a metamodel for modeling control and data flows on subclass scales
in object-oriented systems. UML Profiles were used as a representation mean and
a complete metamodel definition was provided with an example of a diagram
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3949</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3949</id><created>2014-12-12</created><authors><author><keyname>Strau&#xdf;</keyname><forenames>Tobias</forenames></author><author><keyname>Gr&#xfc;ning</keyname><forenames>Tobias</forenames></author><author><keyname>Leifert</keyname><forenames>Gundram</forenames></author><author><keyname>Labahn</keyname><forenames>Roger</forenames></author><author><keyname>CITlab</keyname><forenames>for the University of Rostock -</forenames></author></authors><title>CITlab ARGUS for historical handwritten documents</title><categories>cs.CV cs.NE</categories><msc-class>68T05, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe CITlab's recognition system for the HTRtS competition attached to
the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR
2014. The task comprises the recognition of historical handwritten documents.
The core algorithms of our system are based on multi-dimensional recurrent
neural networks (MDRNN) and connectionist temporal classification (CTC). The
software modules behind that as well as the basic utility technologies are
essentially powered by PLANET's ARGUS framework for intelligent text
recognition and image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3955</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3955</id><created>2014-12-12</created><updated>2016-01-25</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Maniatis</keyname><forenames>Spyridon</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>The Parameterized Complexity of Graph Cyclability</title><categories>math.CO cs.CC cs.DM cs.DS</categories><msc-class>05C10, 05C83</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cyclability of a graph is the maximum integer $k$ for which every $k$
vertices lie on a cycle. The algorithmic version of the problem, given a graph
$G$ and a non-negative integer $k,$ decide whether the cyclability of $G$ is at
least $k,$ is {\sf NP}-hard. We study the parametrized complexity of this
problem. We prove that this problem, parameterized by $k,$ is ${\sf
co\mbox{-}W[1]}$-hard and that its does not admit a polynomial kernel on planar
graphs, unless ${\sf NP}\subseteq{\sf co}\mbox{-}{\sf NP}/{\sf poly}$. On the
positive side, we give an {\sf FPT} algorithm for planar graphs that runs in
time $2^{2^{O(k^2\log k)}}\cdot n^2$. Our algorithm is based on a series of
graph-theoretical results on cyclic linkages in planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3958</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3958</id><created>2014-12-12</created><authors><author><keyname>Abdelsamea</keyname><forenames>Mohammed M.</forenames></author></authors><title>An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation</title><categories>cs.CV</categories><comments>appears in Proceedings of International Conference on Environment and
  Bio-Science 2011. subset of arXiv:1407.3664</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an automatic seeded region growing algorithm is proposed for
cellular image segmentation. First, the regions of interest (ROIs) extracted
from the preprocessed image. Second, the initial seeds are automatically
selected based on ROIs extracted from the image. Third, the most reprehensive
seeds are selected using a machine learning algorithm. Finally, the cellular
image is segmented into regions where each region corresponds to a seed. The
aim of the proposed is to automatically extract the Region of Interests (ROI)
from the cellular images in terms of overcoming the explosion, under
segmentation and over segmentation problems. Experimental results show that the
proposed algorithm can improve the segmented image and the segmented results
are less noisy as compared to some existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3964</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3964</id><created>2014-12-12</created><updated>2015-08-11</updated><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>K&#xfc;rzl</keyname><forenames>Alexander</forenames></author><author><keyname>Mezghani</keyname><forenames>Amine</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>Asymptotic Parameter Tracking Performance with Measurement Data of 1-bit
  Resolution</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2458778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of signal parameter estimation and tracking with measurement data
of low resolution is considered. In comparison to an ideal receiver with
infinite receive resolution, the performance loss of a simplistic receiver with
1-bit resolution is investigated. For the case where the measurement data is
preprocessed by a symmetric hard-limiting device with 1-bit output, it is
well-understood that the performance for low SNR channel parameter estimation
degrades moderately by 2/pi (-1.96 dB). Here we show that the 1-bit
quantization loss can be significantly smaller if information about the
temporal evolution of the channel parameters is taken into account in the form
of a state-space model. By the analysis of a Bayesian bound for the achievable
tracking performance, we attain the result that the quantization loss in dB is
in general smaller by a factor of two if the channel evolution is slow. For the
low SNR regime, this is equivalent to a reduced loss of sqrt(2/pi) (-0.98 dB).
By simulating non-linear filtering algorithms for a satellite-based ranging
application (GPS) and a UWB channel estimation problem, both with
low-complexity 1-bit analog-to-digital converter (ADC) at the receiver, we
verify that the analytical characterization of the tracking error is accurate.
This shows that the performance loss due to observations with low amplitude
resolution can, in practice, be much less pronounced than indicated by
classical results. Finally, we discuss the implication of the result for medium
SNR applications like channel estimation in the context of mobile wireless
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3976</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3976</id><created>2014-12-12</created><authors><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author></authors><title>Reconfiguration of Cliques in a Graph</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study reconfiguration problems for cliques in a graph, which determine
whether there exists a sequence of cliques that transforms a given clique into
another one in a step-by-step fashion. As one step of a transformation, we
consider three different types of rules, which are defined and studied in
reconfiguration problems for independent sets. We first prove that all the
three rules are equivalent in cliques. We then show that the problems are
PSPACE-complete for perfect graphs, while we give polynomial-time algorithms
for several classes of graphs, such as even-hole-free graphs and cographs. In
particular, the shortest variant, which computes the shortest length of a
desired sequence, can be solved in polynomial time for chordal graphs,
bipartite graphs, planar graphs, and bounded treewidth graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3978</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3978</id><created>2014-12-12</created><updated>2015-09-24</updated><authors><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Delay Games with WMSO+U Winning Conditions</title><categories>cs.GT cs.FL</categories><comments>A short version appears in the proceedings of CSR 2015. The
  definition of the equivalence relation introduced in Section 3 is updated:
  the previous one was inadequate, which invalidates the proof of Lemma 2. The
  correction presented here suffices to prove Lemma 2 and does not affect our
  main theorem. arXiv admin note: text overlap with arXiv:1412.3701</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay games are two-player games of infinite duration in which one player may
delay her moves to obtain a lookahead on her opponent's moves. We consider
delay games with winning conditions expressed in weak monadic second order
logic with the unbounding quantifier, which is able to express (un)boundedness
properties. We show that it is decidable whether the delaying player has a
winning strategy using bounded lookahead and give a doubly-exponential upper
bound on the necessary lookahead. In contrast, we show that bounded lookahead
is not always sufficient to win such a game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3984</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3984</id><created>2014-12-12</created><authors><author><keyname>Hoffmann</keyname><forenames>Frank</forenames></author><author><keyname>Kriegel</keyname><forenames>Klaus</forenames></author><author><keyname>Willert</keyname><forenames>Max</forenames></author></authors><title>Almost Tight Bounds for Conflict-Free Chromatic Guarding of Orthogonal
  Galleries</title><categories>cs.CG</categories><comments>18 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address recently proposed chromatic versions of the classic Art Gallery
Problem. Assume a simple polygon $P$ is guarded by a finite set of point guards
and each guard is assigned one of $t$ colors. Such a chromatic guarding is said
to be conflict-free if each point $p\in P$ sees at least one guard with a
unique color among all guards visible from $p$. The goal is to establish bounds
on the function $\chi_{cf}(n)$ of the number of colors sufficient to guarantee
the existence of a conflict-free chromatic guarding for any $n$-vertex polygon.
B\&quot;artschi and Suri showed $\chi_{cf}(n)\in O(\log n)$ (Algorithmica, 2014) for
simple orthogonal polygons and the same bound applies to general simple
polygons (B\&quot;artschi et al., SoCG 2014). In this paper, we assume the
r-visibility model instead of standard line visibility. Points $p$ and $q$ in
an orthogonal polygon are r-visible to each other if the rectangle spanned by
the points is contained in $P$. For this model we show $\chi_{cf}(n)\in
O(\log\log n)$ and $\chi_{cf}(n)\in \Omega(\log\log n /\log\log\log n)$. Most
interestingly, we can show that the lower bound proof extends to guards with
line visibility. To this end we introduce and utilize a novel discrete
combinatorial structure called multicolor tableau. This is the first
non-trivial lower bound for this problem setting.Furthermore, for the strong
chromatic version of the problem, where all guards r-visible from a point must
have distinct colors, we prove a $\Theta(\log n)$-bound. Our results can be
interpreted as coloring results for special geometric hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.3987</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.3987</id><created>2014-12-12</created><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Fisikopoulos</keyname><forenames>Vissarion</forenames></author><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author></authors><title>Efficient edge-skeleton computation for polytopes defined by oracles</title><categories>cs.CG cs.SC math.OC</categories><comments>22 pages, 2 figures</comments><msc-class>52-XX, 14Qxx</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In general dimension, there is no known total polynomial algorithm for either
convex hull or vertex enumeration, i.e. an algorithm whose complexity depends
polynomially on the input and output sizes. It is thus important to identify
problems (and polytope representations) for which total polynomial-time
algorithms can be obtained. We offer the first total polynomial-time algorithm
for computing the edge-skeleton (including vertex enumeration) of a polytope
given by an optimization or separation oracle, where we are also given a
superset of its edge directions. We also offer a space-efficient variant of our
algorithm by employing reverse search. All complexity bounds refer to the
(oracle) Turing machine model. There is a number of polytope classes naturally
defined by oracles; for some of them neither vertex nor facet representation is
obvious. We consider two main applications, where we obtain (weakly) total
polynomial-time algorithms: Signed Minkowski sums of convex polytopes, where
polytopes can be subtracted provided the signed sum is a convex polytope, and
computation of secondary, resultant, and discriminant polytopes. Further
applications include convex combinatorial optimization and convex integer
programming, where we offer a new approach, thus removing the complexity's
exponential dependence in the dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4005</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4005</id><created>2014-12-09</created><authors><author><keyname>Bobin</keyname><forenames>Jerome</forenames></author><author><keyname>Rapin</keyname><forenames>Jeremy</forenames></author><author><keyname>Larue</keyname><forenames>Anthony</forenames></author><author><keyname>Starck</keyname><forenames>Jean-Luc</forenames></author></authors><title>Sparsity and adaptivity for the blind separation of partially correlated
  sources</title><categories>stat.AP cs.LG stat.ML</categories><comments>submitted to IEEE Transactions on signal processing</comments><doi>10.1109/TSP.2015.2391071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind source separation (BSS) is a very popular technique to analyze
multichannel data. In this context, the data are modeled as the linear
combination of sources to be retrieved. For that purpose, standard BSS methods
all rely on some discrimination principle, whether it is statistical
independence or morphological diversity, to distinguish between the sources.
However, dealing with real-world data reveals that such assumptions are rarely
valid in practice: the signals of interest are more likely partially
correlated, which generally hampers the performances of standard BSS methods.
In this article, we introduce a novel sparsity-enforcing BSS method coined
Adaptive Morphological Component Analysis (AMCA), which is designed to retrieve
sparse and partially correlated sources. More precisely, it makes profit of an
adaptive re-weighting scheme to favor/penalize samples based on their level of
correlation. Extensive numerical experiments have been carried out which show
that the proposed method is robust to the partial correlation of sources while
standard BSS techniques fail. The AMCA algorithm is evaluated in the field of
astrophysics for the separation of physical components from microwave data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4020</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4020</id><created>2014-12-12</created><updated>2015-02-28</updated><authors><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Equivariant algorithms for constraint satisfaction problems over coset
  templates</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Constraint Satisfaction Problem (CSP) over templates with
a group structure, and algorithms solving CSP that are equivariant, i.e.
invariant under a natural group action induced by a template. Our main result
is a method of proving the implication: if CSP over a coset template T is
solvable by an equivariant algorithm then T is 2-Helly (or equivalently, has a
majority polymorphism). Therefore bounded width, and definability in
fixed-point logics, coincide with 2-Helly. Even if these facts may be derived
from already known results, our new proof method has two advantages. First, the
proof is short, self-contained, and completely avoids referring to the
omitting-types theorems. Second, it brings to light some new connections
between CSP theory and descriptive complexity theory, via a construction
similar to CFI graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4021</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4021</id><created>2014-12-12</created><updated>2015-12-19</updated><authors><author><keyname>Nguyen</keyname><forenames>Dat Quoc</forenames></author><author><keyname>Nguyen</keyname><forenames>Dai Quoc</forenames></author><author><keyname>Pham</keyname><forenames>Dang Duc</forenames></author><author><keyname>Pham</keyname><forenames>Son Bao</forenames></author></authors><title>A Robust Transformation-Based Learning Approach Using Ripple Down Rules
  for Part-of-Speech Tagging</title><categories>cs.CL</categories><comments>Version 1: 13 pages. Version 2: Submitted to AI Communications - the
  European Journal on Artificial Intelligence. Version 3: Resubmitted after
  major revisions. Version 4: Resubmitted after minor revisions. Version 5: to
  appear in AI Communications (accepted for publication on 3/12/2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new approach to construct a system of
transformation rules for the Part-of-Speech (POS) tagging task. Our approach is
based on an incremental knowledge acquisition method where rules are stored in
an exception structure and new rules are only added to correct the errors of
existing rules; thus allowing systematic control of the interaction between the
rules. Experimental results on 13 languages show that our approach is fast in
terms of training time and tagging speed. Furthermore, our approach obtains
very competitive accuracy in comparison to state-of-the-art POS and
morphological taggers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4030</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4030</id><created>2014-12-12</created><updated>2015-01-05</updated><authors><author><keyname>Ameloot</keyname><forenames>Tom J.</forenames></author><author><keyname>Geck</keyname><forenames>Gaetano</forenames></author><author><keyname>Ketsman</keyname><forenames>Bas</forenames></author><author><keyname>Neven</keyname><forenames>Frank</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author></authors><title>Parallel-Correctness and Transferability for Conjunctive Queries</title><categories>cs.DB</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dominant cost for query evaluation in modern massively distributed systems
is the number of communication rounds. For this reason, there is a growing
interest in single-round multiway join algorithms where data is first
reshuffled over many servers and then evaluated in a parallel but
communication-free way. The reshuffling itself is specified as a distribution
policy. We introduce a correctness condition, called parallel-correctness, for
the evaluation of queries w.r.t. a distribution policy. We study the complexity
of parallel-correctness for conjunctive queries as well as transferability of
parallel-correctness between queries. We also investigate the complexity of
transferability for certain families of distribution policies, including, for
instance, the Hypercube distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4031</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4031</id><created>2014-12-11</created><authors><author><keyname>Konnik</keyname><forenames>Mikhail</forenames></author><author><keyname>Welsh</keyname><forenames>James</forenames></author></authors><title>High-level numerical simulations of noise in CCD and CMOS photosensors:
  review and tutorial</title><categories>astro-ph.IM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, such as development and testing of image processing
algorithms, it is often necessary to simulate images containing realistic noise
from solid-state photosensors. A high-level model of CCD and CMOS photosensors
based on a literature review is formulated in this paper. The model includes
photo-response non-uniformity, photon shot noise, dark current Fixed Pattern
Noise, dark current shot noise, offset Fixed Pattern Noise, source follower
noise, sense node reset noise, and quantisation noise. The model also includes
voltage-to-voltage, voltage-to-electrons, and analogue-to-digital converter
non-linearities. The formulated model can be used to create synthetic images
for testing and validation of image processing algorithms in the presence of
realistic images noise. An example of the simulated CMOS photosensor and a
comparison with a custom-made CMOS hardware sensor is presented. Procedures for
characterisation from both light and dark noises are described. Experimental
results that confirm the validity of the numerical model are provided. The
paper addresses the issue of the lack of comprehensive high-level photosensor
models that enable engineers to simulate realistic effects of noise on the
images obtained from solid-state photosensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4039</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4039</id><created>2014-12-11</created><authors><author><keyname>Degrave</keyname><forenames>Jonas</forenames></author></authors><title>Resolving multi-proxy transitive vote delegation</title><categories>cs.MA</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Solving a delegation graph for transitive votes is already a non-trivial task
for many programmers. When extending the current main paradigm, where each
voter can only appoint a single transitive delegation, to a system where each
vote can be separated over multiple delegations, solving the delegation graph
becomes even harder. This article presents a solution of an example graph, and
a non-formal proof of why this algorithm works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4042</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4042</id><created>2014-12-12</created><authors><author><keyname>Kondor</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Sz&#xfc;le</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>P&#xf3;sfai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Vattay</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Inferring the interplay of network structure and market effects in
  Bitcoin</title><categories>physics.soc-ph cs.SI</categories><comments>project website: http://www.vo.elte.hu/bitcoin</comments><journal-ref>New J. Phys. 16 (2014) 125003</journal-ref><doi>10.1088/1367-2630/16/12/125003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A main focus in economics research is understanding the time series of prices
of goods and assets. While statistical models using only the properties of the
time series itself have been successful in many aspects, we expect to gain a
better understanding of the phenomena involved if we can model the underlying
system of interacting agents. In this article, we consider the history of
Bitcoin, a novel digital currency system, for which the complete list of
transactions is available for analysis. Using this dataset, we reconstruct the
transaction network between users and analyze changes in the structure of the
subgraph induced by the most active users. Our approach is based on the
unsupervised identification of important features of the time variation of the
network. Applying the widely used method of Principal Component Analysis to the
matrix constructed from snapshots of the network at different times, we are
able to show how structural changes in the network accompany significant
changes in the exchange price of bitcoins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4044</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4044</id><created>2014-12-12</created><updated>2015-04-18</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Yue</forenames></author></authors><title>Adaptive Stochastic Gradient Descent on the Grassmannian for Robust
  Low-Rank Subspace Recovery and Clustering</title><categories>stat.ML cs.CV cs.NA math.OC</categories><comments>13 pages, 12 figures and 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient
for $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to
robustly recover the low-rank subspace from a large matrix. In the presence of
column outliers, we reformulate the batch mode matrix $L_{2,1}$ norm
minimization with rank constraint problem as a stochastic optimization approach
constrained on Grassmann manifold. For each observed data vector, the low-rank
subspace $\mathcal{S}$ is updated by taking a gradient step along the geodesic
of Grassmannian. In order to accelerate the convergence rate of the stochastic
gradient method, we choose to adaptively tune the constant step-size by
leveraging the consecutive gradients. Furthermore, we demonstrate that with
proper initialization, the K-subspaces extension, K-GASG21, can robustly
cluster a large number of corrupted data vectors into a union of subspaces.
Numerical experiments on synthetic and real data demonstrate the efficiency and
accuracy of the proposed algorithms even with heavy column outliers corruption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4051</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4051</id><created>2014-12-11</created><authors><author><keyname>Donovan</keyname><forenames>Zola</forenames></author><author><keyname>Mkrtchyan</keyname><forenames>Vahan</forenames></author><author><keyname>Subramani</keyname><forenames>K.</forenames></author></authors><title>Clustering without replication: approximation and inapproximability</title><categories>cs.DS cs.DM</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering the nodes of directed acyclic graphs
when the replication of logic is not allowed. We show that the problem is {\bf
NP-hard} even when the vertices of the DAG are unweighted and the cluster
capacity is $2$. Moreover, when the vertices of the DAG are weighted, the
problem does not admit a $(2-\epsilon)$-approximation algorithm for each
$\epsilon &gt;0$, unless {\bf P=NP}. On the positive side, we show that in case
the vertices of the DAG are unweighted and $M=2$, the problem admits a
$2$-approximation algorithm. Finally, we present some cases when the problem
can be solved in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4052</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4052</id><created>2014-12-11</created><authors><author><keyname>Lagrange</keyname><forenames>Mathieu</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Lafay</keyname><forenames>Gr&#xe9;goire</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Defreville</keyname><forenames>Boris</forenames></author><author><keyname>Aucouturier</keyname><forenames>Jean-Julien</forenames></author></authors><title>The bag-of-frames approach: a not so sufficient model for urban
  soundscapes</title><categories>cs.SD</categories><proxy>ccsd</proxy><doi>10.1121/1.4935350</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;bag-of-frames&quot; approach (BOF), which encodes audio signals as the
long-term statistical distribution of short-term spectral features, is commonly
regarded as an effective and sufficient way to represent environmental sound
recordings (soundscapes) since its introduction in an influential 2007 article.
The present paper describes a conceptual replication of this seminal article
using several new soundscape datasets, with results strongly questioning the
adequacy of the BOF approach for the task. We show that the good accuracy
originally reported with BOF likely result from a particularly thankful dataset
with low within-class variability, and that for more realistic datasets, BOF in
fact does not perform significantly better than a mere one-point average of the
signal's features. Soundscape modeling, therefore, may not be the closed case
it was once thought to be. Progress, we argue, could lie in reconsidering the
problem of considering individual acoustical events within each soundscape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4053</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4053</id><created>2014-12-12</created><updated>2015-01-16</updated><authors><author><keyname>Toronto</keyname><forenames>Neil</forenames></author><author><keyname>McCarthy</keyname><forenames>Jay</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Running Probabilistic Programs Backwards</title><categories>cs.PL</categories><comments>26 pages, ESOP 2015 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many probabilistic programming languages allow programs to be run under
constraints in order to carry out Bayesian inference. Running programs under
constraints could enable other uses such as rare event simulation and
probabilistic verification---except that all such probabilistic languages are
necessarily limited because they are defined or implemented in terms of an
impoverished theory of probability. Measure-theoretic probability provides a
more general foundation, but its generality makes finding computational content
difficult.
  We develop a measure-theoretic semantics for a first-order probabilistic
language with recursion, which interprets programs as functions that compute
preimages. Preimage functions are generally uncomputable, so we derive an
abstract semantics. We implement the abstract semantics and use the
implementation to carry out Bayesian inference, stochastic ray tracing (a rare
event simulation), and probabilistic verification of floating-point error
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4054</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4054</id><created>2014-12-12</created><authors><author><keyname>Campos</keyname><forenames>Filipe</forenames></author><author><keyname>Pereira</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>An Experimental Evaluation of Machine-to-Machine Coordination
  Middleware: Extended Version</title><categories>cs.DC cs.NI</categories><comments>24 pages, Technical Report</comments><acm-class>C.2.4; B.8.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vision of the Internet-of-Things (IoT) embodies the seam- less discovery,
configuration, and interoperability of networked devices in various settings,
ranging from home automation and multimedia to autonomous vehicles and
manufacturing equipment. As these ap- plications become increasingly critical,
the middleware coping with Machine-to-Machine (M2M) communication and
coordination has to deal with fault tolerance and increasing complexity, while
still abiding to resource constraints of target devices. In this report, we
focus on configuration management and coordi- nation of services in a M2M
scenario. On one hand, we consider Zoo- Keeper, originally developed for cloud
data centers, offering a simple file-system abstraction, and embodying
replication for fault-tolerance and scalability based on a consensus protocol.
On the other hand, we consider the Devices Profile for Web Services (DPWS)
stack with replicated services based on our implementation of the Raft
consensus protocol. We show that the latter offers adequate performance for the
targeted applications while providing increasing flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4055</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4055</id><created>2014-12-12</created><authors><author><keyname>Risuleo</keyname><forenames>Riccardo Sven</forenames></author><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>A kernel-based approach to Hammerstein system identification</title><categories>cs.SY</categories><comments>6 pages, submitted to IFAC SysId 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel algorithm for the identification of
Hammerstein systems. Adopting a Bayesian approach, we model the impulse
response of the unknown linear dynamic system as a realization of a zero-mean
Gaussian process. The covariance matrix (or kernel) of this process is given by
the recently introduced stable-spline kernel, which encodes information on the
stability and regularity of the impulse response. The static non-linearity of
the model is identified using an Empirical Bayes approach, i.e. by maximizing
the output marginal likelihood, which is obtained by integrating out the
unknown impulse response. The related optimization problem is solved adopting a
novel iterative scheme based on the Expectation-Maximization (EM) method, where
each iteration consists in a simple sequence of update rules. Numerical
experiments show that the proposed method compares favorably with a standard
algorithm for Hammerstein system identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4056</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4056</id><created>2014-12-12</created><authors><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Risuleo</keyname><forenames>Riccardo S.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>Blind system identification using kernel-based methods</title><categories>cs.SY stat.ML</categories><comments>6 pages; Submitted to IFAC Sysid 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for blind system identification (BSI). Resorting to a
Gaussian regression framework, we model the impulse response of the unknown
linear system as a realization of a Gaussian process. The structure of the
covariance matrix (or kernel) of such a process is given by the stable spline
kernel, which has been recently introduced for system identification purposes
and depends on an unknown hyperparameter. We assume that the input can be
linearly described by few parameters. We estimate these parameters, together
with the kernel hyperparameter and the noise variance, using an empirical Bayes
approach. The related optimization problem is efficiently solved with a novel
iterative scheme based on the Expectation-Maximization (EM) method. In
particular, we show that each iteration consists of a set of simple update
rules. We show, through some numerical experiments, very promising performance
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4062</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4062</id><created>2014-12-12</created><authors><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>Permutation Reconstruction from MinMax-Betweenness Constraints</title><categories>cs.DS cs.DM</categories><comments>16 pages, 3 figures</comments><msc-class>68W32</msc-class><acm-class>G.2.1; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the reconstruction of permutations on {1, 2,
..., n} from betweenness constraints involving the minimum and the maximum
element located between t and t+1, for all t=1, 2, ..., n-1. We propose two
variants of the problem (directed and undirected), and focus first on the
directed version, for which we draw up general features and design a polynomial
algorithm in a particular case. Then, we investigate necessary and sufficient
conditions for the uniqueness of the reconstruction in both directed and
undirected versions, using a parameter k whose variation controls the
stringency of the betweenness constraints. We finally point out open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4067</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4067</id><created>2014-12-12</created><updated>2015-08-20</updated><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Lemm</keyname><forenames>Marius</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Monotonicity of quantum relative entropy and recoverability</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v3: 22 pages, 1 figure, accepted for publication in Quantum
  Information and Computation</comments><journal-ref>Quantum Information and Computation vol. 15, no. 15 &amp; 16, pages
  1333-1354, November 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relative entropy is a principal measure of distinguishability in quantum
information theory, with its most important property being that it is
non-increasing with respect to noisy quantum operations. Here, we establish a
remainder term for this inequality that quantifies how well one can recover
from a loss of information by employing a rotated Petz recovery map. The main
approach for proving this refinement is to combine the methods of [Fawzi and
Renner, arXiv:1410.0664] with the notion of a relative typical subspace from
[Bjelakovic and Siegmund-Schultze, arXiv:quant-ph/0307170]. Our paper
constitutes partial progress towards a remainder term which features just the
Petz recovery map (not a rotated Petz map), a conjecture which would have many
consequences in quantum information theory.
  A well known result states that the monotonicity of relative entropy with
respect to quantum operations is equivalent to each of the following
inequalities: strong subadditivity of entropy, concavity of conditional
entropy, joint convexity of relative entropy, and monotonicity of relative
entropy with respect to partial trace. We show that this equivalence holds true
for refinements of all these inequalities in terms of the Petz recovery map. So
either all of these refinements are true or all are false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4071</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4071</id><created>2014-12-12</created><updated>2016-01-13</updated><authors><author><keyname>Cogoni</keyname><forenames>Marco</forenames></author><author><keyname>Busonera</keyname><forenames>Giovanni</forenames></author><author><keyname>Anedda</keyname><forenames>Paolo</forenames></author><author><keyname>Zanetti</keyname><forenames>Gianluigi</forenames></author></authors><title>Transition to congestion in communication/computation networks: from
  ideal to realistic resource allocation via Montecarlo simulations</title><categories>cs.NI</categories><comments>third version, 21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize previous studies on critical phenomena in communication
networks by adding computational capabilities to the nodes to better describe
real-world situations such as cloud computing. A set of tasks with random
origin and destination with a multi-tier computational structure is distributed
on a network modeled as a graph. The execution time (or latency) of each task
is statically computed and the sum is used as the energy in a Montecarlo
simulation in which the temperature parameter controls the resource allocation
optimality. We study the transition to congestion by varying temperature and
system load. A method to approximately recover the time-evolution of the system
by interpolating the latency probability distributions is presented. This
allows us to study the standard transition to the congested phase by varying
the task production rate. We are able to reproduce the main known results on
network congestion and to gain a deeper insight over the maximum theoretical
performance of a system and its sensitivity to routing and load balancing
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4074</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4074</id><created>2014-12-12</created><updated>2015-09-08</updated><authors><author><keyname>Di Bartolomeo</keyname><forenames>Marco</forenames></author><author><keyname>Di Donato</keyname><forenames>Valentino</forenames></author><author><keyname>Pizzonia</keyname><forenames>Maurizio</forenames></author><author><keyname>Squarcella</keyname><forenames>Claudio</forenames></author><author><keyname>Rimondini</keyname><forenames>Massimo</forenames></author></authors><title>Mining Network Events using Traceroute Empathy</title><categories>cs.NI</categories><comments>8 pages, 7 figures, extended version of Discovering High-Impact
  Routing Events using Traceroutes, in Proc. 20th International Symposium on
  Computers and Communications (ISCC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the never-ending quest for tools that enable an ISP to smooth
troubleshooting and improve awareness of network behavior, very much effort has
been devoted in the collection of data by active and passive measurement at the
data plane and at the control plane level. Exploitation of collected data has
been mostly focused on anomaly detection and on root-cause analysis. Our
objective is somewhat in the middle. We consider traceroutes collected by a
network of probes and aim at introducing a practically applicable methodology
to quickly spot measurements that are related to high-impact events happened in
the network. Such filtering process eases further in- depth human-based
analysis, for example with visual tools which are effective only when handling
a limited amount of data. We introduce the empathy relation between traceroutes
as the cornerstone of our formal characterization of the traceroutes related to
a network event. Based on this model, we describe an algorithm that finds
traceroutes related to high-impact events in an arbitrary set of measurements.
Evidence of the effectiveness of our approach is given by experimental results
produced on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4076</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4076</id><created>2014-12-12</created><updated>2015-01-18</updated><authors><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Fischer</keyname><forenames>Mareike</forenames></author></authors><title>On the complexity of computing MP distance between binary phylogenetic
  trees</title><categories>q-bio.PE cs.CE cs.DS math.CO</categories><comments>37 pages, 8 figures</comments><msc-class>05C15, 05C35, 90C35, 92D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the field of phylogenetics there is great interest in distance
measures to quantify the dissimilarity of two trees. Recently, a new distance
measure has been proposed: the Maximum Parsimony (MP) distance. This is based
on the difference of the parsimony scores of a single character on both trees
under consideration, and the goal is to find the character which maximizes this
difference. Here we show that computation of MP distance on two \emph{binary}
phylogenetic trees is NP-hard. This is a highly nontrivial extension of an
earlier NP-hardness proof for two multifurcating phylogenetic trees, and it is
particularly relevant given the prominence of binary trees in the phylogenetics
literature. As a corollary to the main hardness result we show that computation
of MP distance is also hard on binary trees if the number of states available
is bounded. In fact, via a different reduction we show that it is hard even if
only two states are available. Finally, as a first response to this hardness we
give a simple Integer Linear Program (ILP) formulation which is capable of
computing the MP distance exactly for small trees (and for larger trees when
only a small number of character states are available) and which is used to
computationally verify several auxiliary results required by the hardness
proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4080</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4080</id><created>2014-12-12</created><authors><author><keyname>Bonnefoy</keyname><forenames>Antoine</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Emiya</keyname><forenames>Valentin</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and
  Group-Lasso</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><doi>10.1109/TSP.2015.2447503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent computational strategies based on screening tests have been proposed
to accelerate algorithms addressing penalized sparse regression problems such
as the Lasso. Such approaches build upon the idea that it is worth dedicating
some small computational effort to locate inactive atoms and remove them from
the dictionary in a preprocessing stage so that the regression algorithm
working with a smaller dictionary will then converge faster to the solution of
the initial problem. We believe that there is an even more efficient way to
screen the dictionary and obtain a greater acceleration: inside each iteration
of the regression algorithm, one may take advantage of the algorithm
computations to obtain a new screening test for free with increasing screening
effects along the iterations. The dictionary is henceforth dynamically screened
instead of being screened statically, once and for all, before the first
iteration. We formalize this dynamic screening principle in a general
algorithmic scheme and apply it by embedding inside a number of first-order
algorithms adapted existing screening tests to solve the Lasso or new screening
tests to solve the Group-Lasso. Computational gains are assessed in a large set
of experiments on synthetic data as well as real-world sounds and images. They
show both the screening efficiency and the gain in terms running times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4081</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4081</id><created>2014-12-12</created><updated>2015-03-12</updated><authors><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>Quantitative Analysis of the Italian National Scientific Qualification</title><categories>cs.DL</categories><comments>ISSN 1751-1577</comments><msc-class>62P99</msc-class><journal-ref>Journal of Informetrics, Volume 9, Issue 2, April 2015, Pages
  285-316</journal-ref><doi>10.1016/j.joi.2015.02.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Italian National Scientific Qualification (ASN) was introduced in 2010 as
part of a major reform of the national university system. Under the new
regulation, the scientific qualification for a specific role (associate or full
professor) and field of study is required to apply to a permanent professor
position. The ASN is peculiar since it makes use of bibliometric indicators
with associated thresholds as one of the parameters used to assess applicants.
Overall, more than 59000 applications were submitted, and the results have been
made publicly available for a short period of time, including the values of the
quantitative indicators for each applicant. The availability of this wealth of
information provides an opportunity to draw a fairly detailed picture of a
nation-wide evaluation exercise, and to study the impact of the bibliometric
indicators on the qualification results. In this paper we provide a first
account of the Italian ASN from a quantitative point of view. We show that
significant differences exist among scientific disciplines, in particular with
respect to the fraction of qualified applicants, that can not be easily
explained. Furthermore, we describe some issues related to the definition and
use of the bibliometric indicators and thresholds. Our analysis aims at drawing
attention to potential problems that should be addressed by decision-makers in
future ASN rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4088</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4088</id><created>2014-12-12</created><updated>2014-12-15</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Faster Sparse Multivariate Polynomial Interpolation of Straight-Line
  Programs</title><categories>cs.SC cs.DS</categories><comments>33 pages. Submitted for publication</comments><msc-class>68W30 (Primary), 12Y05, 68W20 (Secondary)</msc-class><acm-class>F.2.1; I.1.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Given a straight-line program whose output is a polynomial function of the
inputs, we present a new algorithm to compute a concise representation of that
unknown function. Our algorithm can handle any case where the unknown function
is a multivariate polynomial, with coefficients in an arbitrary finite field,
and with a reasonable number of nonzero terms but possibly very large degree.
It is competitive with previously known sparse interpolation algorithms that
work over an arbitrary finite field, and provides an improvement when there are
a large number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4100</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4100</id><created>2014-12-12</created><authors><author><keyname>Hoske</keyname><forenames>Daniel</forenames></author><author><keyname>Rollin</keyname><forenames>Jonathan</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author><author><keyname>Walzer</keyname><forenames>Stefan</forenames></author></authors><title>Playing weighted Tron on Trees</title><categories>math.CO cs.DM</categories><comments>10 pages, 5 figures</comments><msc-class>91A46</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the weighted version of the Tron game on graphs where two
players, Alice and Bob, each build their own path by claiming one vertex at a
time, starting with Alice. The vertices carry non-negative weights that sum up
to 1 and either player tries to claim a path with larger total weight than the
opponent. We show that if the graph is a tree then Alice can always ensure to
get at most 1/5 less than Bob, and that there exist trees where Bob can ensure
to get at least 1/5 more than Alice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4102</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4102</id><created>2014-12-12</created><authors><author><keyname>Wang</keyname><forenames>Chunyu</forenames></author><author><keyname>Flynn</keyname><forenames>John</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Representing Data by a Mixture of Activated Simplices</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new model which represents data as a mixture of simplices.
Simplices are geometric structures that generalize triangles. We give a simple
geometric understanding that allows us to learn a simplicial structure
efficiently. Our method requires that the data are unit normalized (and thus
lie on the unit sphere). We show that under this restriction, building a model
with simplices amounts to constructing a convex hull inside the sphere whose
boundary facets is close to the data. We call the boundary facets of the convex
hull that are close to the data Activated Simplices. While the total number of
bases used to build the simplices is a parameter of the model, the dimensions
of the individual activated simplices are learned from the data. Simplices can
have different dimensions, which facilitates modeling of inhomogeneous data
sources. The simplicial structure is bounded --- this is appropriate for
modeling data with constraints, such as human elbows can not bend more than 180
degrees. The simplices are easy to interpret and extremes within the data can
be discovered among the vertices. The method provides good reconstruction and
regularization. It supports good nearest neighbor classification and it allows
realistic generative models to be constructed. It achieves state-of-the-art
results on benchmark datasets, including 3D poses and digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4113</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4113</id><created>2014-12-12</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Conformity enhances network reciprocity in evolutionary social dilemmas</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>8 two-column pages, 5 figures; accepted for publication in Journal of
  the Royal Society Interface</comments><journal-ref>J. R. Soc. Interface 12 (2015) 20141299</journal-ref><doi>10.1098/rsif.2014.1299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pursuit of highest payoffs in evolutionary social dilemmas is risky and
sometimes inferior to conformity. Choosing the most common strategy within the
interaction range is safer because it ensures that the payoff of an individual
will not be much lower than average. Herding instincts and crowd behavior in
humans and social animals also compel to conformity on their own right.
Motivated by these facts, we here study the impact of conformity on the
evolution of cooperation in social dilemmas. We show that an appropriate
fraction of conformists within the population introduces an effective surface
tension around cooperative clusters and ensures smooth interfaces between
different strategy domains. Payoff-driven players brake the symmetry in favor
of cooperation and enable an expansion of clusters past the boundaries imposed
by traditional network reciprocity. This mechanism works even under the most
testing conditions, and it is robust against variations of the interaction
network as long as degree-normalized payoffs are applied. Conformity may thus
be beneficial for the resolution of social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4130</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4130</id><created>2014-12-12</created><authors><author><keyname>Blake</keyname><forenames>Christopher</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Energy Consumption of VLSI Decoders</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson's model of VLSI computation relates the energy of a computation to
the product of the circuit area and the number of clock cycles needed to carry
out the computation. It is shown that for any family of circuits implemented
according to this model, using any algorithm that performs decoding of a
codeword passed through a binary erasure channel, as the block length
approaches infinity either (a) the probability of block error is asymptotically
lower bounded by 1/2 or (b) the energy of the computation scales at least as
Omega(n(log n)^(1/2)), and so the energy of successful decoding, per decoded
bit, must scale at least as Omega((log n)^(1/2)). This implies that the average
energy per decoded bit must approach infinity for any sequence of codes that
approaches capacity. The analysis techniques used are then extended to the case
of serial computation, showing that if a circuit is restricted to serial
computation, then as block length approaches infinity, either the block error
probability is lower bounded by 1/2 or the energy scales at least as fast as
Omega(n log(n)). In a very general case that allows for the number of output
pins to vary with block length, it is shown that the average energy per decoded
bit must scale as Omega(n(log n)^(1/5)). A simple example is provided of a
class of circuits performing low-density parity-check decoding whose energy
complexity scales as O(n^2 log log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4141</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4141</id><created>2014-12-12</created><authors><author><keyname>Zhu</keyname><forenames>Kai</forenames></author><author><keyname>Chen</keyname><forenames>Zhen</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Locating Contagion Sources in Networks with Partial Timestamps</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of identifying the contagion source when
partial timestamps of a contagion process are available. We formulate the
source localization problem as a ranking problem on graphs, where infected
nodes are ranked according to their likelihood of being the source. Two ranking
algorithms, cost-based ranking (CR) and tree-based ranking (TR), are proposed
in this paper. Experimental evaluations with synthetic and real-world data show
that our algorithms significantly improve the ranking accuracy compared with
four existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4155</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4155</id><created>2014-12-12</created><updated>2014-12-18</updated><authors><author><keyname>Karawia</keyname><forenames>A. A.</forenames></author></authors><title>On the Inverting of A General Heptadiagonal Matrix</title><categories>math.NA cs.SC</categories><msc-class>15A15, 15A23, 68W30, 11Y05, 33F10</msc-class><acm-class>F.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we developed new numeric and symbolic algorithms to find the
inverse of any nonsingular heptadiagonal matrix. Symbolic algorithm will not
break and it is without setting any restrictive conditions. The computational
cost of our algorithms is $O(n)$. The algorithms are suitable for
implementation using computer algebra system such as MAPLE, MATLAB and
MATHEMATICA. Examples are given to illustrate the efficiency of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4160</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4160</id><created>2014-12-12</created><updated>2015-11-04</updated><authors><author><keyname>Nguyen</keyname><forenames>Dat Quoc</forenames></author><author><keyname>Nguyen</keyname><forenames>Dai Quoc</forenames></author><author><keyname>Pham</keyname><forenames>Son Bao</forenames></author></authors><title>Ripple Down Rules for Question Answering</title><categories>cs.CL cs.IR</categories><comments>V1: 21 pages, 7 figures, 10 tables. V2: 8 figures, 10 tables; shorten
  section 2; change sections 4.3 and 5.1.2. V3: Accepted for publication in the
  Semantic Web journal. V4 (Author's manuscript): camera ready version,
  available from the Semantic Web journal at
  http://www.semantic-web-journal.net</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed a new trend of building ontology-based question
answering systems. These systems use semantic web information to produce more
precise answers to users' queries. However, these systems are mostly designed
for English. In this paper, we introduce an ontology-based question answering
system named KbQAS which, to the best of our knowledge, is the first one made
for Vietnamese. KbQAS employs our question analysis approach that
systematically constructs a knowledge base of grammar rules to convert each
input question into an intermediate representation element. KbQAS then takes
the intermediate representation element with respect to a target ontology and
applies concept-matching techniques to return an answer. On a wide range of
Vietnamese questions, experimental results show that the performance of KbQAS
is promising with accuracies of 84.1% and 82.4% for analyzing input questions
and retrieving output answers, respectively. Furthermore, our question analysis
approach can easily be applied to new domains and new languages, thus saving
time and human effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4166</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4166</id><created>2014-12-12</created><authors><author><keyname>Muralidharan</keyname><forenames>Arjun</forenames></author><author><keyname>Yan</keyname><forenames>Yuan</forenames></author><author><keyname>Mostofi</keyname><forenames>Yasamin</forenames></author></authors><title>Binary Log-Linear Learning with Stochastic Communication Links</title><categories>cs.MA</categories><comments>Double column, 7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider distributed decision-making over stochastic
communication links in multi-agent systems. We show how to extend the current
literature on potential games with binary log-linear learning (which mainly
focuses on ideal communication links) to consider the impact of stochastic
communication channels. More specifically, we derive conditions on the
probability of link connectivity to achieve a target probability for the set of
potential maximizers (in the stationary distribution). Furthermore, our toy
example demonstrates a transition phenomenon for achieving any target
probability for the set of potential maximizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4168</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4168</id><created>2014-12-12</created><authors><author><keyname>Zhao</keyname><forenames>Mo</forenames></author><author><keyname>Blick</keyname><forenames>Robert H.</forenames></author></authors><title>In-vivo Network of Sensors and Actuators</title><categories>cs.SY</categories><comments>16 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An advanced system of sensors/actuators should allow the direct feedback of a
sensed signal into an actuation, e.g., an action potential propagation through
an axon or a special cell activity might be sensed and suppressed by an
actuator through voltage stimulation or chemical delivery. Such a complex
procedure of sensing and stimulation calls for direct communication among these
sensors and actuators. In addition, minimizing the sensor/actuator to the size
of a biological cell can enable the cell-level automatic therapy. For this
objective, we propose such an approach to form a peer-to-peer network of
\emph{in vivo} sensors/actuators (S/As) that can be deployed with or even
inside biological cells. The S/As can communicate with each other via
electromagnetic waves of optical frequencies. In comparison with the comparable
techniques including the radio-frequency identification (RFID) and the wireless
sensor network (WSN), this technique is well adapted for the cell-level
sensing-actuating tasks considering the requirements on size, actuation speed,
signal-collision avoidance, etc.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="69000" completeListSize="102538">1122234|70001</resumptionToken>
</ListRecords>
</OAI-PMH>
