<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:54:15Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|25001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6651</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6651</id><created>2011-09-29</created><authors><author><keyname>Liuni</keyname><forenames>Marco</forenames></author><author><keyname>Balazs</keyname><forenames>Peter</forenames></author><author><keyname>R&#xf6;bel</keyname><forenames>Axel</forenames></author></authors><title>Sound Analysis and Synthesis Adaptive in Time and Two Frequency Bands</title><categories>cs.SD</categories><journal-ref>Proc. of the 14th Int. Conference on Digital Audio Effects
  (DAFx-11), Paris, France, September 19-23, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for sound analysis and resynthesis with local
automatic adaptation of time-frequency resolution. There exists several
algorithms allowing to adapt the analysis window depending on its time or
frequency location; in what follows we propose a method which select the
optimal resolution depending on both time and frequency. We consider an
approach that we denote as analysis-weighting, from the point of view of Gabor
frame theory. We analyze in particular the case of different adaptive
time-varying resolutions within two complementary frequency bands; this is a
typical case where perfect signal reconstruction cannot in general be achieved
with fast algorithms, causing a certain error to be minimized. We provide
examples of adaptive analyses of a music sound, and outline several
possibilities that this work opens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6665</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6665</id><created>2011-09-29</created><updated>2013-02-08</updated><authors><author><keyname>Ahmadi</keyname><forenames>Behzad</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Distributed and Cascade Lossy Source Coding with a Side Information
  &quot;Vending Machine&quot;</title><categories>cs.IT math.IT</categories><comments>33 pages, 7 figures, submitted to IEEE Transactions on Information
  Theory (1st revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source coding with a side information &quot;vending machine&quot; is a recently
proposed framework in which the statistical relationship between the side
information and the source, instead of being given and fixed as in the
classical Wyner-Ziv problem, can be controlled by the decoder. This control
action is selected by the decoder based on the message encoded by the source
node. Unlike conventional settings, the message can thus carry not only
information about the source to be reproduced at the decoder, but also control
information aimed at improving the quality of the side information. In this
paper, the analysis of the trade-offs between rate, distortion and cost
associated with the control actions is extended from the previously studied
point-to-point set-up to two basic multiterminal models. First, a distributed
source coding model is studied, in which two encoders communicate over
rate-limited links to a decoder, whose side information can be controlled. The
control actions are selected by the decoder based on the messages encoded by
both source nodes. For this set-up, inner bounds are derived on the
rate-distortion-cost region for both cases in which the side information is
available causally and non-causally at the decoder. These bounds are shown to
be tight under specific assumptions, including the scenario in which the
sequence observed by one of the nodes is a function of the source observed by
the other and the side information is available causally at the decoder. Then,
a cascade scenario in which three nodes are connected in a cascade and the last
node has controllable side information, is also investigated. For this model,
the rate-distortion-cost region is derived for general distortion requirements
and under the assumption of causal availability of side information at the last
node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6698</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6698</id><created>2011-09-29</created><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Improving Recommendation Quality by Merging Collaborative Filtering and
  Social Relationships</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>6 pages, Proceedings of the 11th International Conference on
  Intelligent Systems Design and Applications</comments><journal-ref>Proceedings of the 11th International Conference on Intelligent
  Systems Design and Applications, pp. 587-592, 2011</journal-ref><doi>10.1109/ISDA.2011.6121719</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix Factorization techniques have been successfully applied to raise the
quality of suggestions generated by Collaborative Filtering Systems (CFSs).
Traditional CFSs based on Matrix Factorization operate on the ratings provided
by users and have been recently extended to incorporate demographic aspects
such as age and gender. In this paper we propose to merge CFS based on Matrix
Factorization and information regarding social friendships in order to provide
users with more accurate suggestions and rankings on items of their interest.
The proposed approach has been evaluated on a real-life online social network;
the experimental results show an improvement against existing CFSs. A detailed
comparison with related literature is also present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6714</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6714</id><created>2011-09-30</created><authors><author><keyname>Zhao</keyname><forenames>Nan</forenames></author></authors><title>A Novel Two-stage Entropy-based Robust Cooperative Spectrum Sensing
  Scheme with Two-bit Decision in Cognitive Radio</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sensing is a key problem in cognitive radio. However, traditional
detectors become ineffective when noise uncertainty is severe. It is shown that
the entropy of Gauss white noise is constant in the frequency domain, and a
robust detector based on the entropy of spectrum amplitude was proposed. In
this paper a novel detector is proposed based on the entropy of spectrum power
density, and its performance is better than the previous scheme with less
computational complexity. Furthermore, to improve the reliability of the
detection, a two-stage entropy-based cooperative spectrum sensing scheme using
two-bit decision is proposed, and simulation results show its superior
performance with relatively low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6717</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6717</id><created>2011-09-30</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Shi</keyname><forenames>Xudong</forenames></author><author><keyname>Wang</keyname><forenames>Liwen</forenames></author></authors><title>Lamarckism and mechanism synthesis: approaching constrained optimization
  with ideas from biology</title><categories>math.OC cs.NE</categories><comments>It have 36 pages, including 7 figures and 7 tables posted in single
  paper</comments><msc-class>90C26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear constrained optimization problems are encountered in many
scientific fields. To utilize the huge calculation power of current computers,
many mathematic models are also rebuilt as optimization problems. Most of them
have constrained conditions which need to be handled. Borrowing biological
concepts, a study is accomplished for dealing with the constraints in the
synthesis of a four-bar mechanism. Biologically regarding the constrained
condition as a form of selection for characteristics of a population, four new
algorithms are proposed, and a new explanation is given for the penalty method.
Using these algorithms, three cases are tested in differential-evolution based
programs. Better, or comparable, results show that the presented algorithms and
methodology may become common means for constraint handling in optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6719</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6719</id><created>2011-09-30</created><authors><author><keyname>Toyota</keyname><forenames>Norihito</forenames></author></authors><title>Separation Number and Generalized Clustering Coefficient in Small World
  Networks based on String Formalism</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reformulated the string formalism given by Aoyama, using an adjacent
matrix of a network and introduced a series of generalized clustering
coefficients based on it. Furthermore we numerically evaluated Milgram
condition proposed by their article in order to explore $q$-$th$ degrees of
separation in scale free networks. In this article, we apply the reformulation
to small world networks and numerically evaluate Milgram condition, especially
the separation number of small world networks and its relation to cycle
structures are discussed. Considering the number of non-zero elements of an
adjacent matrix, the average path length and Milgram condition, we show that
the formalism proposed by us is effective to analyze the six degrees of
separation, especially effective for analyzing the relation between the
separation number and cycle structures in a network.
  By this analysis of small world networks, it proves that a sort of power low
holds between $M_n$, which is a key quantity in Milgram condition, and the
generalized clustering coefficients. This property in small world networks
stands in contrast to that of scale free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6726</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6726</id><created>2011-09-30</created><authors><author><keyname>Rathipriya</keyname><forenames>R.</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author></authors><title>A Fuzzy Co-Clustering approach for Clickstream Data Pattern</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Usage mining is a very important tool to extract the hidden business
intelligence data from large databases. The extracted information provides the
organizations with the ability to produce results more effectively to improve
their businesses and increasing of sales. Co-clustering is a powerful
bipartition technique which identifies group of users associated to group of
web pages. These associations are quantified to reveal the users' interest in
the different web pages' clusters. In this paper, Fuzzy Co-Clustering algorithm
is proposed for clickstream data to identify the subset of users of similar
navigational behavior /interest over a subset of web pages of a website.
Targeting the users group for various promotional activities is an important
aspect of marketing practices. Experiments are conducted on real dataset to
prove the efficiency of proposed algorithm. The results and findings of this
algorithm could be used to enhance the marketing strategy for directing
marketing, advertisements for web based businesses and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6740</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6740</id><created>2011-09-30</created><authors><author><keyname>Correia</keyname><forenames>Anacleto</forenames></author><author><keyname>Abreu</keyname><forenames>Fernando Brito e</forenames></author><author><keyname>Amaral</keyname><forenames>Vasco</forenames></author></authors><title>SLALOM: a Language for SLA specification and monitoring</title><categories>cs.SE</categories><comments>12 pages, 5 figures</comments><acm-class>D.2.1</acm-class><journal-ref>Universidade de Coimbra, actas da confer\^encia INFORUM'2011, ISBN
  978-989-96001-5-7 pp. 556-567, 8-9 Setembro 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IT services provisioning is usually underpinned by service level agreements
(SLAs), aimed at guaranteeing services quality. However, there is a gap between
the customer perspective (business oriented) and that of the service provider
(implementation oriented) that becomes more evident while defining and
monitoring SLAs. This paper proposes a domain specific language (SLA Language
for specificatiOn and Monitoring - SLALOM) to bridge the previous gap. The
first step in SLALOM creation was factoring out common concepts, by composing
the BPMN metamodel with that of the SLA life cycle, as described in ITIL. The
derived metamodel expresses the SLALOM abstract syntax model. The second step
was to write concrete syntaxes targeting different aims, such as SLA
representation in process models. An example of SLALOM's concrete syntax model
instantiation for an IT service sup-ported by self-service financial terminals
is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6757</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6757</id><created>2011-09-30</created><authors><author><keyname>Funder</keyname><forenames>Jakob</forenames></author></authors><title>New entropic uncertainty relations for prime power dimensions</title><categories>quant-ph cs.IT math.IT</categories><comments>26 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of entropic uncertainty relations for prime power
dimensions. In order to improve upon such uncertainty relations for higher
dimensional quantum systems, we derive a tight lower bound amount of entropy
for multiple probability distributions under the constraint that the sum of the
collision probabilities for all distributions is fixed. This is purely a
classical information theoretical result, however using an interesting result
by Larsen \cite{Larsen90} allows us to connect this to an entropic uncertainty
relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6761</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6761</id><created>2011-09-30</created><authors><author><keyname>Alvim</keyname><forenames>M&#xe1;rio S.</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Andr&#xe9;s</keyname><forenames>Miguel E.</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Chatzikokolakis</keyname><forenames>Konstantinos</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>On the relation between Differential Privacy and Quantitative
  Information Flow</title><categories>cs.LO cs.CR</categories><proxy>ccsd</proxy><journal-ref>38th International Colloquium on Automata, Languages and
  Programming - ICALP 2011 6756 (2011) 60-76</journal-ref><doi>10.1007/978-3-642-22012-8_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a notion that has emerged in the community of
statistical databases, as a response to the problem of protecting the privacy
of the database's participants when performing statistical queries. The idea is
that a randomized query satisfies differential privacy if the likelihood of
obtaining a certain answer for a database $x$ is not too different from the
likelihood of obtaining the same answer on adjacent databases, i.e. databases
which differ from $x$ for only one individual. Information flow is an area of
Security concerned with the problem of controlling the leakage of confidential
information in programs and protocols. Nowadays, one of the most established
approaches to quantify and to reason about leakage is based on the R\'enyi min
entropy version of information theory. In this paper, we analyze critically the
notion of differential privacy in light of the conceptual framework provided by
the R\'enyi min information theory. We show that there is a close relation
between differential privacy and leakage, due to the graph symmetries induced
by the adjacency relation. Furthermore, we consider the utility of the
randomized answer, which measures its expected degree of accuracy. We focus on
certain kinds of utility functions called &quot;binary&quot;, which have a close
correspondence with the R\'enyi min mutual information. Again, it turns out
that there can be a tight correspondence between differential privacy and
utility, depending on the symmetries induced by the adjacency relation and by
the query. Depending on these symmetries we can also build an optimal-utility
randomization mechanism while preserving the required level of differential
privacy. Our main contribution is a study of the kind of structures that can be
induced by the adjacency relation and the query, and how to use them to derive
bounds on the leakage and achieve the optimal utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6776</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6776</id><created>2011-09-30</created><authors><author><keyname>Takatsu</keyname><forenames>Asuka</forenames></author></authors><title>Behaviors of $\phi$-exponential distributions in Wasserstein geometry
  and an evolution equation</title><categories>math.MG cs.IT math.IT</categories><comments>13 pages</comments><msc-class>60D05, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\phi$-exponential distribution is a generalization of an exponential
distribution associated to functions $\phi$ in an appropriate class, and the
space of $\phi$-exponential distributions has a dually flat structure. We study
features of the space of $\phi$-exponential distributions, such as the
convexity in Wasserstein geometry and the stability under an evolution
equation. From this study, we provide the new characterizations to the space of
Gaussian measures and the space of $q$-Gaussian measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6794</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6794</id><created>2011-09-30</created><authors><author><keyname>Gabriel</keyname><forenames>Pedro</forenames></author><author><keyname>Goul&#xe3;o</keyname><forenames>Miguel</forenames></author><author><keyname>Amaral</keyname><forenames>Vasco</forenames></author></authors><title>Do Software Languages Engineers Evaluate their Languages?</title><categories>cs.SE</categories><acm-class>D.2.8; D.2.9; D.2.13; D.3.2</acm-class><journal-ref>Proceedings of the XIII Congreso Iberoamericano en &quot;Software
  Engineering&quot; (CIbSE'2010), Universidad del Azuay, ISBN-978-9978-325-10-0,
  Cuenca, Ecuador, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain Specific Languages (DSLs) can contribute to increment productivity,
while reducing the required maintenance and programming expertise. We
hypothesize that Software Languages Engineering (SLE) developers consistently
skip, or relax, Language Evaluation. Based on the experience of engineering
other types of software products, we assume that this may potentially lead to
the deployment of inadequate languages. The fact that the languages already
deal with concepts from the problem domain, and not the solution domain, is not
enough to validate several issues at stake, such as its expressiveness,
usability, effectiveness, maintainability, or even the domain expert's
productivity while using them. We present a systematic review on articles
published in top ranked venues, from 2001 to 2008, which report DSLs'
construction, to characterize the common practice. This work con?rms our
initial hypothesis and lays the ground for the discussion on how to include a
systematic approach to DSL evaluation in the SLE process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6802</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6802</id><created>2011-09-30</created><authors><author><keyname>Goul&#xe3;o</keyname><forenames>Miguel</forenames></author><author><keyname>Abreu</keyname><forenames>Fernando Brito e</forenames></author></authors><title>An overview of metrics-based approaches to support software components
  reusability assessment</title><categories>cs.SE</categories><acm-class>D.2.8; D.2.9</acm-class><journal-ref>&quot;An overview of metrics-based approaches to support software
  components reusability assessment&quot; in Software Quality Measurement: Concepts
  and Approaches, ICFAI Books, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: To present an overview on the current state of the art concerning
metrics-based quality evaluation of software components and component
assemblies. Method: Comparison of several approaches available in the
literature, using a framework comprising several aspects, such as scope,
intent, definition technique, and maturity. Results: The identification of
common shortcomings of current approaches, such as ambiguity in definition,
lack of adequacy of the specifying formalisms and insufficient validation of
current quality models and metrics for software components. Conclusions:
Quality evaluation of components and component-based infrastructures presents
new challenges to the Experimental Software Engineering community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6809</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6809</id><created>2011-09-30</created><authors><author><keyname>Sehati</keyname><forenames>Ali</forenames></author><author><keyname>Talebi</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author></authors><title>NUM-Based Rate Allocation for Streaming Traffic via Sequential Convex
  Programming</title><categories>cs.NI math.OC</categories><comments>6 pages, conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been an increasing demand for ubiquitous streaming
like applications in data networks. In this paper, we concentrate on NUM-based
rate allocation for streaming applications with the so-called S-curve utility
functions. Due to non-concavity of such utility functions, the underlying NUM
problem would be non-convex for which dual methods might become quite useless.
To tackle the non-convex problem, using elementary techniques we make the
utility of the network concave, however this results in reverse-convex
constraints which make the problem non-convex. To deal with such a transformed
NUM, we leverage Sequential Convex Programming (SCP) approach to approximate
the non-convex problem by a series of convex ones. Based on this approach, we
propose a distributed rate allocation algorithm and demonstrate that under mild
conditions, it converges to a locally optimal solution of the original NUM.
Numerical results validate the effectiveness, in terms of tractable convergence
of the proposed rate allocation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6838</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6838</id><created>2011-09-30</created><authors><author><keyname>Nikumbh</keyname><forenames>Sarvesh</forenames></author><author><keyname>Nathaman</keyname><forenames>Joeprakash</forenames></author><author><keyname>Vartak</keyname><forenames>Rahul</forenames></author></authors><title>Distributed Air Traffic Control : A Human Safety Perspective</title><categories>cs.MA cs.AI</categories><comments>Extended Abstract, 3 pages, Accepted at IBM Collaborative Academia
  Research Exchange (I-CARE)-2011, uses ACM-Proceeding style file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issues in air traffic control have so far been addressed with the intent
to improve resource utilization and achieve an optimized solution with respect
to fuel comsumption of aircrafts, efficient usage of the available airspace
with minimal congestion related losses under various dynamic constraints. So
the focus has almost always been more on smarter management of traffic to
increase profits while human safety, though achieved in the process, we
believe, has remained less seriously attended. This has become all the more
important given that we have overburdened and overstressed air traffic
controllers managing hundreds of airports and thousands of aircrafts per day.
  We propose a multiagent system based distributed approach to handle air
traffic ensuring complete human (passenger) safety without removing any humans
(ground controllers) from the loop thereby also retaining the earlier
advantages in the new solution. The detailed design of the agent system, which
will be easily interfacable with the existing environment, is described. Based
on our initial findings from simulations, we strongly believe the system to be
capable of handling the nuances involved, to be extendable and customizable at
any later point in time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6840</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6840</id><created>2011-09-30</created><authors><author><keyname>Mishra</keyname><forenames>Sumita</forenames></author></authors><title>A Novel comprehensive method for real time Video Motion Detection
  Surveillance</title><categories>cs.CV</categories><comments>5 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a comprehensive system for surveillance and monitoring
applications. The development of an efficient real time video motion detection
system is motivated by their potential for deployment in the areas where
security is the main concern. The paper presents a platform for real time video
motion detection and subsequent generation of an alarm condition as set by the
parameters of the control system. The prototype consists of a mobile platform
mounted with RF camera which provides continuous feedback of the environment.
The received visual information is then analyzed by user for appropriate
control action, thus enabling the user to operate the system from a remote
location. The system is also equipped with the ability to process the image of
an object and generate control signals which are automatically transmitted to
the mobile platform to track the object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6841</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6841</id><created>2011-09-30</created><authors><author><keyname>Liang</keyname><forenames>Percy</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author></authors><title>Learning Dependency-Based Compositional Semantics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we want to build a system that answers a natural language question by
representing its semantics as a logical form and computing the answer given a
structured database of facts. The core part of such a system is the semantic
parser that maps questions to logical forms. Semantic parsers are typically
trained from examples of questions annotated with their target logical forms,
but this type of annotation is expensive.
  Our goal is to learn a semantic parser from question-answer pairs instead,
where the logical form is modeled as a latent variable. Motivated by this
challenging learning problem, we develop a new semantic formalism,
dependency-based compositional semantics (DCS), which has favorable linguistic,
statistical, and computational properties. We define a log-linear distribution
over DCS logical forms and estimate the parameters using a simple procedure
that alternates between beam search and numerical optimization. On two standard
semantic parsing benchmarks, our system outperforms all existing
state-of-the-art systems, despite using no annotated logical forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6845</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6845</id><created>2011-09-30</created><updated>2012-02-09</updated><authors><author><keyname>He</keyname><forenames>Fei</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Xiao</keyname><forenames>Limin</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>Optimal Power Allocation for Two-Way Decode-and-Forward OFDM Relay
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 2 figures, accepted by IEEE ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel two-way decode-and-forward (DF) relay strategy
for Orthogonal Frequency Division Multiplexing (OFDM) relay networks. This DF
relay strategy employs multi-subcarrier joint channel coding to leverage
frequency selective fading, and thus can achieve a higher data rate than the
conventional per-subcarrier DF relay strategies. We further propose a
low-complexity, optimal power allocation strategy to maximize the data rate of
the proposed relay strategy. Simulation results suggest that our strategy
obtains a substantial gain over the per-subcarrier DF relay strategies, and
also outperforms the amplify-and-forward (AF) relay strategy in a wide
signal-to-noise-ratio (SNR) region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6851</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6851</id><created>2011-09-30</created><updated>2012-08-17</updated><authors><author><keyname>Hajiesmaili</keyname><forenames>Mohammad Hassan</forenames></author><author><keyname>Sehati</keyname><forenames>Ali</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author><author><keyname>Talebi</keyname><forenames>Mohammad Sadegh</forenames></author></authors><title>Content-Aware Rate Control for Video Transmission with Buffer
  Constraints in Multipath Networks</title><categories>cs.MM cs.NI</categories><comments>This paper has been withdrawn by the last author since there is a
  minor change in the list of authors. In the new version, the last author is
  not included in the paper any longer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being an integral part of the network traffic, nowadays it's vital to design
robust mechanisms to provide QoS for multimedia applications. The main goal of
this paper is to provide an efficient solution to support content-aware video
transmission mechanism with buffer underflow avoidance at the receiver in
multipath networks. Towards this, we introduce a content-aware time-varying
utility function, where the quality impacts of video content is incorporated
into its definition. Using the proposed utility function, we formulate a
multipath Dynamic Network Utility Maximization (DNUM) problem for the rate
allocation of video streams, where it takes into account QoS demand of video
streams in terms of buffer underflow avoidance. Finally, using primal-dual
method, we propose a distributed solution that optimally allocates the shared
bandwidth to video streams. The numerical examples demonstrate the efficacy of
the proposed content-aware rate allocation algorithm for video sources in both
single and multiple path network models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6862</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6862</id><created>2011-09-30</created><authors><author><keyname>S.</keyname><forenames>Sankirti</forenames></author><author><keyname>Kamade</keyname><forenames>P. M.</forenames></author></authors><title>Video OCR for Video Indexing</title><categories>cs.IR cs.MM</categories><comments>3 Pages</comments><acm-class>H.3.3</acm-class><journal-ref>IACSIT International Journal of Engineering and Technology, Vol.3,
  No.3, June 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video OCR is a technique that can greatly help to locate the topics of
interest in video via the automatic extraction and reading of captions and
annotations. Text in video can provide key indexing information. Recognizing
such text for search application is critical. Major difficult problem for
character recognition for videos is degraded and deformated characters, low
resolution characters or very complex background. To tackle the problem
preprocessing on text image plays vital role. Most of the OCR engines are
working on the binary image so to find a better binarization procedure for
image to get a desired result is important.Accurate binarization process
minimizes the error rate of video OCR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6872</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6872</id><created>2011-09-30</created><updated>2011-12-29</updated><authors><author><keyname>Hen</keyname><forenames>Itay</forenames></author><author><keyname>Young</keyname><forenames>A. P.</forenames></author></authors><title>Exponential Complexity of the Quantum Adiabatic Algorithm for certain
  Satisfiability Problems</title><categories>cond-mat.stat-mech cs.CC quant-ph</categories><comments>9 pages, 7 figures</comments><journal-ref>Phys. Rev. E 84, 061152 (2011)</journal-ref><doi>10.1103/PhysRevE.84.061152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the complexity of several constraint satisfaction problems using
the quantum adiabatic algorithm in its simplest implementation. We do so by
studying the size dependence of the gap to the first excited state of &quot;typical&quot;
instances. We find that at large sizes N, the complexity increases
exponentially for all models that we study. We also compare our results against
the complexity of the analogous classical algorithm WalkSAT and show that the
harder the problem is for the classical algorithm the harder it is also for the
quantum adiabatic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6874</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6874</id><created>2011-09-30</created><authors><author><keyname>Bachrach</keyname><forenames>Dustin</forenames></author><author><keyname>Nunu</keyname><forenames>Christopher</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>#h00t: Censorship Resistant Microblogging</title><categories>cs.CR cs.SI</categories><comments>10 pages, 4 figures. keywords: censorship resistance, twitter,
  microblogging, covert channels, group anonymity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microblogging services such as Twitter are an increasingly important way to
communicate, both for individuals and for groups through the use of hashtags
that denote topics of conversation. However, groups can be easily blocked from
communicating through blocking of posts with the given hashtags. We propose
#h00t, a system for censorship resistant microblogging. #h00t presents an
interface that is much like Twitter, except that hashtags are replaced with
very short hashes (e.g., 24 bits) of the group identifier. Naturally, with such
short hashes, hashtags from different groups may collide and #h00t users will
actually seek to create collisions. By encrypting all posts with keys derived
from the group identifiers, #h00t client software can filter out other groups'
posts while making such filtering difficult for the adversary. In essence, by
leveraging collisions, groups can tunnel their posts in other groups' posts. A
censor could not block a given group without also blocking the other groups
with colliding hashtags. We evaluate the feasibility of #h00t through traces
collected from Twitter, showing that a single modern computer has enough
computational throughput to encrypt every tweet sent through Twitter in real
time. We also use these traces to analyze the bandwidth and anonymity tradeoffs
that would come with different variations on how group identifiers are encoded
and hashtags are selected to purposefully collide with one another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6880</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6880</id><created>2011-09-30</created><authors><author><keyname>Fabbri</keyname><forenames>Daniel</forenames></author><author><keyname>LeFevre</keyname><forenames>Kristen</forenames></author></authors><title>Explanation-Based Auditing</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp. 1-12
  (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To comply with emerging privacy laws and regulations, it has become common
for applications like electronic health records systems (EHRs) to collect
access logs, which record each time a user (e.g., a hospital employee) accesses
a piece of sensitive data (e.g., a patient record). Using the access log, it is
easy to answer simple queries (e.g., Who accessed Alice's medical record?), but
this often does not provide enough information. In addition to learning who
accessed their medical records, patients will likely want to understand why
each access occurred. In this paper, we introduce the problem of generating
explanations for individual records in an access log. The problem is motivated
by user-centric auditing applications, and it also provides a novel approach to
misuse detection. We develop a framework for modeling explanations which is
based on a fundamental observation: For certain classes of databases, including
EHRs, the reason for most data accesses can be inferred from data stored
elsewhere in the database. For example, if Alice has an appointment with Dr.
Dave, this information is stored in the database, and it explains why Dr. Dave
looked at Alice's record. Large numbers of data accesses can be explained using
general forms called explanation templates. Rather than requiring an
administrator to manually specify explanation templates, we propose a set of
algorithms for automatically discovering frequent templates from the database
(i.e., those that explain a large number of accesses). We also propose
techniques for inferring collaborative user groups, which can be used to
enhance the quality of the discovered explanations. Finally, we have evaluated
our proposed techniques using an access log and data from the University of
Michigan Health System. Our results demonstrate that in practice we can provide
explanations for over 94% of data accesses in the log.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6881</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6881</id><created>2011-09-30</created><authors><author><keyname>Marcus</keyname><forenames>Adam</forenames></author><author><keyname>Wu</keyname><forenames>Eugene</forenames></author><author><keyname>Karger</keyname><forenames>David</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Miller</keyname><forenames>Robert</forenames></author></authors><title>Human-powered Sorts and Joins</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  13-24 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible
to task people with small jobs, such as labeling images or looking up phone
numbers, via a programmatic interface. MTurk tasks for processing datasets with
humans are currently designed with significant reimplementation of common
workflows and ad-hoc selection of parameters such as price to pay per task. We
describe how we have integrated crowds into a declarative workflow engine
called Qurk to reduce the burden on workflow designers. In this paper, we focus
on how to use humans to compare items for sorting and joining data, two of the
most common operations in DBMSs. We describe our basic query interface and the
user interface of the tasks we post to MTurk. We also propose a number of
optimizations, including task batching, replacing pairwise comparisons with
numerical ratings, and pre-filtering tables before joining them, which
dramatically reduce the overall cost of running sorts and joins on the crowd.
In an experiment joining two sets of images, we reduce the overall cost from
$67 in a naive implementation to about $3, without substantially affecting
accuracy or latency. In an end-to-end experiment, we reduced cost by a factor
of 14.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6882</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6882</id><created>2011-09-30</created><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author><author><keyname>Yi</keyname><forenames>Ke</forenames></author></authors><title>Verifying Computations with Streaming Interactive Proofs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  25-36 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When computation is outsourced, the data owner would like to be assured that
the desired computation has been performed correctly by the service provider.
In theory, proof systems can give the necessary assurance, but prior work is
not sufficiently scalable or practical. In this paper, we develop new proof
protocols for verifying computations which are streaming in nature: the
verifier (data owner) needs only logarithmic space and a single pass over the
input, and after observing the input follows a simple protocol with a prover
(service provider) that takes logarithmic communication spread over a
logarithmic number of rounds. These ensure that the computation is performed
correctly: that the service provider has not made any errors or missed out some
data. The guarantee is very strong: even if the service provider deliberately
tries to cheat, there is only vanishingly small probability of doing so
undetected, while a correct computation is always accepted. We first observe
that some theoretical results can be modified to work with streaming verifiers,
showing that there are efficient protocols for problems in the complexity
classes NP and NC. Our main results then seek to bridge the gap between theory
and practice by developing usable protocols for a variety of problems of
central importance in streaming and database processing. All these problems
require linear space in the traditional streaming model, and therefore our
protocols demonstrate that adding a prover can exponentially reduce the effort
needed by the verifier. Our experimental results show that our protocols are
practical and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6883</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6883</id><created>2011-09-30</created><authors><author><keyname>Lin</keyname><forenames>Dan</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Xiao</keyname><forenames>Lu</forenames></author><author><keyname>Lu</keyname><forenames>Jiaheng</forenames></author></authors><title>A MovingObject Index for Efficient Query Processing with Peer-Wise
  Location Privacy</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  37-48 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing use of location-based services, location privacy attracts
increasing attention from users, industry, and the research community. While
considerable effort has been devoted to inventing techniques that prevent
service providers from knowing a user's exact location, relatively little
attention has been paid to enabling so-called peer-wise privacy--the protection
of a user's location from unauthorized peer users. This paper identifies an
important efficiency problem in existing peer-privacy approaches that simply
apply a filtering step to identify users that are located in a query range, but
that do not want to disclose their location to the querying peer. To solve this
problem, we propose a novel, privacy-policy enabled index called the PEB-tree
that seamlessly integrates location proximity and policy compatibility. We
propose efficient algorithms that use the PEB-tree for processing privacy-aware
range and kNN queries. Extensive experiments suggest that the PEB-tree enables
efficient query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6884</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6884</id><created>2011-09-30</created><authors><author><keyname>Mansour</keyname><forenames>Essam</forenames></author><author><keyname>Allam</keyname><forenames>Amin</forenames></author><author><keyname>Skiadopoulos</keyname><forenames>Spiros</forenames></author><author><keyname>Kalnis</keyname><forenames>Panos</forenames></author></authors><title>ERA: Efficient Serial and Parallel Suffix Tree Construction for Very
  Long Strings</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  49-60 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The suffix tree is a data structure for indexing strings. It is used in a
variety of applications such as bioinformatics, time series analysis,
clustering, text editing and data compression. However, when the string and the
resulting suffix tree are too large to fit into the main memory, most existing
construction algorithms become very inefficient. This paper presents a
disk-based suffix tree construction method, called Elastic Range (ERa), which
works efficiently with very long strings that are much larger than the
available memory. ERa partitions the tree construction process horizontally and
vertically and minimizes I/Os by dynamically adjusting the horizontal
partitions independently for each vertical partition, based on the evolving
shape of the tree and the available memory. Where appropriate, ERa also groups
vertical partitions together to amortize the I/O cost. We developed a serial
version; a parallel version for shared-memory and shared-disk multi-core
systems; and a parallel version for shared-nothing architectures. ERa indexes
the entire human genome in 19 minutes on an ordinary desktop computer. For
comparison, the fastest existing method needs 15 minutes using 1024 CPUs on an
IBM BlueGene supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6885</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6885</id><created>2011-09-30</created><authors><author><keyname>Krueger</keyname><forenames>Jens</forenames></author><author><keyname>Kim</keyname><forenames>Changkyu</forenames></author><author><keyname>Grund</keyname><forenames>Martin</forenames></author><author><keyname>Satish</keyname><forenames>Nadathur</forenames></author><author><keyname>Schwalb</keyname><forenames>David</forenames></author><author><keyname>Chhugani</keyname><forenames>Jatin</forenames></author><author><keyname>Plattner</keyname><forenames>Hasso</forenames></author><author><keyname>Dubey</keyname><forenames>Pradeep</forenames></author><author><keyname>Zeier</keyname><forenames>Alexander</forenames></author></authors><title>Fast Updates on Read-Optimized Databases Using Multi-Core CPUs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  61-72 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Read-optimized columnar databases use differential updates to handle writes
by maintaining a separate write-optimized delta partition which is periodically
merged with the read-optimized and compressed main partition. This merge
process introduces significant overheads and unacceptable downtimes in update
intensive systems, aspiring to combine transactional and analytical workloads
into one system. In the first part of the paper, we report data analyses of 12
SAP Business Suite customer systems. In the second half, we present an
optimized merge process reducing the merge overhead of current systems by a
factor of 30. Our linear-time merge algorithm exploits the underlying high
compute and bandwidth resources of modern multi-core CPUs with
architecture-aware optimizations and efficient parallelization. This enables
compressed in-memory column stores to handle the transactional update rate
required by enterprise applications, while keeping properties of read-optimized
databases for analytic-style queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6886</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6886</id><created>2011-09-30</created><authors><author><keyname>Goyal</keyname><forenames>Amit</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>A Data-Based Approach to Social Influence Maximization</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp.
  73-84 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is the problem of finding a set of users in a social
network, such that by targeting this set, one maximizes the expected spread of
influence in the network. Most of the literature on this topic has focused
exclusively on the social graph, overlooking historical data, i.e., traces of
past action propagations. In this paper, we study influence maximization from a
novel data-based perspective. In particular, we introduce a new model, which we
call credit distribution, that directly leverages available propagation traces
to learn how influence flows in the network and uses this to estimate expected
influence spread. Our approach also learns the different levels of
influenceability of users, and it is time-aware in the sense that it takes the
temporal nature of influence into account. We show that influence maximization
under the credit distribution model is NP-hard and that the function that
defines expected spread under our model is submodular. Based on these, we
develop an approximation algorithm for solving the influence maximization
problem that at once enjoys high accuracy compared to the standard approach,
while being several orders of magnitude faster and more scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6914</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6914</id><created>2011-09-30</created><authors><author><keyname>Del Tedesco</keyname><forenames>Filippo</forenames></author><author><keyname>Hunt</keyname><forenames>Sebastian</forenames></author><author><keyname>Sands</keyname><forenames>David</forenames></author></authors><title>A Semantic Hierarchy for Erasure Policies</title><categories>cs.CR</categories><comments>18 pages, ICISS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of logical data erasure, contrasting with physical
erasure in the same way that end-to-end information flow control contrasts with
access control. We present a semantic hierarchy for erasure policies, using a
possibilistic knowledge-based semantics to define policy satisfaction such that
there is an intuitively clear upper bound on what information an erasure policy
permits to be retained. Our hierarchy allows a rich class of erasure policies
to be expressed, taking account of the power of the attacker, how much
information may be retained, and under what conditions it may be retained.
While our main aim is to specify erasure policies, the semantic framework
allows quite general information-flow policies to be formulated for a variety
of semantic notions of secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6925</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6925</id><created>2011-09-30</created><authors><author><keyname>Adolphs</keyname><forenames>C. P. J.</forenames></author><author><keyname>Berenbrink</keyname><forenames>P.</forenames></author></authors><title>Distributed Selfish Load Balancing with Weights and Speeds</title><categories>cs.DC cs.DS</categories><comments>29 pages, submitted to STACS 2012</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider neighborhood load balancing in the context of
selfish clients. We assume that a network of n processors and m tasks is given.
The processors may have different speeds and the tasks may have different
weights. Every task is controlled by a selfish user. The objective of the user
is to allocate his/her task to a processor with minimum load. We revisit the
concurrent probabilistic protocol introduced in [6], which works in sequential
rounds. In each round every task is allowed to query the load of one randomly
chosen neighboring processor. If that load is smaller the task will migrate to
that processor with a suitably chosen probability. Using techniques from
spectral graph theory we obtain upper bounds on the expected convergence time
towards approximate and exact Nash equilibria that are significantly better
than the previous results in [6]. We show results for uniform tasks on
non-uniform processors and the general case where the tasks have different
weights and the machines have speeds. To the best of our knowledge, these are
the first results for this general setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1109.6926</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1109.6926</id><created>2011-09-30</created><authors><author><keyname>Beyer</keyname><forenames>Dirk</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Keremoglu</keyname><forenames>M. Erkan</forenames></author><author><keyname>Wendler</keyname><forenames>Philipp</forenames></author></authors><title>Conditional Model Checking</title><categories>cs.SE cs.PL</categories><comments>14 pages, 8 figures, 3 tables</comments><report-no>Technical Report, Number MIP-1107, University of Passau, Germany</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software model checking, as an undecidable problem, has three possible
outcomes: (1) the program satisfies the specification, (2) the program does not
satisfy the specification, and (3) the model checker fails. The third outcome
usually manifests itself in a space-out, time-out, or one component of the
verification tool giving up; in all of these failing cases, significant
computation is performed by the verification tool before the failure, but no
result is reported. We propose to reformulate the model-checking problem as
follows, in order to have the verification tool report a summary of the
performed work even in case of failure: given a program and a specification,
the model checker returns a condition P ---usually a state predicate--- such
that the program satisfies the specification under the condition P ---that is,
as long as the program does not leave states in which P is satisfied. We are of
course interested in model checkers that return conditions P that are as weak
as possible. Instead of outcome (1), the model checker will return P = true;
instead of (2), the condition P will return the part of the state space that
satisfies the specification; and in case (3), the condition P can summarize the
work that has been performed by the model checker before space-out, time-out,
or giving up. If complete verification is necessary, then a different
verification method or tool may be used to focus on the states that violate the
condition. We give such conditions as input to a conditional model checker,
such that the verification problem is restricted to the part of the state space
that satisfies the condition. Our experiments show that repeated application of
conditional model checkers, using different conditions, can significantly
improve the verification results, state-space coverage, and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0010</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0010</id><created>2011-09-30</created><authors><author><keyname>Jakeman</keyname><forenames>John D.</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen G.</forenames></author></authors><title>Local and Dimension Adaptive Sparse Grid Interpolation and Quadrature</title><categories>math.NA cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a locally and dimension-adaptive sparse grid method
for interpolation and integration of high-dimensional functions with
discontinuities. The proposed algorithm combines the strengths of the
generalised sparse grid algorithm and hierarchical surplus-guided local
adaptivity. A high-degree basis is used to obtain a high-order method which,
given sufficient smoothness, performs significantly better than the
piecewise-linear basis. The underlying generalised sparse grid algorithm
greedily selects the dimensions and variable interactions that contribute most
to the variability of a function. The hierarchical surplus of points within the
sparse grid is used as an error criterion for local refinement with the aim of
concentrating computational effort within rapidly varying or discontinuous
regions. This approach limits the number of points that are invested in
`unimportant' dimensions and regions within the high-dimensional domain. We
show the utility of the proposed method for non-smooth functions with hundreds
of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0020</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0020</id><created>2011-09-30</created><authors><author><keyname>Y&#x131;lmaz</keyname><forenames>&#xd6;.</forenames></author><author><keyname>Say</keyname><forenames>A. C. C.</forenames></author></authors><title>Causes of Ineradicable Spurious Predictions in Qualitative Simulation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  551-575, 2006</journal-ref><doi>10.1613/jair.2065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently proved that a sound and complete qualitative simulator does
not exist, that is, as long as the input-output vocabulary of the
state-of-the-art QSIM algorithm is used, there will always be input models
which cause any simulator with a coverage guarantee to make spurious
predictions in its output. In this paper, we examine whether a meaningfully
expressive restriction of this vocabulary is possible so that one can build a
simulator with both the soundness and completeness properties. We prove several
negative results: All sound qualitative simulators, employing subsets of the
QSIM representation which retain the operating region transition feature, and
support at least the addition and constancy constraints, are shown to be
inherently incomplete. Even when the simulations are restricted to run in a
single operating region, a constraint vocabulary containing just the addition,
constancy, derivative, and multiplication relations makes the construction of
sound and complete qualitative simulators impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0021</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0021</id><created>2011-09-30</created><authors><author><keyname>Apel</keyname><forenames>Sven</forenames></author><author><keyname>Speidel</keyname><forenames>Hendrik</forenames></author><author><keyname>Wendler</keyname><forenames>Philipp</forenames></author><author><keyname>von Rhein</keyname><forenames>Alexander</forenames></author><author><keyname>Beyer</keyname><forenames>Dirk</forenames></author></authors><title>Feature-Aware Verification</title><categories>cs.SE cs.PL</categories><comments>12 pages, 9 figures, 1 table</comments><report-no>Technical Report, Number MIP-1105, University of Passau, Germany</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software product line is a set of software products that are distinguished
in terms of features (i.e., end-user--visible units of behavior). Feature
interactions ---situations in which the combination of features leads to
emergent and possibly critical behavior--- are a major source of failures in
software product lines. We explore how feature-aware verification can improve
the automatic detection of feature interactions in software product lines.
Feature-aware verification uses product-line verification techniques and
supports the specification of feature properties along with the features in
separate and composable units. It integrates the technique of variability
encoding to verify a product line without generating and checking a possibly
exponential number of feature combinations. We developed the tool suite
SPLverifier for feature-aware verification, which is based on standard
model-checking technology. We applied it to an e-mail system that incorporates
domain knowledge of AT&amp;T. We found that feature interactions can be detected
automatically based on specifications that have only feature-local knowledge,
and that variability encoding significantly improves the verification
performance when proving the absence of interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0023</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0023</id><created>2011-09-30</created><authors><author><keyname>Liu</keyname><forenames>L.</forenames></author><author><keyname>Truszczynski</keyname><forenames>M.</forenames></author></authors><title>Properties and Applications of Programs with Monotone and Convex
  Constraints</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  299-334, 2006</journal-ref><doi>10.1613/jair.2009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study properties of programs with monotone and convex constraints. We
extend to these formalisms concepts and results from normal logic programming.
They include the notions of strong and uniform equivalence with their
characterizations, tight programs and Fages Lemma, program completion and loop
formulas. Our results provide an abstract account of properties of some recent
extensions of logic programming with aggregates, especially the formalism of
lparse programs. They imply a method to compute stable models of lparse
programs by means of off-the-shelf solvers of pseudo-boolean constraints, which
is often much faster than the smodels system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0024</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0024</id><created>2011-09-30</created><authors><author><keyname>Smith</keyname><forenames>S. F.</forenames></author><author><keyname>Streeter</keyname><forenames>M. J.</forenames></author></authors><title>How the Landscape of Random Job Shop Scheduling Instances Depends on the
  Ratio of Jobs to Machines</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  247-287, 2006</journal-ref><doi>10.1613/jair.2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the search landscape of random instances of the job shop
scheduling problem (JSP). Specifically, we investigate how the expected values
of (1) backbone size, (2) distance between near-optimal schedules, and (3)
makespan of random schedules vary as a function of the job to machine ratio
(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we
provide analytical results, while for intermediate values of N/M we perform
experiments. We prove that as N/M approaches 0, backbone size approaches 100%,
while as N/M approaches infinity the backbone vanishes. In the process we show
that as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules
almost surely generate an optimal schedule, providing theoretical evidence of
an &quot;easy-hard-easy&quot; pattern of typical-case instance difficulty in job shop
scheduling. We also draw connections between our theoretical results and the
&quot;big valley&quot; picture of JSP landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0025</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0025</id><created>2011-09-30</created><authors><author><keyname>Nisan</keyname><forenames>N.</forenames></author><author><keyname>Ronen</keyname><forenames>A.</forenames></author></authors><title>Computationally Feasible VCG Mechanisms</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  19-47, 2007</journal-ref><doi>10.1613/jair.2046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major achievement of mechanism design theory is a general method for the
construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When
applying this method to complex problems such as combinatorial auctions, a
difficulty arises: VCG mechanisms are required to compute optimal outcomes and
are, therefore, computationally infeasible. However, if the optimal outcome is
replaced by the results of a sub-optimal algorithm, the resulting mechanism
(termed VCG-based) is no longer necessarily truthful. The first part of this
paper studies this phenomenon in depth and shows that it is near universal.
Specifically, we prove that essentially all reasonable approximations or
heuristics for combinatorial auctions as well as a wide class of cost
minimization problems yield non-truthful VCG-based mechanisms. We generalize
these results for affine maximizers.
  The second part of this paper proposes a general method for circumventing the
above problem. We introduce a modification of VCG-based mechanisms in which the
agents are given a chance to improve the output of the underlying algorithm.
When the agents behave truthfully, the welfare obtained by the mechanism is at
least as good as the one obtained by the algorithms output. We provide a strong
rationale for truth-telling behavior. Our method satisfies individual
rationality as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0026</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0026</id><created>2011-09-30</created><authors><author><keyname>Faltings</keyname><forenames>B.</forenames></author><author><keyname>Pu</keyname><forenames>P.</forenames></author><author><keyname>Viappiani</keyname><forenames>P.</forenames></author></authors><title>Preference-based Search using Example-Critiquing with Suggestions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  465-503, 2006</journal-ref><doi>10.1613/jair.2075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider interactive tools that help users search for their most preferred
item in a large collection of options. In particular, we examine
example-critiquing, a technique for enabling users to incrementally construct
preference models by critiquing example options that are presented to them. We
present novel techniques for improving the example-critiquing technology by
adding suggestions to its displayed options. Such suggestions are calculated
based on an analysis of users current preference model and their potential
hidden preferences. We evaluate the performance of our model-based suggestion
techniques with both synthetic and real users. Results show that such
suggestions are highly attractive to users and can stimulate them to express
more preferences to improve the chance of identifying their most preferred item
by up to 78%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0027</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0027</id><created>2011-09-30</created><updated>2011-10-04</updated><authors><author><keyname>Pineau</keyname><forenames>J.</forenames></author><author><keyname>Gordon</keyname><forenames>G.</forenames></author><author><keyname>Thrun</keyname><forenames>S.</forenames></author></authors><title>Anytime Point-Based Approximations for Large POMDPs</title><categories>cs.AI</categories><proxy>Daniel Bryce</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  335-380, 2006</journal-ref><doi>10.1613/jair.2078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Partially Observable Markov Decision Process has long been recognized as
a rich framework for real-world planning and control problems, especially in
robotics. However exact solutions in this framework are typically
computationally intractable for all but the smallest problems. A well-known
technique for speeding up POMDP solving involves performing value backups at
specific belief points, rather than over the entire belief simplex. The
efficiency of this approach, however, depends greatly on the selection of
points. This paper presents a set of novel techniques for selecting informative
belief points which work well in practice. The point selection procedure is
combined with point-based value backups to form an effective anytime POMDP
algorithm called Point-Based Value Iteration (PBVI). The first aim of this
paper is to introduce this algorithm and present a theoretical analysis
justifying the choice of belief selection technique. The second aim of this
paper is to provide a thorough empirical comparison between PBVI and other
state-of-the-art POMDP methods, in particular the Perseus algorithm, in an
effort to highlight their similarities and differences. Evaluation is performed
using both standard POMDP domains and realistic robotic tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0028</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0028</id><created>2011-09-30</created><authors><author><keyname>Guestrin</keyname><forenames>C.</forenames></author><author><keyname>Hauskrecht</keyname><forenames>M.</forenames></author><author><keyname>Kveton</keyname><forenames>B.</forenames></author></authors><title>Solving Factored MDPs with Hybrid State and Action Variables</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  153-201, 2006</journal-ref><doi>10.1613/jair.2085</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient representations and solutions for large decision problems with
continuous and discrete variables are among the most important challenges faced
by the designers of automated decision support systems. In this paper, we
describe a novel hybrid factored Markov decision process (MDP) model that
allows for a compact representation of these problems, and a new hybrid
approximate linear programming (HALP) framework that permits their efficient
solutions. The central idea of HALP is to approximate the optimal value
function by a linear combination of basis functions and optimize its weights by
linear programming. We analyze both theoretical and computational aspects of
this approach, and demonstrate its scale-up potential on several hybrid
optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0029</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0029</id><created>2011-09-30</created><updated>2011-10-04</updated><authors><author><keyname>Surdeanu</keyname><forenames>M.</forenames></author><author><keyname>Marquez</keyname><forenames>L.</forenames></author><author><keyname>Carreras</keyname><forenames>X.</forenames></author><author><keyname>Comas</keyname><forenames>P. R.</forenames></author></authors><title>Combination Strategies for Semantic Role Labeling</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  105-151, 2007</journal-ref><doi>10.1613/jair.2088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and analyzes a battery of inference models for the
problem of semantic role labeling: one based on constraint satisfaction, and
several strategies that model the inference as a meta-learning problem using
discriminative classifiers. These classifiers are developed with a rich set of
novel features that encode proposition and sentence-level information. To our
knowledge, this is the first work that: (a) performs a thorough analysis of
learning-based inference models for semantic role labeling, and (b) compares
several inference strategies in this context. We evaluate the proposed
inference strategies in the framework of the CoNLL-2005 shared task using only
automatically-generated syntactic information. The extensive experimental
evaluation and analysis indicates that all the proposed inference strategies
are successful -they all outperform the current best results reported in the
CoNLL-2005 evaluation exercise- but each of the proposed approaches has its
advantages and disadvantages. Several important traits of a state-of-the-art
SRL combination strategy emerge from this analysis: (i) individual models
should be combined at the granularity of candidate arguments rather than at the
granularity of complete solutions; (ii) the best combination strategy uses an
inference model based in learning; and (iii) the learning-based inference
benefits from max-margin classifiers and global feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0061</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0061</id><created>2011-09-30</created><authors><author><keyname>Pankov</keyname><forenames>Sergey</forenames></author></authors><title>Learning image transformations without training examples</title><categories>cs.LG cs.CV</categories><comments>15 pages, 1 figure, ISVC11</comments><journal-ref>Proc. 7th International Symposium on Visual Computing, part II, pp
  168-179, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of image transformations is essential for efficient modeling and
learning of visual data. But the class of relevant transformations is large:
affine transformations, projective transformations, elastic deformations, ...
the list goes on. Therefore, learning these transformations, rather than hand
coding them, is of great conceptual interest. To the best of our knowledge, all
the related work so far has been concerned with either supervised or weakly
supervised learning (from correlated sequences, video streams, or
image-transform pairs). In this paper, on the contrary, we present a simple
method for learning affine and elastic transformations when no examples of
these transformations are explicitly given, and no prior knowledge of space
(such as ordering of pixels) is included either. The system has only access to
a moderately large database of natural images arranged in no particular order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0070</identifier>
 <datestamp>2012-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0070</id><created>2011-10-01</created><updated>2012-01-13</updated><authors><author><keyname>Vasantrao</keyname><forenames>Kardile Vilas</forenames></author></authors><title>Enhance accuracy in Software cost and schedule estimation by using
  &quot;Uncertainty Analysis and Assessment&quot; in the system modeling process</title><categories>cs.SE</categories><comments>9 pages,4 fig,4 tabel. arXiv admin note: substantial text overlap
  with arXiv:1108.1461</comments><journal-ref>International Journal of Research and Innovation in Computer
  Engineering (IJRICE) ISSN 2249 - 6580IJRICE - Volume 1, Issue 1, August 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate software cost and schedule estimation are essential for software
project success. Often it referred to as the &quot;black art&quot; because of its
complexity and uncertainty, software estimation is not as difficult or puzzling
as people think. In fact, generating accurate estimates is straightforward-once
you understand the intensity of uncertainty and framework for the modeling
process. The mystery to successful software estimation-distilling academic
information and real-world experience into a practical guide for working
software professionals. Instead of arcane treatises and rigid modeling
techniques, this will guide highlights a proven set of procedures,
understandable formulas, and heuristics that individuals and development teams
can apply to their projects to help achieve estimation proficiency with choose
appropriate development approaches In the early stage of software life cycle
project manager are inefficient to estimate the effort, schedule, cost
estimation and its development approach .This in turn, confuses the manager to
bid effectively on software project and choose incorrect development approach.
That will directly effect on productivity cycle and increase level of
uncertainty. This becomes a strong cause of project failure. So to avoid such
problem if we know level and sources of uncertainty in model design, It will
directive the developer to design accurate software cost and schedule
estimation, which are essential for software project success. However once the
required efforts have estimated, little is done to recalibrate and reduce the
uncertainty of the initial estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0073</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0073</id><created>2011-10-01</created><updated>2011-10-10</updated><authors><author><keyname>Zhou</keyname><forenames>Tianyi</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Hamming Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>33 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Compressed sensing (CS) and 1-bit CS cannot directly recover quantized
signals and require time consuming recovery. In this paper, we introduce
\textit{Hamming compressed sensing} (HCS) that directly recovers a k-bit
quantized signal of dimensional $n$ from its 1-bit measurements via invoking
$n$ times of Kullback-Leibler divergence based nearest neighbor search.
Compared with CS and 1-bit CS, HCS allows the signal to be dense, takes
considerably less (linear) recovery time and requires substantially less
measurements ($\mathcal O(\log n)$). Moreover, HCS recovery can accelerate the
subsequent 1-bit CS dequantizer. We study a quantized recovery error bound of
HCS for general signals and &quot;HCS+dequantizer&quot; recovery error bound for sparse
signals. Extensive numerical simulations verify the appealing accuracy,
robustness, efficiency and consistency of HCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0084</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0084</id><created>2011-10-01</created><updated>2011-11-16</updated><authors><author><keyname>Namboodiri</keyname><forenames>Vishnu</forenames></author><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Wireless Bidirectional Relaying and Latin Squares</title><categories>cs.IT math.IT</categories><comments>20 pages, 30 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of modulation schemes for the physical layer network-coded two way
relaying scenario is considered with the protocol which employs two phases:
Multiple access (MA) Phase and Broadcast (BC) Phase. It was observed by
Koike-Akino et al. that adaptively changing the network coding map used at the
relay according to the channel conditions greatly reduces the impact of
multiple access interference which occurs at the relay during the MA Phase and
all these network coding maps should satisfy a requirement called the {\it
exclusive law}. We highlight the issues associated with the scheme proposed by
Koike-Akino et al. and propose a scheme which solves these issues. We show that
every network coding map that satisfies the exclusive law is representable by a
Latin Square and conversely, and this relationship can be used to get the
network coding maps satisfying the exclusive law. Using the structural
properties of the Latin Squares for a given set of parameters, the problem of
finding all the required maps is reduced to finding a small set of maps for
$M-$PSK constellations. This is achieved using the notions of isotopic and
transposed Latin Squares. Even though, the completability of partially filled
$M \times M$ Latin Square using $M$ symbols is an open problem, two specific
cases where such a completion is always possible are identified and explicit
construction procedures are provided. The Latin Squares constructed using the
first procedure, helps towards reducing the total number of network coding maps
used. The second procedure helps in the construction of certain Latin Squares
for $M$-PSK signal set from the Latin squares obtained for $M/2$-PSK signal
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0100</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0100</id><created>2011-10-01</created><updated>2011-12-07</updated><authors><author><keyname>Jouguet</keyname><forenames>Paul</forenames></author><author><keyname>Kunz-Jacques</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Leverrier</keyname><forenames>Anthony</forenames></author></authors><title>Long Distance Continuous-Variable Quantum Key Distribution with a
  Gaussian Modulation</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, 5 figures, 5 tables</comments><journal-ref>Phys. Rev. A 84, 062317 (2011)</journal-ref><doi>10.1103/PhysRevA.84.062317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We designed high-efficiency error correcting codes allowing to extract an
errorless secret key in a continuous-variable quantum key distribution protocol
using a Gaussian modulation of coherent states and a homodyne detection. These
codes are available for a wide range of signal-to-noise ratios on an AWGN
channel with a binary modulation and can be combined with a multidimensional
reconciliation method proven secure against arbitrary collective attacks. This
improved reconciliation procedure considerably extends the secure range of a
continuous-variable quantum key distribution with a Gaussian modulation, giving
a secret key rate of about 10^{-3} bit per pulse at a distance of 120 km for
reasonable physical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0105</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0105</id><created>2011-10-01</created><authors><author><keyname>Villadsen</keyname><forenames>J&#xf8;rgen</forenames></author><author><keyname>Ettienne</keyname><forenames>Mikko Berggren</forenames></author><author><keyname>Vester</keyname><forenames>Steen</forenames></author></authors><title>Multi-Agent Programming Contest 2011 - The Python-DTU Team</title><categories>cs.MA</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a brief description of the Python-DTU system, including the
overall design, the tools and the algorithms that we plan to use in the agent
contest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0107</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0107</id><created>2011-10-01</created><updated>2012-04-05</updated><authors><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Learning to relate images: Mapping units, complex cells and simultaneous
  eigenspaces</title><categories>cs.CV cs.AI nlin.AO stat.ML</categories><comments>Revised argument in sections 4 and 3.3. Added illustration of
  subspaces (Figure 13). Added inference Equation (Eq. 17)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental operation in many vision tasks, including motion understanding,
stereopsis, visual odometry, or invariant recognition, is establishing
correspondences between images or between images and data from other
modalities. We present an analysis of the role that multiplicative interactions
play in learning such correspondences, and we show how learning and inferring
relationships between images can be viewed as detecting rotations in the
eigenspaces shared among a set of orthogonal matrices. We review a variety of
recent multiplicative sparse coding methods in light of this observation. We
also review how the squaring operation performed by energy models and by models
of complex cells can be thought of as a way to implement multiplicative
interactions. This suggests that the main utility of including complex cells in
computational models of vision may be that they can encode relations not
invariances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0124</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0124</id><created>2011-10-01</created><updated>2012-07-16</updated><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author></authors><title>Capacity Bounds for State-Dependent Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive information-theoretic performance limits for three
classes of two-user state-dependent discrete memoryless broadcast channels,
with noncausal side-information at the encoder. The first class of channels
comprises a sender broadcasting two independent messages to two non-cooperating
receivers; for channels of the second class, each receiver is given the message
it need not decode; and the third class comprises channels where the sender is
constrained to keep each message confidential from the unintended receiver. We
derive inner bounds for all the three classes of channels. For the first and
second class of channels, we discuss the rate penalty on the achievable region
for having to deal with side-information. For channels of third class, we
characterize the rate penalties for having to deal not only with
side-information, but also to satisfy confidentiality constraints. We then
derive outer bounds, where we present an explicit characterization of sum-rate
bounds for the first and third class of channels. For channels of the second
class, we show that our outer bounds are within a fixed gap away from the
achievable rate region, where the gap is independent of the distribution
characterizing this class of channels. The channel models presented in this
paper are useful variants of the classical broadcast channel, and provide
fundamental building blocks for cellular downlink communications with
side-information, such as fading in the wireless medium, interference caused by
neighboring nodes in the network, {\etc}. at the encoder; two-way relay
communications; and secure wireless broadcasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0129</identifier>
 <datestamp>2012-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0129</id><created>2011-10-01</created><updated>2012-01-20</updated><authors><author><keyname>Lu</keyname><forenames>Yuan</forenames></author><author><keyname>Duel-Hallen</keyname><forenames>Alexandra</forenames></author></authors><title>CSI-aided MAC with Multiuser Diversity for Cognitive Radio Networks</title><categories>cs.NI</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio (CR) aims to increase the spectrum utilization by allowing
secondary users (SU) to access unused licensed spectrum bands. To maximize the
throughput given limited sensing capability, SUs need to strike a balance
between sensing the channels that are not heavily used by primary users (PU)
and avoiding collisions with other SUs. To randomize sensing decisions without
resorting to multiuser sensing policies, it is proposed to exploit the
spatially-variant fading channel conditions on different links by adapting the
reward to the channel state information (CSI). Moreover, the proposed
channel-adaptive policy favors links with high achievable transmission rate and
thus further improves the network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0146</identifier>
 <datestamp>2012-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0146</id><created>2011-10-01</created><updated>2012-06-19</updated><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Reputation- and Trust-Based Systems for Wireless Self-organizing
  Networks</title><categories>cs.CR cs.NI</categories><comments>withdrawn by author. arXiv admin note: v1 substantial text overlap
  with arXiv:1012.2529</comments><journal-ref>Book Chapter No 5, pp. 91- 122, in Security of Self-Organizing
  Networks: MANET, WSN, WMN, VANET, Editor: Al-Shakib Khan Pathan, Aurbach
  Publications, CRC Press, Taylor &amp; Francis Group, USA, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional approach of providing network security has been to borrow tools
and mechanisms from cryptography. However, the conventional view of security
based on cryptography alone is not sufficient for the defending against unique
and novel types of misbehavior exhibited by nodes in wireless self-organizing
networks such as mobile ad hoc networks and wireless sensor networks.
Reputation-based frameworks, where nodes maintain reputation of other nodes and
use it to evaluate their trustworthiness, are deployed to provide scalable,
diverse and a generalized approach for countering different types of
misbehavior resulting form malicious and selfish nodes in these networks. In
this chapter, we present a comprehensive discussion on reputation and
trust-based systems for wireless self-organizing networks. Different classes of
reputation system are described along with their unique characteristics and
working principles. A number of currently used reputation systems are
critically reviewed and compared with respect to their effectiveness and
efficiency of performance. Some open problems in the area of reputation and
trust-based system within the domain of wireless self-organizing networks are
also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0147</identifier>
 <datestamp>2012-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0147</id><created>2011-10-01</created><updated>2012-06-19</updated><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Cross-Layer Protocols for Multimedia Communications over Wireless
  Networks</title><categories>cs.NI cs.MM</categories><comments>withdrawn by author. arXiv admin note: v1 substantial text overlap
  with arXiv:1012.2518</comments><journal-ref>Book Chapter in &quot;Advanced Communication Protocol Technologies:
  Solutions, Methods and Applications&quot;, Editors: Katalin Tarnay, Guzstav Adamis
  and Tibor Dulai, pp. 318 - 354, IGI-Global Publishers, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, the Internet throughput, usage and reliability have
increased almost exponentially. The introduction of broadband wireless mobile
ad hoc networks (MANETs) and cellular networks together with increased
computational power have opened the door for a new breed of applications to be
created, namely real-time multimedia applications. Delivering real-time
multimedia traffic over a complex network like the Internet is a particularly
challenging task since these applications have strict quality-of-service (QoS)
requirements on bandwidth, delay, and delay jitter. Traditional Internet
protocol (IP)-based best effort service is not able to meet these stringent
requirements. The time-varying nature of wireless channels and resource
constrained wireless devices make the problem even more difficult. To improve
perceived media quality by end users over wireless Internet, QoS supports can
be addressed in different layers, including application layer, transport layer
and link layer. Cross layer design is a well-known approach to achieve this
adaptation. In cross-layer design, the challenges from the physical wireless
medium and the QoS-demands from the applications are taken into account so that
the rate, power, and coding at the physical (PHY) layer can adapted to meet the
requirements of the applications given the current channel and network
conditions. A number of propositions for cross-layer designs exist in the
literature. In this chapter, an extensive review has been made on these
cross-layer architectures that combine the application-layer, transport layer
and the link layer controls. Particularly, the issues like channel estimation
techniques, adaptive controls at the application and link layers for energy
efficiency, priority based scheduling, transmission rate control at the
transport layer, and adaptive automatic repeat request (ARQ) are discussed in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0150</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0150</id><created>2011-10-02</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Secure and Privacy- Aware Searching in Peer-to-Peer Networks</title><categories>cs.CR cs.NI</categories><comments>17 pages, 11 figures. In the Pre-proceedings of the 6th International
  Workshop on Data Privacy Management (DPM) - colocated with 16th European
  Symposium on Research in Computer Security (ESORICS 2011), Paper ID: 10,
  September 15 - 16, 2011, Leuven, Belgium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing peer-to-peer networks have several problems such as fake content
distribution, free riding, white-washing and poor search scalability, lack of a
robust trust model and absence of user privacy protection mechanism. Although,
several trust management and semantic community-based mechanisms for combating
free riding and distribution of malicious contents have been proposed by some
researchers, most of these schemes lack scalability due to their high
computational, communication and storage overhead. This paper presents a robust
trust management scheme for P2P networks that utilizes topology adaptation by
constructing an overlay of trusted peers where the neighbors are selected based
on their trust ratings and content similarities. While increasing the search
efficiency by intelligently exploiting the formation of semantic community
structures by topology adaptation among the trustworthy peers, the scheme
provides the users a very high level of privacy protection of their usage and
consumption patterns of network resources. Simulation results demonstrate that
the proposed scheme provides efficient searching to good peers while penalizing
the malicious peers by increasing their search times as the network topology
stabilizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0169</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0169</id><created>2011-10-02</created><authors><author><keyname>Beliakov</keyname><forenames>Gleb</forenames></author><author><keyname>Kelarev</keyname><forenames>Andrei</forenames></author><author><keyname>Yearwood</keyname><forenames>John</forenames></author></authors><title>Robust artificial neural networks and outlier detection. Technical
  report</title><categories>math.OC cs.CV cs.NA cs.NE math.NA stat.ME</categories><msc-class>90C26, 65D15</msc-class><doi>10.1080/02331934.2012.674946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large outliers break down linear and nonlinear regression models. Robust
regression methods allow one to filter out the outliers when building a model.
By replacing the traditional least squares criterion with the least trimmed
squares criterion, in which half of data is treated as potential outliers, one
can fit accurate regression models to strongly contaminated data.
High-breakdown methods have become very well established in linear regression,
but have started being applied for non-linear regression only recently. In this
work, we examine the problem of fitting artificial neural networks to
contaminated data using least trimmed squares criterion. We introduce a
penalized least trimmed squares criterion which prevents unnecessary removal of
valid data. Training of ANNs leads to a challenging non-smooth global
optimization problem. We compare the efficiency of several derivative-free
optimization methods in solving it, and show that our approach identifies the
outliers correctly when ANNs are used for nonlinear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0177</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0177</id><created>2011-10-02</created><authors><author><keyname>Abbott</keyname><forenames>Alastair A.</forenames></author><author><keyname>Bechmann</keyname><forenames>Matthias</forenames></author><author><keyname>Calude</keyname><forenames>Cristian S.</forenames></author><author><keyname>Sebald</keyname><forenames>Angelika</forenames></author></authors><title>A Nuclear Magnetic Resonance Implementation of a Classical Deutsch-Jozsa
  Algorithm</title><categories>cs.ET quant-ph</categories><journal-ref>International Journal of Unconventional Computing, 2012, 8,
  161-175</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nuclear magnetic resonance (NMR) has been widely used as a demonstrative
medium for showcasing the ability for quantum computations to outperform
classical ones. A large number of such experiments performed have been
implementations of the Deutsch-Jozsa algorithm. It is known, however, that in
some cases the Deutsch-Jozsa problem can be solved classically using as many
queries to the black-box as in the quantum solution. In this paper we describe
experiments in which we take the contrasting approach of using NMR as a
classical computing medium, treating the nuclear spin vectors classically and
utilising an alternative embedding of bits into the physical medium. This
allows us to determine the actual Boolean function computed by the black-box
for the n=1,2 cases, as opposed to only the nature (balanced or constant) as
conventional quantum algorithms do. Discussion of these experiments leads to
some clarification of the complications surrounding the comparison of different
quantum algorithms, particularly black-box type algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0178</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0178</id><created>2011-10-02</created><authors><author><keyname>Pal</keyname><forenames>Suryakanta</forenames></author><author><keyname>Sahoo</keyname><forenames>Sudhakar</forenames></author><author><keyname>Nayak</keyname><forenames>Birendra Kumar</forenames></author></authors><title>Properties of Carry Value Transformation</title><categories>cs.DM math.DS</categories><comments>8 pages, 2 figures and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of Carry Value Transformation (CVT) is a model of Discrete
Deterministic Dynamical System. In this paper, we have studied some interesting
properties of CVT and proved that (1) the addition of any two non-negative
integers is same as the sum of their CVT and XOR values. (2) While performing
the repeated addition of CVT and XOR of two non-negative integers &quot;a&quot; and &quot;b&quot;
(where a &gt;= b), the number of iterations required to get either CVT=0 or XOR=0
is at most the length of &quot;a&quot; when both are expressed as binary strings. A
similar process of addition of Modified Carry Value Transformation (MCVT) and
XOR requires a maximum of two iterations for MCVT to be zero. (3) An
equivalence relation is defined in the set (Z x Z) which divides the CV table
into disjoint equivalence classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0180</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0180</id><created>2011-10-02</created><authors><author><keyname>Novichkov</keyname><forenames>Gleb</forenames></author></authors><title>An efficient algorithm to find a set of nearest elements in a mesh</title><categories>cs.DS</categories><comments>raw version. to be improved later on</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A linear time algorithm to find a set of nearest elements in a mesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0187</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0187</id><created>2011-10-02</created><authors><author><keyname>Jiang</keyname><forenames>Minghui</forenames></author><author><keyname>Zhang</keyname><forenames>Yong</forenames></author></authors><title>Parameterized complexity in multiple-interval graphs: domination,
  partition, separation, irredundancy</title><categories>cs.CC cs.DM</categories><comments>A preliminary version of this article appeared in two parts in COCOON
  2011 and IPEC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem k-Dominating Set and its several variants including
k-Connected Dominating Set, k-Independent Dominating Set, and k-Dominating
Clique, when parameterized by the solution size k, are W[1]-hard in either
multiple-interval graphs or their complements or both. On the other hand, we
show that these problems belong to W[1] when restricted to multiple-interval
graphs and their complements. This answers an open question of Fellows et al.
In sharp contrast, we show that d-Distance k-Dominating Set for d &gt;= 2 is
W[2]-complete in multiple-interval graphs and their complements. We also show
that k-Perfect Code and d-Distance k-Perfect Code for d &gt;= 2 are W[1]-complete
even in unit 2-track interval graphs. In addition, we present various new
results on the parameterized complexities of k-Vertex Clique Partition and
k-Separating Vertices in multiple-interval graphs and their complements, and
present a very simple alternative proof of the W[1]-hardness of k-Irredundant
Set in general graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0194</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0194</id><created>2011-10-02</created><updated>2011-10-04</updated><authors><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Rate-Dependent Analysis of the Asymptotic Behavior of Channel
  Polarization</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a binary-input memoryless symmetric channel $W$, we consider the
asymptotic behavior of the polarization process in the large block-length
regime when transmission takes place over $W$. In particular, we study the
asymptotics of the cumulative distribution $\mathbb{P}(Z_n \leq z)$, where
$\{Z_n\}$ is the Bhattacharyya process defined from $W$, and its dependence on
the rate of transmission. On the basis of this result, we characterize the
asymptotic behavior, as well as its dependence on the rate, of the block error
probability of polar codes using the successive cancellation decoder. This
refines the original bounds by Ar{\i}kan and Telatar. Our results apply to
general polar codes based on $\ell \times \ell$ kernel matrices.
  We also provide lower bounds on the block error probability of polar codes
using the MAP decoder. The MAP lower bound and the successive cancellation
upper bound coincide when $\ell=2$, but there is a gap for $\ell&gt;2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0196</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0196</id><created>2011-10-02</created><updated>2012-07-05</updated><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author></authors><title>Self-Adjusting Networks to Minimize Expected Path Length</title><categories>cs.NI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a network infrastructure (e.g., data-center or on-chip-network) and a
distribution on the source-destination requests, the expected path (route)
length is an important measure for the performance, efficiency and power
consumption of the network. In this work we initiate a study on self-adjusting
networks: networks that use local-distributed mechanisms to adjust the position
of the nodes (e.g., virtual machines) in the network to best fit the route
requests distribution. Finding the optimal placement of nodes is defined as the
minimum expected path length (MEPL) problem. This is a generalization of the
minimum linear arrangement (MLA) problem where the network infrastructure is a
line and the computation is done centrally. In contrast to previous work, we
study the distributed version and give efficient and simple approximation
algorithms for interesting and practically relevant special cases of the
problem. In particular, we consider grid networks in which the distribution of
requests is a symmetric product distribution. In this setting, we show that a
simple greedy policy of position switching between neighboring nodes to locally
minimize an objective function, achieves good approximation ratios. We are able
to prove this result using the useful notions of expected rank of the
distribution and the expected distance to the center of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0200</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0200</id><created>2011-10-02</created><updated>2011-11-21</updated><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>NP is not AL and P is not NC is not NL is not L</title><categories>cs.CC</categories><comments>7 pages, in English and Japanese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper talk about that NP is not AL and P, P is not NC, NC is not NL, and
NL is not L. The point about this paper is the depend relation of the problem
that need other problem's result to compute it. I show the structure of depend
relation that could divide each complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0204</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0204</id><created>2011-10-02</created><authors><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>A note on Pr\&quot;ufer-like coding and counting forests of uniform
  hypertrees</title><categories>cs.DM</categories><comments>Version 2; 8th International Conference on Computer Science and
  Information Technologies (CSIT 2011), Erevan : Armenia (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents an encoding and a decoding algorithms for a forest of
(labelled) rooted uniform hypertrees and hypercycles in linear time, by using
as few as $n - 2$ integers in the range $[1,n]$. It is a simple extension of
the classical Pr\&quot;{u}fer code for (labelled) rooted trees to an encoding for
forests of (labelled) rooted uniform hypertrees and hypercycles, which allows
to count them up according to their number of vertices, hyperedges and
hypertrees. In passing, we also find Cayley's formula for the number of
(labelled) rooted trees as well as its generalisation to the number of
hypercycles found by Selivanov in the early 70's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0207</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0207</id><created>2011-10-02</created><authors><author><keyname>Tamayo</keyname><forenames>Alain</forenames></author><author><keyname>Granell</keyname><forenames>Carlos</forenames></author><author><keyname>Huerta</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Analysing complexity of XML Schemas in geospatial web services</title><categories>cs.DB</categories><comments>9 pages, 10 tables, 4 figures; COM.Geo '11 Proceedings of the 2nd
  International Conference on Computing for Geospatial Research &amp; Applications,
  ACM, no. 17, 2011</comments><acm-class>D.2.8</acm-class><journal-ref>COM.Geo '11 Proceedings of the 2nd International Conference on
  Computing for Geospatial Research &amp; Applications, ACM, no. 17, 2011</journal-ref><doi>10.1145/1999320.1999337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML Schema is the language used to define the structure of messages exchanged
between OGC-based web service clients and providers. The size of these schemas
has been growing with time, reaching a state that makes its understanding and
effective application a hard task. A first step to cope with this situation is
to provide different ways to measure the complexity of the schemas. In this
regard, we present in this paper an analysis of the complexity of XML schemas
in OGC web services. We use a group of metrics found in the literature and
introduce new metrics to measure size and/or complexity of these schemas. The
use of adequate metrics allows us to quantify the complexity, quality and other
properties of the schemas, which can be very useful in different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0209</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0209</id><created>2011-10-02</created><authors><author><keyname>Tamayo</keyname><forenames>Alain</forenames></author><author><keyname>Granell</keyname><forenames>Carlos</forenames></author><author><keyname>Huerta</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Dealing with large schema sets in mobile SOS-based applications</title><categories>cs.DB</categories><comments>9 pages, 2 tables, 7 figures</comments><acm-class>I.7.2</acm-class><journal-ref>COM.Geo '11 Proceedings of the 2nd International Conference on
  Computing for Geospatial Research &amp; Applications, ACM, no 16, 2011</journal-ref><doi>10.1145/1999320.1999336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the adoption of OGC Web Services for server, desktop and web
applications has been successful, its penetration in mobile devices has been
slow. One of the main reasons is the performance problems associated with XML
processing as it consumes a lot of memory and processing time, which are scarce
resources in a mobile device. In this paper we propose an algorithm to generate
efficient code for XML data binding for mobile SOS-based applications. The
algorithm take advantage of the fact that individual implementations use only
some portions of the standards' schemas, which allows the simplification of
large XML schema sets in an application-specific manner by using a subset of
XML instance files conforming to these schemas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0214</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0214</id><created>2011-10-02</created><authors><author><keyname>Iqbal</keyname><forenames>Ridwan Al</forenames></author></authors><title>Eclectic Extraction of Propositional Rules from Neural Networks</title><categories>cs.LG cs.AI cs.CV cs.NE</categories><comments>ICCIT 2011, Dhaka, Bangladesh</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Neural Network is among the most popular algorithm for supervised
learning. However, Neural Networks have a well-known drawback of being a &quot;Black
Box&quot; learner that is not comprehensible to the Users. This lack of transparency
makes it unsuitable for many high risk tasks such as medical diagnosis that
requires a rational justification for making a decision. Rule Extraction
methods attempt to curb this limitation by extracting comprehensible rules from
a trained Network. Many such extraction algorithms have been developed over the
years with their respective strengths and weaknesses. They have been broadly
categorized into three types based on their approach to use internal model of
the Network. Eclectic Methods are hybrid algorithms that combine the other
approaches to attain more performance. In this paper, we present an Eclectic
method called HERETIC. Our algorithm uses Inductive Decision Tree learning
combined with information of the neural network structure for extracting
logical rules. Experiments and theoretical analysis show HERETIC to be better
in terms of speed and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0215</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0215</id><created>2011-10-02</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Completion Time in Broadcast Channel and Interference Channel</title><categories>cs.IT math.IT</categories><comments>presented at Allerton Conference on Communication, Control, and
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-user channel, completion time refers to the number of channel uses
required for users, each with some given fixed bit pool, to complete the
transmission of all their data bits. This paper extends the information
theoretic formulation of multi-access completion time to broadcast channel and
interference channel, enabling us to obtain the so-called completion time
region (CTR), which, analogous to capacity region, characterizes all possible
trade-offs between users' completion times. Specifically, for Gaussian
broadcast channel (GBC) and Gaussian interference channel (GIC) in the
strong/very strong regime, the exact CTR is obtained. For GIC in the weak/mixed
regime, an achievable CTR based on the Etkin-Tse-Wang scheme and an outer-bound
are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0235</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0235</id><created>2011-10-02</created><authors><author><keyname>Cordero</keyname><forenames>Pablo</forenames></author><author><keyname>Lucks</keyname><forenames>Julius</forenames></author><author><keyname>Das</keyname><forenames>Rhiju</forenames></author></authors><title>The Stanford RNA Mapping Database for sharing and visualizing RNA
  structure mapping experiments</title><categories>q-bio.BM cs.DB</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have established an RNA Mapping Database (RMDB) to enable a new generation
of structural, thermodynamic, and kinetic studies from quantitative
single-nucleotide-resolution RNA structure mapping (freely available at
http://rmdb.stanford.edu). Chemical and enzymatic mapping is a rapid, robust,
and widespread approach to RNA characterization. Since its recent coupling with
high-throughput sequencing techniques, accelerated software pipelines, and
large-scale mutagenesis, the volume of mapping data has greatly increased, and
there is a critical need for a database to enable sharing, visualization, and
meta-analyses of these data. Through its on-line front-end, the RMDB allows
users to explore single-nucleotide-resolution chemical accessibility data in
heat-map, bar-graph, and colored secondary structure graphics; to leverage
these data to generate secondary structure hypotheses; and to download the data
in standardized and computer-friendly files, including the RDAT and
community-consensus SNRNASM formats. At the time of writing, the database
houses 38 entries, describing 2659 RNA sequences and comprising 355,084 data
points, and is growing rapidly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0244</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0244</id><created>2011-10-02</created><updated>2011-10-04</updated><authors><author><keyname>Appaiah</keyname><forenames>Kumar</forenames></author><author><keyname>Zisman</keyname><forenames>Sagi</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author><author><keyname>Bank</keyname><forenames>Seth R.</forenames></author></authors><title>Analysis of Laser &amp; Detector Placement in MIMO Multimode Optical Fiber
  Systems</title><categories>physics.optics cs.IT math.IT</categories><comments>This paper has been withdrawn due to incomplete analysis of the model
  considered. A suitably altered version shall follow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimode fibers (MMFs) offer a cost-effective connection solution for small
and medium length networks. However, data rates through multimode fibers are
traditionally limited by modal dispersion. Signal processing and Multiple-Input
Multiple-Output (MIMO) have been shown to be effective at combating these
limitations, but device design for the specific purpose of MIMO in MMFs is
still an open issue. This paper utilizes a statistical field propagation model
for MMFs to aid the analysis and designs of MMF laser and detector arrays, and
aims to improve data rates of the fiber. Simulations reveal that optimal device
designs could possess 2-3 times the data carrying capacity of suboptimal ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0248</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0248</id><created>2011-10-02</created><authors><author><keyname>Cao</keyname><forenames>Yongzhi</forenames></author><author><keyname>Wang</keyname><forenames>Huaiqing</forenames></author><author><keyname>Sun</keyname><forenames>Sherry X.</forenames></author><author><keyname>Chen</keyname><forenames>Guoqing</forenames></author></authors><title>A Behavioral Distance for Fuzzy-Transition Systems</title><categories>cs.AI</categories><comments>12 double column pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In contrast to the existing approaches to bisimulation for fuzzy systems, we
introduce a behavioral distance to measure the behavioral similarity of states
in a nondeterministic fuzzy-transition system. This behavioral distance is
defined as the greatest fixed point of a suitable monotonic function and
provides a quantitative analogue of bisimilarity. The behavioral distance has
the important property that two states are at zero distance if and only if they
are bisimilar. Moreover, for any given threshold, we find that states with
behavioral distances bounded by the threshold are equivalent. In addition, we
show that two system combinators---parallel composition and product---are
non-expansive with respect to our behavioral distance, which makes
compositional verification possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0252</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0252</id><created>2011-10-02</created><authors><author><keyname>Yedla</keyname><forenames>Arvind</forenames></author><author><keyname>Nguyen</keyname><forenames>Phong S.</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Universal Codes for the Gaussian MAC via Spatial Coupling</title><categories>cs.IT math.IT</categories><comments>8 pages, to appear in proceedings of Allerton 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider transmission of two independent and separately encoded sources
over a two-user binary-input Gaussian multiple-access channel. The channel
gains are assumed to be unknown at the transmitter and the goal is to design an
encoder-decoder pair that achieves reliable communication for all channel gains
where this is theoretically possible. We call such a system \emph{universal}
with respect to the channel gains.
  Kudekar et al. recently showed that terminated low-density parity-check
convolutional codes (a.k.a. spatially-coupled low-density parity-check
ensembles) have belief-propagation thresholds that approach their maximum
a-posteriori thresholds. This was proven for binary erasure channels and shown
empirically for binary memoryless symmetric channels. It was conjectured that
the principle of spatial coupling is very general and the phenomenon of
threshold saturation applies to a very broad class of graphical models. In this
work, we derive an area theorem for the joint decoder and empirically show that
threshold saturation occurs for this problem. As a result, we demonstrate
near-universal performance for this problem using the proposed
spatially-coupled coding system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0259</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0259</id><created>2011-10-02</created><updated>2013-04-25</updated><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Fixed-Parameter Tractability of Directed Multiway Cut Parameterized by
  the Size of the Cutset</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed graph $G$, a set of $k$ terminals and an integer $p$, the
\textsc{Directed Vertex Multiway Cut} problem asks if there is a set $S$ of at
most $p$ (nonterminal) vertices whose removal disconnects each terminal from
all other terminals. \textsc{Directed Edge Multiway Cut} is the analogous
problem where $S$ is a set of at most $p$ edges. These two problems indeed are
known to be equivalent. A natural generalization of the multiway cut is the
\emph{multicut} problem, in which we want to disconnect only a set of $k$ given
pairs instead of all pairs. Marx (Theor. Comp. Sci. 2006) showed that in
undirected graphs multiway cut is fixed-parameter tractable (FPT) parameterized
by $p$. Marx and Razgon (STOC 2011) showed that undirected multicut is FPT and
directed multicut is W[1]-hard parameterized by $p$. We complete the picture
here by our main result which is that both \textsc{Directed Vertex Multiway
Cut} and \textsc{Directed Edge Multiway Cut} can be solved in time
$2^{2^{O(p)}}n^{O(1)}$, i.e., FPT parameterized by size $p$ of the cutset of
the solution. This answers an open question raised by Marx (Theor. Comp. Sci.
2006) and Marx and Razgon (STOC 2011). It follows from our result that
\textsc{Directed Multicut} is FPT for the case of $k=2$ terminal pairs, which
answers another open problem raised in Marx and Razgon (STOC 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0264</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0264</id><created>2011-10-03</created><authors><author><keyname>Li</keyname><forenames>Hanxi</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Gao</keyname><forenames>Yongsheng</forenames></author></authors><title>Face Recognition using Optimal Representation Ensemble</title><categories>cs.CV</categories><comments>36-page draft for IEEE Transactions on Image Processing (TIP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the face recognizers based on linear representations have been
shown to deliver state-of-the-art performance. In real-world applications,
however, face images usually suffer from expressions, disguises and random
occlusions. The problematic facial parts undermine the validity of the
linear-subspace assumption and thus the recognition performance deteriorates
significantly. In this work, we address the problem in a
learning-inference-mixed fashion. By observing that the linear-subspace
assumption is more reliable on certain face patches rather than on the holistic
face, some Bayesian Patch Representations (BPRs) are randomly generated and
interpreted according to the Bayes' theory. We then train an ensemble model
over the patch-representations by minimizing the empirical risk w.r.t the
&quot;leave-one-out margins&quot;. The obtained model is termed Optimal Representation
Ensemble (ORE), since it guarantees the optimality from the perspective of
Empirical Risk Minimization. To handle the unknown patterns in test faces, a
robust version of BPR is proposed by taking the non-face category into
consideration. Equipped with the Robust-BPRs, the inference ability of ORE is
increased dramatically and several record-breaking accuracies (99.9% on Yale-B
and 99.5% on AR) and desirable efficiencies (below 20 ms per face in Matlab)
are achieved. It also overwhelms other modular heuristics on the faces with
random occlusions, extreme expressions and disguises. Furthermore, to
accommodate immense BPRs sets, a boosting-like algorithm is also derived. The
boosted model, a.k.a Boosted-ORE, obtains similar performance to its prototype.
Besides the empirical superiorities, two desirable features of the proposed
methods, namely, the training-determined model-selection and the
data-weight-free boosting procedure, are also theoretically verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0271</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0271</id><created>2011-10-03</created><authors><author><keyname>Martin-Delgado</keyname><forenames>Miguel-Angel</forenames></author></authors><title>Alan Turing and the Origins of Complexity</title><categories>cs.CC cond-mat.stat-mech quant-ph</categories><comments>Invited contribution to 'ARBOR: scientific journal of CSIC' special
  edition devoted to commemorate the Year of Alan Turing. This special issue is
  entitled &quot;The Legacy of Alan Turing&quot;. Coordinators: Manuel de Leon, Alberto
  Ibort and David Martin de Diego</comments><journal-ref>ARBOR Vol 189, No 764 (2013), a083</journal-ref><doi>10.3989/arbor.2013.i764.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 75th anniversary of Turing's seminal paper and his centennial year
anniversary occur in 2011 and 2012, respectively. It is natural to review and
assess Turing's contributions in diverse fields in the light of new
developments that his thoughts has triggered in many scientific communities.
Here, the main idea is to discuss how the work of Turing allows us to change
our views on the foundations of Mathematics, much like quantum mechanics
changed our conception of the world of Physics. Basic notions like
computability and universality are discussed in a broad context, making special
emphasis on how the notion of complexity can be given a precise meaning after
Turing, i.e., not just qualitative but also quantitative. Turing's work is
given some historical perspective with respect to some of his precursors,
contemporaries and mathematicians who took up his ideas farther.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0279</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0279</id><created>2011-10-03</created><updated>2012-02-09</updated><authors><author><keyname>Cheraghchi</keyname><forenames>Mahdi</forenames></author></authors><title>Coding-Theoretic Methods for Sparse Recovery</title><categories>cs.IT cs.DM math.IT</categories><comments>Added Lemma 34 in the first revision. Original version in Proceedings
  of the Allerton Conference on Communication, Control and Computing, September
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review connections between coding-theoretic objects and sparse learning
problems. In particular, we show how seemingly different combinatorial objects
such as error-correcting codes, combinatorial designs, spherical codes,
compressed sensing matrices and group testing designs can be obtained from one
another. The reductions enable one to translate upper and lower bounds on the
parameters attainable by one object to another. We survey some of the
well-known reductions in a unified presentation, and bring some existing gaps
to attention. New reductions are also introduced; in particular, we bring up
the notion of minimum &quot;L-wise distance&quot; of codes and show that this notion
closely captures the combinatorial structure of RIP-2 matrices. Moreover, we
show how this weaker variation of the minimum distance is related to
combinatorial list-decoding properties of codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0289</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0289</id><created>2011-10-03</created><authors><author><keyname>Kembellec</keyname><forenames>G&#xe9;rald</forenames></author></authors><title>Repr\'esentation de donn\'ees et m\'etadonn\'ees dans une biblioth\`eque
  virtuelle pour une ad\'equation avec l'usager et les outils de glanage ou
  moissonnage scientifique</title><categories>cs.IR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vehicles for official knowledge, as well as university libraries, suffer
from an increasingly visible lack of interest. This is due to the advent of
fully digital practices. By studying the psychological and cognitive models in
information retrieval initiated in the 1980s, it is possible to use these
theories and apply them practically to the Information Retrieval System, taking
into account the requirements of virtual libraries. New metadata standards
along with modern tools that help managing references should help automating
the process of scientific research. We offer a practical implementation of the
given theories to test them when they are applied to the information retrieval
in computer sciences. This case under study will highlight good practices in
gleaning and harvesting scientific literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0305</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0305</id><created>2011-10-03</created><updated>2013-02-01</updated><authors><author><keyname>Mirshahvalad</keyname><forenames>Atieh</forenames></author><author><keyname>Lindholm</keyname><forenames>Johan</forenames></author><author><keyname>Derlen</keyname><forenames>Mattias</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Significant communities in large sparse networks</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 7 figures</comments><doi>10.1371/journal.pone.0033721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers use community-detection algorithms to reveal large-scale
organization in biological and social networks, but community detection is
useful only if the communities are significant and not a result of noisy data.
To assess the statistical significance of the network communities, or the
robustness of the detected structure, one approach is to perturb the network
structure by removing links and measure how much the communities change.
However, perturbing sparse networks is challenging because they are inherently
sensitive; they shatter easily if links are removed. Here we propose a simple
method to perturb sparse networks and assess the significance of their
communities. We generate resampled networks by adding extra links based on
local information, then we aggregate the information from multiple resampled
networks to find a coarse-grained description of significant clusters. In
addition to testing our method on benchmark networks, we use our method on the
sparse network of the European Court of Justice (ECJ) case law, to detect
significant and insignificant areas of law. We use our significance analysis to
draw a map of the ECJ case law network that reveals the relations between the
areas of law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0310</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0310</id><created>2011-10-03</created><authors><author><keyname>Vangala</keyname><forenames>Harish</forenames></author><author><keyname>Meshram</keyname><forenames>Rahul</forenames></author><author><keyname>Sharma</keyname><forenames>Prof. Vinod</forenames></author></authors><title>Joint Routing, Scheduling And Power Control For Multihop Wireless
  Networks With Multiple Antennas</title><categories>cs.NI stat.CO</categories><comments>Submitted to NCC-2012. First Draft is here. Final version has many
  changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Joint Routing, Scheduling and Power-control (JRSP)
problem for multihop wireless networks (MHWN) with multiple antennas. We extend
the problem and a (sub-optimal) heuristic solution method for JRSP in MHWN with
single antennas. We present an iterative scheme to calculate link
capacities(achievable rates) in the interference environment of the network
using SINR model. We then present the algorithm for solving the JRSP problem.
This completes a feasible system model for MHWN when nodes have multiple
antennas. We show that the gain we achieve by using multiple antennas in the
network is linear both in optimal performance as well as heuristic algorithmic
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0334</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0334</id><created>2011-10-03</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, IUF</affiliation></author></authors><title>A Taxonomy of Daemons in Self-stabilization</title><categories>cs.DC cs.DM</categories><comments>26 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey existing scheduling hypotheses made in the literature in
self-stabilization, commonly referred to under the notion of daemon. We show
that four main characteristics (distribution, fairness, boundedness, and
enabledness) are enough to encapsulate the various differences presented in
existing work. Our naming scheme makes it easy to compare daemons of particular
classes, and to extend existing possibility or impossibility results to new
daemons. We further examine existing daemon transformer schemes and provide the
exact transformed characteristics of those transformers in our taxonomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0336</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0336</id><created>2011-10-03</created><authors><author><keyname>Kembellec</keyname><forenames>G&#xe9;rald</forenames><affiliation>LIASD</affiliation></author><author><keyname>Saleh</keyname><forenames>Imad</forenames><affiliation>LIASD</affiliation></author><author><keyname>Sauvaget</keyname><forenames>Catherine</forenames><affiliation>LIASD</affiliation></author></authors><title>OntologyNavigator: WEB 2.0 scalable ontology based CLIR portal to IT
  scientific corpus for researchers</title><categories>cs.IR cs.DL cs.HC</categories><comments>International Journal of Design Sciences and Technology 16, 2 (2009)
  http://europia.org/IJDST/vol16/IJDST_V16N2_2009_paper%203.pdf</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents the architecture used in the ongoing OntologyNavigator
project. It is a research tool to help advanced learners to find adapted IT
papers to create scientific bibliographies. The purpose is the use of an IT
representation as educational research software for researchers. We use an
ontology based on the ACM's Computing Classification System in order to find
scientific papers directly related to the new researcher's domain without any
formal request. An ontology translation in French is automatically proposed and
can be based on Web 2.0 enhanced by a community of users. A visualization and
navigation model is proposed to make it more accessible and examples are given
to show the interface of the tool. This model offers the possibility of cross
language query. Users deeply interact with the translation by providing
alternative translation of the node label. Customers also enrich the ontology
node labels with implicit descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0341</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0341</id><created>2011-10-03</created><authors><author><keyname>Bazgan</keyname><forenames>Cristina</forenames></author><author><keyname>Chopin</keyname><forenames>Morgan</forenames></author><author><keyname>Ries</keyname><forenames>Bernard</forenames></author></authors><title>The firefighter problem with more than one firefighter on trees</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the complexity of the firefighter problem and related
problems on trees when more than one firefighter is available at each time
step, and answer several open questions of Finbow and MacGillivray 2009. More
precisely, when $b \geq 2$ firefighters are allowed at each time step, the
problem is NP-complete for trees of maximum degree $b+2$ and polynomial-time
solvable for trees of maximum degree $b+2$ when the fire breaks out at a vertex
of degree at most $b+1$. Moreover we present a polynomial-time algorithm for a
subclass of trees, namely $k$-caterpillars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0347</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0347</id><created>2011-10-03</created><authors><author><keyname>Singh</keyname><forenames>P.</forenames></author><author><keyname>Sreenivasan</keyname><forenames>S.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Accelerating consensus on co-evolving networks: the effect of committed
  individuals</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. E 85, 046104 (2012)</journal-ref><doi>10.1103/PhysRevE.85.046104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are not static but rather constantly evolve in time. One of
the elements thought to drive the evolution of social network structure is
homophily - the need for individuals to connect with others who are similar to
them. In this paper, we study how the spread of a new opinion, idea, or
behavior on such a homophily-driven social network is affected by the changing
network structure. In particular, using simulations, we study a variant of the
Axelrod model on a network with a homophilic rewiring rule imposed. First, we
find that the presence of homophilic rewiring within the network, in general,
impedes the reaching of consensus in opinion, as the time to reach consensus
diverges exponentially with network size $N$. We then investigate whether the
introduction of committed individuals who are rigid in their opinion on a
particular issue, can speed up the convergence to consensus on that issue. We
demonstrate that as committed agents are added, beyond a critical value of the
committed fraction, the consensus time growth becomes logarithmic in network
size $N$. Furthermore, we show that slight changes in the interaction rule can
produce strikingly different results in the scaling behavior of $T_c$. However,
the benefit gained by introducing committed agents is qualitatively preserved
across all the interaction rules we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0360</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0360</id><created>2011-10-03</created><authors><author><keyname>Jain</keyname><forenames>Aanchal</forenames></author><author><keyname>Richariya</keyname><forenames>Vineet</forenames></author></authors><title>Implementing a Web Browser with Phishing Detection Techniques</title><categories>cs.CR</categories><comments>3 Pages; (WCSIT), ISSN: 2221-0741</comments><journal-ref>World of Computer Science and Information Technology Journal, Vol.
  1, No. 7, 289-291, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0367</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0367</id><created>2011-10-03</created><authors><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Distributed Maximal Matching: Greedy is Optimal</title><categories>cs.DC cs.CC</categories><comments>1+15 pages</comments><doi>10.1145/2332432.2332464</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed algorithms that find a maximal matching in an anonymous,
edge-coloured graph. If the edges are properly coloured with $k$ colours, there
is a trivial greedy algorithm that finds a maximal matching in $k-1$
synchronous communication rounds. The present work shows that the greedy
algorithm is optimal in the general case: any algorithm that finds a maximal
matching in anonymous, $k$-edge-coloured graphs requires $k-1$ rounds.
  If we focus on graphs of maximum degree $\Delta$, it is known that a maximal
matching can be found in $O(\Delta + \log^* k)$ rounds, and prior work implies
a lower bound of $\Omega(\polylog(\Delta) + \log^* k)$ rounds. Our work closes
the gap between upper and lower bounds: the complexity is $\Theta(\Delta +
\log^* k)$ rounds. To our knowledge, this is the first linear-in-$\Delta$ lower
bound for the distributed complexity of a classical graph problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0376</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0376</id><created>2011-10-03</created><authors><author><keyname>Saavedra</keyname><forenames>Serguei</forenames></author><author><keyname>Reed-Tsochas</keyname><forenames>Felix</forenames></author><author><keyname>Uzzi</keyname><forenames>Brian</forenames></author></authors><title>Common Organizing Mechanisms in Ecological and Socio-economic Networks</title><categories>physics.soc-ph cs.SI physics.data-an q-bio.PE</categories><comments>In F. Reed-Tsochas and N. Johnson (eds.) Complex Systems and
  Interdisciplinary Sciences. London: World Scientific Publishing (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work has shown that species interacting in an ecosystem and actors
transacting in an economic context may have notable similarities in behavior.
However, the specific mechanism that may underlie similarities in nature and
human systems has not been analyzed. Building on stochastic food-web models, we
propose a parsimonious bipartite-cooperation model that reproduces the key
features of mutualistic networks - degree distribution, nestedness and
modularity -- for both ecological networks and socio-economic networks. Our
analysis uses two diverse networks. Mutually-beneficial interactions between
plants and their pollinators, and cooperative economic exchanges between
designers and their contractors. We find that these mutualistic networks share
a key hierarchical ordering of their members, along with an exponential
constraint in the number and type of partners they can cooperate with. We use
our model to show that slight changes in the interaction constraints can
produce either extremely nested or random structures, revealing that these
constraints play a key role in the evolution of mutualistic networks. This
could also encourage a new systematic approach to study the functional and
structural properties of networks. The surprising correspondence across
mutualistic networks suggests their broadly representativeness and their
potential role in the productive organization of exchange systems, both
ecological and social.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0378</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0378</id><created>2011-10-03</created><authors><author><keyname>Kim</keyname><forenames>Jong Min</forenames></author><author><keyname>Lee</keyname><forenames>Ok Kyun</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>Exact Dynamic Support Tracking with Multiple Measurement Vectors using
  Compressive MUSIC</title><categories>cs.IT math.IT</categories><msc-class>94A13, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic tracking of sparse targets has been one of the important topics in
array signal processing. Recently, compressed sensing (CS) approaches have been
extensively investigated as a new tool for this problem using partial support
information obtained by exploiting temporal redundancy. However, most of these
approaches are formulated under single measurement vector compressed sensing
(SMV-CS) framework, where the performance guarantees are only in a
probabilistic manner. The main contribution of this paper is to allow
\textit{deterministic} tracking of time varying supports with multiple
measurement vectors (MMV) by exploiting multi-sensor diversity. In particular,
we show that a novel compressive MUSIC (CS-MUSIC) algorithm with optimized
partial support selection not only allows removal of inaccurate portion of
previous support estimation but also enables addition of newly emerged part of
unknown support. Numerical results confirm the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0381</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0381</id><created>2011-10-03</created><authors><author><keyname>Saavedra</keyname><forenames>Serguei</forenames></author><author><keyname>Hagerty</keyname><forenames>Kathleen</forenames></author><author><keyname>Uzzi</keyname><forenames>Brian</forenames></author></authors><title>Synchronicity, Instant Messaging and Performance among Financial Traders</title><categories>physics.soc-ph cs.SI physics.data-an q-bio.PE</categories><journal-ref>PNAS 108: 5296-5301 (2011)</journal-ref><doi>10.1073/pnas.1018462108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful animal systems often manage risk through synchronous behavior that
spontaneously arises without leadership. In critical human systems facing risk,
such as financial markets or military operations, our understanding of the
benefits associated to synchronicity is nascent but promising. Building on
previous work illuminating commonalities between ecological and human systems,
we compare the activity patterns of individual financial traders with the
simultaneous activity of other traders---an individual and spontaneous
characteristic we call synchronous trading. Additionally, we examine the
association of synchronous trading with individual performance and
communication patterns. Analyzing empirical data on day traders'
second-to-second trading and instant messaging, we find that the higher the
traders' synchronous trading, the less likely they lose money at the end of the
day. We also find that the daily instant messaging patterns of traders are
closely associated with their level of synchronous trading. This suggests that
synchronicity and vanguard technology may help cope with risky decisions in
complex systems and furnish new prospects for achieving collective and
individual goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0404</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0404</id><created>2011-10-03</created><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Wilde</keyname><forenames>Michael</forenames></author></authors><title>Many-Task Computing Tools for Multiscale Modeling</title><categories>cs.DC cs.PL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper discusses the use of many-task computing tools for multiscale
modeling. It defines multiscale modeling and places different examples of it on
a coupling spectrum, discusses the Swift parallel scripting language, describes
three multiscale modeling applications that could use Swift, and then talks
about how the Swift model is being extended to cover more of the multiscale
modeling coupling spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0413</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0413</id><created>2011-10-03</created><authors><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Jacob</keyname><forenames>Laurent</forenames><affiliation>CBIO</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Group Lasso with Overlaps: the Latent Group Lasso approach</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a norm for structured sparsity which leads to sparse linear
predictors whose supports are unions of prede ned overlapping groups of
variables. We call the obtained formulation latent group Lasso, since it is
based on applying the usual group Lasso penalty on a set of latent variables. A
detailed analysis of the norm and its properties is presented and we
characterize conditions under which the set of groups associated with latent
variables are correctly identi ed. We motivate and discuss the delicate choice
of weights associated to each group, and illustrate this approach on simulated
data and on the problem of breast cancer prognosis from gene expression data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0425</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0425</id><created>2011-10-03</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Schieler</keyname><forenames>Curt</forenames></author></authors><title>Hybrid Codes Needed for Coordination over the Point-to-Point Channel</title><categories>cs.IT math.IT</categories><comments>Allerton 2011, 5 pages, 1 figure, uses IEEEtran.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new fundamental question regarding the point-to-point
memoryless channel. The source-channel separation theorem indicates that random
codebook construction for lossy source compression and channel coding can be
independently constructed and paired to achieve optimal performance for
coordinating a source sequence with a reconstruction sequence. But what if we
want the channel input to also be coordinated with the source and
reconstruction? Such situations arise in network communication problems, where
the correlation inherent in the information sources can be used to correlate
channel inputs.
  Hybrid codes have been shown to be useful in a number of network
communication problems. In this work we highlight their advantages over purely
digital codebook construction by applying them to the point-to-point setting,
coordinating both the channel input and the reconstruction with the source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0428</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0428</id><created>2011-10-03</created><authors><author><keyname>Feizi</keyname><forenames>Soheil</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>A Power Efficient Sensing/Communication Scheme: Joint
  Source-Channel-Network Coding by Using Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>Presented at Allerton Conference 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a joint source-channel-network coding scheme, based on compressive
sensing principles, for wireless networks with AWGN channels (that may include
multiple access and broadcast), with sources exhibiting temporal and spatial
dependencies. Our goal is to provide a reconstruction of sources within an
allowed distortion level at each receiver. We perform joint source-channel
coding at each source by randomly projecting source values to a lower
dimensional space. We consider sources that satisfy the restricted eigenvalue
(RE) condition as well as more general sources for which the randomness of the
network allows a mapping to lower dimensional spaces. Our approach relies on
using analog random linear network coding. The receiver uses compressive
sensing decoders to reconstruct sources. Our key insight is the fact that,
compressive sensing and analog network coding both preserve the source
characteristics required for compressive sensing decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0436</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0436</id><created>2011-10-03</created><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>The BlueNetwork Concept</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of energy efficiency and carbon reduction initiatives and concepts
attempt to regulate and optimize machines behavior, and therefore, human
behavior itself is left neglected. Although most of energy and resource
consumption is the result of machines functioning and behavior (including
domesticated animals such as cows), these behaviors themselves are actually in
answer to humans demands and needs, and therefore, can be considered the
indirect results of humans behavior. Resolving the source of problems, i.e.,
the unhealthy human behavior, not only reduces these footprints including
energy and water consumption, and GHG emissions, it also helps increasing the
quality of life in society. Here, we propose an approach which focuses on
adjusting humans behavior toward eliminating unnecessary demand on the machines
that consequently lowers the consumption. This goal is achieved by creating a
social environment in which directed and selective interactions help humans to
adjust to healthier behavior. The solution consists of human-friendly
interfaces and also artificial intelligence software in order to learn and
emulate human interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0461</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0461</id><created>2011-10-03</created><updated>2012-06-21</updated><authors><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author></authors><title>LSM is not generated by binary functions</title><categories>cs.CC</categories><comments>Superseded by arXiv:1108.5288v4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The material in this note is now superseded by arXiv:1108.5288v4.
  Bulatov et al. [1] defined the operation of (efficient)
pps_\omega-definability in order to study the computational complexity of
certain approximate counting problems. They asked whether all log-supermodular
functions can be defined by binary implication and unary functions in this
sense. We give a negative answer to this question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0463</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0463</id><created>2011-10-02</created><authors><author><keyname>Gorbachev</keyname><forenames>V. N.</forenames></author><author><keyname>Yakovleva</keyname><forenames>E. S.</forenames></author></authors><title>A binary noisy channel to model errors in printing process</title><categories>cs.OH</categories><comments>5 pages 7 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  To model printing noise a binary noisy channel and a set of controlled gates
are introduced. The channel input is an image created by a halftoning algorithm
and its output is the printed picture. Using this channel robustness to noise
between halftoning algorithms can be studied. We introduced relative entropy to
describe immunity of the algorithm to noise and tested several halftoning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0477</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0477</id><created>2011-10-03</created><authors><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Distributed Evolutionary Graph Partitioning</title><categories>cs.NE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel distributed evolutionary algorithm, KaFFPaE, to solve the
Graph Partitioning Problem, which makes use of KaFFPa (Karlsruhe Fast Flow
Partitioner). The use of our multilevel graph partitioner KaFFPa provides new
effective crossover and mutation operators. By combining these with a scalable
communication protocol we obtain a system that is able to improve the best
known partitioning results for many inputs in a very short amount of time. For
example, in Walshaw's well known benchmark tables we are able to improve or
recompute 76% of entries for the tables with 1%, 3% and 5% imbalance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0517</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0517</id><created>2011-10-03</created><authors><author><keyname>Ruan</keyname><forenames>Ning</forenames></author><author><keyname>Jin</keyname><forenames>Ruoming</forenames></author><author><keyname>Huang</keyname><forenames>Yan</forenames></author></authors><title>Distance Preserving Graph Simplification</title><categories>cs.SI</categories><comments>A short version of this paper will be published for ICDM'11, December
  2011</comments><journal-ref>Proceedings of IEEE International Conference on Data Mining
  (ICDM), pp. 1200-1205 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large graphs are difficult to represent, visualize, and understand. In this
paper, we introduce &quot;gate graph&quot; - a new approach to perform graph
simplification. A gate graph provides a simplified topological view of the
original graph. Specifically, we construct a gate graph from a large graph so
that for any &quot;non-local&quot; vertex pair (distance higher than some threshold) in
the original graph, their shortest-path distance can be recovered by
consecutive &quot;local&quot; walks through the gate vertices in the gate graph. We
perform a theoretical investigation on the gate-vertex set discovery problem.
We characterize its computational complexity and reveal the upper bound of
minimum gate-vertex set using VC-dimension theory. We propose an efficient
mining algorithm to discover a gate-vertex set with guaranteed logarithmic
bound. We further present a fast technique for pruning redundant edges in a
gate graph. The detailed experimental results using both real and synthetic
graphs demonstrate the effectiveness and efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0532</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0532</id><created>2011-10-03</created><authors><author><keyname>Phillips</keyname><forenames>Caleb</forenames></author><author><keyname>Becker</keyname><forenames>Lee</forenames></author><author><keyname>Bradley</keyname><forenames>Elizabeth</forenames></author></authors><title>Strange Beta: An Assistance System for Indoor Rock Climbing Route
  Setting Using Chaotic Variations and Machine Learning</title><categories>cs.AI cs.HC stat.AP</categories><comments>University of Colorado Computer Science Department Technical Report</comments><report-no>CU-CS-1087-11</report-no><journal-ref>Chaos 22, 013130 (2012)</journal-ref><doi>10.1063/1.3693047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies machine learning and the mathematics of chaos to the task
of designing indoor rock-climbing routes. Chaotic variation has been used to
great advantage on music and dance, but the challenges here are quite
different, beginning with the representation. We present a formalized system
for transcribing rock climbing problems, then describe a variation generator
that is designed to support human route-setters in designing new and
interesting climbing problems. This variation generator, termed Strange Beta,
combines chaos and machine learning, using the former to introduce novelty and
the latter to smooth transitions in a manner that is consistent with the style
of the climbs This entails parsing the domain-specific natural language that
rock climbers use to describe routes and movement and then learning the
patterns in the results. We validated this approach with a pilot study in a
small university rock climbing gym, followed by a large blinded study in a
commercial climbing gym, in cooperation with experienced climbers and expert
route setters. The results show that {\sc Strange Beta} can help a human setter
produce routes that are at least as good as, and in some cases better than,
those produced in the traditional manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0535</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0535</id><created>2011-10-03</created><authors><author><keyname>Toole</keyname><forenames>Jameson L.</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author></authors><title>Modeling the adoption of innovations in the presence of geographic and
  media influences</title><categories>cs.SI nlin.AO physics.soc-ph</categories><doi>10.1371/journal.pone.0029528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While there has been much work examining the affects of social network
structure on innovation adoption, models to date have lacked important features
such as meta-populations reflecting real geography or influence from mass media
forces. In this article, we show these are features crucial to producing more
accurate predictions of a social contagion and technology adoption at the city
level. Using data from the adoption of the popular micro-blogging platform,
Twitter, we present a model of adoption on a network that places friendships in
real geographic space and exposes individuals to mass media influence. We show
that homopholy both amongst individuals with similar propensities to adopt a
technology and geographic location are critical to reproduce features of real
spatiotemporal adoption. Furthermore, we estimate that mass media was
responsible for increasing Twitter's user base two to four fold. To reflect
this strength, we extend traditional contagion models to include an endogenous
mass media agent that responds to those adopting an innovation as well as
influencing agents to adopt themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0543</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0543</id><created>2011-10-03</created><authors><author><keyname>Jorissen</keyname><forenames>Kevin</forenames></author><author><keyname>Vila</keyname><forenames>Fernando D.</forenames></author><author><keyname>Rehr</keyname><forenames>John J.</forenames></author></authors><title>A high performance scientific cloud computing environment for materials
  simulations</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.CE</categories><doi>10.1016/j.cpc.2012.04.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the development of a scientific cloud computing (SCC) platform
that offers high performance computation capability. The platform consists of a
scientific virtual machine prototype containing a UNIX operating system and
several materials science codes, together with essential interface tools (an
SCC toolset) that offers functionality comparable to local compute clusters. In
particular, our SCC toolset provides automatic creation of virtual clusters for
parallel computing, including tools for execution and monitoring performance,
as well as efficient I/O utilities that enable seamless connections to and from
the cloud. Our SCC platform is optimized for the Amazon Elastic Compute Cloud
(EC2). We present benchmarks for prototypical scientific applications and
demonstrate performance comparable to local compute clusters. To facilitate
code execution and provide user-friendly access, we have also integrated cloud
computing capability in a JAVA-based GUI. Our SCC platform may be an
alternative to traditional HPC resources for materials science or quantum
chemistry applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0550</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0550</id><created>2011-10-03</created><authors><author><keyname>Lin</keyname><forenames>Pey-Chang Kent</forenames></author><author><keyname>Mandal</keyname><forenames>Ayan</forenames></author><author><keyname>Khatri</keyname><forenames>Sunil P</forenames></author></authors><title>Boolean Satisfiability using Noise Based Logic</title><categories>cs.CC cs.DS</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel algorithm to solve the Boolean
Satisfiability (SAT) problem, using noise-based logic (NBL). Contrary to what
the name may suggest, NBL is not a random/fuzzy logic system. In fact, it is a
completely deterministic logic system. A key property of NBL is that it allows
us to apply a superposition of many input vectors to a SAT instance at the same
time, circumventing a key restriction and assumption in the traditional
approach to solving SAT. By exploiting the superposition property of NBL, our
NBL-based SAT algorithm can determine whether an instance is SAT or not in a
single operation. A satisfying solution can be found by iteratively performing
SAT check operations up to n times, where n is the number of variables in the
SAT instance. Although this paper does not focus on the realization of an
NBL-based SAT engine, such an engine can be conceived using analog circuits
(wide-band amplifiers, adders and multipliers), FPGAs or ASICs. Additionally,
we also discus scalability of our approach, which can apply to NBL in general.
The NBL-based SAT engine described in this paper has been simulated in software
for validation purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0560</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0560</id><created>2011-10-03</created><authors><author><keyname>Jeong</keyname><forenames>Seongwook</forenames></author><author><keyname>Moon</keyname><forenames>Jaekyun</forenames></author></authors><title>Easily Computed Lower Bounds on the Information Rate of Intersymbol
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>14 pages, 14 figures including subfigures. arXiv admin note:
  substantial text overlap with arXiv:1001.3911</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provable lower bounds are presented for the information rate I(X; X+S+N)
where X is the symbol drawn independently and uniformly from a finite-size
alphabet, S is a discrete-valued random variable (RV) and N is a Gaussian RV.
It is well known that with S representing the precursor intersymbol
interference (ISI) at the decision feedback equalizer (DFE) output, I(X; X+S+N)
serves as a tight lower bound for the symmetric information rate (SIR) as well
as capacity of the ISI channel corrupted by Gaussian noise. When evaluated on a
number of well-known finite-ISI channels, these new bounds provide a very
similar level of tightness against the SIR to the conjectured lower bound by
Shamai and Laroia at all signal-to-noise ratio (SNR) ranges, while being
actually tighter when viewed closed up at high SNRs. The new lower bounds are
obtained in two steps: First, a &quot;mismatched&quot; mutual information function is
introduced which can be proved as a lower bound to I(X; X+S+N). Secondly, this
function is further bounded from below by an expression that can be computed
easily via a few single-dimensional integrations with a small computational
load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0564</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0564</id><created>2011-10-03</created><updated>2011-10-08</updated><authors><author><keyname>Gorantla</keyname><forenames>Anusha</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Diversity Order Vs Rate in an AWGN Channel</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the diversity order vs rate of an additive white Gaussian noise
(AWGN) channel in the whole capacity region. We show that for discrete input as
well as for continuous input, Gallager's upper bounds on error probability have
exponential diversity in low and high rate region but only subexponential in
the mid-rate region. For the best available lower bounds and for the practical
codes one observes exponential diversity throughout the capacity region.
However we also show that performance of practical codes is close to Gallager's
upper bounds and the mid-rate subexponential diversity has a bearing on the
performance of the practical codes. Finally we show that the upper bounds with
Gaussian input provide good approximation throughout the capacity region even
for finite constellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0569</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0569</id><created>2011-10-03</created><authors><author><keyname>Caplan</keyname><forenames>R. M.</forenames></author><author><keyname>Carretero-Gonz&#xe1;lez</keyname><forenames>R.</forenames></author></authors><title>A Modulus-Squared Dirichlet Boundary Condition for Time-Dependent
  Complex Partial Differential Equations and its Application to the Nonlinear
  Schr\&quot;odinger Equation</title><categories>cs.NA math.NA</categories><comments>19 pages, 7 figures</comments><msc-class>65M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An easy to implement modulus-squared Dirichlet (MSD) boundary condition is
formulated for numerical simulations of time-dependent complex partial
differential equations in multidimensional settings. The MSD boundary condition
approximates a constant modulus-square value of the solution at the boundaries.
Application of the MSD boundary condition to the nonlinear Schr\&quot;odinger
equation is shown, and numerical simulations are performed to demonstrate its
usefulness and advantages over other simple boundary conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0578</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0578</id><created>2011-10-04</created><authors><author><keyname>Vasev</keyname><forenames>Pavel</forenames></author></authors><title>Open Input: A New Way for Websites to Grow</title><categories>cs.HC cs.CY cs.SI</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regardless of current web 2.0 and 3.0 trends, there are still a lot of
websites made in web 1.0 style. These websites have fixed pages which are
editable only by owner and not by community. It is normal for a lot of cases,
but looks like not modern and engaging approach. Are there any ways to make
these sites closer to life? This paper is devoted to open input technique, a
way for websites of web 1.0 era to grow and evolve community. The idea of open
input, in general, means that anybody from the web can add information to any
section of the website even without registration on that website. People can
add news, billboard announcements, testimonials, questions, pictures, videos
etc - whatever site owner permitted. We have tested this idea in practice and
have positive results approving that open input is a vital approach for
collaboration on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0583</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0583</id><created>2011-10-04</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Poon</keyname><forenames>Sheung-Hung</forenames></author><author><keyname>Ung</keyname><forenames>Chin-Ting</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>Algorithms for the strong chromatic index of Halin graphs,
  distance-hereditary graphs and maximal outerplanar graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exist linear-time algorithms that compute the strong
chromatic index of Halin graphs, of maximal outerplanar graphs and of
distance-hereditary graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0585</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0585</id><created>2011-10-04</created><authors><author><keyname>Whitehill</keyname><forenames>Jacob</forenames></author><author><keyname>Movellan</keyname><forenames>Javier</forenames></author></authors><title>Discriminately Decreasing Discriminability with Learned Image Filters</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning and computer vision, input images are often filtered to
increase data discriminability. In some situations, however, one may wish to
purposely decrease discriminability of one classification task (a &quot;distractor&quot;
task), while simultaneously preserving information relevant to another (the
task-of-interest): For example, it may be important to mask the identity of
persons contained in face images before submitting them to a crowdsourcing site
(e.g., Mechanical Turk) when labeling them for certain facial attributes.
Another example is inter-dataset generalization: when training on a dataset
with a particular covariance structure among multiple attributes, it may be
useful to suppress one attribute while preserving another so that a trained
classifier does not learn spurious correlations between attributes. In this
paper we present an algorithm that finds optimal filters to give high
discriminability to one task while simultaneously giving low discriminability
to a distractor task. We present results showing the effectiveness of the
proposed technique on both simulated data and natural face images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0593</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0593</id><created>2011-10-04</created><authors><author><keyname>Blythe</keyname><forenames>Duncan A. J.</forenames></author></authors><title>Two Projection Pursuit Algorithms for Machine Learning under
  Non-Stationarity</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis derives, tests and applies two linear projection algorithms for
machine learning under non-stationarity. The first finds a direction in a
linear space upon which a data set is maximally non-stationary. The second aims
to robustify two-way classification against non-stationarity. The algorithm is
tested on a key application scenario, namely Brain Computer Interfacing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0594</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0594</id><created>2011-10-04</created><authors><author><keyname>Aktas</keyname><forenames>Tugcan</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ali Ozgur</forenames></author><author><keyname>Aktas</keyname><forenames>Emre</forenames></author></authors><title>Practical Wireless Network Coding and Decoding Methods for Multiple
  Unicast Transmissions</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, Submitted to WCNC 2012, IEEE Wireless
  Communication and Networking Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple yet effective wireless network coding and decoding
technique. It utilizes spatial diversity through cooperation between nodes
which carry out distributed encoding operations dictated by generator matrices
of linear block codes. For this purpose, we make use of greedy codes over the
binary field and show that desired diversity orders can be flexibly assigned to
nodes in a multiple unicast network, contrary to the previous findings in the
literature. Furthermore, we present the optimal detection rule for the given
model that accounts for intermediate node errors and suggest a network decoder
using the sum-product algorithm. The proposed sum-product detector exhibits
near optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0620</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0620</id><created>2011-10-04</created><authors><author><keyname>Imahori</keyname><forenames>Shinji</forenames></author><author><keyname>Matsui</keyname><forenames>Tomomi</forenames></author><author><keyname>Miyashiro</keyname><forenames>Ryuhei</forenames></author></authors><title>A 2.75-Approximation Algorithm for the Unconstrained Traveling
  Tournament Problem</title><categories>cs.DS</categories><comments>12 pages, 1 figure</comments><journal-ref>Annals of Operations Research, Volume 218 (2014), Issue 1, pp
  237-247</journal-ref><doi>10.1007/s10479-012-1161-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 2.75-approximation algorithm is proposed for the unconstrained traveling
tournament problem, which is a variant of the traveling tournament problem. For
the unconstrained traveling tournament problem, this is the first proposal of
an approximation algorithm with a constant approximation ratio. In addition,
the proposed algorithm yields a solution that meets both the no-repeater and
mirrored constraints. Computational experiments show that the algorithm
generates solutions of good quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0623</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0623</id><created>2011-10-04</created><updated>2011-10-06</updated><authors><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Schmidt</keyname><forenames>Johannes</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>On the Parameterized Complexity of Default Logic and Autoepistemic Logic</title><categories>cs.CC cs.AI</categories><comments>12 pages + 2 pages appendix, 1 figure, Version without Appendix
  submitted to LATA 2012</comments><msc-class>68Q17</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the application of Courcelle's Theorem and the logspace
version of Elberfeld etal. in the context of the implication problem for
propositional sets of formulae, the extension existence problem for default
logic, as well as the expansion existence problem for autoepistemic logic and
obtain fixed-parameter time and space efficient algorithms for these problems.
On the other hand, we exhibit, for each of the above problems, families of
instances of a very simple structure that, for a wide range of different
parameterizations, do not have efficient fixed-parameter algorithms (even in
the sense of the large class XPnu), unless P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0624</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0624</id><created>2011-10-04</created><authors><author><keyname>Dovier</keyname><forenames>Agostino</forenames></author><author><keyname>Formisano</keyname><forenames>Andrea</forenames></author><author><keyname>Pontelli</keyname><forenames>Enrico</forenames></author></authors><title>Autonomous Agents Coordination: Action Languages meet CLP(FD) and Linda</title><categories>cs.LO cs.AI cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a knowledge representation formalism, in the form of a
high-level Action Description Language for multi-agent systems, where
autonomous agents reason and act in a shared environment. Agents are
autonomously pursuing individual goals, but are capable of interacting through
a shared knowledge repository. In their interactions through shared portions of
the world, the agents deal with problems of synchronization and concurrency;
the action language allows the description of strategies to ensure a consistent
global execution of the agents' autonomously derived plans. A distributed
planning problem is formalized by providing the declarative specifications of
the portion of the problem pertaining a single agent. Each of these
specifications is executable by a stand-alone CLP-based planner. The
coordination among agents exploits a Linda infrastructure. The proposal is
validated in a prototype implementation developed in SICStus Prolog.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0631</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0631</id><created>2011-10-04</created><authors><author><keyname>Riguzzi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Swift</keyname><forenames>Terrance</forenames></author></authors><title>Well-Definedness and Efficient Inference for Probabilistic Logic
  Programming under the Distribution Semantics</title><categories>cs.AI cs.LO cs.PL</categories><comments>31 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distribution semantics is one of the most prominent approaches for the
combination of logic programming and probability theory. Many languages follow
this semantics, such as Independent Choice Logic, PRISM, pD, Logic Programs
with Annotated Disjunctions (LPADs) and ProbLog. When a program contains
functions symbols, the distribution semantics is well-defined only if the set
of explanations for a query is finite and so is each explanation.
Well-definedness is usually either explicitly imposed or is achieved by
severely limiting the class of allowed programs. In this paper we identify a
larger class of programs for which the semantics is well-defined together with
an efficient procedure for computing the probability of queries. Since LPADs
offer the most general syntax, we present our results for them, but our results
are applicable to all languages under the distribution semantics. We present
the algorithm &quot;Probabilistic Inference with Tabling and Answer subsumption&quot;
(PITA) that computes the probability of queries by transforming a probabilistic
program into a normal program and then applying SLG resolution with answer
subsumption. PITA has been implemented in XSB and tested on six domains: two
with function symbols and four without. The execution times are compared with
those of ProbLog, cplint and CVE, PITA was almost always able to solve larger
problems in a shorter time, on domains with and without function symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0641</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0641</id><created>2011-10-04</created><authors><author><keyname>Nikulin</keyname><forenames>Vladimir</forenames></author></authors><title>Identifying relationships between drugs and medical conditions: winning
  experience in the Challenge 2 of the OMOP 2010 Cup</title><categories>stat.ML cs.CV stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing interest in using a longitudinal observational databases
to detect drug safety signal. In this paper we present a novel method, which we
used online during the OMOP Cup. We consider homogeneous ensembling, which is
based on random re-sampling (known, also, as bagging) as a main innovation
compared to the previous publications in the related field. This study is based
on a very large simulated database of the 10 million patients records, which
was created by the Observational Medical Outcomes Partnership (OMOP). Compared
to the traditional classification problem, the given data are unlabelled. The
objective of this study is to discover hidden associations between drugs and
conditions. The main idea of the approach, which we used during the OMOP Cup is
to compare the numbers of observed and expected patterns. This comparison may
be organised in several different ways, and the outcomes (base learners) may be
quite different as well. It is proposed to construct the final decision
function as an ensemble of the base learners. Our method was recognised
formally by the Organisers of the OMOP Cup as a top performing method for the
Challenge N2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0645</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0645</id><created>2011-10-04</created><updated>2011-11-07</updated><authors><author><keyname>Sharif</keyname><forenames>Puya</forenames></author><author><keyname>Heydari</keyname><forenames>Hoshang</forenames></author></authors><title>Strategies and payoffs in quantum minority games</title><categories>quant-ph cs.GT</categories><comments>5 pages, 6 figures; Proceedings of the International Conference on
  Advances in Quantum Theory, Volume 1327, AIP,(2011)</comments><journal-ref>Proc. AQT, Volume 1327, AIP(2011), 477-481</journal-ref><doi>10.1063/1.3567477</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Game theory is the mathematical framework for analyzing strategic
interactions in conflict and competition situations. In recent years quantum
game theory has earned the attention of physicists, and has emerged as a branch
of quantum information theory [1]. With the aid of entanglement and linear
superposition of strategies, quantum games are shown to yield signifcant
advantage over their classical counterparts. In this paper we explore optimal
and equilibrium solutions to quantum minority games. Initial states with
different level of entanglement are investigated. Focus will be on 4 and 6
player games with some N-player generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0667</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0667</id><created>2011-10-04</created><authors><author><keyname>Osama</keyname><forenames>Safarini</forenames></author></authors><title>Storage Area Network Implementation on an Educational Institute Network
  Computer Networking and Communication</title><categories>cs.NI</categories><comments>5 Pages; ISSN: 2221-0741</comments><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT), Vol. 1 No. 7, 292-296, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  the storage infrastructure is the foundation on which information relies and
therefore must support a company's business objectives and business model. In
this environment, simply deploying more and faster storage devices is not
enough; a new kind of infrastructure is needed, one that provides more enhanced
network availability, data accessibility, and system manageability than is
provided by today's infrastructure. The SAN meets this challenge. The SAN
liberates the storage device, so it is not on a particular server bus, and
attaches it directly to the network. In other words, storage is externalized
and functionally distributed across the organization. The SAN also enables the
centralizing of storage devices and the clustering of servers, which makes for
easier and less expensive administration. So the idea is to create an
intelligent SAN infrastructure that stretches to meet increased demands, allows
highly available and heterogeneous access to expanding information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0678</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0678</id><created>2011-10-04</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Interference Alignment and Neutralization in a Cognitive 3-User
  MAC-Interference Channel: Degrees of Freedom</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure, presented at the CWIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network consisting of a point-to-point (P2P) link and a multiple access
channel (MAC) sharing the same medium is considered. The resulting interference
network, with three transmitters and two receivers is studied from degrees of
freedom (DoF) perspective, with and without cognition. Several cognition
variants are examined. Namely, the setup is studied with (1) no cognitive
transmitters, (2) a cognitive P2P transmitter, (3) one cognitive MAC
transmitter, and (4) with two cognitive MAC transmitters. It is shown that
having a cognitive P2P transmitter does not bring any DoF gain to the network.
This is obtained by showing that the DoF of the two former cases (1) and (2) is
1. However, it is shown that a cognitive MAC transmitter is more beneficial
since the latter two cases (3) and (4) have 3/2 DoF. The achievability of 3/2
DoF is guaranteed by using a combination of interference neutralization and
interference alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0685</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0685</id><created>2011-10-04</created><authors><author><keyname>Carrasco</keyname><forenames>Rodrigo A.</forenames></author><author><keyname>Iyengar</keyname><forenames>Garud</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author></authors><title>Energy Aware Scheduling for Weighted Completion Time and Weighted
  Tardiness</title><categories>cs.DM math.CO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever increasing adoption of mobile devices with limited energy storage
capacity, on the one hand, and more awareness of the environmental impact of
massive data centres and server pools, on the other hand, have both led to an
increased interest in energy management algorithms.
  The main contribution of this paper is to present several new constant factor
approximation algorithms for energy aware scheduling problems where the
objective is to minimize weighted completion time plus the cost of the energy
consumed, in the one machine non-preemptive setting, while allowing release
dates and deadlines.Unlike previous known algorithms these new algorithms can
handle general job-dependent energy cost functions, extending the application
of these algorithms to settings outside the typical CPU-energy one. These new
settings include problems where in addition, or instead, of energy costs we
also have maintenance costs, wear and tear, replacement costs, etc., which in
general depend on the speed at which the machine runs but also depend on the
types of jobs processed. Our algorithms also extend to approximating weighted
tardiness plus energy cost, an inherently more difficult problem that has not
been addressed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0693</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0693</id><created>2011-10-04</created><updated>2011-11-23</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames><affiliation>CNRS/LIX, Ecole Polytechnique, Palaiseau, France</affiliation></author><author><keyname>Mueller</keyname><forenames>Jens K</forenames><affiliation>Friedrich-Schiller-University Jena, Germany</affiliation></author></authors><title>The Complexity of Rooted Phylogeny Problems</title><categories>cs.CC cs.CE</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (November
  24, 2011) lmcs:906</journal-ref><doi>10.2168/LMCS-7(4:6)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several computational problems in phylogenetic reconstruction can be
formulated as restrictions of the following general problem: given a formula in
conjunctive normal form where the literals are rooted triples, is there a
rooted binary tree that satisfies the formula? If the formulas do not contain
disjunctions, the problem becomes the famous rooted triple consistency problem,
which can be solved in polynomial time by an algorithm of Aho, Sagiv,
Szymanski, and Ullman. If the clauses in the formulas are restricted to
disjunctions of negated triples, Ng, Steel, and Wormald showed that the problem
remains NP-complete. We systematically study the computational complexity of
the problem for all such restrictions of the clauses in the input formula. For
certain restricted disjunctions of triples we present an algorithm that has
sub-quadratic running time and is asymptotically as fast as the fastest known
algorithm for the rooted triple consistency problem. We also show that any
restriction of the general rooted phylogeny problem that does not fall into our
tractable class is NP-complete, using known results about the complexity of
Boolean constraint satisfaction problems. Finally, we present a pebble game
argument that shows that the rooted triple consistency problem (and also all
generalizations studied in this paper) cannot be solved by Datalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0704</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0704</id><created>2011-10-04</created><authors><author><keyname>Barenboim</keyname><forenames>Ronen</forenames></author><author><keyname>Bortnikov</keyname><forenames>Edward</forenames></author><author><keyname>Golbandi</keyname><forenames>Nadav</forenames></author><author><keyname>Kagian</keyname><forenames>Amit</forenames></author><author><keyname>Katzir</keyname><forenames>Liran</forenames></author><author><keyname>Lempel</keyname><forenames>Ronny</forenames></author><author><keyname>Makabee</keyname><forenames>Hayim</forenames></author><author><keyname>Roy</keyname><forenames>Scott</forenames></author><author><keyname>Somekh</keyname><forenames>Oren</forenames></author></authors><title>Hierarchical Composable Optimization of Web Pages</title><categories>cs.IR</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of creating modern Web media experiences is challenged by the
need to adapt the content and presentation choices to dynamic real-time
fluctuations of user interest across multiple audiences. We introduce FAME - a
Framework for Agile Media Experiences - which addresses this scalability
problem. FAME allows media creators to define abstract page models that are
subsequently transformed into real experiences through algorithmic
experimentation. FAME's page models are hierarchically composed of simple
building blocks, mirroring the structure of most Web pages. They are resolved
into concrete page instances by pluggable algorithms which optimize the pages
for specific business goals. Our framework allows retrieving dynamic content
from multiple sources, defining the experimentation's degrees of freedom, and
constraining the algorithmic choices. It offers an effective separation of
concerns in the media creation process, enabling multiple stakeholders with
profoundly different skills to apply their crafts and perform their duties
independently, composing and reusing each other's work in modular ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0715</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0715</id><created>2011-10-04</created><authors><author><keyname>Rosebrugh</keyname><forenames>R.</forenames></author><author><keyname>Sabadini</keyname><forenames>N.</forenames></author><author><keyname>Walters</keyname><forenames>R. F. C.</forenames></author></authors><title>Tangled Circuits</title><categories>math.CT cs.LO</categories><msc-class>18D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theme of the paper is the use of commutative Frobenius algebras in
braided strict monoidal categories in the study of varieties of circuits and
communicating systems which occur in Computer Science, including circuits in
which the wires are tangled. We indicate also some possible novel geometric
interest in such algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0718</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0718</id><created>2011-10-04</created><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Directed information and Pearl's causal calculus</title><categories>cs.IT cs.LG cs.SY math.IT</categories><comments>8 pages, uses ieeeconf.cls; to appear in Proc. 49th Annual Allerton
  Conf. on Communication, Control and Computing (2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models are a fundamental tool in statistics, machine
learning, signal processing, and control. When such a model is defined on a
directed acyclic graph (DAG), one can assign a partial ordering to the events
occurring in the corresponding stochastic system. Based on the work of Judea
Pearl and others, these DAG-based &quot;causal factorizations&quot; of joint probability
measures have been used for characterization and inference of functional
dependencies (causal links). This mostly expository paper focuses on several
connections between Pearl's formalism (and in particular his notion of
&quot;intervention&quot;) and information-theoretic notions of causality and feedback
(such as causal conditioning, directed stochastic kernels, and directed
information). As an application, we show how conditional directed information
can be used to develop an information-theoretic version of Pearl's &quot;back-door&quot;
criterion for identifiability of causal effects from passive observations. This
suggests that the back-door criterion can be thought of as a causal analog of
statistical sufficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0724</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0724</id><created>2011-09-21</created><updated>2011-10-12</updated><authors><author><keyname>Hassan</keyname><forenames>Sk. S.</forenames></author><author><keyname>Choudhury</keyname><forenames>P. Pal</forenames></author><author><keyname>Nayak</keyname><forenames>B. K.</forenames></author><author><keyname>Ghosh</keyname><forenames>A.</forenames></author><author><keyname>Banerjee</keyname><forenames>J.</forenames></author></authors><title>Integral Value Transformations: A Class of Affine Discrete Dynamical
  Systems and an Application</title><categories>cs.DM nlin.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, the notion of Integral Value Transformations (IVTs), a class
of Discrete Dynamical Maps has been introduced. Then notion of Affine Discrete
Dynamical System (ADDS) in the light of IVTs is defined and some rudimentary
mathematical properties of the system are depicted. Collatz Conjecture is one
of the most enigmatic problems in 20th Century. The Conjecture was posed by
German Mathematician L. Collatz in 1937. There are much advancement in
generalizing and defining analogous conjectures, but even to the date, there is
no fruitful result for the advancement for the settlement of the conjecture. We
have made an effort to make a Collatz type problem in the domain of IVTs and we
have been able to solve the problem in 2011 [1]. Here mainly, we have focused
and inquired on Collatz-like ADDS. Finally, we have designed the Optimal
Distributed and Parallel Environment (ODPE) in the light of ADDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0725</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0725</id><created>2011-10-04</created><authors><author><keyname>Jesus</keyname><forenames>Paulo</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author></authors><title>A Survey of Distributed Data Aggregation Algorithms</title><categories>cs.DC cs.DS cs.IR cs.NI</categories><comments>45 pages, Technical Report</comments><acm-class>C.2.4; A.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed data aggregation is an important task, allowing the decentralized
determination of meaningful global properties, that can then be used to direct
the execution of other applications. The resulting values result from the
distributed computation of functions like COUNT, SUM and AVERAGE. Some
application examples can found to determine the network size, total storage
capacity, average load, majorities and many others.
  In the last decade, many different approaches have been proposed, with
different trade-offs in terms of accuracy, reliability, message and time
complexity. Due to the considerable amount and variety of aggregation
algorithms, it can be difficult and time consuming to determine which
techniques will be more appropriate to use in specific settings, justifying the
existence of a survey to aid in this task.
  This work reviews the state of the art on distributed data aggregation
algorithms, providing three main contributions. First, it formally defines the
concept of aggregation, characterizing the different types of aggregation
functions. Second, it succinctly describes the main aggregation techniques,
organizing them in a taxonomy. Finally, it provides some guidelines toward the
selection and use of the most relevant techniques, summarizing their principal
characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0728</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0728</id><created>2011-10-04</created><authors><author><keyname>Huber</keyname><forenames>K. T.</forenames></author><author><keyname>Moulton</keyname><forenames>V.</forenames></author></authors><title>Encoding and Constructing 1-Nested Phylogenetic Networks with Trinets</title><categories>q-bio.PE cs.DS</categories><msc-class>05C05, 92D15, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phylogenetic networks are a generalization of phylogenetic trees that are
used in biology to represent reticulate or non-treelike evolution. Recently,
several algorithms have been developed which aim to construct phylogenetic
networks from biological data using {\em triplets}, i.e. binary phylogenetic
trees on 3-element subsets of a given set of species. However, a fundamental
problem with this approach is that the triplets displayed by a phylogenetic
network do not necessary uniquely determine or {\em encode} the network. Here
we propose an alternative approach to encoding and constructing phylogenetic
networks, which uses phylogenetic networks on 3-element subsets of a set, or
{\em trinets}, rather than triplets. More specifically, we show that for a
special, well-studied type of phylogenetic network called a 1-nested network,
the trinets displayed by a 1-nested network always encode the network. We also
present an efficient algorithm for deciding whether a {\em dense} set of
trinets (i.e. one that contains a trinet on every 3-element subset of a set)
can be displayed by a 1-nested network or not and, if so, constructs that
network. In addition, we discuss some potential new directions that this new
approach opens up for constructing and comparing phylogenetic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0732</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0732</id><created>2011-10-04</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author><author><keyname>Ben-Av</keyname><forenames>Radel</forenames></author></authors><title>Z-States Algebra for a Tunable Multi-Party Entanglement-Distillation
  Protocol</title><categories>quant-ph cs.DC</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  W-States have achieved the status of the standard fully symmetric entangled
states, for many entanglement application purposes. Z-States are a
generalization of W-States that display an elegant algebra, enabling short
paths to desired results. This paper describes Z-States algebra starting from
neat definitions and laying down explicitly some fundamental theorems on
composition and distillation, needed for applications. These theorems are
synthesized into a generic tunable Entanglement-Distillation Protocol.
Applications are readily developed based upon the tunable protocol. A few
examples are provided to illustrate the approach generality. A concomitant
graphical representation allows fast comprehension of the protocol inputs,
operations and outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0748</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0748</id><created>2011-10-04</created><updated>2011-10-31</updated><authors><author><keyname>Zhong</keyname><forenames>Peng</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Compress-Forward without Wyner-Ziv Binning for the One-Way and Two-Way
  Relay Channels</title><categories>cs.IT math.IT</categories><comments>Appeared at Allerton conference Sept 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the role of Wyner-Ziv binning in compress-forward for relay
channels. In the one-way relay channel, we analyze a compress-forward scheme
without Wyner- Ziv binning but with joint decoding of both the message and
compression index. It achieves the same rate as the original compress-forward
scheme with binning and successive decoding. Therefore, binning helps reduce
decoding complexity by allowing successive decoding, but has no impact on
achievable rate for the one-way relay channel. On the other hand, no binning
simplifies relay operation. By extending compress-forward without binning to
the two-way relay channel, we can achieve a larger rate region than the
original compress-forward scheme when the channel is asymmetric for the two
users. Binning and successive decoding limits the compression rate to match the
weaker of the channels from relay to two users, whereas without binning, this
restriction no longer applies. Compared with noisy network coding,
compress-forward without binning achieves the same rate region in certain
Gaussian channel configurations, and it has much less delay. This work is a
step toward understanding the role of Wyner-Ziv binning in compress-forward
relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0751</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0751</id><created>2011-10-04</created><updated>2012-06-04</updated><authors><author><keyname>Moriano</keyname><forenames>P.</forenames></author><author><keyname>Finke</keyname><forenames>J.</forenames></author></authors><title>Power-law weighted networks from local attachments</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 3 figures; Proceedings of the IEEE Conference on Decision
  and Control and the European Control Conference, Orlando, FL, Dec. 2011;
  Added references; We modified the model in order to take into account
  extended power-law distributions which better fit to the citations data sets;
  Added proofs of theorems; Shorten version; Updated plot</comments><journal-ref>Europhysics Letters, vol. 99, no. 1, p. 18002, 2012</journal-ref><doi>10.1209/0295-5075/99/18002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter introduces a mechanism for constructing, through a process of
distributed decision-making, substrates for the study of collective dynamics on
extended power-law weighted networks with both a desired scaling exponent and a
fixed clustering coefficient. The analytical results show that the connectivity
distribution converges to the scaling behavior often found in social and
engineering systems. To illustrate the approach of the proposed framework we
generate network substrates that resemble steady state properties of the
empirical citation distributions of (i) publications indexed by the Institute
for Scientific Information from 1981 to 1997; (ii) patents granted by the U.S.
Patent and Trademark Office from 1975 to 1999; and (iii) opinions written by
the Supreme Court and the cases they cite from 1754 to 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0760</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0760</id><created>2011-10-04</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames></author><author><keyname>Kopecki</keyname><forenames>Steffen</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Iterated Hairpin Completions of Non-crossing Words</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterated hairpin completion is an operation on formal languages that is
inspired by the hairpin formation in DNA biochemistry. Iterated hairpin
completion of a word (or more precisely a singleton language) is always a
context-sensitive language and for some words it is known to be
non-context-free. However, it is unknown whether regularity of iterated hairpin
completion of a given word is decidable. Also the question whether iterated
hairpin completion of a word can be context-free but not regular was asked in
literature. In this paper we investigate iterated hairpin completions of
non-crossing words and, within this setting, we are able to answer both
questions. For non-crossing words we prove that the regularity of iterated
hairpin completions is decidable and that if iterated hairpin completion of a
non-crossing word is not regular, then it is not context-free either.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0784</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0784</id><created>2011-10-04</created><authors><author><keyname>Sridharan</keyname><forenames>Srinivas</forenames></author></authors><title>Optimal rotation of a qubit under dynamic measurement and velocity
  control</title><categories>quant-ph cs.SY math.OC</categories><comments>6 pages</comments><journal-ref>In American Control Conference June 2012 pages 5078, 5083</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we explore a modification in the problem of controlling the
rotation of a two level quantum system from an initial state to a final state
in minimum time. Specifically we consider the case where the qubit is being
weakly monitored -- albeit with an assumption that both the measurement
strength as well as the angular velocity are assumed to be control signals.
This modification alters the dynamics significantly and enables the
exploitation of the measurement backaction to assist in achieving the control
objective. The proposed method yields a significant speedup in achieving the
desired state transfer compared to previous approaches. These results are
demonstrated via numerical solutions for an example problem on a single qubit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0791</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0791</id><created>2011-10-04</created><authors><author><keyname>Bozhevolnyi</keyname><forenames>Sergey I.</forenames></author></authors><title>Rapid, Impartial and Comprehensive (RIC) publishing: A new concept for
  scientific journals</title><categories>cs.DL physics.soc-ph</categories><comments>6 pages, 2 figures, 4 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Publishing scientific journals governed by editors relying on anonymous peer
reviewing is slow (even one round of reviewing involves several communications
between authors, editor and reviewers), partial (arguments of authors can
hardly overrule those of reviewers) and not using all available scientific
material (even the most thorough and insightful reviews remain for the eyes of
authors and editors only). Here I propose a new concept for scientific journals
that ensures rapid, impartial and comprehensive (RIC) publishing. RIC concept
is based on implementation of two novel publishing principles: the first
(rapid) editorial screening of a submitted manuscript should result in its
either &quot;rejection&quot; or &quot;acceptance with optional revisions&quot;, and, in the latter
case, the optionally revised (taking into account open reviews) paper should be
published along with all (positive and negative) reviews, presenting thereby to
the scientific community all available scientific material on the topic in
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0812</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0812</id><created>2011-10-04</created><updated>2011-12-31</updated><authors><author><keyname>Bourgain</keyname><forenames>Jean</forenames></author><author><keyname>Garaev</keyname><forenames>Moubariz Z.</forenames></author><author><keyname>Konyagin</keyname><forenames>Sergei V.</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>On the Hidden Shifted Power Problem</title><categories>cs.CC math.NT</categories><comments>Moubariz Garaev (who has now become a co-author) has introduced some
  new ideas that have led to stronger results. Several imprecision of the
  previous version have been corrected too</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a hidden element $s$ of a finite field
$\F_q$ of $q$ elements from queries to an oracle that for a given $x\in \F_q$
returns $(x+s)^e$ for a given divisor $e\mid q-1$. We use some techniques from
additive combinatorics and analytic number theory that lead to more efficient
algorithms than the naive interpolation algorithm, for example, they use
substantially fewer queries to the oracle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0819</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0819</id><created>2011-10-04</created><authors><author><keyname>Oikonomou</keyname><forenames>Kostas N.</forenames></author></authors><title>Analytical Forms for Most Likely Matrices Derived from Incomplete
  Information</title><categories>cs.IT math.IT</categories><comments>International Journal of Systems Science, Taylor &amp; Francis Online, 02
  September 2010</comments><doi>10.1080/00207721.2010.502600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a rectangular matrix describing some type of communication or
transportation between a set of origins and a set of destinations, or a
classification of objects by two attributes. The problem is to infer the
entries of the matrix from limited information in the form of constraints,
generally the sums of the elements over various subsets of the matrix, such as
rows, columns, etc, or from bounds on these sums, down to individual elements.
Such problems are routinely addressed by applying the maximum entropy method to
compute the matrix numerically, but in this paper we derive analytical,
closed-form solutions. For the most complicated cases we consider the solution
depends on the root of a non-linear equation, for which we provide an
analytical approximation in the form of a power series. Some of our solutions
extend to 3-dimensional matrices. Besides being valid for matrices of arbitrary
size, the analytical solutions exhibit many of the appealing properties of
maximum entropy, such as precise use of the available data, intuitive behavior
with respect to changes in the constraints, and logical consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0864</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0864</id><created>2011-10-04</created><updated>2012-04-10</updated><authors><author><keyname>Kurihara</keyname><forenames>Kazutaka</forenames></author></authors><title>CinemaGazer: a System for Watching Video at Very High Speed</title><categories>cs.HC</categories><comments>8 pages, 9 figures</comments><acm-class>H.5.1; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technology that enables the watching of videos at very
high speed. Subtitles are widely used in DVD movies, and provide useful
supplemental information for understanding video contents. We propose a
&quot;two-level fast-forwarding&quot; scheme for videos with subtitles, which controls
the speed of playback depending on the context: very fast during segments
without language, such as subtitles or speech, and &quot;understandably fast&quot; during
segments with such language. This makes it possible to watch videos at a higher
speed than usual while preserving the entertainment values of the contents. We
also propose &quot;centering&quot; and &quot;fading&quot; features for the display of subtitles to
reduce fatigue when watching high-speed video. We implement a versatile video
encoder that enables movie viewing with two-level fast-forwarding on any mobile
device by specifying the speed of playback, the reading rate, or the overall
viewing time. The effectiveness of our proposed method was demonstrated in an
evaluation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0872</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0872</id><created>2011-10-04</created><authors><author><keyname>Kubota</keyname><forenames>Toshiro</forenames></author></authors><title>Non-Gaussian Scale Space Filtering with 2 by 2 Matrix of Linear Filters</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Construction of a scale space with a convolution filter has been studied
extensively in the past. It has been proven that the only convolution kernel
that satisfies the scale space requirements is a Gaussian type. In this paper,
we consider a matrix of convolution filters introduced in [1] as a building
kernel for a scale space, and shows that we can construct a non-Gaussian scale
space with a $2\times 2$ matrix of filters. The paper derives sufficient
conditions for the matrix of filters for being a scale space kernel, and
present some numerical demonstrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0879</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0879</id><created>2011-10-04</created><authors><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author></authors><title>Linearized Additive Classifiers</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the additive model learning literature and adapt a penalized
spline formulation due to Eilers and Marx, to train additive classifiers
efficiently. We also propose two new embeddings based two classes of orthogonal
basis with orthogonal derivatives, which can also be used to efficiently learn
additive classifiers. This paper follows the popular theme in the current
literature where kernel SVMs are learned much more efficiently using a
approximate embedding and linear machine. In this paper we show that spline
basis are especially well suited for learning additive models because of their
sparsity structure and the ease of computing the embedding which enables one to
train these models in an online manner, without incurring the memory overhead
of precomputing the storing the embeddings. We show interesting connections
between B-Spline basis and histogram intersection kernel and show that for a
particular choice of regularization and degree of the B-Splines, our proposed
learning algorithm closely approximates the histogram intersection kernel SVM.
This enables one to learn additive models with almost no memory overhead
compared to fast a linear solver, such as LIBLINEAR, while being only 5-6X
slower on average. On two large scale image classification datasets, MNIST and
Daimler Chrysler pedestrians, the proposed additive classifiers are as accurate
as the kernel SVM, while being two orders of magnitude faster to train.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0881</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0881</id><created>2011-10-04</created><updated>2011-11-21</updated><authors><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author><author><keyname>Wang</keyname><forenames>Chuang</forenames></author><author><keyname>Xiao</keyname><forenames>Jing-Qing</forenames></author><author><keyname>Bi</keyname><forenames>Zedong</forenames></author></authors><title>Partition Function Expansion on Region-Graphs and Message-Passing
  Equations</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>10 pages including two figures. New theoretical and numerical results
  added. Will be published by JSTAT as a letter</comments><journal-ref>J. Stat. Mech.: Theor. Exper. L12001 (2011)</journal-ref><doi>10.1088/1742-5468/2011/12/L12001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disordered and frustrated graphical systems are ubiquitous in physics,
biology, and information science. For models on complete graphs or random
graphs, deep understanding has been achieved through the mean-field replica and
cavity methods. But finite-dimensional `real' systems persist to be very
challenging because of the abundance of short loops and strong local
correlations. A statistical mechanics theory is constructed in this paper for
finite-dimensional models based on the mathematical framework of partition
function expansion and the concept of region-graphs. Rigorous expressions for
the free energy and grand free energy are derived. Message-passing equations on
the region-graph, such as belief-propagation and survey-propagation, are also
derived rigorously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0886</identifier>
 <datestamp>2012-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0886</id><created>2011-10-04</created><updated>2012-07-29</updated><authors><author><keyname>Kao</keyname><forenames>David T. -H.</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Two-User Interference Channels with Local Views: On Capacity Regions of
  TDM-Dominating Policies</title><categories>cs.IT math.IT</categories><comments>revised 22 Jun, including updated title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity regions of two-user interference channels where
transmitters base their transmission schemes on local views of the channel
state. Under the local view model, each transmitter knows only a subset of the
four channel gains, which may be mismatched from the other transmitter.
  We consider a set of seven local views, and find that for five out of the
seven local views, TDM is sufficient to achieve the qualified notion of
capacity region for the linear deterministic interference channel which
approximates the Gaussian interference channel. For these five local views, the
qualified capacity result implies that no policy can achieve a rate point
outside the TDM region without inducing a corner case of sub-TDM performance in
another channel state. The common trait shared by the two remaining local views
- those with the potential to outperform TDM - is transmitter knowledge of the
outgoing interference link accompanied by some common knowledge of state,
emphasizing their importance in creating opportunities to coordinate usage of
more advanced schemes.
  Our conclusions are extended to bounded gap characterizations of the capacity
region for the Gaussian interference channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0892</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0892</id><created>2011-10-05</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Roy</keyname><forenames>Swapnoneel</forenames></author></authors><title>On Approximability of Block Sorting</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block Sorting is a well studied problem, motivated by its applications in
Optical Character Recognition (OCR), and Computational Biology. Block Sorting
has been shown to be NP-Hard, and two separate polynomial time 2-approximation
algorithms have been designed for the problem. But questions like whether a
better approximation algorithm can be designed, and whether the problem is
APX-Hard have been open for quite a while now.
  In this work we answer the latter question by proving Block Sorting to be
Max-SNP-Hard (APX-Hard). The APX-Hardness result is based on a linear reduction
of Max-3SAT to Block Sorting. We also provide a new lower bound for the problem
via a new parametrized problem k-Block Merging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0895</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0895</id><created>2011-10-05</created><updated>2012-07-02</updated><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr</forenames></author><author><keyname>Friedlander</keyname><forenames>Michael P.</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Tristan</forenames></author></authors><title>Robust inversion via semistochastic dimensionality reduction</title><categories>cs.CE cs.NA</categories><comments>Mathematical Programming, 2012</comments><doi>10.1007/s10107-012-0571-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of inverse problems where it is possible to aggregate the
results of multiple experiments. This class includes problems where the forward
model is the solution operator to linear ODEs or PDEs. The tremendous size of
such problems motivates dimensionality reduction techniques based on randomly
mixing experiments. These techniques break down, however, when robust
data-fitting formulations are used, which are essential in cases of missing
data, unusually large errors, and systematic features in the data unexplained
by the forward model. We survey robust methods within a statistical framework,
and propose a semistochastic optimization approach that allows dimensionality
reduction. The efficacy of the methods are demonstrated for a large-scale
seismic inverse problem using the robust Student's t-distribution, where a
useful synthetic velocity model is recovered in the extreme scenario of 60%
data missing at random. The semistochastic approach achieves this recovery
using 20% of the effort required by a direct robust approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0897</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0897</id><created>2011-10-05</created><updated>2011-10-06</updated><authors><author><keyname>Ren</keyname><forenames>Tian Peng</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Zhang</keyname><forenames>Er Yang</forenames></author></authors><title>Block-Orthogonal Space-Time Code Structure and Its Impact on QRDM
  Decoding Complexity Reduction</title><categories>cs.IT math.IT</categories><comments>IEEE Journal of Selected Topics in Signal Processing, Vol. 5, No. 8,
  December 2011</comments><doi>10.1109/JSTSP.2011.2166755</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-rate space time codes (STC) with rate = number of transmit antennas have
high multiplexing gain, but high decoding complexity even when decoded using
reduced-complexity decoders such as sphere or QRDM decoders. In this paper, we
introduce a new code property of STC called block-orthogonal property, which
can be exploited by QR-decomposition-based decoders to achieve significant
decoding complexity reduction without performance loss. We show that such
complexity reduction principle can benefit the existing algebraic codes such as
Perfect and DjABBA codes due to their inherent (but previously undiscovered)
block-orthogonal property. In addition, we construct and optimize new full-rate
BOSTC (Block-Orthogonal STC) that further maximize the QRDM complexity
reduction potential. Simulation results of bit error rate (BER) performance
against decoding complexity show that the new BOSTC outperforms all previously
known codes as long as the QRDM decoder operates in reduced-complexity mode,
and the code exhibits a desirable complexity saturation property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0911</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0911</id><created>2011-10-05</created><updated>2012-08-28</updated><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Purkayastha</keyname><forenames>Punarbasu</forenames></author></authors><title>Estimates on the Size of Symbol Weight Codes</title><categories>cs.IT math.IT</categories><comments>14 pages, 4 figures</comments><msc-class>94B65, 94B25</msc-class><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of codes for powerlines communication has garnered much interest
over the past decade. Various types of codes such as permutation codes,
frequency permutation arrays, and constant composition codes have been proposed
over the years. In this work we study a type of code called the bounded symbol
weight codes which was first introduced by Versfeld et al. in 2005, and a
related family of codes that we term constant symbol weight codes. We provide
new upper and lower bounds on the size of bounded symbol weight and constant
symbol weight codes. We also give direct and recursive constructions of codes
for certain parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0937</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0937</id><created>2011-10-05</created><authors><author><keyname>Belikov</keyname><forenames>A. N.</forenames></author><author><keyname>Dijkstra</keyname><forenames>F.</forenames></author><author><keyname>Gankema</keyname><forenames>J. A.</forenames></author><author><keyname>van Hoof</keyname><forenames>J. B. A. N.</forenames></author><author><keyname>Koopman</keyname><forenames>R.</forenames></author></authors><title>Information Systems Playground - The Target Infrastructure, Scaling
  Astro-WISE into the Petabyte range</title><categories>astro-ph.IM cs.DC</categories><comments>Accepted for publication in topical issue of Experimental Astronomy
  on Astro-WISE information system</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Target infrastructure has been specially built as a storage and compute
infrastructure for the information systems derived from Astro-WISE. This
infrastructure will be used by several applications that collaborate in the
area of information systems within the Target project. It currently consists of
10 PB of storage and thousands of computational cores. The infrastructure has
been constructed based on the requirements of the applications. The storage is
controlled by the Global Parallel File System of IBM. This file system takes
care of the required flexibility by combining storage hardware with different
characteristics into a single file system. It is also very scalable, which
allows the system to be extended into the future, while replacing old hardware
with new technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0938</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0938</id><created>2011-10-05</created><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author></authors><title>Wireless Connectivity and Capacity</title><categories>cs.DS cs.NI</categories><comments>to appear in SODA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ wireless transceivers located in a plane, a fundamental problem in
wireless communications is to construct a strongly connected digraph on them
such that the constituent links can be scheduled in fewest possible time slots,
assuming the SINR model of interference.
  In this paper, we provide an algorithm that connects an arbitrary point set
in $O(\log n)$ slots, improving on the previous best bound of $O(\log^2 n)$ due
to Moscibroda. This is complemented with a super-constant lower bound on our
approach to connectivity. An important feature is that the algorithms allow for
bi-directional (half-duplex) communication.
  One implication of this result is an improved bound of $\Omega(1/\log n)$ on
the worst-case capacity of wireless networks, matching the best bound known for
the extensively studied average-case.
  We explore the utility of oblivious power assignments, and show that
essentially all such assignments result in a worst case bound of $\Omega(n)$
slots for connectivity. This rules out a recent claim of a $O(\log n)$ bound
using oblivious power. On the other hand, using our result we show that
$O(\min(\log \Delta, \log n \cdot (\log n + \log \log \Delta)))$ slots suffice,
where $\Delta$ is the ratio between the largest and the smallest links in a
minimum spanning tree of the points.
  Our results extend to the related problem of minimum latency aggregation
scheduling, where we show that aggregation scheduling with $O(\log n)$ latency
is possible, improving upon the previous best known latency of $O(\log^3 n)$.
We also initiate the study of network design problems in the SINR model beyond
strong connectivity, obtaining similar bounds for biconnected and $k$-edge
connected structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0957</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0957</id><created>2011-10-05</created><authors><author><keyname>Couzinie-Devy</keyname><forenames>Florent</forenames></author><author><keyname>Mairal</keyname><forenames>Julien</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Ponce</keyname><forenames>Jean</forenames></author></authors><title>Dictionary Learning for Deblurring and Digital Zoom</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach to image deblurring and digital zooming
using sparse local models of image appearance. These models, where small image
patches are represented as linear combinations of a few elements drawn from
some large set (dictionary) of candidates, have proven well adapted to several
image restoration tasks. A key to their success has been to learn dictionaries
adapted to the reconstruction of small image patches. In contrast, recent works
have proposed instead to learn dictionaries which are not only adapted to data
reconstruction, but also tuned for a specific task. We introduce here such an
approach to deblurring and digital zoom, using pairs of blurry/sharp (or
low-/high-resolution) images for training, as well as an effective stochastic
gradient algorithm for solving the corresponding optimization task. Although
this learning problem is not convex, once the dictionaries have been learned,
the sharp/high-resolution image can be recovered via convex optimization at
test time. Experiments with synthetic and real data demonstrate the
effectiveness of the proposed approach, leading to state-of-the-art performance
for non-blind image deblurring and digital zoom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0976</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0976</id><created>2011-10-05</created><authors><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>So&#x142;tys</keyname><forenames>Karolina</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author><author><keyname>Wu</keyname><forenames>Xi</forenames></author></authors><title>Hierarchies of Inefficient Kernelizability</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of Bodlaender et al. (ICALP 2008) and Fortnow and Santhanam
(STOC 2008) allows us to exclude the existence of polynomial kernels for a
range of problems under reasonable complexity-theoretical assumptions. However,
there are also some issues that are not addressed by this framework, including
the existence of Turing kernels such as the &quot;kernelization&quot; of Leaf Out
Branching(k) into a disjunction over n instances of size poly(k). Observing
that Turing kernels are preserved by polynomial parametric transformations, we
define a kernelization hardness hierarchy, akin to the M- and W-hierarchy of
ordinary parameterized complexity, by the PPT-closure of problems that seem
likely to be fundamentally hard for efficient Turing kernelization. We find
that several previously considered problems are complete for our fundamental
hardness class, including Min Ones d-SAT(k), Binary NDTM Halting(k), Connected
Vertex Cover(k), and Clique(k log n), the clique problem parameterized by k log
n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0983</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0983</id><created>2011-10-05</created><authors><author><keyname>Gusenbauer</keyname><forenames>Markus</forenames></author><author><keyname>Kovacs</keyname><forenames>Alexander</forenames></author><author><keyname>Reichel</keyname><forenames>Franz</forenames></author><author><keyname>Exl</keyname><forenames>Lukas</forenames></author><author><keyname>Bance</keyname><forenames>Simon</forenames></author><author><keyname>Ozelt</keyname><forenames>Harald</forenames></author><author><keyname>Schrefl</keyname><forenames>Thomas</forenames></author></authors><title>Self-organizing magnetic beads for biomedical applications</title><categories>physics.bio-ph cs.CE physics.flu-dyn</categories><journal-ref>Journal of Magnetism and Magnetic Materials 324.6 (2012): 977-982</journal-ref><doi>10.1016/j.jmmm.2011.09.034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of biomedicine magnetic beads are used for drug delivery and to
treat hyperthermia. Here we propose to use self-organized bead structures to
isolate circulating tumor cells using lab-on-chip technologies. Typically blood
flows past microposts functionalized with antibodies for circulating tumor
cells. Creating these microposts with interacting magnetic beads makes it
possible to tune the geometry in size, position and shape. We developed a
simulation tool that combines micromagnetics and discrete particle dynamics, in
order to design micropost arrays made of interacting beads. The simulation
takes into account the viscous drag of the blood flow, magnetostatic
interactions between the magnetic beads and gradient forces from external
aligned magnets. We developed a particle-particle particle-mesh method for
effective computation of the magnetic force and torque acting on the particles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0990</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0990</id><created>2011-10-05</created><authors><author><keyname>Molinaro</keyname><forenames>Marco</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author></authors><title>The Query-commit Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the query-commit problem we are given a graph where edges have distinct
probabilities of existing. It is possible to query the edges of the graph, and
if the queried edge exists then its endpoints are irrevocably matched. The goal
is to find a querying strategy which maximizes the expected size of the
matching obtained. This stochastic matching setup is motivated by applications
in kidney exchanges and online dating.
  In this paper we address the query-commit problem from both theoretical and
experimental perspectives. First, we show that a simple class of edges can be
queried without compromising the optimality of the strategy. This property is
then used to obtain in polynomial time an optimal querying strategy when the
input graph is sparse. Next we turn our attentions to the kidney exchange
application, focusing on instances modeled over real data from existing
exchange programs. We prove that, as the number of nodes grows, almost every
instance admits a strategy which matches almost all nodes. This result supports
the intuition that more exchanges are possible on a larger pool of
patient/donors and gives theoretical justification for unifying the existing
exchange programs. Finally, we evaluate experimentally different querying
strategies over kidney exchange instances. We show that even very simple
heuristics perform fairly well, being within 1.5% of an optimal clairvoyant
strategy, that knows in advance the edges in the graph. In such a
time-sensitive application, this result motivates the use of committing
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0995</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0995</id><created>2011-10-05</created><authors><author><keyname>Gusenbauer</keyname><forenames>Markus</forenames></author><author><keyname>Cimrak</keyname><forenames>Ivan</forenames></author><author><keyname>Bance</keyname><forenames>Simon</forenames></author><author><keyname>Exl</keyname><forenames>Lukas</forenames></author><author><keyname>Reichel</keyname><forenames>Franz</forenames></author><author><keyname>Oezelt</keyname><forenames>Harald</forenames></author><author><keyname>Schrefl</keyname><forenames>Thomas</forenames></author></authors><title>A tunable cancer cell filter using magnetic beads: cellular and fluid
  dynamic simulations</title><categories>physics.flu-dyn cs.CE physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of biomedicine magnetic beads are used for drug delivery and to
treat hyperthermia. Here we propose to use self-organized bead structures to
isolate circulating tumor cells using lab-on-chip technologies. Typically blood
flows past microposts functionalized with antibodies for circulating tumor
cells. Creating these microposts with interacting magnetic beads makes it
possible to tune the geometry in size, position and shape. We develop a
simulation tool that combines micromagnetics, discrete particle dynamics and
fluid dynamics, in order to design micropost arrays made of interacting beads.
For the simulation of blood flow we use the Lattice-Boltzmann method with
immersed elastic blood cell models. Parallelization distributes large fluid and
particle dynamic simulations over available resources to reduce overall
calculation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.0999</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.0999</id><created>2011-10-05</created><authors><author><keyname>Fioravanti</keyname><forenames>Fabio</forenames></author><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author><author><keyname>Senni</keyname><forenames>Valerio</forenames></author></authors><title>Generalization Strategies for the Verification of Infinite State Systems</title><categories>cs.LO cs.AI cs.SE</categories><comments>24 pages, 2 figures, 5 tables</comments><msc-class>68T15 (Primary) 68N17, 68T27 (Secondary)</msc-class><acm-class>D.1.6; D.2.4; F.3.1; I.2.2; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for the automated verification of temporal properties of
infinite state systems. Our verification method is based on the specialization
of constraint logic programs (CLP) and works in two phases: (1) in the first
phase, a CLP specification of an infinite state system is specialized with
respect to the initial state of the system and the temporal property to be
verified, and (2) in the second phase, the specialized program is evaluated by
using a bottom-up strategy. The effectiveness of the method strongly depends on
the generalization strategy which is applied during the program specialization
phase. We consider several generalization strategies obtained by combining
techniques already known in the field of program analysis and program
transformation, and we also introduce some new strategies. Then, through many
verification experiments, we evaluate the effectiveness of the generalization
strategies we have considered. Finally, we compare the implementation of our
specialization-based verification method to other constraint-based model
checking tools. The experimental results show that our method is competitive
with the methods used by those other tools. To appear in Theory and Practice of
Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1015</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1015</id><created>2011-10-05</created><updated>2011-10-13</updated><authors><author><keyname>Perri</keyname><forenames>Simona</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author><author><keyname>Sirianni</keyname><forenames>Marco</forenames></author></authors><title>Parallel Instantiation of ASP Programs: Techniques and Experiments</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a powerful logic-based programming language,
which is enjoying increasing interest within the scientific community and (very
recently) in industry. The evaluation of ASP programs is traditionally carried
out in two steps. At the first step an input program P undergoes the so-called
instantiation (or grounding) process, which produces a program P' semantically
equivalent to P, but not containing any variable; in turn, P' is evaluated by
using a backtracking search algorithm in the second step. It is well-known that
instantiation is important for the efficiency of the whole evaluation, might
become a bottleneck in common situations, is crucial in several realworld
applications, and is particularly relevant when huge input data has to be dealt
with. At the time of this writing, the available instantiator modules are not
able to exploit satisfactorily the latest hardware, featuring
multi-core/multi-processor SMP (Symmetric MultiProcessing) technologies. This
paper presents some parallel instantiation techniques, including load-balancing
and granularity control heuristics, which allow for the effective exploitation
of the processing power offered by modern SMP machines. This is confirmed by an
extensive experimental analysis herein reported.
  To appear in Theory and Practice of Logic Programming (TPLP).
  KEYWORDS: Answer Set Programming, Instantiation, Parallelism, Heuristics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1016</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1016</id><created>2011-09-29</created><authors><author><keyname>Edelkamp</keyname><forenames>S.</forenames></author><author><keyname>Englert</keyname><forenames>R.</forenames></author><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author><author><keyname>Liporace</keyname><forenames>F.</forenames></author><author><keyname>Thiebaux</keyname><forenames>S.</forenames></author><author><keyname>Trueg</keyname><forenames>S.</forenames></author></authors><title>Engineering Benchmarks for Planning: the Domains Used in the
  Deterministic Part of IPC-4</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  453-541, 2006</journal-ref><doi>10.1613/jair.1982</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a field of research about general reasoning mechanisms, it is essential to
have appropriate benchmarks. Ideally, the benchmarks should reflect possible
applications of the developed technology. In AI Planning, researchers more and
more tend to draw their testing examples from the benchmark collections used in
the International Planning Competition (IPC). In the organization of (the
deterministic part of) the fourth IPC, IPC-4, the authors therefore invested
significant effort to create a useful set of benchmarks. They come from five
different (potential) real-world applications of planning: airport ground
traffic control, oil derivative transportation in pipeline networks,
model-checking safety properties, power supply restoration, and UMTS call
setup. Adapting and preparing such an application for use as a benchmark in the
IPC involves, at the time, inevitable (often drastic) simplifications, as well
as careful choice between, and engineering of, domain encodings. For the first
time in the IPC, we used compilations to formulate complex domain features in
simple languages such as STRIPS, rather than just dropping the more interesting
problem constraints in the simpler language subsets. The article explains and
discusses the five application domains and their adaptation to form the PDDL
test suites used in IPC-4. We summarize known theoretical results on structural
properties of the domains, regarding their computational complexity and
provable properties of their topology under the h+ function (an idealized
version of the relaxed plan heuristic). We present new (empirical) results
illuminating properties such as the quality of the most wide-spread heuristic
functions (planning graph, serial planning graph, and relaxed plan), the growth
of propositional representations over instance size, and the number of actions
available to achieve each fact; we discuss these data in conjunction with the
best results achieved by the different kinds of planners participating in
IPC-4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1029</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1029</id><created>2011-10-05</created><updated>2011-10-27</updated><authors><author><keyname>Fischbach</keyname><forenames>Marcell</forenames></author><author><keyname>Meurer</keyname><forenames>Benedikt</forenames></author></authors><title>Towards a native toplevel for the OCaml language</title><categories>cs.PL cs.PF</categories><comments>10 pages, 5 figures, technical report</comments><acm-class>D.3.3; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the current state of our work on an interactive toplevel
for the OCaml language based on the optimizing native code compiler and
runtime. Our native toplevel is up to 100 times faster than the default OCaml
toplevel, which is based on the byte code compiler and interpreter. It uses
Just-In-Time techniques to compile toplevel phrases to native code at runtime,
and currently works with various Unix-like systems running on x86 or x86-64
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1038</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1038</id><created>2011-10-05</created><authors><author><keyname>Soleimani</keyname><forenames>Parisa</forenames></author><author><keyname>Sabbaghi-Nadooshan</keyname><forenames>Reza</forenames></author><author><keyname>Mirzakuchaki</keyname><forenames>Sattar</forenames></author><author><keyname>Bagheri</keyname><forenames>Mahdi</forenames></author></authors><title>Using Genetic Algorithm in the Evolutionary Design of Sequential Logic
  Circuits</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolvable hardware (EHW) is a set of techniques that are based on the idea of
combining reconfiguration hardware systems with evolutionary algorithms. In
other word, EHW has two sections; the reconfigurable hardware and evolutionary
algorithm where the configurations are under the control of an evolutionary
algorithm. This paper, suggests a method to design and optimize the synchronous
sequential circuits. Genetic algorithm (GA) was applied as evolutionary
algorithm. In this approach, for building input combinational logic circuit of
each DFF, and also output combinational logic circuit, the cell arrays have
been used. The obtained results show that our method can reduce the average
number of generations by limitation the search space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1044</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1044</id><created>2011-10-05</created><authors><author><keyname>Oliveira</keyname><forenames>Roberto I.</forenames></author><author><keyname>Prata</keyname><forenames>Alan</forenames></author></authors><title>Rumor Spreading on Percolation Graphs</title><categories>math.PR cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relation between the performance of the randomized rumor
spreading (push model) in a d-regular graph G and the performance of the same
algorithm in the percolated graph G_p. We show that if the push model
successfully broadcast the rumor within T rounds in the graph G then only (1 +
\epsilon)T rounds are needed to spread the rumor in the graph G_p when T =
o(pd).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1046</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1046</id><created>2011-10-05</created><authors><author><keyname>Javashi</keyname><forenames>Hamid</forenames></author><author><keyname>Sabbaghi-Nadooshan</keyname><forenames>Reza</forenames></author></authors><title>A Novel Elliptic curve cryptography Processor using NoC design</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an elliptic curve key generation processor over
GF(2m) and GF(P) with Network-on-Chip (NoC) design scheme based on binary
scalar multiplication algorithm. Over the Two last decades, Elliptic Curve
Cryptography (ECC) has gained increasing acceptance in the industry and the
academic community. This interest is mainly caused by the same level of
security with relatively small keys provided by ECC comparing to large key size
in Rivest Shamir Adleman (RSA). Parallelism can be utilized in different
hierarchy levels as shown in many publications. By using NoC, a new method with
the reduced latency of point multiplication (with parallel field arithmetic) is
introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1052</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1052</id><created>2011-10-05</created><authors><author><keyname>Stern</keyname><forenames>Jesse</forenames></author></authors><title>Spider Solitaire is NP-Complete</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project investigates the potential of computers to solve complex tasks
such as games. The paper proves that the complexity of a generalized version of
spider solitaire is NP-Complete and uses much of structure of the proof that
FreeCell is NP-Hard in the paper Helmert, M. &quot;Complexity Results for Standard
Benchmark Domains in Planning.&quot; Artificial Intelligence 143.2 (2003): 219-62.
Print. A given decision problem falls in to the class NP-Complete if it is
proven to be both in NP and in NP-Hard. To prove that this is the case the
paper shows that, not only do the kinds of possible moves that can be reversed
prove this, but it is also shown that no spider solitaire game of size n will
take more than a polynomial number of moves to complete if such a completion is
possible. The paper reduces 3-SAT to SpiderSolitaire (the name used throughout
the proof when referring to the generalized version of popular solitaire
variant &quot;Spider Solitaire&quot;) by showing that any 3-SAT instance can be
replicated using an appropriately arranged initial tableau. The example
provided reinforces the proof of NP-Hardness and helps to make the proof easier
to understand, but the definitive proof lies in the equations providing
instruction on how to set up any 3-SAT instance of clause size C as a instance
of SpiderSolitaire.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1060</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1060</id><created>2011-10-05</created><updated>2012-08-30</updated><authors><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Kim</keyname><forenames>Dongho</forenames></author><author><keyname>Hu</keyname><forenames>Yih-Chun</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author></authors><title>Mirage: Towards Deployable DDoS Defense for Web Applications</title><categories>cs.NI cs.CR</categories><comments>16 pages, 8 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Denial of Service (DDoS) attacks form a serious threat to the
security of Internet services. However, despite over a decade of research, and
the existence of several proposals to address this problem, there has been
little progress to date on actual adoption. We present Mirage, a protocol that
achieves comparable performance to other DDoS mitigation schemes while
providing benefits when deployed only in the server's local network and its
upstream ISP, where local business objectives may incentivize deployment.
Mirage does not require source end hosts to install any software to access
Mirage protected websites. Unlike previous proposals, Mirage only requires
functionality from routers that is already deployed in today's routers, though
this functionality may need to be scaled depending on the point of deployment.
  Our approach is that end hosts can thwart the attackers by employing the
principle of a moving target: end hosts in our architecture periodically change
IP addresses to keep the attackers guessing. Knowledge of an active IP address
of the destination end host can act as an implicit authorization to send data.
We evaluate Mirage using theoretical analysis, simulations and a prototype
implementation on PlanetLab. We find that our design provides a first step
towards a deployable, yet effective DDoS defense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1064</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1064</id><created>2011-10-05</created><authors><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Tan</keyname><forenames>Ning</forenames></author></authors><title>Approximating CSPs with Global Cardinality Constraints Using SDP
  Hierarchies</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with approximating constraint satisfaction problems
(CSPs) with an additional global cardinality constraints. For example, \maxcut
is a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find
a cut $S \cup \bar S = V$ that maximizes the numberof crossing edges,
$|E(S,\bar S)|$. The \maxbisection problem is a variant of \maxcut with an
additional global constraint that each side of the cut has exactly half the
vertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like
\minbisection and approximating Graph Expansion can be formulated as CSPs with
global constraints.
  In this work, we formulate a general approach towards approximating CSPs with
global constraints using SDP hierarchies. To demonstrate the approach we
present the following results:
  Using the Lasserre hierarchy, we present an algorithm that runs in time
$O(n^{poly(1/\epsilon)})$ that given an instance of \maxbisection with value
$1-\epsilon$, finds a bisection with value $1-O(\sqrt{\epsilon})$. This
approximation is near-optimal (up to constant factors in $O()$) under the
Unique Games Conjecture.
  By a computer-assisted proof, we show that the same algorithm also achieves a
0.85-approximation for \maxbisection, improving on the previous bound of 0.70
(note that it is \uniquegames hard to approximate better than a 0.878 factor).
The same algorithm also yields a 0.92-approximation for \maxtwosat with
cardinality constraints.
  For every CSP with a global cardinality constraints, we present a generic
conversion from integrality gap instances for the Lasserre hierarchy to a {\it
dictatorship test} whose soundness is at most integrality gap. Dictatorship
testing gadgets are central to hardness results for CSPs, and a generic
conversion of the above nature lies at the core of the tight Unique Games based
hardness result for CSPs. \cite{Raghavendra08}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1073</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1073</id><created>2011-10-05</created><authors><author><keyname>Knoblock</keyname><forenames>C. A.</forenames></author><author><keyname>Minton</keyname><forenames>S.</forenames></author><author><keyname>Muslea</keyname><forenames>I.</forenames></author></authors><title>Active Learning with Multiple Views</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  203-233, 2006</journal-ref><doi>10.1613/jair.2005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learners alleviate the burden of labeling large amounts of data by
detecting and asking the user to label only the most informative examples in
the domain. We focus here on active learning for multi-view domains, in which
there are several disjoint subsets of features (views), each of which is
sufficient to learn the target concept. In this paper we make several
contributions. First, we introduce Co-Testing, which is the first approach to
multi-view active learning. Second, we extend the multi-view learning framework
by also exploiting weak views, which are adequate only for learning a concept
that is more general/specific than the target concept. Finally, we empirically
show that Co-Testing outperforms existing active learners on a variety of real
world domains such as wrapper induction, Web page classification, advertisement
removal, and discourse tree parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1075</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1075</id><created>2011-10-05</created><authors><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author><author><keyname>Mavroforakis</keyname><forenames>Michael</forenames></author></authors><title>The Augmented Complex Kernel LMS</title><categories>cs.LG</categories><comments>manuscript submitted to IEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2012.2200479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a unified framework for adaptive kernel based signal processing of
complex data was presented by the authors, which, besides offering techniques
to map the input data to complex Reproducing Kernel Hilbert Spaces, developed a
suitable Wirtinger-like Calculus for general Hilbert Spaces. In this short
paper, the extended Wirtinger's calculus is adopted to derive complex
kernel-based widely-linear estimation filters. Furthermore, we illuminate
several important characteristics of the widely linear filters. We show that,
although in many cases the gains from adopting widely linear estimation
filters, as alternatives to ordinary linear ones, are rudimentary, for the case
of kernel based widely linear filters significant performance improvements can
be obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1078</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1078</id><created>2011-10-05</created><authors><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Nehorai</keyname><forenames>Arye</forenames></author></authors><title>Fixed point theory and semidefinite programming for computable
  performance analysis of block-sparsity recovery</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we employ fixed point theory and semidefinite programming to
compute the performance bounds on convex block-sparsity recovery algorithms. As
a prerequisite for optimal sensing matrix design, a computable performance
bound would open doors for wide applications in sensor arrays, radar, DNA
microarrays, and many other areas where block-sparsity arises naturally. We
define a family of goodness measures for arbitrary sensing matrices as the
optimal values of certain optimization problems. The reconstruction errors of
convex recovery algorithms are bounded in terms of these goodness measures. We
demonstrate that as long as the number of measurements is relatively large,
these goodness measures are bounded away from zero for a large class of random
sensing matrices, a result parallel to the probabilistic analysis of the block
restricted isometry property. As the primary contribution of this work, we
associate the goodness measures with the fixed points of functions defined by a
series of semidefinite programs. This relation with fixed point theory yields
efficient algorithms with global convergence guarantees to compute the goodness
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1079</identifier>
 <datestamp>2011-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1079</id><created>2011-10-05</created><authors><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Rosen</keyname><forenames>Michal</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>A Near-Optimal Sublinear-Time Algorithm for Approximating the Minimum
  Vertex Cover Size</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a nearly optimal sublinear-time algorithm for approximating the size
of a minimum vertex cover in a graph G. The algorithm may query the degree
deg(v) of any vertex v of its choice, and for each 1 &lt;= i &lt;= deg(v), it may ask
for the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex
cover in G, the algorithm outputs, with high constant success probability, an
estimate VC_estimate(G) such that VC_opt(G) &lt;= VC_estimate(G) &lt;= 2 * VC_opt(G)
+ epsilon*n, where epsilon is a given additive approximation parameter. We
refer to such an estimate as a (2,epsilon)-estimate. The query complexity and
running time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg
denotes the average vertex degree in the graph. The best previously known
sublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and
running time O(d^4/epsilon^2), where d is the maximum degree in the graph.
Given the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining
such an estimate (with any constant multiplicative factor) due to Parnas and
Ron (TCS 2007), our result is nearly optimal.
  In the case that the graph is dense, that is, the number of edges is
Theta(n^2), we consider another model, in which the algorithm may ask, for any
pair of vertices u and v, whether there is an edge between u and v. We show how
to adapt the algorithm that uses neighbor queries to this model and obtain an
algorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex
cover whose query complexity and running time are ~O(n) * poly(1/epsilon).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1091</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1091</id><created>2011-10-05</created><updated>2012-05-07</updated><authors><author><keyname>Lemmen</keyname><forenames>Carsten</forenames></author><author><keyname>Khan</keyname><forenames>Aurangzeb</forenames></author></authors><title>A simulation of the Neolithic transition in the Indus valley</title><categories>q-bio.PE cs.MA</categories><comments>Chapter manuscript revision submitted to AGU monograph &quot;Climates,
  landscapes and civilizations&quot;, 6 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Indus Valley Civilization (IVC) was one of the first great civilizations
in prehistory. This bronze age civilization flourished from the end of the
fourth millennium BC. It disintegrated during the second millennium BC; despite
much research effort, this decline is not well understood. Less research has
been devoted to the emergence of the IVC, which shows continuous cultural
precursors since at least the seventh millennium BC. To understand the decline,
we believe it is necessary to investigate the rise of the IVC, i.e., the
establishment of agriculture and livestock, dense populations and technological
developments 7000--3000 BC. Although much archaeological information is
available, our capability to investigate the system is hindered by poorly
resolved chronology, and by a lack of field work in the intermediate areas
between the Indus valley and Mesopotamia. We thus employ a complementary
numerical simulation to develop a consistent picture of technology,
agropastoralism and population developments in the IVC domain. Results from
this Global Land Use and technological Evolution Simulator show that there is
(1) fair agreement between the simulated timing of the agricultural transition
and radiocarbon dates from early agricultural sites, but the transition is
simulated first in India then Pakistan; (2) an independent agropastoralism
developing on the Indian subcontinent; and (3) a positive relationship between
archeological artifact richness and simulated population density which remains
to be quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1112</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1112</id><created>2011-10-05</created><authors><author><keyname>Kang</keyname><forenames>Changsung</forenames></author><author><keyname>Lin</keyname><forenames>Xiaotong</forenames></author><author><keyname>Wang</keyname><forenames>Xuanhui</forenames></author><author><keyname>Chang</keyname><forenames>Yi</forenames></author><author><keyname>Tseng</keyname><forenames>Belle</forenames></author></authors><title>Modeling Perceived Relevance for Tail Queries without Click-Through Data</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Click-through data has been used in various ways in Web search such as
estimating relevance between documents and queries. Since only search snippets
are perceived by users before issuing any clicks, the relevance induced by
clicks are usually called \emph{perceived relevance} which has proven to be
quite useful for Web search. While there is plenty of click data for popular
queries, very little information is available for unpopular tail ones. These
tail queries take a large portion of the search volume but search accuracy for
these queries is usually unsatisfactory due to data sparseness such as limited
click information. In this paper, we study the problem of modeling perceived
relevance for queries without click-through data. Instead of relying on users'
click data, we carefully design a set of snippet features and use them to
approximately capture the perceived relevance. We study the effectiveness of
this set of snippet features in two settings: (1) predicting perceived
relevance and (2) enhancing search engine ranking. Experimental results show
that our proposed model is effective to predict the relative perceived
relevance of Web search results. Furthermore, our proposed snippet features are
effective to improve search accuracy for longer tail queries without
click-through data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1124</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1124</id><created>2011-10-05</created><authors><author><keyname>Chen</keyname><forenames>Shiyao</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author></authors><title>Optimal Deadline Scheduling with Commitment</title><categories>cs.DS</categories><comments>8 pages, 10 figures, 49th Allerton Conference on Communication,
  Control and Computing, Monticello, IL, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an online preemptive scheduling problem where jobs with deadlines
arrive sporadically. A commitment requirement is imposed such that the
scheduler has to either accept or decline a job immediately upon arrival. The
scheduler's decision to accept an arriving job constitutes a contract with the
customer; if the accepted job is not completed by its deadline as promised, the
scheduler loses the value of the corresponding job and has to pay an additional
penalty depending on the amount of unfinished workload. The objective of the
online scheduler is to maximize the overall profit, i.e., the total value of
the admitted jobs completed before their deadlines less the penalty paid for
the admitted jobs that miss their deadlines. We show that the maximum
competitive ratio is $3-2\sqrt{2}$ and propose a simple online algorithm to
achieve this competitive ratio. The optimal scheduling includes a threshold
admission and a greedy scheduling policies. The proposed algorithm has direct
applications to the charging of plug-in hybrid electrical vehicles (PHEV) at
garages or parking lots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1131</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1131</id><created>2011-10-05</created><authors><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Brodowicz</keyname><forenames>Maciej</forenames></author><author><keyname>Kaiser</keyname><forenames>Hartmut</forenames></author><author><keyname>Adelstein-Lelbach</keyname><forenames>Bryce</forenames></author><author><keyname>Sterling</keyname><forenames>Thomas</forenames></author></authors><title>Adaptive Mesh Refinement for Astrophysics Applications with ParalleX</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several applications in astrophysics require adequately resolving many
physical and temporal scales which vary over several orders of magnitude.
Adaptive mesh refinement techniques address this problem effectively but often
result in constrained strong scaling performance. The ParalleX execution model
is an experimental execution model that aims to expose new forms of program
parallelism and eliminate any global barriers present in a scaling-impaired
application such as adaptive mesh refinement. We present two astrophysics
applications using the ParalleX execution model: a tabulated equation of state
component for neutron star evolutions and a cosmology model evolution.
Performance and strong scaling results from both simulations are presented. The
tabulated equation of state data are distributed with transparent access over
the nodes of the cluster. This allows seamless overlapping of computation with
the latencies introduced by the remote access to the table. Because of the
expected size increases to the equation of state table, this type of table
partitioning for neutron star simulations is essential while the implementation
is greatly simplified by ParalleX semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1151</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1151</id><created>2011-10-05</created><authors><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author></authors><title>Mathematical aspects of decentralized control of formations in the plane</title><categories>math.OC cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In formation control, an ensemble of autonomous agents is required to
stabilize at a given configuration in the plane, doing so while agents are
allowed to observe only a subset of the ensemble. As such, formation control
provides a rich class of problems for decentralized control methods and
techniques. Additionally, it can be used to model a wide variety of scenarios
where decentralization is a main characteristic. We introduce here some
mathematical background necessary to address questions of stability in
decentralized control in general and formation control in particular. This
background includes an extension of the notion of global stability to systems
evolving on manifolds and a notion of robustness of feedback control for
nonlinear systems. We then formally introduce the class of formation control
problems, and summarize known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1152</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1152</id><created>2011-10-05</created><authors><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author></authors><title>Known unknowns, unknown unknowns and information flow: new concepts in
  decentralized control</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a model for decentral- ized control. The model is
broad enough to include problems such as formation control, decentralization of
the power grid and flocking. The objective of this paper is twofold. First, we
show how the issue of decentralization goes beyond having agents know only part
of the state of the system. In fact, we argue that a complete theory of
decentralization should take into account the fact that agents can be made
aware of only part of the global objective of the ensemble. A second
contribution of this paper is the introduction of a rigorous definition of
information flow for a decentralized system: we show how to attach to a general
nonlinear decentralized system a unique information flow graph that is an
invariant of the system. In order to address some finer issues in decentralized
system, such as the existence of so-called &quot;information loops&quot;, we further
refine the information flow graph to a simplicial complex-more precisely, a
Whitney complex. We illustrate the main results on a variety of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1161</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1161</id><created>2011-10-06</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>Interval edge-colorings of cubic graphs</title><categories>cs.DM math.CO</categories><comments>3 pages</comments><journal-ref>Proceedings of the CSIT Conference, Yerevan, 2011, pp. 86-88</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An edge-coloring of a multigraph G with colors 1,2,...,t is called an
interval t-coloring if all colors are used, and the colors of edges incident to
any vertex of G are distinct and form an interval of integers. In this paper we
prove that if G is a connected cubic multigraph (a connected cubic graph) that
admits an interval t-coloring, then t\leq |V(G)| +1 (t\leq |V(G)|), where V(G)
is the set of vertices of G. Moreover, if G is a connected cubic graph, G\neq
K_{4}, and G has an interval t-coloring, then t\leq |V(G)| -1. We also show
that these upper bounds are sharp. Finally, we prove that if G is a bipartite
subcubic multigraph, then G has an interval edge-coloring with no more than
four colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1165</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1165</id><created>2011-10-06</created><updated>2012-01-31</updated><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author><author><keyname>Khachatrian</keyname><forenames>Hrant H.</forenames></author><author><keyname>Yepremyan</keyname><forenames>Liana E.</forenames></author><author><keyname>Tananyan</keyname><forenames>Hovhannes G.</forenames></author></authors><title>Interval edge-colorings of graph products</title><categories>cs.DM math.CO</categories><comments>4 pages</comments><journal-ref>Proceedings of the CSIT Conference, Yerevan, 2011, pp. 89-92</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An interval t-coloring of a graph G is a proper edge-coloring of G with
colors 1,2,...,t such that at least one edge of G is colored by i, i=1,2,...,t,
and the edges incident to each vertex v\in V(G) are colored by d_{G}(v)
consecutive colors, where d_{G}(v) is the degree of the vertex v in G. In this
paper interval edge-colorings of various graph products are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1180</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1180</id><created>2011-10-06</created><updated>2012-07-02</updated><authors><author><keyname>Khopkar</keyname><forenames>Abhijeet</forenames></author><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author></authors><title>On Computing Optimal Locally Gabriel Graphs</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delaunay and Gabriel graphs are widely studied geometric proximity
structures. Motivated by applications in wireless routing, relaxed versions of
these graphs known as \emph{Locally Delaunay Graphs} ($LDGs$) and \emph{Locally
Gabriel Graphs} ($LGGs$) were proposed. We propose another generalization of
$LGGs$ called \emph{Generalized Locally Gabriel Graphs} ($GLGGs$) in the
context when certain edges are forbidden in the graph. Unlike a Gabriel Graph,
there is no unique $LGG$ or $GLGG$ for a given point set because no edge is
necessarily included or excluded. This property allows us to choose an
$LGG/GLGG$ that optimizes a parameter of interest in the graph. We show that
computing an edge maximum $GLGG$ for a given problem instance is NP-hard and
also APX-hard. We also show that computing an $LGG$ on a given point set with
dilation $\le k$ is NP-hard. Finally, we give an algorithm to verify whether a
given geometric graph $G=(V,E)$ is a valid $LGG$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1193</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1193</id><created>2011-10-06</created><updated>2012-04-04</updated><authors><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Gaborit</keyname><forenames>Philippe</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>A new class of codes for Boolean masking of cryptographic computations</title><categories>cs.IT math.IT</categories><comments>19 pages. IEEE Trans. on Information Theory, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of rate one-half binary codes: {\bf complementary
information set codes.} A binary linear code of length $2n$ and dimension $n$
is called a complementary information set code (CIS code for short) if it has
two disjoint information sets. This class of codes contains self-dual codes as
a subclass. It is connected to graph correlation immune Boolean functions of
use in the security of hardware implementations of cryptographic primitives.
Such codes permit to improve the cost of masking cryptographic algorithms
against side channel attacks. In this paper we investigate this new class of
codes: we give optimal or best known CIS codes of length $&lt;132.$ We derive
general constructions based on cyclic codes and on double circulant codes. We
derive a Varshamov-Gilbert bound for long CIS codes, and show that they can all
be classified in small lengths $\le 12$ by the building up construction. Some
nonlinear permutations are constructed by using $\Z_4$-codes, based on the
notion of dual distance of an unrestricted code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1194</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1194</id><created>2011-10-06</created><authors><author><keyname>Chroni</keyname><forenames>Maria</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Stavros D.</forenames></author></authors><title>Efficient Encoding of Watermark Numbers as Reducible Permutation Graphs</title><categories>cs.DS</categories><acm-class>G.2.2; G.2.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a software watermarking environment, several graph theoretic watermark
methods use numbers as watermark values, where some of these methods encode the
watermark numbers as graph structures. In this paper we extended the class of
error correcting graphs by proposing an efficient and easily implemented codec
system for encoding watermark numbers as reducible permutation flow-graphs.
More precisely, we first present an efficient algorithm which encodes a
watermark number $w$ as self-inverting permutation $\pi^*$ and, then, an
algorithm which encodes the self-inverting permutation $\pi^*$ as a reducible
permutation flow-graph $F[\pi^*]$ by exploiting domination relations on the
elements of $\pi^*$ and using an efficient DAG representation of $\pi^*$. The
whole encoding process takes O(n) time and space, where $n$ is the binary size
of the number $w$ or, equivalently, the number of elements of the permutation
$\pi^*$. We also propose efficient decoding algorithms which extract the number
$w$ from the reducible permutation flow-graph $F[\pi^*]$ within the same time
and space complexity. The two main components of our proposed codec system,
i.e., the self-inverting permutation $\pi^*$ and the reducible permutation
graph $F[\pi^*]$, incorporate important structural properties which make our
system resilient to attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1198</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1198</id><created>2011-10-06</created><authors><author><keyname>Fay</keyname><forenames>Damien</forenames></author><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Yoneki</keyname><forenames>Eiko</forenames></author></authors><title>On Joint Diagonalisation for Dynamic Network Analysis</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint diagonalisation (JD) is a technique used to estimate an average
eigenspace of a set of matrices. Whilst it has been used successfully in many
areas to track the evolution of systems via their eigenvectors; its application
in network analysis is novel. The key focus in this paper is the use of JD on
matrices of spanning trees of a network. This is especially useful in the case
of real-world contact networks in which a single underlying static graph does
not exist. The average eigenspace may be used to construct a graph which
represents the `average spanning tree' of the network or a representation of
the most common propagation paths. We then examine the distribution of
deviations from the average and find that this distribution in real-world
contact networks is multi-modal; thus indicating several \emph{modes} in the
underlying network. These modes are identified and are found to correspond to
particular times. Thus JD may be used to decompose the behaviour, in time, of
contact networks and produce average static graphs for each time. This may be
viewed as a mixture between a dynamic and static graph approach to contact
network analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1208</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1208</id><created>2011-10-06</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Jyoti</keyname><forenames>Divya</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>Rotation, Scaling and Translation Analysis of Biometric Signature
  Templates</title><categories>cs.CV cs.CR</categories><comments>rotation; scaling; translation; RST; image registration; signature
  verification</comments><journal-ref>International Journal of Computer Technology and Applications, Vol
  2 No 5, 2011, 1419 - 1425</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric authentication systems that make use of signature verification
methods often render optimum performance only under limited and restricted
conditions. Such methods utilize several training samples so as to achieve high
accuracy. Moreover, several constraints are imposed on the end-user so that the
system may work optimally, and as expected. For example, the user is made to
sign within a small box, in order to limit their signature to a predefined set
of dimensions, thus eliminating scaling. Moreover, the angular rotation with
respect to the referenced signature that will be inadvertently introduced as
human error, hampers performance of biometric signature verification systems.
To eliminate this, traditionally, a user is asked to sign exactly on top of a
reference line. In this paper, we propose a robust system that optimizes the
signature obtained from the user for a large range of variation in
Rotation-Scaling-Translation (RST) and resolves these error parameters in the
user signature according to the reference signature stored in the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1209</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1209</id><created>2011-10-06</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Gangundi</keyname><forenames>Sandeep</forenames></author><author><keyname>Goel</keyname><forenames>Rishabh</forenames></author><author><keyname>Dave</keyname><forenames>Hiren</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>Audio Watermarking with Error Correction</title><categories>cs.CR</categories><comments>watermarking; audio watermarking; data hiding; data confidentiality</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications ( IJACSA ) , Vol 2 Issue 9, 2011, 113 - 118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, communication through the internet has tremendously
facilitated the distribution of multimedia data. Although this is indubitably a
boon, one of its repercussions is that it has also given impetus to the
notorious issue of online music piracy. Unethical attempts can also be made to
deliberately alter such copyrighted data and thus, misuse it. Copyright
violation by means of unauthorized distribution, as well as unauthorized
tampering of copyrighted audio data is an important technological and research
issue. Audio watermarking has been proposed as a solution to tackle this issue.
The main purpose of audio watermarking is to protect against possible threats
to the audio data and in case of copyright violation or unauthorized tampering,
authenticity of such data can be disputed by virtue of audio watermarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1212</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1212</id><created>2011-10-06</created><updated>2012-02-15</updated><authors><author><keyname>Rots</keyname><forenames>Arnold H.</forenames></author><author><keyname>Winkelman</keyname><forenames>Sherry L.</forenames></author><author><keyname>Becker</keyname><forenames>Glenn</forenames></author></authors><title>Chandra Publication Statistics</title><categories>astro-ph.IM cs.DL</categories><comments>22 pages, 8 figures, 3 tables; revised manuscript submitted to PASP</comments><doi>10.1086/665581</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we develop and propose publication metrics, based on an
analysis of data from the Chandra bibliographic database, that are more
meaningful and less sensitive to observatory-specific characteristics than the
traditional metrics. They fall in three main categories: speed of publication;
fraction of observing time published; and archival usage. Citation of results
is a fourth category, but lends itself less well to definite statements. For
Chandra, the median time from observation to publication is 2.36 years; after
about 7 years 90% of the observing time is published; after 10 years 70% of the
observing time is published more than twice; and the total annual publication
output of the mission is 60-70% of the cumulative observing time available,
assuming a two year lag between data retrieval and publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1220</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1220</id><created>2011-10-06</created><authors><author><keyname>Prakash</keyname><forenames>Hari</forenames></author><author><keyname>Verma</keyname><forenames>Vikram</forenames></author></authors><title>Standard Quantum Teleportation of an Arbitrary N-Qubit State,
  Non-Existence of Magic Basis and Existence of Magic Partial Bases for 2N
  Entangled Qubit States with N&gt;1</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple and precise protocol for standard quantum teleportation
of N-qubit state, considering the most general resource q-channel and Bell
states. We find condition on these states for perfect teleportation and give
explicitly the unitary transformation required to be done by Bob for achieving
perfect teleportation. We discuss connection of our simple theory with the
complicated related work on this subject and with character matrix,
transformation, judgment and kernel operators defined in this context. We also
prove that the magic basis discussed by Hill and Wootters [Phys. Rev. Lett. 78
(1997) 5022] does not exist for entangled 2N-qubit states with N &gt; 1 but magic
partial bases, similar to those discussed recently by Prakash and Maurya
[Optics Commun. 284 (2011) 5024] do exist. We give explicitly all magic partial
bases for N = 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1221</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1221</id><created>2011-10-06</created><authors><author><keyname>Kara</keyname><forenames>Ahmet</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author><author><keyname>Tan</keyname><forenames>Tony</forenames></author></authors><title>Feasible Automata for Two-Variable Logic with Successor on Data Words</title><categories>cs.FL cs.LO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an automata model for data words, that is words that carry at
each position a symbol from a finite alphabet and a value from an unbounded
data domain. The model is (semantically) a restriction of data automata,
introduced by Bojanczyk, et. al. in 2006, therefore it is called weak data
automata. It is strictly less expressive than data automata and the expressive
power is incomparable with register automata. The expressive power of weak data
automata corresponds exactly to existential monadic second order logic with
successor +1 and data value equality \sim, EMSO2(+1,\sim). It follows from
previous work, David, et. al. in 2010, that the nonemptiness problem for weak
data automata can be decided in 2-NEXPTIME. Furthermore, we study weak B\&quot;uchi
automata on data omega-strings. They can be characterized by the extension of
EMSO2(+1,\sim) with existential quantifiers for infinite sets. Finally, the
same complexity bound for its nonemptiness problem is established by a
nondeterministic polynomial time reduction to the nonemptiness problem of weak
data automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1228</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1228</id><created>2011-10-06</created><updated>2012-08-31</updated><authors><author><keyname>Dzhafarov</keyname><forenames>Ehtibar N.</forenames></author><author><keyname>Kujala</keyname><forenames>Janne V.</forenames></author></authors><title>Order-distance and other metric-like functions on jointly distributed
  random variables</title><categories>math.PR cs.AI math.ST q-bio.QM stat.TH</categories><comments>14 pages, to appear in Proc. Amer. Math. Soc</comments><msc-class>60B99 (Primary) 81Q99, 91E45 (Secondary)</msc-class><journal-ref>Proc. Amer. Math. Soc, 141, 3291-3301, 2013</journal-ref><doi>10.1090/S0002-9939-2013-11575-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a class of real-valued nonnegative binary functions on a set of
jointly distributed random variables, which satisfy the triangle inequality and
vanish at identical arguments (pseudo-quasi-metrics). These functions are
useful in dealing with the problem of selective probabilistic causality
encountered in behavioral sciences and in quantum physics. The problem reduces
to that of ascertaining the existence of a joint distribution for a set of
variables with known distributions of certain subsets of this set. Any
violation of the triangle inequality or its consequences by one of our
functions when applied to such a set rules out the existence of this joint
distribution. We focus on an especially versatile and widely applicable
pseudo-quasi-metric called an order-distance and its special case called a
classification distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1237</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1237</id><created>2011-10-06</created><authors><author><keyname>Speicher</keyname><forenames>Roland</forenames></author><author><keyname>Vargas</keyname><forenames>Carlos</forenames></author><author><keyname>Mai</keyname><forenames>Tobias</forenames></author></authors><title>Free Deterministic Equivalents, Rectangular Random Matrix Models, and
  Operator-Valued Free Probability Theory</title><categories>cs.IT math.IT math.OA</categories><comments>21 pages (Appendix by Tobias Mai)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the asymptotic collective behavior of random and deterministic
matrices, we propose an approximation (called &quot;free deterministic equivalent&quot;)
to quite general random matrix models, by replacing the matrices with operators
satisfying certain freeness relations. We comment on the relation between our
free deterministic equivalent and deterministic equivalents considered in the
engineering literature. We do not only consider the case of square matrices,
but also show how rectangular matrices can be treated. Furthermore, we
emphasize how operator-valued free probability techniques can be used to solve
our free deterministic equivalents.
  As an illustration of our methods we consider a random matrix model studied
first by R. Couillet, J. Hoydis, and M. Debbah. We show how its free
deterministic equivalent can be treated and we thus recover in a conceptual way
their result.
  On a technical level, we generalize a result from scalar valued free
probability, by showing that randomly rotated deterministic matrices of
different sizes are asymptotically free from deterministic rectangular
matrices, with amalgamation over a certain algebra of projections.
  In the Appendix, we show how estimates for differences between Cauchy
transforms can be extended from a neighborhood of infinity to a region close to
the real axis. This is of some relevance if one wants to compare the original
random matrix problem with its free deterministic equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1245</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1245</id><created>2011-10-06</created><authors><author><keyname>Cuda</keyname><forenames>Davide</forenames></author><author><keyname>Indre</keyname><forenames>Raluca-Maria</forenames></author><author><keyname>Rouzic</keyname><forenames>Esther Le</forenames></author><author><keyname>Roberts</keyname><forenames>James</forenames></author></authors><title>Getting routers out of the core: Building an optical wide area network
  with &quot;multipaths&quot;</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an all-optical networking solution for a wide area network (WAN)
based on the notion of multipoint-to-multipoint lightpaths that, for short, we
call &quot;multipaths&quot;. A multipath concentrates the traffic of a group of source
nodes on a wavelength channel using an adapted MAC protocol and multicasts this
traffic to a group of destination nodes that extract their own data from the
confluent stream. The proposed network can be built using existing components
and appears less complex and more efficient in terms of energy consumption than
alternatives like OPS and OBS. The paper presents the multipath architecture
and compares its energy consumption to that of a classical router-based ISP
network. A flow-aware dynamic bandwidth allocation algorithm is proposed and
shown to have excellent performance in terms of throughput and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1259</identifier>
 <datestamp>2011-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1259</id><created>2011-10-06</created><authors><author><keyname>Dominguez</keyname><forenames>E.</forenames></author><author><keyname>Lage-Castellanos</keyname><forenames>A.</forenames></author><author><keyname>Mulet</keyname><forenames>R.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Rizzo</keyname><forenames>T.</forenames></author></authors><title>Characterizing and Improving Generalized Belief Propagation Algorithms
  on the 2D Edwards-Anderson Model</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.IT math.IT</categories><comments>19 pages, 13 figures</comments><journal-ref>J. Stat. Mech. P12007 (2011)</journal-ref><doi>10.1088/1742-5468/2011/12/P12007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of different message passing algorithms in the two
dimensional Edwards Anderson model. We show that the standard Belief
Propagation (BP) algorithm converges only at high temperature to a paramagnetic
solution. Then, we test a Generalized Belief Propagation (GBP) algorithm,
derived from a Cluster Variational Method (CVM) at the plaquette level. We
compare its performance with BP and with other algorithms derived under the
same approximation: Double Loop (DL) and a two-ways message passing algorithm
(HAK). The plaquette-CVM approximation improves BP in at least three ways: the
quality of the paramagnetic solution at high temperatures, a better estimate
(lower) for the critical temperature, and the fact that the GBP message passing
algorithm converges also to non paramagnetic solutions. The lack of convergence
of the standard GBP message passing algorithm at low temperatures seems to be
related to the implementation details and not to the appearance of long range
order. In fact, we prove that a gauge invariance of the constrained CVM free
energy can be exploited to derive a new message passing algorithm which
converges at even lower temperatures. In all its region of convergence this new
algorithm is faster than HAK and DL by some orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1263</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1263</id><created>2011-10-06</created><authors><author><keyname>Geffert</keyname><forenames>Viliam</forenames></author><author><keyname>Guillon</keyname><forenames>Bruno</forenames></author><author><keyname>Pighizzini</keyname><forenames>Giovanni</forenames></author></authors><title>Two-Way Automata Making Choices Only at the Endmarkers</title><categories>cs.FL cs.CC</categories><comments>23 pages</comments><acm-class>F.1.1; F.1.3; F.2.3; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of the state-size cost for simulation of two-way
nondeterministic automata (2NFAs) by two-way deterministic automata (2DFAs) was
raised in 1978 and, despite many attempts, it is still open. Subsequently, the
problem was attacked by restricting the power of 2DFAs (e.g., using a
restricted input head movement) to the degree for which it was already possible
to derive some exponential gaps between the weaker model and the standard
2NFAs. Here we use an opposite approach, increasing the power of 2DFAs to the
degree for which it is still possible to obtain a subexponential conversion
from the stronger model to the standard 2DFAs. In particular, it turns out that
subexponential conversion is possible for two-way automata that make
nondeterministic choices only when the input head scans one of the input tape
endmarkers. However, there is no restriction on the input head movement. This
implies that an exponential gap between 2NFAs and 2DFAs can be obtained only
for unrestricted 2NFAs using capabilities beyond the proposed new model. As an
additional bonus, conversion into a machine for the complement of the original
language is polynomial in this model. The same holds for making such machines
self-verifying, halting, or unambiguous. Finally, any superpolynomial lower
bound for the simulation of such machines by standard 2DFAs would imply L&lt;&gt;NL.
In the same way, the alternating version of these machines is related to L =?
NL =? P, the classical computational complexity problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1277</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1277</id><created>2011-10-05</created><authors><author><keyname>Salem-Mhamdia</keyname><forenames>Amel Ben Hadj</forenames></author><author><keyname>Ghadhab</keyname><forenames>Bahia Bejar</forenames></author></authors><title>Performance improvement of the software development project using the
  Value Management approach</title><categories>cs.SE cs.PF</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 2, 2011, 298-306</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving performance and delivering value for customers have become a
central theme in business. The software industry has become an increasingly
important sector for the economy growth in Tunisia. This study aims to show how
using Value Management in the Tunisian software industry for project analysis
gives new insight about true project value and performance. This new approach
is considered as an appropriate tool for guiding the process of making
decisions. It offers tools in order to analyze the service value from the
customer and organization perspectives. The results showed that the VM allows
to have better performance in the software development project by linking
customer satisfaction and cost analysis. The present case shows to service
managers how they can benchmark project function to reduce their costs and
improve resource allocation taking into consideration what customers consider
important during their overall service experience. It can identify best
professional practices, orient decisions to improve service value
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1301</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1301</id><created>2011-10-06</created><authors><author><keyname>Deynet</keyname><forenames>Michael</forenames></author></authors><title>Predicting User Actions in Software Processes</title><categories>cs.SE cs.AI</categories><comments>4th Workshop on Intelligent Techniques in Software Engineering, 5
  September 2011, at the European Conference on Machine Learning and Principles
  and Practices of Knowledge Discovery in Databases (ECML-PKDD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an approach for user (e.g. SW architect) assisting in
software processes. The approach observes the user's action and tries to
predict his next step. For this we use approaches in the area of machine
learning (sequence learning) and adopt these for the use in software processes.
  Keywords: Software engineering, Software process description languages,
Software processes, Machine learning, Sequence prediction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1303</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1303</id><created>2011-10-06</created><authors><author><keyname>Kosti</keyname><forenames>Makrina Viola</forenames></author><author><keyname>Lazaridou</keyname><forenames>Sofia</forenames></author><author><keyname>Bourazani</keyname><forenames>Nikoleta</forenames></author><author><keyname>Angelis</keyname><forenames>Lefteris</forenames></author></authors><title>Discovering patterns of correlation and similarities in software project
  data with the Circos visualization tool</title><categories>cs.SE cs.AI</categories><comments>4th Workshop on Intelligent Techniques in Software Engineering, 5
  September 2011 at the European Conference on Machine Learning and Principles
  and Practices of Knowledge Discovery in Databases (ECML-PKDD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software cost estimation based on multivariate data from completed projects
requires the building of efficient models. These models essentially describe
relations in the data, either on the basis of correlations between variables or
of similarities between the projects. The continuous growth of the amount of
data gathered and the need to perform preliminary analysis in order to discover
patterns able to drive the building of reasonable models, leads the researchers
towards intelligent and time-saving tools which can effectively describe data
and their relationships. The goal of this paper is to suggest an innovative
visualization tool, widely used in bioinformatics, which represents relations
in data in an aesthetic and intelligent way. In order to illustrate the
capabilities of the tool, we use a well known dataset from software engineering
projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1304</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1304</id><created>2011-10-06</created><updated>2011-10-17</updated><authors><author><keyname>Kozawa</keyname><forenames>Kyohei</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author></authors><title>On spanning tree congestion of Hamming graphs</title><categories>cs.DM</categories><comments>3 pages, fixed typos again</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a tight lower bound for the spanning tree congestion of Hamming
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1320</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1320</id><created>2011-10-06</created><updated>2011-10-25</updated><authors><author><keyname>Eisenstat</keyname><forenames>David</forenames></author><author><keyname>Klein</keyname><forenames>Philip</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author></authors><title>An efficient polynomial-time approximation scheme for Steiner forest in
  planar graphs</title><categories>cs.DS</categories><comments>added material on balanced branch decompositions; fixed theorem
  references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $O(n \log^3 n)$ approximation scheme for Steiner forest in planar
graphs, improving on the previous approximation scheme for this problem, which
runs in $O(n^{f(\epsilon)})$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1328</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1328</id><created>2011-10-06</created><updated>2012-03-28</updated><authors><author><keyname>Satuluri</keyname><forenames>Venu</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author></authors><title>Bayesian Locality Sensitive Hashing for Fast Similarity Search</title><categories>cs.DB cs.AI cs.DS cs.IR</categories><comments>13 pages, 5 Tables, 21 figures. Added acknowledgments in v3. A
  slightly shorter version of this paper without the appendix has been
  published in the PVLDB journal, 5(5):430-441, 2012.
  http://vldb.org/pvldb/vol5/p430_venusatuluri_vldb2012.pdf</comments><acm-class>H.2.8; H.3.3; I.5.3</acm-class><journal-ref>PVLDB 5(5):430-441, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a collection of objects and an associated similarity measure, the
all-pairs similarity search problem asks us to find all pairs of objects with
similarity greater than a certain user-specified threshold. Locality-sensitive
hashing (LSH) based methods have become a very popular approach for this
problem. However, most such methods only use LSH for the first phase of
similarity search - i.e. efficient indexing for candidate generation. In this
paper, we present BayesLSH, a principled Bayesian algorithm for the subsequent
phase of similarity search - performing candidate pruning and similarity
estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates
similarities exactly, is also presented. BayesLSH is able to quickly prune away
a large majority of the false positive candidate pairs, leading to significant
speedups over baseline approaches. For BayesLSH, we also provide probabilistic
guarantees on the quality of the output, both in terms of accuracy and recall.
Finally, the quality of BayesLSH's output can be easily tuned and does not
require any manual setting of the number of hashes to use for similarity
estimation, unlike standard approaches. For two state-of-the-art candidate
generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups,
typically in the range 2x-20x for a wide variety of datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1347</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1347</id><created>2011-10-06</created><authors><author><keyname>Perea-Vega</keyname><forenames>Diego</forenames></author><author><keyname>Girard</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>A Dual-based Method for Resource Allocation in OFDMA-SDMA Systems with
  Minimum Rate Constraints</title><categories>cs.IT math.IT</categories><comments>submitted to WiNET Springer in August 2011</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We consider multi-antenna base stations using orthogonal frequency-division
multiple access (OFDMA) and space division multiple access (SDMA) techniques to
serve single antenna users, where some of those users have minimum rate
requirements and must be served in the current time slot (real time users),
while others do not have strict timing constraints (non real time users) and
are served on a best effort basis. The resource allocation problem is to find
the user assignment to subcarriers and the transmit beamforming vectors that
maximize a linear utility function of the user rates subject to power and
minimum rate constraints. The exact optimal solution to this problem can not be
reasonably obtained for practical parameters values of the communication
system. We thus derive a dual problem formulation whose optimal solution
provides an upper bound to all feasible solutions and can be used to benchmark
the performance of any heuristic method used to solve this problem. We also
derive from this dual optimal solution a primal-feasible dual-based method to
solve the problem and we compare its performance and computation time against a
standard weight adjustment method. We find that our method follows the dual
optimal bound more closely than the weight adjustment method. This off-line
algorithm can serve as the basis to develop more efficient heuristic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1349</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1349</id><created>2011-10-06</created><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Reid</keyname><forenames>Fergal</forenames></author><author><keyname>Sheridan</keyname><forenames>Gavin</forenames></author><author><keyname>Cunningham</keyname><forenames>Padraig</forenames></author></authors><title>Supporting the Curation of Twitter User Lists</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter introduced lists in late 2009 as a means of curating tweets into
meaningful themes. Lists were quickly adopted by media companies as a means of
organising content around news stories. Thus the curation of these lists is
important, they should contain the key information gatekeepers and present a
balanced perspective on the story. Identifying members to add to a list on an
emerging topic is a delicate process. From a network analysis perspective there
are a number of views on the Twitter network that can be explored, e.g.
followers, retweets mentions etc. We present a process for integrating these
views in order to recommend authoritative commentators to include on a list.
This process is evaluated on manually curated lists about unrest in Bahrain and
the Iowa caucuses for the 2012 US election.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1354</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1354</id><created>2011-10-06</created><authors><author><keyname>Artho</keyname><forenames>Cyrille</forenames><affiliation>RCIS</affiliation></author><author><keyname>Di Cosmo</keyname><forenames>Roberto</forenames><affiliation>PPS</affiliation></author><author><keyname>Suzaki</keyname><forenames>Kuniyasu</forenames><affiliation>RCIS</affiliation></author><author><keyname>Zacchiroli</keyname><forenames>Stefano</forenames><affiliation>PPS</affiliation></author></authors><title>Sources of Inter-package Conflicts in Debian</title><categories>cs.SE</categories><comments>LoCoCo 2011 International Workshop on Logics for Component
  Configuration, Perugia : Italy (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-package conflicts require the presence of two or more packages in a
particular configuration, and thus tend to be harder to detect and localize
than conventional (intra-package) defects. Hundreds of such inter-package
conflicts go undetected by the normal testing and distribution process until
they are later reported by a user. The reason for this is that current
meta-data is not fine-grained and accurate enough to cover all common types of
conflicts. A case study of inter-package conflicts in Debian has shown that
with more detailed package meta-data, at least one third of all package
conflicts could be prevented relatively easily, while another one third could
be found by targeted testing of packages that share common resources or
characteristics. This paper reports the case study and proposes ideas to detect
inter-package conflicts in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1358</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1358</id><created>2011-10-06</created><updated>2012-09-07</updated><authors><author><keyname>Chin</keyname><forenames>Hui Han</forenames></author><author><keyname>Madry</keyname><forenames>Aleksander</forenames></author><author><keyname>Miller</keyname><forenames>Gary</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Runtime Guarantees for Regression Problems</title><categories>cs.DS cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study theoretical runtime guarantees for a class of optimization problems
that occur in a wide variety of inference problems. these problems are
motivated by the lasso framework and have applications in machine learning and
computer vision.
  Our work shows a close connection between these problems and core questions
in algorithmic graph theory. While this connection demonstrates the
difficulties of obtaining runtime guarantees, it also suggests an approach of
using techniques originally developed for graph algorithms.
  We then show that most of these problems can be formulated as a grouped least
squares problem, and give efficient algorithms for this formulation. Our
algorithms rely on routines for solving quadratic minimization problems, which
in turn are equivalent to solving linear systems. Finally we present some
experimental results on applying our approximation algorithm to image
processing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1360</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1360</id><created>2011-10-06</created><authors><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Charikar</keyname><forenames>Moses</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Polynomial integrality gaps for strong SDP relaxations of Densest
  k-subgraph</title><categories>cs.DS cs.CC</categories><comments>26 ages, 1 figure. To appear in Symposium on Discrete Algorithms
  (SODA) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The densest k-subgraph (DkS) problem (i.e. find a size k subgraph with
maximum number of edges), is one of the notorious problems in approximation
algorithms. There is a significant gap between known upper and lower bounds for
DkS: the current best algorithm gives an ~ O(n^{1/4}) approximation, while even
showing a small constant factor hardness requires significantly stronger
assumptions than P != NP. In addition to interest in designing better
algorithms, a number of recent results have exploited the conjectured hardness
of densest k-subgraph and its variants. Thus, understanding the approximability
of DkS is an important challenge.
  In this work, we give evidence for the hardness of approximating DkS within
polynomial factors. Specifically, we expose the limitations of strong
semidefinite programs from SDP hierarchies in solving densest k-subgraph. Our
results include:
  * A lower bound of Omega(n^{1/4}/log^3 n) on the integrality gap for
Omega(log n/log log n) rounds of the Sherali-Adams relaxation for DkS. This
also holds for the relaxation obtained from Sherali-Adams with an added SDP
constraint. Our gap instances are in fact Erdos-Renyi random graphs.
  * For every epsilon &gt; 0, a lower bound of n^{2/53-eps} on the integrality gap
of n^{Omega(eps)} rounds of the Lasserre SDP relaxation for DkS, and an
n^{Omega_eps(1)} gap for n^{1-eps} rounds. Our construction proceeds via a
reduction from random instances of a certain Max-CSP over large domains.
  In the absence of inapproximability results for DkS, our results show that
even the most powerful SDPs are unable to beat a factor of n^{Omega(1)}, and in
fact even improving the best known n^{1/4} factor is a barrier for current
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1388</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1388</id><created>2011-10-06</created><updated>2011-10-14</updated><authors><author><keyname>Benioff</keyname><forenames>Paul</forenames></author></authors><title>Effects on quantum physics of the local availability of mathematics and
  space time dependent scaling factors for number systems</title><categories>quant-ph cs.IT gr-qc math-ph math.IT math.MP</categories><comments>31 pages, 2 figures, To appear as chapter in book, Quantum Theory,
  Publ. by Intech: Typos corrected, some paragraphs rewritten</comments><journal-ref>Chapter 2, in Quantum Theory, I. Cotaescu, Editor, Intech open
  access publisher, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work is based on two premises: local availability of mathematics to an
observer at any space time location, and the observation that number systems,
as structures satisfying axioms for the number type being considered, can be
scaled by arbitrary, positive real numbers. Local availability leads to the
assignment of mathematical universes, $V_{x},$ to each point, $x,$ of space
time. $V_{x}$ contains all the mathematics that an observer, $O_{x},$ at $x,$
can know. Each $V_{x}$ contains many types of mathematical systems. These
include the different types of numbers (natural numbers, integers, rationals,
and real and complex numbers), Hilbert spaces, algebras, and many other types
of systems. Space time dependent scaling of number systems is used to define
representations, in $V_{x}$, of real and complex number systems in $V_{y}$. The
representations are scaled by a factor $r_{y,x}$ relative to the systems in
$V_{x}.$ For $y$ a neighbor point of $x,$ $r_{y,x}$ is the exponential of the
scalar product of a gauge field, $\vec{A}(x),$ and the vector from $x$ to $y.$
For $y$ distant from $x,$ $r_{y,x}$ is a path integral from $x$ to $y.$ Some
consequences of the two premises will be examined. Number scaling has no effect
on general comparisons of numbers obtained as computations or as experimental
outputs. The effect is limited to mathematical expressions that include space
or space time integrals or derivatives. The effect of $\vec{A}$ on wave packets
and canonical momenta in quantum theory, and some properties of $\vec{A}$ in
gauge theories, is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1391</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1391</id><created>2011-10-06</created><authors><author><keyname>Choi</keyname><forenames>K.</forenames></author><author><keyname>Isahara</keyname><forenames>H.</forenames></author><author><keyname>Oh</keyname><forenames>J.</forenames></author></authors><title>A Comparison of Different Machine Transliteration Models</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  119-151, 2006</journal-ref><doi>10.1613/jair.1999</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine transliteration is a method for automatically converting words in one
language into phonetically equivalent ones in another language. Machine
transliteration plays an important role in natural language applications such
as information retrieval and machine translation, especially for handling
proper nouns and technical terms. Four machine transliteration models --
grapheme-based transliteration model, phoneme-based transliteration model,
hybrid transliteration model, and correspondence-based transliteration model --
have been proposed by several researchers. To date, however, there has been
little research on a framework in which multiple transliteration models can
operate simultaneously. Furthermore, there has been no comparison of the four
models within the same framework and using the same data. We addressed these
problems by 1) modeling the four models within the same framework, 2) comparing
them under the same conditions, and 3) developing a way to improve machine
transliteration through this comparison. Our comparison showed that the hybrid
and correspondence-based models were the most effective and that the four
models can be used in a complementary manner to improve machine transliteration
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1393</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1393</id><created>2011-10-06</created><authors><author><keyname>Alibart</keyname><forenames>Fabien</forenames></author><author><keyname>Gao</keyname><forenames>Ligang</forenames></author><author><keyname>Hoskins</keyname><forenames>Brian</forenames></author><author><keyname>Strukov</keyname><forenames>Dmitri</forenames></author></authors><title>High-Precision Tuning of State for Memristive Devices by Adaptable
  Variation-Tolerant Algorithm</title><categories>cond-mat.mtrl-sci cs.AR</categories><comments>20 pages, 6 figures</comments><journal-ref>Nanotechnology, vol. 23, art. 075201, 2012</journal-ref><doi>10.1088/0957-4484/23/7/075201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using memristive properties common for the titanium dioxide thin film
devices, we designed a simple write algorithm to tune device conductance at a
specific bias point to 1% relative accuracy (which is roughly equivalent to
7-bit precision) within its dynamic range even in the presence of large
variations in switching behavior. The high precision state is nonvolatile and
the results are likely to be sustained for nanoscale memristive devices because
of the inherent filamentary nature of the resistive switching. The proposed
functionality of memristive devices is especially attractive for analog
computing with low precision data. As one representative example we demonstrate
hybrid circuitry consisting of CMOS summing amplifier and two memristive
devices to perform analog multiply and accumulate computation, which is a
typical bottleneck operation in information processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1394</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1394</id><created>2011-10-06</created><authors><author><keyname>Lapata</keyname><forenames>M.</forenames></author><author><keyname>Lascarides</keyname><forenames>A.</forenames></author></authors><title>Learning Sentence-internal Temporal Relations</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  85-117, 2006</journal-ref><doi>10.1613/jair.2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a data intensive approach for inferring
sentence-internal temporal relations. Temporal inference is relevant for
practical NLP applications which either extract or synthesize temporal
information (e.g., summarisation, question answering). Our method bypasses the
need for manual coding by exploiting the presence of markers like after&quot;, which
overtly signal a temporal relation. We first show that models trained on main
and subordinate clauses connected with a temporal marker achieve good
performance on a pseudo-disambiguation task simulating temporal inference
(during testing the temporal marker is treated as unseen and the models must
select the right marker from a set of possible candidates). Secondly, we assess
whether the proposed approach holds promise for the semi-automatic creation of
temporal annotations. Specifically, we use a model trained on noisy and
approximate data (i.e., main and subordinate clauses) to predict
intra-sentential relations present in TimeBank, a corpus annotated rich
temporal information. Our experiments compare and contrast several
probabilistic models differing in their feature space, linguistic assumptions
and data requirements. We evaluate performance against gold standard corpora
and also against human subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1409</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1409</id><created>2011-10-06</created><authors><author><keyname>Rutherford</keyname><forenames>Alex</forenames></author><author><keyname>Harmon</keyname><forenames>Dion</forenames></author><author><keyname>Werfel</keyname><forenames>Justin</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Shlomiya</forenames></author><author><keyname>Gard-Murray</keyname><forenames>Alexander</forenames></author><author><keyname>Gros</keyname><forenames>Andreas</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Good Fences: The Importance of Setting Boundaries for Peaceful
  Coexistence</title><categories>physics.soc-ph cs.SI</categories><comments>paper pages 1-14, 4 figures; appendices pages 15-43, 20 figures</comments><report-no>NECSI 2011-10-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the conditions of peace and violence among ethnic groups, testing
a theory designed to predict the locations of violence and interventions that
can promote peace. Characterizing the model's success in predicting peace
requires examples where peace prevails despite diversity. Switzerland is
recognized as a country of peace, stability and prosperity. This is surprising
because of its linguistic and religious diversity that in other parts of the
world lead to conflict and violence. Here we analyze how peaceful stability is
maintained. Our analysis shows that peace does not depend on integrated
coexistence, but rather on well defined topographical and political boundaries
separating groups. Mountains and lakes are an important part of the boundaries
between sharply defined linguistic areas. Political canton and circle
(sub-canton) boundaries often separate religious groups. Where such boundaries
do not appear to be sufficient, we find that specific aspects of the population
distribution either guarantee sufficient separation or sufficient mixing to
inhibit intergroup violence according to the quantitative theory of conflict.
In exactly one region, a porous mountain range does not adequately separate
linguistic groups and violent conflict has led to the recent creation of the
canton of Jura. Our analysis supports the hypothesis that violence between
groups can be inhibited by physical and political boundaries. A similar
analysis of the area of the former Yugoslavia shows that during widespread
ethnic violence existing political boundaries did not coincide with the
boundaries of distinct groups, but peace prevailed in specific areas where they
did coincide. The success of peace in Switzerland may serve as a model to
resolve conflict in other ethnically diverse countries and regions of the
world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1416</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1416</id><created>2011-10-06</created><updated>2011-10-18</updated><authors><author><keyname>Yuming</keyname><forenames>Xu</forenames></author></authors><title>The matrices of argumentation frameworks</title><categories>cs.IT cs.AI math.IT</categories><comments>20pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce matrix and its block to the Dung's theory of argumentation
frameworks. It is showed that each argumentation framework has a matrix
representation, and the common extension-based semantics of argumentation
framework can be characterized by blocks of matrix and their relations. In
contrast with traditional method of directed graph, the matrix way has the
advantage of computability. Therefore, it has an extensive perspective to bring
the theory of matrix into the research of argumentation frameworks and related
areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1428</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1428</id><created>2011-10-07</created><authors><author><keyname>Ly</keyname><forenames>Duy Khang</forenames></author><author><keyname>Sugiyama</keyname><forenames>Kazunari</forenames></author><author><keyname>Lin</keyname><forenames>Ziheng</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author></authors><title>Product Review Summarization based on Facet Identification and Sentence
  Clustering</title><categories>cs.CL cs.DL</categories><comments>10 pages, 3 figures, 3 tables, short paper version published in JCDL
  2011</comments><report-no>TR30/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product review nowadays has become an important source of information, not
only for customers to find opinions about products easily and share their
reviews with peers, but also for product manufacturers to get feedback on their
products. As the number of product reviews grows, it becomes difficult for
users to search and utilize these resources in an efficient way. In this work,
we build a product review summarization system that can automatically process a
large collection of reviews and aggregate them to generate a concise summary.
More importantly, the drawback of existing product summarization systems is
that they cannot provide the underlying reasons to justify users' opinions. In
our method, we solve this problem by applying clustering, prior to selecting
representative candidates for summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1439</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1439</id><created>2011-10-07</created><updated>2012-03-03</updated><authors><author><keyname>Schwentick</keyname><forenames>Thomas</forenames><affiliation>TU Dortmund University</affiliation></author><author><keyname>Zeume</keyname><forenames>Thomas</forenames><affiliation>TU Dortmund University</affiliation></author></authors><title>Two-Variable Logic with Two Order Relations</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 2,
  2012) lmcs:715</journal-ref><doi>10.2168/LMCS-8(1:15)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the finite satisfiability problem for two-variable logic
over structures with one total preorder relation, its induced successor
relation, one linear order relation and some further unary relations is
EXPSPACE-complete. Actually, EXPSPACE-completeness already holds for structures
that do not include the induced successor relation. As a special case, the
EXPSPACE upper bound applies to two-variable logic over structures with two
linear orders. A further consequence is that satisfiability of two-variable
logic over data words with a linear order on positions and a linear order and
successor relation on the data is decidable in EXPSPACE. As a complementing
result, it is shown that over structures with two total preorder relations as
well as over structures with one total preorder and two linear order relations,
the finite satisfiability problem for two-variable logic is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1462</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1462</id><created>2011-10-07</created><authors><author><keyname>Irpino</keyname><forenames>Antonio</forenames></author><author><keyname>Verde</keyname><forenames>Rosanna</forenames></author><author><keyname>De Carvalho</keyname><forenames>Francisco de AT</forenames></author></authors><title>Dynamic Clustering of Histogram Data Based on Adaptive Squared
  Wasserstein Distances</title><categories>math.ST cs.DS math.PR stat.ME stat.OT stat.TH</categories><journal-ref>Expert Systems with Applications, vol. 41, p. 3351-3366, 2014</journal-ref><doi>10.1016/j.eswa.2013.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with clustering methods based on adaptive distances for
histogram data using a dynamic clustering algorithm. Histogram data describes
individuals in terms of empirical distributions. These kind of data can be
considered as complex descriptions of phenomena observed on complex objects:
images, groups of individuals, spatial or temporal variant data, results of
queries, environmental data, and so on. The Wasserstein distance is used to
compare two histograms. The Wasserstein distance between histograms is
constituted by two components: the first based on the means, and the second, to
internal dispersions (standard deviation, skewness, kurtosis, and so on) of the
histograms. To cluster sets of histogram data, we propose to use Dynamic
Clustering Algorithm, (based on adaptive squared Wasserstein distances) that is
a k-means-like algorithm for clustering a set of individuals into $K$ classes
that are apriori fixed.
  The main aim of this research is to provide a tool for clustering histograms,
emphasizing the different contributions of the histogram variables, and their
components, to the definition of the clusters. We demonstrate that this can be
achieved using adaptive distances. Two kind of adaptive distances are
considered: the first takes into account the variability of each component of
each descriptor for the whole set of individuals; the second takes into account
the variability of each component of each descriptor in each cluster. We
furnish interpretative tools of the obtained partition based on an extension of
the classical measures (indexes) to the use of adaptive distances in the
clustering criterion function. Applications on synthetic and real-world data
corroborate the proposed procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1468</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1468</id><created>2011-10-07</created><updated>2012-01-03</updated><authors><author><keyname>Slomczynski</keyname><forenames>Wojciech</forenames></author><author><keyname>Zyczkowski</keyname><forenames>Karol</forenames></author></authors><title>Mathematical aspects of degressive proportionality</title><categories>physics.soc-ph cs.SI</categories><comments>several minor corrections, revised version 10 pages in two column
  style, one figure and two tables included</comments><journal-ref>Math. Soc. Sciences 63, 94-101 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze properties of apportionment functions in context of the problem of
allocating seats in the European Parliament. Necessary and sufficient
conditions for apportionment functions are investigated. Some exemplary
families of apportionment functions are specified and the corresponding
partitions of the seats in the European Parliament among the Member States of
the European Union are presented. Although the choice of the allocation
functions is theoretically unlimited, we show that the constraints are so
strong that the acceptable functions lead to rather similar solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1470</identifier>
 <datestamp>2012-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1470</id><created>2011-10-07</created><updated>2012-02-02</updated><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames></author></authors><title>A Constraint-Satisfaction Parser for Context-Free Grammars</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional language processing tools constrain language designers to
specific kinds of grammars. In contrast, model-based language specification
decouples language design from language processing. As a consequence,
model-based language specification tools need general parsers able to parse
unrestricted context-free grammars. As languages specified following this
approach may be ambiguous, parsers must deal with ambiguities. Model-based
language specification also allows the definition of associativity, precedence,
and custom constraints. Therefore parsers generated by model-driven language
specification tools need to enforce constraints. In this paper, we propose
Fence, an efficient bottom-up chart parser with lexical and syntactic ambiguity
support that allows the specification of constraints and, therefore, enables
the use of model-based language specification in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1485</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1485</id><created>2011-10-07</created><authors><author><keyname>Imtiaz</keyname><forenames>Hafiz</forenames></author><author><keyname>Fattah</keyname><forenames>Shaikh Anowarul</forenames></author></authors><title>A Face Recognition Scheme using Wavelet Based Dominant Features</title><categories>cs.CV</categories><comments>12 pages, 12 figures, Published in Signal and Image Processing: An
  International Journal Vol 2 No 3</comments><journal-ref>Signal and Image Processing: An International Journal, Vol. 2, No.
  3, Sept 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multi-resolution feature extraction algorithm for face
recognition is proposed based on two-dimensional discrete wavelet transform
(2D-DWT), which efficiently exploits the local spatial variations in a face
image. For the purpose of feature extraction, instead of considering the entire
face image, an entropy-based local band selection criterion is developed, which
selects high-informative horizontal segments from the face image. In order to
capture the local spatial variations within these highinformative horizontal
bands precisely, the horizontal band is segmented into several small spatial
modules. Dominant wavelet coefficients corresponding to each local region
residing inside those horizontal bands are selected as features. In the
selection of the dominant coefficients, a threshold criterion is proposed,
which not only drastically reduces the feature dimension but also provides high
within-class compactness and high between-class separability. A principal
component analysis is performed to further reduce the dimensionality of the
feature space. Extensive experimentation is carried out upon standard face
databases and a very high degree of recognition accuracy is achieved by the
proposed method in comparison to those obtained by some of the existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1488</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1488</id><created>2011-10-07</created><authors><author><keyname>Chakravarthy</keyname><forenames>A. S. N.</forenames></author><author><keyname>Raja</keyname><forenames>Penmetsa V. Krishna</forenames></author><author><keyname>Avadhani</keyname><forenames>P. S.</forenames></author></authors><title>Handwritten Text Image Authentication using Back Propagation</title><categories>cs.CR</categories><comments>10 pages pdf file</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.5, Sep 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication is the act of confirming the truth of an attribute of a datum
or entity. This might involve confirming the identity of a person, tracing the
origins of an artefact, ensuring that a product is what it's packaging and
labelling claims to be, or assuring that a computer program is a trusted one.
The authentication of information can pose special problems (especially
man-in-the-middle attacks), and is often wrapped up with authenticating
identity. Literary can involve imitating the style of a famous author. If an
original manuscript, typewritten text, or recording is available, then the
medium itself (or its packaging - anything from a box to e-mail headers) can
help prove or disprove the authenticity of the document. The use of digital
images of handwritten historical documents has become more popular in recent
years. Volunteers around the world now read thousands of these images as part
of their indexing process. Handwritten text images of old documents are
sometimes difficult to read or noisy due to the preservation of the document
and quality of the image [1]. Handwritten text offers challenges that are
rarely encountered in machine-printed text. In addition, most problems faced in
reading machine- printed text (e.g., character recognition, word segmentation,
letter segmentation, etc.) are more severe, in handwritten text. In this paper
we Here in this paper we proposed a method for authenticating hand written text
images using back propagation algorithm..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1490</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1490</id><created>2011-10-07</created><authors><author><keyname>Chakravarthy</keyname><forenames>A. S. N.</forenames></author><author><keyname>Raja</keyname><forenames>Penmetsa V. Krishna</forenames></author><author><keyname>Avadhani</keyname><forenames>P. S</forenames></author></authors><title>A Novel Approach for Pass Word Authentication using Brain -State -In -A
  Box (BSB) Model</title><categories>cs.CR cs.NE</categories><comments>five pages</comments><journal-ref>International Journal of Computer Science and Information
  Technologies (IJCSIT), Volume 2 Issue 5 2011,2127-2131</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication is the act of confirming the truth of an attribute of a datum
or entity. This might involve confirming the identity of a person, tracing the
origins of an artefact, ensuring that a product is what it's packaging and
labelling claims to be, or assuring that a computer program is a trusted one.
The authentication of information can pose special problems (especially
man-in-the-middle attacks), and is often wrapped up with authenticating
identity. Password authentication using Brain-State -In-A Box is presented in
this paper. Here in this paper we discuss Brain-State -In-A Box Scheme for
Textual and graphical passwords which will be converted in to probabilistic
values Password. We observe how to get password authentication Probabilistic
values for Text and Graphical image. This study proposes the use of a
Brain-State -In-A Box technique for password authentication. In comparison to
existing layered neural network techniques, the proposed method provides better
accuracy and quicker response time to registration and password changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1491</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1491</id><created>2011-10-07</created><authors><author><keyname>Ruso</keyname><forenames>T.</forenames></author><author><keyname>Chellappan</keyname><forenames>C.</forenames></author></authors><title>Netrawalm: Network Based Resource Aware Application Layer Multicast for
  Multiparty Video Conference</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IP Multicast is one of the most absolute method for large bandwidth Internet
applications such as video conference, IPTV, E-Learning and Telemedicine etc.,
But due to security and management reason IP Multicast is not enabled in
Internet backbone routers. To achieve these challenges, lot of Application
Layer Multicast (ALM) has been proposed. All the existing protocols such as
NICE, ZIGZAG and OMNI are trying to reduce average delay by forming a Multicast
tree. But still that problem has not been addressed fully. We are proposing a
new protocol called NetRawALM, which will address the average delay,
Reliability between nodes, Scalability of conference, Heterogeneity and
resilient data distribution for real time multimedia applications by
constructing the Network based Resource aware Multicast tree algorithm. This is
very dynamic and decentralised. The proposed architecture is a LAN aware; it is
used to reduce Internet Traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1494</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1494</id><created>2011-10-07</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author></authors><title>Counterflow in Evacuations</title><categories>physics.soc-ph cs.MA</categories><comments>Preprint of TGF11 (Traffic and Granular Flow, Moscow, September 2011)
  conference proceedings contribution</comments><doi>10.1007/978-3-642-39669-4_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown in this work that the average individual egress time and other
performance indicators for egress of people from a building can be improved
under certain circumstances if counterflow occurs. The circumstances include
widely varying walking speeds and two differently far located exits with
different capacity. The result is achieved both with a paper and pencil
calculation as well as with a micro simulation of an example scenario. As the
difficulty of exit signage with counterflow remains one cannot conclude from
the result that an emergency evacuation procedure with counterflow would really
be the better variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1495</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1495</id><created>2011-10-07</created><authors><author><keyname>Chakravarthy</keyname><forenames>ASN</forenames></author><author><keyname>Avadhani</keyname><forenames>Prof. P S</forenames></author></authors><title>A Probabilistic Approach for Authenticating Text or Graphical Passwords
  Using Back Propagation</title><categories>cs.CR cs.NE</categories><comments>ten pages</comments><journal-ref>International Journal of Computer Science and Network Security,
  VOL.11 No.5, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Password authentication is a common approach to the system security and it is
also a very important procedure to gain access to user resources. In the
conventional password authentication methods a server has to authenticate the
legitimate user. In our proposed method users can freely choose their passwords
from a defined character set or they can use a graphical image as password and
that input will be normalized. Neural networks have been used recently for
password authentication in order to overcome pitfall of traditional password
authentication methods. In this paper we proposed a method for password
authentication using alphanumeric password and graphical password. We used Back
Propagation algorithm for both alphanumeric (Text) and graphical password by
which the level of security can be enhanced. This paper along with test results
show that converting user password in to Probabilistic values enhances the
security of the system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1496</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1496</id><created>2011-10-07</created><authors><author><keyname>Nandi</keyname><forenames>Sukumar</forenames></author><author><keyname>Yadav</keyname><forenames>Aditya</forenames></author></authors><title>Cross Layer Adaptation for QoS in WSN</title><categories>cs.NI</categories><comments>17 Pages, 9 Figures, Published in IJCNC in September 2011 issue</comments><acm-class>C.2.1</acm-class><journal-ref>IJCNC, September 2011, Volume 3. Number 5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose QoS aware MAC protocol for Wire- less Sensor
Networks and its cross layer extension to network layer for providing QoS in
delay sensitive WSN scenarios. In WSNs, there can be two types of traffic one
is event driven traffic which requires immedi- ate attention and another is
periodic reporting. Event driven traffic is classified as Class I(delay
sensitive) traffic and periodic reporting is clas- sified as Class II(Best
Effort) Traffic. MAC layer adaptation can take place in terms of (i) Dynamic
contention window adjustment per class, (ii) Reducing the delay suffered by
difference in Sleep schedules(DSS) of communicating nodes by dynamically
adjusting Duty Cycle based on Utilization and DSS delay of class I traffic,
(iii) Different DIFS (DCF Inter Frame Spacing) per class, (iv) Adjusting all
the three schemes pro- posed above simultaneously. Cross layer extension is
also proposed, in which MAC layer uses network layer's next hop information for
better adaptation of duty cycle based on DSS delay. Routing protocols can uti-
lize MAC layer parameter DSS delay to select the routes which offer least DSS
delay latency, there by minimizing the overall end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1497</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1497</id><created>2011-10-07</created><authors><author><keyname>Chakravarthy</keyname><forenames>ASN</forenames></author><author><keyname>Toyaza</keyname><forenames>A. S. S. D.</forenames></author></authors><title>A Novel Approach For Intranet Mailing For Providing User Authentication</title><categories>cs.CR cs.NI</categories><comments>6 pages</comments><journal-ref>International Journal of Computer Science and Information
  Security, Vol. 9, No. 6, June 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosion of the public Internet and e-commerce, private computers,
and computer networks, if not adequately secured, are increasingly vulnerable
to damaging attacks. Hackers, viruses, vindictive employees and even human
error all represent clear and present dangers to networks. Various antidotes
that are in fact inextricable with security issues are - Cryptography,
Authentication, Integrity and Non Repudiation, Key Distribution and
certification, Access control by implementing Firewalls etc.The main idea of
this paper is to overcome the PGP's(Pretty Good Privacy) main limitation of
incomplete non-repudiation Service, which increases the degree of security and
efficiency of an email message communication through NRR(Non-Repudiation of
Receipt) and including PGPs original feature of NRO(Non-Repudiation of Origin),
and there it assures new security service of Mutual Non-Repudiation (MNR) .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1498</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1498</id><created>2011-10-07</created><authors><author><keyname>Raja</keyname><forenames>Penmetsa V. Krishna</forenames></author><author><keyname>Chakravarthy</keyname><forenames>A. S. N.</forenames></author><author><keyname>Avadhani</keyname><forenames>P. S.</forenames></author></authors><title>A Cryptosystem Based on Hilbert Matrix using Cipher Block Chaining Mode</title><categories>cs.CR cs.NI</categories><comments>six pages; International Journal of Mathematics Trends and
  Technology- July to Aug Issue 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography is the science of using mathematics to encrypt and decrypt data.
Cryptography enables you to store sensitive information or transmit it across
insecure networks so that it cannot be read by anyone except the intended
recipient. While cryptography is the science of securing data, cryptanalysis is
the science of analyzing and breaking secure communication. Classical
cryptanalysis involves an interesting combination of analytical reasoning,
application of mathematical tools and pattern finding. The objectives of the
proposed work are to propose a new cryptographic method based on the special
matrix called the Hilbert matrix for authentication and confidentiality and to
propose a model for confidentiality and authentication using a combination of
symmetric and public cryptosystems. Further, it is extended to shared key
cryptosystems with the concept of digital enveloping using a session key. In
the present work an algorithm for shared key encryption is developed using
Hilbert matrix cryptosystem. In this the block chaining modes of operation have
been used to tackle the issues of confusion and diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1502</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1502</id><created>2011-10-07</created><authors><author><keyname>Raja</keyname><forenames>Penmetsa V. Krishna</forenames></author><author><keyname>Chakravarthy</keyname><forenames>A. S. N.</forenames></author><author><keyname>Avadhani</keyname><forenames>P. S.</forenames></author></authors><title>Hilbert Matrix Based Cryptosystem using a Session Key</title><categories>cs.CR</categories><comments>five pages</comments><journal-ref>International Journal of Engineering Research and Applications
  (IJERA) Vol. 1, Issue 3, 2011, pp.711-715</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography protects users by providing functionality for the encryption of
data and authentication of other users. This technology lets the receiver of an
electronic message verify the sender, ensures that a message can be read only
by the intended person, and assures the recipient that a message has not be
altered in transit. Classical cryptanalysis involves an interesting combination
of analytical reasoning, application of mathematical tools and pattern finding.
The objectives of the proposed work are to propose a new cryptographic method
based on the special matrix called the Hilbert matrix for authentication and
confidentiality and to propose a model for confidentiality and authentication
using shared key cryptosystems with the concept of digital enveloping using a
session key. In the present work various algorithms are presented for
encryption and authentication based on Hilbert matrix using a session key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1509</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1509</id><created>2011-10-07</created><authors><author><keyname>Kadir</keyname><forenames>A.</forenames></author><author><keyname>Nugroho</keyname><forenames>L. E.</forenames></author><author><keyname>Susanto</keyname><forenames>A.</forenames></author><author><keyname>Santosa</keyname><forenames>P. I.</forenames></author></authors><title>A Comparative Experiment of Several Shape Methods in Recognizing Plants</title><categories>cs.CV</categories><comments>8 pages; International Journal of Computer Science &amp; Information
  Technology (IJCSIT), Vol 3, No 3, June 2011</comments><doi>10.5121/ijcsit.2011.3318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape is an important aspects in recognizing plants. Several approaches have
been introduced to identify objects, including plants. Combination of geometric
features such as aspect ratio, compactness, and dispersion, or moments such as
moment invariants were usually used toidentify plants. In this research, a
comparative experiment of 4 methods to identify plants using shape features was
accomplished. Two approaches have never been used in plants identification yet,
Zernike moments and Polar Fourier Transform (PFT), were incorporated. The
experimental comparison was done on 52 kinds of plants with various shapes. The
result, PFT gave best performance with 64% in accuracy and outperformed the
other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1510</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1510</id><created>2011-10-07</created><updated>2012-01-17</updated><authors><author><keyname>Hartung</keyname><forenames>Sepp</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>NP-Hardness and Fixed-Parameter Tractability of Realizing Degree
  Sequences with Directed Acyclic Graphs</title><categories>cs.CC</categories><comments>new author Sepp Hartung, new section with fixed-parameter
  tractability result; 25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In graph realization problems one is given a degree sequence and the task is
to decide whether there is a graph whose vertex degrees match to the given
sequence. This realization problem is known to be polynomial-time solvable when
the graph is directed or undirected. In contrary, we show NP-completeness for
the problem of realizing a given sequence of pairs of positive integers
(representing indegrees and outdegrees) with a directed acyclic graph,
answering an open question of Berger and M\&quot;uller-Hannemann [FCT 2011].
Furthermore, we classify the problem as fixed-parameter tractable with respect
to the parameter &quot;maximum degree&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1513</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1513</id><created>2011-10-07</created><authors><author><keyname>Kadir</keyname><forenames>Abdul</forenames></author><author><keyname>Nugroho</keyname><forenames>Lukito Edi</forenames></author><author><keyname>Susanto</keyname><forenames>Adhi</forenames></author><author><keyname>Santosa</keyname><forenames>Paulus Insap</forenames></author></authors><title>Foliage Plant Retrieval using Polar Fourier Transform, Color Moments and
  Vein Features</title><categories>cs.CV</categories><comments>13 pages; Signal &amp; Image Processing : An International Journal
  (SIPIJ) Vol.2, No.3, September 2011</comments><doi>10.5121/sipij.2011.2301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a method that combines Polar Fourier Transform, color
moments, and vein features to retrieve leaf images based on a leaf image. The
method is very useful to help people in recognizing foliage plants. Foliage
plants are plants that have various colors and unique patterns in the leaf.
Therefore, the colors and its patterns are information that should be counted
on in the processing of plant identification. To compare the performance of
retrieving system to other result, the experiments used Flavia dataset, which
is very popular in recognizing plants. The result shows that the method gave
better performance than PNN, SVM, and Fourier Transform. The method was also
tested using foliage plants with various colors. The accuracy was 90.80% for 50
kinds of plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1514</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1514</id><created>2011-10-07</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Blackwell Approachability and Minimax Theory</title><categories>cs.GT cs.LG</categories><comments>18 pages</comments><msc-class>Primary: 91A05, 49K35, Secondary: 91A20, 91A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript investigates the relationship between Blackwell
Approachability, a stochastic vector-valued repeated game, and minimax theory,
a single-play scalar-valued scenario. First, it is established in a general
setting --- one not permitting invocation of minimax theory --- that
Blackwell's Approachability Theorem and its generalization due to Hou are still
valid. Second, minimax structure grants a result in the spirit of Blackwell's
weak-approachability conjecture, later resolved by Vieille, that any set is
either approachable by one player, or avoidable by the opponent. This analysis
also reveals a strategy for the opponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1519</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1519</id><created>2011-10-07</created><authors><author><keyname>Shabbir</keyname><forenames>Noman</forenames></author><author><keyname>Sadiq</keyname><forenames>Muhammad T.</forenames></author><author><keyname>Kashif</keyname><forenames>Hasnain</forenames></author><author><keyname>Ullah</keyname><forenames>Rizwan</forenames></author></authors><title>Comparison of Radio Propagation Models for Long Term Evolution (LTE)
  Network</title><categories>cs.IT math.IT</categories><comments>15 pages, 10 figures; International Journal of Next-Generation
  Networks (IJNGN) September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns about the radio propagation models used for the upcoming
4th Generation (4G) of cellular networks known as Long Term Evolution (LTE).
The radio wave propagation model or path loss model plays a very significant
role in planning of any wireless communication systems. In this paper, a
comparison is made between different proposed radio propagation models that
would be used for LTE, like Stanford University Interim (SUI) model, Okumura
model, Hata COST 231 model, COST Walfisch-Ikegami &amp; Ericsson 9999 model. The
comparison is made using different terrains e.g. urban, suburban and rural
area.SUI model shows the lowest path lost in all the terrains while COST 231
Hata model illustrates highest path loss in urban area and COST
Walfisch-Ikegami model has highest path loss for suburban and rural
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1522</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1522</id><created>2011-10-07</created><authors><author><keyname>Wang</keyname><forenames>Junjie</forenames></author><author><keyname>Zhou</keyname><forenames>Shuigeng</forenames></author><author><keyname>Guan</keyname><forenames>Jihong</forenames></author></authors><title>Detecting Collusive Cliques in Futures Markets Based on Trading
  Behaviors from Real Data</title><categories>q-fin.TR cs.NE</categories><comments>13 pages, 5 figures and 3 tables. submitted to Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In financial markets, abnormal trading behaviors pose a serious challenge to
market surveillance and risk management. What is worse, there is an increasing
emergence of abnormal trading events that some experienced traders constitute a
collusive clique and collaborate to manipulate some instruments, thus mislead
other investors by applying similar trading behaviors for maximizing their
personal benefits. In this paper, a method is proposed to detect the hidden
collusive cliques involved in an instrument of future markets by first
calculating the correlation coefficient between any two eligible unified
aggregated time series of signed order volume, and then combining the connected
components from multiple sparsified weighted graphs constructed by using the
correlation matrices where each correlation coefficient is over a
user-specified threshold. Experiments conducted on real order data from the
Shanghai Futures Exchange show that the proposed method can effectively detect
suspect collusive cliques. A tool based on the proposed method has been
deployed in the exchange as a pilot application for futures market surveillance
and risk management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1538</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1538</id><created>2011-10-07</created><authors><author><keyname>Greferath</keyname><forenames>Marcus</forenames></author><author><keyname>Fadden</keyname><forenames>Cathy Mc</forenames></author><author><keyname>Zumbr&#xe4;gel</keyname><forenames>Jens</forenames></author></authors><title>Characteristics of Invariant Weights Related to Code Equivalence over
  Rings</title><categories>math.RA cs.IT math.IT</categories><comments>11 pages</comments><msc-class>94B05, 11T71, 05E99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Equivalence Theorem states that, for a given weight on the alphabet,
every linear isometry between linear codes extends to a monomial transformation
of the entire space. This theorem has been proved for several weights and
alphabets, including the original MacWilliams' Equivalence Theorem for the
Hamming weight on codes over finite fields. The question remains: What
conditions must a weight satisfy so that the Extension Theorem will hold? In
this paper we provide an algebraic framework for determining such conditions,
generalising the approach taken in [Greferath, Honold '06].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1549</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1549</id><created>2011-10-07</created><authors><author><keyname>Reddy</keyname><forenames>Sunil Gavaskar</forenames></author><author><keyname>prasad</keyname><forenames>Rajendra</forenames></author></authors><title>Power comparison of CMOS and adiabatic full adder circuit</title><categories>cs.AR</categories><comments>11pages</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.2, No.3, September 2011</journal-ref><doi>10.5121/vlsic.2011.2306</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Full adders are important components in applications such as digital signal
processors (DSP) architectures and microprocessors. Apart from the basic
addition adders also used in performing useful operations such as subtraction,
multiplication, division, address calculation, etc. In most of these systems
the adder lies in the critical path that determines the overall performance of
the system. In this paper conventional complementary metal oxide semiconductor
(CMOS) and adiabatic adder circuits are analyzed in terms of power and
transistor count using 0.18UM technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1553</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1553</id><created>2011-10-07</created><authors><author><keyname>Dongarra</keyname><forenames>Jack</forenames></author><author><keyname>Faverge</keyname><forenames>Mathieu</forenames></author><author><keyname>Herault</keyname><forenames>Thomas</forenames></author><author><keyname>Langou</keyname><forenames>Julien</forenames></author><author><keyname>Robert</keyname><forenames>and Yves</forenames></author></authors><title>Hierarchical QR factorization algorithms for multi-core cluster systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new QR factorization algorithm which is especially
designed for massively parallel platforms combining parallel distributed
multi-core nodes. These platforms make the present and the foreseeable future
of high-performance computing. Our new QR factorization algorithm falls in the
category of the tile algorithms which naturally enables good data locality for
the sequential kernels executed by the cores (high sequential performance), low
number of messages in a parallel distributed setting (small latency term), and
fine granularity (high parallelism).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1560</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1560</id><created>2011-10-07</created><authors><author><keyname>Amdouni</keyname><forenames>Ichrak</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Adjih</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Minet</keyname><forenames>Pascale</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>On the Coloring of Grid Wireless Sensor Networks: the Vector-Based
  Coloring Method</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph coloring is used in wireless networks to optimize network resources:
bandwidth and energy. Nodes access the medium according to their color. It is
the responsibility of the coloring algorithm to ensure that interfering nodes
do not have the same color. In this research report, we focus on wireless
sensor networks with grid topologies. How does a coloring algorithm take
advantage of the regularity of grid topology to provide an optimal periodic
coloring, that is a coloring with the minimum number of colors? We propose the
Vector-Based Coloring Method, denoted VCM, a new method that is able to provide
an optimal periodic coloring for any radio transmission range and for any h-hop
coloring, h&gt;=1. This method consists in determining at which grid nodes a color
can be reproduced without creating interferences between these nodes while
minimizing the number of colors used. We compare the number of colors provided
by VCM with the number of colors obtained by a distributed coloring algorithm
with line and column priority assignments. We also provide bounds on the number
of colors of optimal general colorings of the infinite grid, and show that
periodic colorings (and thus VCM) are asymptotically optimal. Finally, we
discuss the applicability of this method to a real wireless network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1563</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1563</id><created>2011-10-07</created><authors><author><keyname>Day</keyname><forenames>Khaled</forenames></author><author><keyname>Touzene</keyname><forenames>Abderezak</forenames></author><author><keyname>Arafeh</keyname><forenames>Bassel</forenames></author><author><keyname>Alzeidi</keyname><forenames>Nasser</forenames></author></authors><title>Parallel routing in Mobile Ad-Hoc Networks</title><categories>cs.NI</categories><comments>18 pages, 13 figures, 6 tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.3, No.5, Sep 2011</journal-ref><doi>10.5121/ijcnc.2011.3506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and evaluates a new position-based Parallel Routing
Protocol (PRP) for simultaneously routing multiple data packets over disjoint
paths in a mobile ad-hoc network (MANET) for higher reliability and reduced
communication delays. PRP views the geographical region where the MANET is
located as a virtual 2-dimensional grid of cells. Cell-disjoint (parallel)
paths between grid cells are constructed and used for building pre-computed
routing tables. A single gateway node in each grid cell handles routing through
that grid cell reducing routing overheads. Each node maintains updated
information about its own location in the virtual grid using GPS. Nodes also
keep track of the location of other nodes using a new proposed cell-based
broadcasting algorithm. Nodes exchange energy level information with neighbors
allowing energy-aware selection of the gateway nodes. Performance evaluation
results have been derived showing the attractiveness of the proposed parallel
routing protocol from different respects including low communication delays,
high packet delivery ratios, high routing path stability, and low routing
overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1569</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1569</id><created>2011-09-26</created><authors><author><keyname>Zhao</keyname><forenames>Yang</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>Robust Estimators for Variance-Based Device-Free Localization and
  Tracking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human motion in the vicinity of a wireless link causes variations in the link
received signal strength (RSS). Device-free localization (DFL) systems, such as
variance-based radio tomographic imaging (VRTI), use these RSS variations in a
static wireless network to detect, locate and track people in the area of the
network, even through walls. However, intrinsic motion, such as branches moving
in the wind and rotating or vibrating machinery, also causes RSS variations
which degrade the performance of a DFL system. In this paper, we propose and
evaluate two estimators to reduce the impact of the variations caused by
intrinsic motion. One estimator uses subspace decomposition, and the other
estimator uses a least squares formulation. Experimental results show that both
estimators reduce localization root mean squared error by about 40% compared to
VRTI. In addition, the Kalman filter tracking results from both estimators have
97% of errors less than 1.3 m, more than 60% improvement compared to tracking
results from VRTI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1579</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1579</id><created>2011-10-07</created><authors><author><keyname>Erritali</keyname><forenames>Mohammed</forenames></author><author><keyname>Reda</keyname><forenames>Oussama Mohamed</forenames></author><author><keyname>Ouahidi</keyname><forenames>Bouabid El</forenames></author></authors><title>A Contribution to Secure the Routing Protocol &quot;Greedy Perimeter
  Stateless Routing&quot; Using a Symmetric Signature-Based AES and MD5 Hash</title><categories>cs.NI cs.DC</categories><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.2, No.5, September 2011 p95-103, ISSN: 0976 - 9757 [Online]; 2229 - 3957
  [Print]</journal-ref><doi>10.5121/ijdps.2011.2509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a contribution to secure the routing protocol GPSR (Greedy
Perimeter Stateless Routing) for vehicular ad hoc networks, we examine the
possible attacks against GPSR and security solutions proposed by different
research teams working on ad hoc network security. Then, we propose a solution
to secure GPSR packet by adding a digital signature based on symmetric
cryptography generated using the AES algorithm and the MD5 hash function more
suited to a mobile environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1580</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1580</id><created>2011-10-07</created><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames><affiliation>Seffi</affiliation></author><author><keyname>Buchbinder</keyname><forenames>Niv</forenames><affiliation>Seffi</affiliation></author><author><keyname>Madry</keyname><forenames>Aleksander</forenames><affiliation>Seffi</affiliation></author><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author></authors><title>A Polylogarithmic-Competitive Algorithm for the k-Server Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first polylogarithmic-competitive randomized online algorithm for
the $k$-server problem on an arbitrary finite metric space. In particular, our
algorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any
metric space on n points. Our algorithm improves upon the deterministic
(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]
whenever n is sub-exponential in k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1590</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1590</id><created>2011-10-07</created><authors><author><keyname>Jandaeng</keyname><forenames>C.</forenames></author><author><keyname>Suntiamontut</keyname><forenames>W.</forenames></author><author><keyname>Elz</keyname><forenames>N.</forenames></author></authors><title>PSA: The Packet Scheduling Algorithm for Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main cause of wasted energy consumption in wireless sensor networks is
packet collision. The packet scheduling algorithm is therefore introduced to
solve this problem. Some packet scheduling algorithms can also influence and
delay the data transmitting in the real-time wireless sensor networks. This
paper presents the packet scheduling algorithm (PSA) in order to reduce the
packet congestion in MAC layer leading to reduce the overall of packet
collision in the system The PSA is compared with the simple CSMA/CA and other
approaches using network topology benchmarks in mathematical method. The
performances of our PSA are better than the standard (CSMA/CA). The PSA
produces better throughput than other algorithms. On other hand, the average
delay of PSA is higher than previous works. However, the PSA utilizes the
channel better than all algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1591</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1591</id><created>2011-10-07</created><authors><author><keyname>Gracia-L&#xe1;zaro</keyname><forenames>Carlos</forenames></author><author><keyname>Quijandr&#xed;a</keyname><forenames>Fernando</forenames></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>Laura</forenames></author><author><keyname>Flor&#xed;a</keyname><forenames>Luis Mario</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Co-evolutionnary network approach to cultural dynamics controlled by
  intolerance</title><categories>physics.soc-ph cs.SI</categories><doi>10.1103/PhysRevE.84.067101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from Axelrod's model of cultural dissemination, we introduce a
rewiring probability, enabling agents to cut the links with their unfriendly
neighbors if their cultural similarity is below a tolerance parameter. For low
values of tolerance, rewiring promotes the convergence to a frozen monocultural
state. However, intermediate tolerance values prevent rewiring once the network
is fragmented, resulting in a multicultural society even for values of initial
cultural diversity in which the original Axelrod model reaches globalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1602</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1602</id><created>2011-10-07</created><authors><author><keyname>Vijayakumar</keyname><forenames>P.</forenames></author><author><keyname>Bose</keyname><forenames>S.</forenames></author><author><keyname>Kannan</keyname><forenames>A.</forenames></author></authors><title>Error Detection and Correction for Distributed Group Key Agreement
  Protocol</title><categories>cs.CR</categories><comments>Extended version of the conference ACITY-2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Integrating an efficient Error detection and correction scheme with less
encoding and decoding complexity to support the distribution of keying material
in a secure group communication is an important issue, since the amount of
information carried out in the wireless channel is high which produces more
errors due to noise available in the communication channel. Moreover, the key
must be sent securely to the group members. In this paper, we propose a new
efficient group key computation protocol that provides more security and also
integrates an encoding method in sender side and decoding method in the
receiver side. To achieve security in key computation process, we propose
Euler's totient function based Diffie-hellman key distribution protocol. To
provide efficient error detection and correction method while distributing the
Keying and re-keying information, we introduce tanner graph based encoding
stopping set construction algorithm in sender and receiver side of the group
communication. Two major operations in this scheme are joining and leaving
operations for managing group memberships. The encoding and decoding complexity
of this approach is computed in this paper and it is proved that this proposed
approach takes less decoding time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1614</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1614</id><created>2011-10-07</created><updated>2011-10-17</updated><authors><author><keyname>Constable</keyname><forenames>Robert</forenames></author><author><keyname>Bickford</keyname><forenames>Mark</forenames></author></authors><title>Intuitionistic Completeness of First-Order Logic</title><categories>cs.LO</categories><comments>2011-10-17: Abstract updated to match 10/14 version. 2011-10-14:
  Edited version posted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish completeness for intuitionistic first-order logic, iFOL, showing
that a formula is provable if and only if its embedding into minimal logic,
mFOL, is uniformly valid under the Brouwer Heyting Kolmogorov (BHK) semantics,
the intended semantics of iFOL and mFOL. Our proof is intuitionistic and
provides an effective procedure Prf that converts uniform minimal evidence into
a formal first-order proof. We have implemented Prf. Uniform validity is
defined using the intersection operator as a universal quantifier over the
domain of discourse and atomic predicates. Formulas of iFOL that are uniformly
valid are also intuitionistically valid, but not conversely. Our strongest
result requires the Fan Theorem; it can also be proved classically by showing
that Prf terminates using Konig's Theorem.
  The fundamental idea behind our completeness theorem is that a single
evidence term evd witnesses the uniform validity of a minimal logic formula F.
Finding even one uniform realizer guarantees intuitionistic validity because
Prf(F, evd) builds a first-order proof of F, establishing its intuitionistic
validity and providing a purely logical normalized realizer.
  We establish completeness for iFOL as follows. Friedman showed that iFOL can
be embedded in minimal logic (mFOL) by his A-transformation, mapping formula F
to FA. If F is uniformly valid, then so is FA, and by our completeness theorem,
we can find a proof of FA in minimal logic. Then we intuitionistically prove F
from FFalse, i.e. by taking False for A and for \bot of mFOL. Our result
resolves an open question posed by Beth in 1947.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1616</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1616</id><created>2011-10-07</created><authors><author><keyname>Carroll-Nellenback</keyname><forenames>Jonathan</forenames></author><author><keyname>Shroyer</keyname><forenames>Brandon</forenames></author><author><keyname>Frank</keyname><forenames>Adam</forenames></author><author><keyname>Ding</keyname><forenames>Chen</forenames></author></authors><title>Efficient Parallelization for AMR MHD Multiphysics Calculations;
  Implementation in AstroBEAR</title><categories>astro-ph.IM astro-ph.SR cs.DC physics.comp-ph</categories><comments>Proceedings From 6th Annual International Conference on Numerical
  Modeling of Space Plasma Flows</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current AMR simulations require algorithms that are highly parallelized and
manage memory efficiently. As compute engines grow larger, AMR simulations will
require algorithms that achieve new levels of efficient parallelization and
memory management. We have attempted to employ new techniques to achieve both
of these goals. Patch or grid based AMR often employs ghost cells to decouple
the hyperbolic advances of each grid on a given refinement level. This
decoupling allows each grid to be advanced independently. In AstroBEAR we
utilize this independence by threading the grid advances on each level with
preference going to the finer level grids. This allows for global load
balancing instead of level by level load balancing and allows for greater
parallelization across both physical space and AMR level. Threading of level
advances can also improve performance by interleaving communication with
computation, especially in deep simulations with many levels of refinement. To
improve memory management we have employed a distributed tree algorithm that
requires processors to only store and communicate local sections of the AMR
tree structure with neighboring processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1627</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1627</id><created>2011-10-07</created><authors><author><keyname>Yerriswamy</keyname><forenames>T.</forenames></author><author><keyname>Jagadeesha</keyname><forenames>S. N.</forenames></author></authors><title>Fault Tolerant Matrix Pencil Method for Direction of Arrival Estimation</title><categories>cs.OH</categories><comments>13 pages, 3 figures, Signal and image processing:An international
  Journal 2011</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.2, No.3, September 2011</journal-ref><doi>10.5121/sipij.2011.2306</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Continuing to estimate the Direction-of-arrival (DOA) of the signals
impinging on the antenna array, even when a few elements of the underlying
Uniform Linear Antenna Array (ULA) fail to work will be of practical interest
in RADAR, SONAR and Wireless Radio Communication Systems. This paper proposes a
new technique to estimate the DOAs when a few elements are malfunctioning. The
technique combines Singular Value Thresholding (SVT) based Matrix Completion
(MC) procedure with the Direct Data Domain (D^3) based Matrix Pencil (MP)
Method. When the element failure is observed, first, the MC is performed to
recover the missing data from failed elements, and then the MP method is used
to estimate the DOAs. We also, propose a very simple technique to detect the
location of elements failed, which is required to perform MC procedure. We
provide simulation studies to demonstrate the performance and usefulness of the
proposed technique. The results indicate a better performance, of the proposed
DOA estimation scheme under different antenna failure scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1628</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1628</id><created>2011-10-07</created><updated>2012-10-11</updated><authors><author><keyname>Montagnier</keyname><forenames>Olivier</forenames></author><author><keyname>Hochard</keyname><forenames>Christian</forenames></author></authors><title>Optimisation of hybrid high-modulus/high-strength carbon fiber
  reinforced plastic composite drive</title><categories>cs.CE cs.SE physics.comp-ph</categories><comments>13 pages, preprint submitted to Materials and Design (Received 22
  February 2012; received in revised form 18 september 2012; accepted 21
  september 2012)</comments><doi>10.1016/j.matdes.2012.09.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study deals with the optimisation of hybrid composite drive shafts
operating at subcritical or supercritical speeds, using a genetic algorithm. A
formulation for the flexural vibrations of a composite drive shaft mounted on
viscoelastic supports including shear effects is developed. In particular, an
analytic stability criterion is developed to ensure the integrity of the system
in the supercritical regime. Then it is shown that the torsional strength can
be computed with the maximum stress criterion. A shell method is developed for
computing drive shaft torsional buckling. The optimisation of a helicopter tail
rotor driveline is then performed. In particular, original hybrid shafts
consisting of high-modulus and high-strength carbon fibre reinforced epoxy
plies were studied. The solutions obtained using the method presented here made
it possible to greatly decrease the number of shafts and the weight of the
driveline under subcritical conditions, and even more under supercritical
conditions. This study yielded some general rules for designing an optimum
composite shaft without any need for optimisation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1658</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1658</id><created>2011-10-05</created><updated>2015-06-02</updated><authors><author><keyname>Steinmetz</keyname><forenames>Jason W.</forenames></author></authors><title>Algorithm that Solves 3-SAT in Polynomial Time</title><categories>cs.CC</categories><comments>This paper has been withdrawn by the author because the integer
  operations within the algorithm cannot be proven to have a polynomial run
  time</comments><acm-class>F.1.3; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of whether the complexity class P is equal to the complexity
class NP has been a seemingly intractable problem for over 4 decades. It has
been clear that if an algorithm existed that would solve the problems in the NP
class in polynomial time then P would equal NP. However, no one has yet been
able to create that algorithm or to successfully prove that such an algorithm
cannot exist. The algorithm that will be presented in this paper solves the
3-satisfiability or 3-CNF-SAT problem, which has been proven to be NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1685</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1685</id><created>2011-10-07</created><authors><author><keyname>Butt</keyname><forenames>Fouad</forenames></author><author><keyname>Bokhari</keyname><forenames>Syed Saadat</forenames></author><author><keyname>Abhari</keyname><forenames>Abdolreza</forenames></author><author><keyname>Ferworn</keyname><forenames>Alexander</forenames></author></authors><title>Scalable Grid Resource Discovery through Distributed Search</title><categories>cs.DC cs.NI</categories><journal-ref>F. Butt, S. S. Bokhari, A. Abhari, and A. Ferworn. &quot;Scalable
  Resource Discovery through Distributed Search.&quot; International Journal of
  Distributed and Parallel Systems (IJDPS) 2.5 (2011): 1-19</journal-ref><doi>10.5121/ijdps.2011.2501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple and scalable web-based model for grid resource
discovery for the Internet. The resource discovery model contains the metadata
and resource finder web services. The information of resource finder web
services is kept in the repositories that are distributed in the application
layer of Internet. The resource finder web services will be discovered by
sending queries to the repositories in a similar way as the DNS protocol. The
underlying technology for implementation of the two architectures of this model
is introduced. These architectures are: Direct and Centralized Web-Based Grid
Resource Discovery. The resource discovery time is computed after simulating
each of these models in GridSim. By performing scalability tests, we found that
when increasing the load on the grid with more users and resources, the cost of
our model in comparison to the grid resource discovery time is marginal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1687</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1687</id><created>2011-10-07</created><updated>2012-04-20</updated><authors><author><keyname>Singla</keyname><forenames>Ankit</forenames></author><author><keyname>Hong</keyname><forenames>Chi-Yao</forenames></author><author><keyname>Popa</keyname><forenames>Lucian</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author></authors><title>Jellyfish: Networking Data Centers Randomly</title><categories>cs.NI</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industry experience indicates that the ability to incrementally expand data
centers is essential. However, existing high-bandwidth network designs have
rigid structure that interferes with incremental expansion. We present
Jellyfish, a high-capacity network interconnect, which, by adopting a random
graph topology, yields itself naturally to incremental expansion. Somewhat
surprisingly, Jellyfish is more cost-efficient than a fat-tree: A Jellyfish
interconnect built using the same equipment as a fat-tree, supports as many as
25% more servers at full capacity at the scale of a few thousand nodes, and
this advantage improves with scale. Jellyfish also allows great flexibility in
building networks with different degrees of oversubscription. However,
Jellyfish's unstructured design brings new challenges in routing, physical
layout, and wiring. We describe and evaluate approaches that resolve these
challenges effectively, indicating that Jellyfish could be deployed in today's
data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1693</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1693</id><created>2011-10-08</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Ung</keyname><forenames>Chin-Ting</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>On the strong chromatic index and maximum induced matching of
  tree-cographs and permutation graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exist linear-time algorithms that compute the strong
chromatic index and a maximum induced matching of tree-cographs when the
decomposition tree is a part of the input. We also show that there exists an
efficient algorithm for the strong chromatic index of permutation graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1700</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1700</id><created>2011-10-08</created><authors><author><keyname>Mohammadi</keyname><forenames>Shirin</forenames></author><author><keyname>Safaei</keyname><forenames>Ali A.</forenames></author><author><keyname>Abdi</keyname><forenames>Fatemeh</forenames></author><author><keyname>Haghjoo</keyname><forenames>Mostafa S.</forenames></author></authors><title>Adaptive Data Stream Management System Using Learning Automata</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many modern applications, data are received as infinite, rapid,
unpredictable and time- variant data elements that are known as data streams.
Systems which are able to process data streams with such properties are called
Data Stream Management Systems (DSMS). Due to the unpredictable and time-
variant properties of data streams as well as system, adaptivity of the DSMS is
a major requirement for each DSMS. Accordingly, determining parameters which
are effective on the most important performance metric of a DSMS (i.e.,
response time) and analysing them will affect on designing an adaptive DSMS. In
this paper, effective parameters on response time of DSMS are studied and
analysed and a solution is proposed for DSMSs' adaptivity. The proposed
adaptive DSMS architecture includes a learning unit that frequently evaluates
system to adjust the optimal value for each of tuneable effective. Learning
Automata is used as the learning mechanism of the learning unit to adjust the
value of tuneable effective parameters. So, when system faces some changes, the
learning unit increases performance by tuning each of tuneable effective
parameters to its optimum value. Evaluation results illustrate that after a
while, parameters reach their optimum value and then DSMS's adaptivity will be
improved considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1701</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1701</id><created>2011-10-08</created><authors><author><keyname>Rasool</keyname><forenames>Shaik</forenames></author><author><keyname>Sridhar</keyname><forenames>G.</forenames></author><author><keyname>Kumar</keyname><forenames>K. Hemanth</forenames></author><author><keyname>Kumar</keyname><forenames>P. Ravi</forenames></author></authors><title>Enhanced Secure Algorithm for Message Communion</title><categories>cs.CR</categories><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.5, Sep 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper puts forward a safe mechanism of data transmission to tackle the
security problem of information which is transmitted in Internet. The
encryption standards such as DES (Data Encryption Standard), AES (Advanced
Encryption Standard) and EES (Escrowed Encryption Standard) are widely used to
solve the problem of communication over an insecure channel. With advanced
technologies in computer hardware and software, these standards seem not to be
as secure and fast as one would like. In this paper we propose a encryption
technique which provides security to both the message and the secret key
achieving confidentiality and authentication. The Symmetric algorithm used has
two advantages over traditional schemes. First, the encryption and decryption
procedures are much simpler, and consequently, much faster. Second, the
security level is higher due to the inherent poly-alphabetic nature of the
substitution mapping method used here, together with the translation and
transposition operations performed in the algorithm. Asymmetric algorithm RSA
is worldwide known for its high security. In this paper a detailed report of
the process is presented and analysis is done comparing our proposed technique
with familiar techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1708</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1708</id><created>2011-10-08</created><authors><author><keyname>Ng</keyname><forenames>E</forenames></author><author><keyname>Sarich</keyname><forenames>J</forenames></author><author><keyname>Wild</keyname><forenames>S M</forenames></author><author><keyname>Munson</keyname><forenames>T</forenames></author><author><keyname>Aktulga</keyname><forenames>H</forenames></author><author><keyname>Yang</keyname><forenames>C</forenames></author><author><keyname>Maris</keyname><forenames>P</forenames></author><author><keyname>Vary</keyname><forenames>J P</forenames></author><author><keyname>Schunck</keyname><forenames>N</forenames></author><author><keyname>Bertolli</keyname><forenames>M G</forenames></author><author><keyname>Kortelainen</keyname><forenames>M</forenames></author><author><keyname>Nazarewicz</keyname><forenames>W</forenames></author><author><keyname>Papenbrock</keyname><forenames>T</forenames></author><author><keyname>Stoitsov</keyname><forenames>M V</forenames></author></authors><title>Advancing Nuclear Physics Through TOPS Solvers and Tools</title><categories>cs.CE physics.comp-ph</categories><comments>SciDAC 2011 Conference, July 10-14, 2011, Denver, CO; 5 pages, 2
  tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of many scientific applications is the solution of algebraic
systems, such as linear systems of equations, eigenvalue problems, and
optimization problems, to name a few. TOPS, which stands for Towards Optimal
Petascale Simulations, is a SciDAC applied math center focused on the
development of solvers for tackling these algebraic systems, as well as the
deployment of such technologies in large-scale scientific applications of
interest to the U.S. Department of Energy. In this paper, we highlight some of
the solver technologies we have developed in optimization and matrix
computations. We also describe some accomplishments achieved using these
technologies in UNEDF, a SciDAC application project on nuclear physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1716</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1716</id><created>2011-10-08</created><updated>2011-11-05</updated><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames></author></authors><title>Treating Insomnia, Amnesia, and Acalculia in Regular Expression Matching</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular expressions provide a flexible means for matching strings and they
are often used in data-intensive applications. They are formally equivalent to
either deterministic finite automata (DFAs) or nondeterministic finite automata
(NFAs). Both DFAs and NFAs are affected by two problems known as amnesia and
acalculia, and DFAs are also affected by a problem known as insomnia. Existing
techniques require an automata conversion and compaction step that prevents the
use of existing automaton databases and hinders the maintenance of the
resulting compact automata. In this paper, we propose Parallel Finite State
Machines (PFSMs), which are able to run any DFA- or NFA-like state machines
without a previous conversion or compaction step. PFSMs report, online, all the
matches found within an input string and they solve the three aforementioned
problems. Parallel Finite State Machines require quadratic time and linear
memory and they are distributable. Parallel Finite State Machines make very
fast distributed regular expression matching in data-intensive applications
feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1729</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1729</id><created>2011-10-08</created><authors><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander</forenames></author><author><keyname>Blakeley</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Budav&#xe1;ri</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Tomic</keyname><forenames>Dragan</forenames></author><author><keyname>Milovanovic</keyname><forenames>Milos</forenames></author><author><keyname>Tintor</keyname><forenames>Marko</forenames></author><author><keyname>Jovanovic</keyname><forenames>Andrija</forenames></author></authors><title>Array Requirements for Scientific Applications and an Implementation for
  Microsoft SQL Server</title><categories>cs.DB</categories><acm-class>H.2.4; H.3.2; H.2.8; E.1; J.2</acm-class><journal-ref>Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases</journal-ref><doi>10.1145/1966895.1966897</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines certain scenarios from the fields of astrophysics and
fluid dynamics simulations which require high performance data warehouses that
support array data type. A common feature of all these use cases is that
subsetting and preprocessing the data on the server side (as far as possible
inside the database server process) is necessary to avoid the client-server
overhead and to minimize IO utilization. Analyzing and summarizing the
requirements of the various fields help software engineers to come up with a
comprehensive design of an array extension to relational database systems that
covers a wide range of scientific applications. We also present a working
implementation of an array data type for Microsoft SQL Server 2008 to support
large-scale scientific applications. We introduce the design of the array type,
results from a performance evaluation, and discuss the lessons learned from
this implementation. The library can be downloaded from our website at
http://voservices.net/sqlarray/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1730</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1730</id><created>2011-10-08</created><authors><author><keyname>Kuribayashi</keyname><forenames>Shin-ichi</forenames></author></authors><title>Optimal Joint Multiple Resource Allocation Method for Cloud Computing
  Environments</title><categories>cs.DC cs.NI</categories><journal-ref>International Journal of Research and Reviews in Computer Science
  (IJRRCS), Vol.2, No.1, March 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a model for enabling convenient, on-demand network access
to a shared pool of configurable computing resources. To provide cloud
computing services economically, it is important to optimize resource
allocation under the assumption that the required resource can be taken from a
shared resource pool. In addition, to be able to provide processing ability and
storage capacity, it is necessary to allocate bandwidth to access them at the
same time.
  This paper proposes an optimal resource allocation method for cloud computing
environments. First, this paper develops a resource allocation model of cloud
computing environments, assuming both processing ability and bandwidth are
allocated simultaneously to each service request and rented out on an hourly
basis. The allocated resources are dedicated to each service request. Next,
this paper proposes an optimal joint multiple resource allocation method, based
on the above resource allocation model. It is demonstrated by simulation
evaluation that the proposed method can reduce the request loss probability and
as a result, reduce the total resource required, compared with the conventional
allocation method. Then, this paper defines basic principles and a measure for
achieving fair resource allocation among multiple users in a cloud computing
environment, and proposes a fair joint multiple resource allocation method. It
is demonstrated by simulation evaluations that the proposed method enables the
fair resource allocation among multiple users without a large decline in
resource efficiency. Keywords: Cloud computing, joint multiple resource
allocation, fairness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1732</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1732</id><created>2011-10-08</created><updated>2011-12-11</updated><authors><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir Medina</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Electrical Vehicles in the Smart Grid: A Mean Field Game Analysis</title><categories>cs.GT physics.soc-ph</categories><comments>submitted to IEEE Journal on Selected Areas in Communications: Smart
  Grid Communications Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate the competitive interaction between
electrical vehicles or hybrid oil-electricity vehicles in a Cournot market
consisting of electricity transactions to or from an underlying electricity
distribution network. We provide a mean field game formulation for this
competition, and introduce the set of fundamental differential equations ruling
the behavior of the vehicles at the feedback Nash equilibrium, referred here to
as the mean field equilibrium. This framework allows for a consistent analysis
of the evolution of the price of electricity as well as of the instantaneous
electricity demand in the power grid. Simulations precisely quantify those
parameters and suggest that significant reduction of the daily electricity peak
demand can be achieved by appropriate electricity pricing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1734</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1734</id><created>2011-10-08</created><authors><author><keyname>Ghosh</keyname><forenames>Debaditya</forenames></author><author><keyname>Majumder</keyname><forenames>Pritam</forenames></author><author><keyname>Das</keyname><forenames>Ayan Kumar</forenames></author></authors><title>A New Energy Efficient Approach Towards WASN Routing with Modified QCS
  Protocol</title><categories>cs.NI</categories><comments>18 pages, 14 figures</comments><journal-ref>International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing
  (IJASUC) Vol.2, No.3, September 2011</journal-ref><doi>10.5121/ijasuc.2011.230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world Wireless Ad-hoc sensor network, consists of small sensor
nodes having limited resources, has a great potential to solve problems in
various domain including disaster management. In this paper &quot;QCS-protocol&quot; is
modified which was introduced in our previous paper [1] and named as &quot;Modified
QCS-protocol&quot;. This is the backbone of our Intelligent Energy Efficient Ad-hoc
Sensor Network. Two other protocols &quot;Irregular Information Transfer&quot; &amp; &quot;Final
Broadcast-Petrol Flow&quot; protocol are also modified to enhance performance of the
new version of QCS protocol to run the system properly and to make the network
more energy efficient and perfect. The challenges in WASN are- limited node
power, Ad-hoc organization of network and reliability. Most of the existing
approaches addressed the problems separately, but not in a totality. This paper
shows how the network can have unlimited life and all time readiness with
overall stability to send information to the base station with minimum power
dissipation with the help of multimode &quot;same type&quot; sensor nodes and type
categorization of generated information. Moreover an effort is made to give
some light to the implementation issues and analyzed overall performance of the
network by MATLAB simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1753</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1753</id><created>2011-10-08</created><authors><author><keyname>Kavisankar</keyname><forenames>L.</forenames></author><author><keyname>Chellappan</keyname><forenames>C.</forenames></author></authors><title>CNoA: Challenging Number Approach for uncovering TCP SYN flooding using
  SYN spoofing attack</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenging number is used for the detection of Spoofing attack. The IP
Spoofing is considered to be one of the potentially brutal attack which acts as
a tool for the DDoS attack which is considered to be a major threat among
security problems in today's internet. These kinds of attack are extremely
severe. They bring down business of company drastically. DDoS attack can easily
exhaust the computing and communication resources of its victim within a short
period of time. There are attacks exploiting some vulnerability or
implementation bug in the software implementation of a service to bring that
down and some attacks will use all the available resources at the target
machine. This deals on attacks that consume all the bandwidth available to the
victim machine. While concentrating on the bandwidth attack the TCP SYN flood
is the more prominent attack. TCP/IP protocol suite is the most widely used
protocol suite for data communication. The TCP SYN flood works by exhausting
the TCP connection queue of the host and thus denying legitimate connection
request. There are various methods used to detect and prevent this attack, one
of which is to block the packet based on SYN flag count from the same IP
address. This kind of prevention methods becomes unsuitable when the attackers
use the Spoofed IP address. The SYN spoofing becomes a major tool the TCP SYN
flooding. For the prevention of this kind of attacks, the TCP specific probing
is used in the proposed scheme where the client is requested challenging number
while sending the ACK in the three way hand shake. This is very useful to find
the Spoofed IP Packets/TCP SYN flood and preventing them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1757</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1757</id><created>2011-10-08</created><updated>2011-10-11</updated><authors><author><keyname>Perry</keyname><forenames>Patrick O.</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Regularized Laplacian Estimation and Fast Eigenvector Approximation</title><categories>cs.DS stat.ML</categories><comments>13 pages and 3 figures. A more detailed version of a paper appearing
  in the 2011 NIPS Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Mahoney and Orecchia demonstrated that popular diffusion-based
procedures to compute a quick \emph{approximation} to the first nontrivial
eigenvector of a data graph Laplacian \emph{exactly} solve certain regularized
Semi-Definite Programs (SDPs). In this paper, we extend that result by
providing a statistical interpretation of their approximation procedure. Our
interpretation will be analogous to the manner in which $\ell_2$-regularized or
$\ell_1$-regularized $\ell_2$-regression (often called Ridge regression and
Lasso regression, respectively) can be interpreted in terms of a Gaussian prior
or a Laplace prior, respectively, on the coefficient vector of the regression
problem. Our framework will imply that the solutions to the Mahoney-Orecchia
regularized SDP can be interpreted as regularized estimates of the
pseudoinverse of the graph Laplacian. Conversely, it will imply that the
solution to this regularized estimation problem can be computed very quickly by
running, e.g., the fast diffusion-based PageRank procedure for computing an
approximation to the first nontrivial eigenvector of the graph Laplacian.
Empirical results are also provided to illustrate the manner in which
approximate eigenvector computation \emph{implicitly} performs statistical
regularization, relative to running the corresponding exact algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1758</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1758</id><created>2011-10-08</created><updated>2012-03-04</updated><authors><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>IDSL, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Witt</keyname><forenames>Andreas</forenames><affiliation>IDS</affiliation></author></authors><title>Data formats for phonological corpora</title><categories>cs.CL</categories><comments>Handbook of Corpus Phonology Oxford University Press (Ed.) (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the present chapter is to explore the possibility of providing
the research (but also the industrial) community that commonly uses spoken
corpora with a stable portfolio of well-documented standardised formats that
allow a high re-use rate of annotated spoken resources and, as a consequence,
better interoperability across tools used to produce or exploit such resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1767</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1767</id><created>2011-10-08</created><authors><author><keyname>Mesmoudi</keyname><forenames>Samira</forenames></author><author><keyname>Feham</keyname><forenames>Mohammed</forenames></author></authors><title>BSK-WBSN: Biometric Symmetric Keys to Secure Wireless Body Sensors
  Networks</title><categories>cs.NI</categories><comments>12 pages; International Journal of Network Security &amp; Its
  Applications (IJNSA), Vol.3, No.5, Sep 2011</comments><doi>10.5121/ijnsa.2011.3512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wireless Sensors Network (WSN) is an emergent technology resulting from
progress of various fields. Many applications of networks WSN are born. One of
the applications which have an operational effectiveness relates to the field
of health and allows a medical remote support. Miniature wireless sensors,
strategically placed on the human body, create a Wireless Body Sensor Network
(WBSN) which allows supervising various essential biological signals (rate of
heartbeat, pressure, etc). The sensitivity of medical information requires
mechanisms of safety. This performance constitutes a challenge for WBSN because
of their limitation in resources energy and data-processing. In this paper we
propose a new approach to symmetric cryptographic key establishment, based on
biometrics physiology. This approach takes into account WBSN constraints and
its topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1769</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1769</id><created>2011-10-08</created><authors><author><keyname>Bento</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>On the trade-off between complexity and correlation decay in structural
  learning algorithms</title><categories>stat.ML cs.LG physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning the structure of Ising models (pairwise
binary Markov random fields) from i.i.d. samples. While several methods have
been proposed to accomplish this task, their relative merits and limitations
remain somewhat obscure. By analyzing a number of concrete examples, we show
that low-complexity algorithms often fail when the Markov random field develops
long-range correlations. More precisely, this phenomenon appears to be related
to the Ising model phase transition (although it does not coincide with it).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1779</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1779</id><created>2011-10-08</created><updated>2011-12-09</updated><authors><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Side-payment profitability and interacting eyeball ISPs under convex
  demand-response modeling congestion-sensitive applications</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the issue of side payments between content
providers (CPs) and Internet service (access bandwidth) providers (ISPs) in an
Internet that is potentially not neutral. We herein generalize past results
modeling the ISP and CP interaction as a noncooperative game in two directions.
We consider different demand response models (price sensitivities) for
different provider types in order to explore when side payments are profitable
to the ISP. Also, we consider convex (non-linear) demand response to model
demand triggered by traffic which is sensitive to access bandwidth congestion,
particularly delay-sensitive interactive real-time applications. Finally, we
consider a model with two competing &quot;eyeball&quot; ISPs with transit pricing of net
traffic at their peering point to study the effects of caching remote content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1781</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1781</id><created>2011-10-08</created><authors><author><keyname>Kesidis</keyname><forenames>G.</forenames></author><author><keyname>Kurve</keyname><forenames>A.</forenames></author></authors><title>A Study of Unsupervised Adaptive Crowdsourcing</title><categories>cs.LG cs.SY</categories><comments>Technical Report, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unsupervised crowdsourcing performance based on the model wherein
the responses of end-users are essentially rated according to how their
responses correlate with the majority of other responses to the same
subtasks/questions. In one setting, we consider an independent sequence of
identically distributed crowdsourcing assignments (meta-tasks), while in the
other we consider a single assignment with a large number of component
subtasks. Both problems yield intuitive results in which the overall
reliability of the crowd is a factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1785</identifier>
 <datestamp>2015-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1785</id><created>2011-10-08</created><authors><author><keyname>Chierichetti</keyname><forenames>Flavio</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>Voting with Limited Information and Many Alternatives</title><categories>cs.DM cs.DS cs.GT math.PR</categories><msc-class>91A12, 60-xx</msc-class><acm-class>G.2.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional axiomatic approach to voting is motivated by the problem of
reconciling differences in subjective preferences. In contrast, a dominant line
of work in the theory of voting over the past 15 years has considered a
different kind of scenario, also fundamental to voting, in which there is a
genuinely &quot;best&quot; outcome that voters would agree on if they only had enough
information. This type of scenario has its roots in the classical Condorcet
Jury Theorem; it includes cases such as jurors in a criminal trial who all want
to reach the correct verdict but disagree in their inferences from the
available evidence, or a corporate board of directors who all want to improve
the company's revenue, but who have different information that favors different
options.
  This style of voting leads to a natural set of questions: each voter has a
{\em private signal} that provides probabilistic information about which option
is best, and a central question is whether a simple plurality voting system,
which tabulates votes for different options, can cause the group decision to
arrive at the correct option. We show that plurality voting is powerful enough
to achieve this: there is a way for voters to map their signals into votes for
options in such a way that --- with sufficiently many voters --- the correct
option receives the greatest number of votes with high probability. We show
further, however, that any process for achieving this is inherently expensive
in the number of voters it requires: succeeding in identifying the correct
option with probability at least $1 - \eta$ requires $\Omega(n^3 \epsilon^{-2}
\log \eta^{-1})$ voters, where $n$ is the number of options and $\epsilon$ is a
distributional measure of the minimum difference between the options.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1796</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1796</id><created>2011-10-09</created><authors><author><keyname>Ray</keyname><forenames>Dip Narayan</forenames></author><author><keyname>Majumder</keyname><forenames>Somajyoti</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Sumit</forenames></author></authors><title>A Behavior-based Approach for Multi-agent Q-learning for Autonomous
  Exploration</title><categories>cs.RO cs.LG</categories><comments>15 pages;(ISSN:2045-8711)</comments><journal-ref>International Journal of Innovative Technology &amp; Creative
  Engineering, Vol.1 No.7 July 2011, page 1-15</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of mobile robots is being popular over the world mainly for
autonomous explorations in hazardous/ toxic or unknown environments. This
exploration will be more effective and efficient if the explorations in unknown
environment can be aided with the learning from past experiences. Currently
reinforcement learning is getting more acceptances for implementing learning in
robots from the system-environment interactions. This learning can be
implemented using the concept of both single-agent and multiagent. This paper
describes such a multiagent approach for implementing a type of reinforcement
learning using a priority based behaviour-based architecture. This proposed
methodology has been successfully tested in both indoor and outdoor
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1801</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1801</id><created>2011-10-09</created><updated>2012-12-10</updated><authors><author><keyname>Angrishi</keyname><forenames>Kishore</forenames></author><author><keyname>Vettukadu</keyname><forenames>Sujaritha</forenames></author><author><keyname>Killat</keyname><forenames>Ulrich</forenames></author></authors><title>A Simple Proof of Linear Scaling of End-to-End Probabilistic Bounds
  using Network Calculus</title><categories>cs.NI</categories><comments>The proof is flawed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical network calculus is the probabilistic extension of network
calculus, which uses a simple envelope approach to describe arrival traffic and
service available for the arrival traffic in a node. One of the key features of
network calculus is the possibility to describe the service available in a
network using a network service envelope constructed from the service envelopes
of the individual nodes constituting the network. It have been shown that the
end-to-end worst case performance measures computed using the network service
envelope is bounded by $ {\cal O} (H) $, where $H$ is the number of nodes
traversed by a flow. There have been many attempts to achieve a similar linear
scaling for end-to-end probabilistic performance measures but with limited
success. In this paper, we present a simple general proof of computing
end-to-end probabilistic performance measures using network calculus that grow
linearly in the number of nodes ($H$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1802</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1802</id><created>2011-10-09</created><updated>2012-01-01</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>World Shares of Publications of the USA, EU-27, and China Compared and
  Predicted using the New Interface of the Web-of-Science versus Scopus</title><categories>cs.DL physics.soc-ph</categories><comments>The paper is forthcoming in El Profesional de la Informacion 21(1)
  (2012), pp. 27-33</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new interface of the Web of Science (of Thomson Reuters) enables users to
retrieve sets larger than 100,000 documents in a single search. This makes it
possible to compare publication trends for China, the USA, EU-27, and a number
of smaller countries. China no longer grew exponentially during the 2000s, but
linearly. Contrary to previous predictions on the basis of exponential growth
or Scopus data, the cross-over of the lines for China and the USA is postponed
to the next decade (after 2020) according to this data. These long
extrapolations, however, should be used only as indicators and not as
predictions. Along with the dynamics in the publication trends, one also has to
take into account the dynamics of the databases used for the measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1804</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1804</id><created>2011-10-09</created><updated>2012-08-25</updated><authors><author><keyname>Pang</keyname><forenames>Zhi-Feng</forenames></author><author><keyname>Wang</keyname><forenames>Li-Lian</forenames></author><author><keyname>Yang</keyname><forenames>Yu-Fei</forenames></author></authors><title>The proximal point method for a hybrid model in image restoration</title><categories>cs.CV cs.IT math.IT math.OC</categories><comments>Since we find that there are some unsuitale errors, I withdraw this
  paper from this website!</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Models including two $L^1$ -norm terms have been widely used in image
restoration. In this paper we first propose the alternating direction method of
multipliers (ADMM) to solve this class of models. Based on ADMM, we then
propose the proximal point method (PPM), which is more efficient than ADMM.
Following the operator theory, we also give the convergence analysis of the
proposed methods. Furthermore, we use the proposed methods to solve a class of
hybrid models combining the ROF model with the LLT model. Some numerical
results demonstrate the viability and efficiency of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1821</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1821</id><created>2011-10-09</created><updated>2011-10-16</updated><authors><author><keyname>Mertens</keyname><forenames>Stephan</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>The complexity of the fermionant, and immanants of constant width</title><categories>cs.CC cond-mat.str-el math.CO</categories><comments>7 pages, 1 figure</comments><journal-ref>Theory of Computing 9 (2013) 273</journal-ref><doi>10.4086/toc.2013.v009a006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of statistical physics, Chandrasekharan and Wiese recently
introduced the \emph{fermionant} $\Ferm_k$, a determinant-like quantity where
each permutation $\pi$ is weighted by $-k$ raised to the number of cycles in
$\pi$. We show that computing $\Ferm_k$ is #P-hard under Turing reductions for
any constant $k &gt; 2$, and is $\oplusP$-hard for $k=2$, even for the adjacency
matrices of planar graphs. As a consequence, unless the polynomial hierarchy
collapses, it is impossible to compute the immanant $\Imm_\lambda \,A$ as a
function of the Young diagram $\lambda$ in polynomial time, even if the width
of $\lambda$ is restricted to be at most 2. In particular, if $\Ferm_2$ is in
P, or if $\Imm_\lambda$ is in P for all $\lambda$ of width 2, then $\NP
\subseteq \RP$ and there are randomized polynomial-time algorithms for
NP-complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1842</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1842</id><created>2011-10-09</created><updated>2011-11-27</updated><authors><author><keyname>Ar&#xe9;valo</keyname><forenames>Sergio</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Imbs</keyname><forenames>Damien</forenames></author><author><keyname>Jim&#xe9;nez</keyname><forenames>Ernesto</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author></authors><title>Failure Detectors in Homonymous Distributed Systems (with an Application
  to Consensus)</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the consensus problem in homonymous distributed systems
where processes are prone to crash failures and have no initial knowledge of
the system membership (&quot;homonymous&quot; means that several processes may have the
same identifier). New classes of failure detectors suited to these systems are
first defined. Among them, the classes H\Omega\ and H\Sigma\ are introduced
that are the homonymous counterparts of the classes \Omega\ and \Sigma,
respectively. (Recall that the pair &lt;\Omega,\Sigma&gt; defines the weakest failure
detector to solve consensus.) Then, the paper shows how H\Omega\ and H\Sigma\
can be implemented in homonymous systems without membership knowledge (under
different synchrony requirements). Finally, two algorithms are presented that
use these failure detectors to solve consensus in homonymous asynchronous
systems where there is no initial knowledge of the membership. One algorithm
solves consensus with &lt;H\Omega,H\Sigma&gt;, while the other uses only H\Omega, but
needs a majority of correct processes.
  Observe that the systems with unique identifiers and anonymous systems are
extreme cases of homonymous systems from which follows that all these results
also apply to these systems. Interestingly, the new failure detector class
H\Omega\ can be implemented with partial synchrony, while the analogous class
A\Omega\ defined for anonymous systems can not be implemented (even in
synchronous systems). Hence, the paper provides us with the first proof showing
that consensus can be solved in anonymous systems with only partial synchrony
(and a majority of correct processes).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1848</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1848</id><created>2011-10-09</created><authors><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Herbrand Consistency of Some Finite Fragments of Bounded Arithmetical
  Theories</title><categories>math.LO cs.LO</categories><comments>14 pages</comments><msc-class>03F40, 03F25, 03F30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize the notion of Herbrand Consistency in an appropriate way for
bounded arithmetics, and show the existence of a finite fragment of ${\rm
I\Delta_0}$ whose Herbrand Consistency is not provable in the thoery ${\rm
I\Delta_0}$. We also show the existence of an ${\rm I\Delta_0}-$derivable
$\Pi_1-$sentence such that ${\rm I\Delta_0}$ cannot prove its Herbrand
Consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1851</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1851</id><created>2011-10-09</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author><author><keyname>Tamassia</keyname><forenames>Roberto</forenames></author></authors><title>Oblivious Storage with Low I/O Overhead</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study oblivious storage (OS), a natural way to model privacy-preserving
data outsourcing where a client, Alice, stores sensitive data at an
honest-but-curious server, Bob. We show that Alice can hide both the content of
her data and the pattern in which she accesses her data, with high probability,
using a method that achieves O(1) amortized rounds of communication between her
and Bob for each data access. We assume that Alice and Bob exchange small
messages, of size $O(N^{1/c})$, for some constant $c\ge2$, in a single round,
where $N$ is the size of the data set that Alice is storing with Bob. We also
assume that Alice has a private memory of size $2N^{1/c}$. These assumptions
model real-world cloud storage scenarios, where trade-offs occur between
latency, bandwidth, and the size of the client's private memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1862</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1862</id><created>2011-10-09</created><authors><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Indices of Power in Optimal IDS Default Configuration: Theory and
  Examples</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion Detection Systems (IDSs) are becoming essential to protecting
modern information infrastructures. The effectiveness of an IDS is directly
related to the computational resources at its disposal. However, it is
difficult to guarantee especially with an increasing demand of network capacity
and rapid proliferation of attacks. On the other hand, modern intrusions often
come as sequences of attacks to reach some predefined goals. It is therefore
critical to identify the best default IDS configuration to attain the highest
possible overall protection within a given resource budget. This paper proposes
a game theory based solution to the problem of optimal signature-based IDS
configuration under resource constraints. We apply the concepts of indices of
power, namely, Shapley value and Banzhaf-Coleman index, from cooperative game
theory to quantify the influence or contribution of libraries in an IDS with
respect to given attack graphs. Such valuations take into consideration the
knowledge on common attack graphs and experienced system attacks and are used
to configure an IDS optimally at its default state by solving a knapsack
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1864</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1864</id><created>2011-10-09</created><updated>2013-11-27</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author></authors><title>Universal computably enumerable sets and initial segment prefix-free
  complexity</title><categories>math.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are Turing complete computably enumerable sets of
arbitrarily low non-trivial initial segment prefix-free complexity. In
particular, given any computably enumerable set $A$ with non-trivial
prefix-free initial segment complexity, there exists a Turing complete
computably enumerable set $B$ with complexity strictly less than the complexity
of $A$. On the other hand it is known that sets with trivial initial segment
prefix-free complexity are not Turing complete.
  Moreover we give a generalization of this result for any finite collection of
computably enumerable sets $A_i, i&lt;k$ with non-trivial initial segment
prefix-free complexity. An application of this gives a negative answer to a
question from \cite[Section 11.12]{rodenisbook} and \cite{MRmerstcdhdtd} which
asked for minimal pairs in the structure of the c.e.\ reals ordered by their
initial segment prefix-free complexity.
  Further consequences concern various notions of degrees of randomness. For
example, the Solovay degrees and the $K$-degrees of computably enumerable reals
and computably enumerable sets are not elementarily equivalent. Also, the
degrees of randomness based on plain and prefix-free complexity are not
elementarily equivalent; the same holds for their $\Delta^0_2$ and $\Sigma^0_1$
substructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1866</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1866</id><created>2011-10-09</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Putting Instruction Sequences into Effect</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attempt is made to define the concept of execution of an instruction
sequence. It is found to be a special case of directly putting into effect of
an instruction sequence. Directly putting into effect of an instruction
sequences comprises interpretation as well as execution. Directly putting into
effect is a special case of putting into effect with other special cases
classified as indirectly putting into effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1884</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1884</id><created>2011-10-09</created><authors><author><keyname>Iribarren</keyname><forenames>Jos&#xe9; Luis</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author></authors><title>Branching Dynamics of Viral Information Spreading</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 9 figures. Accepted in Physical Review E</comments><doi>10.1103/PhysRevE.84.046116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its importance for rumors or innovations propagation, peer-to-peer
collaboration, social networking or Marketing, the dynamics of information
spreading is not well understood. Since the diffusion depends on the
heterogeneous patterns of human behavior and is driven by the participants'
decisions, its propagation dynamics shows surprising properties not explained
by traditional epidemic or contagion models. Here we present a detailed
analysis of our study of real Viral Marketing campaigns where tracking the
propagation of a controlled message allowed us to analyze the structure and
dynamics of a diffusion graph involving over 31,000 individuals. We found that
information spreading displays a non-Markovian branching dynamics that can be
modeled by a two-step Bellman-Harris Branching Process that generalizes the
static models known in the literature and incorporates the high variability of
human behavior. It explains accurately all the features of information
propagation under the &quot;tipping-point&quot; and can be used for prediction and
management of viral information spreading processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1891</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1891</id><created>2011-10-09</created><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author><author><keyname>Luo</keyname><forenames>Jie</forenames></author></authors><title>Channel Coding in Random Access Communication over Compound Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the short and bursty incoming messages, channel access activities in a
wireless random access system are often fractional. The lack of frequent data
support consequently makes it difficult for the receiver to estimate and track
the time varying channel states with high precision. This paper investigates
random multiple access communication over a compound wireless channel where
channel realization is known neither at the transmitters nor at the receiver.
An achievable rate and error probability tradeoff bound is derived under the
non-asymptotic assumption of a finite codeword length. The results are then
extended to the random multiple access system where the receiver is only
interested in decoding messages from a user subset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1892</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1892</id><created>2011-10-09</created><updated>2015-07-05</updated><authors><author><keyname>Rossi</keyname><forenames>Roberto</forenames></author><author><keyname>Hnich</keyname><forenames>Brahim</forenames></author><author><keyname>Tarim</keyname><forenames>S. Armagan</forenames></author><author><keyname>Prestwich</keyname><forenames>Steven</forenames></author></authors><title>Confidence-based Reasoning in Stochastic Constraint Programming</title><categories>math.OC cs.AI math.CO math.PR stat.OT</categories><comments>53 pages, working draft</comments><journal-ref>Artificial Intelligence, Elsevier, 228(1):129-152, 2015</journal-ref><doi>10.1016/j.artint.2015.07.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce a novel approach, based on sampling, for finding
assignments that are likely to be solutions to stochastic constraint
satisfaction problems and constraint optimisation problems. Our approach
reduces the size of the original problem being analysed; by solving this
reduced problem, with a given confidence probability, we obtain assignments
that satisfy the chance constraints in the original model within prescribed
error tolerance thresholds. To achieve this, we blend concepts from stochastic
constraint programming and statistics. We discuss both exact and approximate
variants of our method. The framework we introduce can be immediately employed
in concert with existing approaches for solving stochastic constraint programs.
A thorough computational study on a number of stochastic combinatorial
optimisation problems demonstrates the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1894</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1894</id><created>2011-10-09</created><authors><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Siminelakis</keyname><forenames>Paris</forenames></author></authors><title>On the Efficiency of Influence-and-Exploit Strategies for Revenue
  Maximization under Positive Externalities</title><categories>cs.DS cs.CC cs.SI</categories><comments>21 pages, 5 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of revenue maximization in the marketing model for
social networks introduced by (Hartline, Mirrokni, Sundararajan, WWW '08). We
restrict our attention to the Uniform Additive Model and mostly focus on
Influence-and-Exploit (IE) marketing strategies. We obtain a comprehensive
collection of results on the efficiency and the approximability of IE
strategies, which also imply a significant improvement on the best known
approximation ratios for revenue maximization. Specifically, we show that in
the Uniform Additive Model, both computing the optimal marketing strategy and
computing the best IE strategy are $\NP$-hard for undirected social networks.
We observe that allowing IE strategies to offer prices smaller than the myopic
price in the exploit step leads to a measurable improvement on their
performance. Thus, we show that the best IE strategy approximates the maximum
revenue within a factor of 0.911 for undirected and of roughly 0.553 for
directed networks. Moreover, we present a natural generalization of IE
strategies, with more than two pricing classes, and show that they approximate
the maximum revenue within a factor of roughly 0.7 for undirected and of
roughly 0.35 for directed networks. Utilizing a connection between good IE
strategies and large cuts in the underlying social network, we obtain
polynomial-time algorithms that approximate the revenue of the best IE strategy
within a factor of roughly 0.9. Hence, we significantly improve on the best
known approximation ratio for revenue maximization to 0.8229 for undirected and
to 0.5011 for directed networks (from 2/3 and 1/3, respectively, by Hartline et
al.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1896</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1896</id><created>2011-10-09</created><authors><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>Restricted Parameter Range Promise Set Cover Problems Are Easy</title><categories>cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $({\bf U},{\bf S},d)$ be an instance of Set Cover Problem, where ${\bf
U}=\{u_1,...,u_n\}$ is a $n$ element ground set, ${\bf S}=\{S_1,...,S_m\}$ is a
set of $m$ subsets of ${\bf U}$ satisfying $\bigcup_{i=1}^m S_i={\bf U}$ and
$d$ is a positive integer. In STOC 1993 M. Bellare, S. Goldwasser, C. Lund and
A. Russell proved the NP-hardness to distinguish the following two cases of
${\bf GapSetCover_{\eta}}$ for any constant $\eta &gt; 1$. The Yes case is the
instance for which there is an exact cover of size $d$ and the No case is the
instance for which any cover of ${\bf U}$ from ${\bf S}$ has size at least
$\eta d$. This was improved by R. Raz and S. Safra in STOC 1997 about the
NP-hardness for ${\bf GapSetCover}_{clogm}$ for some constant $c$. In this
paper we prove that restricted parameter range subproblem is easy. For any
given function of $n$ satisfying $\eta(n) \geq 1$, we give a polynomial time
algorithm not depending on $\eta(n)$ to distinguish between
  {\bf YES:} The instance $({\bf U},{\bf S}, d)$ where $d&gt;\frac{2 |{\bf
S}|}{3\eta(n)-1}$, for which there exists an exact cover of size at most $d$;
  {\bf NO:} The instance $({\bf U},{\bf S}, d)$ where $d&gt;\frac{2 |{\bf
S}|}{3\eta(n)-1}$, for which any cover from ${\bf S}$ has size larger than
$\eta(n) d$.
  The polynomial time reduction of this restricted parameter range set cover
problem is constructed by using the lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1914</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1914</id><created>2011-10-09</created><authors><author><keyname>Zhi-Yun</keyname><forenames>Zou</forenames></author><author><keyname>Peng</keyname><forenames>Liu</forenames></author><author><keyname>Li</keyname><forenames>Lei</forenames></author><author><keyname>Jian-Zhi</keyname><forenames>Gao</forenames></author></authors><title>An evolving network model with modular growth</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an evolving network model growing fast in units of
module, based on the analysis of the evolution characteristics in real complex
networks. Each module is a small-world network containing several
interconnected nodes, and the nodes between the modules are linked by
preferential attachment on degree of nodes. We study the modularity measure of
the proposed model, which can be adjusted by changing ratio of the number of
inner-module edges and the number of inter-module edges. Based on the mean
field theory, we develop an analytical function of the degree distribution,
which is verified by a numerical example and indicates that the degree
distribution shows characteristics of the small-world network and the
scale-free network distinctly at different segments. The clustering coefficient
and the average path length of the network are simulated numerically,
indicating that the network shows the small-world property and is affected
little by the randomness of the new module.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1915</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1915</id><created>2011-10-09</created><authors><author><keyname>Chen</keyname><forenames>Lily</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Lian</keyname><forenames>Huishu</forenames></author></authors><title>Further hardness results on the rainbow vertex-connection number of
  graphs</title><categories>math.CO cs.CC</categories><comments>10 pages</comments><msc-class>05C15, 05C40, 68Q17, 68Q25, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vertex-colored graph $G$ is {\it rainbow vertex-connected} if any pair of
vertices in $G$ are connected by a path whose internal vertices have distinct
colors, which was introduced by Krivelevich and Yuster. The {\it rainbow
vertex-connection number} of a connected graph $G$, denoted by $rvc(G)$, is the
smallest number of colors that are needed in order to make $G$ rainbow
vertex-connected. In a previous paper we showed that it is NP-Complete to
decide whether a given graph $G$ has $rvc(G)=2$. In this paper we show that for
every integer $k\geq 2$, deciding whether $rvc(G)\leq k$ is NP-Hard. We also
show that for any fixed integer $k\geq 2$, this problem belongs to NP-class,
and so it becomes NP-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1928</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1928</id><created>2011-10-10</created><authors><author><keyname>Buch</keyname><forenames>Dhara Hitarth</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>Prevention of Wormhole Attack in Wireless Sensor Network</title><categories>cs.CR</categories><comments>14 pages; International Journal of Network Security &amp; Its
  Applications Year: 2011 Vol: 3 Issue: 5</comments><doi>10.5121/ijnsa.2011.3507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous and pervasive applications, where the Wireless Sensor Networks are
typically deployed, lead to the susceptibility to many kinds of security
attacks. Sensors used for real time response capability also make it difficult
to devise the resource intensive security protocols because of their limited
battery, power, memory and processing capabilities. One of potent form of
Denial of Service attacks is Wormhole attack that affects on the network layer.
In this paper, the techniques dealing with wormhole attack are investigated and
an approach for wormhole prevention is proposed. Our approach is based on the
analysis of the two-hop neighbors forwarding Route Reply packet. To check the
validity of the sender, a unique key between the individual sensor node and the
base station is required to be generated by suitable scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1930</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1930</id><created>2011-10-10</created><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>Statistical Mechanical Analysis of Low-Density Parity-Check Codes on
  General Markov Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure. Submitted to SITA2011, which is a domestic
  conference in Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) codes on symmetric memoryless channels have
been analyzed using statistical physics by several authors. In this paper,
statistical mechanical analysis of LDPC codes is performed for asymmetric
memoryless channels and general Markov channels. It is shown that the saddle
point equations of the replica symmetric solution for a Markov channel is
equivalent to the density evolution of the belief propagation on the factor
graph representing LDPC codes on the Markov channel. The derivation uses the
method of types for Markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1957</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1957</id><created>2011-10-10</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Delen</keyname><forenames>G. P. A. J.</forenames></author><author><keyname>van Vlijmen</keyname><forenames>S. F. M.</forenames></author></authors><title>Stratified Outsourcing Theory</title><categories>cs.SE</categories><acm-class>K.6.0; J.4; H.4.0; D.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The terminology of sourcing, outsourcing and insourcing is developed in
detail on the basis of the preliminary definitions of outsourcing and
insourcing and related activities and competences as given in our three
previous papers on business mereology, on the concept of a sourcement, and on
outsourcing competence respectively.
  Besides providing more a detailed semantic analysis we will introduce,
explain, and illustrate a number of additional concepts including: principal
unit of a sourcement, theme of a sourcement, current sourcement, (un)stable
sourcement, and sourcement transformation.
  A three level terminology is designed: (i) factual level: operational facts
that hold for sourcements including histories thereof, (ii) business level:
roles and objectives of various parts of the factual level description, thus
explaining each partner's business process and business objectives, (iii)
contract level: specification of intended facts and intended business models as
found at the business level. Orthogonal to these three conceptual levels, are
four temporal aspects: history, now (actuality), transformation, and
transition.
  A detailed description of the well-known range of sourcement transformations
is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1964</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1964</id><created>2011-10-10</created><authors><author><keyname>Kowalik</keyname><forenames>Lukasz</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Suchan</keyname><forenames>Karol</forenames></author></authors><title>Towards optimal kernel for connected vertex cover in planar graphs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of the connected version of the vertex
cover problem, where the solution set has to induce a connected subgraph.
Although this problem does not admit a polynomial kernel for general graphs
(unless NP is a subset of coNP/poly), for planar graphs Guo and Niedermeier
[ICALP'08] showed a kernel with at most 14k vertices, subsequently improved by
Wang et al. [MFCS'11] to 4k. The constant 4 here is so small that a natural
question arises: could it be already an optimal value for this problem? In this
paper we answer this quesion in negative: we show a (11/3)k-vertex kernel for
Connected Vertex Cover in planar graphs. We believe that this result will
motivate further study in search for an optimal kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1976</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1976</id><created>2011-10-10</created><authors><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author><author><keyname>Guo</keyname><forenames>Jia-Feng</forenames></author></authors><title>Exploring the structural regularities in networks</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 5 figures</comments><doi>10.1103/PhysRevE.84.056111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of exploring structural regularities
of networks by dividing the nodes of a network into groups such that the
members of each group have similar patterns of connections to other groups.
Specifically, we propose a general statistical model to describe network
structure. In this model, group is viewed as hidden or unobserved quantity and
it is learned by fitting the observed network data using the
expectation-maximization algorithm. Compared with existing models, the most
prominent strength of our model is the high flexibility. This strength enables
it to possess the advantages of existing models and overcomes their
shortcomings in a unified way. As a result, not only broad types of structure
can be detected without prior knowledge of what type of intrinsic regularities
exist in the network, but also the type of identified structure can be directly
learned from data. Moreover, by differentiating outgoing edges from incoming
edges, our model can detect several types of structural regularities beyond
competing models. Tests on a number of real world and artificial networks
demonstrate that our model outperforms the state-of-the-art model at shedding
light on the structural features of networks, including the overlapping
community structure, multipartite structure and several other types of
structure which are beyond the capability of existing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1980</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1980</id><created>2011-10-10</created><updated>2013-01-11</updated><authors><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Ventre</keyname><forenames>Carmine</forenames></author></authors><title>Using Lotteries to Approximate the Optimal Revenue</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much recent work on the revenue-raising properties of truthful
mechanisms for selling goods to selfish bidders. Typically the revenue of a
mechanism is compared against a benchmark (such as, the maximum revenue
obtainable by an omniscient seller selling at a fixed price to at least two
customers), with a view to understanding how much lower the mechanism's revenue
is than the benchmark, in the worst case.
  We study this issue in the context of {\em lotteries}, where the seller may
sell a probability of winning an item. We are interested in two general issues.
Firstly, we aim at using the true optimum revenue as benchmark for our
auctions. Secondly, we study the extent to which the expressive power resulting
from lotteries, helps to improve the worst-case ratio. We study this in the
well-known context of {\em digital goods}, where the production cost is zero.
We show that in this scenario, collusion-resistant lotteries (these are
lotteries for which no coalition of bidders exchanging side payments has an
advantage in lying) are as powerful as truthful ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1990</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1990</id><created>2011-10-10</created><updated>2012-06-07</updated><authors><author><keyname>Isheden</keyname><forenames>Christian</forenames></author><author><keyname>Chong</keyname><forenames>Zhijiat</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Framework for Link-Level Energy Efficiency Optimization with Informed
  Transmitter</title><categories>cs.IT math.IT</categories><doi>10.1109/TWC.2012.12.111829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dramatic increase of network infrastructure comes at the cost of rapidly
increasing energy consumption, which makes optimization of energy efficiency
(EE) an important topic. Since EE is often modeled as the ratio of rate to
power, we present a mathematical framework called fractional programming that
provides insight into this class of optimization problems, as well as
algorithms for computing the solution. The main idea is that the objective
function is transformed to a weighted sum of rate and power. A generic problem
formulation for systems dissipating transmit-independent circuit power in
addition to transmit-dependent power is presented. We show that a broad class
of EE maximization problems can be solved efficiently, provided the rate is a
concave function of the transmit power. We elaborate examples of various system
models including time-varying parallel channels. Rate functions with an
arbitrary discrete modulation scheme are also treated. The examples considered
lead to water-filling solutions, but these are different from the dual problems
of power minimization under rate constraints and rate maximization under power
constraints, respectively, because the constraints need not be active. We also
demonstrate that if the solution to a rate maximization problem is known, it
can be utilized to reduce the EE problem into a one-dimensional convex problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1991</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1991</id><created>2011-10-10</created><authors><author><keyname>Payli</keyname><forenames>Resat Umit</forenames></author><author><keyname>Erciyes</keyname><forenames>Kayhan</forenames></author><author><keyname>Dagdeviren</keyname><forenames>Orhan</forenames></author></authors><title>Cluster-Based Load Balancing Algorithms for Grids</title><categories>cs.DC</categories><comments>17 pages, 11 figures; International Journal of Computer Networks,
  volume3, number 5, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-science applications may require huge amounts of data and high processing
power where grid infrastructures are very suitable for meeting these
requirements. The load distribution in a grid may vary leading to the
bottlenecks and overloaded sites. We describe a hierarchical dynamic load
balancing protocol for Grids. The Grid consists of clusters and each cluster is
represented by a coordinator. Each coordinator first attempts to balance the
load in its cluster and if this fails, communicates with the other coordinators
to perform transfer or reception of load. This process is repeated
periodically. We analyze the correctness, performance and scalability of the
proposed protocol and show from the simulation results that our algorithm
balances the load by decreasing the number of high loaded nodes in a grid
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.1992</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.1992</id><created>2011-10-10</created><authors><author><keyname>Constantinou</keyname><forenames>Eleni</forenames></author><author><keyname>Kakarontzas</keyname><forenames>George</forenames></author><author><keyname>Stamelos</keyname><forenames>Ioannis</forenames></author></authors><title>Open Source Software: How Can Design Metrics Facilitate Architecture
  Recovery?</title><categories>cs.SE cs.AI</categories><comments>4th Workshop on Intelligent Techniques in Software Engineering, 5
  September 2011 at the European Conference on Machine Learning and Principles
  and Practices of Knowledge Discovery in Databases (ECML-PKDD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern software development methodologies include reuse of open source code.
Reuse can be facilitated by architectural knowledge of the software, not
necessarily provided in the documentation of open source software. The effort
required to comprehend the system's source code and discover its architecture
can be considered a major drawback in reuse. In a recent study we examined the
correlations between design metrics and classes' architecture layer. In this
paper, we apply our methodology in more open source projects to verify the
applicability of our method. Keywords: system understanding; program
comprehension; object-oriented; reuse; architecture layer; design metrics;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2049</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2049</id><created>2011-10-10</created><authors><author><keyname>Kucerova</keyname><forenames>A.</forenames></author><author><keyname>Sykora</keyname><forenames>J.</forenames></author><author><keyname>Rosic</keyname><forenames>B.</forenames></author><author><keyname>Matthies</keyname><forenames>H. G.</forenames></author></authors><title>Acceleration of Uncertainty Updating in the Description of Transport
  Processes in Heterogeneous Materials</title><categories>cs.CE</categories><journal-ref>Journal of Computational and Applied Mathematics, 236(18),
  4862-4872, 2012</journal-ref><doi>10.1016/j.cam.2012.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prediction of thermo-mechanical behaviour of heterogeneous materials such
as heat and moisture transport is strongly influenced by the uncertainty in
parameters. Such materials occur e.g. in historic buildings, and the durability
assessment of these therefore needs a reliable and probabilistic simulation of
transport processes, which is related to the suitable identification of
material parameters. In order to include expert knowledge as well as
experimental results, one can employ an updating procedure such as Bayesian
inference. The classical probabilistic setting of the identification process in
Bayes's form requires the solution of a stochastic forward problem via
computationally expensive sampling techniques, which makes the method almost
impractical. In this paper novel stochastic computational techniques such as
the stochastic Galerkin method are applied in order to accelerate the updating
procedure. The idea is to replace the computationally expensive forward
simulation via the conventional finite element (FE) method by the evaluation of
a polynomial chaos expansion (PCE). Such an approximation of the FE model for
the forward simulation perfectly suits the Bayesian updating. The presented
uncertainty updating techniques are applied to the numerical model of coupled
heat and moisture transport in heterogeneous materials with spatially varying
coefficients defined by random fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2053</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2053</id><created>2011-10-10</created><updated>2012-08-31</updated><authors><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Steps Towards a Theory of Visual Information: Active Perception,
  Signal-to-Symbol Conversion and the Interplay Between Sensing and Control</title><categories>cs.CV</categories><comments>151 pages; preliminary version TR UCLA-CSD100028 of September 13,
  20010</comments><report-no>UCLA CSD100028</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript describes the elements of a theory of information tailored to
control and decision tasks and specifically to visual data. The concept of
Actionable Information is described, that relates to a notion of information
championed by J. Gibson, and a notion of &quot;complete information&quot; that relates to
the minimal sufficient statistics of a complete representation. It is shown
that the &quot;actionable information gap&quot; between the two can be reduced by
exercising control on the sensing process. Thus, senging, control and
information are inextricably tied. This has consequences in the so-called
&quot;signal-to-symbol barrier&quot; problem, as well as in the analysis and design of
active sensing systems. It has ramifications in vision-based control,
navigation, 3-D reconstruction and rendering, as well as detection,
localization, recognition and categorization of objects and scenes in live
video.
  This manuscript has been developed from a set of lecture notes for a summer
course at the First International Computer Vision Summer School (ICVSS) in
Scicli, Italy, in July of 2008. They were later expanded and amended for
subsequent lectures in the same School in July 2009. Starting on November 1,
2009, they were further expanded for a special topics course, CS269, taught at
UCLA in the Spring term of 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2055</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2055</id><created>2011-10-10</created><authors><author><keyname>Sykora</keyname><forenames>J.</forenames></author><author><keyname>Krejci</keyname><forenames>T.</forenames></author><author><keyname>Kruis</keyname><forenames>J.</forenames></author><author><keyname>Sejnoha</keyname><forenames>M.</forenames></author></authors><title>Computational homogenization of non-stationary transport processes in
  masonry structures</title><categories>cs.CE physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fully coupled transient heat and moisture transport in a masonry structure
is examined in this paper. Supported by several successful applications in
civil engineering the nonlinear diffusion model proposed by K\&quot;{u}nzel is
adopted in the present study. A strong material heterogeneity together with a
significant dependence of the model parameters on initial conditions as well as
the gradients of heat and moisture fields vindicates the use of a hierarchical
modeling strategy to solve the problem of this kind. Attention is limited to
the classical first order homogenization in a spatial domain developed here in
the framework of a two step (meso-macro) multi-scale computational scheme (FE^2
problem). Several illustrative examples are presented to investigate the
influence of transient flow at the level of constituents (meso-scale) on the
macroscopic response including the effect of macro-scale boundary conditions. A
two-dimensional section of Charles Bridge subjected to actual climatic
conditions is analyzed next to confirm the suitability of algorithmic format of
FE^2 scheme for the parallel computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2065</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2065</id><created>2011-10-10</created><updated>2012-04-27</updated><authors><author><keyname>Krishnamurthy</keyname><forenames>Supriya</forenames></author><author><keyname>Sumedha</keyname></author></authors><title>On the behaviour of random K-SAT on trees</title><categories>cond-mat.stat-mech cs.CC</categories><comments>22 pages, 5 figures,accepted for publication in J. Stat. Mech</comments><journal-ref>J. Stat. Mech. (2012) P05009</journal-ref><doi>10.1088/1742-5468/2012/05/P05009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the K-satisfiability problem on a regular d-ary rooted tree. For
this model, we demonstrate how we can calculate in closed form, the moments of
the total number of solutions as a function of d and K, where the average is
over all realizations, for a fixed assignment of the surface variables. We find
that different moments pick out different 'critical' values of d, below which
they diverge as the total number of variables on the tree goes to infinity and
above which they decay. We show that K-SAT on the random graph also behaves
similarly. We also calculate exactly the fraction of instances that have
solutions for all K. On the tree, this quantity decays to 0 (as the number of
variables increases) for any d&gt;1. However the recursion relations for this
quantity have a non-trivial fixed-point solution which indicates the existence
of a different transition in the interior of an infinite rooted tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2074</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2074</id><created>2011-10-10</created><authors><author><keyname>Klimo</keyname><forenames>Martin</forenames></author><author><keyname>Such</keyname><forenames>Ondrej</forenames></author></authors><title>Memristors can implement fuzzy logic</title><categories>cs.ET cond-mat.mtrl-sci cs.NE cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we propose implementing fuzzy logic using memristors. Min and max
operations are done by antipodally configured memristor circuits that may be
assembled into computational circuits. We discuss computational power of such
circuits with respect to m-efficiency and experimentally observed behavior of
memristive devices. Circuits implemented with real devices are likely to
manifest learning behavior. The circuits presented in the work may be
applicable for instance in fuzzy classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2096</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2096</id><created>2011-10-10</created><updated>2012-04-03</updated><authors><author><keyname>Herrmann</keyname><forenames>Philipp N.</forenames></author><author><keyname>Kundisch</keyname><forenames>Dennis O.</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad S.</forenames></author></authors><title>Beating Irrationality: Does Delegating to IT Alleviate the Sunk Cost
  Effect?</title><categories>cs.HC cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, we investigate the impact of delegating decision making to
information technology (IT) on an important human decision bias - the sunk cost
effect. To address our research question, we use a unique and very rich dataset
containing actual market transaction data for approximately 7,000 pay-per-bid
auctions. Thus, unlike previous studies that are primarily laboratory
experiments, we investigate the effects of using IT on the proneness to a
decision bias in real market transactions. We identify and analyze irrational
decision scenarios of auction participants. We find that participants with a
higher monetary investment have an increased likelihood of violating the
assumption of rationality, due to the sunk cost effect. Interestingly, after
controlling for monetary investments, participants who delegate their decision
making to IT and, consequently, have comparably lower behavioral investments
(e.g., emotional attachment, effort, time) are less prone to the sunk cost
effect. In particular, delegation to IT reduces the impact of overall
investments on the sunk cost effect by approximately 50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2098</identifier>
 <datestamp>2012-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2098</id><created>2011-10-10</created><updated>2012-08-04</updated><authors><author><keyname>Sun</keyname><forenames>John Z.</forenames></author><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>Subbian</keyname><forenames>Karthik</forenames></author></authors><title>Dynamic Matrix Factorization: A State Space Approach</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix factorization from a small number of observed entries has recently
garnered much attention as the key ingredient of successful recommendation
systems. One unresolved problem in this area is how to adapt current methods to
handle changing user preferences over time. Recent proposals to address this
issue are heuristic in nature and do not fully exploit the time-dependent
structure of the problem. As a principled and general temporal formulation, we
propose a dynamical state space model of matrix factorization. Our proposal
builds upon probabilistic matrix factorization, a Bayesian model with Gaussian
priors. We utilize results in state tracking, such as the Kalman filter, to
provide accurate recommendations in the presence of both process and
measurement noise. We show how system parameters can be learned via
expectation-maximization and provide comparisons to current published
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2136</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2136</id><created>2011-10-10</created><updated>2012-06-20</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Begleiter</keyname><forenames>Ron</forenames></author><author><keyname>Ezra</keyname><forenames>Esther</forenames></author></authors><title>Active Learning Using Smooth Relative Regret Approximations with
  Applications</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The disagreement coefficient of Hanneke has become a central data independent
invariant in proving active learning rates. It has been shown in various ways
that a concept class with low complexity together with a bound on the
disagreement coefficient at an optimal solution allows active learning rates
that are superior to passive learning ones.
  We present a different tool for pool based active learning which follows from
the existence of a certain uniform version of low disagreement coefficient, but
is not equivalent to it. In fact, we present two fundamental active learning
problems of significant interest for which our approach allows nontrivial
active learning bounds. However, any general purpose method relying on the
disagreement coefficient bounds only fails to guarantee any useful bounds for
these problems.
  The tool we use is based on the learner's ability to compute an estimator of
the difference between the loss of any hypotheses and some fixed &quot;pivotal&quot;
hypothesis to within an absolute error of at most $\eps$ times the
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2153</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2153</id><created>2011-10-10</created><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Characterizing and modeling citation dynamics</title><categories>physics.soc-ph cs.DL cs.SI physics.data-an</categories><comments>8 pages, 5 figures</comments><journal-ref>PLoS ONE 6(9): e24926 (2011)</journal-ref><doi>10.1371/journal.pone.0024926</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation distributions are crucial for the analysis and modeling of the
activity of scientists. We investigated bibliometric data of papers published
in journals of the American Physical Society, searching for the type of
function which best describes the observed citation distributions. We used the
goodness of fit with Kolmogorov-Smirnov statistics for three classes of
functions: log-normal, simple power law and shifted power law. The shifted
power law turns out to be the most reliable hypothesis for all citation
networks we derived, which correspond to different time spans. We find that
citation dynamics is characterized by bursts, usually occurring within a few
years since publication of a paper, and the burst size spans several orders of
magnitude. We also investigated the microscopic mechanisms for the evolution of
citation networks, by proposing a linear preferential attachment with time
dependent initial attractiveness. The model successfully reproduces the
empirical citation distributions and accounts for the presence of citation
bursts as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2162</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2162</id><created>2011-10-10</created><updated>2011-10-13</updated><authors><author><keyname>Sipos</keyname><forenames>Ruben</forenames></author><author><keyname>Shivaswamy</keyname><forenames>Pannaga</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Large-Margin Learning of Submodular Summarization Methods</title><categories>cs.AI cs.CL cs.LG</categories><comments>update: improved formatting (figure placement) and algorithm
  pseudocode clarity (Fig. 3)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a supervised learning approach to training
submodular scoring functions for extractive multi-document summarization. By
taking a structured predicition approach, we provide a large-margin method that
directly optimizes a convex relaxation of the desired performance measure. The
learning method applies to all submodular summarization methods, and we
demonstrate its effectiveness for both pairwise as well as coverage-based
scoring functions on multiple datasets. Compared to state-of-the-art functions
that were tuned manually, our method significantly improves performance and
enables high-fidelity models with numbers of parameters well beyond what could
reasonbly be tuned by hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2186</identifier>
 <datestamp>2012-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2186</id><created>2011-10-10</created><updated>2012-01-23</updated><authors><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Diaz-Guilera</keyname><forenames>Albert</forenames></author></authors><title>Consensus in networks of mobile communicating agents</title><categories>physics.soc-ph cond-mat.stat-mech cs.MA cs.SI q-bio.PE</categories><comments>7 pages, 7 figures</comments><journal-ref>Phys. Rev. E 85, 016113 (2012)</journal-ref><doi>10.1103/PhysRevE.85.016113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Populations of mobile and communicating agents describe a vast array of
technological and natural systems, ranging from sensor networks to animal
groups. Here, we investigate how a group-level agreement may emerge in the
continuously evolving network defined by the local interactions of the moving
individuals. We adopt a general scheme of motion in two dimensions and we let
the individuals interact through the minimal naming game, a prototypical scheme
to investigate social consensus. We distinguish different regimes of
convergence determined by the emission range of the agents and by their
mobility, and we identify the corresponding scaling behaviors of the consensus
time. In the same way, we rationalize also the behavior of the maximum memory
used during the convergence process, which determines the minimum
cognitive/storage capacity needed by the individuals. Overall, we believe that
the simple and general model presented in this paper can represent a helpful
reference for a better understanding of the behavior of populations of mobile
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2196</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2196</id><created>2011-10-10</created><updated>2011-10-13</updated><authors><author><keyname>Giusti</keyname><forenames>Marc</forenames></author><author><keyname>Heintz</keyname><forenames>Joos</forenames></author><author><keyname>Kuijpers</keyname><forenames>Bart</forenames></author></authors><title>The evaluation of geometric queries: constraint databases and quantifier
  elimination</title><categories>cs.DB cs.CC cs.LO</categories><comments>This paper is representing work in progress of the authors. It is not
  aimed for publication in the present form</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model the algorithmic task of geometric elimination (e.g., quantifier
elimination in the elementary field theories of real and complex numbers) by
means of certain constraint database queries, called geometric queries. As a
particular case of such a geometric elimination task, we consider sample point
queries. We show exponential lower complexity bounds for evaluating geometric
queries in the general and in the particular case of sample point queries.
Although this paper is of theoretical nature, its aim is to explore the
possibilities and (complexity-)limits of computer implemented query evaluation
algorithms for Constraint Databases, based on the principles of the most
advanced geometric elimination procedures and their implementations, like,
e.g., the software package &quot;Kronecker&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2200</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2200</id><created>2011-10-10</created><authors><author><keyname>Fox</keyname><forenames>M.</forenames></author><author><keyname>Long</keyname><forenames>D.</forenames></author></authors><title>Modelling Mixed Discrete-Continuous Domains for Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  235-297, 2006</journal-ref><doi>10.1613/jair.2044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present pddl+, a planning domain description language for
modelling mixed discrete-continuous planning domains. We describe the syntax
and modelling style of pddl+, showing that the language makes convenient the
modelling of complex time-dependent effects. We provide a formal semantics for
pddl+ by mapping planning instances into constructs of hybrid automata. Using
the syntax of HAs as our semantic model we construct a semantic mapping to
labelled transition systems to complete the formal interpretation of pddl+
planning instances. An advantage of building a mapping from pddl+ to HA theory
is that it forms a bridge between the Planning and Real Time Systems research
communities. One consequence is that we can expect to make use of some of the
theoretical properties of HAs. For example, for a restricted class of HAs the
Reachability problem (which is equivalent to Plan Existence) is decidable.
pddl+ provides an alternative to the continuous durative action model of
pddl2.1, adding a more flexible and robust model of time-dependent behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2203</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2203</id><created>2011-10-10</created><authors><author><keyname>Yap</keyname><forenames>R. H. C.</forenames></author><author><keyname>Zhang</keyname><forenames>Y.</forenames></author></authors><title>Set Intersection and Consistency in Constraint Networks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  441-464, 2006</journal-ref><doi>10.1613/jair.2058</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that there is a close relation between consistency in
a constraint network and set intersection. A proof schema is provided as a
generic way to obtain consistency properties from properties on set
intersection. This approach not only simplifies the understanding of and
unifies many existing consistency results, but also directs the study of
consistency to that of set intersection properties in many situations, as
demonstrated by the results on the convexity and tightness of constraints in
this paper. Specifically, we identify a new class of tree convex constraints
where local consistency ensures global consistency. This generalizes row convex
constraints. Various consistency results are also obtained on constraint
networks where only some, in contrast to all in the existing work,constraints
are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2204</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2204</id><created>2011-10-10</created><authors><author><keyname>Culberson</keyname><forenames>J.</forenames></author><author><keyname>Gao</keyname><forenames>Y.</forenames></author></authors><title>Consistency and Random Constraint Satisfaction Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  517-557, 2007</journal-ref><doi>10.1613/jair.2155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the possibility of designing non-trivial random CSP
models by exploiting the intrinsic connection between structures and
typical-case hardness. We show that constraint consistency, a notion that has
been developed to improve the efficiency of CSP algorithms, is in fact the key
to the design of random CSP models that have interesting phase transition
behavior and guaranteed exponential resolution complexity without putting much
restriction on the parameter of constraint tightness or the domain size of the
problem. We propose a very flexible framework for constructing problem
instances withinteresting behavior and develop a variety of concrete methods to
construct specific random CSP models that enforce different levels of
constraint consistency. A series of experimental studies with interesting
observations are carried out to illustrate the effectiveness of introducing
structural elements in random instances, to verify the robustness of our
proposal, and to investigate features of some specific models based on our
framework that are highly related to the behavior of backtracking search
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2205</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2205</id><created>2011-10-10</created><authors><author><keyname>Pontelli</keyname><forenames>E.</forenames></author><author><keyname>Son</keyname><forenames>T. C.</forenames></author><author><keyname>Tu</keyname><forenames>P. H.</forenames></author></authors><title>Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  353-389, 2007</journal-ref><doi>10.1613/jair.2171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present two alternative approaches to defining answer sets
for logic programs with arbitrary types of abstract constraint atoms (c-atoms).
These approaches generalize the fixpoint-based and the level mapping based
answer set semantics of normal logic programs to the case of logic programs
with arbitrary types of c-atoms. The results are four different answer set
definitions which are equivalent when applied to normal logic programs. The
standard fixpoint-based semantics of logic programs is generalized in two
directions, called answer set by reduct and answer set by complement. These
definitions, which differ from each other in the treatment of
negation-as-failure (naf) atoms, make use of an immediate consequence operator
to perform answer set checking, whose definition relies on the notion of
conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other
two definitions, called strongly and weakly well-supported models, are
generalizations of the notion of well-supported models of normal logic programs
to the case of programs with c-atoms. As for the case of fixpoint-based
semantics, the difference between these two definitions is rooted in the
treatment of naf atoms. We prove that answer sets by reduct (resp. by
complement) are equivalent to weakly (resp. strongly) well-supported models of
a program, thus generalizing the theorem on the correspondence between stable
models and well-supported models of a normal logic program to the class of
programs with c-atoms. We show that the newly defined semantics coincide with
previously introduced semantics for logic programs with monotone c-atoms, and
they extend the original answer set semantics of normal logic programs. We also
study some properties of answer sets of programs with c-atoms, and relate our
definitions to several semantics for logic programs with aggregates presented
in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2207</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2207</id><created>2011-10-10</created><updated>2013-03-01</updated><authors><author><keyname>Im</keyname><forenames>Sungjin</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author><author><keyname>van der Zwaan</keyname><forenames>Ruben</forenames></author></authors><title>Minimum Latency Submodular Cover</title><categories>cs.DS</categories><comments>23 pages, 1 figure</comments><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Minimum Latency Submodular Cover problem (MLSC), which consists
of a metric $(V,d)$ with source $r\in V$ and $m$ monotone submodular functions
$f_1, f_2, ..., f_m: 2^V \rightarrow [0,1]$. The goal is to find a path
originating at $r$ that minimizes the total cover time of all functions. This
generalizes well-studied problems, such as Submodular Ranking [AzarG11] and
Group Steiner Tree [GKR00]. We give a polynomial time $O(\log \frac{1}{\eps}
\cdot \log^{2+\delta} |V|)$-approximation algorithm for MLSC, where
$\epsilon&gt;0$ is the smallest non-zero marginal increase of any
$\{f_i\}_{i=1}^m$ and $\delta&gt;0$ is any constant.
  We also consider the Latency Covering Steiner Tree problem (LCST), which is
the special case of \mlsc where the $f_i$s are multi-coverage functions. This
is a common generalization of the Latency Group Steiner Tree
[GuptaNR10a,ChakrabartyS11] and Generalized Min-sum Set Cover [AzarGY09,
BansalGK10] problems. We obtain an $O(\log^2|V|)$-approximation algorithm for
LCST.
  Finally we study a natural stochastic extension of the Submodular Ranking
problem, and obtain an adaptive algorithm with an $O(\log 1/ \eps)$
approximation ratio, which is best possible. This result also generalizes some
previously studied stochastic optimization problems, such as Stochastic Set
Cover [GoemansV06] and Shared Filter Evaluation [MunagalaSW07, LiuPRY08].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2209</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2209</id><created>2011-10-10</created><authors><author><keyname>Fukunaga</keyname><forenames>A. S.</forenames></author><author><keyname>Korf</keyname><forenames>R. E.</forenames></author></authors><title>Bin Completion Algorithms for Multicontainer Packing, Knapsack, and
  Covering Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  393-429, 2007</journal-ref><doi>10.1613/jair.2106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many combinatorial optimization problems such as the bin packing and multiple
knapsack problems involve assigning a set of discrete objects to multiple
containers. These problems can be used to model task and resource allocation
problems in multi-agent systems and distributed systms, and can also be found
as subproblems of scheduling problems. We propose bin completion, a
branch-and-bound strategy for one-dimensional, multicontainer packing problems.
Bin completion combines a bin-oriented search space with a powerful dominance
criterion that enables us to prune much of the space. The performance of the
basic bin completion framework can be enhanced by using a number of extensions,
including nogood-based pruning techniques that allow further exploitation of
the dominance criterion. Bin completion is applied to four problems: multiple
knapsack, bin covering, min-cost covering, and bin packing. We show that our
bin completion algorithms yield new, state-of-the-art results for the multiple
knapsack, bin covering, and min-cost covering problems, outperforming previous
algorithms by several orders of magnitude with respect to runtime on some
classes of hard, random problem instances. For the bin packing problem, we
demonstrate significant improvements compared to most previous results, but
show that bin completion is not competitive with current state-of-the-art
cutting-stock based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2210</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2210</id><created>2011-10-10</created><authors><author><keyname>Jodogne</keyname><forenames>S. R.</forenames></author><author><keyname>Piater</keyname><forenames>J. H.</forenames></author></authors><title>Closed-Loop Learning of Visual Control Policies</title><categories>cs.CV</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  349-391, 2007</journal-ref><doi>10.1613/jair.2110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a general, flexible framework for learning mappings
from images to actions by interacting with the environment. The basic idea is
to introduce a feature-based image classifier in front of a reinforcement
learning algorithm. The classifier partitions the visual space according to the
presence or absence of few highly informative local descriptors that are
incrementally selected in a sequence of attempts to remove perceptual aliasing.
We also address the problem of fighting overfitting in such a greedy algorithm.
Finally, we show how high-level visual features can be generated when the power
of local descriptors is insufficient for completely disambiguating the aliased
states. This is done by building a hierarchy of composite features that consist
of recursive spatial combinations of visual features. We demonstrate the
efficacy of our algorithms by solving three visual navigation tasks and a
visual version of the classical Car on the Hill control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2211</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2211</id><created>2011-10-10</created><authors><author><keyname>Kaelbling</keyname><forenames>L. P.</forenames></author><author><keyname>Pasula</keyname><forenames>H. M.</forenames></author><author><keyname>Zettlemoyer</keyname><forenames>L. S.</forenames></author></authors><title>Learning Symbolic Models of Stochastic Domains</title><categories>cs.LG cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  309-352, 2007</journal-ref><doi>10.1613/jair.2113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we work towards the goal of developing agents that can learn
to act in complex worlds. We develop a probabilistic, relational planning rule
representation that compactly models noisy, nondeterministic action effects,
and show how such rules can be effectively learned. Through experiments in
simple planning domains and a 3D simulated blocks world with realistic physics,
we demonstrate that this learning algorithm allows agents to effectively model
world dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2212</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2212</id><created>2011-10-10</created><authors><author><keyname>Rossi</keyname><forenames>F.</forenames></author><author><keyname>Venable</keyname><forenames>K. B.</forenames></author><author><keyname>Yorke-Smith</keyname><forenames>N.</forenames></author></authors><title>Uncertainty in Soft Temporal Constraint Problems:A General Framework and
  Controllability Algorithms forThe Fuzzy Case</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  617-674, 2006</journal-ref><doi>10.1613/jair.2135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-life temporal scenarios, uncertainty and preferences are often
essential and coexisting aspects. We present a formalism where quantitative
temporal constraints with both preferences and uncertainty can be defined. We
show how three classical notions of controllability (that is, strong, weak, and
dynamic), which have been developed for uncertain temporal problems, can be
generalized to handle preferences as well. After defining this general
framework, we focus on problems where preferences follow the fuzzy approach,
and with properties that assure tractability. For such problems, we propose
algorithms to check the presence of the controllability properties. In
particular, we show that in such a setting dealing simultaneously with
preferences and uncertainty does not increase the complexity of controllability
testing. We also develop a dynamic execution algorithm, of polynomial
complexity, that produces temporal plans under uncertainty that are optimal
with respect to fuzzy preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2213</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2213</id><created>2011-10-10</created><authors><author><keyname>Bettini</keyname><forenames>C.</forenames></author><author><keyname>Mascetti</keyname><forenames>S.</forenames></author><author><keyname>Wang</keyname><forenames>X. S.</forenames></author></authors><title>Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal
  Periodic Sets</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  299-348, 2007</journal-ref><doi>10.1613/jair.2136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years several research efforts have focused on the concept of
time granularity and its applications. A first stream of research investigated
the mathematical models behind the notion of granularity and the algorithms to
manage temporal data based on those models. A second stream of research
investigated symbolic formalisms providing a set of algebraic operators to
define granularities in a compact and compositional way. However, only very
limited manipulation algorithms have been proposed to operate directly on the
algebraic representation making it unsuitable to use the symbolic formalisms in
applications that need manipulation of granularities.
  This paper aims at filling the gap between the results from these two streams
of research, by providing an efficient conversion from the algebraic
representation to the equivalent low-level representation based on the
mathematical models. In addition, the conversion returns a minimal
representation in terms of period length. Our results have a major practical
impact: users can more easily define arbitrary granularities in terms of
algebraic operators, and then access granularity reasoning and other services
operating efficiently on the equivalent, minimal low-level representation. As
an example, we illustrate the application to temporal constraint reasoning with
multiple granularities.
  From a technical point of view, we propose an hybrid algorithm that
interleaves the conversion of calendar subexpressions into periodical sets with
the minimization of the period length. The algorithm returns set-based
granularity representations having minimal period length, which is the most
relevant parameter for the performance of the considered reasoning services.
Extensive experimental work supports the techniques used in the algorithm, and
shows the efficiency and effectiveness of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2215</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2215</id><created>2011-10-10</created><authors><author><keyname>Evans</keyname><forenames>R. J.</forenames></author><author><keyname>Orasan</keyname><forenames>C.</forenames></author></authors><title>NP Animacy Identification for Anaphora Resolution</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  79-103, 2007</journal-ref><doi>10.1613/jair.2179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In anaphora resolution for English, animacy identification can play an
integral role in the application of agreement restrictions between pronouns and
candidates, and as a result, can improve the accuracy of anaphora resolution
systems. In this paper, two methods for animacy identification are proposed and
evaluated using intrinsic and extrinsic measures. The first method is a
rule-based one which uses information about the unique beginners in WordNet to
classify NPs on the basis of their animacy. The second method relies on a
machine learning algorithm which exploits a WordNet enriched with animacy
information for each sense. The effect of word sense disambiguation on the two
methods is also assessed. The intrinsic evaluation reveals that the machine
learning method reaches human levels of performance. The extrinsic evaluation
demonstrates that animacy identification can be beneficial in anaphora
resolution, especially in the cases where animate entities are identified with
high precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2216</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2216</id><created>2011-10-10</created><authors><author><keyname>Felzenszwalb</keyname><forenames>P. F.</forenames></author><author><keyname>McAllester</keyname><forenames>D.</forenames></author></authors><title>The Generalized A* Architecture</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  153-190, 2007</journal-ref><doi>10.1613/jair.2187</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a lightest derivation of a global
structure using a set of weighted rules. A large variety of inference problems
in AI can be formulated in this framework. We generalize A* search and
heuristics derived from abstractions to a broad class of lightest derivation
problems. We also describe a new algorithm that searches for lightest
derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss
how the algorithms described here provide a general architecture for addressing
the pipeline problem --- the problem of passing information back and forth
between various stages of processing in a perceptual system. We consider
examples in computer vision and natural language processing. We apply the
hierarchical search algorithm to the problem of estimating the boundaries of
convex objects in grayscale images and compare it to other search methods. A
second set of experiments demonstrate the use of a new compositional model for
finding salient curves in images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2227</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2227</id><created>2011-10-10</created><authors><author><keyname>Rustamov</keyname><forenames>Raif M.</forenames></author></authors><title>Average Interpolating Wavelets on Point Clouds and Graphs</title><categories>math.FA cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new wavelet transform suitable for analyzing functions on
point clouds and graphs. Our construction is based on a generalization of the
average interpolating refinement scheme of Donoho. The most important
ingredient of the original scheme that needs to be altered is the choice of the
interpolant. Here, we define the interpolant as the minimizer of a smoothness
functional, namely a generalization of the Laplacian energy, subject to the
averaging constraints. In the continuous setting, we derive a formula for the
optimal solution in terms of the poly-harmonic Green's function. The form of
this solution is used to motivate our construction in the setting of graphs and
point clouds. We highlight the empirical convergence of our refinement scheme
and the potential applications of the resulting wavelet transform through
experiments on a number of data stets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2230</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2230</id><created>2011-10-10</created><authors><author><keyname>Neary</keyname><forenames>Turlough</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>The complexity of small universal Turing machines: a survey</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey some work concerned with small universal Turing machines, cellular
automata, tag systems, and other simple models of computation. For example it
has been an open question for some time as to whether the smallest known
universal Turing machines of Minsky, Rogozhin, Baiocchi and Kudlek are
efficient (polynomial time) simulators of Turing machines. These are some of
the most intuitively simple computational devices and previously the best known
simulations were exponentially slow. We discuss recent work that shows that
these machines are indeed efficient simulators. In addition, another related
result shows that Rule 110, a well-known elementary cellular automaton, is
efficiently universal. We also discuss some old and new universal program size
results, including the smallest known universal Turing machines. We finish the
survey with results on generalised and restricted Turing machine models
including machines with a periodic background on the tape (instead of a blank
symbol), multiple tapes, multiple dimensions, and machines that never write to
their tape. We then discuss some ideas for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2240</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2240</id><created>2011-10-10</created><authors><author><keyname>Zangerl</keyname><forenames>Alexander</forenames></author></authors><title>DDNFS: a Distributed Digital Notary File System</title><categories>cs.CR</categories><comments>International Journal of Network Security &amp; Its Applications (IJNSA),
  September 2011, Volume 3, Number 5</comments><doi>10.5121/ijnsa.2011.3508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safeguarding online communications using public key cryptography is a
well-established practice today, but with the increasing reliance on
`faceless', solely online entities one of the core aspects of public key
cryptography is becoming a substantial problem in practice: Who can we trust to
introduce us to and vouch for some online party whose public key we see for the
first time? Most existing certification models lack flexibility and have come
under attack repeatedly in recent years, and finding practical improvements has
a high priority.
  We propose that the real-world concept of a notary or certifying witness can
be adapted to today's online environment quite easily, and that such a system
when combined with peer-to-peer technologies for defense in depth is a viable
alternative to monolithic trust infrastructures.
  Instead of trusting assurances from a single party, integrity certifications
(and data replication) can be provided among a group of independent parties in
a peer-to-peer fashion. As the likelihood of all such assurance providers being
subverted at the very same time is very much less than that of a single party,
overall robustness is improved.
  This paper presents the design and the implementation of our prototype online
notary system where independent computer notaries provide integrity
certification and highly-available replicated storage, and discusses how this
online notary system handles some common threat patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2258</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2258</id><created>2011-10-10</created><authors><author><keyname>Munusamy</keyname><forenames>Kanmani</forenames></author><author><keyname>Selamat</keyname><forenames>Harihodin</forenames></author><author><keyname>Ibrahim</keyname><forenames>Suhaimi</forenames></author><author><keyname>Baba</keyname><forenames>Mohd Sapiyan</forenames></author></authors><title>A comparative study of process mediator components that support
  behavioral incompatibility</title><categories>cs.SE</categories><comments>20 Pages, 9 figures and 8 Tables; International Journal on Web
  Service Computing (IJWSC), September 2011, Volume 2, Number 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most businesses these days use the web services technology as a medium to
allow interaction between a service provider and a service requestor. However,
both the service provider and the requestor would be unable to achieve their
business goals when there are miscommunications between their processes. This
research focuses on the process incompatibility between the web services and
the way to automatically resolve them by using a process mediator. This paper
presents an overview of the behavioral incompatibility between web services and
the overview of process mediation in order to resolve the complications faced
due to the incompatibility. Several state-of the-art approaches have been
selected and analyzed to understand the existing process mediation components.
This paper aims to provide a valuable gap analysis that identifies the
important research areas in process mediation that have yet to be fully
explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2263</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2263</id><created>2011-10-11</created><authors><author><keyname>Winfield</keyname><forenames>Christopher J.</forenames></author></authors><title>Asymptotic Methods of ODEs: Exploring Singularities of the Second Kind</title><categories>cs.SC cs.MS math.CA math.NA</categories><comments>12 pages</comments><msc-class>34E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop symbolic methods of asymptotic approximations for solutions of
linear ordinary differential equations and use to them stabilize numerical
calculations. Our method follows classical analysis for first-order systems and
higher-order scalar equations where growth behavior is expressed in terms of
elementary functions. We then recast our equations in mollified form - thereby
obtaining stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2267</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2267</id><created>2011-10-11</created><authors><author><keyname>Seshadri</keyname><forenames>R.</forenames></author><author><keyname>Penchalaiah</keyname><forenames>N.</forenames></author></authors><title>Method for reducing of noise by improving signal-to-noise-ratio in
  wireless LAN</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The signal to noise ratio (SNR) is one of the important measures for reducing
the noise.A technique that uses a linear prediction error filter (LPEF) and an
adaptive digital filter (ADF) to achieve noise reduction in a speech and image
degraded by additive background noise is proposed. Since a speech signal can be
represented as the stationary signal over a short interval of time, most of
speech signal can be predicted by the LPEF. This estimation is performed by the
ADF which is used as system identification. Noise reduction is achieved by
subtracting the reconstructed noise from the speech degraded by additive
background noise. Most of the MR image accelerating methods suffers from
degradation of acquired images, which is often correlated with the degree of
acceleration. However, Wideband MRI is a novel technique that transcends such
flaws.In this paper we proposed LPEF and ADF for reducing the noise in speech
and also we demonstrate that Wideband MRI is capable of obtaining images with
identical quality as conventional MR images in terms of SNR in wireless LAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2270</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2270</id><created>2011-10-11</created><authors><author><keyname>Seshadri</keyname><forenames>R.</forenames></author><author><keyname>Penchalaiah</keyname><forenames>N.</forenames></author></authors><title>Noise Analysis and Detection Based on RF Energy Duration in wireless LAN</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise is the major problem while working with wireless LAN. In this paper we
analyze the noise by using active receiving antenna and also propose the
detection mechanism based on RF energy duration. The standard back off
mechanism of 802.11 wireless LAN (WLAN) increases the contention window when a
transmission failure occurs in order to alleviate contentions in a WLAN. In
addition, many proposed schemes for 802.11 WLAN behave adaptively to
transmission failures. Transmission failures in WLANs occur mostly by two
causes: collision and channel noise. However, in 802.11 WLAN, a station cannot
know the cause of a transmission failure, thus the adaptive schemes assume the
ideal situation in which all transmission failures occur by only one of two
causes. For this reason, they may behave erroneously in a real world where
transmission failures occur by both causes. In this paper, we propose a novel
scheme to detect collision, which utilizes transmission time information and RF
energy duration on the channel. By detecting collisions, a station can
differentiate the causes of transmission failures and the adaptive schemes can
operate correctly by using the detection information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2272</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2272</id><created>2011-10-11</created><authors><author><keyname>Bar&#xe1;t</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Disproof of the List Hadwiger Conjecture</title><categories>math.CO cs.DM</categories><msc-class>05C83, 05C15</msc-class><journal-ref>Electronic J. Combinatorics 18:P232, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The List Hadwiger Conjecture asserts that every $K_t$-minor-free graph is
$t$-choosable. We disprove this conjecture by constructing a
$K_{3t+2}$-minor-free graph that is not $4t$-choosable for every integer $t\geq
1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2274</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2274</id><created>2011-10-11</created><authors><author><keyname>Cranston</keyname><forenames>Daniel W.</forenames></author><author><keyname>Smyth</keyname><forenames>Clifford D.</forenames></author><author><keyname>West</keyname><forenames>Douglas B.</forenames></author></authors><title>Revolutionaries and spies on trees and unicyclic graphs</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><journal-ref>Journal of Combinatorics. Vol. 3, No. 2 (2012), pp. 195-205</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A team of $r$ {\it revolutionaries} and a team of $s$ {\it spies} play a game
on a graph $G$. Initially, revolutionaries and then spies take positions at
vertices. In each subsequent round, each revolutionary may move to an adjacent
vertex or not move, and then each spy has the same option. The revolutionaries
want to hold an {\it unguarded meeting}, meaning $m$ revolutionaries at some
vertex having no spy at the end of a round. To prevent this forever, trivially
at least $\min\{|V(G)|,\FL{r/m}\}$ spies are needed. When $G$ is a tree, this
many spies suffices. When $G$ is a unicyclic graph, $\min\{|V(G)|,\CL{r/m}\}$
spies suffice, and we characterize those unicyclic graphs where $\FL{r/m}+1$
spies are needed. \def\FL#1{\lfloor #1 \rfloor} \def\CL#1{\lceil #1 \rceil}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2286</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2286</id><created>2011-10-11</created><authors><author><keyname>Hasan</keyname><forenames>Yasmeen</forenames></author></authors><title>Power aware physical model for 3d IC's</title><categories>cs.ET</categories><comments>10 pages, publised in International Journal of VLSI design &amp;
  Communication Systems (VLSICS) Vol.2, No.3, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we have proposed a geometric model that is employed to devise a
scheme for identifying the hotspots and zones in a chip. These spots or zone
need to be guarded thermally to ensure performance and reliability of the chip.
The model namely continuous unit sphere model has been presented taking into
account that the 3D region of the chip is uniform, thereby reflecting on the
possible locations of heat sources and the target observation points. The
experimental results for the - continuous domain establish that a region which
does not contain any heat sources may become hotter than the regions containing
the thermal sources. Thus a hotspot may appear away from the active sources,
and placing heat sinks on the active thermal sources alone may not suffice to
tackle thermal imbalance. Power management techniques aid in obtaining a
uniform power profile throughout the chip, but we propose an algorithm using
minimum bipartite matching where we try to move the sources minimally (with
minimum perturbation in the chip floor plan) near cooler points (blocks) to
obtain a uniform power profile due to diffusion of heat from hotter point to
cooler ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2288</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2288</id><created>2011-10-11</created><authors><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Chaporkar</keyname><forenames>Prasanna</forenames></author></authors><title>Optimal Power Allocation for Renewable Energy Source</title><categories>cs.SY cs.IT math.IT math.OC math.PR</categories><comments>7 pages, 4 figures, Technical Report for paper submitted in NCC 2012
  conference (IEEE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Battery powered transmitters face energy constraint, replenishing their
energy by a renewable energy source (like solar or wind power) can lead to
longer lifetime. We consider here the problem of finding the optimal power
allocation under random channel conditions for a wireless transmitter, such
that rate of information transfer is maximized. Here a rechargeable battery,
which is periodically charged by renewable source, is used to power the
transmitter. All of above is formulated as a Markov Decision Process.
Structural properties like the monotonicity of the optimal value and policy
derived in this paper will be of vital importance in understanding the kind of
algorithms and approximations needed in real-life scenarios. The effect of
curse of dimensionality which is prevalent in Dynamic programming problems can
thus be reduced. We show our results under the most general of assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2289</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2289</id><created>2011-10-11</created><authors><author><keyname>Fard</keyname><forenames>Mohammad Amin Kheirandish</forenames></author><author><keyname>Karamizadeh</keyname><forenames>Sasan</forenames></author><author><keyname>Aflaki</keyname><forenames>Mohammad</forenames></author></authors><title>Enhancing congestion control to address link failure loss over mobile
  ad-hoc network</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Standard congestion control cannot detect link failure losses which occur due
to mobility and power scarcity in multi-hop Ad-Hoc network (MANET). Moreover,
successive executions of Back-off algorithm deficiently grow Retransmission
Timeout (RTO) exponentially for new route. The importance of detecting and
responding link failure losses is to prevent sender from remaining idle
unnecessarily and manage number of packet retransmission overhead. In contrast
to Cross-layer approaches which require feedback information from lower layers,
this paper operates purely in Transport layer. This paper explores an
end-to-end threshold-based algorithm which enhances congestion control to
address link failure loss in MANET. It consists of two phases. First,
threshold-based loss classification algorithm distinguishes losses due to link
failure by estimating queue usage based on Relative One-way Trip Time (ROTT).
Second phase adjusts RTO for new route by comparing capabilities of new route
to the broken route using available information in Transport layer such as ROTT
and number of hops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2294</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2294</id><created>2011-10-11</created><authors><author><keyname>Buddelmeijer</keyname><forenames>Hugo</forenames></author><author><keyname>Valentijn</keyname><forenames>Edwin A.</forenames></author></authors><title>Query Driven Visualization of Astronomical Catalogs</title><categories>astro-ph.IM cs.DB</categories><comments>Accepted for publication in topical issue of Experimental Astronomy
  on Astro-WISE information system</comments><doi>10.1007/s10686-011-9263-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive visualization of astronomical catalogs requires novel techniques
due to the huge volumes and complex structure of the data produced by existing
and upcoming astronomical surveys. The creation as well as the disclosure of
the catalogs can be handled by data pulling mechanisms. These prevent
unnecessary processing and facilitate data sharing by having users request the
desired end products.
  In this work we present query driven visualization as a logical continuation
of data pulling. Scientists can request catalogs in a declarative way and set
process parameters directly from within the visualization. This results in
profound interoperation between software with a high level of abstraction.
  New messages for the Simple Application Messaging Protocol are proposed to
achieve this abstraction. Support for these messages are implemented in the
Astro-WISE information system and in a set of demonstrational applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2305</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2305</id><created>2011-10-11</created><updated>2011-11-16</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>de Moya-Aneg&#xf3;n</keyname><forenames>Felix</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The new Excellence Indicator in the World Report of the SCImago
  Institutions Rankings 2011</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new excellence indicator in the World Report of the SCImago Institutions
Rankings (SIR) makes it possible to test differences in the ranking in terms of
statistical significance. For example, at the 17th position of these rankings,
UCLA has an output of 37,994 papers with an excellence indicator of 28.9.
Stanford University follows at the 19th position with 37,885 papers and 29.1
excellence, and z = - 0.607. The difference between these two institution thus
is not statistically significant. We provide a calculator at
http://www.leydesdorff.net/scimago11/scimago11.xls in which one can fill out
this test for any two institutions and also for each institution on whether its
score is significantly above or below expectation (assuming that 10% of the
papers are for stochastic reasons in the top-10% set).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2306</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2306</id><created>2011-10-11</created><authors><author><keyname>Cuturi</keyname><forenames>Marco</forenames></author><author><keyname>Avis</keyname><forenames>David</forenames></author></authors><title>Ground Metric Learning</title><categories>stat.ML cs.CV cs.LG</categories><comments>32 pages, 4 figures</comments><journal-ref>Journal of Machine Learning Research, 15, 533-564. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transportation distances have been used for more than a decade now in machine
learning to compare histograms of features. They have one parameter: the ground
metric, which can be any metric between the features themselves. As is the case
for all parameterized distances, transportation distances can only prove useful
in practice when this parameter is carefully chosen. To date, the only option
available to practitioners to set the ground metric parameter was to rely on a
priori knowledge of the features, which limited considerably the scope of
application of transportation distances. We propose to lift this limitation and
consider instead algorithms that can learn the ground metric using only a
training set of labeled histograms. We call this approach ground metric
learning. We formulate the problem of learning the ground metric as the
minimization of the difference of two polyhedral convex functions over a convex
set of distance matrices. We follow the presentation of our algorithms with
promising experimental results on binary classification tasks using GIST
descriptors of images taken in the Caltech-256 set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2317</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2317</id><created>2011-10-11</created><authors><author><keyname>Pratt-Hartmann</keyname><forenames>Ian</forenames></author></authors><title>The Syllogistic with Unity</title><categories>cs.LO math.LO</categories><msc-class>03B65</msc-class><acm-class>F.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the language of the classical syllogisms with the sentence-forms
&quot;At most 1 p is a q&quot; and &quot;More than 1 p is a q&quot;. We show that the resulting
logic does not admit a finite set of syllogism-like rules whose associated
derivation relation is sound and complete, even when reductio ad absurdum is
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2318</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2318</id><created>2011-10-11</created><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>Compressed Membership for NFA (DFA) with Compressed Labels is in NP (P)</title><categories>cs.FL</categories><acm-class>F.4.3; F.4.2; F.2.2; F.1.1</acm-class><doi>10.1007/s00224-013-9443-6</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a compressed membership problem for finite automata, both
deterministic and non-deterministic, with compressed transition labels is
studied. The compression is represented by straight-line programs (SLPs), i.e.
context-free grammars generating exactly one string. A novel technique of
dealing with SLPs is introduced: the SLPs are recompressed, so that substrings
of the input text are encoded in SLPs labelling the transitions of the NFA
(DFA) in the same way, as in the SLP representing the input text. To this end,
the SLPs are locally decompressed and then recompressed in a uniform way.
Furthermore, such recompression induces only small changes in the automaton, in
particular, the size of the automaton remains polynomial.
  Using this technique it is shown that the compressed membership for NFA with
compressed labels is in NP, thus confirming the conjecture of Plandowski and
Rytter and extending the partial result of Lohrey and Mathissen; as it is
already known, that this problem is NP-hard, we settle its exact computational
complexity. Moreover, the same technique applied to the compressed membership
for DFA with compressed labels yields that this problem is in P; for this
problem, only trivial upper-bound PSPACE was known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2341</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2341</id><created>2011-10-11</created><updated>2011-10-26</updated><authors><author><keyname>Kashefikia</keyname><forenames>Mehdi</forenames></author><author><keyname>Nematbakhsh</keyname><forenames>Nasser</forenames></author><author><keyname>Moghadam</keyname><forenames>Reza Askari</forenames></author></authors><title>Multiple ant-bee colony optimization for load balancing in
  packet-switched networks</title><categories>cs.NI cs.AI</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important issues in computer networks is &quot;Load Balancing&quot; which
leads to efficient use of the network resources. To achieve a balanced network
it is necessary to find different routes between the source and destination. In
the current paper we propose a new approach to find different routes using
swarm intelligence techniques and multi colony algorithms. In the proposed
algorithm that is an improved version of MACO algorithm, we use different
colonies of ants and bees and appoint these colony members as intelligent
agents to monitor the network and update the routing information. The survey
includes comparison and critiques of MACO. The simulation results show a
tangible improvement in the aforementioned approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2343</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2343</id><created>2011-10-11</created><authors><author><keyname>Mahdaviani</keyname><forenames>Kaveh</forenames></author><author><keyname>Ardakani</keyname><forenames>Masoud</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Annotated Raptor Codes</title><categories>cs.IT math.IT</categories><comments>This paper has been Accepted for presentation in IEEE Information
  Theory Workshop (ITW) 2011, Paraty, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an extension of raptor codes is introduced which keeps all the
desirable properties of raptor codes, including the linear complexity of
encoding and decoding per information bit, unchanged. The new design, however,
improves the performance in terms of the reception rate. Our simulations show a
10% reduction in the needed overhead at the benchmark block length of 64,520
bits and with the same complexity per information bit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2350</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2350</id><created>2011-10-11</created><updated>2013-01-16</updated><authors><author><keyname>Amadio</keyname><forenames>Roberto M.</forenames><affiliation>PPS</affiliation></author><author><keyname>Regis-Gianas</keyname><forenames>Yann</forenames><affiliation>PPS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Certifying and reasoning about cost annotations of functional programs</title><categories>cs.PL</categories><comments>Higher-Order and Symbolic Computation (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a so-called labelling method to insert cost annotations in a
higher-order functional program, to certify their correctness with respect to a
standard compilation chain to assembly code including safe memory management,
and to reason on them in a higher-order Hoare logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2382</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2382</id><created>2011-10-11</created><updated>2014-02-24</updated><authors><author><keyname>Rowland</keyname><forenames>Eric</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Automatic sets of rational numbers</title><categories>cs.FL cs.DM math.NT</categories><comments>Previous version appeared in Proc. LATA 2012 conference</comments><journal-ref>International Journal of Foundations of Computer Science 26 (2015)
  343-365</journal-ref><doi>10.1142/S0129054115500197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of a k-automatic set of integers is well-studied. We develop a new
notion - the k-automatic set of rational numbers - and prove basic properties
of these sets, including closure properties and decidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2392</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2392</id><created>2011-10-11</created><updated>2011-10-13</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>A Variant of Azuma's Inequality for Martingales with Subgaussian Tails</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a variant of Azuma's concentration inequality for martingales, in
which the standard boundedness requirement is replaced by the milder
requirement of a subgaussian tail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2396</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2396</id><created>2011-10-11</created><authors><author><keyname>Albertoni</keyname><forenames>Riccardo</forenames></author><author><keyname>De Martino</keyname><forenames>Monica</forenames></author></authors><title>Semantic Technology to Exploit Digital Content Exposed as Linked Data</title><categories>cs.DL</categories><comments>Published in eChallenges e-2011 Conference Proceedings Paul
  Cunningham and Miriam Cunningham (Eds) IIMC International Information
  Management Corporation, 2011 ISBN: 978-1-905824-27-4</comments><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper illustrates the research result of the application of semantic
technology to ease the use and reuse of digital contents exposed as Linked Data
on the web. It focuses on the specific issue of explorative research for the
resource selection: a context dependent semantic similarity assessment is
proposed in order to compare datasets annotated through terminologies exposed
as Linked Data (e.g. habitats, species). Semantic similarity is shown as a
building block technology to sift linked data resources. From semantic
similarity application, we derived a set of recommendations underlying open
issues in scaling the similarity assessment up to the Web of Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2400</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2400</id><created>2011-10-11</created><authors><author><keyname>Kiefer</keyname><forenames>Stephan</forenames></author><author><keyname>Rauch</keyname><forenames>Jochen</forenames></author><author><keyname>Albertoni</keyname><forenames>Riccardo</forenames></author><author><keyname>Attene</keyname><forenames>Marco</forenames></author><author><keyname>Giannini</keyname><forenames>Franca</forenames></author><author><keyname>Marini</keyname><forenames>Simone</forenames></author><author><keyname>Schneider</keyname><forenames>Luc</forenames></author><author><keyname>Mesquita</keyname><forenames>Carlos</forenames></author><author><keyname>Xing</keyname><forenames>Xin</forenames></author><author><keyname>Lawo</keyname><forenames>Michael</forenames></author></authors><title>The CHRONIOUS Ontology-Driven Search Tool: Enabling Access to Focused
  and Up-to-Date Healthcare Literature</title><categories>cs.DL</categories><comments>published in eChallenges e-2011 Conference Proceedings Paul
  Cunningham and Miriam Cunningham (Eds) IIMC International Information
  Management Corporation, 2011 ISBN: 978-1-905824-27-4</comments><acm-class>I.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an advanced search engine prototype for bibliography
retrieval developed within the CHRONIOUS European IP project of the seventh
Framework Program (FP7). This search engine is specifically targeted to
clinicians and healthcare practitioners searching for documents related to
Chronic Obstructive Pulmonary Disease (COPD) and Chronic Kidney Disease (CKD).
To this aim, the presented tool exploits two pathology-specific ontologies that
allow focused document indexing and retrieval. These ontologies have been
developed on the top of the Middle Layer Ontology for Clinical Care (MLOCC),
which provides a link with the Basic Formal Ontology, a foundational ontology
used in the Open Biological and Biomedical Ontologies (OBO) Foundry. In
addition link with the terms of the MeSH (Medical Subject Heading) thesaurus
has been provided to guarantee the coverage with the general certified medical
terms and multilingual capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2407</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2407</id><created>2011-10-11</created><authors><author><keyname>Caicedo</keyname><forenames>Xavier</forenames></author><author><keyname>Rodriguez</keyname><forenames>Ricardo Oscar</forenames></author></authors><title>Bi-modal G\&quot;odel logic over [0,1]-valued Kripke frames</title><categories>math.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the G\&quot;odel bi-modal logic determined by fuzzy Kripke models
where both the propositions and the accessibility relation are infinitely
valued over the standard G\&quot;odel algebra [0,1] and prove strong completeness of
Fischer Servi intuitionistic modal logic IK plus the prelinearity axiom with
respect to this semantics. We axiomatize also the bi-modal analogues of $T,$
$S4,$ and $S5$ obtained by restricting to models over frames satisfying the
[0,1]-valued versions of the structural properties which characterize these
logics. As application of the completeness theorems we obtain a representation
theorem for bi-modal G\&quot;odel algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2416</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2416</id><created>2011-10-11</created><authors><author><keyname>Schleif</keyname><forenames>F. -M.</forenames></author><author><keyname>Gisbrecht</keyname><forenames>A.</forenames></author><author><keyname>Hammer</keyname><forenames>B.</forenames></author></authors><title>Supervised learning of short and high-dimensional temporal sequences for
  life science measurements</title><categories>cs.LG</categories><report-no>DPA-11341</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The analysis of physiological processes over time are often given by
spectrometric or gene expression profiles over time with only few time points
but a large number of measured variables. The analysis of such temporal
sequences is challenging and only few methods have been proposed. The
information can be encoded time independent, by means of classical expression
differences for a single time point or in expression profiles over time.
Available methods are limited to unsupervised and semi-supervised settings. The
predictive variables can be identified only by means of wrapper or
post-processing techniques. This is complicated due to the small number of
samples for such studies. Here, we present a supervised learning approach,
termed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a
supervised mapping of the temporal sequences onto a low dimensional grid. We
utilize a hidden markov model (HMM) to account for the time domain and
relevance learning to identify the relevant feature dimensions most predictive
over time. The learned mapping can be used to visualize the temporal sequences
and to predict the class of a new sequence. The relevance learning permits the
identification of discriminating masses or gen expressions and prunes
dimensions which are unnecessary for the classification task or encode mainly
noise. In this way we obtain a very efficient learning system for temporal
sequences. The results indicate that using simultaneous supervised learning and
metric adaptation significantly improves the prediction accuracy for
synthetically and real life data in comparison to the standard techniques. The
discriminating features, identified by relevance learning, compare favorably
with the results of alternative methods. Our method permits the visualization
of the data on a low dimensional grid, highlighting the observed temporal
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2417</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2417</id><created>2011-10-11</created><updated>2015-03-23</updated><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>New Improvements on the Echelon-Ferrers Construction</title><categories>cs.IT math.IT</categories><comments>In Proceedings of the 19th International Symposium on Mathematical
  Theory of Networks and Systems - MTNS, Budapest, Hungary, 2010, pp. 405-408</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to improve the echelon-Ferrers construction of random network
codes introduced by Etzion and Silberstein to attain codes of larger size for a
given minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2435</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2435</id><created>2011-10-11</created><authors><author><keyname>Surana</keyname><forenames>Amit</forenames></author><author><keyname>Sahai</keyname><forenames>Tuhin</forenames></author><author><keyname>Banaszuk</keyname><forenames>Andrzej</forenames></author></authors><title>Iterative Methods for Scalable Uncertainty Quantification in Complex
  Networks</title><categories>stat.CO cs.DC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of uncertainty management for robust
design, and verification of large dynamic networks whose performance is
affected by an equally large number of uncertain parameters. Many such networks
(e.g. power, thermal and communication networks) are often composed of weakly
interacting subnetworks. We propose intrusive and non-intrusive iterative
schemes that exploit such weak interconnections to overcome dimensionality
curse associated with traditional uncertainty quantification methods (e.g.
generalized Polynomial Chaos, Probabilistic Collocation) and accelerate
uncertainty propagation in systems with large number of uncertain parameters.
This approach relies on integrating graph theoretic methods and waveform
relaxation with generalized Polynomial Chaos, and Probabilistic Collocation,
rendering these techniques scalable. We analyze convergence properties of this
scheme and illustrate it on several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2436</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2436</id><created>2011-10-11</created><authors><author><keyname>Ram&#xed;rez</keyname><forenames>Ignacio</forenames><affiliation>University of Minnesota</affiliation></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames><affiliation>University of Minnesota</affiliation></author></authors><title>An MDL framework for sparse coding and dictionary learning</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power of sparse signal modeling with learned over-complete dictionaries
has been demonstrated in a variety of applications and fields, from signal
processing to statistical inference and machine learning. However, the
statistical properties of these models, such as under-fitting or over-fitting
given sets of data, are still not well characterized in the literature. As a
result, the success of sparse modeling depends on hand-tuning critical
parameters for each data and application. This work aims at addressing this by
providing a practical and objective characterization of sparse models by means
of the Minimum Description Length (MDL) principle -- a well established
information-theoretic approach to model selection in statistical inference. The
resulting framework derives a family of efficient sparse coding and dictionary
learning algorithms which, by virtue of the MDL principle, are completely
parameter free. Furthermore, such framework allows to incorporate additional
prior information to existing models, such as Markovian dependencies, or to
define completely new problem formulations, including in the matrix analysis
area, in a natural way. These virtues will be demonstrated with parameter-free
algorithms for the classic image denoising and classification problems, and for
low-rank matrix recovery in video applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2477</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2477</id><created>2011-10-11</created><authors><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Roux</keyname><forenames>Alet</forenames></author><author><keyname>Zastawniak</keyname><forenames>Tomasz</forenames></author></authors><title>Parallel Binomial American Option Pricing with (and without) Transaction
  Costs</title><categories>cs.DC q-fin.CP</categories><msc-class>62L15, 90C15, 91B28, 60G42</msc-class><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel algorithm that computes the ask and bid prices of an
American option when proportional transaction costs apply to the trading of the
underlying asset. The algorithm computes the prices on recombining binomial
trees, and is designed for modern multi-core processors. Although parallel
option pricing has been well studied, none of the existing approaches takes
transaction costs into consideration. The algorithm that we propose partitions
a binomial tree into blocks. In any round of computation a block is further
partitioned into regions which are assigned to distinct processors. To minimise
load imbalance the assignment of nodes to processors is dynamically adjusted
before each new round starts. Synchronisation is required both within a round
and between two successive rounds. The parallel speedup of the algorithm is
proportional to the number of processors used. The parallel algorithm was
implemented in C/C++ via POSIX Threads, and was tested on a machine with 8
processors. In the pricing of an American put option, the parallel speedup
against an efficient sequential implementation was 5.26 using 8 processors and
1500 time steps, achieving a parallel efficiency of 65.75%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2478</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2478</id><created>2011-10-11</created><updated>2011-12-01</updated><authors><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author><author><keyname>Baronio</keyname><forenames>Roberta</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Countering Gattaca: Efficient and Secure Testing of Fully-Sequenced
  Human Genomes (Full Version)</title><categories>cs.CR cs.CE</categories><comments>18th ACM Conference on Computer and Communications Security (CCS
  2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in DNA sequencing technologies have put ubiquitous
availability of fully sequenced human genomes within reach. It is no longer
hard to imagine the day when everyone will have the means to obtain and store
one's own DNA sequence. Widespread and affordable availability of fully
sequenced genomes immediately opens up important opportunities in a number of
health-related fields. In particular, common genomic applications and tests
performed in vitro today will soon be conducted computationally, using
digitized genomes. New applications will be developed as genome-enabled
medicine becomes increasingly preventive and personalized. However, this
progress also prompts significant privacy challenges associated with potential
loss, theft, or misuse of genomic data. In this paper, we begin to address
genomic privacy by focusing on three important applications: Paternity Tests,
Personalized Medicine, and Genetic Compatibility Tests. After carefully
analyzing these applications and their privacy requirements, we propose a set
of efficient techniques based on private set operations. This allows us to
implement in in silico some operations that are currently performed via in
vitro methods, in a secure fashion. Experimental results demonstrate that
proposed techniques are both feasible and practical today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2480</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2480</id><created>2011-10-11</created><updated>2012-09-03</updated><authors><author><keyname>Schurgot</keyname><forenames>Mary R.</forenames></author><author><keyname>Comaniciu</keyname><forenames>Cristina</forenames></author><author><keyname>Jaffr&#xe8;s-Runser</keyname><forenames>Katia</forenames></author></authors><title>Beyond Traditional DTN Routing: Social Networks for Opportunistic
  Communication</title><categories>cs.NI cs.SI</categories><comments>8 pages, 4 figures, 1 table</comments><journal-ref>Schurgot, M.R.; Comaniciu, C.; Jaffres-Runser, K.; , &quot;Beyond
  Traditional DTN Routing: Social Networks for Opportunistic Communication,&quot;
  IEEE Communications Magazine, vol.50, no.7, pp.155-162, July 2012</journal-ref><doi>10.1109/MCOM.2012.6231292</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines the evolution of routing protocols for intermittently
connected ad hoc networks and discusses the trend toward social-based routing
protocols. A survey of current routing solutions is presented, where routing
protocols for opportunistic networks are classified based on the network graph
employed. The need to capture performance tradeoffs from a multi-objective
perspective is highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2515</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2515</id><created>2011-10-11</created><updated>2013-08-02</updated><authors><author><keyname>McDaid</keyname><forenames>Aaron F.</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Hurley</keyname><forenames>Neil</forenames></author></authors><title>Normalized Mutual Information to evaluate overlapping community finding
  algorithms</title><categories>physics.soc-ph cs.SI physics.data-an</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given the increasing popularity of algorithms for overlapping clustering, in
particular in social network analysis, quantitative measures are needed to
measure the accuracy of a method. Given a set of true clusters, and the set of
clusters found by an algorithm, these sets of clusters must be compared to see
how similar or different the sets are. A normalized measure is desirable in
many contexts, for example assigning a value of 0 where the two sets are
totally dissimilar, and 1 where they are identical. A measure based on
normalized mutual information, [1], has recently become popular. We demonstrate
unintuitive behaviour of this measure, and show how this can be corrected by
using a more conventional normalization. We compare the results to that of
other measures, such as the Omega index [2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2520</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2520</id><created>2011-10-11</created><authors><author><keyname>Bergner</keyname><forenames>Steven</forenames></author><author><keyname>Crider</keyname><forenames>Matthew</forenames></author><author><keyname>Kirkpatrick</keyname><forenames>Arthur E.</forenames></author><author><keyname>M&#xf6;ller</keyname><forenames>Torsten</forenames></author></authors><title>Mixing Board Versus Mouse Interaction In Value Adjustment Tasks</title><categories>cs.HC</categories><report-no>SFU-CMPT TR 2011-4</report-no><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a controlled, quantitative study with 12 participants comparing
interaction with a haptically enhanced mixing board against interaction with a
mouse in an abstract task that is motivated by several practical parameter
space exploration settings.
  The study participants received 24 sets of one to eight integer values
between 0 and 127, which they had to match by making adjustments with physical
or graphical sliders. Based on recorded slider motion path data, we developed
an analysis algorithm that identifies and measures different types of activity
intervals, including error time moving irrelevant sliders and end time in
breaks after completing each trial item. Our results showed a significant
increase in speed of the mixing board interaction accompanied by reduced
perceived cognitive load when compared with the traditional mouse-based GUI
interaction. The gains in speed are largely due to the improved times required
for the hand to reach for the first slider (acquisition time) and also when
moving between different ones, while the actual time spent manipulating
relevant sliders is very similar for either input device. These results agree
strongly with qualitative predictions from Fitts' Law that the larger targets
afforded by the mixer handles contributed to its faster performance. For
further investigation, we computed a measure of motion simultaneity based on
velocity correlation, which allowed to identify types of items for which
increased simultaneous adjustments occur.
  For continuous parameter space exploration our findings suggest that mixing
boards are a good option to provide detailed multi-value control. The strengths
of this input method particularly show in settings where screen space is
precious and undisrupted visual focus is crucial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2529</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2529</id><created>2011-10-11</created><updated>2012-06-06</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Duchi</keyname><forenames>John C.</forenames></author></authors><title>The Generalization Ability of Online Algorithms for Dependent Data</title><categories>stat.ML cs.LG math.OC</categories><comments>26 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the generalization performance of online learning algorithms trained
on samples coming from a dependent source of data. We show that the
generalization error of any stable online algorithm concentrates around its
regret--an easily computable statistic of the online performance of the
algorithm--when the underlying ergodic process is $\beta$- or $\phi$-mixing. We
show high probability error bounds assuming the loss function is convex, and we
also establish sharp convergence rates and deviation bounds for strongly convex
losses and several linear prediction problems such as linear and logistic
regression, least-squares SVM, and boosting on dependent data. In addition, our
results have straightforward applications to stochastic optimization with
dependent data, and our analysis requires only martingale convergence
arguments; we need not rely on more powerful statistical tools such as
empirical process theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2557</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2557</id><created>2011-10-11</created><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>Constructions of Rank Modulation Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank modulation is a way of encoding information to correct errors in flash
memory devices as well as impulse noise in transmission lines. Modeling rank
modulation involves construction of packings of the space of permutations
equipped with the Kendall tau distance.
  We present several general constructions of codes in permutations that cover
a broad range of code parameters. In particular, we show a number of ways in
which conventional error-correcting codes can be modified to correct errors in
the Kendall space. Codes that we construct afford simple encoding and decoding
algorithms of essentially the same complexity as required to correct errors in
the Hamming metric. For instance, from binary BCH codes we obtain codes
correcting $t$ Kendall errors in $n$ memory cells that support the order of
$n!/(\log_2n!)^t$ messages, for any constant $t= 1,2,...$ We also construct
families of codes that correct a number of errors that grows with $n$ at
varying rates, from $\Theta(n)$ to $\Theta(n^{2})$. One of our constructions
gives rise to a family of rank modulation codes for which the trade-off between
the number of messages and the number of correctable Kendall errors approaches
the optimal scaling rate. Finally, we list a number of possibilities for
constructing codes of finite length, and give examples of rank modulation codes
with specific parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2558</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2558</id><created>2011-10-11</created><updated>2012-08-08</updated><authors><author><keyname>Sikic</keyname><forenames>Mile</forenames></author><author><keyname>Lancic</keyname><forenames>Alen</forenames></author><author><keyname>Antulov-Fantulin</keyname><forenames>Nino</forenames></author><author><keyname>Stefancic</keyname><forenames>Hrvoje</forenames></author></authors><title>Epidemic centrality - is there an underestimated epidemic impact of
  network peripheral nodes?</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><doi>10.1140/epjb/e2013-31025-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of disease spreading on empirical complex networks in SIR model,
initially infected nodes can be ranked according to some measure of their
epidemic impact. The highest ranked nodes, also referred to as
&quot;superspreaders&quot;, are associated to dominant epidemic risks and therefore
deserve special attention. In simulations on studied empirical complex
networks, it is shown that the ranking depends on the dynamical regime of the
disease spreading. A possible mechanism leading to this dependence is
illustrated in an analytically tractable example. In systems where the
allocation of resources to counter disease spreading to individual nodes is
based on their ranking, the dynamical regime of disease spreading is frequently
not known before the outbreak of the disease. Therefore, we introduce a
quantity called epidemic centrality as an average over all relevant regimes of
disease spreading as a basis of the ranking. A recently introduced concept of
phase diagram of epidemic spreading is used as a framework in which several
types of averaging are studied. The epidemic centrality is compared to
structural properties of nodes such as node degree, k-cores and betweenness.
There is a growing trend of epidemic centrality with degree and k-cores values,
but the variation of epidemic centrality is much smaller than the variation of
degree or k-cores value. It is found that the epidemic centrality of the
structurally peripheral nodes is of the same order of magnitude as the epidemic
centrality of the structurally central nodes. The implications of these
findings for the distributions of resources to counter disease spreading are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2561</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2561</id><created>2011-10-11</created><authors><author><keyname>Peretyatko</keyname><forenames>Alexey A.</forenames></author><author><keyname>Bogatyrev</keyname><forenames>Ivan A.</forenames></author><author><keyname>Kapitan</keyname><forenames>Vitaliy Yu.</forenames></author><author><keyname>Kirienko</keyname><forenames>Yury V.</forenames></author><author><keyname>Nefedev</keyname><forenames>Konstantin V.</forenames></author><author><keyname>Belokon</keyname><forenames>Valery I.</forenames></author></authors><title>Rigorous Calculation of the Partition Function for the Finite Number of
  Ising Spins</title><categories>cs.DC</categories><comments>4 pages 3 figures</comments><acm-class>D.3.2</acm-class><journal-ref>Materials of the 4th International Multi-Conference on Engineering
  and Technological Innovation: IMETI 2011 July 19th - July 22nd, 2011 -
  Orlando, FL, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high-performance scalable parallel algorithm for rigorous calculation of
partition function of lattice systems with finite number Ising spins was
developed. The parallel calculations run by C++ code with using of Message
Passing Interface and massive parallel instructions. The algorithm can be used
for the research of the interacting spin systems in the Ising models of 2D and
3D. The processing power and scalability is analyzed for different parallel and
distributed systems. Different methods of the speed up measuring allow obtain
the super-linear speeding up for the small number of processes. Program code
could be useful also for research by exact method of different Ising spin
systems, e.g. system with competition interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2574</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2574</id><created>2011-10-12</created><updated>2013-03-20</updated><authors><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author></authors><title>Synthesis and Optimization of Reversible Circuits - A Survey</title><categories>cs.ET quant-ph</categories><comments>34 pages, 15 figures, 2 tables</comments><acm-class>B.6.3; B.6.1</acm-class><journal-ref>M. Saeedi and I. L. Markov, &quot;Synthesis and Optimization of
  Reversible Circuits - A Survey&quot;, ACM Computing Surveys, 45, 2, Article 21 (34
  pages), 2013</journal-ref><doi>10.1145/2431211.2431220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic circuits have been historically motivated by theoretical
research in low-power electronics as well as practical improvement of
bit-manipulation transforms in cryptography and computer graphics. Recently,
reversible circuits have attracted interest as components of quantum
algorithms, as well as in photonic and nano-computing technologies where some
switching devices offer no signal gain. Research in generating reversible logic
distinguishes between circuit synthesis, post-synthesis optimization, and
technology mapping. In this survey, we review algorithmic paradigms ---
search-based, cycle-based, transformation-based, and BDD-based --- as well as
specific algorithms for reversible synthesis, both exact and heuristic. We
conclude the survey by outlining key open challenges in synthesis of reversible
and quantum logic, as well as most common misconceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2593</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2593</id><created>2011-10-12</created><authors><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author></authors><title>Blind Source Separation with Compressively Sensed Linear Mixtures</title><categories>cs.IT math.IT</categories><comments>9 pages, 2 figures</comments><doi>10.1109/LSP.2011.2181945</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work studies the problem of simultaneously separating and reconstructing
signals from compressively sensed linear mixtures. We assume that all source
signals share a common sparse representation basis. The approach combines
classical Compressive Sensing (CS) theory with a linear mixing model. It allows
the mixtures to be sampled independently of each other. If samples are acquired
in the time domain, this means that the sensors need not be synchronized. Since
Blind Source Separation (BSS) from a linear mixture is only possible up to
permutation and scaling, factoring out these ambiguities leads to a
minimization problem on the so-called oblique manifold. We develop a geometric
conjugate subgradient method that scales to large systems for solving the
problem. Numerical results demonstrate the promising performance of the
proposed algorithm compared to several state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2595</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2595</id><created>2011-10-12</created><authors><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>Survey of Latest Wireless Cellular Technologies for Enhancement of
  Spectral Density at Reduced Cost</title><categories>cs.CY</categories><comments>7 pages, 4 figures, international journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 2, 2011, 491-497</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The future of mobile wireless communication networks will include existing
3rd generation, 4th generation (implemented in Japan, USA, South Korea etc.),
5th generation (based on cognitive radio which implies the whole wireless world
interconnection &amp; WISDOM - Wireless innovative System for Dynamic Operating
Megacommunications concept), 6th generation (with very high data rates Quality
of Service (QoS) and service applications) and 7th generation (with space
roaming). This paper is focused on the specifications of future generations and
latest technologies to be used in future wireless mobile communication
networks. However keeping in view the general poor masses of India, some of the
future generation technologies will be embedded with 2G and 2.5G so that
general masses may get the advantage of internet, multimedia services and the
operators may get proper revenues with little extra expenditure in the existing
mobile communication networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2610</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2610</id><created>2011-10-12</created><authors><author><keyname>Agarwal</keyname><forenames>Parul</forenames></author><author><keyname>Alam</keyname><forenames>M. Afshar</forenames></author><author><keyname>Biswas</keyname><forenames>Ranjit</forenames></author></authors><title>Issues,Challenges and Tools of Clustering Algorithms</title><categories>cs.IR cs.LG</categories><comments>6 PAGES</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 2, May 2011 ISSN (Online): 1694-0814 page numbers 523-528</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an unsupervised technique of Data Mining. It means grouping
similar objects together and separating the dissimilar ones. Each object in the
data set is assigned a class label in the clustering process using a distance
measure. This paper has captured the problems that are faced in real when
clustering algorithms are implemented .It also considers the most extensively
used tools which are readily available and support functions which ease the
programming. Once algorithms have been implemented, they also need to be tested
for its validity. There exist several validation indexes for testing the
performance and accuracy which have also been discussed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2613</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2613</id><created>2011-10-12</created><updated>2012-10-01</updated><authors><author><keyname>Lang</keyname><forenames>Alex</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Coecke</keyname><forenames>Bob</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Trichromatic Open Digraphs for Understanding Qubits</title><categories>math.CT cs.LO math.QA quant-ph</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 193-209</journal-ref><doi>10.4204/EPTCS.95.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a trichromatic graphical calculus for quantum computing. The
generators represent three complementary observables that are treated on equal
footing, hence reflecting the symmetries of the Bloch sphere. We derive the
Euler angle decomposition of the Hadamard gate within it as well as the
so-called supplementary relationships, which are valid equations for qubits
that were not derivable within Z/X-calculus of Coecke and Duncan. More
specifically, we have: dichromatic Z/X-calculus + Euler angle decomposition of
the Hadamard gate = trichromatic calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2615</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2615</id><created>2011-10-12</created><updated>2012-10-03</updated><authors><author><keyname>Simon</keyname><forenames>Emile</forenames></author><author><keyname>Wertz</keyname><forenames>Vincent</forenames></author></authors><title>Alternatives with stronger convergence than coordinate-descent iterative
  LMI algorithms</title><categories>math.OC cs.SY</categories><comments>3 pages. Main experimental results reproducible from files available
  on http://www.mathworks.com/matlabcentral/fileexchange/33219 This work has
  been submitted to the IEEE for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we aim at putting more emphasis on the fact that trying to solve
non-convex optimization problems with coordinate-descent iterative linear
matrix inequality algorithms leads to suboptimal solutions, and put forward
other optimization methods better equipped to deal with such problems (having
theoretical convergence guarantees and/or being more efficient in practice).
This fact, already outlined at several places in the literature, still appears
to be disregarded by a sizable part of the systems and control community. Thus,
main elements on this issue and better optimization alternatives are presented
and illustrated by means of an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2626</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2626</id><created>2011-10-12</created><authors><author><keyname>Rani</keyname><forenames>K. Usha</forenames></author></authors><title>Analysis of Heart Diseases Dataset using Neural Network Approach</title><categories>cs.LG cs.DB</categories><comments>8 pages, 2 figures, 1 table; International Journal of Data Mining &amp;
  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011</comments><doi>10.5121/ijdkp.2011.1501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important techniques of Data mining is Classification. Many real
world problems in various fields such as business, science, industry and
medicine can be solved by using classification approach. Neural Networks have
emerged as an important tool for classification. The advantages of Neural
Networks helps for efficient classification of given data. In this study a
Heart diseases dataset is analyzed using Neural Network approach. To increase
the efficiency of the classification process parallel approach is also adopted
in the training phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2627</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2627</id><created>2011-10-12</created><authors><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>Hierarchical Cellular Structures in High-Capacity Cellular Communication
  Systems</title><categories>cs.CY</categories><comments>7 pages, 8 figures, International Journal</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No 9, 2011, 51-57</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the prevailing cellular environment, it is important to provide the
resources for the fluctuating traffic demand exactly in the place and at the
time where and when they are needed. In this paper, we explored the ability of
hierarchical cellular structures with inter layer reuse to increase the
capacity of mobile communication network by applying total frequency hopping
(T-FH) and adaptive frequency allocation (AFA) as a strategy to reuse the macro
and micro cell resources without frequency planning in indoor pico cells [11].
The practical aspects for designing macro- micro cellular overlays in the
existing big urban areas are also explained [4]. Femto cells are inducted in
macro / micro / pico cells hierarchical structure to achieve the required QoS
cost effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2646</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2646</id><created>2011-10-12</created><authors><author><keyname>Texier</keyname><forenames>Jose</forenames></author></authors><title>Notas metodol\'ogicas para cubrir la etapa de documentar una
  investigaci\'on</title><categories>cs.DL</categories><comments>Trabajo final del curso.
  http://sedici.unlp.edu.ar/search/request.php?id_document=ARG-UNLP-PAC-0000000043</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The search process of scientific articles (papers) and review articles
(reviews) is one of the pillars of the scientific world, and is performed by
people in the research as well as for people who want to keep abreast specific
topics. Scopus (there are other databases) or Google Scholar are proposed
options to find articles, but is recommended by Scopus its extensive database
and its versatility in the search options it offers. This paper proposes is a
plan that allows a systematic search and keep the items in an orderly,
consistent and coherent within own repository for cataloging and consultation,
which will serve for many tasks to establish the state of the art of a topic,
staff training in an area and/or writing articles, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2653</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2653</id><created>2011-10-12</created><updated>2012-03-17</updated><authors><author><keyname>Tian</keyname><forenames>Miaomiao</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author></authors><title>Security of a biometric identity-based encryption scheme</title><categories>cs.CR</categories><comments>Journal version of the paper will be appearing in International
  Journal of Network Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric identity-based encryption (Bio-IBE) is a kind of fuzzy
identity-based encryption (fuzzy IBE) where a ciphertext encrypted under an
identity w' can be decrypted using a secret key corresponding to the identity w
which is close to w' as measured by some metric. Recently, Yang et al. proposed
a constant-size Bio-IBE scheme and proved that it is secure against adaptive
chosen-ciphertext attack (CCA2) in the random oracle model. Unfortunately, in
this paper, we will show that their Bio-IBE scheme is even not chosen-plaintext
secure. Specifically, user w using his secret key is able to decrypt any
ciphertext encrypted under an identity w' even though w is not close to w'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2654</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2654</id><created>2011-10-12</created><authors><author><keyname>Nechta</keyname><forenames>Ivan</forenames></author><author><keyname>Fionov</keyname><forenames>Andrei</forenames></author></authors><title>Applying statistical methods to text steganography</title><categories>cs.CR</categories><comments>7 pages, 1 figure and 4 tables</comments><acm-class>K.6.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey of text steganography methods used for hid- ing
secret information inside some covertext. Widely known hiding techniques (such
as translation based steganography, text generating and syntactic embed- ding)
and detection are considered. It is shown that statistical analysis has an
important role in text steganalysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2659</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2659</id><created>2011-10-12</created><authors><author><keyname>Ohara</keyname><forenames>Kouzou</forenames></author><author><keyname>Saito</keyname><forenames>Kazumi</forenames></author><author><keyname>Kimura</keyname><forenames>Masahiro</forenames></author><author><keyname>Motoda</keyname><forenames>Hiroshi</forenames></author></authors><title>Efficient Detection of Hot Span in Information Diffusion from
  Observation</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 11 figures; IJCAI11 Workshop on Link Analysis in
  Heterogeneous Information Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We addressed the problem of detecting the change in behavior of information
diffusion from a small amount of observation data, where the behavior changes
were assumed to be effectively reflected in changes in the diffusion parameter
value. The problem is to detect where in time and how long this change
persisted and how big this change is. We solved this problem by searching the
change pattern that maximizes the likelihood of generating the observed
diffusion sequences. The naive learning algorithm has to iteratively update the
patten boundaries, each requiring optimization of diffusion parameters by the
EM algorithm, and is very inefficient. We devised a very efficient search
algorithm using the derivative of likelihood which avoids parameter value
optimization during the search. The results tested using three real world
network structures confirmed that the algorithm can efficiently identify the
correct change pattern. We further compared our algorithm with the naive method
that finds the best combination of change boundaries by an exhaustive search
through a set of randomly selected boundary candidates, and showed that the
proposed algorithm far outperforms the native method both in terms of accuracy
and computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2677</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2677</id><created>2011-10-12</created><authors><author><keyname>Donfack</keyname><forenames>Simplice</forenames></author><author><keyname>Grigori</keyname><forenames>Laura</forenames></author><author><keyname>Gropp</keyname><forenames>William D.</forenames></author><author><keyname>Kale</keyname><forenames>Vivek</forenames></author></authors><title>Hybrid static/dynamic scheduling for already optimized dense matrix
  factorization</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the use of a hybrid static/dynamic scheduling strategy of the task
dependency graph for direct methods used in dense numerical linear algebra.
This strategy provides a balance of data locality, load balance, and low
dequeue overhead. We show that the usage of this scheduling in communication
avoiding dense factorization leads to significant performance gains. On a 48
core AMD Opteron NUMA machine, our experiments show that we can achieve up to
64% improvement over a version of CALU that uses fully dynamic scheduling, and
up to 30% improvement over the version of CALU that uses fully static
scheduling. On a 16-core Intel Xeon machine, our hybrid static/dynamic
scheduling approach is up to 8% faster than the version of CALU that uses a
fully static scheduling or fully dynamic scheduling. Our algorithm leads to
speedups over the corresponding routines for computing LU factorization in well
known libraries. On the 48 core AMD NUMA machine, our best implementation is up
to 110% faster than MKL, while on the 16 core Intel Xeon machine, it is up to
82% faster than MKL. Our approach also shows significant speedups compared with
PLASMA on both of these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2704</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2704</id><created>2011-10-12</created><authors><author><keyname>Nguyen</keyname><forenames>Huu Hoa</forenames><affiliation>ERIC</affiliation></author><author><keyname>Harbi</keyname><forenames>Nouria</forenames><affiliation>ERIC</affiliation></author><author><keyname>Darmont</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>ERIC</affiliation></author></authors><title>An Efficient Fuzzy Clustering-Based Approach for Intrusion Detection</title><categories>cs.DB</categories><comments>15th East-European Conference on Advances and Databases and
  Information Systems (ADBIS 11), Vienna : Austria (2011)</comments><proxy>ccsd</proxy><report-no>ERIC:11-029</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to increase accuracy in detecting sophisticated cyber attacks poses
a great challenge not only to the research community but also to corporations.
So far, many approaches have been proposed to cope with this threat. Among
them, data mining has brought on remarkable contributions to the intrusion
detection problem. However, the generalization ability of data mining-based
methods remains limited, and hence detecting sophisticated attacks remains a
tough task. In this thread, we present a novel method based on both clustering
and classification for developing an efficient intrusion detection system
(IDS). The key idea is to take useful information exploited from fuzzy
clustering into account for the process of building an IDS. To this aim, we
first present cornerstones to construct additional cluster features for a
training set. Then, we come up with an algorithm to generate an IDS based on
such cluster features and the original input features. Finally, we
experimentally prove that our method outperforms several well-known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2711</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2711</id><created>2011-10-12</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Generalized network community detection</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>Proceedings of the ECML PKDD Workshop on Finding Patterns of Human
  Behaviors in Network and Mobility Data 2011 (NEMO '11), pp. 66-84</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure is largely regarded as an intrinsic property of complex
real-world networks. However, recent studies reveal that networks comprise even
more sophisticated modules than classical cohesive communities. More precisely,
real-world networks can also be naturally partitioned according to common
patterns of connections between the nodes. Recently, a propagation based
algorithm has been proposed for the detection of arbitrary network modules. We
here advance the latter with a more adequate community modeling based on
network clustering. The resulting algorithm is evaluated on various synthetic
benchmark networks and random graphs. It is shown to be comparable to current
state-of-the-art algorithms, however, in contrast to other approaches, it does
not require some prior knowledge of the true community structure. To
demonstrate its generality, we further employ the proposed algorithm for
community detection in different unipartite and bipartite real-world networks,
for generalized community detection and also predictive data clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2712</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2712</id><created>2011-10-12</created><updated>2012-02-21</updated><authors><author><keyname>Levine</keyname><forenames>Lionel</forenames></author><author><keyname>Sheffield</keyname><forenames>Scott</forenames></author><author><keyname>Stange</keyname><forenames>Katherine E.</forenames></author></authors><title>A duality principle for selection games</title><categories>cs.GT math.CO</categories><comments>8 pages, 2 figures</comments><msc-class>91A10, 91A18, 91A06, 91A50</msc-class><journal-ref>Proceedings of the American Mathematical Society, 141 (2013),
  4349-4356</journal-ref><doi>10.1090/S0002-9939-2013-11707-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dinner table seats k guests and holds n discrete morsels of food. Guests
select morsels in turn until all are consumed. Each guest has a ranking of the
morsels according to how much he would enjoy eating them; these rankings are
commonly known.
  A gallant knight always prefers one food division over another if it provides
strictly more enjoyable collections of food to one or more other players
(without giving a less enjoyable collection to any other player) even if it
makes his own collection less enjoyable. A boorish lout always selects the
morsel that gives him the most enjoyment on the current turn, regardless of
future consumption by himself and others.
  We show the way the food is divided when all guests are gallant knights is
the same as when all guests are boorish louts but turn order is reversed. This
implies and generalizes a classical result of Kohler and Chandrasekaran (1971)
about two players strategically maximizing their own enjoyments. We also treat
the case that the table contains a mixture of boorish louts and gallant
knights.
  Our main result can also be formulated in terms of games in which selections
are made by groups. In this formulation, the surprising fact is that a group
can always find a selection that is simultaneously optimal for each member of
the group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2722</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2722</id><created>2011-10-12</created><updated>2012-05-17</updated><authors><author><keyname>Lexa</keyname><forenames>Michael A.</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author><author><keyname>Thompson</keyname><forenames>John S.</forenames></author></authors><title>Compressive and Noncompressive Power Spectral Density Estimation from
  Periodic Nonuniform Samples</title><categories>cs.IT math.IT</categories><comments>26 pages, single spaced, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel power spectral density estimation technique for
band-limited, wide-sense stationary signals from sub-Nyquist sampled data. The
technique employs multi-coset sampling and incorporates the advantages of
compressed sensing (CS) when the power spectrum is sparse, but applies to
sparse and nonsparse power spectra alike. The estimates are consistent
piecewise constant approximations whose resolutions (width of the piecewise
constant segments) are controlled by the periodicity of the multi-coset
sampling. We show that compressive estimates exhibit better tradeoffs among the
estimator's resolution, system complexity, and average sampling rate compared
to their noncompressive counterparts. For suitable sampling patterns,
noncompressive estimates are obtained as least squares solutions. Because of
the non-negativity of power spectra, compressive estimates can be computed by
seeking non-negative least squares solutions (provided appropriate sampling
patterns exist) instead of using standard CS recovery algorithms. This
flexibility suggests a reduction in computational overhead for systems
estimating both sparse and nonsparse power spectra because one algorithm can be
used to compute both compressive and noncompressive estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2724</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2724</id><created>2011-10-12</created><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Information Transfer in Social Media</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has explored the increasingly important role of social media
by examining the dynamics of individual and group behavior, characterizing
patterns of information diffusion, and identifying influential individuals. In
this paper we suggest a measure of causal relationships between nodes based on
the information-theoretic notion of transfer entropy, or information transfer.
This theoretically grounded measure is based on dynamic information, captures
fine-grain notions of influence, and admits a natural, predictive
interpretation. Causal networks inferred by transfer entropy can differ
significantly from static friendship networks because most friendship links are
not useful for predicting future dynamics. We demonstrate through analysis of
synthetic and real-world data that transfer entropy reveals meaningful hidden
network structures. In addition to altering our notion of who is influential,
transfer entropy allows us to differentiate between weak influence over large
groups and strong influence over small groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2726</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2726</id><created>2011-10-12</created><authors><author><keyname>Gabelaia</keyname><forenames>D.</forenames></author><author><keyname>Kontchakov</keyname><forenames>R.</forenames></author><author><keyname>Kurucz</keyname><forenames>A.</forenames></author><author><keyname>Wolter</keyname><forenames>F.</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>M.</forenames></author></authors><title>Combining Spatial and Temporal Logics: Expressiveness vs. Complexity</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 23, pages
  167-243, 2005</journal-ref><doi>10.1613/jair.1537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct and investigate a hierarchy of spatio-temporal
formalisms that result from various combinations of propositional spatial and
temporal logics such as the propositional temporal logic PTL, the spatial
logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a
clear picture of the trade-off between expressiveness and computational
realisability within the hierarchy. We demonstrate how different combining
principles as well as spatial and temporal primitives can produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out
of components that are at most NP- or PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2728</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2728</id><created>2011-10-12</created><authors><author><keyname>Gerevini</keyname><forenames>A.</forenames></author><author><keyname>Saetti</keyname><forenames>A.</forenames></author><author><keyname>Serina</keyname><forenames>I.</forenames></author></authors><title>An Approach to Temporal Planning and Scheduling in Domains with
  Predictable Exogenous Events</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  187-231, 2006</journal-ref><doi>10.1613/jair.1742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The treatment of exogenous events in planning is practically important in
many real-world domains where the preconditions of certain plan actions are
affected by such events. In this paper we focus on planning in temporal domains
with exogenous events that happen at known times, imposing the constraint that
certain actions in the plan must be executed during some predefined time
windows. When actions have durations, handling such temporal constraints adds
an extra difficulty to planning. We propose an approach to planning in these
domains which integrates constraint-based temporal reasoning into a graph-based
planning framework using local search. Our techniques are implemented in a
planner that took part in the 4th International Planning Competition (IPC-4). A
statistical analysis of the results of IPC-4 demonstrates the effectiveness of
our approach in terms of both CPU-time and plan quality. Additional experiments
show the good performance of the temporal reasoning techniques integrated into
our planner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2729</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2729</id><created>2011-10-12</created><authors><author><keyname>Bacchus</keyname><forenames>F.</forenames></author></authors><title>The Power of Modeling - a Response to PDDL2.1</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  125-132, 2003</journal-ref><doi>10.1613/jair.1993</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this commentary I argue that although PDDL is a very useful standard for
the planning competition, its design does not properly consider the issue of
domain modeling. Hence, I would not advocate its use in specifying planning
domains outside of the context of the planning competition. Rather, the field
needs to explore different approaches and grapple more directly with the
problem of effectively modeling and utilizing all of the diverse pieces of
knowledge we typically have about planning domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2730</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2730</id><created>2011-10-12</created><authors><author><keyname>Boddy</keyname><forenames>M. S.</forenames></author></authors><title>Imperfect Match: PDDL 2.1 and Real Applications</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  133-137, 2003</journal-ref><doi>10.1613/jair.1994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PDDL was originally conceived and constructed as a lingua franca for the
International Planning Competition. PDDL2.1 embodies a set of extensions
intended to support the expression of something closer to real planning
problems. This objective has only been partially achieved, due in large part to
a deliberate focus on not moving too far from classical planning models and
solution methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2731</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2731</id><created>2011-10-12</created><authors><author><keyname>Geffner</keyname><forenames>H. A.</forenames></author></authors><title>PDDL 2.1: Representation vs. Computation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 20, pages
  139-144, 2003</journal-ref><doi>10.1613/jair.1995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I comment on the PDDL 2.1 language and its use in the planning competition,
focusing on the choices made for accommodating time and concurrency. I also
discuss some methodological issues that have to do with the move toward more
expressive planning languages and the balance needed in planning research
between semantics and computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2732</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2732</id><created>2011-10-12</created><authors><author><keyname>Beck</keyname><forenames>J. C.</forenames></author><author><keyname>Wilson</keyname><forenames>N.</forenames></author></authors><title>Proactive Algorithms for Job Shop Scheduling with Probabilistic
  Durations</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  183-232, 2007</journal-ref><doi>10.1613/jair.2080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most classical scheduling formulations assume a fixed and known duration for
each activity. In this paper, we weaken this assumption, requiring instead that
each duration can be represented by an independent random variable with a known
mean and variance. The best solutions are ones which have a high probability of
achieving a good makespan. We first create a theoretical framework, formally
showing how Monte Carlo simulation can be combined with deterministic
scheduling algorithms to solve this problem. We propose an associated
deterministic scheduling problem whose solution is proved, under certain
conditions, to be a lower bound for the probabilistic problem. We then propose
and investigate a number of techniques for solving such problems based on
combinations of Monte Carlo simulation, solutions to the associated
deterministic problem, and either constraint programming or tabu search. Our
empirical results demonstrate that a combination of the use of the associated
deterministic problem and Monte Carlo simulation results in algorithms that
scale best both in terms of problem size and uncertainty. Further experiments
point to the correlation between the quality of the deterministic solution and
the quality of the probabilistic solution as a major factor responsible for
this success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2733</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2733</id><created>2011-10-12</created><authors><author><keyname>Blumrosen</keyname><forenames>L.</forenames></author><author><keyname>Nisan</keyname><forenames>N.</forenames></author><author><keyname>Segal</keyname><forenames>I.</forenames></author></authors><title>Auctions with Severely Bounded Communication</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  233-266, 2007</journal-ref><doi>10.1613/jair.2081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study auctions with severe bounds on the communication allowed: each
bidder may only transmit t bits of information to the auctioneer. We consider
both welfare- and profit-maximizing auctions under this communication
restriction. For both measures, we determine the optimal auction and show that
the loss incurred relative to unconstrained auctions is mild. We prove
non-surprising properties of these kinds of auctions, e.g., that in optimal
mechanisms bidders simply report the interval in which their valuation lies in,
as well as some surprising properties, e.g., that asymmetric auctions are
better than symmetric ones and that multi-round auctions reduce the
communication complexity only by a linear factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2734</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2734</id><created>2011-10-12</created><authors><author><keyname>Darwiche</keyname><forenames>A.</forenames></author><author><keyname>Huang</keyname><forenames>J.</forenames></author></authors><title>The Language of Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  191-219, 2007</journal-ref><doi>10.1613/jair.2097</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a class of algorithms that perform exhaustive
search on propositional knowledge bases. We show that each of these algorithms
defines and generates a propositional language. Specifically, we show that the
trace of a search can be interpreted as a combinational circuit, and a search
algorithm then defines a propositional language consisting of circuits that are
generated across all possible executions of the algorithm. In particular, we
show that several versions of exhaustive DPLL search correspond to such
well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF.
By thus mapping search algorithms to propositional languages, we provide a
uniform and practical framework in which successful search techniques can be
harnessed for compilation of knowledge into various languages of interest, and
a new methodology whereby the power and limitations of search algorithms can be
understood by looking up the tractability and succinctness of the corresponding
propositional languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2735</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2735</id><created>2011-10-12</created><authors><author><keyname>Barbulescu</keyname><forenames>L.</forenames></author><author><keyname>Howe</keyname><forenames>A. E.</forenames></author><author><keyname>Roberts</keyname><forenames>M.</forenames></author><author><keyname>Whitley</keyname><forenames>L. D.</forenames></author></authors><title>Understanding Algorithm Performance on an Oversubscribed Scheduling
  Application</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  577-615, 2006</journal-ref><doi>10.1613/jair.2038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best performing algorithms for a particular oversubscribed scheduling
application, Air Force Satellite Control Network (AFSCN) scheduling, appear to
have little in common. Yet, through careful experimentation and modeling of
performance in real problem instances, we can relate characteristics of the
best algorithms to characteristics of the application. In particular, we find
that plateaus dominate the search spaces (thus favoring algorithms that make
larger changes to solutions) and that some randomization in exploration is
critical to good performance (due to the lack of gradient information on the
plateaus). Based on our explanations of algorithm performance, we develop a new
algorithm that combines characteristics of the best performers; the new
algorithms performance is better than the previous best. We show how hypothesis
driven experimentation and search modeling can both explain algorithm
performance and motivate the design of a new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2736</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2736</id><created>2011-10-12</created><authors><author><keyname>Coles</keyname><forenames>A. I.</forenames></author><author><keyname>Smith</keyname><forenames>A. J.</forenames></author></authors><title>Marvin: A Heuristic Search Planner with Online Macro-Action Learning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  119-156, 2007</journal-ref><doi>10.1613/jair.2077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Marvin, a planner that competed in the Fourth
International Planning Competition (IPC 4). Marvin uses
action-sequence-memoisation techniques to generate macro-actions, which are
then used during search for a solution plan. We provide an overview of its
architecture and search behaviour, detailing the algorithms used. We also
empirically demonstrate the effectiveness of its features in various planning
domains; in particular, the effects on performance due to the use of
macro-actions, the novel features of its search behaviour, and the native
support of ADL and Derived Predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2737</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2737</id><created>2011-10-12</created><authors><author><keyname>Hansen</keyname><forenames>E. A.</forenames></author><author><keyname>Zhou</keyname><forenames>R.</forenames></author></authors><title>Anytime Heuristic Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  267-297, 2007</journal-ref><doi>10.1613/jair.2096</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how to convert the heuristic search algorithm A* into an anytime
algorithm that finds a sequence of improved solutions and eventually converges
to an optimal solution. The approach we adopt uses weighted heuristic search to
find an approximate solution quickly, and then continues the weighted search to
find improved solutions as well as to improve a bound on the suboptimality of
the current solution. When the time available to solve a search problem is
limited or uncertain, this creates an anytime heuristic search algorithm that
allows a flexible tradeoff between search time and solution quality. We analyze
the properties of the resulting Anytime A* algorithm, and consider its
performance in three domains; sliding-tile puzzles, STRIPS planning, and
multiple sequence alignment. To illustrate the generality of this approach, we
also describe how to transform the memory-efficient search algorithm Recursive
Best-First Search (RBFS) into an anytime algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2738</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2738</id><created>2011-10-12</created><authors><author><keyname>Chen</keyname><forenames>Y.</forenames></author><author><keyname>Lin</keyname><forenames>F.</forenames></author></authors><title>Discovering Classes of Strongly Equivalent Logic Programs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  431-451, 2007</journal-ref><doi>10.1613/jair.2131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply computer-aided theorem discovery technique to discover
theorems about strongly equivalent logic programs under the answer set
semantics. Our discovered theorems capture new classes of strongly equivalent
logic programs that can lead to new program simplification rules that preserve
strong equivalence. Specifically, with the help of computers, we discovered
exact conditions that capture the strong equivalence between a rule and the
empty set, between two rules, between two rules and one of the two rules,
between two rules and another rule, and between three rules and two of the
three rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2739</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2739</id><created>2011-10-12</created><authors><author><keyname>Creignou</keyname><forenames>N.</forenames></author><author><keyname>Daude</keyname><forenames>H.</forenames></author><author><keyname>Egly</keyname><forenames>U.</forenames></author></authors><title>Phase Transition for Random Quantified XOR-Formulas</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  1-18, 2007</journal-ref><doi>10.1613/jair.2120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The QXORSAT problem is the quantified version of the satisfiability problem
XORSAT in which the connective exclusive-or is used instead of the usual or. We
study the phase transition associated with random QXORSAT instances. We give a
description of this phase transition in the case of one alternation of
quantifiers, thus performing an advanced practical and theoretical study on the
phase transition of a quantified roblem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2740</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2740</id><created>2011-10-12</created><authors><author><keyname>Bidyuk</keyname><forenames>B.</forenames></author><author><keyname>Dechter</keyname><forenames>R.</forenames></author></authors><title>Cutset Sampling for Bayesian Networks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  1-48, 2007</journal-ref><doi>10.1613/jair.2149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new sampling methodology for Bayesian networks that
samples only a subset of variables and applies exact inference to the rest.
Cutset sampling is a network structure-exploiting application of the
Rao-Blackwellisation principle to sampling in Bayesian networks. It improves
convergence by exploiting memory-based inference algorithms. It can also be
viewed as an anytime approximation of the exact cutset-conditioning algorithm
developed by Pearl. Cutset sampling can be implemented efficiently when the
sampled variables constitute a loop-cutset of the Bayesian network and, more
generally, when the induced width of the networks graph conditioned on the
observed sampled variables is bounded by a constant w. We demonstrate
empirically the benefit of this scheme on a range of benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2741</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2741</id><created>2011-10-12</created><authors><author><keyname>Pralet</keyname><forenames>C.</forenames></author><author><keyname>Schiex</keyname><forenames>T.</forenames></author><author><keyname>Verfaillie</keyname><forenames>G.</forenames></author></authors><title>An Algebraic Graphical Model for Decision with Uncertainties,
  Feasibilities, and Utilities</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  421-489, 2007</journal-ref><doi>10.1613/jair.2151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous formalisms and dedicated algorithms have been designed in the last
decades to model and solve decision making problems. Some formalisms, such as
constraint networks, can express &quot;simple&quot; decision problems, while others are
designed to take into account uncertainties, unfeasible decisions, and
utilities. Even in a single formalism, several variants are often proposed to
model different types of uncertainty (probability, possibility...) or utility
(additive or not). In this article, we introduce an algebraic graphical model
that encompasses a large number of such formalisms: (1) we first adapt previous
structures from Friedman, Chu and Halpern for representing uncertainty,
utility, and expected utility in order to deal with generic forms of sequential
decision making; (2) on these structures, we then introduce composite graphical
models that express information via variables linked by &quot;local&quot; functions,
thanks to conditional independence; (3) on these graphical models, we finally
define a simple class of queries which can represent various scenarios in terms
of observabilities and controllabilities. A natural decision-tree semantics for
such queries is completed by an equivalent operational semantics, which induces
generic algorithms. The proposed framework, called the
Plausibility-Feasibility-Utility (PFU) framework, not only provides a better
understanding of the links between existing formalisms, but it also covers yet
unpublished frameworks (such as possibilistic influence diagrams) and unifies
formalisms such as quantified boolean formulas and influence diagrams. Our
backtrack and variable elimination generic algorithms are a first step towards
unified algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2742</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2742</id><created>2011-10-12</created><authors><author><keyname>Di Noia</keyname><forenames>T.</forenames></author><author><keyname>Di Sciascio</keyname><forenames>E.</forenames></author><author><keyname>Donini</keyname><forenames>F. M.</forenames></author></authors><title>Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic
  Approach</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  269-307, 2007</journal-ref><doi>10.1613/jair.2153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matchmaking arises when supply and demand meet in an electronic marketplace,
or when agents search for a web service to perform some task, or even when
recruiting agencies match curricula and job profiles. In such open
environments, the objective of a matchmaking process is to discover best
available offers to a given request. We address the problem of matchmaking from
a knowledge representation perspective, with a formalization based on
Description Logics. We devise Concept Abduction and Concept Contraction as
non-monotonic inferences in Description Logics suitable for modeling
matchmaking in a logical framework, and prove some related complexity results.
We also present reasonable algorithms for semantic matchmaking based on the
devised inferences, and prove that they obey to some commonsense properties.
Finally, we report on the implementation of the proposed matchmaking framework,
which has been used both as a mediator in e-marketplaces and for semantic web
services discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2743</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2743</id><created>2011-10-12</created><authors><author><keyname>Beck</keyname><forenames>J. C.</forenames></author></authors><title>Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  49-77, 2007</journal-ref><doi>10.1613/jair.2169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel
constructive search technique that performs a series of resource-limited tree
searches where each search begins either from an empty solution (as in
randomized restart) or from a solution that has been encountered during the
search. A small number of these &quot;elite solutions is maintained during the
search. We introduce the technique and perform three sets of experiments on the
job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS
is carried out to evaluate the performance impact of various parameter
settings. Second, we inquire into the diversity of the elite solution set,
showing, contrary to expectations, that a less diverse set leads to stronger
performance. Finally, we compare the best parameter setting of SGMPCS from the
first two experiments to chronological backtracking, limited discrepancy
search, randomized restart, and a sophisticated tabu search algorithm on a set
of well-known benchmark problems. Results demonstrate that SGMPCS is
significantly better than the other constructive techniques tested, though lags
behind the tabu search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2753</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2753</id><created>2011-10-12</created><updated>2012-05-05</updated><authors><author><keyname>Zhu</keyname><forenames>Ji</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Stability of a Peer-to-Peer Communication System</title><categories>cs.PF math.PR</categories><comments>33 pages, 6 figures</comments><acm-class>C.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the stationary portion of file download in an
unstructured peer-to-peer network, which typically follows for many hours after
a flash crowd initiation. The model includes the case that peers can have some
pieces at the time of arrival. The contribution of the paper is to identify how
much help is needed from the seeds, either fixed seeds or peer seeds (which are
peers remaining in the system after obtaining a complete collection) to
stabilize the system. The dominant cause for instability is the missing piece
syndrome, whereby one piece becomes very rare in the network. It is shown that
stability can be achieved with only a small amount of help from peer
seeds--even with very little help from a fixed seed, peers need dwell as peer
seeds on average only long enough to upload one additional piece. The region of
stability is insensitive to the piece selection policy. Network coding can
substantially increase the region of stability in case a portion of the new
peers arrive with randomly coded pieces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2755</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2755</id><created>2011-10-12</created><updated>2012-07-10</updated><authors><author><keyname>Gyorgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Efficient Tracking of Large Classes of Experts</title><categories>cs.LG cs.IT math.IT</categories><comments>17 pages, to appear in the IEEE Transactions on Information Theory</comments><msc-class>68Q32, 68P30</msc-class><acm-class>I.2.6; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of prediction of individual sequences, sequential prediction
methods are to be constructed that perform nearly as well as the best expert
from a given class. We consider prediction strategies that compete with the
class of switching strategies that can segment a given sequence into several
blocks, and follow the advice of a different &quot;base&quot; expert in each block. As
usual, the performance of the algorithm is measured by the regret defined as
the excess loss relative to the best switching strategy selected in hindsight
for the particular sequence to be predicted. In this paper we construct
prediction strategies of low computational cost for the case where the set of
base experts is large. In particular we provide a method that can transform any
prediction algorithm $\A$ that is designed for the base class into a tracking
algorithm. The resulting tracking algorithm can take advantage of the
prediction performance and potential computational efficiency of $\A$ in the
sense that it can be implemented with time and space complexity only
$O(n^{\gamma} \ln n)$ times larger than that of $\A$, where $n$ is the time
horizon and $\gamma \ge 0$ is a parameter of the algorithm. With $\A$ properly
chosen, our algorithm achieves a regret bound of optimal order for $\gamma&gt;0$,
and only $O(\ln n)$ times larger than the optimal order for $\gamma=0$ for all
typical regret bound types we examined. For example, for predicting binary
sequences with switching parameters under the logarithmic loss, our method
achieves the optimal $O(\ln n)$ regret rate with time complexity
$O(n^{1+\gamma}\ln n)$ for any $\gamma\in (0,1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2765</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2765</id><created>2011-10-12</created><authors><author><keyname>Fatima</keyname><forenames>S. S.</forenames></author><author><keyname>Jennings</keyname><forenames>N. R.</forenames></author><author><keyname>Wooldridge</keyname><forenames>M. J.</forenames></author></authors><title>Multi-Issue Negotiation with Deadlines</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  381-417, 2006</journal-ref><doi>10.1613/jair.2056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies bilateral multi-issue negotiation between self-interested
autonomous agents. Now, there are a number of different procedures that can be
used for this process; the three main ones being the package deal procedure in
which all the issues are bundled and discussed together, the simultaneous
procedure in which the issues are discussed simultaneously but independently of
each other, and the sequential procedure in which the issues are discussed one
after another. Since each of them yields a different outcome, a key problem is
to decide which one to use in which circumstances. Specifically, we consider
this question for a model in which the agents have time constraints (in the
form of both deadlines and discount factors) and information uncertainty (in
that the agents do not know the opponents utility function). For this model, we
consider issues that are both independent and those that are interdependent and
determine equilibria for each case for each procedure. In so doing, we show
that the package deal is in fact the optimal procedure for each party. We then
go on to show that, although the package deal may be computationally more
complex than the other two procedures, it generates Pareto optimal outcomes
(unlike the other two), it has similar earliest and latest possible times of
agreement to the simultaneous procedure (which is better than the sequential
procedure), and that it (like the other two procedures) generates a unique
outcome only under certain conditions (which we define).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2766</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2766</id><created>2011-10-12</created><authors><author><keyname>Everaere</keyname><forenames>P.</forenames></author><author><keyname>Konieczny</keyname><forenames>S.</forenames></author><author><keyname>Marquis</keyname><forenames>P.</forenames></author></authors><title>The Strategy-Proofness Landscape of Merging</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 28, pages
  49-105, 2007</journal-ref><doi>10.1613/jair.2034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merging operators aim at defining the beliefs/goals of a group of agents from
the beliefs/goals of each member of the group. Whenever an agent of the group
has preferences over the possible results of the merging process (i.e., the
possible merged bases), she can try to rig the merging process by lying on her
true beliefs/goals if this leads to better merged base according to her point
of view. Obviously, strategy-proof operators are highly desirable in order to
guarantee equity among agents even when some of them are not sincere. In this
paper, we draw the strategy-proof landscape for many merging operators from the
literature, including model-based ones and formula-based ones. Both the general
case and several restrictions on the merging process are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2767</identifier>
 <datestamp>2011-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2767</id><created>2011-10-12</created><authors><author><keyname>Dolgov</keyname><forenames>D. A.</forenames></author><author><keyname>Durfee</keyname><forenames>E. H.</forenames></author></authors><title>Resource Allocation Among Agents with MDP-Induced Preferences</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 27, pages
  505-549, 2006</journal-ref><doi>10.1613/jair.2102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Allocating scarce resources among agents to maximize global utility is, in
general, computationally challenging. We focus on problems where resources
enable agents to execute actions in stochastic environments, modeled as Markov
decision processes (MDPs), such that the value of a resource bundle is defined
as the expected value of the optimal MDP policy realizable given these
resources. We present an algorithm that simultaneously solves the
resource-allocation and the policy-optimization problems. This allows us to
avoid explicitly representing utilities over exponentially many resource
bundles, leading to drastic (often exponential) reductions in computational
complexity. We then use this algorithm in the context of self-interested agents
to design a combinatorial auction for allocating resources. We empirically
demonstrate the effectiveness of our approach by showing that it can, in
minutes, optimally solve problems for which a straightforward combinatorial
resource-allocation technique would require the agents to enumerate up to 2^100
resource bundles and the auctioneer to solve an NP-complete problem with an
input of that size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2773</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2773</id><created>2011-10-12</created><authors><author><keyname>Feier</keyname><forenames>Cristina</forenames></author><author><keyname>Heymans</keyname><forenames>Stijn</forenames></author></authors><title>Reasoning with Forest Logic Programs and f-hybrid Knowledge Bases</title><categories>cs.LO</categories><acm-class>F.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open Answer Set Programming (OASP) is an undecidable framework for
integrating ontologies and rules. Although several decidable fragments of OASP
have been identified, few reasoning procedures exist. In this article, we
provide a sound, complete, and terminating algorithm for satisfiability
checking w.r.t. Forest Logic Programs (FoLPs), a fragment of OASP where rules
have a tree shape and allow for inequality atoms and constants. The algorithm
establishes a decidability result for FoLPs. Although believed to be decidable,
so far only the decidability for two small subsets of FoLPs, local FoLPs and
acyclic FoLPs, has been shown. We further introduce f-hybrid knowledge bases, a
hybrid framework where \SHOQ{} knowledge bases and forest logic programs
co-exist, and we show that reasoning with such knowledge bases can be reduced
to reasoning with forest logic programs only. We note that f-hybrid knowledge
bases do not require the usual (weakly) DL-safety of the rule component,
providing thus a genuine alternative approach to current integration approaches
of ontologies and rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2776</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2776</id><created>2011-10-12</created><updated>2012-04-10</updated><authors><author><keyname>Tan</keyname><forenames>Tony</forenames></author></authors><title>Graph Reachability and Pebble Automata over Infinite Alphabets</title><categories>cs.FL cs.LO</categories><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let D denote an infinite alphabet -- a set that consists of infinitely many
symbols. A word w = a_0 b_0 a_1 b_1 ... a_n b_n of even length over D can be
viewed as a directed graph G_w whose vertices are the symbols that appear in w,
and the edges are (a_0,b_0),(a_1,b_1),...,(a_n,b_n). For a positive integer m,
define a language R_m such that a word w = a_0 b_0 ... a_n b_n is in R_m if and
only if there is a path in the graph G_w of length &lt;= m from the vertex a_0 to
the vertex b_n.
  We establish the following hierarchy theorem for pebble automata over
infinite alphabet. For every positive integer k, (i) there exists a k-pebble
automaton that accepts the language R_{2^k-1}; (ii) there is no k-pebble
automaton that accepts the language R_{2^{k+1} - 2}. Based on this result, we
establish a number of previously unknown relations among some classes of
languages over infinite alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2809</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2809</id><created>2011-10-12</created><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>The complexity of conservative valued CSPs</title><categories>cs.CC</categories><comments>38 pages. Full version of the paper that will appear in SODA 12</comments><journal-ref>Journal of the ACM 60(2) Article No. 10 (2013)</journal-ref><doi>10.1145/2450142.2450146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of valued constraint satisfaction problems (VCSP). A
problem from VCSP is characterised by a \emph{constraint language}, a fixed set
of cost functions over a finite domain. An instance of the problem is specified
by a sum of cost functions from the language and the goal is to minimise the
sum.
  We consider the case of languages containing all possible unary cost
functions. In the case of languages consisting of only $\{0,\infty\}$-valued
cost functions (i.e. relations), such languages have been called
\emph{conservative} and studied by Bulatov [LICS'03] and recently by Barto
[LICS'11]. Since we study valued languages, we call a language conservative if
it contains all finite-valued unary cost functions. The complexity of
conservative valued languages has been studied by Cohen et al. [AIJ'06] for
languages over Boolean domains, by Deineko et al. [JACM'08] for
$\{0,1\}$-valued languages (a.k.a Max-CSP), and by Takhanov [STACS'10] for
$\{0,\infty\}$-valued languages containing all finite-valued unary cost
functions (a.k.a. Min-Cost-Hom).
  We prove a Schaefer-like dichotomy theorem for conservative valued languages:
if all cost functions in the language satisfy a certain condition (specified by
a complementary combination of \emph{STP and MJN multimorphisms}), then any
instance can be solved in polytime (via a new algorithm developed in this
paper), otherwise the language is NP-hard. This is the \emph{first} complete
complexity classification of \emph{general-valued constraint languages} over
non-Boolean domains.
  This generalises previous results by Takhanov [STACS'10] and (a subset of
results) by Cohen et al. [AIJ'06] and Deineko et al. [JACM'08]. Moreover, our
results do not rely on any computer-assisted search as in Deineko et al.
[JACM'08], and provide a powerful tool for proving hardness of finite- and
general-valued languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2813</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2813</id><created>2011-10-12</created><updated>2013-05-26</updated><authors><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>Towards Quantifying Vertex Similarity in Networks</title><categories>cs.SI physics.soc-ph</categories><comments>16 papers, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vertex similarity is a major problem in network science with a wide range of
applications. In this work we provide novel perspectives on finding
(dis)similar vertices within a network and across two networks with the same
number of vertices (graph matching). With respect to the former problem, we
propose to optimize a geometric objective which allows us to express each
vertex uniquely as a convex combination of a few extreme types of vertices. Our
method has the important advantage of supporting efficiently several types of
queries such as &quot;which other vertices are most similar to this vertex?&quot; by the
use of the appropriate data structures and of mining interesting patterns in
the network. With respect to the latter problem (graph matching), we propose
the generalized condition number --a quantity widely used in numerical
analysis-- $\kappa(L_G,L_H)$ of the Laplacian matrix representations of $G,H$
as a measure of graph similarity, where $G,H$ are the graphs of interest. We
show that this objective has a solid theoretical basis and propose a
deterministic and a randomized graph alignment algorithm. We evaluate our
algorithms on both synthetic and real data. We observe that our proposed
methods achieve high-quality results and provide us with significant insights
into the network structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2825</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2825</id><created>2011-10-12</created><updated>2012-03-11</updated><authors><author><keyname>Lee</keyname><forenames>Deok-Sun</forenames></author><author><keyname>Maeng</keyname><forenames>Seong Eun</forenames></author><author><keyname>Lee</keyname><forenames>Jae Woo</forenames></author></authors><title>Scaling of nestedness in complex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>9 pages, 4 figures, final version</comments><journal-ref>J. Korean Phys. Soc. 60, 648 (2012)</journal-ref><doi>10.3938/jkps.60.648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nestedness characterizes the linkage pattern of networked systems, indicating
the likelihood that a node is linked to the nodes linked to the nodes with
larger degrees than it. Networks of mutualistic relationship between distinct
groups of species in ecological communities exhibit such nestedness, which is
known to support the network robustness. Despite such importance, quantitative
characteristics of nestedness is little understood. Here we take
graph-theoretic approach to derive the scaling properties of nestedness in
various model networks. Our results show how the heterogeneous connectivity
patterns enhance nestedness. Also we find that the nestedness of bipartite
networks depend sensitively on the fraction of different types of nodes,
causing nestedness to scale differently for nodes of different types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2828</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2828</id><created>2011-10-12</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Fox</keyname><forenames>Jacob</forenames></author></authors><title>Testing perfection is hard</title><categories>math.CO cs.DM cs.DS</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph property P is strongly testable if for every fixed \epsilon&gt;0 there
is a one-sided \epsilon-tester for P whose query complexity is bounded by a
function of \epsilon. In classifying the strongly testable graph properties,
the first author and Shapira showed that any hereditary graph property (such as
P the family of perfect graphs) is strongly testable. A property is easily
testable if it is strongly testable with query complexity bounded by a
polynomial function of \epsilon^{-1}, and otherwise it is hard. One of our main
results shows that testing perfectness is hard. The proof shows that testing
perfectness is at least as hard as testing triangle-freeness, which is hard. On
the other hand, we show that induced P_3-freeness is easily testable. This
settles one of the two exceptional graphs, the other being C_4 (and its
complement), left open in the characterization by the first author and Shapira
of graphs H for which induced H-freeness is easily testable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2834</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2834</id><created>2011-10-13</created><updated>2012-03-11</updated><authors><author><keyname>Maeng</keyname><forenames>Seong Eun</forenames></author><author><keyname>Lee</keyname><forenames>Jae Woo</forenames></author><author><keyname>Lee</keyname><forenames>Deok-Sun</forenames></author></authors><title>Interspecific competition underlying mutualistic networks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>5 pages, 3 figures, accepted version in PRL</comments><journal-ref>Physical Review Letters 108, 108701 (2012)</journal-ref><doi>10.1103/PhysRevLett.108.108701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The architecture of bipartite networks linking two classes of constituents is
affected by the interactions within each class. For the bipartite networks
representing the mutualistic relationship between pollinating animals and
plants, it has been known that their degree distributions are broad but often
deviate from power-law form, more significantly for plants than animals. Here
we consider a model for the evolution of the mutualistic networks and find that
their topology is strongly dependent on the asymmetry and non-linearity of the
preferential selection of mutualistic partners. Real-world mutualistic networks
analyzed in the framework of the model show that a new animal species
determines its partners not only by their attractiveness but also as a result
of the competition with pre-existing animals, which leads to the
stretched-exponential degree distributions of plant species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2842</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2842</id><created>2011-10-13</created><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>A tail inequality for quadratic forms of subgaussian random vectors</title><categories>math.PR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an exponential probability tail inequality for positive semidefinite
quadratic forms in a subgaussian random vector. The bound is analogous to one
that holds when the vector has independent Gaussian entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2849</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2849</id><created>2011-10-13</created><authors><author><keyname>Jayaraman</keyname><forenames>Karthick</forenames></author><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author><author><keyname>Tripunitara</keyname><forenames>Mahesh</forenames></author><author><keyname>Rinard</keyname><forenames>Martin C</forenames></author><author><keyname>Chapin</keyname><forenames>Steve J.</forenames></author></authors><title>ARBAC Policy for a Large Multi-National Bank</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Administrative role-based access control (ARBAC) is the first comprehensive
administrative model proposed for role-based access control (RBAC). ARBAC has
several features for designing highly expressive policies, but current work has
not highlighted the utility of these expressive policies. In this report, we
present a case study of designing an ARBAC policy for a bank comprising 18
branches. Using this case study we provide an assessment about the features of
ARBAC that are likely to be used in realistic policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2855</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2855</id><created>2011-10-13</created><authors><author><keyname>Beno&#xee;t</keyname><forenames>Louise</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Ponce</keyname><forenames>Jean</forenames><affiliation>INRIA Paris - Rocquencourt</affiliation></author></authors><title>Sparse Image Representation with Epitomes</title><categories>cs.LG cs.CV stat.ML</categories><comments>Computer Vision and Pattern Recognition, Colorado Springs : United
  States (2011)</comments><proxy>ccsd</proxy><journal-ref>Computer Vision and Pattern Recognition, Colorado Springs :
  \'Etats-Unis (2011)</journal-ref><doi>10.1109/CVPR.2011.5995636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding, which is the decomposition of a vector using only a few basis
elements, is widely used in machine learning and image processing. The basis
set, also called dictionary, is learned to adapt to specific data. This
approach has proven to be very effective in many image processing tasks.
Traditionally, the dictionary is an unstructured &quot;flat&quot; set of atoms. In this
paper, we study structured dictionaries which are obtained from an epitome, or
a set of epitomes. The epitome is itself a small image, and the atoms are all
the patches of a chosen size inside this image. This considerably reduces the
number of parameters to learn and provides sparse image decompositions with
shiftinvariance properties. We propose a new formulation and an algorithm for
learning the structured dictionaries associated with epitomes, and illustrate
their use in image denoising tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2859</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2859</id><created>2011-10-13</created><authors><author><keyname>Elfaki</keyname><forenames>Abdelrahman Osman</forenames></author><author><keyname>Johar</keyname><forenames>Md Gapar Md</forenames></author><author><keyname>Aik</keyname><forenames>Kevin Loo Teow</forenames></author><author><keyname>Fong</keyname><forenames>Sim Liew</forenames></author><author><keyname>Bachok</keyname><forenames>Ruzi</forenames></author></authors><title>Towards Representation and Validation of Knowledge in Students' Learning
  Pathway Using Variability Modeling Technique</title><categories>cs.CY</categories><journal-ref>Journal: International Journal of Computer Science Issues Issn:
  16940784 EIssn: 16940814 Year: 2011 Volume: 8 Issue: 3 pages/rec.No: 30-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, E-learning system is considered as one of the main pillars in the
learning system. Mainly, E-Learning system is designed to serve different types
of students. Thus, providing different learning pathways are a must. In this
paper, we introduce the variability technique to represent the knowledge in
E-learning system. This representation provides different learning pathways
which supports the students' diversity. Moreover, we validate the selection of
learning pathway by introducing First Order Logic (FOL) rules. Keywords
Learning Pathway ; Variability and knowledge representation ; IJCSI
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2867</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2867</id><created>2011-10-13</created><updated>2012-03-22</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Robust Beamforming in Interference Channels with Imperfect Transmitter
  Channel Information</title><categories>cs.IT math.IT</categories><comments>27 pages, 5 figures, EURASIP Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider $K$ links operating concurrently in the same spectral band. Each
transmitter has multiple antennas, while each receiver uses a single antenna.
This setting corresponds to the multiple-input single-output interference
channel. We assume perfect channel state information at the single-user
decoding receivers whereas the transmitters only have estimates of the true
channels. The channel estimation errors are assumed to be bounded in elliptical
regions whose geometry is known at the transmitters. Robust beamforming
optimizes worst-case received power gains, and a Pareto optimal point is a
worst-case achievable rate tuple from which it is impossible to increase a
link's performance without degrading the performance of another. We
characterize the robust beamforming vectors necessary to operate at any Pareto
optimal point. Moreover, these beamforming vectors are parameterized by
$K(K-1)$ real-valued parameters. We analyze the system's spectral efficiency at
high and low signal-to-noise ratio (SNR). Zero forcing transmission achieves
full multiplexing gain at high SNR only if the estimation errors scale linearly
with inverse SNR. If the errors are SNR independent, then single-user
transmission is optimal at high SNR. At low SNR, robust maximum ratio
transmission optimizes the minimum energy per bit for reliable communication.
Numerical simulations illustrate the gained theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2872</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2872</id><created>2011-10-13</created><updated>2013-06-28</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Exchange Economy in Two-User Multiple-Input Single-Output Interference
  Channels</title><categories>cs.GT cs.IT math.IT</categories><comments>30 pages, 9 figures</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing, vol. 6, no.
  2, Apr. 2012</journal-ref><doi>10.1109/JSTSP.2011.2174962</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the conflict between two links in a multiple-input single-output
interference channel. This setting is strictly competitive and can be related
to perfectly competitive market models. In such models, general equilibrium
theory is used to determine equilibrium measures that are Pareto optimal.
First, we consider the links to be consumers that can trade goods within
themselves. The goods in our setting correspond to beamforming vectors. We
utilize the conflict representation of the consumers in the Edgeworth box, a
graphical tool that depicts the allocation of the goods for the two consumers,
to provide closed-form solution to all Pareto optimal outcomes. Afterwards, we
model the situation between the links as a competitive market which
additionally defines prices for the goods. The equilibrium in this economy is
called Walrasian and corresponds to the prices that equate the demand to the
supply of goods. We calculate the unique Walrasian equilibrium and propose a
coordination process that is realized by an arbitrator which distributes the
Walrasian prices to the consumers. The consumers then calculate in a
decentralized manner their optimal demand corresponding to beamforming vectors
that achieve the Walrasian equilibrium. This outcome is Pareto optimal and
dominates the noncooperative outcome of the systems. Thus, based on the game
theoretic model and solution concept, an algorithm for a distributed
implementation of the beamforming problem in multiple-input single-output
interference channels is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2890</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2890</id><created>2011-10-13</created><authors><author><keyname>Zhou</keyname><forenames>Rui</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Li</keyname><forenames>Jianxin</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author></authors><title>ELCA Evaluation for Keyword Search on Probabilistic XML Data</title><categories>cs.DB</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As probabilistic data management is becoming one of the main research focuses
and keyword search is turning into a more popular query means, it is natural to
think how to support keyword queries on probabilistic XML data. With regards to
keyword query on deterministic XML documents, ELCA (Exclusive Lowest Common
Ancestor) semantics allows more relevant fragments rooted at the ELCAs to
appear as results and is more popular compared with other keyword query result
semantics (such as SLCAs).
  In this paper, we investigate how to evaluate ELCA results for keyword
queries on probabilistic XML documents. After defining probabilistic ELCA
semantics in terms of possible world semantics, we propose an approach to
compute ELCA probabilities without generating possible worlds. Then we develop
an efficient stack-based algorithm that can find all probabilistic ELCA results
and their ELCA probabilities for a given keyword query on a probabilistic XML
document. Finally, we experimentally evaluate the proposed ELCA algorithm and
compare it with its SLCA counterpart in aspects of result effectiveness, time
and space efficiency, and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2893</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2893</id><created>2011-10-13</created><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author><author><keyname>Wind</keyname><forenames>David Kofoed</forenames></author></authors><title>String Matching with Variable Length Gaps</title><categories>cs.DS</categories><comments>draft of full version, extended abstract at SPIRE 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider string matching with variable length gaps. Given a string $T$ and
a pattern $P$ consisting of strings separated by variable length gaps
(arbitrary strings of length in a specified range), the problem is to find all
ending positions of substrings in $T$ that match $P$. This problem is a basic
primitive in computational biology applications. Let $m$ and $n$ be the lengths
of $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We
present a new algorithm achieving time $O(n\log k + m +\alpha)$ and space $O(m
+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in
$P$ and $\alpha$ is the total number of occurrences of the strings in $P$
within $T$. Compared to the previous results this bound essentially achieves
the best known time and space complexities simultaneously. Consequently, our
algorithm obtains the best known bounds for almost all combinations of $m$,
$n$, $k$, $A$, and $\alpha$. Our algorithm is surprisingly simple and
straightforward to implement. We also present algorithms for finding and
encoding the positions of all strings in $P$ for every match of the pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2897</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2897</id><created>2011-10-13</created><updated>2014-11-04</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Randomized Dimensionality Reduction for k-means Clustering</title><categories>cs.DS cs.LG</categories><comments>IEEE Transactions on Information Theory, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the topic of dimensionality reduction for $k$-means clustering.
Dimensionality reduction encompasses the union of two approaches: \emph{feature
selection} and \emph{feature extraction}. A feature selection based algorithm
for $k$-means clustering selects a small subset of the input features and then
applies $k$-means clustering on the selected features. A feature extraction
based algorithm for $k$-means clustering constructs a small set of new
artificial features and then applies $k$-means clustering on the constructed
features. Despite the significance of $k$-means clustering as well as the
wealth of heuristic methods addressing it, provably accurate feature selection
methods for $k$-means clustering are not known. On the other hand, two provably
accurate feature extraction methods for $k$-means clustering are known in the
literature; one is based on random projections and the other is based on the
singular value decomposition (SVD).
  This paper makes further progress towards a better understanding of
dimensionality reduction for $k$-means clustering. Namely, we present the first
provably accurate feature selection method for $k$-means clustering and, in
addition, we present two feature extraction methods. The first feature
extraction method is based on random projections and it improves upon the
existing results in terms of time complexity and number of features needed to
be extracted. The second feature extraction method is based on fast approximate
SVD factorizations and it also improves upon the existing results in terms of
time complexity. The proposed algorithms are randomized and provide
constant-factor approximation guarantees with respect to the optimal $k$-means
objective value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2899</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2899</id><created>2011-10-13</created><authors><author><keyname>Takahashi</keyname><forenames>Toshimitsu</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Yamanishi</keyname><forenames>Kenji</forenames></author></authors><title>Discovering Emerging Topics in Social Streams via Link Anomaly Detection</title><categories>stat.ML cs.LG cs.SI physics.soc-ph</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of emerging topics are now receiving renewed interest motivated by
the rapid growth of social networks. Conventional term-frequency-based
approaches may not be appropriate in this context, because the information
exchanged are not only texts but also images, URLs, and videos. We focus on the
social aspects of theses networks. That is, the links between users that are
generated dynamically intentionally or unintentionally through replies,
mentions, and retweets. We propose a probability model of the mentioning
behaviour of a social network user, and propose to detect the emergence of a
new topic from the anomaly measured through the model. We combine the proposed
mention anomaly score with a recently proposed change-point detection technique
based on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or
with Kleinberg's burst model. Aggregating anomaly scores from hundreds of
users, we show that we can detect emerging topics only based on the
reply/mention relationships in social network posts. We demonstrate our
technique in a number of real data sets we gathered from Twitter. The
experiments show that the proposed mention-anomaly-based approaches can detect
new topics at least as early as the conventional term-frequency-based approach,
and sometimes much earlier when the keyword is ill-defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2906</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2906</id><created>2011-10-13</created><authors><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author><author><keyname>Poria</keyname><forenames>Swarup</forenames></author></authors><title>Multiple dynamical time-scales in networks with hierarchically nested
  modular organization</title><categories>physics.soc-ph cs.SI physics.bio-ph</categories><comments>10 pages, 4 figures</comments><doi>10.1007/s12043-011-0196-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many natural and engineered complex networks have intricate mesoscopic
organization, e.g., the clustering of the constituent nodes into several
communities or modules. Often, such modularity is manifested at several
different hierarchical levels, where the clusters defined at one level appear
as elementary entities at the next higher level. Using a simple model of a
hierarchical modular network, we show that such a topological structure gives
rise to characteristic time-scale separation between dynamics occurring at
different levels of the hierarchy. This generalizes our earlier result for
simple modular networks, where fast intra-modular and slow inter-modular
processes were clearly distinguished. Investigating the process of
synchronization of oscillators in a hierarchical modular network, we show the
existence of as many distinct time-scales as there are hierarchical levels in
the system. This suggests a possible functional role of such mesoscopic
organization principle in natural systems, viz., in the dynamical separation of
events occurring at different spatial scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2907</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2907</id><created>2011-10-13</created><updated>2013-06-06</updated><authors><author><keyname>Wen</keyname><forenames>Fuxi</forenames></author></authors><title>System Identification Using Reweighted Zero Attracting Least Absolute
  Deviation Algorithm</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the l1 norm penalty on the filter coefficients is incorporated
in the least mean absolute deviation (LAD) algorithm to improve the performance
of the LAD algorithm. The performance of LAD, zero-attracting LAD (ZA-LAD) and
reweighted zero-attracting LAD (RZA-LAD) are evaluated for linear time varying
system identification under the non-Gaussian (alpha-stable) noise environments.
Effectiveness of the ZA-LAD type algorithms is demonstrated through computer
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2921</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2921</id><created>2011-10-13</created><updated>2012-08-20</updated><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Barba</keyname><forenames>L. A.</forenames></author></authors><title>FMM-based vortex method for simulation of isotropic turbulence on GPUs,
  compared with a spectral method</title><categories>cs.NA physics.comp-ph physics.flu-dyn</categories><msc-class>76F05</msc-class><acm-class>G.1.2; G.1.9</acm-class><doi>10.1016/j.compfluid.2012.08.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lagrangian vortex method offers an alternative numerical approach for
direct numerical simulation of turbulence. The fact that it uses the fast
multipole method (FMM)--a hierarchical algorithm for N-body problems with
highly scalable parallel implementations--as numerical engine makes it a
potentially good candidate for exascale systems. However, there have been few
validation studies of Lagrangian vortex simulations and the insufficient
comparisons against standard DNS codes has left ample room for skepticism. This
paper presents a comparison between a Lagrangian vortex method and a
pseudo-spectral method for the simulation of decaying homogeneous isotropic
turbulence. This flow field is chosen despite the fact that it is not the most
favorable flow problem for particle methods (which shine in wake flows or where
vorticity is compact), due to the fact that it is ideal for the quantitative
validation of DNS codes. We use a 256^3 grid with Re_lambda=50 and 100 and look
at the turbulence statistics, including high-order moments. The focus is on the
effect of the various parameters in the vortex method, e.g., order of FMM
series expansion, frequency of reinitialization, overlap ratio and time step.
The vortex method uses an FMM code (exaFMM) that runs on GPU hardware using
CUDA, while the spectral code (hit3d) runs on CPU only. Results indicate that,
for this application (and with the current code implementations), the spectral
method is an order of magnitude faster than the vortex method when using a
single GPU for the FMM and six CPU cores for the FFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2931</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2931</id><created>2011-10-13</created><updated>2012-10-02</updated><authors><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author><author><keyname>Saad</keyname><forenames>David</forenames></author></authors><title>Networking - A Statistical Physics Perspective</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>(Review article) 71 pages, 14 figures</comments><journal-ref>J. Phys. A: Math. Theor., Vol. 46, P. 103001 (2013)</journal-ref><doi>10.1088/1751-8113/46/10/103001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient networking has a substantial economic and societal impact in a
broad range of areas including transportation systems, wired and wireless
communications and a range of Internet applications. As transportation and
communication networks become increasingly more complex, the ever increasing
demand for congestion control, higher traffic capacity, quality of service,
robustness and reduced energy consumption require new tools and methods to meet
these conflicting requirements. The new methodology should serve for gaining
better understanding of the properties of networking systems at the macroscopic
level, as well as for the development of new principled optimization and
management algorithms at the microscopic level. Methods of statistical physics
seem best placed to provide new approaches as they have been developed
specifically to deal with non-linear large scale systems. This paper aims at
presenting an overview of tools and methods that have been developed within the
statistical physics community and that can be readily applied to address the
emerging problems in networking. These include diffusion processes, methods
from disordered systems and polymer physics, probabilistic inference, which
have direct relevance to network routing, file and frequency distribution, the
exploration of network structures and vulnerability, and various other
practical networking applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2953</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2953</id><created>2011-10-13</created><authors><author><keyname>Bach</keyname><forenames>Walter</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author></authors><title>Approximation for Maximum Surjective Constraint Satisfaction Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum surjective constraint satisfaction problems (Max-Sur-CSPs) are
computational problems where we are given a set of variables denoting values
from a finite domain B and a set of constraints on the variables. A solution to
such a problem is a surjective mapping from the set of variables to B such that
the number of satisfied constraints is maximized. We study the approximation
performance that can be acccchieved by algorithms for these problems, mainly by
investigating their relation with Max-CSPs (which are the corresponding
problems without the surjectivity requirement). Our work gives a complexity
dichotomy for Max-Sur-CSP(B) between PTAS and APX-complete, under the
assumption that there is a complexity dichotomy for Max-CSP(B) between PO and
APX-complete, which has already been proved on the Boolean domain and 3-element
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2980</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2980</id><created>2011-10-13</created><authors><author><keyname>Hennemann</keyname><forenames>Stefan</forenames></author><author><keyname>Rybski</keyname><forenames>Diego</forenames></author><author><keyname>Liefner</keyname><forenames>Ingo</forenames></author></authors><title>The Myth of Global Science Collaboration - Collaboration patterns in
  epistemic communities</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>13 pages, 3 figures, 1 table</comments><msc-class>90C35</msc-class><journal-ref>Journal of Informetrics 6(2), April 2012, pp. 217-225</journal-ref><doi>10.1016/j.joi.2011.12.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific collaboration is often perceived as a joint global process that
involves researchers worldwide, regardless of their place of work and
residence. Globalization of science, in this respect, implies that
collaboration among scientists takes place along the lines of common topics and
irrespective of the spatial distances between the collaborators. The networks
of collaborators, termed 'epistemic communities', should thus have a
space-independent structure. This paper shows that such a notion of globalized
scientific collaboration is not supported by empirical data. It introduces a
novel approach of analyzing distance-dependent probabilities of collaboration.
The results of the analysis of six distinct scientific fields reveal that
intra-country collaboration is about 10-50 times more likely to occur than
international collaboration. Moreover, strong dependencies exist between
collaboration activity (measured in co-authorships) and spatial distance when
confined to national borders. However, the fact that distance becomes
irrelevant once collaboration is taken to the international scale suggests a
globalized science system that is strongly influenced by the gravity of local
science clusters. The similarity of the probability functions of the six
science fields analyzed suggests a universal mode of spatial governance that is
independent from the mode of knowledge creation in science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2982</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2982</id><created>2011-10-13</created><authors><author><keyname>Day</keyname><forenames>Troy</forenames></author></authors><title>Computability, G\&quot;odel's Incompleteness Theorem, and an inherent limit
  on the predictability of evolution</title><categories>q-bio.PE cs.LO math.LO</categories><comments>Journal of the Royal Society, Interface 2011</comments><doi>10.1098/rsif.2011.0479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of evolutionary diversification unfolds in a vast genotypic space
of potential outcomes. During the past century there have been remarkable
advances in the development of theory for this diversification, and the
theory's success rests, in part, on the scope of its applicability. A great
deal of this theory focuses on a relatively small subset of the space of
potential genotypes, chosen largely based on historical or contemporary
patterns, and then predicts the evolutionary dynamics within this pre-defined
set. To what extent can such an approach be pushed to a broader perspective
that accounts for the potential open-endedness of evolutionary diversification?
There have been a number of significant theoretical developments along these
lines but the question of how far such theory can be pushed has not been
addressed. Here a theorem is proven demonstrating that, because of the digital
nature of inheritance, there are inherent limits on the kinds of questions that
can be answered using such an approach. In particular, even in extremely simple
evolutionary systems a complete theory accounting for the potential
open-endedness of evolution is unattainable unless evolution is progressive.
The theorem is closely related to G\&quot;odel's Incompleteness Theorem and to the
Halting Problem from computability theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.2995</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.2995</id><created>2011-10-13</created><updated>2013-06-10</updated><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>On sequences of projections of the cubic lattice</title><categories>math.CO cs.CG cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><journal-ref>Computational and Applied Mathematics, 32(1):57-69, 2013</journal-ref><doi>10.1007/s40314-013-0009-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study sequences of lattices which are, up to similarity,
projections of $\mathbb{Z}^{n+1}$ onto a hyperplane $\bm{v}^{\perp}$, with
$\bm{v} \in \mathbb{Z}^{n+1}$ and converge to a target lattice $\Lambda$ which
is equivalent to an integer lattice. We show a sufficient condition to
construct sequences converging at rate $O(1/ |\bm{v}|^{2/n})$ and exhibit
explicit constructions for some important families of lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3001</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3001</id><created>2011-10-13</created><authors><author><keyname>Cheng</keyname><forenames>Peng</forenames></author></authors><title>Step size adaptation in first-order method for stochastic strongly
  convex programming</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a first-order method for stochastic strongly convex optimization
that attains $O(1/n)$ rate of convergence, analysis show that the proposed
method is simple, easily to implement, and in worst case, asymptotically four
times faster than its peers. We derive this method from several intuitive
observations that are generalized from existing first order optimization
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3002</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3002</id><created>2011-10-13</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Are Minds Computable?</title><categories>cs.AI</categories><comments>7 pages, comments welcome</comments><report-no>C3 Report 2011.08</report-no><msc-class>68T01, 68Q05</msc-class><acm-class>I.2.0; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This essay explores the limits of Turing machines concerning the modeling of
minds and suggests alternatives to go beyond those limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3005</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3005</id><created>2011-10-13</created><updated>2013-04-19</updated><authors><author><keyname>Bourla</keyname><forenames>Avraham</forenames></author></authors><title>Symmetry in the sequence of approximation coefficients</title><categories>math.NT cs.IT math.DS math.HO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\{a_n\}_1^\infty$ and $\{\theta_n\}_0^\infty$ be the sequences of
partial quotients and approximation coefficients for the continued fraction
expansion of an irrational number. We will provide a function $f$ such that
$a_{n+1} = f(\theta_{n\pm1},\theta_n)$. In tandem with a formula due to Dajani
and Kraaikamp, we will write $\theta_{n \pm 1}$ as a function of $(\theta_{n
\mp 1}, \theta_n)$, revealing an elegant symmetry in this classical sequence
and allowing for its recovery from a pair of consecutive terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3014</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3014</id><created>2011-10-13</created><updated>2012-05-24</updated><authors><author><keyname>Aoshima</keyname><forenames>Yoshikazu</forenames></author><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Deering</keyname><forenames>Theresa</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yoshitake</forenames></author><author><keyname>Moriyama</keyname><forenames>Sonoko</forenames></author></authors><title>On the Existence of Hamiltonian Paths for History Based Pivot Rules on
  Acyclic Unique Sink Orientations of Hypercubes</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acyclic USO on a hypercube is formed by directing its edges in such as way
that the digraph is acyclic and each face of the hypercube has a unique sink
and a unique source. A path to the global sink of an acyclic USO can be modeled
as pivoting in a unit hypercube of the same dimension with an abstract
objective function, and vice versa. In such a way, Zadeh's 'least entered rule'
and other history based pivot rules can be applied to the problem of finding
the global sink of an acyclic USO. In this paper we present some theoretical
and empirical results on the existence of acyclic USOs for which the various
history based pivot rules can be made to follow a Hamiltonian path. In
particular, we develop an algorithm that can enumerate all such paths up to
dimension 6 using efficient pruning techniques. We show that Zadeh's original
rule admits Hamiltonian paths up to dimension 9 at least, and prove that most
of the other rules do not for all dimensions greater than 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3017</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3017</id><created>2011-10-13</created><authors><author><keyname>Sequeda</keyname><forenames>Juan</forenames></author><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author></authors><title>Towards a Query Language for the Web of Data (A Vision Paper)</title><categories>cs.DB cs.NI</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on querying the Web of Data is still in its infancy. In this paper,
we provide an initial set of general features that we envision should be
considered in order to define a query language for the Web of Data.
Furthermore, for each of these features, we pose questions that have not been
addressed before in the context of querying the Web of Data. We believe that
addressing these questions and studying these features may guide the next 10
years of research on the Web of Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3018</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3018</id><created>2011-10-13</created><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author></authors><title>Robust Localization from Incomplete Local Information</title><categories>cs.NI stat.AP</categories><comments>40 pages, 13 figures</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of localizing wireless devices in an ad-hoc network
embedded in a d-dimensional Euclidean space. Obtaining a good estimation of
where wireless devices are located is crucial in wireless network applications
including environment monitoring, geographic routing and topology control. When
the positions of the devices are unknown and only local distance information is
given, we need to infer the positions from these local distance measurements.
This problem is particularly challenging when we only have access to
measurements that have limited accuracy and are incomplete. We consider the
extreme case of this limitation on the available information, namely only the
connectivity information is available, i.e., we only know whether a pair of
nodes is within a fixed detection range of each other or not, and no
information is known about how far apart they are. Further, to account for
detection failures, we assume that even if a pair of devices is within the
detection range, it fails to detect the presence of one another with some
probability and this probability of failure depends on how far apart those
devices are. Given this limited information, we investigate the performance of
a centralized positioning algorithm MDS-MAP introduced by Shang et al., and a
distributed positioning algorithm, introduced by Savarese et al., called
HOP-TERRAIN. In particular, for a network consisting of n devices positioned
randomly, we provide a bound on the resulting error for both algorithms. We
show that the error is bounded, decreasing at a rate that is proportional to
R/Rc, where Rc is the critical detection range when the resulting random
network starts to be connected, and R is the detection range of each device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3030</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3030</id><created>2011-10-13</created><updated>2012-04-25</updated><authors><author><keyname>Heintz</keyname><forenames>Joos</forenames></author><author><keyname>Kuijpers</keyname><forenames>Bart</forenames></author><author><keyname>Paredes</keyname><forenames>Andres Rojas</forenames></author></authors><title>Software Engineering and Complexity in Effective Algebraic Geometry</title><categories>cs.CC</categories><comments>70 pages. arXiv admin note: substantial text overlap with
  arXiv:1201.4344</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of a robust parameterized arithmetic circuit for the
evaluation of algebraic families of multivariate polynomials. Based on this
notion, we present a computation model, adapted to Scientific Computing, which
captures all known branching parsimonious symbolic algorithms in effective
Algebraic Geometry. We justify this model by arguments from Software
Engineering. Finally we exhibit a class of simple elimination problems of
effective Algebraic Geometry which require exponential time to be solved by
branching parsimonious algorithms of our computation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3038</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3038</id><created>2011-10-13</created><updated>2012-11-15</updated><authors><author><keyname>Herrero</keyname><forenames>Maria Isabel</forenames></author><author><keyname>Jeronimo</keyname><forenames>Gabriela</forenames></author><author><keyname>Sabia</keyname><forenames>Juan</forenames></author></authors><title>Affine solution sets of sparse polynomial systems</title><categories>math.AG cs.DS cs.SC math.AC</categories><comments>26 pages</comments><msc-class>14Q20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the equidimensional decomposition of affine varieties
defined by sparse polynomial systems. For generic systems with fixed supports,
we give combinatorial conditions for the existence of positive dimensional
components which characterize the equidimensional decomposition of the
associated affine variety. This result is applied to design an equidimensional
decomposition algorithm for generic sparse systems. For arbitrary sparse
systems of n polynomials in n variables with fixed supports, we obtain an upper
bound for the degree of the affine variety defined and we present an algorithm
which computes finite sets of points representing its equidimensional
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3062</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3062</id><created>2011-10-13</created><authors><author><keyname>Saffar</keyname><forenames>Hamidreza Ebrahimzadeh</forenames></author><author><keyname>Alian</keyname><forenames>Ehsan Haj Mirza</forenames></author><author><keyname>Mitran</keyname><forenames>Patrick</forenames></author></authors><title>Separation Theorems for Phase-Incoherent Multiple-User Channels</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the transmission of two correlated and memoryless sources $(U,V)$
over several multiple-user phase asynchronous channels. Namely, we consider a
class of phase-incoherent multiple access relay channels (MARC) with both
non-causal and causal unidirectional cooperation between encoders, referred to
as phase-incoherent unidirectional non-causal cooperative MARC (PI-UNCC-MARC),
and phase-incoherent unidirectional causal cooperative MARC (PI-UCC-MARC)
respectively. We also consider phase-incoherent interference channels (PI-IC),
and interference relay channel (PI-IRC) models in the same context. In all
cases, the input signals are assumed to undergo non-ergodic phase shifts due to
the channel. The shifts are assumed to be unknown to the transmitters and known
to the receivers as a realistic assumption. Both necessary and sufficient
conditions in order to reliably send the correlated sources to the destinations
over the considered channels are derived. In particular, for all of the channel
models, we first derive an outer bound for reliable communication that is
defined with respect to the source entropy content (i.e., the triple
$(H(U|V),H(V|U),H(U,V))$). Then, using {\em separate} source and channel
coding, under specific gain conditions, we establish the same region as the
inner bound and therefore obtain tight conditions for reliable communication
for the specific channel under study. We thus establish a source-channel
separation theorem for each channel and conclude that without the knowledge of
the phase shifts at the transmitter sides, separation is optimal. It is further
conjectured that separation in general is optimal for all channel coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3069</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3069</id><created>2011-10-13</created><updated>2012-07-11</updated><authors><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Multiterminal Source Coding under Logarithmic Loss</title><categories>cs.IT math.IT</categories><comments>52 pages, 5 figures. Submitted to IEEE Transactions on Information
  Theory, partial version appeared in IEEE International Symposium on
  Information Theory (ISIT) 2012. Version 3 contains minor revisions and
  corrects the proof of the strengthened converse of Theorem 6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical two-encoder multiterminal source coding problem
where distortion is measured under logarithmic loss. We provide a single-letter
characterization of the achievable rate distortion region for arbitrarily
correlated sources with finite alphabets. In doing so, we also give the rate
distortion region for the $m$-encoder CEO problem (also under logarithmic
loss). Several applications and examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3076</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3076</id><created>2011-10-13</created><authors><author><keyname>Ye</keyname><forenames>Gui-Bo</forenames></author><author><keyname>Wang</keyname><forenames>Yuanfeng</forenames></author><author><keyname>Chen</keyname><forenames>Yifei</forenames></author><author><keyname>Xie</keyname><forenames>Xiaohui</forenames></author></authors><title>Efficient Latent Variable Graphical Model Selection via Split Bregman
  Method</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of covariance matrix estimation in the presence of
latent variables. Under suitable conditions, it is possible to learn the
marginal covariance matrix of the observed variables via a tractable convex
program, where the concentration matrix of the observed variables is decomposed
into a sparse matrix (representing the graphical structure of the observed
variables) and a low rank matrix (representing the marginalization effect of
latent variables). We present an efficient first-order method based on split
Bregman to solve the convex problem. The algorithm is guaranteed to converge
under mild conditions. We show that our algorithm is significantly faster than
the state-of-the-art algorithm on both artificial and real-world data. Applying
the algorithm to a gene expression data involving thousands of genes, we show
that most of the correlation between observed variables can be explained by
only a few dozen latent factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3078</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3078</id><created>2011-10-13</created><updated>2012-09-29</updated><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Miyata</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Moriyama</keyname><forenames>Sonoko</forenames></author></authors><title>Families of polytopal digraphs that do not satisfy the shelling property</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polytopal digraph $G(P)$ is an orientation of the skeleton of a convex
polytope $P$. The possible non-degenerate pivot operations of the simplex
method in solving a linear program over $P$ can be represented as a special
polytopal digraph known as an LP digraph. Presently there is no general
characterization of which polytopal digraphs are LP digraphs, although four
necessary properties are known: acyclicity, unique sink orientation(USO), the
Holt-Klee property and the shelling property. The shelling property was
introduced by Avis and Moriyama (2009), where two examples are given in $d=4$
dimensions of polytopal digraphs satisfying the first three properties but not
the shelling property. The smaller of these examples has $n=7$ vertices. Avis,
Miyata and Moriyama(2009) constructed for each $d \ge 4$ and $n \ge d+2$, a
$d$-polytope $P$ with $n$ vertices which has a polytopal digraph which is an
acyclic USO that satisfies the Holt-Klee property, but does not satisfy the
shelling property. The construction was based on a minimal such example, which
has $d=4$ and $n=6$. In this paper we explore the shelling condition further.
First we give an apparently stronger definition of the shelling property, which
we then prove is equivalent to the original definition. Using this stronger
condition we are able to give a more general construction of such families. In
particular, we show that given any 4-dimensional polytope $P$ with $n_0$
vertices whose unique sink is simple, we can extend $P$ for any $d \ge 4$ and
$n \ge n_0 + d-4$ to a $d$-polytope with these properties that has $n$
vertices. Finally we investigate the strength of the shelling condition for
$d$-crosspolytopes, for which Develin (2004) has given a complete
characterization of LP orientations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3088</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3088</id><created>2011-10-13</created><authors><author><keyname>Collier</keyname><forenames>Nigel</forenames></author></authors><title>Towards cross-lingual alerting for bursty epidemic events</title><categories>cs.CL cs.IR cs.SI</categories><acm-class>I.2.7; I.5.2; I.5.4; I.6.4</acm-class><journal-ref>Journal of Biomedical Semantics 2011, 2(Suppl 5):S10</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Background: Online news reports are increasingly becoming a source for event
based early warning systems that detect natural disasters. Harnessing the
massive volume of information available from multilingual newswire presents as
many challenges as opportunities due to the patterns of reporting complex
spatiotemporal events. Results: In this article we study the problem of
utilising correlated event reports across languages. We track the evolution of
16 disease outbreaks using 5 temporal aberration detection algorithms on
text-mined events classified according to disease and outbreak country. Using
ProMED reports as a silver standard, comparative analysis of news data for 13
languages over a 129 day trial period showed improved sensitivity, F1 and
timeliness across most models using cross-lingual events. We report a detailed
case study analysis for Cholera in Angola 2010 which highlights the challenges
faced in correlating news events with the silver standard. Conclusions: The
results show that automated health surveillance using multilingual text mining
has the potential to turn low value news into high value alerts if informed
choices are used to govern the selection of models and data sources. An
implementation of the C2 alerting algorithm using multilingual news is
available at the BioCaster portal http://born.nii.ac.jp/?page=globalroundup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3089</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3089</id><created>2011-10-13</created><authors><author><keyname>Collier</keyname><forenames>Nigel</forenames></author><author><keyname>Son</keyname><forenames>Nguyen Truong</forenames></author><author><keyname>Nguyen</keyname><forenames>Ngoc Mai</forenames></author></authors><title>OMG U got flu? Analysis of shared health messages for bio-surveillance</title><categories>cs.CL cs.IR cs.SI</categories><acm-class>I.2.7; I.5.2; I.5.4; I.6.4</acm-class><journal-ref>Journal of Biomedical Semantics 2011, 2(Suppl 5):S9</journal-ref><doi>10.1186/2041-1480-2-S5-S9</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Background: Micro-blogging services such as Twitter offer the potential to
crowdsource epidemics in real-time. However, Twitter posts ('tweets') are often
ambiguous and reactive to media trends. In order to ground user messages in
epidemic response we focused on tracking reports of self-protective behaviour
such as avoiding public gatherings or increased sanitation as the basis for
further risk analysis. Results: We created guidelines for tagging self
protective behaviour based on Jones and Salath\'e (2009)'s behaviour response
survey. Applying the guidelines to a corpus of 5283 Twitter messages related to
influenza like illness showed a high level of inter-annotator agreement (kappa
0.86). We employed supervised learning using unigrams, bigrams and regular
expressions as features with two supervised classifiers (SVM and Naive Bayes)
to classify tweets into 4 self-reported protective behaviour categories plus a
self-reported diagnosis. In addition to classification performance we report
moderately strong Spearman's Rho correlation by comparing classifier output
against WHO/NREVSS laboratory data for A(H1N1) in the USA during the 2009-2010
influenza season. Conclusions: The study adds to evidence supporting a high
degree of correlation between pre-diagnostic social media signals and
diagnostic influenza case data, pointing the way towards low cost sensor
networks. We believe that the signals we have modelled may be applicable to a
wide range of diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3091</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3091</id><created>2011-10-13</created><authors><author><keyname>Collier</keyname><forenames>Nigel</forenames></author></authors><title>What's unusual in online disease outbreak news?</title><categories>cs.CL cs.IR cs.SI</categories><acm-class>I.2.7; I.5.2; I.5.4; I.6.4</acm-class><journal-ref>Journal of Biomedical Semantics 2010, 1:2</journal-ref><doi>10.1186/2041-1480-1-2</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Background: Accurate and timely detection of public health events of
international concern is necessary to help support risk assessment and response
and save lives. Novel event-based methods that use the World Wide Web as a
signal source offer potential to extend health surveillance into areas where
traditional indicator networks are lacking. In this paper we address the issue
of systematically evaluating online health news to support automatic alerting
using daily disease-country counts text mined from real world data using
BioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration
detection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance
against expert moderated ProMED-mail postings. Results: We report sensitivity,
specificity, positive predictive value (PPV), negative predictive value (NPV),
mean alerts/100 days and F1, at 95% confidence interval (CI) for 287
ProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period.
Results indicate that W2 had the best F1 with a slight benefit for day of week
effect over C2. In drill down analysis we indicate issues arising from the
granular choice of country-level modeling, sudden drops in reporting due to day
of week effects and reporting bias. Automatic alerting has been implemented in
BioCaster available from http://born.nii.ac.jp. Conclusions: Online health news
alerts have the potential to enhance manual analytical methods by increasing
throughput, timeliness and detection rates. Systematic evaluation of health
news aberrations is necessary to push forward our understanding of the complex
relationship between news report volumes and case numbers and to select the
best performing features and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3094</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3094</id><created>2011-10-13</created><authors><author><keyname>Collier</keyname><forenames>Nigel</forenames></author><author><keyname>Doan</keyname><forenames>Son</forenames></author></authors><title>Syndromic classification of Twitter messages</title><categories>cs.CL cs.IR cs.SI</categories><comments>10 pages, 2 figures, eHealth 2011 conference, Malaga (Spain)
  (accepted)</comments><acm-class>I.2.7; I.5.2; I.5.4; I.6.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recent studies have shown strong correlation between social networking data
and national influenza rates. We expanded upon this success to develop an
automated text mining system that classifies Twitter messages in real time into
six syndromic categories based on key terms from a public health ontology.
10-fold cross validation tests were used to compare Naive Bayes (NB) and
Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM
performed better than NB on 4 out of 6 syndromes. The best performing
classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB);
gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6
(SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1);
constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9
(NB). The resulting classifiers were deployed together with an EARS C2
aberration detection algorithm in an experimental online system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3100</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3100</id><created>2011-10-13</created><authors><author><keyname>Dar</keyname><forenames>Eyal Even</forenames></author><author><keyname>Sandler</keyname><forenames>Mark</forenames></author></authors><title>Telling Two Distributions Apart: a Tight Characterization</title><categories>cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distinguishing between two arbitrary black-box
distributions defined over the domain [n], given access to $s$ samples from
both. It is known that in the worst case O(n^{2/3}) samples is both necessary
and sufficient, provided that the distributions have L1 difference of at least
{\epsilon}. However, it is also known that in many cases fewer samples suffice.
We identify a new parameter, that provides an upper bound on how many samples
needed, and present an efficient algorithm that requires the number of samples
independent of the domain size. Also for a large subclass of distributions we
provide a lower bound, that matches our upper bound up to a poly-logarithmic
factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3109</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3109</id><created>2011-10-13</created><updated>2013-01-10</updated><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Peng</keyname><forenames>Yuxin</forenames></author></authors><title>Robust Image Analysis by L1-Norm Semi-supervised Learning</title><categories>cs.CV cs.LG</categories><comments>This is an extension of our long paper in ACM MM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel L1-norm semi-supervised learning algorithm for
robust image analysis by giving new L1-norm formulation of Laplacian
regularization which is the key step of graph-based semi-supervised learning.
Since our L1-norm Laplacian regularization is defined directly over the
eigenvectors of the normalized Laplacian matrix, we successfully formulate
semi-supervised learning as an L1-norm linear reconstruction problem which can
be effectively solved with sparse coding. By working with only a small subset
of eigenvectors, we further develop a fast sparse coding algorithm for our
L1-norm semi-supervised learning. Due to the sparsity induced by sparse coding,
the proposed algorithm can deal with the noise in the data to some extent and
thus has important applications to robust image analysis, such as noise-robust
image classification and noise reduction for visual and textual bag-of-words
(BOW) models. In particular, this paper is the first attempt to obtain robust
image representation by sparse co-refinement of visual and textual BOW models.
The experimental results have shown the promising performance of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3121</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3121</id><created>2011-10-14</created><updated>2013-02-05</updated><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author></authors><title>Transient fluctuation of the prosperity of firms in a network economy</title><categories>q-bio.MN cs.CE physics.bio-ph physics.soc-ph</categories><doi>10.1016/j.physa.2013.03.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transient fluctuation of the prosperity of firms in a network economy is
investigated with an abstract stochastic model. The model describes the profit
which firms make when they sell materials to a firm which produces a product
and the fixed cost expense to the firms to produce those materials and product.
The formulae for this model are parallel to those for population dynamics. The
swinging changes in the fluctuation in the transient state from the initial
growth to the final steady state are the consequence of a topology-dependent
time trial competition between the profitable interactions and expense. The
firm in a sparse random network economy is more likely to go bankrupt than
expected from the value of the limit of the fluctuation in the steady state,
and there is a risk of failing to reach by far the less fluctuating steady
state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3126</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3126</id><created>2011-10-14</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Zapilko</keyname><forenames>Benjamin</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Mathiak</keyname><forenames>Brigitte</forenames></author></authors><title>Web-Based Multi-View Visualizations for Aggregated Statistics</title><categories>cs.HC</categories><comments>To be published in Proceedings of the 5th International Workshop on
  Web APIs and Services Mashups Proceedings (Mashups '11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of the open data movement a lot of statistical data has been
made publicly available by governments, statistical offices and other
organizations. First efforts to visualize are made by the data providers
themselves. Data aggregators go a step beyond: they collect data from different
open data repositories and make them comparable by providing data sets from
different providers and showing different statistics in the same chart. Another
approach is to visualize two different indicators in a scatter plot or on a
map. The integration of several data sets in one graph can have several
drawbacks: different scales and units are mixed, the graph gets visually
cluttered and one cannot easily distinguish between different indicators. Our
approach marks a combination of (1) the integration of live data from different
data sources, (2) presenting different indicators in coordinated visualizations
and (3) allows adding user visualizations to enrich official statistics with
personal data. Each indicator gets its own visualization, which fits best for
the individual indicator in case of visualization type, scale, unit etc. The
different visualizations are linked, so that related items can easily be
identified by using mouse over effects on data items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3147</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3147</id><created>2011-10-14</created><updated>2011-11-14</updated><authors><author><keyname>Huang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Shi</keyname><forenames>Yongtang</forenames></author></authors><title>Rainbow connections for planar graphs and line graphs</title><categories>cs.CC math.CO</categories><comments>13 pages</comments><msc-class>68Q25, 68R10, 05C10, 05C12, 05C15, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge-colored graph $G$ is rainbow connected if any two vertices are
connected by a path whose edges have distinct colors. The rainbow connection
number of a connected graph $G$, denoted by $rc(G)$, is the smallest number of
colors that are needed in order to make $G$ rainbow connected. It was proved
that computing $rc(G)$ is an NP-Hard problem, as well as that even deciding
whether a graph has $rc(G)=2$ is NP-Complete. It is known that deciding whether
a given edge-colored graph is rainbow connected is NP-Complete. We will prove
that it is still NP-Complete even when the edge-colored graph is a planar
bipartite graph. We also give upper bounds of the rainbow connection number of
outerplanar graphs with small diameters. A vertex-colored graph is rainbow
vertex-connected if any two vertices are connected by a path whose internal
vertices have distinct colors. The rainbow vertex-connection number of a
connected graph $G$, denoted by $rvc(G)$, is the smallest number of colors that
are needed in order to make $G$ rainbow vertex-connected. It is known that
deciding whether a given vertex-colored graph is rainbow vertex-connected is
NP-Complete. We will prove that it is still NP-Complete even when the
vertex-colored graph is a line graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3158</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3158</id><created>2011-10-14</created><authors><author><keyname>Salem</keyname><forenames>Rashed</forenames><affiliation>ERIC</affiliation></author><author><keyname>Darmont</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>ERIC</affiliation></author><author><keyname>Boussa&#xef;d</keyname><forenames>Omar</forenames><affiliation>ERIC</affiliation></author></authors><title>Efficient Incremental Breadth-Depth XML Event Mining</title><categories>cs.DB</categories><proxy>ccsd</proxy><report-no>ERIC:11-013</report-no><journal-ref>15th International Database Engineering and Applications Symposium
  (IDEAS 2011), Lisbon : Portugal (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications log a large amount of events continuously. Extracting
interesting knowledge from logged events is an emerging active research area in
data mining. In this context, we propose an approach for mining frequent events
and association rules from logged events in XML format. This approach is
composed of two-main phases: I) constructing a novel tree structure called
Frequency XML-based Tree (FXT), which contains the frequency of events to be
mined; II) querying the constructed FXT using XQuery to discover frequent
itemsets and association rules. The FXT is constructed with a single-pass over
logged data. We implement the proposed algorithm and study various performance
issues. The performance study shows that the algorithm is efficient, for both
constructing the FXT and discovering association rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3177</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3177</id><created>2011-10-14</created><authors><author><keyname>Bracken</keyname><forenames>Carl</forenames></author><author><keyname>Tan</keyname><forenames>Chik How</forenames></author><author><keyname>Yin</keyname><forenames>Tan</forenames></author></authors><title>On a Class of Quadratic Polynomials with no Zeros and its Application to
  APN Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the there exists an infinite family of APN functions of the form
$F(x)=x^{2^{s}+1} + x^{2^{k+s}+2^k} + cx^{2^{k+s}+1} + c^{2^k}x^{2^k + 2^s} +
\delta x^{2^{k}+1}$, over $\gf_{2^{2k}}$, where $k$ is an even integer and
$\gcd(2k,s)=1, 3\nmid k$. This is actually a proposed APN family of Lilya
Budaghyan and Claude Carlet who show in \cite{carlet-1} that the function is
APN when there exists $c$ such that the polynomial
$y^{2^s+1}+cy^{2^s}+c^{2^k}y+1=0$ has no solutions in the field $\gf_{2^{2k}}$.
In \cite{carlet-1} they demonstrate by computer that such elements $c$ can be
found over many fields, particularly when the degree of the field is not
divisible by 3. We show that such $c$ exists when $k$ is even and $3\nmid k$
(and demonstrate why the $k$ odd case only re-describes an existing family of
APN functions). The form of these coefficients is given so that we may write
the infinite family of APN functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3185</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3185</id><created>2011-10-14</created><authors><author><keyname>Dahan</keyname><forenames>X.</forenames></author></authors><title>Structure of lexicographic Groebner bases in three variables of ideals
  of dimension zero</title><categories>cs.SC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the structural theorem of Lazard in 1985, from 2 variables to 3
variables. We use the Gianni-Kalkbrener result to do this, which implies some
restrictions inside which lies the case of a radical ideal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3189</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3189</id><created>2011-10-14</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>About set-theoretic properties of one-way functions</title><categories>cs.CC</categories><comments>5 pages</comments><msc-class>94A60, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of cryptanalysis as a problem belonging to the
class NP. A class of problems UF is defined for which the time constructing any
feasible solution is polynomial. The properties of the problems of NP, which
may be one-way functions, are established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3194</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3194</id><created>2011-10-14</created><authors><author><keyname>Jin</keyname><forenames>Qiyu</forenames></author><author><keyname>Grama</keyname><forenames>Ion</forenames></author><author><keyname>Liu</keyname><forenames>Quansheng</forenames></author></authors><title>Controlled Total Variation regularization for inverse problems</title><categories>cs.CV</categories><comments>10 pages, 10 figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a new algorithm for solving inverse problems, based on
the minimization of the $L^2$ norm and on the control of the Total Variation.
It consists in relaxing the role of the Total Variation in the classical Total
Variation minimization approach, which permits us to get better approximation
to the inverse problems. The numerical results on the deconvolution problem
show that our method outperforms some previous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3195</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3195</id><created>2011-10-14</created><authors><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author><author><keyname>Wang</keyname><forenames>Hui</forenames></author></authors><title>Blind Known Interference Cancellation</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates interference-cancellation schemes at the receiver, in
which the original data of the interference is known a priori. Such a priori
knowledge is common in wireless relay networks. For example, a transmitting
relay could be relaying data that was previously transmitted by a node, in
which case the interference received by the node now is actually self
information. Besides the case of self information, the node could also have
overheard or received the interference data in a prior transmission by another
node. Directly removing the known interference requires accurate estimate of
the interference channel, which may be difficult in many situations. In this
paper, we propose a novel scheme, Blind Known Interference Cancellation (BKIC),
to cancel known interference without interference channel information. BKIC
consists of two steps. The first step combines adjacent symbols to cancel the
interference, exploiting the fact that the channel coefficients are almost the
same between successive symbols. After such interference cancellation, however,
the signal of interest is also distorted. The second step recovers the signal
of interest amidst the distortion. We propose two algorithms for the critical
second steps. The first algorithm (BKIC-S) is based on the principle of
smoothing. It is simple and has near optimal performance in the slow fading
scenario. The second algorithm (BKIC-RBP) is based on the principle of
real-valued belief propagation. It can achieve MAP-optimal performance with
fast convergence, and has near optimal performance even in the fast fading
scenario. Both BKIC schemes outperform the traditional self-interference
cancellation schemes with perfect initial channel information by a large
margin, while having lower complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3197</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3197</id><created>2011-10-14</created><authors><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author><author><keyname>Zhou</keyname><forenames>Qingfeng</forenames></author><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Wang</keyname><forenames>Hui</forenames></author></authors><title>Non-memoryless Analog Network Coding in Two-Way Relay Channel</title><categories>cs.IT cs.NI math.IT</categories><comments>ICC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical-layer Network Coding (PNC) can significantly improve the throughput
of two-way relay channels. An interesting variant of PNC is Analog Network
Coding (ANC). Almost all ANC schemes proposed to date, however, operate in a
symbol by symbol manner (memoryless) and cannot exploit the redundant
information in channel-coded packets to enhance performance. This paper
proposes a non-memoryless ANC scheme. In particular, we design a soft-input
soft-output decoder for the relay node to process the superimposed packets from
the two end nodes to yield an estimated MMSE packet for forwarding back to the
end nodes. Our decoder takes into account the correlation among different
symbols in the packets due to channel coding, and provides significantly
improved MSE performance. Our analysis shows that the SNR improvement at the
relay node is lower bounded by 1/R (R is the code rate) with the simplest LDPC
code (repeat code). The SNR improvement is also verified by numerical
simulation with LDPC code. Our results indicate that LDPC codes of different
degrees are preferred in different SNR regions. Generally speaking, smaller
degrees are preferred for lower SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3211</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3211</id><created>2011-10-14</created><authors><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Tron, a combinatorial Game on abstract Graphs</title><categories>cs.CC cs.DM cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the combinatorial two-player game Tron. We answer the extremal
question on general graphs and also consider smaller graph classes. Bodlaender
and Kloks conjectured in [2] PSPACE- completeness. We proof this conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3216</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3216</id><created>2011-10-14</created><authors><author><keyname>Bui</keyname><forenames>Huyen-Chi</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author></authors><title>An Enhanced Multiple Random Access Scheme for Satellite Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce Multi-Slots Coded ALOHA (MuSCA) as a multiple
random access method for satellite communications. This scheme can be
considered as a generalization of the Contention Resolution Diversity Slotted
Aloha (CRDSA) mechanism. Instead of transmitting replicas, this system replaces
them by several parts of a single word of an error correcting code. It is also
different from Coded Slotted ALOHA (CSA) as the assumption of destructive
collisions is not adopted. In MuSCA, the entity in charge of the decoding
mechanism collects all bursts of the same user (including the interfered slots)
before decoding and implements a successive interference cancellation (SIC)
process to remove successfully decoded signals. Simulations show that for a
frame of 100 slots, the achievable total normalized throughput is greater than
1.25 and 1.4 for a frame of 500 slots, resulting in a gain of 80% and 75% with
respect to CRDSA and CSA respectively. This paper is a first analysis of the
proposed scheme and opens several perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3225</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3225</id><created>2011-10-14</created><authors><author><keyname>Dries</keyname><forenames>Anton</forenames></author><author><keyname>Nijssen</keyname><forenames>Siegfried</forenames></author></authors><title>Mining Patterns in Networks using Homomorphism</title><categories>cs.DS cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years many algorithms have been developed for finding patterns in
graphs and networks. A disadvantage of these algorithms is that they use
subgraph isomorphism to determine the support of a graph pattern; subgraph
isomorphism is a well-known NP complete problem. In this paper, we propose an
alternative approach which mines tree patterns in networks by using subgraph
homomorphism. The advantage of homomorphism is that it can be computed in
polynomial time, which allows us to develop an algorithm that mines tree
patterns in arbitrary graphs in incremental polynomial time. Homomorphism
however entails two problems not found when using isomorphism: (1) two patterns
of different size can be equivalent; (2) patterns of unbounded size can be
frequent. In this paper we formalize these problems and study solutions that
easily fit within our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3233</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3233</id><created>2011-10-14</created><authors><author><keyname>Mulansky</keyname><forenames>Mario</forenames></author><author><keyname>Ahnert</keyname><forenames>Karsten</forenames></author></authors><title>Metaprogramming Applied to Numerical Problems</title><categories>physics.comp-ph cs.MS</categories><comments>4 pages,2 figures,3 listings</comments><journal-ref>AIP Conf. Proc. 1389, pp. 1582-1585 (2011)</journal-ref><doi>10.1063/1.3637933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the discovery that the template system of C++ forms a Turing complete
language in 1994, a programming technique called Template Metaprogramming has
emerged that allows for the creation of faster, more generic and better code.
Here, we apply Template Metaprogramming to implement a generic Runge-Kutta
scheme that can be used to numerically solve ordinary differential equations.
We show that using Template Metaprogramming results in a significantly improved
performance compared to a classical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3239</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3239</id><created>2011-10-12</created><authors><author><keyname>Corani</keyname><forenames>Giorgio</forenames></author><author><keyname>De Campos</keyname><forenames>Cassio P.</forenames></author></authors><title>Improving parameter learning of Bayesian nets from incomplete data</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the estimation of parameters of a Bayesian network from
incomplete data. The task is usually tackled by running the
Expectation-Maximization (EM) algorithm several times in order to obtain a high
log-likelihood estimate. We argue that choosing the maximum log-likelihood
estimate (as well as the maximum penalized log-likelihood and the maximum a
posteriori estimate) has severe drawbacks, being affected both by overfitting
and model uncertainty. Two ideas are discussed to overcome these issues: a
maximum entropy approach and a Bayesian model averaging approach. Both ideas
can be easily applied on top of EM, while the entropy idea can be also
implemented in a more sophisticated way, through a dedicated non-linear solver.
A vast set of experiments shows that these ideas produce significantly better
estimates and inferences than the traditional and widely used maximum
(penalized) log-likelihood and maximum a posteriori estimates. In particular,
if EM is adopted as optimization engine, the model averaging approach is the
best performing one; its performance is matched by the entropy approach when
implemented using the non-linear solver. The results suggest that the
applicability of these ideas is immediate (they are easy to implement and to
integrate in currently available inference engines) and that they constitute a
better way to learn Bayesian network parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3264</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3264</id><created>2011-10-14</created><authors><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Reduced-dimension multiuser detection: detectors and performance
  guarantees</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, submitted to IEEE International Conference on
  communications (ICC) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore several reduced-dimension multiuser detection (RD-MUD) structures
that significantly decrease the number of required correlation branches at the
receiver front-end, while still achieving performance similar to that of the
conventional matched-filter (MF) bank. RD-MUD exploits the fact that the number
of active users is typically small relative to the total number of users in the
system and relies on ideas of analog compressed sensing to reduce the number of
correlators. We first develop a general framework for both linear and nonlinear
RD-MUD detectors. We then present theoretical performance analysis for two
specific detectors: the linear reduced-dimension decorrelating (RDD) detector,
which combines subspace projection and thresholding to determine active users
and sign detection for data recovery, and the nonlinear reduced-dimension
decision-feedback (RDDF) detector, which combines decision-feedback orthogonal
matching pursuit for active user detection and sign detection for data
recovery. The theoretical performance results for both detectors are validated
via numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3267</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3267</id><created>2011-10-14</created><authors><author><keyname>Madhusudhanan</keyname><forenames>Prasanna</forenames><affiliation>Eugene</affiliation></author><author><keyname>Restrepo</keyname><forenames>Juan G.</forenames><affiliation>Eugene</affiliation></author><author><keyname>Youjian</keyname><affiliation>Eugene</affiliation></author><author><keyname>Liu</keyname></author><author><keyname>Brown</keyname><forenames>Timothy X</forenames></author><author><keyname>Baker</keyname><forenames>Kenneth R.</forenames></author></authors><title>Multi-tier Network Performance Analysis using a Shotgun Cellular System</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted at IEEE Globecom 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the carrier-to-interference ratio (CIR) and
carrier-to-interference-plus-noise ratio (CINR) performance at the mobile
station (MS) within a multi-tier network composed of M tiers of wireless
networks, with each tier modeled as the homogeneous n-dimensional (n-D, n=1,2,
and 3) shotgun cellular system, where the base station (BS) distribution is
given by the homogeneous Poisson point process in n-D. The CIR and CINR at the
MS in a single tier network are thoroughly analyzed to simplify the analysis of
the multi-tier network. For the multi-tier network with given system
parameters, the following are the main results of this paper: (1)
semi-analytical expressions for the tail probabilities of CIR and CINR; (2) a
closed form expression for the tail probability of CIR in the range
[1,Infinity); (3) a closed form expression for the tail probability of an
approximation to CIR in the entire range [0,Infinity); (4) a lookup table based
approach for obtaining the tail probability of CINR, and (5) the study of the
effect of shadow fading and BSs with ideal sectorized antennas on the CIR and
CINR. Based on these results, it is shown that, in a practical cellular system,
the installation of additional wireless networks (microcells, picocells and
femtocells) with low power BSs over the already existing macrocell network will
always improve the CINR performance at the MS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3271</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3271</id><created>2011-10-14</created><updated>2012-02-20</updated><authors><author><keyname>Evans</keyname><forenames>T. S.</forenames></author><author><keyname>Hopkins</keyname><forenames>N.</forenames></author><author><keyname>Kaube</keyname><forenames>B. S.</forenames></author></authors><title>Universality of Performance Indicators based on Citation and Reference
  Counts</title><categories>physics.soc-ph cs.DL</categories><comments>15 pages, 14 figures, 11 pages of supplementary material. Submitted
  to Scientometrics</comments><report-no>Imperial/TP/11/TSE/5</report-no><journal-ref>Scientometrics, 2012, 93, 473-495</journal-ref><doi>10.1007/s11192-012-0694-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find evidence for the universality of two relative bibliometric indicators
of the quality of individual scientific publications taken from different data
sets. One of these is a new index that considers both citation and reference
counts. We demonstrate this universality for relatively well cited publications
from a single institute, grouped by year of publication and by faculty or by
department. We show similar behaviour in publications submitted to the arXiv
e-print archive, grouped by year of submission and by sub-archive. We also find
that for reasonably well cited papers this distribution is well fitted by a
lognormal with a variance of around 1.3 which is consistent with the results of
Radicchi, Fortunato, and Castellano (2008). Our work demonstrates that
comparisons can be made between publications from different disciplines and
publication dates, regardless of their citation count and without expensive
access to the whole world-wide citation graph. Further, it shows that averages
of the logarithm of such relative bibliometric indices deal with the issue of
long tails and avoid the need for statistics based on lengthy ranking
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3280</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3280</id><created>2011-10-14</created><authors><author><keyname>Madhusudhanan</keyname><forenames>Prasanna</forenames><affiliation>Eugene</affiliation></author><author><keyname>Restrepo</keyname><forenames>Juan G.</forenames><affiliation>Eugene</affiliation></author><author><keyname>Youjian</keyname><affiliation>Eugene</affiliation></author><author><keyname>Liu</keyname></author><author><keyname>Brown</keyname><forenames>Timothy X</forenames></author><author><keyname>Baker</keyname><forenames>Kenneth R.</forenames></author></authors><title>Stochastic Ordering based Carrier-to-Interference Ratio Analysis for the
  Shotgun Cellular Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 0 figures, submitted for review to IEEE Wireless
  Communications Letters on October 11, 2011</comments><journal-ref>IEEE Wireless Communications Letters, 2012, vol.1, no.6,
  pp.565-568</journal-ref><doi>10.1109/WCL.2012.080112.120458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple analytical tool based on stochastic ordering is developed to compare
the distributions of carrier-to-interference ratio at the mobile station of two
cellular systems where the base stations are distributed randomly according to
certain non-homogeneous Poisson point processes. The comparison is conveniently
done by studying only the base station densities without having to solve for
the distributions of the carrier-to-interference ratio, that are often hard to
obtain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3281</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3281</id><created>2011-10-14</created><updated>2012-04-26</updated><authors><author><keyname>Ramkumar</keyname><forenames>B.</forenames></author><author><keyname>Sreedeep</keyname><forenames>V.</forenames></author><author><keyname>Kittur</keyname><forenames>Harish M</forenames></author></authors><title>Faster Energy Efficient Dadda Based Baugh-Wooley Multipliers</title><categories>cs.AR</categories><acm-class>B.2.4; B.7.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work faster Baugh-Wooley multiplication has been achieved by using a
combination of two design techniques: partition of the partial products into
two parts for independent parallel column compression and acceleration of the
final addition using a hybrid adder proposed in this work. Based on the
proposed techniques 8, 16, 32 and 64-bit Dadda based Baugh-Wooley multipliers
has been developed and compared with the regular Baugh-Wooley multiplier. The
performance of the proposed multiplier is analyzed by evaluating the delay,
area and power, with 180 nm process technologies on interconnect and layout
using industry standard design and layout tools. The result analysis shows that
the 64-bit proposed multiplier is as much as 26.9% faster than the regular
Baugh-Wooley multiplier and requires only 2.21% more power. Also the
power-delay product of the proposed design is significantly lower than that of
the regular Baugh-Wooley multiplier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3294</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3294</id><created>2011-10-14</created><authors><author><keyname>Grellois</keyname><forenames>Charles</forenames></author></authors><title>Algebraic theories, monads, and arities</title><categories>math.CT cs.LO</categories><comments>65 pages, this is a survey made for a Master thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monads are of interest both in semantics and in higher dimensional algebra.
It turns out that the idea behind usual notion finitary monads (whose values on
all sets can be computed from their values on finite sets) extends to a more
general class of monads called monads with arities, so that not only algebraic
theories can be computed from a proper set of arities, but also more general
structures like n-categories, the computing process being realized using Kan
extensions. This Master thesis compiles the required material in order to
understand this question of arities and reconstruction of monads, and tries to
give some examples of relevant interest from both semantics and higher category
theory. A discussion on the promising field of operads is then provided as
appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3315</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3315</id><created>2011-10-14</created><authors><author><keyname>Forgerini</keyname><forenames>F. L.</forenames></author><author><keyname>Crokidakis</keyname><forenames>N.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Evolution of spatially embedded branching trees with interacting nodes</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>13 pages, 9 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of branching trees embedded in Euclidean spaces with
suppressed branching of spatially close nodes. This cooperative branching
process accounts for the effect of overcrowding of nodes in the embedding space
and mimics the evolution of life processes (the so-called &quot;tree of life&quot;) in
which a new level of complexity emerges as a short transition followed by a
long period of gradual evolution or even complete extinction. We consider the
models of branching trees in which each new node can produce up to two twigs
within a unit distance from the node in the Euclidean space, but this branching
is suppressed if the newborn node is closer than at distance $a$ from one of
the previous generation nodes. This results in an explosive (exponential)
growth in the initial period, and, after some crossover time $t_x \sim
\ln(1/a)$ for small $a$, in a slow (power-law) growth. This special point is
also a transition from &quot;small&quot; to &quot;large words&quot; in terms of network science. We
show that if the space is restricted, then this evolution may end by
extinction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3316</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3316</id><created>2011-10-14</created><authors><author><keyname>G&#xf3;mez-Cruz</keyname><forenames>Nelson Alfonso</forenames></author><author><keyname>Maldonado</keyname><forenames>Carlos Eduardo</forenames></author></authors><title>Biological Computation as the Revolution of Complex Engineered Systems</title><categories>cs.OH</categories><comments>9 pages, 0 figures</comments><msc-class>68-02</msc-class><journal-ref>This text is an extension of: G\'omez-Cruz, N. &amp; Maldonado, C.E.
  (2011). Biological Computation: A Road to Complex Engineered Systems. In:
  Unifying Themes in Complex Systems Volume VIII, pp. 918-927. Cambridge, MA:
  NECSI Knowledge Press</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Provided that there is no theoretical frame for complex engineered systems
(CES) as yet, this paper claims that bio-inspired engineering can help provide
such a frame. Within CES bio-inspired systems play a key role. The disclosure
from bio-inspired systems and biological computation has not been sufficiently
worked out, however. Biological computation is to be taken as the processing of
information by living systems that is carried out in polynomial time, i.e.,
efficiently; such processing however is grasped by current science and research
as an intractable problem (for instance, the protein folding problem). A remark
is needed here: P versus NP problems should be well defined and delimited but
biological computation problems are not. The shift from conventional
engineering to bio-inspired engineering needs bring the subject (or problem) of
computability to a new level. Within the frame of computation, so far, the
prevailing paradigm is still the Turing-Church thesis. In other words,
conventional engineering is still ruled by the Church-Turing thesis (CTt).
However, CES is ruled by CTt, too. Contrarily to the above, we shall argue here
that biological computation demands a more careful thinking that leads us
towards hypercomputation. Bio-inspired engineering and CES thereafter, must
turn its regard toward biological computation. Thus, biological computation can
and should be taken as the ground for engineering complex non-linear systems.
Biological systems do compute in terms of hypercomputation, indeed. If so, then
the focus is not algorithmic or computational complexity but
computation-beyond-the-Church-Turing-barrier. We claim that we need a new
computational theory that encompasses biological processes wherein the
Turing-Church thesis is but a particular case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3347</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3347</id><created>2011-10-14</created><authors><author><keyname>Azimi</keyname><forenames>Javad</forenames></author><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli</forenames></author></authors><title>Dynamic Batch Bayesian Optimization</title><categories>cs.LG</categories><comments>6 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization (BO) algorithms try to optimize an unknown function
that is expensive to evaluate using minimum number of evaluations/experiments.
Most of the proposed algorithms in BO are sequential, where only one experiment
is selected at each iteration. This method can be time inefficient when each
experiment takes a long time and more than one experiment can be ran
concurrently. On the other hand, requesting a fix-sized batch of experiments at
each iteration causes performance inefficiency in BO compared to the sequential
policies. In this paper, we present an algorithm that asks a batch of
experiments at each time step t where the batch size p_t is dynamically
determined in each step. Our algorithm is based on the observation that the
sequence of experiments selected by the sequential policy can sometimes be
almost independent from each other. Our algorithm identifies such scenarios and
request those experiments at the same time without degrading the performance.
We evaluate our proposed method using the Expected Improvement policy and the
results show substantial speedup with little impact on the performance in eight
real and synthetic benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3365</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3365</id><created>2011-10-14</created><authors><author><keyname>Bagherikaram</keyname><forenames>Ghadamali</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author></authors><title>Secure Hybrid Digital-Analog Coding With Side Information at the
  Receiver</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the problem of transmitting an i.i.d Gaussian source over an
i.i.d Gaussian wiretap channel with an i.i.d Gaussian side information
available at the intended receiver is considered. The intended receiver is
assumed to have a certain minimum SNR and the eavesdropper is assumed to have a
strictly lower SNR, compared to the intended receiver. The objective is to
minimize the distortion of source reconstruction at the intended receiver. In
this work, it is shown that the source-channel separation coding scheme is
optimum in the sense of achieving minimum distortion. Two hybrid digital-analog
Wyner-Ziv coding schemes are then proposed which achieve the minimum
distortion. These secure joint source-channel coding schemes are based on the
Wyner-Ziv coding scheme and wiretap channel coding scheme when the analog
source is not explicitly quantized. The proposed secure hybrid digital-analog
schemes are analyzed under the main channel SNR mismatch. It is proven that the
proposed schemes can give a graceful degradation of distortion with SNR under
SNR mismatch, i.e., when the actual SNR is larger than the designed SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3366</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3366</id><created>2011-10-14</created><authors><author><keyname>Bagherikaram</keyname><forenames>Ghadamali</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author></authors><title>Optimum Relay Scheme in a Secure Two-Hop Amplify and Forward Cooperative
  Communication System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A MIMO secure two-hop wireless communication system is considered in this
paper. In this model, there are no direct links between the source-destination
and the source-eavesdropper. The problem is maximizing the secrecy capacity of
the system over all possible amplify and forward (AF) relay strategies, such
that the power consumption at the source node and the relay node is limited.
When all the nodes are equipped with single antenna, this non-convex
optimization problem is fully characterized. When all the nodes (except the
intended receiver) are equipped with multiple antennas, the optimization
problem is characterized based on the generalized eigenvalues-eigenvectors of
the channel gain matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3376</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3376</id><created>2011-10-14</created><updated>2011-10-19</updated><authors><author><keyname>Sreedeep</keyname><forenames>V.</forenames></author><author><keyname>Ramkumar</keyname><forenames>B.</forenames></author><author><keyname>Kittur</keyname><forenames>Harish M</forenames></author></authors><title>Faster and Low Power Twin Precision Multiplier</title><categories>cs.AR</categories><comments>5 pages, 10 figures and 5 tables</comments><acm-class>B.2.4; B.7.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work faster unsigned multiplication has been achieved by using a
combination of High Performance Multiplication [HPM] column reduction technique
and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive
multiplication) and acceleration of the final addition using a hybrid adder.
Low power has been achieved by using clock gating technique. Based on the
proposed technique 16 and 32-bit multipliers are developed. The performance of
the proposed multiplier is analyzed by evaluating the delay, area and power,
with TCBNPHP 90 nm process technology on interconnect and layout using Cadence
NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit
proposed multiplier is as much as 22% faster, occupies only 3% more area and
consumes 30% lesser power with respect to the recently reported twin precision
multiplier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3379</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3379</id><created>2011-10-15</created><authors><author><keyname>Saha</keyname><forenames>Rahul</forenames></author><author><keyname>Geetha</keyname><forenames>Dr. G.</forenames></author></authors><title>Identifying Reference Objects by Hierarchical Clustering in Java
  Environment</title><categories>cs.SE</categories><comments>8 pages,13 tables,2 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, September 2011 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Java programming environment has become so popular. Java programming
language is a language that is designed to be portable enough to be executed in
wide range of computers ranging from cell phones to supercomputers. Computer
programs written in Java are compiled into Java Byte code instructions that are
suitable for execution by a Java Virtual Machine implementation. Java virtual
Machine is commonly implemented in software by means of an interpreter for the
Java Virtual Machine instruction set. As an object oriented language, Java
utilizes the concept of objects. Our idea is to identify the candidate objects'
references in a Java environment through hierarchical cluster analysis using
reference stack and execution stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3380</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3380</id><created>2011-10-15</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Efficient Traffic Control of VoD System</title><categories>cs.NI</categories><comments>12 pages, 12 figure</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.3, No.5, Sep 2011</journal-ref><doi>10.5121/ijcnc.2011.3507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been a challenging issue to provide digital quality multimedia data
stream to the remote user through the distributed system. The main aspects to
design the real distributed system, which reduce the cost of the network by
means of reduce packet loss and enhanced over all system performance. Since the
number of user increased rapidly in the network it posed heavy load to the
video servers. The requested clients, servers are all distributed in nature and
the data stream delivered to the user without error. In this work I have
presented the performance of the video on demand server by efficient traffic
control at real time with respect to incoming multirate traffic pattern . In
this work, I present how the overall system performance gradually decreases
when the client population sized in the clusters increase. This work indicated
the load balancing required for the on demand video distributed system to
provide efficient cost effective service to the local or remote clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3381</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3381</id><created>2011-10-15</created><authors><author><keyname>Franceschini</keyname><forenames>Gianni</forenames></author><author><keyname>Grossi</keyname><forenames>Roberto</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author></authors><title>Partial Data Compression and Text Indexing via Optimal Suffix
  Multi-Selection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an input text string T[1,N] drawn from an unbounded alphabet. We
study partial computation in suffix-based problems for Data Compression and
Text Indexing such as
  (I) retrieve any segment of K&lt;=N consecutive symbols from the Burrows-Wheeler
transform of T, and
  (II) retrieve any chunk of K&lt;=N consecutive entries of the Suffix Array or
the Suffix Tree.
  Prior literature would take O(N log N) comparisons (and time) to solve these
problems by solving the total problem of building the entire Burrows-Wheeler
transform or Text Index for T, and performing a post-processing to single out
the wanted portion.
  We introduce a novel adaptive approach to partial computational problems
above, and solve both the partial problems in O(K log K + N) comparisons and
time, improving the best known running times of O(N log N) for K=o(N).
  These partial-computation problems are intimately related since they share a
common bottleneck: the suffix multi-selection problem, which is to output the
suffixes of rank r_1,r_2,...,r_K under the lexicographic order, where
r_1&lt;r_2&lt;...&lt;r_K, r_i in [1,N]. Special cases of this problem are well known:
K=N is the suffix sorting problem that is the workhorse in Stringology with
hundreds of applications, and K=1 is the recently studied suffix selection.
  We show that suffix multi-selection can be solved in Theta(N log N -
sum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,
r_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0&lt;=j&lt;=K. This is asymptotically
optimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for
multi-selection on atomic elements (not suffixes). Matching the bound known for
atomic elements for strings is a long running theme and challenge from 70's,
which we achieve for the suffix multi-selection problem. The partial suffix
problems as well as the suffix multi-selection problem have many applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3382</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3382</id><created>2011-10-15</created><authors><author><keyname>Boulkaibet</keyname><forenames>I.</forenames></author><author><keyname>Marwala</keyname><forenames>T.</forenames></author><author><keyname>Mthembu</keyname><forenames>L.</forenames></author><author><keyname>Friswell</keyname><forenames>M. I.</forenames></author><author><keyname>Adhikari</keyname><forenames>S.</forenames></author></authors><title>Sampling Techniques in Bayesian Finite Element Model Updating</title><categories>cs.CE</categories><comments>Paper Accepted in the 25th International Modal Analysis Conference,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent papers in the field of Finite Element Model (FEM) updating have
highlighted the benefits of Bayesian techniques. The Bayesian approaches are
designed to deal with the uncertainties associated with complex systems, which
is the main problem in the development and updating of FEMs. This paper
highlights the complexities and challenges of implementing any Bayesian method
when the analysis involves a complicated structural dynamic model. In such
systems an analytical Bayesian formulation might not be available in an
analytic form; therefore this leads to the use of numerical methods, i.e.
sampling methods. The main challenge then is to determine an efficient sampling
of the model parameter space. In this paper, three sampling techniques, the
Metropolis-Hastings (MH) algorithm, Slice Sampling and the Hybrid Monte Carlo
(HMC) technique, are tested by updating a structural beam model. The efficiency
and limitations of each technique is investigated when the FEM updating problem
is implemented using the Bayesian Approach. Both MH and HMC techniques are
found to perform better than the Slice sampling when Young's modulus is chosen
as the updating parameter. The HMC method gives better results than MH and
Slice sampling techniques, when the area moment of inertias and section areas
are updated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3384</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3384</id><created>2011-10-15</created><authors><author><keyname>Kumar</keyname><forenames>Gulshan</forenames></author><author><keyname>Rai</keyname><forenames>Prof. Mritunjay Kumar</forenames></author></authors><title>X-ray view on a Class using Conceptual Analysis in Java Environment</title><categories>cs.SE</categories><comments>6 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, September 2011 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is one of the most important principles in software engineering
and a necessity for every practical software. Since the design space of
software is generally quite large, it is valuable to provide automatic means to
help modularizing it. An automatic technique for software modularization is
object- oriented concept analysis (OOCA). X-ray view of the class is one of the
aspect of this Object oriented concept analysis. We shall use this concept in a
java environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3385</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3385</id><created>2011-10-15</created><authors><author><keyname>Patel</keyname><forenames>Pretesh</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author></authors><title>Fuzzy Inference Systems Optimization</title><categories>cs.AI</categories><comments>Paper Submitted to INTECH</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares various optimization methods for fuzzy inference system
optimization. The optimization methods compared are genetic algorithm, particle
swarm optimization and simulated annealing. When these techniques were
implemented it was observed that the performance of each technique within the
fuzzy inference system classification was context dependent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3386</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3386</id><created>2011-10-15</created><authors><author><keyname>Kumar</keyname><forenames>Gulshan</forenames></author><author><keyname>Rai</keyname><forenames>Mritunjay</forenames></author></authors><title>An Approach to Provide Security in Mobile Ad-Hoc Networks Using Counter
  Mode of Encryption on Mac Layer</title><categories>cs.NI</categories><comments>11 pages, 6 figures, accepted and cited in &quot;First International
  Conference on Computer Science, Engineering and Applications (CCSEA-2011)&quot;</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.4, July 2011</journal-ref><doi>10.5121/ijnsa.2011.3413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security in any of the networks became an important issue in this paper we
have implemented a security mechanism on Medium Access Control layer by Assured
Neighbor based Security Protocol to provide authentication and confidentiality
of packets along with High speed transmission for Ad hoc networks. Here we have
divided the protocol into two different parts. The first part deals with
Routing layer information; in this part we have tried to implement a possible
strategy for detecting and isolating the malicious nodes. A trust counter for
each node is determined which can be actively increased and decreased depending
upon the trust value for the purpose of forwarding the packets from source node
to destination node with the help of intermediate nodes. A threshold level is
also predetermined to detect the malicious nodes. If the value of the node in
trust counter is less than the threshold value then the node is denoted
'malicious'. The second part of our protocol deals with the security in the
link layer. For this security reason we have used CTR (Counter) approach for
authentication and encryption. We have simulated all our strategies and schemes
in NS-2, the result of which gives a conclusion that our proposed protocol i.e.
Assured Neighbor based Security Protocol can perform high packet delivery
against various intruders and also packet delivery ratio against mobility with
low delays and low overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3397</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3397</id><created>2011-10-15</created><authors><author><keyname>Ahnert</keyname><forenames>Karsten</forenames></author><author><keyname>Mulansky</keyname><forenames>Mario</forenames></author></authors><title>Odeint - Solving ordinary differential equations in C++</title><categories>cs.MS nlin.CD physics.comp-ph</categories><comments>4 pages, 1 figure</comments><journal-ref>IP Conf. Proc. - September 14, 2011 - Volume 1389, pp. 1586-1589</journal-ref><doi>10.1063/1.3637934</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many physical, biological or chemical systems are modeled by ordinary
differential equations (ODEs) and finding their solution is an every-day-task
for many scientists. Here, we introduce a new C++ library dedicated to find
numerical solutions of initial value problems of ODEs: odeint (www.odeint.com).
odeint is implemented in a highly generic way and provides extensive
interoperability at top performance. For example, due to it's modular design it
can be easily parallized with OpenMP and even runs on CUDA GPUs. Despite that,
it provides a convenient interface that allows for a simple and easy usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3425</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3425</id><created>2011-10-15</created><authors><author><keyname>Ibrahim</keyname><forenames>Mohamed</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>CellSense: An Accurate Energy-Efficient GSM Positioning System</title><categories>cs.NI</categories><comments>11 pages, 12 figures, accepted for publication in TVT(TRANSACTIONS ON
  VEHICULAR TECHNOLOGY)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-aware applications have been gaining huge interest in the last few
years. With cell phones becoming ubiquitous computing devices, cell phone
localization has become an important research problem. In this paper, we
present CellSense, a prob- abilistic RSSI-based fingerprinting location
determi- nation system for GSM phones. We discuss the chal- lenges of
implementing a probabilistic fingerprinting localization technique in GSM
networks and present the details of the CellSense systemand how it addresses
these challenges. We then extend the proposed system using a hybrid technique
that combines probabilistic and deterministic estimation to achieve both high
ac- curacy and low computational overhead.Moreover, the accuracy of the hybrid
technique is robust to changes in its parameter values. To evaluate our
proposed system, we implemented CellSense on Android-based phones. Results from
two different testbeds, represent- ing urban and rural environments, for three
differ- ent cellular providers show that CellSense provides at least 108.57%
enhancement in accuracy in rural areas and at least 89.03% in urban areas
compared to the current state of the art RSSI-based GSM localization systems.
In additional, the proposed hybrid technique provides more than 6 times and 5.4
times reduction in computational requirements compared to the state of the art
RSSI-based GSM localization systems for the rural and urban testbeds
respectively.We also evaluate the effect of changing the different system
parameters on the accuracy-complexity tradeoff and how the cell towers density
and fingerprint density affect the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3450</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3450</id><created>2011-10-15</created><authors><author><keyname>Laska</keyname><forenames>Jason N.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Regime Change: Bit-Depth versus Measurement-Rate in Compressive Sensing</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2012.2194710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced compressive sensing (CS) framework enables digital
signal acquisition systems to take advantage of signal structures beyond
bandlimitedness. Indeed, the number of CS measurements required for stable
reconstruction is closer to the order of the signal complexity than the Nyquist
rate. To date, the CS theory has focused on real-valued measurements, but in
practice, measurements are mapped to bits from a finite alphabet. Moreover, in
many potential applications the total number of measurement bits is
constrained, which suggests a tradeoff between the number of measurements and
the number of bits per measurement. We study this situation in this paper and
show that there exist two distinct regimes of operation that correspond to
high/low signal-to-noise ratio (SNR). In the measurement compression (MC)
regime, a high SNR favors acquiring fewer measurements with more bits per
measurement; in the quantization compression (QC) regime, a low SNR favors
acquiring more measurements with fewer bits per measurement. A surprise from
our analysis and experiments is that in many practical applications it is
better to operate in the QC regime, even acquiring as few as 1 bit per
measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3459</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3459</id><created>2011-10-16</created><authors><author><keyname>Huang</keyname><forenames>Chao-Wei</forenames></author></authors><title>Two-Way Training Design for Discriminatory Channel Estimation in
  Wireless MIMO Systems</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines the use of two-way training in multiple-input
multiple-output (MIMO) wireless systems to discriminate the channel estimation
performances between a legitimate receiver (LR) and an unauthorized receiver
(UR). This thesis extends upon the previously proposed discriminatory channel
estimation (DCE) scheme that allows only the transmitter to send training
signals. The goal of DCE is to minimize the channel estimation error at LR
while requiring the channel estimation error at UR to remain beyond a certain
level. If the training signal is sent only by the transmitter, the performance
discrimination between LR and UR will be limited since the training signals
help both receivers perform estimates of their downlink channels. In this work,
we consider instead the two-way training methodology that allows both the
transmitter and LR to send training signals. In this case, the training signal
sent by LR helps the transmitter obtain knowledge of the transmitter-to-LR
channel, but does not help UR estimate its downlink channel (i.e., the
transmitter-to-UR channel). With transmitter knowledge of the estimated
transmitter-to-LR channel, artificial noise (AN) can then be embedded in the
null space of the transmitter-to-LR channel to disrupt UR's channel estimation
without severely degrading the channel estimation at LR. Based on these ideas,
two-way DCE training schemes are developed for both reciprocal and
non-reciprocal channels. The optimal power allocation between training and AN
signals is devised under both average and individual power constraints.
Numerical results are provided to demonstrate the efficacy of the proposed
two-way DCE training schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3460</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3460</id><created>2011-10-16</created><authors><author><keyname>Rubio</keyname><forenames>Francisco</forenames></author><author><keyname>Mestre</keyname><forenames>Xavier</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Performance analysis and optimal selection of large mean-variance
  portfolios under estimation risk</title><categories>q-fin.PM cs.IT math.IT</categories><doi>10.1109/JSTSP.2012.2202634</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the consistency of sample mean-variance portfolios of arbitrarily
high dimension that are based on Bayesian or shrinkage estimation of the input
parameters as well as weighted sampling. In an asymptotic setting where the
number of assets remains comparable in magnitude to the sample size, we provide
a characterization of the estimation risk by providing deterministic
equivalents of the portfolio out-of-sample performance in terms of the
underlying investment scenario. The previous estimates represent a means of
quantifying the amount of risk underestimation and return overestimation of
improved portfolio constructions beyond standard ones. Well-known for the
latter, if not corrected, these deviations lead to inaccurate and overly
optimistic Sharpe-based investment decisions. Our results are based on recent
contributions in the field of random matrix theory. Along with the asymptotic
analysis, the analytical framework allows us to find bias corrections improving
on the achieved out-of-sample performance of typical portfolio constructions.
Some numerical simulations validate our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3466</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3466</id><created>2011-10-16</created><authors><author><keyname>Nechta</keyname><forenames>Ivan</forenames></author></authors><title>Effective Steganography Detection Based On Data Compression</title><categories>cs.CR</categories><comments>Steganography, steganalysis, linguistic stegosystem, text
  steganography,Texto</comments><acm-class>K.6.m</acm-class><journal-ref>Nechta I. Effective steganography detection based on data
  compression, Vestnik SIBSUTIS 2010 No.1. P. 50-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes novel text steganalysis method. The archiver &quot;Bzip2&quot;
used for detection stegotext generated by Texto stegosystem. Experiments show
that proposed approach gets better performance than typical existing methods.
The detection accuracy exceeds 99.98% for text segments with size 400 bytes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3470</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3470</id><created>2011-10-16</created><authors><author><keyname>Lievens</keyname><forenames>David</forenames></author><author><keyname>Harrison</keyname><forenames>Bill</forenames></author></authors><title>Symmetric Encapsulated Multi-Methods</title><categories>cs.PL</categories><comments>This paper is a variant of David Lievens, William Harrison: Symmetric
  encapsulated multi-methods to abstract over application structure. SAC 2009:
  1873-1880 that includes full details of the proof of the type soundness
  result stated in the original</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In object systems, classes take the role of modules, and interfaces consist
of methods. Because methods are encapsulated in objects, interfaces in object
systems do not allow abstracting over \emph{where} methods are implemented.
This implies that any change to the implementation structure may cause a
rippling effect. Sometimes this unduly restricts the scope of software
evolution, in particular for methods with multiple parameters where there is no
clear owner. We propose a simple scheme where symmetric methods may be defined
in the classes of any of their parameters. This allows client code to be
oblivious of what class contains a method implementation, and therefore immune
against it changing. When combined with multiple dynamic dispatch, this scheme
allows for modular extensibility where a method defined in one class is
overridden by a method defined in a class that is not its subtype. In this
paper, we illustrate the scheme by extending a core calculus of class-based
languages with these symmetric encapsulated multi-methods, and prove the result
sound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3531</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3531</id><created>2011-10-16</created><authors><author><keyname>Kang</keyname><forenames>Kang</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourabh</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Switching Strategies for Linear Feedback Stabilization with Sparsified
  State Measurements</title><categories>math.OC cs.SY</categories><comments>American Control Conference 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of stabilization in continuous time
linear dynamical systems using state feedback when compressive sampling
techniques are used for state measurement and reconstruction. In [5], we had
introduced the concept of using l1 reconstruction technique, commonly used in
sparse data reconstruction, for state measurement and estimation in a discrete
time linear system. In this work, we extend the previous scenario to analyse
continuous time linear systems. We investigate the effect of switching within a
set of sparsifiers, introduced in [5], on the stability of a linear plant in
continuous time settings. Initially, we analyze the problem of stabilization in
low dimensional systems, following which we generalize the results to address
the problem of stabilization in systems of arbitrary dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3535</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3535</id><created>2011-10-16</created><authors><author><keyname>Venu</keyname><forenames>Balaji</forenames></author></authors><title>Multi-core processors - An overview</title><categories>cs.AR</categories><comments>6 pages, Best Literature review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microprocessors have revolutionized the world we live in and continuous
efforts are being made to manufacture not only faster chips but also smarter
ones. A number of techniques such as data level parallelism, instruction level
parallelism and hyper threading (Intel's HT) already exists which have
dramatically improved the performance of microprocessor cores. This paper
briefs on evolution of multi-core processors followed by introducing the
technology and its advantages in today's world. The paper concludes by
detailing on the challenges currently faced by multi-core processors and how
the industry is trying to address these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3546</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3546</id><created>2011-10-16</created><updated>2013-03-09</updated><authors><author><keyname>Berman</keyname><forenames>Piotr</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Kaligounder</keyname><forenames>Lakshmi</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>On the Computational Complexity of Measuring Global Stability of Banking
  Networks</title><categories>q-fin.RM cs.CC cs.CE cs.DM</categories><comments>to appear in Algorithmica</comments><msc-class>68Q17, 68Q25, 68W40, 68R10, 05C85, 91G50, 91B99, 62P05, 62P20</msc-class><acm-class>F.2.2; J.4; G.2.1; G.2.2; G.2.3</acm-class><journal-ref>Algorithmica, 70(4), 595-647, 2014</journal-ref><doi>10.1007/s00453-013-9769-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threats on the stability of a financial system may severely affect the
functioning of the entire economy, and thus considerable emphasis is placed on
the analyzing the cause and effect of such threats. The financial crisis in the
current and past decade has shown that one important cause of instability in
global markets is the so-called financial contagion, namely the spreading of
instabilities or failures of individual components of the network to other,
perhaps healthier, components. This leads to a natural question of whether the
regulatory authorities could have predicted and perhaps mitigated the current
economic crisis by effective computations of some stability measure of the
banking networks. Motivated by such observations, we consider the problem of
defining and evaluating stabilities of both homogeneous and heterogeneous
banking networks against propagation of synchronous idiosyncratic shocks given
to a subset of banks. We formalize the homogeneous banking network model of
Nier et al. and its corresponding heterogeneous version, formalize the
synchronous shock propagation procedures, define two appropriate stability
measures and investigate the computational complexities of evaluating these
measures for various network topologies and parameters of interest. Our results
and proofs also shed some light on the properties of topologies and parameters
of the network that may lead to higher or lower stabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3547</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3547</id><created>2011-10-16</created><authors><author><keyname>Burger</keyname><forenames>John Robert</forenames></author></authors><title>A Theory of Consciousness Founded on Neurons That Behave Like Qubits</title><categories>q-bio.NC cs.ET</categories><acm-class>C.0; I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hypothesis that consciousness is a natural result of
neurons that become connected recursively, and work synchronously between short
and long term memories. Such neurons demonstrate qubit-like properties, each
supporting a probabilistic combination of true and false at a given phase.
Advantages of qubits include probabilistic modifications of cues for searching
associations in long term memory, and controlled toggling for parallel,
reversible computations to prioritize multiple recalls and to facilitate
mathematical abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3559</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3559</id><created>2011-10-16</created><updated>2012-12-17</updated><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author></authors><title>Separation of source-network coding and channel coding in wireline
  networks</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove the separation of source-network coding and channel
coding in wireline networks. For the purposes of this work, a wireline network
is any network of independent, memoryless, point-to-point, finite-alphabet
channels used to transmit dependent sources either losslessly or subject to a
distortion constraint. In deriving this result, we also prove that in a general
memoryless network with dependent sources, lossless and zero-distortion
reconstruction are equivalent provided that the conditional entropy of each
source given the other sources is non-zero. Furthermore, we extend the
separation result to the case of continuous-alphabet, point-to-point channels
such as additive white Gaussian noise (AWGN) channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3561</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3561</id><created>2011-10-16</created><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author></authors><title>Minimum Complexity Pursuit</title><categories>cs.IT math.IT</categories><comments>presented at 2011 Allerton Conference on Communication, Control and
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast growing field of compressed sensing is founded on the fact that if a
signal is 'simple' and has some 'structure', then it can be reconstructed
accurately with far fewer samples than its ambient dimension. Many different
plausible structures have been explored in this field, ranging from sparsity to
low-rankness and to finite rate of innovation. However, there are important
abstract questions that are yet to be answered. For instance, what are the
general abstract meanings of 'structure' and 'simplicity'? Do there exist
universal algorithms for recovering such simple structured objects from fewer
samples than their ambient dimension? In this paper, we aim to address these
two questions. Using algorithmic information theory tools such as Kolmogorov
complexity, we provide a unified method of describing 'simplicity' and
'structure'. We then explore the performance of an algorithm motivated by
Ocam's Razor (called MCP for minimum complexity pursuit) and show that it
requires $O(k\log n)$ number of samples to recover a signal, where $k$ and $n$
represent its complexity and ambient dimension, respectively. Finally, we
discuss more general classes of signals and provide guarantees on the
performance of MCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3563</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3563</id><created>2011-10-16</created><authors><author><keyname>DuBois</keyname><forenames>Thomas</forenames></author><author><keyname>Golbeck</keyname><forenames>Jennifer</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>Network Clustering Approximation Algorithm Using One Pass Black Box
  Sampling</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a good clustering of vertices in a network, where vertices in the
same cluster are more tightly connected than those in different clusters, is a
useful, important, and well-studied task. Many clustering algorithms scale
well, however they are not designed to operate upon internet-scale networks
with billions of nodes or more. We study one of the fastest and most memory
efficient algorithms possible - clustering based on the connected components in
a random edge-induced subgraph. When defining the cost of a clustering to be
its distance from such a random clustering, we show that this surprisingly
simple algorithm gives a solution that is within an expected factor of two or
three of optimal with either of two natural distance functions. In fact, this
approximation guarantee works for any problem where there is a probability
distribution on clusterings. We then examine the behavior of this algorithm in
the context of social network trust inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3564</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3564</id><created>2011-10-16</created><updated>2013-03-26</updated><authors><author><keyname>Karger</keyname><forenames>David R.</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems</title><categories>cs.LG cs.DS cs.HC stat.ML</categories><comments>38 pages, 4 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing systems, in which numerous tasks are electronically distributed
to numerous &quot;information piece-workers&quot;, have emerged as an effective paradigm
for human-powered solving of large scale problems in domains such as image
classification, data entry, optical character recognition, recommendation, and
proofreading. Because these low-paid workers can be unreliable, nearly all such
systems must devise schemes to increase confidence in their answers, typically
by assigning each task multiple times and combining the answers in an
appropriate manner, e.g. majority voting.
  In this paper, we consider a general model of such crowdsourcing tasks and
pose the problem of minimizing the total price (i.e., number of task
assignments) that must be paid to achieve a target overall reliability. We give
a new algorithm for deciding which tasks to assign to which workers and for
inferring correct answers from the workers' answers. We show that our
algorithm, inspired by belief propagation and low-rank matrix approximation,
significantly outperforms majority voting and, in fact, is optimal through
comparison to an oracle that knows the reliability of every worker. Further, we
compare our approach with a more general class of algorithms which can
dynamically assign tasks. By adaptively deciding which questions to ask to the
next arriving worker, one might hope to reduce uncertainty more efficiently. We
show that, perhaps surprisingly, the minimum price necessary to achieve a
target reliability scales in the same manner under both adaptive and
non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under
both scenarios. This strongly relies on the fact that workers are fleeting and
can not be exploited. Therefore, architecturally, our results suggest that
building a reliable worker-reputation system is essential to fully harnessing
the potential of adaptive designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3566</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3566</id><created>2011-10-16</created><updated>2012-09-20</updated><authors><author><keyname>Haukkanen</keyname><forenames>Pentti</forenames></author><author><keyname>Merikoski</keyname><forenames>Jorma K.</forenames></author></authors><title>Asymptotics of the number of threshold functions on a two-dimensional
  rectangular grid</title><categories>math.CO cs.IT math.IT math.LO math.NT</categories><msc-class>03B50, 05A99, 11N37, 11P21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $m,n\ge 2$, $m\le n$. It is well-known that the number of
(two-dimensional) threshold functions on an $m\times n$ rectangular grid is
{eqnarray*} t(m,n)=\frac{6}{\pi^2}(mn)^2+O(m^2n\log{n})+O(mn^2\log{\log{n}})=
\frac{6}{\pi^2}(mn)^2+O(mn^2\log{m}). {eqnarray*} We improve the error term by
showing that $$ t(m,n)=\frac{6}{\pi^2}(mn)^2+O(mn^2). $$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3569</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3569</id><created>2011-10-16</created><authors><author><keyname>Sembiring</keyname><forenames>Rahmat Widia</forenames></author><author><keyname>Zain</keyname><forenames>Jasni Mohamad</forenames></author><author><keyname>Embong</keyname><forenames>Abdullah</forenames></author></authors><title>Dimension Reduction of Health Data Clustering</title><categories>cs.DB</categories><comments>10 pages, 9 figures, published at International Journal on New
  Computer Architectures and Their Applications (IJNCAA)</comments><journal-ref>International Journal on New Computer Architectures and Their
  Applications (IJNCAA), 2011, Vol.1, No.4, 1041-1050</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current data tends to be more complex than conventional data and need
dimension reduction. Dimension reduction is important in cluster analysis and
creates a smaller data in volume and has the same analytical results as the
original representation. A clustering process needs data reduction to obtain an
efficient processing time while clustering and mitigate curse of
dimensionality. This paper proposes a model for extracting multidimensional
data clustering of health database. We implemented four dimension reduction
techniques such as Singular Value Decomposition (SVD), Principal Component
Analysis (PCA), Self Organizing Map (SOM) and FastICA. The results show that
dimension reductions significantly reduce dimension and shorten processing time
and also increased performance of cluster in several health datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3579</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3579</id><created>2011-10-17</created><authors><author><keyname>Nasri</keyname><forenames>Salem</forenames></author></authors><title>Network on Chip: a New Approach of QoS Metric Modeling Based on Calculus
  Theory</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2011.3504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A NoC is composed by IP cores (Intellectual Propriety) and switches connected
among themselves by communication channels. End-to-End Delay (EED)
communication is accomplished by the exchange of data among IP cores. Often,
the structure of particular messages is not adequate for the communication
purposes. This leads to the concept of packet switching. In the context of
NoCs, packets are composed by header, payload, and trailer. Packets are divided
into small pieces called Flits. It appears of importance, to meet the required
performance in NoC hardware resources. It should be specified in an earlier
step of the system design. The main attention should be given to the choice of
some network parameters such as the physical buffer size in the node. The EED
and packet loss are some of the critical QoS metrics. Some real-time and
multimedia applications bound up these parameters and require specific hardware
resources and particular management approaches in the NoC switch. A traffic
contract (SLA, Service Level Agreement) specifies the ability of a network or
protocol to give guaranteed performance, throughput or latency bounds based on
mutually agreed measures, usually by prioritizing traffic. A defined Quality of
Service (QoS) may be required for some types of network real time traffic or
multimedia applications. The main goal of this paper is, using the Network on
Chip modeling architecture, to define a QoS metric. We focus on the network
delay bound and packet losses. This approach is based on the Network Calculus
theory, a mathematical model to represent the data flows behavior between IPs
interconnected over NoC. We propose an approach of QoS-metric based on
QoS-parameter prioritization factors for multi applications-service using
calculus model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3584</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3584</id><created>2011-10-17</created><authors><author><keyname>B.</keyname><forenames>Ramkumar</forenames></author><author><keyname>Kittur</keyname><forenames>Harish M.</forenames></author></authors><title>Optimal Final Carry Propagate Adder Design for Parallel Multipliers</title><categories>cs.AR</categories><comments>7 pages, 7 figures, 2 tables, Submitted 0n 26 August 2011 to IEEE
  Transactions on VLSI Systems</comments><acm-class>B.2.4; B.7.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the ASIC layout level simulation of 7 types of adder structures each
of four different sizes, i.e. a total of 28 adders, we propose expressions for
the width of each of the three regions of the final Carry Propagate Adder (CPA)
to be used in parallel multipliers. We also propose the types of adders to be
used in each region that would lead to the optimal performance of the hybrid
final adders in parallel multipliers. This work evaluates the complete
performance of the analyzed designs in terms of delay, area, power through
custom design and layout in 0.18 um CMOS process technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3586</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3586</id><created>2011-10-17</created><updated>2012-04-08</updated><authors><author><keyname>Ndoundam</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Period-halving Bifurcation of a Neuronal Recurrence Equation</title><categories>cs.NE math.DS nlin.CD</categories><comments>50 pages. This paper was submitted to Complex in July 2010. This
  paper is the full version of the paper to appear in Volume 20 Issue 4 of
  Complex Systems</comments><msc-class>92B20, 34C23, 34K18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sequences generated by neuronal recurrence equations of the form
$x(n) = {\bf 1}[\sum_{j=1}^{h} a_{j} x(n-j)- \theta]$. From a neuronal
recurrence equation of memory size $h$ which describes a cycle of length
$\rho(m) \times lcm(p_0, p_1,..., p_{-1+\rho(m)})$, we construct a set of
$\rho(m)$ neuronal recurrence equations whose dynamics describe respectively
the transient of length $O(\rho(m) \times lcm(p_0, ..., p_{d}))$ and the cycle
of length $O(\rho(m) \times lcm(p_{d+1}, ..., p_{-1+\rho(m)}))$ if $0 \leq d
\leq -2+\rho(m)$ and 1 if $d=\rho(m)-1$.
  This result shows the exponential time of the convergence of neuronal
recurrence equation to fixed points and the existence of the period-halving
bifurcation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3592</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3592</id><created>2011-10-17</created><updated>2011-11-28</updated><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Information, learning and falsification</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are (at least) three approaches to quantifying information. The first,
algorithmic information or Kolmogorov complexity, takes events as strings and,
given a universal Turing machine, quantifies the information content of a
string as the length of the shortest program producing it. The second, Shannon
information, takes events as belonging to ensembles and quantifies the
information resulting from observing the given event in terms of the number of
alternate events that have been ruled out. The third, statistical learning
theory, has introduced measures of capacity that control (in part) the expected
risk of classifiers. These capacities quantify the expectations regarding
future data that learning algorithms embed into classifiers.
  This note describes a new method of quantifying information, effective
information, that links algorithmic information to Shannon information, and
also links both to capacities arising in statistical learning theory. After
introducing the measure, we show that it provides a non-universal analog of
Kolmogorov complexity. We then apply it to derive basic capacities in
statistical learning theory: empirical VC-entropy and empirical Rademacher
complexity. A nice byproduct of our approach is an interpretation of the
explanatory power of a learning algorithm in terms of the number of hypotheses
it falsifies, counted in two different ways for the two capacities. We also
discuss how effective information relates to information gain, Shannon and
mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3597</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3597</id><created>2011-10-17</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author><author><keyname>Siraj</keyname><forenames>M</forenames></author></authors><title>Performance Measurement of the Heterogeneous Network</title><categories>cs.NI</categories><comments>7 pages; ISSN : 1738-7906</comments><journal-ref>IJCSNS 9(8) (2009) 255-261</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Till today we dreamt of imperceptible delay in a network. The computer
science research grows today faster than ever offering more and more services
(computational representational, graphical, intelligent implication etc) to its
user. But the problem lies in &quot;greater the volume of services greater the
problem of delay&quot;. So tracing delay, or performance analysis focusing on time
required for computation, in a existing or newly configured network is
necessary to conclude the improvement. In this paper, we have done the job of
delay analysis in a multi-server system,. For this proposed work we have used
continuous -parameter Markov chains (Non -Birth -Death Process),for developing
the required models, and for developing the simulator we have used queuing
networking, different scheduling algorithms at the servers queue and process
scheduling . The work can be further extended to test the performance of
wireless domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3619</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3619</id><created>2011-10-17</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Winzen</keyname><forenames>Carola</forenames></author></authors><title>Playing Mastermind With Constant-Size Memory</title><categories>cs.DS cs.NE</categories><comments>23 pages</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the classic board game of Mastermind with $n$ holes and a constant
number of colors. A result of Chv\'atal (Combinatorica 3 (1983), 325-329)
states that the codebreaker can find the secret code with $\Theta(n / \log n)$
questions. We show that this bound remains valid if the codebreaker may only
store a constant number of guesses and answers. In addition to an intrinsic
interest in this question, our result also disproves a conjecture of Droste,
Jansen, and Wegener (Theory of Computing Systems 39 (2006), 525-544) on the
memory-restricted black-box complexity of the OneMax function class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3639</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3639</id><created>2011-10-17</created><updated>2012-06-13</updated><authors><author><keyname>Kotek</keyname><forenames>Tomer</forenames></author></authors><title>Complexity of Ising Polynomials</title><categories>cs.CC cond-mat.stat-mech math-ph math.CO math.MP</categories><journal-ref>Combinatorics, Probability and Computing, Volume 21, Issue 5
  (2012), pp. 743-772</journal-ref><doi>10.1017/S0963548312000259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the partition function of the Ising model from
statistical mechanics, which is used to study phase transitions in physical
systems. A special case of interest is that of the Ising model with constant
energies and external field. One may consider such an Ising system as a simple
graph together with vertex and edge weights. When these weights are considered
indeterminates, the partition function for the constant case is a trivariate
polynomial Z(G;x,y,z). This polynomial was studied with respect to its
approximability by L. A. Goldberg, M. Jerrum and M. Paterson in 2003.
Z(G;x,y,z) generalizes a bivariate polynomial Z(G;t,y), which was studied by D.
Andr\'{e}n and K. Markstr\&quot;{o}m in 2009.
  We consider the complexity of Z(G;t,y) and Z(G;x,y,z) in comparison to that
of the Tutte polynomial, which is well-known to be closely related to the Potts
model in the absence of an external field. We show that Z(G;\x,\y,\z) is
#P-hard to evaluate at all points in $mathbb{Q}^3$, except those in an
exception set of low dimension, even when restricted to simple graphs which are
bipartite and planar. A counting version of the Exponential Time Hypothesis,
#ETH, was introduced by H. Dell, T. Husfeldt and M. Wahl\'{e}n in 2010 in order
to study the complexity of the Tutte polynomial. In analogy to their results,
we give a dichotomy theorem stating that evaluations of Z(G;t,y) either take
exponential time in the number of vertices of $G$ to compute, or can be done in
polynomial time. Finally, we give an algorithm for computing Z(G;x,y,z) in
polynomial time on graphs of bounded clique-width, which is not known in the
case of the Tutte polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3649</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3649</id><created>2011-10-17</created><updated>2012-03-15</updated><authors><author><keyname>Boyer</keyname><forenames>D.</forenames></author><author><keyname>Lipman</keyname><forenames>Y.</forenames></author><author><keyname>Clair</keyname><forenames>E. St.</forenames></author><author><keyname>Puente</keyname><forenames>J.</forenames></author><author><keyname>Funkhouser</keyname><forenames>T.</forenames></author><author><keyname>Patel</keyname><forenames>B.</forenames></author><author><keyname>Jernvall</keyname><forenames>J.</forenames></author><author><keyname>Daubechies</keyname><forenames>I.</forenames></author></authors><title>Algorithms to automatically quantify the geometric similarity of
  anatomical surfaces</title><categories>math.NA cs.CV cs.GR</categories><comments>Changes with respect to v1, v2: an Erratum was added, correcting the
  references for one of the three datasets. Note that the datasets and code for
  this paper can be obtained from the Data Conservancy (see Download column on
  v1, v2)</comments><journal-ref>PNAS 2011 108 (45) 18221-18226</journal-ref><doi>10.1073/pnas.1112822108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe new approaches for distances between pairs of 2-dimensional
surfaces (embedded in 3-dimensional space) that use local structures and global
information contained in inter-structure geometric relationships. We present
algorithms to automatically determine these distances as well as geometric
correspondences. This is motivated by the aspiration of students of natural
science to understand the continuity of form that unites the diversity of life.
At present, scientists using physical traits to study evolutionary
relationships among living and extinct animals analyze data extracted from
carefully defined anatomical correspondence points (landmarks). Identifying and
recording these landmarks is time consuming and can be done accurately only by
trained morphologists. This renders these studies inaccessible to
non-morphologists, and causes phenomics to lag behind genomics in elucidating
evolutionary patterns. Unlike other algorithms presented for morphological
correspondences our approach does not require any preliminary marking of
special features or landmarks by the user. It also differs from other seminal
work in computational geometry in that our algorithms are polynomial in nature
and thus faster, making pairwise comparisons feasible for significantly larger
numbers of digitized surfaces. We illustrate our approach using three datasets
representing teeth and different bones of primates and humans, and show that it
leads to highly accurate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3655</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3655</id><created>2011-10-17</created><authors><author><keyname>Silva</keyname><forenames>Jorge Luiz e</forenames></author><author><keyname>Lopes</keyname><forenames>Joelmir Jose</forenames></author><author><keyname>Silva</keyname><forenames>Bruno de Abreu</forenames></author><author><keyname>da Silva</keyname><forenames>Antonio Carlos Fernandes</forenames></author></authors><title>Accelerating Algorithms using a Dataflow Graph in a Reconfigurable
  System</title><categories>cs.AR</categories><comments>13 pages, 8 figures, 1 listing, 1 algorithm, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the acceleration of algorithms using a design of a field
programmable gate array (FPGA) as a prototype of a static dataflow architecture
is discussed. The static dataflow architecture using operators interconnected
by parallel buses was implemented. Accelerating algorithms using a dataflow
graph in a reconfigurable system shows the potential for high computation
rates. The results of benchmarks implemented using the static dataflow
architecture are reported at the end of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3672</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3672</id><created>2011-10-17</created><authors><author><keyname>Giordano</keyname><forenames>Laura</forenames></author><author><keyname>Martelli</keyname><forenames>Alberto</forenames></author><author><keyname>Dupr&#xe9;</keyname><forenames>Daniele Theseider</forenames></author></authors><title>Reasoning about Actions with Temporal Answer Sets</title><categories>cs.AI cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we combine Answer Set Programming (ASP) with Dynamic Linear
Time Temporal Logic (DLTL) to define a temporal logic programming language for
reasoning about complex actions and infinite computations. DLTL extends
propositional temporal logic of linear time with regular programs of
propositional dynamic logic, which are used for indexing temporal modalities.
The action language allows general DLTL formulas to be included in domain
descriptions to constrain the space of possible extensions. We introduce a
notion of Temporal Answer Set for domain descriptions, based on the usual
notion of Answer Set. Also, we provide a translation of domain descriptions
into standard ASP and we use Bounded Model Checking techniques for the
verification of DLTL constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3687</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3687</id><created>2011-10-17</created><authors><author><keyname>Sanderson</keyname><forenames>Robert</forenames></author><author><keyname>Brugman</keyname><forenames>Hennie</forenames></author><author><keyname>Albritton</keyname><forenames>Benjamin</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>Evaluating the SharedCanvas Manuscript Data Model in CATCHPlus</title><categories>cs.DL</categories><comments>8 pages, accepted to SDH2011. Images down-sampled due to arXiv
  restrictions</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we present the SharedCanvas model for describing the layout of
culturally important, hand-written objects such as medieval manuscripts, which
is intended to be used as a common input format to presentation interfaces. The
model is evaluated using two collections from CATCHPlus not consulted during
the design phase, each with their own complex requirements, in order to
determine if further development is required or if the model is ready for
general usage. The model is applied to the new collections, revealing several
new areas of concern for user interface production and discovery of the
constituent resources. However, the fundamental information modelling aspects
of SharedCanvas and the underlying Open Annotation Collaboration ontology are
demonstrated to be sufficient to cover the challenging new requirements. The
distributed, Linked Open Data approach is validated as an important methodology
to seamlessly allow simultaneous interaction with multiple repositories, and at
the same time to facilitate both scholarly commentary and crowd-sourcing of the
production of transcriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3695</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3695</id><created>2011-10-17</created><authors><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Jiang</keyname><forenames>Xianhua</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author></authors><title>Geometric methods for estimation of structured covariances</title><categories>math.OC cs.SY math.ST stat.TH</categories><comments>12 pages, 3 figures</comments><msc-class>93E10, 93E12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider problems of estimation of structured covariance matrices, and in
particular of matrices with a Toeplitz structure. We follow a geometric
viewpoint that is based on some suitable notion of distance. To this end, we
overview and compare several alternatives metrics and divergence measures. We
advocate a specific one which represents the Wasserstein distance between the
corresponding Gaussians distributions and show that it coincides with the
so-called Bures/Hellinger distance between covariance matrices as well. Most
importantly, besides the physically appealing interpretation, computation of
the metric requires solving a linear matrix inequality (LMI). As a consequence,
computations scale nicely for problems involving large covariance matrices, and
linear prior constraints on the covariance structure are easy to handle. We
compare this transportation/Bures/Hellinger metric with the maximum likelihood
and the Burg methods as to their performance with regard to estimation of power
spectra with spectral lines on a representative case study from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3704</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3704</id><created>2011-10-17</created><authors><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Kini</keyname><forenames>Dileep</forenames></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames></author></authors><title>Using non-convex approximations for efficient analysis of timed automata</title><categories>cs.LO cs.FL</categories><comments>Extended version of FSTTCS 2011 paper</comments><doi>10.4230/LIPIcs.FSTTCS.2011.78</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The reachability problem for timed automata asks if there exists a path from
an initial state to a target state. The standard solution to this problem
involves computing the zone graph of the automaton, which in principle could be
infinite. In order to make the graph finite, zones are approximated using an
extrapolation operator. For reasons of efficiency in current algorithms
extrapolation of a zone is always a zone and in particular it is convex.
  In this paper, we propose to solve the reachability problem without such
extrapolation operators. To ensure termination, we provide an efficient
algorithm to check if a zone is included in the so called region closure of
another. Although theoretically better, closure cannot be used in the standard
algorithm since a closure of a zone may not be convex.
  An additional benefit of the proposed approach is that it permits to
calculate approximating parameters on-the-fly during exploration of the zone
graph, as opposed to the current methods which do it by a static analysis of
the automaton prior to the exploration. This allows for further improvements in
the algorithm. Promising experimental results are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3705</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3705</id><created>2011-10-17</created><updated>2013-01-18</updated><authors><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames></author></authors><title>Better abstractions for timed automata</title><categories>cs.LO cs.FL</categories><comments>Extended version of LICS 2012 paper + fixes Theorem 34</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reachability problem for timed automata. A standard solution
to this problem involves computing a search tree whose nodes are abstractions
of zones. These abstractions preserve underlying simulation relations on the
state space of the automaton. For both effectiveness and efficiency reasons,
they are parametrized by the maximal lower and upper bounds (LU-bounds)
occurring in the guards of the automaton. We consider the aLU abstraction
defined by Behrmann et al. Since this abstraction can potentially yield
non-convex sets, it has not been used in implementations. We prove that aLU
abstraction is the biggest abstraction with respect to LU-bounds that is sound
and complete for reachability. We also provide an efficient technique to use
the aLU abstraction to solve the reachability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3706</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3706</id><created>2011-10-17</created><authors><author><keyname>Ramli</keyname><forenames>Carroline Dewi Puspa Kencana</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author></authors><title>The Logic of XACML - Extended</title><categories>cs.CR cs.LO</categories><comments>Extended paper from The Logic of XACML, presented in FACS 2011 (8th
  International Symposium on Formal Aspects of Component Software)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the international standard XACML 3.0 for describing security access
control policy in a compositional way. Our main contribution is to derive a
logic that precisely captures the idea behind the standard and to formally
define the semantics of the policy combining algorithms of XACML. To guard
against modelling artefacts we provide an alternative way of characterizing the
policy combining algorithms and we formally prove the equivalence of these
approaches. This allows us to pinpoint the shortcoming of previous approaches
to formalization based either on Belnap logic or on D-algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3711</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3711</id><created>2011-10-17</created><updated>2011-11-18</updated><authors><author><keyname>Dom&#xed;nguez</keyname><forenames>Jose M.</forenames></author><author><keyname>Crespo</keyname><forenames>Alejandro J. C.</forenames></author><author><keyname>G&#xf3;mez-Gesteira</keyname><forenames>Moncho</forenames></author></authors><title>Optimization strategies for parallel CPU and GPU implementations of a
  meshfree particle method</title><categories>cs.PF cs.CE</categories><comments>18 pages, 21 figures</comments><msc-class>68Uxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the current focus in high performance computing (HPC) for
computational fluid dynamics (CFD) deals with grid based methods. However,
parallel implementations for new meshfree particle methods such as Smoothed
Particle Hydrodynamics (SPH) are less studied. In this work, we present
optimizations for both central processing unit (CPU) and graphics processing
unit (GPU) of a SPH method. These optimization strategies can be further
applied to many other meshfree methods. The obtained performance for each
architecture and a comparison between the most efficient implementations for
CPU and GPU are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3717</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3717</id><created>2011-10-17</created><updated>2011-10-18</updated><authors><author><keyname>Staiger</keyname><forenames>C.</forenames></author><author><keyname>Cadot</keyname><forenames>S.</forenames></author><author><keyname>Kooter</keyname><forenames>R.</forenames></author><author><keyname>Dittrich</keyname><forenames>M.</forenames></author><author><keyname>Mueller</keyname><forenames>T.</forenames></author><author><keyname>Klau</keyname><forenames>G. W.</forenames></author><author><keyname>Wessels</keyname><forenames>L. F. A.</forenames></author></authors><title>A critical evaluation of network and pathway based classifiers for
  outcome prediction in breast cancer</title><categories>cs.LG q-bio.QM</categories><doi>10.1371/journal.pone.0034796</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several classifiers that combine primary tumor data, like gene
expression data, and secondary data sources, such as protein-protein
interaction networks, have been proposed for predicting outcome in breast
cancer. In these approaches, new composite features are typically constructed
by aggregating the expression levels of several genes. The secondary data
sources are employed to guide this aggregation. Although many studies claim
that these approaches improve classification performance over single gene
classifiers, the gain in performance is difficult to assess. This stems mainly
from the fact that different breast cancer data sets and validation procedures
are employed to assess the performance. Here we address these issues by
employing a large cohort of six breast cancer data sets as benchmark set and by
performing an unbiased evaluation of the classification accuracies of the
different approaches. Contrary to previous claims, we find that composite
feature classifiers do not outperform simple single gene classifiers. We
investigate the effect of (1) the number of selected features; (2) the specific
gene set from which features are selected; (3) the size of the training set and
(4) the heterogeneity of the data set on the performance of composite feature
and single gene classifiers. Strikingly, we find that randomization of
secondary data sources, which destroys all biological information in these
sources, does not result in a deterioration in performance of composite feature
classifiers. Finally, we show that when a proper correction for gene set size
is performed, the stability of single gene sets is similar to the stability of
composite feature sets. Based on these results there is currently no reason to
prefer prognostic classifiers based on composite features over single gene
classifiers for predicting outcome in breast cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3741</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3741</id><created>2011-10-17</created><updated>2013-01-07</updated><authors><author><keyname>Hsiao</keyname><forenames>Ko-Jen</forenames></author><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Calder</keyname><forenames>Jeff</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Multi-criteria Anomaly Detection using Pareto Depth Analysis</title><categories>cs.LG cs.CV cs.DB stat.ML</categories><comments>Removed an unnecessary line from Algorithm 1</comments><acm-class>I.5; G.3; H.2.8</acm-class><journal-ref>Advances in Neural Information Processing Systems 25 (2012)
  854-862</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying patterns in a data set that exhibit
anomalous behavior, often referred to as anomaly detection. In most anomaly
detection algorithms, the dissimilarity between data samples is calculated by a
single criterion, such as Euclidean distance. However, in many cases there may
not exist a single dissimilarity measure that captures all possible anomalous
patterns. In such a case, multiple criteria can be defined, and one can test
for anomalies by scalarizing the multiple criteria using a linear combination
of them. If the importance of the different criteria are not known in advance,
the algorithm may need to be executed multiple times with different choices of
weights in the linear combination. In this paper, we introduce a novel
non-parametric multi-criteria anomaly detection method using Pareto depth
analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies
under multiple criteria without having to run an algorithm multiple times with
different choices of weights. The proposed PDA approach scales linearly in the
number of criteria and is provably better than linear combinations of the
criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3767</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3767</id><created>2011-10-17</created><updated>2011-10-25</updated><authors><author><keyname>J&#xe9;gou</keyname><forenames>Herv&#xe9;</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Furon</keyname><forenames>Teddy</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Fuchs</keyname><forenames>Jean-Jacques</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Anti-sparse coding for approximate nearest neighbor search</title><categories>cs.CV cs.DB cs.IR cs.IT math.IT</categories><comments>submitted to ICASSP'2012; RR-7771 (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a binarization scheme for vectors of high dimension based
on the recent concept of anti-sparse coding, and shows its excellent
performance for approximate nearest neighbor search. Unlike other binarization
schemes, this framework allows, up to a scaling factor, the explicit
reconstruction from the binary representation of the original vector. The paper
also shows that random projections which are used in Locality Sensitive Hashing
algorithms, are significantly outperformed by regular frames for both synthetic
and real data if the number of bits exceeds the vector dimensionality, i.e.,
when high precision is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3774</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3774</id><created>2011-10-17</created><authors><author><keyname>Feizi</keyname><forenames>Soheil</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Time-Stampless Adaptive Nonuniform Sampling for Stochastic Signals</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing (partially
  presented at Allerton 2010)</comments><journal-ref>IEEE Trans. on Signal Processing, vol. 60, no. 10, pp. 5440-5450,
  October 2012</journal-ref><doi>10.1109/TSP.2012.2208633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a time-stampless adaptive nonuniform sampling
(TANS) framework, in which time increments between samples are determined by a
function of the $m$ most recent increments and sample values. Since only past
samples are used in computing time increments, it is not necessary to save
sampling times (time stamps) for use in the reconstruction process. We focus on
two TANS schemes for discrete-time stochastic signals: a greedy method, and a
method based on dynamic programming. We analyze the performances of these
schemes by computing (or bounding) their trade-offs between sampling rate and
expected reconstruction distortion for autoregressive and Markovian signals.
Simulation results support the analysis of the sampling schemes. We show that,
by opportunistically adapting to local signal characteristics, TANS may lead to
improved power efficiency in some applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3832</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3832</id><created>2011-10-17</created><authors><author><keyname>Asztalos</keyname><forenames>Andrea</forenames></author><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Distributed flow optimization and cascading effects in weighted complex
  networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Eur. Phys. J. B 85 (2012) 288</journal-ref><doi>10.1140/epjb/e2012-30122-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the effect of a specific edge weighting scheme $\sim (k_i
k_j)^{\beta}$ on distributed flow efficiency and robustness to cascading
failures in scale-free networks. In particular, we analyze a simple, yet
fundamental distributed flow model: current flow in random resistor networks.
By the tuning of control parameter $\beta$ and by considering two general cases
of relative node processing capabilities as well as the effect of bandwidth, we
show the dependence of transport efficiency upon the correlations between the
topology and weights. By studying the severity of cascades for different
control parameter $\beta$, we find that network resilience to cascading
overloads and network throughput is optimal for the same value of $\beta$ over
the range of node capacities and available bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3843</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3843</id><created>2011-10-17</created><updated>2012-03-27</updated><authors><author><keyname>Zhang</keyname><forenames>Haotian</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Robustness of Information Diffusion Algorithms to Locally Bounded
  Adversaries</title><categories>cs.SI cs.DC cs.MA cs.SY math.OC physics.soc-ph</categories><comments>Preprint of results to appear at 2012 American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of diffusing information in networks that contain
malicious nodes. We assume that each normal node in the network has no
knowledge of the network topology other than an upper bound on the number of
malicious nodes in its neighborhood. We introduce a topological property known
as r-robustness of a graph, and show that this property provides improved
bounds on tolerating malicious behavior, in comparison to traditional concepts
such as connectivity and minimum degree. We use this topological property to
analyze the canonical problems of distributed consensus and broadcasting, and
provide sufficient conditions for these operations to succeed. Finally, we
provide a construction for r-robust graphs and show that the common
preferential-attachment model for scale-free networks produces a robust graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3844</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3844</id><created>2011-10-17</created><authors><author><keyname>Khan</keyname><forenames>Wazir Zada</forenames></author><author><keyname>Aalsalem</keyname><forenames>Mohammed Y.</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author></authors><title>A Graphical Password Based System for Small Mobile Devices</title><categories>cs.CR</categories><comments>10 Pages, 7 figures, 1 Table</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 2, 2011, 145-154</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passwords provide security mechanism for authentication and protection
services against unwanted access to resources. A graphical based password is
one promising alternatives of textual passwords. According to human psychology,
humans are able to remember pictures easily. In this paper, we have proposed a
new hybrid graphical password based system, which is a combination of
recognition and recall based techniques that offers many advantages over the
existing systems and may be more convenient for the user. Our scheme is
resistant to shoulder surfing attack and many other attacks on graphical
passwords. This scheme is proposed for smart mobile devices (like smart phones
i.e. ipod, iphone, PDAs etc) which are more handy and convenient to use than
traditional desktop computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3850</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3850</id><created>2011-10-17</created><authors><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>On the Power of Adaptivity in Sparse Recovery</title><categories>cs.DS</categories><comments>18 pages; appearing at FOCS 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation
$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is
to recover $x*$ such that ||x-x*||_p &lt;= C min_{k-sparse x'} ||x-x'||_q for some
constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or
$p=q=2$, this task can be accomplished using $m=O(k \log (n/k))$ non-adaptive
measurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].
  In this paper we show that if one is allowed to perform measurements that are
adaptive, then the number of measurements can be considerably reduced.
Specifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k
log log (n eps/k))$ measurements that uses $O(log* k \log \log (n eps/k))$
rounds. This is a significant improvement over the best possible non-adaptive
bound. - A scheme with $m=O((1/eps) k log (k/eps) + k \log (n/k))$ measurements
that uses /two/ rounds. This improves over the best possible non-adaptive
bound. To the best of our knowledge, these are the first results of this type.
As an independent application, we show how to solve the problem of finding a
duplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using
$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best
possible space complexity achievable using a single pass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3853</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3853</id><created>2011-10-17</created><authors><author><keyname>Honda</keyname><forenames>Kohei</forenames></author><author><keyname>Mycroft</keyname><forenames>Alan</forenames></author></authors><title>Proceedings Third Workshop on Programming Language Approaches to
  Concurrency and communication-cEntric Software</title><categories>cs.PL cs.DC</categories><comments>EPTCS 69, 2011</comments><proxy>EPTCS</proxy><acm-class>D3.2</acm-class><doi>10.4204/EPTCS.69</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the proceedings of PLACES'10, the 3rd Workshop on Programming
Language Approaches to Concurrency and Communication-cEntric Software, held in
Pathos, Cyprus, on 21st Mach, 2010, co-located with the ETAPS federated
conferences. PLACES aims to offer a forum where researchers from different
fields exchange new ideas on one of the central challenges in programming in
near future, the development of programming methodologies and infrastructures
where concurrency and distribution are a norm rather than a marginal concern.
The Program Committee, after a careful and thorough reviewing process, selected
for presentation in the programme 10 papers out of 14 submissions. Each
submission was evaluated by at least two referees, and the accepted papers were
selected during two weeks' electronic discussions. This post-proceedings
contain the papers which are based on these submissions, incorporating the
result of these and further reviews, resulting in strengthened technical
results and presentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3854</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3854</id><created>2011-10-17</created><updated>2015-03-17</updated><authors><author><keyname>Zhao</keyname><forenames>Yunpeng</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Zhu</keyname><forenames>Ji</forenames></author></authors><title>Consistency of community detection in networks under degree-corrected
  stochastic block models</title><categories>math.ST cs.SI physics.soc-ph stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1036 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org). With Corrections</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1036</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 2266-2292</journal-ref><doi>10.1214/12-AOS1036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is a fundamental problem in network analysis, with
applications in many diverse areas. The stochastic block model is a common tool
for model-based community detection, and asymptotic tools for checking
consistency of community detection under the block model have been recently
developed. However, the block model is limited by its assumption that all nodes
within a community are stochastically equivalent, and provides a poor fit to
networks with hubs or highly varying node degrees within communities, which are
common in practice. The degree-corrected stochastic block model was proposed to
address this shortcoming and allows variation in node degrees within a
community while preserving the overall block community structure. In this paper
we establish general theory for checking consistency of community detection
under the degree-corrected stochastic block model and compare several community
detection criteria under both the standard and the degree-corrected models. We
show which criteria are consistent under which models and constraints, as well
as compare their relative performance in practice. We find that methods based
on the degree-corrected block model, which includes the standard block model as
a special case, are consistent under a wider class of models and that
modularity-type methods require parameter constraints for consistency, whereas
likelihood-based methods do not. On the other hand, in practice, the degree
correction involves estimating many more parameters, and empirically we find it
is only worth doing if the node degrees within communities are indeed highly
variable. We illustrate the methods on simulated networks and on a network of
political blogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3855</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3855</id><created>2011-10-17</created><authors><author><keyname>Pang</keyname><forenames>Yimin</forenames></author><author><keyname>Honold</keyname><forenames>Thomas</forenames></author></authors><title>An Upper Bound on Broadcast Subspace Codes</title><categories>cs.IT math.AG math.IT</categories><comments>4 pages; The 1st International ICST Workshop on Network Coding in
  Wireless Relay Networks, 2011</comments><msc-class>94B65</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear operator broadcast channel (LOBC) models the scenario of multi-rate
packet broadcasting over a network, when random network coding is applied. This
paper presents the framework of algebraic coding for LOBCs and provides a
Hamming-like upper bound on (multishot) subspace codes for LOBCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3860</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3860</id><created>2011-10-17</created><authors><author><keyname>Almquist</keyname><forenames>Zack W.</forenames></author><author><keyname>Butts</keyname><forenames>Carter T.</forenames></author></authors><title>Contending Parties: A Logistic Choice Analysis of Inter- and Intra-group
  Blog Citation Dynamics in the 2004 US Presidential Election</title><categories>cs.SI physics.soc-ph stat.AP stat.OT</categories><report-no>MBS 11-06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2004 US Presidential Election cycle marked the debut of Internet-based
media such as blogs and social networking websites as institutionally
recognized features of the American political landscape. Using a longitudinal
sample of all DNC/RNC-designated blog-citation networks we are able to test the
influence of various strategic, institutional, and balance-theoretic mechanisms
and exogenous factors such as seasonality and political events on the
propensity of blogs to cite one another over time. Capitalizing on the temporal
resolution of our data, we utilize an autoregressive network regression
framework to carry out inference for a logistic choice process. Using a
combination of deviance-based model selection criteria and simulation-based
model adequacy tests, we identify the combination of processes that best
characterizes the choice behavior of the contending blogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3875</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3875</id><created>2011-10-18</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Hui</forenames></author><author><keyname>Kan</keyname><forenames>Haibin</forenames></author></authors><title>Constructing and Counting Even-Variable Symmetric Boolean Functions with
  Algebraic Immunity not Less Than $d$</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explicitly construct a large class of symmetric Boolean
functions on $2k$ variables with algebraic immunity not less than $d$, where
integer $k$ is given arbitrarily and $d$ is a given suffix of $k$ in binary
representation. If let $d = k$, our constructed functions achieve the maximum
algebraic immunity. Remarkably, $2^{\lfloor \log_2{k} \rfloor + 2}$ symmetric
Boolean functions on $2k$ variables with maximum algebraic immunity are
constructed, which is much more than the previous constructions. Based on our
construction, a lower bound of symmetric Boolean functions with algebraic
immunity not less than $d$ is derived, which is $2^{\lfloor \log_2{d} \rfloor +
2(k-d+1)}$. As far as we know, this is the first lower bound of this kind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3876</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3876</id><created>2011-10-18</created><updated>2012-11-18</updated><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Kan</keyname><forenames>Haibin</forenames></author><author><keyname>Kokichi</keyname><forenames>Futatsugi</forenames></author></authors><title>A Note on &quot;On the Construction of Boolean Functions with Optimal
  Algebraic Immunity&quot;</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to the quality of
  ideas</comments><doi>10.1587/transfun.E94.A.1877</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we go further on the &quot;basis exchange&quot; idea presented in
\cite{LiNa1} by using Mobious inversion. We show that the matrix
$S_1(f)S_0(f)^{-1}$ has a nice form when $f$ is chosen to be the majority
function, where $S_1(f)$ is the matrix with row vectors $\upsilon_k(\alpha)$
for all $\alpha \in 1_f$ and $S_0(f)=S_1(f\oplus1)$. And an exact counting for
Boolean functions with maximum algebraic immunity by exchanging one point in
on-set with one point in off-set of the majority function is given.
Furthermore, we present a necessary condition according to weight distribution
for Boolean functions to achieve algebraic immunity not less than a given
number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3879</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3879</id><created>2011-10-18</created><authors><author><keyname>Inokuchi</keyname><forenames>Akihiro</forenames></author><author><keyname>Ikuta</keyname><forenames>Hiroaki</forenames></author><author><keyname>Washio</keyname><forenames>Takashi</forenames></author></authors><title>GTRACE-RS: Efficient Graph Sequence Mining using Reverse Search</title><categories>cs.DB</categories><doi>10.1587/transinf.E95.D.1947</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mining of frequent subgraphs from labeled graph data has been studied
extensively. Furthermore, much attention has recently been paid to frequent
pattern mining from graph sequences. A method, called GTRACE, has been proposed
to mine frequent patterns from graph sequences under the assumption that
changes in graphs are gradual. Although GTRACE mines the frequent patterns
efficiently, it still needs substantial computation time to mine the patterns
from graph sequences containing large graphs and long sequences. In this paper,
we propose a new version of GTRACE that enables efficient mining of frequent
patterns based on the principle of a reverse search. The underlying concept of
the reverse search is a general scheme for designing efficient algorithms for
hard enumeration problems. Our performance study shows that the proposed method
is efficient and scalable for mining both long and large graph sequence
patterns and is several orders of magnitude faster than the original GTRACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3888</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3888</id><created>2011-10-18</created><updated>2011-10-20</updated><authors><author><keyname>Yuming</keyname><forenames>Xu</forenames></author></authors><title>Handling controversial arguments by matrix</title><categories>cs.AI</categories><comments>21 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce matrix and its block to the Dung's theory of argumentation
framework. It is showed that each argumentation framework has a matrix
representation, and the indirect attack relation and indirect defence relation
can be characterized by computing the matrix. This provide a powerful
mathematics way to determine the &quot;controversial arguments&quot; in an argumentation
framework. Also, we introduce several kinds of blocks based on the matrix, and
various prudent semantics of argumentation frameworks can all be determined by
computing and comparing the matrices and their blocks which we have defined. In
contrast with traditional method of directed graph, the matrix method has an
excellent advantage: computability(even can be realized on computer easily).
So, there is an intensive perspective to import the theory of matrix to the
research of argumentation frameworks and its related areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3898</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3898</id><created>2011-10-18</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Gentner</keyname><forenames>Christian</forenames><affiliation>DLR</affiliation></author><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>An Interpolation Procedure for List Decoding Reed--Solomon codes Based
  on Generalized Key Equations</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory (2011)</comments><proxy>ccsd</proxy><doi>10.1109/TIT.2011.2162160</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key step of syndrome-based decoding of Reed-Solomon codes up to half the
minimum distance is to solve the so-called Key Equation. List decoding
algorithms, capable of decoding beyond half the minimum distance, are based on
interpolation and factorization of multivariate polynomials. This article
provides a link between syndrome-based decoding approaches based on Key
Equations and the interpolation-based list decoding algorithms of Guruswami and
Sudan for Reed-Solomon codes. The original interpolation conditions of
Guruswami and Sudan for Reed-Solomon codes are reformulated in terms of a set
of Key Equations. These equations provide a structured homogeneous linear
system of equations of Block-Hankel form, that can be solved by an adaption of
the Fundamental Iterative Algorithm. For an $(n,k)$ Reed-Solomon code, a
multiplicity $s$ and a list size $\listl$, our algorithm has time complexity
\ON{\listl s^4n^2}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3907</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3907</id><created>2011-10-18</created><updated>2012-07-04</updated><authors><author><keyname>Sun</keyname><forenames>Peng</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author></authors><title>AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem</title><categories>stat.ML cs.AI cs.CV</categories><comments>8-pages camera ready version for ICML2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an improvement to model learning when using multi-class
LogitBoost for classification. Motivated by the statistical view, LogitBoost
can be seen as additive tree regression. Two important factors in this setting
are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the
dense Hessian matrices that arise when computing tree node split gain and node
value fittings. In general, this setting is too complicated for a tractable
model learning algorithm. However, too aggressive simplification of the setting
may lead to degraded performance. For example, the original LogitBoost is
outperformed by ABC-LogitBoost due to the latter's more careful treatment of
the above two factors.
  In this paper we propose techniques to address the two main difficulties of
the LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is
vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block
coordinate descent that exploits the dense Hessian when computing tree split
gain and node values. Higher classification accuracy and faster convergence
rates are observed for a range of public data sets when compared to both the
original and the ABC-LogitBoost implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3917</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3917</id><created>2011-10-18</created><authors><author><keyname>Lueks</keyname><forenames>Wouter</forenames></author><author><keyname>Mokbel</keyname><forenames>Bassam</forenames></author><author><keyname>Biehl</keyname><forenames>Michael</forenames></author><author><keyname>Hammer</keyname><forenames>Barbara</forenames></author></authors><title>How to Evaluate Dimensionality Reduction? - Improving the Co-ranking
  Matrix</title><categories>cs.LG cs.IR</categories><comments>This is an article for the Dagstuhl Preprint Archive, belonging to
  Dagstuhl Seminar No. 11341 &quot;Learning in the context of very high dimensional
  data&quot;</comments><report-no>DPA-11341</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing number of dimensionality reduction methods available for data
visualization has recently inspired the development of quality assessment
measures, in order to evaluate the resulting low-dimensional representation
independently from a methods' inherent criteria. Several (existing) quality
measures can be (re)formulated based on the so-called co-ranking matrix, which
subsumes all rank errors (i.e. differences between the ranking of distances
from every point to all others, comparing the low-dimensional representation to
the original data). The measures are often based on the partioning of the
co-ranking matrix into 4 submatrices, divided at the K-th row and column,
calculating a weighted combination of the sums of each submatrix. Hence, the
evaluation process typically involves plotting a graph over several (or even
all possible) settings of the parameter K. Considering simple artificial
examples, we argue that this parameter controls two notions at once, that need
not necessarily be combined, and that the rectangular shape of submatrices is
disadvantageous for an intuitive interpretation of the parameter. We debate
that quality measures, as general and flexible evaluation tools, should have
parameters with a direct and intuitive interpretation as to which specific
error types are tolerated or penalized. Therefore, we propose to replace K with
two parameters to control these notions separately, and introduce a differently
shaped weighting on the co-ranking matrix. The two new parameters can then
directly be interpreted as a threshold up to which rank errors are tolerated,
and a threshold up to which the rank-distances are significant for the
evaluation. Moreover, we propose a color representation of local quality to
visually support the evaluation process for a given mapping, where every point
in the mapping is colored according to its local contribution to the overall
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3939</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3939</id><created>2011-10-18</created><authors><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Clone Structures in Voters' Preferences</title><categories>cs.GT</categories><comments>35 pages, 3 figures</comments><acm-class>I.2.11; F.2.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In elections, a set of candidates ranked consecutively (though possibly in
different order) by all voters is called a clone set, and its members are
called clones. A clone structure is a family of all clone sets of a given
election. In this paper we study properties of clone structures. In particular,
we give an axiomatic characterization of clone structures, show their
hierarchical structure, and analyze clone structures in single-peaked and
single-crossing elections. We give a polynomial-time algorithm that finds a
minimal collection of clones that need to be collapsed for an election to
become single-peaked, and we show that this problem is NP-hard for
single-crossing elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3959</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3959</id><created>2011-10-18</created><authors><author><keyname>Trinca</keyname><forenames>Dragos</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author></authors><title>Parallel Algorithms for DNA Probe Placement on Small Oligonucleotide
  Arrays</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oligonucleotide arrays are used in a wide range of genomic analyses, such as
gene expression profiling, comparative genomic hybridization, chromatin
immunoprecipitation, SNP detection, etc. During fabrication, the sites of an
oligonucleotide array are selectively exposed to light in order to activate
oligonucleotides for further synthesis. Optical effects can cause unwanted
illumination at masked sites that are adjacent to the sites intentionally
exposed to light. This results in synthesis of unforeseen sequences in masked
sites and compromises interpretation of experimental data. To reduce such
uncertainty, one can exploit freedom in how probes are assigned to array sites.
The border length minimization problem (BLMP) seeks a placement of probes that
minimizes the sum of border lengths in all masks. In this paper, we propose two
parallel algorithms for the BLMP. The proposed parallel algorithms have the
local-search paradigm at their core, and are especially developed for the BLMP.
The results reported show that, for small microarrays with at most 1156 probes,
the proposed parallel algorithms perform better than the best previous
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3961</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3961</id><created>2011-10-18</created><authors><author><keyname>Gaur</keyname><forenames>Vibha</forenames></author><author><keyname>Sharma</keyname><forenames>Neeraj Kumar</forenames></author></authors><title>A Dynamic Framework of Reputation Systems for an Agent Mediated e-market</title><categories>cs.MA cs.AI cs.IT cs.SI math.IT</categories><comments>19 Pages; International Journal of Computer Science Issues
  (IJCSI),Vol 8, Issue 4, July 2011, ISSN(online): 1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of an agent mediated e-market system lies in the underlying
reputation management system to improve the quality of services in an
information asymmetric e-market. Reputation provides an operatable metric for
establishing trustworthiness between mutually unknown online entities.
Reputation systems encourage honest behaviour and discourage malicious
behaviour of participating agents in the e-market. A dynamic reputation model
would provide virtually instantaneous knowledge about the changing e-market
environment and would utilise Internets' capacity for continuous interactivity
for reputation computation. This paper proposes a dynamic reputation framework
using reinforcement learning and fuzzy set theory that ensures judicious use of
information sharing for inter-agent cooperation. This framework is sensitive to
the changing parameters of e-market like the value of transaction and the
varying experience of agents with the purpose of improving inbuilt defense
mechanism of the reputation system against various attacks so that e-market
reaches an equilibrium state and dishonest agents are weeded out of the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.3969</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.3969</id><created>2011-10-17</created><authors><author><keyname>Sadi</keyname><forenames>Muhammad Sheikh</forenames></author><author><keyname>Khan</keyname><forenames>Md. Mizanur Rahman</forenames></author><author><keyname>Uddin</keyname><forenames>Md. Nazim</forenames></author><author><keyname>J&#xfc;rjens</keyname><forenames>Jan</forenames></author></authors><title>An Efficient Approach towards Mitigating Soft Errors Risks</title><categories>cs.OH</categories><comments>Signal &amp; Image Processing: An International Journal(SIPIJ) Vol. 2,
  No. 3, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smaller feature size, higher clock frequency and lower power consumption are
of core concerns of today's nano-technology, which has been resulted by
continuous downscaling of CMOS technologies. The resultant 'device shrinking'
reduces the soft error tolerance of the VLSI circuits, as very little energy is
needed to change their states. Safety critical systems are very sensitive to
soft errors. A bit flip due to soft error can change the value of critical
variable and consequently the system control flow can completely be changed
which leads to system failure. To minimize soft error risks, a novel
methodology is proposed to detect and recover from soft errors considering only
'critical code blocks' and 'critical variables' rather than considering all
variables and/or blocks in the whole program. The proposed method shortens
space and time overhead in comparison to existing dominant approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4015</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4015</id><created>2011-10-18</created><authors><author><keyname>Franceschet</keyname><forenames>Massimo</forenames></author></authors><title>The large-scale structure of journal citation networks</title><categories>cs.SI cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the large-scale structure of the journal citation network built
from information contained in the Thomson-Reuters Journal Citation Reports. To
this end, we take advantage of the network science paraphernalia and explore
network properties like density, percolation robustness, average and largest
node distances, reciprocity, incoming and outgoing degree distributions, as
well as assortative mixing by node degrees. We discover that the journal
citation network is a dense, robust, small, and reciprocal world. Furthermore,
in and out node degree distributions display long-tails, with few vital
journals and many trivial ones, and they are strongly positively correlated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4034</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4034</id><created>2011-10-18</created><authors><author><keyname>Kontchakov</keyname><forenames>Roman</forenames></author><author><keyname>Nenov</keyname><forenames>Yavor</forenames></author><author><keyname>Pratt-Hartmann</keyname><forenames>Ian</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>Michael</forenames></author></authors><title>Topological Logics with Connectedness over Euclidean Spaces</title><categories>cs.LO math.GT</categories><msc-class>68T30 (Primary) 03D15, 68Q17 (Secondary)</msc-class><acm-class>I.2.4; F.4.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantifier-free languages, Bc and Bc0, obtained by augmenting
the signature of Boolean algebras with a unary predicate representing,
respectively, the property of being connected, and the property of having a
connected interior. These languages are interpreted over the regular closed
sets of n-dimensional Euclidean space (n greater than 1) and, additionally,
over the regular closed polyhedral sets of n-dimensional Euclidean space. The
resulting logics are examples of formalisms that have recently been proposed in
the Artificial Intelligence literature under the rubric &quot;Qualitative Spatial
Reasoning.&quot; We prove that the satisfiability problem for Bc is undecidable over
the regular closed polyhedra in all dimensions greater than 1, and that the
satisfiability problem for both languages is undecidable over both the regular
closed sets and the regular closed polyhedra in the Euclidean plane. However,
we also prove that the satisfiability problem for Bc0 is NP-complete over the
regular closed sets in all dimensions greater than 2, while the corresponding
problem for the regular closed polyhedra is ExpTime-complete. Our results show,
in particular, that spatial reasoning over Euclidean spaces is much harder than
reasoning over arbitrary topological spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4050</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4050</id><created>2011-10-18</created><authors><author><keyname>Aggarwal</keyname><forenames>Rohit</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Joint Scheduling and Resource Allocation in OFDMA Downlink Systems via
  ACK/NAK Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of joint scheduling and resource
allocation in the OFDMA downlink, with the goal of maximizing an expected
long-term goodput-based utility subject to an instantaneous sum-power
constraint, and where the feedback to the base station consists only of
ACK/NAKs from recently scheduled users. We first establish that the optimal
solution is a partially observable Markov decision process (POMDP), which is
impractical to implement. In response, we propose a greedy approach to joint
scheduling and resource allocation that maintains a posterior channel
distribution for every user, and has only polynomial complexity. For
frequency-selective channels with Markov time-variation, we then outline a
recursive method to update the channel posteriors, based on the ACK/NAK
feedback, that is made computationally efficient through the use of particle
filtering. To gauge the performance of our greedy approach relative to that of
the optimal POMDP, we derive a POMDP performance upper-bound. Numerical
experiments show that, for slowly fading channels, the performance of our
greedy scheme is relatively close to the upper bound, and much better than
fixed-power random user scheduling (FP-RUS), despite its relatively low
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4052</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4052</id><created>2011-10-18</created><updated>2013-09-29</updated><authors><author><keyname>Kleiman</keyname><forenames>Howard</forenames></author></authors><title>The General Traveling Salesman Problem, Version 5</title><categories>cs.DS math.CO</categories><comments>This Version 5 corrects some omissions that occurred on the earlier
  version as well as some corrections. arXiv admin note: substantial text
  overlap with arXiv:math/0508212</comments><msc-class>05</msc-class><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is example 5 in chapter 5. Let H be an n-cycle. A permutation s is
H-admissible if Hs = H' where H' is an n-cycle. Here we define a 19 X 19
matrix, M, in the following way: We obtain the remainders modulo 100 of each of
the smallest 342 odd primes. we obtain the remainders modulo 100 of each of the
primes. They are placed in M according to the original value of each prime.
Thus their placement depends on the the original ordinal values of the primes
according to size. We use this ordering to place the primes in M. Let H_0 be an
initial 19 cycles arbitrarily chosen. We apply a sequence of up to [ln(n)+1]
H_0 3-cycles to obtain a 19-cycle of smaller value than H_0, call the new
19-cycle H_1. We follow this procedure to obtain H_1. We call [ln(n)] + 1 a
chain. We add up the values of the 19-cycles in each chain. This procedure
continues until we cannot obtain a chain the sum of whose values is not
negative. COMMENT. I've renamed the document &quot;Yhe General Traveling Salesman
Problem, Version 5&quot;. I preciously named it &quot;The Traveling Salesman, Version 5&quot;.
Although the algorithms work on the GTSP, I thought that more people would
google it if it was named &quot;The Traveling Salesman Problem.&quot; Rhar qas because my
work is only available through arxiv.org,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4069</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4069</id><created>2011-10-18</created><updated>2012-02-08</updated><authors><author><keyname>Mohammadi</keyname><forenames>Elaheh</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Aghaeinia</keyname><forenames>Hassan</forenames></author></authors><title>Transmission of non-linear binary input functions over a CDMA System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of transmission of binary input non-linear functions
over a network of mobiles based on CDMA. Motivation for this study comes from
the application of using cheap measurement devices installed on personal
cell-phones to monitor environmental parameters such as air pollution,
temperature and noise level. Our model resembles the MAC model of Nazer and
Gastpar except that the encoders are restricted to be CDMA encoders. Unlike the
work of Nazer and Gastpar whose main attention is transmission of linear
functions, we deal with non-linear functions with binary inputs. A main
contribution of this paper is a lower bound on the computational capacity for
this problem. While in the traditional CDMA system the signature matrix of the
CDMA system preferably has independent rows, in our setup the signature matrix
of the CDMA system is viewed as the parity check matrix of a linear code,
reflecting our treatment of the interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4076</identifier>
 <datestamp>2011-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4076</id><created>2011-09-26</created><authors><author><keyname>Bulitko</keyname><forenames>V.</forenames></author><author><keyname>Lee</keyname><forenames>G.</forenames></author></authors><title>Learning in Real-Time Search: A Unifying Framework</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  119-157, 2006</journal-ref><doi>10.1613/jair.1789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time search methods are suited for tasks in which the agent is
interacting with an initially unknown environment in real time. In such
simultaneous planning and learning problems, the agent has to select its
actions in a limited amount of time, while sensing only a local part of the
environment centered at the agents current location. Real-time heuristic search
agents select actions using a limited lookahead search and evaluating the
frontier states with a heuristic function. Over repeated experiences, they
refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and
hardware agents has led to an explosion of real-time search algorithms over the
last two decades. Not only is a potential user confronted with a hodgepodge of
algorithms, but he also faces the choice of control parameters they use. In
this paper we address both problems. The first contribution is an introduction
of a simple three-parameter framework (named LRTS) which extracts the core
ideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,
SLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they
are unified and extended with additional features. Second, we prove
completeness and convergence of any algorithm covered by the LRTS framework.
Third, we prove several upper-bounds relating the control parameters and
solution quality. Finally, we analyze the influence of the three control
parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as
well as routing in ad hoc sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4077</identifier>
 <datestamp>2012-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4077</id><created>2011-10-18</created><updated>2012-01-12</updated><authors><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author><author><keyname>Scott</keyname><forenames>Alexander</forenames></author></authors><title>The Parameterised Complexity of List Problems on Graphs of Bounded
  Treewidth</title><categories>cs.CC cs.DM cs.DS math.CO</categories><comments>24 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the parameterised complexity of several list problems on graphs,
with parameter treewidth or pathwidth. In particular, we show that List Edge
Chromatic Number and List Total Chromatic Number are fixed parameter tractable,
parameterised by treewidth, whereas List Hamilton Path is W[1]-hard, even
parameterised by pathwidth. These results resolve two open questions of
Fellows, Fomin, Lokshtanov, Rosamond, Saurabh, Szeider and Thomassen (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4094</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4094</id><created>2011-10-18</created><updated>2014-01-17</updated><authors><author><keyname>Baldan</keyname><forenames>Paolo</forenames></author><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author></authors><title>A Logic for True Concurrency</title><categories>cs.LO</categories><comments>31 pages, a preliminary version appeared in CONCUR 2010</comments><acm-class>F.3.1; F.4.1; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a logic for true concurrency whose formulae predicate about events
in computations and their causal dependencies. The induced logical equivalence
is hereditary history preserving bisimilarity, and fragments of the logic can
be identified which correspond to other true concurrent behavioural
equivalences in the literature: step, pomset and history preserving
bisimilarity. Standard Hennessy-Milner logic, and thus (interleaving)
bisimilarity, is also recovered as a fragment. We also propose an extension of
the logic with fixpoint operators, thus allowing to describe causal and
concurrency properties of infinite computations. We believe that this work
contributes to a rational presentation of the true concurrent spectrum and to a
deeper understanding of the relations between the involved behavioural
equivalences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4099</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4099</id><created>2011-10-18</created><updated>2011-10-19</updated><authors><author><keyname>Maldonado</keyname><forenames>Carlos Eduardo</forenames></author><author><keyname>G&#xf3;mez-Cruz</keyname><forenames>Nelson Alfonso</forenames></author></authors><title>The Complexification of Engineering</title><categories>nlin.AO cs.AI</categories><comments>9 pages, 1 figure, 1 table, preprint; Complexity. In the print (2011)</comments><journal-ref>2012, Complexity 17(4), pp. 8-15</journal-ref><doi>10.1002/cplx.20395</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper deals with the arrow of complexification of engineering. We claim
that the complexification of engineering consists in (a) that shift throughout
which engineering becomes a science; thus it ceases to be a (mere) praxis or
profession; (b) becoming a science, engineering can be considered as one of the
sciences of complexity. In reality, the complexification of engineering is the
process by which engineering can be studied, achieved and understood in terms
of knowledge, and not of goods and services any longer. Complex engineered
systems and bio-inspired engineering are so far the two expressions of a
complex engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4102</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4102</id><created>2011-10-18</created><authors><author><keyname>Albers</keyname><forenames>D. J.</forenames></author><author><keyname>Hripcsak</keyname><forenames>George</forenames></author></authors><title>Using time-delayed mutual information to discover and interpret temporal
  correlation structure in complex populations</title><categories>nlin.CD cs.IT math.DS math.IT stat.ME</categories><doi>10.1063/1.3675621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses how to calculate and interpret the time-delayed mutual
information for a complex, diversely and sparsely measured, possibly
non-stationary population of time-series of unknown composition and origin. The
primary vehicle used for this analysis is a comparison between the time-delayed
mutual information averaged over the population and the time-delayed mutual
information of an aggregated population (here aggregation implies the
population is conjoined before any statistical estimates are implemented).
Through the use of information theoretic tools, a sequence of practically
implementable calculations are detailed that allow for the average and
aggregate time-delayed mutual information to be interpreted. Moreover, these
calculations can be also be used to understand the degree of homo- or
heterogeneity present in the population. To demonstrate that the proposed
methods can be used in nearly any situation, the methods are applied and
demonstrated on the time series of glucose measurements from two different
subpopulations of individuals from the Columbia University Medical Center
electronic health record repository, revealing a picture of the composition of
the population as well as physiological features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4123</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4123</id><created>2011-10-18</created><updated>2012-05-27</updated><authors><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Positive words carry less information than negative words</title><categories>cs.CL cs.IR physics.soc-ph</categories><comments>16 pages, 3 figures, 3 tables</comments><journal-ref>EPJ Data Science 2012, 1:3</journal-ref><doi>10.1140/epjds3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the frequency of word use is not only determined by the word
length \cite{Zipf1935} and the average information content
\cite{Piantadosi2011}, but also by its emotional content. We have analyzed
three established lexica of affective word usage in English, German, and
Spanish, to verify that these lexica have a neutral, unbiased, emotional
content. Taking into account the frequency of word usage, we find that words
with a positive emotional content are more frequently used. This lends support
to Pollyanna hypothesis \cite{Boucher1969} that there should be a positive bias
in human expression. We also find that negative words contain more information
than positive words, as the informativeness of a word increases uniformly with
its valence decrease. Our findings support earlier conjectures about (i) the
relation between word frequency and information content, and (ii) the impact of
positive emotions on communication and social links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4126</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4126</id><created>2011-10-18</created><authors><author><keyname>Atapattu</keyname><forenames>Saman</forenames></author><author><keyname>Jing</keyname><forenames>Yindi</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Relay Selection and Performance Analysis in Multiple-User Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the relay selection (RS) problem in networks with
multiple users and multiple common amplify-and-forward (AF) relays. Considering
the overall quality-of-service of the network, we first specify our definition
of optimal RS for multiple-user relay networks. Then an optimal RS (ORS)
algorithm is provided, which is a straightforward extension of an RS scheme in
the literature that maximizes the minimum end-to-end receive signal-to-noise
ratio (SNR) of all users. The complexity of the ORS is quadratic in both the
number of users and the number of relays. Then a suboptimal RS (SRS) scheme is
proposed, which has linear complexity in the number of relays and quadratic
complexity in the number of users. Furthermore, diversity orders of both the
ORS and the proposed SRS are theoretically derived and compared with those of a
naive RS scheme and the single-user RS network. It is shown that the ORS
achieves full diversity; while the diversity order of the SRS decreases with
the the number of users. For two-user networks, the outage probabilities and
array gains corresponding to the minimum SNR of the RS schemes are derived in
closed forms. It is proved that the advantage of the SRS over the naive RS
scheme increases as the number of relays in the network increases. Simulation
results are provided to corroborate the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4136</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4136</id><created>2011-10-18</created><authors><author><keyname>Tan</keyname><forenames>Shuo</forenames></author></authors><title>The non-abelian squares are not context-free</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answering a recent question of Crochemore, we prove that the language of
words that are not abelian squares is not context-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4150</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4150</id><created>2011-10-18</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author></authors><title>Traffic-Redundancy Aware Network Design</title><categories>cs.DS</categories><comments>17 pages. To be published in the proceedings of the Twenty-Third
  Annual ACM-SIAM Symposium on Discrete Algorithms</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider network design problems for information networks where routers
can replicate data but cannot alter it. This functionality allows the network
to eliminate data-redundancy in traffic, thereby saving on routing costs. We
consider two problems within this framework and design approximation
algorithms.
  The first problem we study is the traffic-redundancy aware network design
(RAND) problem. We are given a weighted graph over a single server and many
clients. The server owns a number of different data packets and each client
desires a subset of the packets; the client demand sets form a laminar set
system. Our goal is to connect every client to the source via a single path,
such that the collective cost of the resulting network is minimized. Here the
transportation cost over an edge is its weight times times the number of
distinct packets that it carries.
  The second problem is a facility location problem that we call RAFL. Here the
goal is to find an assignment from clients to facilities such that the total
cost of routing packets from the facilities to clients (along unshared paths),
plus the total cost of &quot;producing&quot; one copy of each desired packet at each
facility is minimized.
  We present a constant factor approximation for the RAFL and an O(log P)
approximation for RAND, where P is the total number of distinct packets. We
remark that P is always at most the number of different demand sets desired or
the number of clients, and is generally much smaller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4156</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4156</id><created>2011-10-18</created><authors><author><keyname>Alves</keyname><forenames>Nuno</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Hu</keyname><forenames>Raymond</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Deni&#xe9;lou</keyname><forenames>Pierre-Malo</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Secure Execution of Distributed Session Programs</title><categories>cs.DC cs.CR cs.NI cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><acm-class>D.3.0;D.3.1;D.3.3;F.3.2;C.2.2;C.2.4</acm-class><journal-ref>EPTCS 69, 2011, pp. 1-11</journal-ref><doi>10.4204/EPTCS.69.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of the SJ Framework for session-based distributed programming
is part of recent and ongoing research into integrating session types and
practical, real-world programming languages. SJ programs featuring session
types (protocols) are statically checked by the SJ compiler to verify the key
property of communication safety, meaning that parties engaged in a session
only communicate messages, including higher-order communications via session
delegation, that are compatible with the message types expected by the
recipient.
  This paper presents current work on security aspects of the SJ Framework.
Firstly, we discuss our implementation experience from improving the SJ Runtime
platform with security measures to protect and augment communication safety at
runtime. We implement a transport component for secure session execution that
uses a modified TLS connection with authentication based on the Secure Remote
Password (SRP) protocol. The key technical point is the delicate treatment of
secure session delegation to counter a previous vulnerability. We find that the
modular design of the SJ Runtime, based on the notion of an Abstract Transport
for session communication, supports rapid extension to utilise additional
transports whilst separating this concern from the application-level session
programming task. In the second part of this abstract, we formally prove the
target security properties by modelling the extended SJ delegation protocols in
the pi-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4157</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4157</id><created>2011-10-18</created><authors><author><keyname>Campos</keyname><forenames>Joana</forenames><affiliation>University of Lisbon</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>University of Lisbon</affiliation></author></authors><title>Channels as Objects in Concurrent Object-Oriented Programming</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 69, 2011, pp. 12-28</journal-ref><doi>10.4204/EPTCS.69.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is often a sort of a protocol associated to each class, stating when
and how certain methods should be called. Given that this protocol is, if at
all, described in the documentation accompanying the class, current mainstream
object-oriented languages cannot provide for the verification of client code
adherence against the sought class behaviour. We have defined a class-based
concurrent object-oriented language that formalises such protocols in the form
of usage types. Usage types are attached to class definitions, allowing for the
specification of (1) the available methods, (2) the tests clients must perform
on the result of methods, and (3) the object status - linear or shared - all of
which depend on the object's state. Our work extends the recent approach on
modular session types by eliminating channel operations, and defining the
method call as the single communication primitive in both sequential and
concurrent settings. In contrast to previous works, we define a single category
for objects, instead of distinct categories for linear and for shared objects,
and let linear objects evolve into shared ones. We introduce a standard sync
qualifier to prevent thread interference in certain operations on shared
objects. We formalise the language syntax, the operational semantics, and a
type system that enforces by static typing that methods are called only when
available, and by a single client if so specified in the usage type. We
illustrate the language via a complete example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4159</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4159</id><created>2011-10-18</created><authors><author><keyname>Carbone</keyname><forenames>Marco</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Grohmann</keyname><forenames>Davide</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Hildebrandt</keyname><forenames>Thomas T.</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>L&#xf3;pez</keyname><forenames>Hugo A.</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>A Logic for Choreographies</title><categories>cs.PL cs.DC cs.LO</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><acm-class>F.3.1; F.3.2; C.2.4</acm-class><journal-ref>EPTCS 69, 2011, pp. 29-43</journal-ref><doi>10.4204/EPTCS.69.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore logical reasoning for the global calculus, a coordination model
based on the notion of choreography, with the aim to provide a methodology for
specification and verification of structured communications. Starting with an
extension of Hennessy-Milner logic, we present the global logic (GL), a modal
logic describing possible interactions among participants in a choreography. We
illustrate its use by giving examples of properties on service specifications.
Finally, we show that, despite GL is undecidable, there is a significant
decidable fragment which we provide with a sound and complete proof system for
checking validity of formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4160</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4160</id><created>2011-10-18</created><authors><author><keyname>Gerakios</keyname><forenames>Prodromos</forenames><affiliation>National Technical University of Athens</affiliation></author><author><keyname>Papaspyrou</keyname><forenames>Nikolaos</forenames><affiliation>National Technical University of Athens</affiliation></author><author><keyname>Sagonas</keyname><forenames>Konstantinos</forenames><affiliation>National Technical University of Athens</affiliation></author></authors><title>A Type System for Unstructured Locking that Guarantees Deadlock Freedom
  without Imposing a Lock Ordering</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><acm-class>D.3.3; D.3.2; D.1.3</acm-class><journal-ref>EPTCS 69, 2011, pp. 44-58</journal-ref><doi>10.4204/EPTCS.69.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deadlocks occur in concurrent programs as a consequence of cyclic resource
acquisition between threads. In this paper we present a novel type system that
guarantees deadlock freedom for a language with references, unstructured
locking primitives, and locks which are implicitly associated with references.
The proposed type system does not impose a strict lock acquisition order and
thus increases programming language expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4161</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4161</id><created>2011-10-18</created><authors><author><keyname>Hildebrandt</keyname><forenames>Thomas T.</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Mukkamala</keyname><forenames>Raghava Rao</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Declarative Event-Based Workflow as Distributed Dynamic Condition
  Response Graphs</title><categories>cs.LO cs.FL cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 69, 2011, pp. 59-73</journal-ref><doi>10.4204/EPTCS.69.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Dynamic Condition Response Graphs (DCR Graphs) as a declarative,
event-based process model inspired by the workflow language employed by our
industrial partner and conservatively generalizing prime event structures. A
dynamic condition response graph is a directed graph with nodes representing
the events that can happen and arrows representing four relations between
events: condition, response, include, and exclude. Distributed DCR Graphs is
then obtained by assigning roles to events and principals. We give a graphical
notation inspired by related work by van der Aalst et al. We exemplify the use
of distributed DCR Graphs on a simple workflow taken from a field study at a
Danish hospital, pointing out their flexibility compared to imperative workflow
models. Finally we provide a mapping from DCR Graphs to Buchi-automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4163</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4163</id><created>2011-10-18</created><authors><author><keyname>Imai</keyname><forenames>Keigo</forenames><affiliation>IT Planning Inc., Japan</affiliation></author><author><keyname>Yuen</keyname><forenames>Shoji</forenames><affiliation>Graduate School of Information Science, Nagoya University, Japan</affiliation></author><author><keyname>Agusa</keyname><forenames>Kiyoshi</forenames><affiliation>Graduate School of Information Science, Nagoya University, Japan</affiliation></author></authors><title>Session Type Inference in Haskell</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><acm-class>D.1.1; D.3.3</acm-class><journal-ref>EPTCS 69, 2011, pp. 74-91</journal-ref><doi>10.4204/EPTCS.69.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an inference system for a version of the Pi-calculus in Haskell
for the session type proposed by Honda et al. The session type is very useful
in checking if the communications are well-behaved. The full session type
implementation in Haskell was first presented by Pucella and Tov, which is
'semi-automatic' in that the manual operations for the type representation was
necessary. We give an automatic type inference for the session type by using a
more abstract representation for the session type based on the 'de Bruijn
levels'. We show an example of the session type inference for a simple SMTP
client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4164</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4164</id><created>2011-10-18</created><authors><author><keyname>Lange</keyname><forenames>Julien</forenames><affiliation>Department of Computer Science, University of Leicester</affiliation></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames><affiliation>Department of Computer Science, University of Leicester</affiliation></author></authors><title>A Modular Toolkit for Distributed Interactions</title><categories>cs.DC cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 69, 2011, pp. 92-110</journal-ref><doi>10.4204/EPTCS.69.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the design, architecture, and implementation of a toolkit which
supports some theories for distributed interactions. The main design principles
of our architecture are flexibility and modularity. Our main goal is to provide
an easily extensible workbench to encompass current algorithms and incorporate
future developments of the theories. With the help of some examples, we
illustrate the main features of our toolkit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4165</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4165</id><created>2011-10-18</created><authors><author><keyname>Martins</keyname><forenames>Francisco</forenames><affiliation>LaSIGE and University of Lisbon, Portugal</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>LaSIGE and University of Lisbon, Portugal</affiliation></author><author><keyname>Cogumbreiro</keyname><forenames>Tiago</forenames><affiliation>LaSIGE and University of Lisbon, Portugal</affiliation></author></authors><title>Types for X10 Clocks</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2010, arXiv:1110.3853</comments><proxy>EPTCS</proxy><acm-class>D.1.3; D.2.4; D.3.1</acm-class><journal-ref>EPTCS 69, 2011, pp. 111-129</journal-ref><doi>10.4204/EPTCS.69.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  X10 is a modern language built from the ground up to handle future parallel
systems, from multicore machines to cluster configurations. We take a closer
look at a pair of synchronisation mechanisms: finish and clocks. The former
waits for the termination of parallel computations, the latter allow multiple
concurrent activities to wait for each other at certain points in time. In
order to better understand these concepts we study a type system for a stripped
down version of X10. The main result assures that well typed programs do not
run into the errors identified in the X10 language reference, namely the
ClockUseException. The study will open, we hope, doors to a more flexible
utilisation of clocks in the X10 language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4174</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4174</id><created>2011-10-18</created><updated>2011-12-07</updated><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>Park</keyname><forenames>Hosung</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Chung</keyname><forenames>Habong</forenames></author></authors><title>Clipping Noise Cancellation for OFDM and OFDMA Systems Using Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be acccessible</comments><msc-class>94A12, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose clipping noise cancellation scheme using compressed
sensing (CS) for orthogonal frequency division multiplexing (OFDM) systems. In
the proposed scheme, only the data tones with high reliability are exploited in
reconstructing the clipping noise instead of the whole data tones. For
reconstructing the clipping noise using a fraction of the data tones at the
receiver, the CS technique is applied. The proposed scheme is also applicable
to interleaved orthogonal frequency division multiple access (OFDMA) systems
due to the decomposition of fast Fourier transform (FFT) structure. Numerical
analysis shows that the proposed scheme performs well for clipping noise
cancellation of both OFDM and OFDMA systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4175</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4175</id><created>2011-10-18</created><authors><author><keyname>Gang</keyname><forenames>Wang</forenames></author><author><keyname>Xia</keyname><forenames>Dai</forenames></author></authors><title>The Price of Anarchy (POA) of network coding and routing based on
  average pricing mechanism</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>5 pages,3 figures,Submitted to ICC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The congestion pricing is an efficient allocation approach to mediate demand
and supply of network resources. Different from the previous pricing using
Affine Marginal Cost (AMC), we focus on studying the game between network
coding and routing flows sharing a single link when users are price
anticipating based on an Average Cost Sharing (ACS) pricing mechanism. We
characterize the worst-case efficiency bounds of the game compared with the
optimal, i.e., the price-of anarchy (POA), which can be low bound 50% with
routing only. When both network coding and routing are applied, the POA can be
as low as 4/9. Therefore, network coding cannot improve the POA significantly
under the ACS. Moreover, for more efficient use of limited resources, it
indicates the sharing users have a higher tendency to choose network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4181</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4181</id><created>2011-10-19</created><authors><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France, LRI, MSR - INRIA</affiliation></author></authors><title>Injecting External Solutions Into CMA-ES</title><categories>cs.LG</categories><comments>No. RR-7748 (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report considers how to inject external candidate solutions into the
CMA-ES algorithm. The injected solutions might stem from a gradient or a Newton
step, a surrogate model optimizer or any other oracle or search mechanism. They
can also be the result of a repair mechanism, for example to render infeasible
solutions feasible. Only small modifications to the CMA-ES are necessary to
turn injection into a reliable and effective method: too long steps need to be
tightly renormalized. The main objective of this report is to reveal this
simple mechanism. Depending on the source of the injected solutions,
interesting variants of CMA-ES arise. When the best-ever solution is always
(re-)injected, an elitist variant of CMA-ES with weighted multi-recombination
arises. When \emph{all} solutions are injected from an \emph{external} source,
the resulting algorithm might be viewed as \emph{adaptive encoding} with
step-size control. In first experiments, injected solutions of very good
quality lead to a convergence speed twice as fast as on the (simple) sphere
function without injection. This means that we observe an impressive speed-up
on otherwise difficult to solve functions. Single bad injected solutions on the
other hand do no significant harm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4193</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4193</id><created>2011-10-19</created><updated>2012-04-10</updated><authors><author><keyname>Chiu</keyname><forenames>Jiawei</forenames></author><author><keyname>Demanet</keyname><forenames>Laurent</forenames></author></authors><title>Sublinear randomized algorithms for skeleton decompositions</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a $n$ by $n$ matrix. A skeleton decomposition is any factorization
of the form $CUR$ where $C$ comprises columns of $A$, and $R$ comprises rows of
$A$. In this paper, we consider uniformly sampling $\l\simeq k \log n$ rows and
columns to produce a skeleton decomposition. The algorithm runs in $O(\l^3)$
time, and has the following error guarantee. Let $\norm{\cdot}$ denote the
2-norm. Suppose $A\simeq X B Y^T$ where $X,Y$ each have $k$ orthonormal
columns. Assuming that $X,Y$ are incoherent, we show that with high
probability, the approximation error $\norm{A-CUR}$ will scale with
$(n/\l)\norm{A-X B Y^T}$ or better. A key step in this algorithm involves
regularization. This step is crucial for a nonsymmetric $A$ as empirical
results suggest. Finally, we use our proof framework to analyze two existing
algorithms in an intuitive way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4196</identifier>
 <datestamp>2012-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4196</id><created>2011-10-19</created><updated>2012-02-11</updated><authors><author><keyname>Tian</keyname><forenames>Miaomiao</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author></authors><title>Cryptanalysis of a lattice-based proxy signature scheme</title><categories>cs.CR</categories><comments>Journal version of the paper will be appearing in International
  Journal of Network Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proxy signature scheme allows a proxy signer to sign messages on behalf of
an original signer. Proxy signature schemes have found numerous practical
applications such as grid computing, mobile agent systems and cloud
applications. Recently, Jiang et al. proposed the first lattice-based proxy
signature scheme and claimed that their scheme provides all the security
properties of a secure proxy signature scheme. However, in this paper, we
disprove their claim and show that an original signer is able to forge a proxy
signature on any message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4198</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4198</id><created>2011-10-19</created><updated>2013-07-11</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Chapelle</keyname><forenames>Olivier</forenames></author><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>A Reliable Effective Terascale Linear Learning System</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system and a set of techniques for learning linear predictors
with convex losses on terascale datasets, with trillions of features, {The
number of features here refers to the number of non-zero entries in the data
matrix.} billions of training examples and millions of parameters in an hour
using a cluster of 1000 machines. Individually none of the component techniques
are new, but the careful synthesis required to obtain an efficient
implementation is. The result is, up to our knowledge, the most scalable and
efficient linear learning system reported in the literature (as of 2011 when
our experiments were conducted). We describe and thoroughly evaluate the
components of the system, showing the importance of the various design choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4201</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4201</id><created>2011-10-19</created><updated>2011-12-07</updated><authors><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>The Complexity of Weighted Counting for Acyclic Conjunctive Queries</title><categories>cs.CC cs.LO math.LO</categories><comments>28 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a study of weighted counting of the solutions of acyclic
conjunctive queries ($\ACQ$). The unweighted quantifier free version of this
problem is known to be tractable (for combined complexity), but it is also
known that introducing even a single quantified variable makes it $\sP$-hard.
We first show that weighted counting for quantifier-free $\ACQ$ is still
tractable and that even minimalistic extensions of the problem lead to hard
cases. We then introduce a new parameter for quantified queries that permits to
isolate large island of tractability. We show that, up to a standard assumption
from parameterized complexity, this parameter fully characterizes tractable
subclasses for counting weighted solutions of $\ACQ$ queries. Thus we
completely determine the tractability frontier for weighted counting for
$\ACQ$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4205</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4205</id><created>2011-10-19</created><authors><author><keyname>Carlson</keyname><forenames>Rosalie</forenames></author><author><keyname>Flood</keyname><forenames>Stephen</forenames></author><author><keyname>O'Neill</keyname><forenames>Kevin</forenames></author><author><keyname>Su</keyname><forenames>Francis Edward</forenames></author></authors><title>A Tur'an-type problem for circular arc graphs</title><categories>math.CO cs.DM</categories><comments>18 pages, 8 figures, related papers at
  http://www.math.hmc.edu/~su/papers.html</comments><msc-class>05C75, 91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A circular arc graph is the intersection graph of a collection of connected
arcs on the circle. We solve a Tur'an-type problem for circular arc graphs: for
n arcs, if m and M are the minimum and maximum number of arcs that contain a
common point, what is the maximum number of edges the circular arc graph can
contain? We establish a sharp bound and produce a maximal construction. For a
fixed m, this can be used to show that if the circular arc graph has enough
edges, there must be a point that is covered by at least M arcs. In the case
m=0, we recover results for interval graphs established by Abbott and
Katchalski (1979). We suggest applications to voting situations with interval
or circular political spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4241</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4241</id><created>2011-10-19</created><updated>2012-08-10</updated><authors><author><keyname>Dall'Aglio</keyname><forenames>Marco</forenames></author><author><keyname>Di Luca</keyname><forenames>Camilla</forenames></author></authors><title>Finding maxmin allocations in cooperative and competitive fair division</title><categories>math.OC cs.GT math.PR</categories><comments>20 pages, 3 figures. This third version improves the overll
  presentation; Optimization and Control (math.OC), Computer Science and Game
  Theory (cs.GT), Probability (math.PR)</comments><report-no>R-2011-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider upper and lower bounds for maxmin allocations of a completely
divisible good in both competitive and cooperative strategic contexts. We then
derive a subgradient algorithm to compute the exact value up to any fixed
degree of precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4245</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4245</id><created>2011-10-19</created><authors><author><keyname>Xiang</keyname><forenames>Luojie</forenames></author><author><keyname>Kurkoski</keyname><forenames>Brian</forenames></author></authors><title>An Improved Analytical Expression for Write Amplification in NAND Flash</title><categories>cs.PF</categories><comments>5 pages, 5 figures, accepted by ICNC 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Agarwal et al. gave an closed-form expression for write amplification in NAND
flash memory by finding the probability of a page being valid over the whole
flash memory. This paper gives an improved analytic expression for write
amplification in NAND flash memory by finding the probability of a page being
invalid over the block selected for garbage collection. The improved expression
uses Lambert W function. Through asymptotic analysis, write amplification is
shown to depend on overprovisioning factor only, consistent with the previous
work. Comparison with numerical simulations shows that the improved expression
achieves a more accurate prediction of write amplification. For example, when
the overprovisioning factor is 0.3, the expression proposed by this paper gives
a write amplification of 2.36 whereas that of the previous work gives 2.17,
when the actual value is 2.35.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4248</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4248</id><created>2011-10-19</created><authors><author><keyname>Xiang</keyname><forenames>Luojie</forenames></author></authors><title>Ideogram Based Chinese Sentiment Word Orientation Computation</title><categories>cs.CL</categories><comments>4 pages, 3 figures, accepted by CET 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a novel algorithm to compute sentiment orientation of
Chinese sentiment word. The algorithm uses ideograms which are a distinguishing
feature of Chinese language. The proposed algorithm can be applied to any
sentiment classification scheme. To compute a word's sentiment orientation
using the proposed algorithm, only the word itself and a precomputed character
ontology is required, rather than a corpus. The influence of three parameters
over the algorithm performance is analyzed and verified by experiment.
Experiment also shows that proposed algorithm achieves an F Measure of 85.02%
outperforming existing ideogram based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4278</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4278</id><created>2011-10-19</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Paulo</forenames><affiliation>LIP</affiliation></author><author><keyname>Mishenin</keyname><forenames>Alexey</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Sokol</keyname><forenames>Marina</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Generalized Optimization Framework for Graph-based Semi-supervised
  Learning</title><categories>cs.NI</categories><proxy>ccsd</proxy><report-no>RR-7774</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a generalized optimization framework for graph-based
semi-supervised learning. The framework gives as particular cases the Standard
Laplacian, Normalized Laplacian and PageRank based methods. We have also
provided new probabilistic interpretation based on random walks and
characterized the limiting behaviour of the methods. The random walk based
interpretation allows us to explain di erences between the performances of
methods with di erent smoothing kernels. It appears that the PageRank based
method is robust with respect to the choice of the regularization parameter and
the labelled data. We illustrate our theoretical results with two realistic
datasets, characterizing di erent challenges: Les Miserables characters social
network and Wikipedia hyper-link graph. The graph-based semi-supervised
learning classi- es the Wikipedia articles with very good precision and perfect
recall employing only the information about the hyper-text links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4285</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4285</id><created>2011-10-18</created><authors><author><keyname>Peel</keyname><forenames>Leto</forenames></author></authors><title>Topological Feature Based Classification</title><categories>cs.SI physics.soc-ph</categories><comments>Awarded 3rd Best Student Paper at 14th International Conference on
  Information Fusion 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lot of interest in developing algorithms to extract clusters
or communities from networks. This work proposes a method, based on
blockmodelling, for leveraging communities and other topological features for
use in a predictive classification task. Motivated by the issues faced by the
field of community detection and inspired by recent advances in Bayesian topic
modelling, the presented model automatically discovers topological features
relevant to a given classification task. In this way, rather than attempting to
identify some universal best set of clusters for an undefined goal, the aim is
to find the best set of clusters for a particular purpose.
  Using this method, topological features can be validated and assessed within
a given context by their predictive performance.
  The proposed model differs from other relational and semi-supervised learning
models as it identifies topological features to explain the classification
decision. In a demonstration on a number of real networks the predictive
capability of the topological features are shown to rival the performance of
content based relational learners. Additionally, the model is shown to
outperform graph-based semi-supervised methods on directed and approximately
bipartite networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4296</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4296</id><created>2011-10-19</created><authors><author><keyname>Paul</keyname><forenames>Satyakama</forenames></author><author><keyname>Twala</keyname><forenames>Bhekisipho</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author></authors><title>Organizational adaptation to Complexity: A study of the South African
  Insurance Market as a Complex Adaptive System through Statistical Risk
  Analysis</title><categories>cs.CY</categories><comments>Paper Presented The 2nd International Conference on Complexity
  Science Management &amp; Intelligent Information System will be held in October
  14, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  South Africa assumes a significant position in the insurance landscape of
Africa. The present research based upon qualitative and quantitative analysis,
shows that it shows the characteristics of a Complex Adaptive System. In
addition, a statistical analysis of risk measures through Value at risk and
Conditional tail expectation is carried out to show how an individual insurance
company copes under external complexities. The authors believe that an
explanation of the coping strategies, and the subsequent managerial
implications would enrich our understanding of complexity in business.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4301</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4301</id><created>2011-10-19</created><updated>2011-10-20</updated><authors><author><keyname>Das</keyname><forenames>Bireswar</forenames></author><author><keyname>Pal</keyname><forenames>Manjish</forenames></author><author><keyname>Visavaliya</keyname><forenames>Vijay</forenames></author></authors><title>The Entropy Influence Conjecture Revisited</title><categories>math.CO cs.CC cs.DM</categories><comments>We thank Kunal Dutta and Justin Salez for pointing out that our
  result can be extended to a high probability statement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that most of the boolean functions, $f : \{-1,1\}^n
\rightarrow \{-1,1\}$ satisfy the Fourier Entropy Influence (FEI) Conjecture
due to Friedgut and Kalai (Proc. AMS'96). The conjecture says that the Entropy
of a boolean function is at most a constant times the Influence of the
function. The conjecture has been proven for families of functions of smaller
sizes. O'donnell, Wright and Zhou (ICALP'11) verified the conjecture for the
family of symmetric functions, whose size is $2^{n+1}$. They are in fact able
to prove the conjecture for the family of $d$-part symmetric functions for
constant $d$, the size of whose is $2^{O(n^d)}$. Also it is known that the
conjecture is true for a large fraction of polynomial sized DNFs (COLT'10).
Using elementary methods we prove that a random function with high probability
satisfies the conjecture with the constant as $(2 + \delta)$, for any constant
$\delta &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4319</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4319</id><created>2011-10-19</created><updated>2011-10-20</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames><affiliation>Seffi</affiliation></author><author><keyname>Feige</keyname><forenames>Uriel</forenames><affiliation>Seffi</affiliation></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames><affiliation>Seffi</affiliation></author><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames><affiliation>Seffi</affiliation></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames><affiliation>Seffi</affiliation></author><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author><author><keyname>Schwartz</keyname><forenames>Roy</forenames></author></authors><title>Min-Max Graph Partitioning and Small Set Expansion</title><categories>cs.DS</categories><comments>Full version of paper appearing in FOCS 2011, 29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study graph partitioning problems from a min-max perspective, in which an
input graph on n vertices should be partitioned into k parts, and the objective
is to minimize the maximum number of edges leaving a single part. The two main
versions we consider are where the k parts need to be of equal-size, and where
they must separate a set of k given terminals. We consider a common
generalization of these two problems, and design for it an $O(\sqrt{\log n\log
k})$-approximation algorithm. This improves over an $O(\log^2 n)$ approximation
for the second version, and roughly $O(k\log n)$ approximation for the first
version that follows from other previous work. We also give an improved
O(1)-approximation algorithm for graphs that exclude any fixed minor.
  Our algorithm uses a new procedure for solving the Small-Set Expansion
problem. In this problem, we are given a graph G and the goal is to find a
non-empty set $S\subseteq V$ of size $|S| \leq \rho n$ with minimum
edge-expansion. We give an $O(\sqrt{\log{n}\log{(1/\rho)}})$ bicriteria
approximation algorithm for the general case of Small-Set Expansion, and O(1)
approximation algorithm for graphs that exclude any fixed minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4322</identifier>
 <datestamp>2012-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4322</id><created>2011-10-19</created><updated>2012-02-14</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Kakade</keyname><forenames>Sham</forenames></author></authors><title>An Optimal Algorithm for Linear Bandits</title><categories>cs.LG stat.ML</categories><comments>This paper is superseded by S. Bubeck, N. Cesa-Bianchi, and S.M.
  Kakade, &quot;Towards minimax policies for online linear optimization with bandit
  feedback&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first algorithm for online bandit linear optimization whose
regret after T rounds is of order sqrt{Td ln N} on any finite class X of N
actions in d dimensions, and of order d*sqrt{T} (up to log factors) when X is
infinite. These bounds are not improvable in general. The basic idea utilizes
tools from convex geometry to construct what is essentially an optimal
exploration basis. We also present an application to a model of linear bandits
with expert advice. Interestingly, these results show that bandit linear
optimization with expert advice in d dimensions is no more difficult (in terms
of the achievable regret) than the online d-armed bandit problem with expert
advice (where EXP4 is optimal).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4350</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4350</id><created>2011-10-19</created><authors><author><keyname>Doliskani</keyname><forenames>Javad</forenames></author><author><keyname>Schost</keyname><forenames>Eric</forenames></author></authors><title>Taking Roots over High Extensions of Finite Fields</title><categories>cs.DS</categories><msc-class>11Y16, 12Y05 (Primary) 68W30 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing $m$-th roots over the finite field
$\F_q$, where $q = p^n$, with $p$ a prime, and $m$ any positive integer. In the
particular case $m=2$, the cost of the new algorithm is an expected
$O(\M(n)\log (p) + \CC(n)\log(n))$ operations in $\F_p$, where $\M(n)$ and
$\CC(n)$ are bounds for the cost of polynomial multiplication and modular
polynomial composition. Known results give $\M(n) = O(n\log (n) \log\log (n))$
and $\CC(n) = O(n^{1.67})$, so our algorithm is subquadratic in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4367</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4367</id><created>2011-10-18</created><updated>2012-02-10</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Peper</keyname><forenames>Ferdinand</forenames></author></authors><title>Information Networks Secured by the Laws of Physics</title><categories>cs.CR quant-ph</categories><comments>In press. Invited survey paper for the special issue &quot;Frontiers of
  Information Network Science&quot; of IEICE Transactions on the Fundamentals of
  Communications, Electronics, Information &amp; Systems (Japan)</comments><journal-ref>IEICE Transactions on the Fundamentals of Communications,
  Electronics, Information &amp; Systems, Vol. E95-B, No.05 (May 2012) pp.1501-1507</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we survey the state of the art of the secure key exchange
method that is secured by the laws of classical statistical physics, and
involves the Kirchhoff's law and the generalized Johnson noise equation, too.
We discuss the major characteristics and advantages of these schemes especially
in comparison with quantum encryption, and analyze some of the technical
challenges of its implementation, too. Finally, we outline some ideas about how
to use already existing and currently used wire lines, such as power lines,
phone lines, internet lines to implement unconditionally secure information
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4412</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4412</id><created>2011-10-19</created><authors><author><keyname>Chasparis</keyname><forenames>Georgios C.</forenames></author><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>Aspiration Learning in Coordination Games</title><categories>cs.GT cs.LG</categories><comments>27 pages</comments><msc-class>68T05, 91A26, 91A22, 93E35, 60J05, 91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed convergence to efficient outcomes in
coordination games through dynamics based on aspiration learning. Under
aspiration learning, a player continues to play an action as long as the
rewards received exceed a specified aspiration level. Here, the aspiration
level is a fading memory average of past rewards, and these levels also are
subject to occasional random perturbations. A player becomes dissatisfied
whenever a received reward is less than the aspiration level, in which case the
player experiments with a probability proportional to the degree of
dissatisfaction. Our first contribution is the characterization of the
asymptotic behavior of the induced Markov chain of the iterated process in
terms of an equivalent finite-state Markov chain. We then characterize
explicitly the behavior of the proposed aspiration learning in a generalized
version of coordination games, examples of which include network formation and
common-pool games. In particular, we show that in generic coordination games
the frequency at which an efficient action profile is played can be made
arbitrarily large. Although convergence to efficient outcomes is desirable, in
several coordination games, such as common-pool games, attainability of fair
outcomes, i.e., sequences of plays at which players experience highly rewarding
returns with the same frequency, might also be of special interest. To this
end, we demonstrate through analysis and simulations that aspiration learning
also establishes fair outcomes in all symmetric coordination games, including
common-pool games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4414</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4414</id><created>2011-10-19</created><updated>2011-12-26</updated><authors><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>(1+eps)-approximate Sparse Recovery</title><categories>cs.DS cs.IT math.IT</categories><comments>21 pages; appeared at FOCS 2011</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The problem central to sparse recovery and compressive sensing is that of
stable sparse recovery: we want a distribution of matrices A in R^{m\times n}
such that, for any x \in R^n and with probability at least 2/3 over A, there is
an algorithm to recover x* from Ax with
  ||x* - x||_p &lt;= C min_{k-sparse x'} ||x - x'||_p for some constant C &gt; 1 and
norm p. The measurement complexity of this problem is well understood for
constant C &gt; 1. However, in a variety of applications it is important to obtain
C = 1 + eps for a small eps &gt; 0, and this complexity is not well understood. We
resolve the dependence on eps in the number of measurements required of a
k-sparse recovery algorithm, up to polylogarithmic factors for the central
cases of p = 1 and p = 2. Namely, we give new algorithms and lower bounds that
show the number of measurements required is (1/eps^{p/2})k polylog(n). For p =
2, our bound of (1/eps) k log(n/k) is tight up to constant factors. We also
give matching bounds when the output is required to be k-sparse, in which case
we achieve (1/eps^p) k polylog(n). This shows the distinction between the
complexity of sparse and non-sparse outputs is fundamental.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4416</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4416</id><created>2011-10-19</created><authors><author><keyname>Lever</keyname><forenames>Guy</forenames></author><author><keyname>Diethe</keyname><forenames>Tom</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author></authors><title>Data-dependent kernels in nearly-linear time</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to efficiently construct data-dependent kernels which can
make use of large quantities of (unlabeled) data. Our construction makes an
approximation in the standard construction of semi-supervised kernels in
Sindhwani et al. 2005. In typical cases these kernels can be computed in
nearly-linear time (in the amount of data), improving on the cubic time of the
standard construction, enabling large scale semi-supervised learning in a
variety of contexts. The methods are validated on semi-supervised and
unsupervised problems on data sets containing upto 64,000 sample points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4428</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4428</id><created>2011-10-19</created><updated>2014-01-21</updated><authors><author><keyname>Iacono</keyname><forenames>John</forenames></author></authors><title>Improved Upper Bounds for Pairing Heaps</title><categories>cs.DS</categories><comments>Preliminary version appeared at the Seventh Scandinavian Workshop on
  Algorithm Theory (SWAT 2000)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairing heaps are shown to have constant amortized time Insert and Meld, thus
showing that pairing heaps have the same amortized runtimes as Fibonacci heaps
for all operations but Decrease-key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4437</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4437</id><created>2011-10-19</created><updated>2014-01-27</updated><authors><author><keyname>Avron</keyname><forenames>Haim</forenames></author><author><keyname>Toledo</keyname><forenames>Sivan</forenames></author></authors><title>Effective Stiffness: Generalizing Effective Resistance Sampling to
  Finite Element Matrices</title><categories>cs.NA cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the notion of effective stiffness and show that it can used to
build sparsifiers, algorithms that sparsify linear systems arising from
finite-element discretizations of PDEs. In particular, we show that sampling
$O(n\log n)$ elements according to probabilities derived from effective
stiffnesses yields a high quality preconditioner that can be used to solve the
linear system in a small number of iterations. Effective stiffness generalizes
the notion of effective resistance, a key ingredient of recent progress in
developing nearly linear symmetric diagonally dominant (SDD) linear solvers.
Solving finite elements problems is of considerably more interest than the
solution of SDD linear systems, since the finite element method is frequently
used to numerically solve PDEs arising in scientific and engineering
applications. Unlike SDD systems, which are relatively easy to solve, there has
been limited success in designing fast solvers for finite element systems, and
previous algorithms usually target discretization of limited class of PDEs like
scalar elliptic or 2D trusses. Our sparsifier is general; it applies to a wide
range of finite-element discretizations. A sparsifier does not constitute a
complete linear solver. To construct a solver, one needs additional components
(e.g., an efficient elimination or multilevel scheme for the sparsified
system). Still, sparsifiers have been a critical tools in efficient SDD
solvers, and we believe that our sparsifier will become a key ingredient in
future fast finite-element solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4441</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4441</id><created>2011-10-20</created><authors><author><keyname>Kanoria</keyname><forenames>Yashodhan</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author></authors><title>Distributed Storage for Intermittent Energy Sources: Control Design and
  Performance Limits</title><categories>cs.SY</categories><comments>25 pages, 4 eps figures. Proceedings of the Forty-Ninth Annual
  Allerton Conference on Communication, Control, and Computing (somewhat
  expanded)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important challenges in the integration of renewable energy
sources into the power grid lies in their `intermittent' nature. The power
output of sources like wind and solar varies with time and location due to
factors that cannot be controlled by the provider. Two strategies have been
proposed to hedge against this variability: 1) use energy storage systems to
effectively average the produced power over time; 2) exploit distributed
generation to effectively average production over location. We introduce a
network model to study the optimal use of storage and transmission resources in
the presence of random energy sources. We propose a Linear-Quadratic based
methodology to design control strategies, and we show that these strategies are
asymptotically optimal for some simple network topologies. For these
topologies, the dependence of optimal performance on storage and transmission
capacity is explicitly quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4473</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4473</id><created>2011-10-20</created><authors><author><keyname>Hardin</keyname><forenames>David</forenames><affiliation>Rockwell Collins</affiliation></author><author><keyname>Schmaltz</keyname><forenames>Julien</forenames><affiliation>Open University of the Netherlands</affiliation></author></authors><title>Proceedings 10th International Workshop on the ACL2 Theorem Prover and
  its Applications</title><categories>cs.LO cs.MS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 70, 2011</journal-ref><doi>10.4204/EPTCS.70</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of ACL2 2011, the International Workshop
on the ACL2 Theorem Prover and its Applications. The workshop was held in
Austin, Texas, USA, on November 3-4 2011. ACL2 2011 is the tenth in a series of
workshops on the ACL2 Theorem Prover and its Applications. The workshop was
co-located with the eleventh Conference on Formal Methods in Computer Aided
Design (FMCAD'11). The ACL2 Workshop series provide a major technical forum for
researchers to present and discuss improvements and extensions to the theorem
prover, comparisons of ACL2 with other systems, and applications of ACL2 in
formal verification or formalized mathematics. Workshops have been held at
approxiamately 18 month intervals since 1999. ACL2 is the most recent
incarnation of the Boyer-Moore family of theorem provers, for which, Robert
Boyer, Matt Kaufmann and J Strother Moore received the 2005 ACM Software System
Award. It is state-of-the-art automated reasoning system that has been
successfully used in academia, government and industry for specification and
verification of computing systems. More details can be found in the proceedings
and on the workshop web page (www.cs.ru.nl/~julien/acl2-11/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4474</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4474</id><created>2011-10-20</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Rosa</keyname><forenames>Marco</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Robustness of Social Networks: Comparative Results Based on Distance
  Distributions</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a social network, which of its nodes have a stronger impact in
determining its structure? More formally: which node-removal order has the
greatest impact on the network structure? We approach this well-known problem
for the first time in a setting that combines both web graphs and social
networks, using datasets that are orders of magnitude larger than those
appearing in the previous literature, thanks to some recently developed
algorithms and software tools that make it possible to approximate accurately
the number of reachable pairs and the distribution of distances in a graph. Our
experiments highlight deep differences in the structure of social networks and
web graphs, show significant limitations of previous experimental results, and
at the same time reveal clustering by label propagation as a new and very
effective way of locating nodes that are important from a structural viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4477</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4477</id><created>2011-10-20</created><authors><author><keyname>Song</keyname><forenames>Won-Min</forenames></author><author><keyname>Di Matteo</keyname><forenames>T.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>Hierarchical information clustering by means of topologically embedded
  graphs</title><categories>physics.data-an cs.DS physics.bio-ph q-bio.QM q-fin.CP</categories><comments>33 Pages, 18 Figures, 5 Tables</comments><journal-ref>PLoS ONE 7 (2012) e31929</journal-ref><doi>10.1371/journal.pone.0031929</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graph-theoretic approach to extract clusters and hierarchies
in complex data-sets in an unsupervised and deterministic manner, without the
use of any prior information. This is achieved by building topologically
embedded networks containing the subset of most significant links and analyzing
the network structure. For a planar embedding, this method provides both the
intra-cluster hierarchy, which describes the way clusters are composed, and the
inter-cluster hierarchy which describes how clusters gather together. We
discuss performance, robustness and reliability of this method by first
investigating several artificial data-sets, finding that it can outperform
significantly other established approaches. Then we show that our method can
successfully differentiate meaningful clusters and hierarchies in a variety of
real data-sets. In particular, we find that the application to gene expression
patterns of lymphoma samples uncovers biologically significant groups of genes
which play key-roles in diagnosis, prognosis and treatment of some of the most
relevant human lymphoid malignancies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4481</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4481</id><created>2011-10-20</created><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Learning Hierarchical and Topographic Dictionaries with Structured
  Sparsity</title><categories>cs.LG</categories><proxy>ccsd</proxy><journal-ref>SPIE Wavelets and Sparsity XIV 81381P (2011)</journal-ref><doi>10.1117/12.893811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in signal processing and statistics have focused on defining new
regularization functions, which not only induce sparsity of the solution, but
also take into account the structure of the problem. We present in this paper a
class of convex penalties introduced in the machine learning community, which
take the form of a sum of l_2 and l_infinity-norms over groups of variables.
They extend the classical group-sparsity regularization in the sense that the
groups possibly overlap, allowing more flexibility in the group design. We
review efficient optimization methods to deal with the corresponding inverse
problems, and their application to the problem of learning dictionaries of
natural image patches: On the one hand, dictionary learning has indeed proven
effective for various signal processing tasks. On the other hand, structured
sparsity provides a natural framework for modeling dependencies between
dictionary elements. We thus consider a structured sparse regularization to
learn dictionaries embedded in a particular structure, for instance a tree or a
two-dimensional grid. In the latter case, the results we obtain are similar to
the dictionaries produced by topographic independent component analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4493</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4493</id><created>2011-10-20</created><authors><author><keyname>Claude</keyname><forenames>Francisco</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Improved Grammar-Based Compressed Indexes</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce the first grammar-compressed representation of a sequence that
supports searches in time that depends only logarithmically on the size of the
grammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar
of $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of
the lengths of the right hands of the rules), a basic grammar-based
representation of $T$ takes $N\lg n$ bits of space. Our representation requires
$2N\lg n + N\lg u + \epsilon\, n\lg n + o(N\lg n)$ bits of space, for any
$0&lt;\epsilon \le 1$. It can find the positions of the $occ$ occurrences of a
pattern of length $m$ in $T$ in $O((m^2/\epsilon)\lg (\frac{\lg u}{\lg n})
+occ\lg n)$ time, and extract any substring of length $\ell$ of $T$ in time
$O(\ell+h\lg(N/h))$, where $h$ is the height of the grammar tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4499</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4499</id><created>2011-10-20</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author><author><keyname>Trott</keyname><forenames>Lowell</forenames></author></authors><title>Category-Based Routing in Social Networks: Membership Dimension and the
  Small-World Phenomenon (Full)</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classic experiment by Milgram shows that individuals can route messages
along short paths in social networks, given only simple categorical information
about recipients (such as &quot;he is a prominent lawyer in Boston&quot; or &quot;she is a
Freshman sociology major at Harvard&quot;). That is, these networks have very short
paths between pairs of nodes (the so-called small-world phenomenon); moreover,
participants are able to route messages along these paths even though each
person is only aware of a small part of the network topology. Some sociologists
conjecture that participants in such scenarios use a greedy routing strategy in
which they forward messages to acquaintances that have more categories in
common with the recipient than they do, and similar strategies have recently
been proposed for routing messages in dynamic ad-hoc networks of mobile
devices. In this paper, we introduce a network property called membership
dimension, which characterizes the cognitive load required to maintain
relationships between participants and categories in a social network. We show
that any connected network has a system of categories that will support greedy
routing, but that these categories can be made to have small membership
dimension if and only if the underlying network exhibits the small-world
phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4500</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4500</id><created>2011-10-20</created><updated>2011-10-21</updated><authors><author><keyname>Kzaz</keyname><forenames>Larbi</forenames></author><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author></authors><title>Semantic conflict resolution for integration of business components</title><categories>cs.SE</categories><comments>4'emes Journees Francophones sur les Ontologies ; ACM SIGAPP JFO
  Journees Francophones sur les Ontologies http://www.jf-ontologies.net 22 - 23
  Juin 2011, Montreal, Canada</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Reusing and integrating Business Components in a new Information System
requires detection and resolution of semantic conflicts. Moreover, most of
integration and semantic conflict resolution systems rely on ontology alignment
methods based on domain ontology. This work is positioned at the intersection
of two research areas: Integration of reusable B C and alignment of ontologies
for semantic conflict resolution. Our contribution concerns both the proposal
of a BC integration solution based on ontologies alignment and a method for
enriching the domain ontology used as a support for alignment
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4501</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4501</id><created>2011-10-20</created><authors><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author><author><keyname>Kzaz</keyname><forenames>Larbi</forenames></author></authors><title>An Ontology-Based Method for Semantic Integration of Business Components</title><categories>cs.SE</categories><comments>IEEE New Technologies of Distributed Systems (NOTERE), 2011 11th
  Annual International Conference; ISSN: 2162-1896 Print ISBN:
  978-1-4577-0729-2 INSPEC Accession Number: 12122775 2011</comments><doi>10.1109/NOTERE.2011.5957993</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Building new business information systems from reusable components is today
an approach widely adopted and used. Using this approach in analysis and design
phases presents a great interest and requires the use of a particular class of
components called Business Components (BC). Business Components are today
developed by several manufacturers and are available in many repositories.
However, reusing and integrating them in a new Information System requires
detection and resolution of semantic conflicts. Moreover, most of integration
and semantic conflict resolution systems rely on ontology alignment methods
based on domain ontology. This work is positioned at the intersection of two
research areas: Integration of reusable Business Components and alignment of
ontologies for semantic conflict resolution. Our contribution concerns both the
proposal of a BC integration solution based on ontologies alignment and a
method for enriching the domain ontology used as a support for alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4535</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4535</id><created>2011-10-20</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Huang</keyname><forenames>Huang</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author></authors><title>A Survey on Delay-Aware Resource Control for Wireless Systems --- Large
  Deviation Theory, Stochastic Lyapunov Drift and Distributed Stochastic
  Learning</title><categories>cs.PF</categories><comments>58 pages, 8 figures; IEEE Transactions on Information Theory, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial paper, a comprehensive survey is given on several major
systematic approaches in dealing with delay-aware control problems, namely the
equivalent rate constraint approach, the Lyapunov stability drift approach and
the approximate Markov Decision Process (MDP) approach using stochastic
learning. These approaches essentially embrace most of the existing literature
regarding delay-aware resource control in wireless systems. They have their
relative pros and cons in terms of performance, complexity and implementation
issues. For each of the approaches, the problem setup, the general solution and
the design methodology are discussed. Applications of these approaches to
delay-aware resource allocation are illustrated with examples in single-hop
wireless networks. Furthermore, recent results regarding delay-aware multi-hop
routing designs in general multi-hop networks are elaborated. Finally, the
delay performance of the various approaches are compared through simulations
using an example of the uplink OFDMA systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4544</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4544</id><created>2011-10-20</created><authors><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI, Amsterdam, The Netherlands</affiliation></author></authors><title>Compression-based Similarity</title><categories>cs.IT math.IT</categories><comments>Latex, 8 pages, 2 fgures, in Proc. IEEE 1st Int. Conf. Data
  Compression, Communication and Processing, Palurno, Italy, June 21-24, 2011,
  111--118</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First we consider pair-wise distances for literal objects consisting of
finite binary files. These files are taken to contain all of their meaning,
like genomes or books. The distances are based on compression of the objects
concerned, normalized, and can be viewed as similarity distances. Second, we
consider pair-wise distances between names of objects, like &quot;red&quot; or
&quot;christianity.&quot; In this case the distances are based on searches of the
Internet. Such a search can be performed by any search engine that returns
aggregate page counts. We can extract a code length from the numbers returned,
use the same formula as before, and derive a similarity or relative semantics
between names for objects. The theory is based on Kolmogorov complexity. We
test both similarities extensively experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4573</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4573</id><created>2011-10-20</created><updated>2011-11-02</updated><authors><author><keyname>Lazarus</keyname><forenames>Francis</forenames></author><author><keyname>Rivaud</keyname><forenames>Julien</forenames></author></authors><title>On the homotopy test on surfaces</title><categories>cs.CG cs.DM cs.DS</categories><comments>33 pages, 11 figures</comments><msc-class>05C10, 57M07, 68R99</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a graph cellularly embedded in a surface S. Given two closed walks c
and d in G, we take advantage of the RAM model to describe linear time
algorithms to decide if c and d are homotopic in S, either freely or with fixed
basepoint. We restrict S to be orientable for the free homotopy test, but allow
non-orientable surfaces when the basepoint is fixed. After O(|G|) time
preprocessing independent of c and d, our algorithms answer the homotopy test
in O(|c|+|d|) time, where |G|, |c| and |d| are the respective numbers of edges
of G, c and d. As a byproduct we obtain linear time algorithms for the word
problem and the conjugacy problem in surface groups. We present a geometric
approach based on previous works by Colin de Verdi\`ere and Erickson.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4604</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4604</id><created>2011-10-20</created><updated>2011-11-01</updated><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Shmoys</keyname><forenames>David B.</forenames></author></authors><title>Improving Christofides' Algorithm for the s-t Path TSP</title><categories>cs.DS</categories><comments>31 pages, 5 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic (1+sqrt(5))/2-approximation algorithm for the s-t
path TSP for an arbitrary metric. Given a symmetric metric cost on n vertices
including two prespecified endpoints, the problem is to find a shortest
Hamiltonian path between the two endpoints; Hoogeveen showed that the natural
variant of Christofides' algorithm is a 5/3-approximation algorithm for this
problem, and this asymptotically tight bound in fact has been the best
approximation ratio known until now. We modify this algorithm so that it
chooses the initial spanning tree based on an optimal solution to the Held-Karp
relaxation rather than a minimum spanning tree; we prove this simple but
crucial modification leads to an improved approximation ratio, surpassing the
20-year-old barrier set by the natural Christofides' algorithm variant. Our
algorithm also proves an upper bound of (1+sqrt(5))/2 on the integrality gap of
the path-variant Held-Karp relaxation. The techniques devised in this paper can
be applied to other optimization problems as well: these applications include
improved approximation algorithms and improved LP integrality gap upper bounds
for the prize-collecting s-t path problem and the unit-weight graphical metric
s-t path TSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4613</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4613</id><created>2011-10-20</created><authors><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Wiretap Channels: Implications of the More Capable Condition and Cyclic
  Shift Symmetry</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, October 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterization of the rate-equivocation region of a general wiretap channel
involves two auxiliary random variables: U, for rate splitting and V, for
channel prefixing. Evaluation of regions involving auxiliary random variables
is generally difficult. In this paper, we explore specific classes of wiretap
channels for which the expression and evaluation of the rate-equivocation
region are simpler. In particular, we show that when the main channel is more
capable than the eavesdropping channel, V=X is optimal and the boundary of the
rate-equivocation region can be achieved by varying U alone. Conversely, we
show under a mild condition that if the main receiver is not more capable, then
V=X is strictly suboptimal. Next, we focus on the class of cyclic shift
symmetric wiretap channels. We explicitly determine the optimal selections of
rate splitting U and channel prefixing V that achieve the boundary of the
rate-equivocation region. We show that optimal U and V are determined via
cyclic shifts of the solution of an auxiliary optimization problem that
involves only one auxiliary random variable. In addition, we provide a
sufficient condition for cyclic shift symmetric wiretap channels to have U=\phi
as an optimal selection. Finally, we apply our results to the binary-input
cyclic shift symmetric wiretap channels. We solve the corresponding constrained
optimization problem by inspecting each point of the I(X;Y)-I(X;Z) function. We
thoroughly characterize the rate-equivocation regions of the BSC-BEC and
BEC-BSC wiretap channels. In particular, we find that U=\phi is optimal and the
boundary of the rate-equivocation region is achieved by varying V alone for the
BSC-BEC wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4623</identifier>
 <datestamp>2011-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4623</id><created>2011-10-20</created><authors><author><keyname>Stuart</keyname><forenames>Jeff A.</forenames></author><author><keyname>Owens</keyname><forenames>John D.</forenames></author></authors><title>Efficient Synchronization Primitives for GPUs</title><categories>cs.OS cs.DC cs.DS cs.GR</categories><comments>13 pages with appendix, several figures, plans to submit to CompSci
  conference in early 2012</comments><acm-class>D.4.1; I.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the design of synchronization
primitives---specifically barriers, mutexes, and semaphores---and how they
apply to the GPU. Previous implementations are insufficient due to the
discrepancies in hardware and programming model of the GPU and CPU. We create
new implementations in CUDA and analyze the performance of spinning on the GPU,
as well as a method of sleeping on the GPU, by running a set of memory-system
benchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class
GPUs from NVIDIA. From our results we define higher-level principles that are
valid for generic many-core processors, the most important of which is to limit
the number of atomic accesses required for a synchronization operation because
atomic accesses are slower than regular memory accesses. We use the results of
the benchmarks to critique existing synchronization algorithms and guide our
new implementations, and then define an abstraction of GPUs to classify any GPU
based on the behavior of the memory system. We use this abstraction to create
suitable implementations of the primitives specifically targeting the GPU, and
analyze the performance of these algorithms on Tesla and Fermi. We then predict
performance on future GPUs based on characteristics of the abstraction. We also
examine the roles of spin waiting and sleep waiting in each primitive and how
their performance varies based on the machine abstraction, then give a set of
guidelines for when each strategy is useful based on the characteristics of the
GPU and expected contention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4624</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4624</id><created>2011-10-20</created><authors><author><keyname>Heath</keyname><forenames>Tom</forenames></author></authors><title>Aladdin: Augmenting Urban Environments with Local Area Linked
  Data-Casting</title><categories>cs.SI cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban environments are brimming with information sources, yet these are
typically disconnected from related information on the Web. Addressing this
disconnect requires an infrastructure able to disseminate information to a
specific micro-location, to be consumed by interested parties. This paper
proposes Aladdin, an infrastructure for highly localised broadcast of Linked
Data via radio waves. When combined with data retrieved from the Web, Aladdin
can enable a new generation of micro-location-aware mobile applications and
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4657</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4657</id><created>2011-10-20</created><authors><author><keyname>Mitavskiy</keyname><forenames>Boris</forenames></author><author><keyname>Rowe</keyname><forenames>Jonathan</forenames></author><author><keyname>Cannings</keyname><forenames>Chris</forenames></author></authors><title>A Version of Geiringer-like Theorem for Decision Making in the
  Environments with Randomness and Incomplete Information</title><categories>cs.AI cs.DM</categories><comments>53 pages in size. This work has been recently submitted to the IJICC
  (International Journal on Intelligent Computing and Cybernetics)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: In recent years Monte-Carlo sampling methods, such as Monte Carlo
tree search, have achieved tremendous success in model free reinforcement
learning. A combination of the so called upper confidence bounds policy to
preserve the &quot;exploration vs. exploitation&quot; balance to select actions for
sample evaluations together with massive computing power to store and to update
dynamically a rather large pre-evaluated game tree lead to the development of
software that has beaten the top human player in the game of Go on a 9 by 9
board. Much effort in the current research is devoted to widening the range of
applicability of the Monte-Carlo sampling methodology to partially observable
Markov decision processes with non-immediate payoffs. The main challenge
introduced by randomness and incomplete information is to deal with the action
evaluation at the chance nodes due to drastic differences in the possible
payoffs the same action could lead to. The aim of this article is to establish
a version of a theorem that originated from population genetics and has been
later adopted in evolutionary computation theory that will lead to novel
Monte-Carlo sampling algorithms that provably increase the AI potential. Due to
space limitations the actual algorithms themselves will be presented in the
sequel papers, however, the current paper provides a solid mathematical
foundation for the development of such algorithms and explains why they are so
promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4671</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4671</id><created>2011-10-20</created><authors><author><keyname>Cowles</keyname><forenames>John R.</forenames><affiliation>University of Wyoming</affiliation></author><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Verifying Sierpi\'nski and Riesel Numbers in ACL2</title><categories>cs.DM cs.LO</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 70, 2011, pp. 20-27</journal-ref><doi>10.4204/EPTCS.70.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Sierpinski number is an odd positive integer, k, such that no positive
integer of the form k * 2^n + 1 is prime. Similar to a Sierpinski number, a
Riesel number is an odd positive integer, k, such that no positive integer of
the form k * 2^n + 1 is prime. A cover for such a k is a finite list of
positive integers such that each integer j of the appropriate form has a
factor, d, in the cover, with 1 &lt; d &lt; j. Given a k and its cover, ACL2 is used
to systematically verify that each integer of the given form has a non-trivial
factor in the cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4672</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4672</id><created>2011-10-20</created><authors><author><keyname>Dahlin</keyname><forenames>Mike</forenames><affiliation>Department of Computer Science, University of Texas at Austin</affiliation></author><author><keyname>Johnson</keyname><forenames>Ryan</forenames><affiliation>Department of Computer Science, University of Texas at Austin</affiliation></author><author><keyname>Krug</keyname><forenames>Robert Bellarmine</forenames><affiliation>Department of Computer Science, University of Texas at Austin</affiliation></author><author><keyname>McCoyd</keyname><forenames>Michael</forenames><affiliation>Department of Computer Science, University of Texas at Austin</affiliation></author><author><keyname>Young</keyname><forenames>William</forenames><affiliation>Department of Computer Science, University of Texas at Austin</affiliation></author></authors><title>Toward the Verification of a Simple Hypervisor</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 70, 2011, pp. 28-45</journal-ref><doi>10.4204/EPTCS.70.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization promises significant benefits in security, efficiency,
dependability, and cost. Achieving these benefits depends upon the reliability
of the underlying virtual machine monitors (hypervisors). This paper describes
an ongoing project to develop and verify MinVisor, a simple but functional
Type-I x86 hypervisor, proving protection properties at the assembly level
using ACL2. Originally based on an existing research hypervisor, MinVisor
provides protection of its own memory from a malicious guest. Our long-term
goal is to fully verify MinVisor, providing a vehicle to investigate the
modeling and verification of hypervisors at the implementation level, and also
a basis for further systems research. Functional segments of the MinVisor C
code base are translated into Y86 assembly, and verified with respect to the
Y86 model. The inductive assertions (also known as &quot;compositional cutpoints&quot;)
methodology is used to prove the correctness of the code. The proof of the code
that sets up the nested page tables is described. We compare this project to
related efforts in systems code verification and outline some useful steps
forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4673</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4673</id><created>2011-10-20</created><authors><author><keyname>Kaufmann</keyname><forenames>Matt</forenames><affiliation>Univ. of Texas at Austin</affiliation></author><author><keyname>Moore</keyname><forenames>J Strother</forenames><affiliation>Univ. of Texas at Austin</affiliation></author></authors><title>How Can I Do That with ACL2? Recent Enhancements to ACL2</title><categories>cs.MS cs.LO cs.SC</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><acm-class>I.2.3; F.4.1</acm-class><journal-ref>EPTCS 70, 2011, pp. 46-60</journal-ref><doi>10.4204/EPTCS.70.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last several years have seen major enhancements to ACL2 functionality,
largely driven by requests from its user community, including utilities now in
common use such as 'make-event', 'mbe', and trust tags. In this paper we
provide user-level summaries of some ACL2 enhancements introduced after the
release of Version 3.5 (in May, 2009, at about the time of the 2009 ACL2
workshop) up through the release of Version 4.3 in July, 2011, roughly a couple
of years later. Many of these features are not particularly well known yet, but
most ACL2 users could take advantage of at least some of them. Some of the
changes could affect existing proof efforts, such as a change that treats pairs
of functions such as 'member' and 'member-equal' as the same function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4674</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4674</id><created>2011-10-20</created><authors><author><keyname>Reid</keyname><forenames>Peter</forenames><affiliation>University of Oklahoma</affiliation></author><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Implementing an Automatic Differentiator in ACL2</title><categories>cs.SC cs.LO</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 70, 2011, pp. 61-69</journal-ref><doi>10.4204/EPTCS.70.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The foundational theory of differentiation was developed as part of the
original release of ACL2(r). In work reported at the last ACL2 Workshop, we
presented theorems justifying the usual differentiation rules, including the
chain rule and the derivative of inverse functions. However, the process of
applying these theorems to formalize the derivative of a particular function is
completely manual. More recently, we developed a macro and supporting functions
that can automate this process. This macro uses the ACL2 table facility to keep
track of functions and their derivatives, and it also interacts with the macro
that introduces inverse functions in ACL2(r), so that their derivatives can
also be automated. In this paper, we present the implementation of this macro
and related functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4675</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4675</id><created>2011-10-20</created><authors><author><keyname>Seidel</keyname><forenames>Peter-Michael</forenames><affiliation>Advanced Micro Devices</affiliation></author></authors><title>Formal Verification of an Iterative Low-Power x86 Floating-Point
  Multiplier with Redundant Feedback</title><categories>cs.LO cs.AR cs.MS</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><acm-class>F.3.1</acm-class><journal-ref>EPTCS 70, 2011, pp. 70-83</journal-ref><doi>10.4204/EPTCS.70.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the formal verification of a low-power x86 floating-point
multiplier. The multiplier operates iteratively and feeds back intermediate
results in redundant representation. It supports x87 and SSE instructions in
various precisions and can block the issuing of new instructions. The design
has been optimized for low-power operation and has not been constrained by the
formal verification effort. Additional improvements for the implementation were
identified through formal verification. The formal verification of the design
also incorporates the implementation of clock-gating and control logic. The
core of the verification effort was based on ACL2 theorem proving.
Additionally, model checking has been used to verify some properties of the
floating-point scheduler that are relevant for the correct operation of the
unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4676</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4676</id><created>2011-10-20</created><authors><author><keyname>Swords</keyname><forenames>Sol</forenames><affiliation>Centaur Technology</affiliation></author><author><keyname>Davis</keyname><forenames>Jared</forenames><affiliation>Centaur Technology</affiliation></author></authors><title>Bit-Blasting ACL2 Theorems</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 70, 2011, pp. 84-102</journal-ref><doi>10.4204/EPTCS.70.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive theorem proving requires a lot of human guidance. Proving a
property involves (1) figuring out why it holds, then (2) coaxing the theorem
prover into believing it. Both steps can take a long time. We explain how to
use GL, a framework for proving finite ACL2 theorems with BDD- or SAT-based
reasoning. This approach makes it unnecessary to deeply understand why a
property is true, and automates the process of admitting it as a theorem. We
use GL at Centaur Technology to verify execution units for x86 integer, MMX,
SSE, and floating-point arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4677</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4677</id><created>2011-10-20</created><authors><author><keyname>Verbeek</keyname><forenames>Freek</forenames><affiliation>Radboud University</affiliation></author><author><keyname>Schmaltz</keyname><forenames>Julien</forenames><affiliation>Open University of The Netherlands</affiliation></author></authors><title>Formal verification of a deadlock detection algorithm</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2011, arXiv:1110.4473</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 70, 2011, pp. 103-112</journal-ref><doi>10.4204/EPTCS.70.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deadlock detection is a challenging issue in the analysis and design of
on-chip networks. We have designed an algorithm to detect deadlocks
automatically in on-chip networks with wormhole switching. The algorithm has
been specified and proven correct in ACL2. To enable a top-down proof
methodology, some parts of the algorithm have been left unimplemented. For
these parts, the ACL2 specification contains constrained functions introduced
with defun-sk. We used single-threaded objects to represent the data structures
used by the algorithm. In this paper, we present details on the proof of
correctness of the algorithm. The process of formal verification was crucial to
get the algorithm flawless. Our ultimate objective is to have an efficient
executable, and formally proven correct implementation of the algorithm running
in ACL2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4697</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4697</id><created>2011-10-20</created><updated>2014-09-03</updated><authors><author><keyname>Shah</keyname><forenames>D.</forenames></author><author><keyname>Walton</keyname><forenames>N. S.</forenames></author><author><keyname>Zhong</keyname><forenames>Y.</forenames></author></authors><title>Optimal queue-size scaling in switched networks</title><categories>math.PR cs.NI</categories><comments>Published in at http://dx.doi.org/10.1214/13-AAP970 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP970</report-no><journal-ref>Annals of Applied Probability 2014, Vol. 24, No. 6, 2207-2245</journal-ref><doi>10.1214/13-AAP970</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a switched (queuing) network in which there are constraints on
which queues may be served simultaneously; such networks have been used to
effectively model input-queued switches and wireless networks. The scheduling
policy for such a network specifies which queues to serve at any point in time,
based on the current state or past history of the system. In the main result of
this paper, we provide a new class of online scheduling policies that achieve
optimal queue-size scaling for a class of switched networks including
input-queued switches. In particular, it establishes the validity of a
conjecture (documented in Shah, Tsitsiklis and Zhong [Queueing Syst. 68 (2011)
375-384]) about optimal queue-size scaling for input-queued switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4703</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4703</id><created>2011-10-21</created><authors><author><keyname>Tadrous</keyname><forenames>John</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>Proactive Resource Allocation: Harnessing the Diversity and Multicast
  Gains</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the novel concept of proactive resource allocation
through which the predictability of user behavior is exploited to balance the
wireless traffic over time, and hence, significantly reduce the bandwidth
required to achieve a given blocking/outage probability. We start with a simple
model in which the smart wireless devices are assumed to predict the arrival of
new requests and submit them to the network T time slots in advance. Using
tools from large deviation theory, we quantify the resulting prediction
diversity gain} to establish that the decay rate of the outage event
probabilities increases with the prediction duration T. This model is then
generalized to incorporate the effect of the randomness in the prediction
look-ahead time T. Remarkably, we also show that, in the cognitive networking
scenario, the appropriate use of proactive resource allocation by the primary
users improves the diversity gain of the secondary network at no cost in the
primary network diversity. We also shed lights on multicasting with predictable
demands and show that the proactive multicast networks can achieve a
significantly higher diversity gain that scales super-linearly with T. Finally,
we conclude by a discussion of the new research questions posed under the
umbrella of the proposed proactive (non-causal) wireless networking framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4713</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4713</id><created>2011-10-21</created><authors><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author><author><keyname>Stern</keyname><forenames>David</forenames></author><author><keyname>Herbrich</keyname><forenames>Ralf</forenames></author><author><keyname>Graepel</keyname><forenames>Thore</forenames></author></authors><title>Kernel Topic Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet Allocation models discrete data as a mixture of discrete
distributions, using Dirichlet beliefs over the mixture weights. We study a
variation of this concept, in which the documents' mixture weight beliefs are
replaced with squashed Gaussian distributions. This allows documents to be
associated with elements of a Hilbert space, admitting kernel topic models
(KTM), modelling temporal, spatial, hierarchical, social and other structure
between documents. The main challenge is efficient approximate inference on the
latent Gaussian. We present an approximate algorithm cast around a Laplace
approximation in a transformed basis. The KTM can also be interpreted as a type
of Gaussian process latent variable model, or as a topic model conditional on
document features, uncovering links between earlier work in these areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4719</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4719</id><created>2011-10-21</created><authors><author><keyname>Petit</keyname><forenames>Thierry</forenames></author><author><keyname>Beldiceanu</keyname><forenames>Nicolas</forenames></author><author><keyname>Lorca</keyname><forenames>Xavier</forenames></author></authors><title>A Generalized Arc-Consistency Algorithm for a Class of Counting
  Constraints: Revised Edition that Incorporates One Correction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the SEQ BIN meta-constraint with a polytime algorithm
achieving general- ized arc-consistency according to some properties. SEQ BIN
can be used for encoding counting con- straints such as CHANGE, SMOOTH or
INCREAS- ING NVALUE. For some of these constraints and some of their variants
GAC can be enforced with a time and space complexity linear in the sum of
domain sizes, which improves or equals the best known results of the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4723</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4723</id><created>2011-10-21</created><authors><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Song</keyname><forenames>Guojie</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Jiang</keyname><forenames>Qingye</forenames></author></authors><title>Influence Blocking Maximization in Social Networks under the Competitive
  Linear Threshold Model Technical Report</title><categories>cs.SI physics.soc-ph</categories><comments>Full version technical report of Paper &quot;Influence Blocking
  Maximization in Social Networks under the Competitive Linear Threshold Model&quot;
  which has been submitted to SDM2012. 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world situations, different and often opposite opinions,
innovations, or products are competing with one another for their social
influence in a networked society. In this paper, we study competitive influence
propagation in social networks under the competitive linear threshold (CLT)
model, an extension to the classic linear threshold model. Under the CLT model,
we focus on the problem that one entity tries to block the influence
propagation of its competing entity as much as possible by strategically
selecting a number of seed nodes that could initiate its own influence
propagation. We call this problem the influence blocking maximization (IBM)
problem. We prove that the objective function of IBM in the CLT model is
submodular, and thus a greedy algorithm could achieve 1-1/e approximation
ratio. However, the greedy algorithm requires Monte-Carlo simulations of
competitive influence propagation, which makes the algorithm not efficient. We
design an efficient algorithm CLDAG, which utilizes the properties of the CLT
model, to address this issue. We conduct extensive simulations of CLDAG, the
greedy algorithm, and other baseline algorithms on real-world and synthetic
datasets. Our results show that CLDAG is able to provide best accuracy in par
with the greedy algorithm and often better than other algorithms, while it is
two orders of magnitude faster than the greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4746</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4746</id><created>2011-10-21</created><updated>2014-04-22</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Abhijit</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>QoS Aware and Survivable Network Design for Planned Wireless Sensor
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of wireless sensor network design by deploying a minimum
number of additional relay nodes (to minimize network cost) at a subset of
given potential relay locations, in order to convey the data from already
existing sensor nodes (hereafter called source nodes) to a Base Station, while
meeting a certain specified hop count bound (the hop count bound is chosen to
ensure a pre-determined probability of the data being delivered to the BS
within a given maximum delay). We study two variations of the problem.
  First we sudy the problem of guaranteed QoS connected network design, where
the objective is to have at least one path from each source to the BS with the
specified hop count bound. We show that the problem is NP-Hard. For a problem
in which the number of existing sensor nodes and potential relay locations is
n, we propose an O(n) approximation algorithm of polynomial time complexity.
Results show that the algorithm performs efficiently (in over 90% of the tested
scenarios, it gave solutions that were either optimal or were worse than
optimal by just one relay) in various randomly generated network scenarios.
  Next, we study the problem of survivable network design with guaranteed QoS,
i.e, the requirement is to have at least k &gt; 1 node disjoint hop constrained
paths from each source to the BS. We show that the problem is NP-Hard. We also
show that the problem of finding a feasible solution to this optimization
problem is NP-Complete. We propose two polynomial time heuristics for this
problem, and compare their performance on various randomly generated network
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4765</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4765</id><created>2011-10-21</created><authors><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>Finding small separators in linear time via treewidth reduction</title><categories>cs.DS cs.DM</categories><comments>A subset of the results was presented at STACS 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for reducing the treewidth of a graph while preserving
all of its minimal $s-t$ separators up to a certain fixed size $k$. This
technique allows us to solve $s-t$ Cut and Multicut problems with various
additional restrictions (e.g., the vertices being removed from the graph form
an independent set or induce a connected graph) in linear time for every fixed
number $k$ of removed vertices.
  Our results have applications for problems that are not directly defined by
separators, but the known solution methods depend on some variant of
separation. for example, we can solve similarly restricted generalizations of
Bipartization (delete at most $k$ vertices from $G$ to make it bipartite) in
almost linear time for every fixed number $k$ of removed vertices. These
results answer a number of open questions in the area of parameterized
complexity. Furthermore, our technique turns out to be relevant for $(H,C,K)$-
and $(H,C,\le K)$-coloring problems as well, which are cardinality constrained
variants of the classical $H$-coloring problem. We make progress in the
classification of the parameterized complexity of these problems by identifying
new cases that can be solved in almost linear time for every fixed cardinality
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4784</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4784</id><created>2011-10-21</created><updated>2012-06-04</updated><authors><author><keyname>Bordino</keyname><forenames>Ilaria</forenames></author><author><keyname>Battiston</keyname><forenames>Stefano</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Cristelli</keyname><forenames>Matthieu</forenames></author><author><keyname>Ukkonen</keyname><forenames>Antti</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Web search queries can predict stock market volumes</title><categories>q-fin.ST cs.LG physics.soc-ph</categories><comments>29 pages, 11 figures, 11 tables + Supporting Information</comments><doi>10.1371/journal.pone.0040014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in a computerized and networked society where many of our actions
leave a digital trace and affect other people's actions. This has lead to the
emergence of a new data-driven research field: mathematical methods of computer
science, statistical physics and sociometry provide insights on a wide range of
disciplines ranging from social science to human mobility. A recent important
discovery is that query volumes (i.e., the number of requests submitted by
users to search engines on the www) can be used to track and, in some cases, to
anticipate the dynamics of social phenomena. Successful exemples include
unemployment levels, car and home sales, and epidemics spreading. Few recent
works applied this approach to stock prices and market sentiment. However, it
remains unclear if trends in financial markets can be anticipated by the
collective wisdom of on-line users on the web. Here we show that trading
volumes of stocks traded in NASDAQ-100 are correlated with the volumes of
queries related to the same stocks. In particular, query volumes anticipate in
many cases peaks of trading by one day or more. Our analysis is carried out on
a unique dataset of queries, submitted to an important web search engine, which
enable us to investigate also the user behavior. We show that the query volume
dynamics emerges from the collective but seemingly uncoordinated activity of
many users. These findings contribute to the debate on the identification of
early warnings of financial systemic risk, based on the activity of users of
the www.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4801</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4801</id><created>2011-10-19</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Fan</keyname><forenames>Xiao</forenames></author></authors><title>Improvement Of Barreto-Voloch Algorithm For Computing $r$th Roots Over
  Finite Fields</title><categories>cs.SC cs.CR math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Root extraction is a classical problem in computers algebra. It plays an
essential role in cryptosystems based on elliptic curves. In 2006, Barreto and
Voloch proposed an algorithm to compute $r$th roots in ${F}_{q^m} $ for certain
choices of $m$ and $q$. If $r\,||\,q-1$ and $ (m, r)=1, $ they proved that the
complexity of their method is $\widetilde{\mathcal {O}}(r(\log m+\log\log
q)m\log q) $. In this paper, we extend the Barreto-Voloch algorithm to the
general case that $r\,||\,q^m-1$, without the restrictions $r\,||\,q-1$ and
$(m, r)=1 $. We also specify the conditions that the Barreto-Voloch algorithm
can be preferably applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4802</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4802</id><created>2011-09-27</created><authors><author><keyname>de Sevricourt</keyname><forenames>O. Cugnon</forenames></author><author><keyname>Tariel</keyname><forenames>V.</forenames></author></authors><title>Cameleon language Part 1: Processor</title><categories>cs.PL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergence is the way complex systems arise out of a multiplicity of
relatively simple interactions between primitives. Since programming problems
become more and more complexes and transverses, our vision is that application
development should be process at two scales: micro- and macro-programming where
at the micro-level the paradigm is step-by-step and at macro-level the paradigm
is emergence. For micro-programming, which focuses on how things happen,
popular languages, Java, C++, Python, are imperative writing languages where
the code is a sequence of sentences executed by the computer. For
macro-programming, which focuses on how things connect, popular languages,
labVIEW, Blender, Simulink, are graphical data flow languages such that the
program is a composition of operators, a unit-process consuming input data and
producing output data, and connectors, a data-flow between an output data and
an input data of two operators. However, despite their fruitful applications,
these macro-languages are not transversal since different data-structures of
native data-structures cannot be integrated in their framework easily. Cameleon
language is a graphical data flow language following a two-scale paradigm. It
allows an easy up-scale that is the integration of any library writing in C++
in the data flow language. Cameleon language aims to democratize
macro-programming by an intuitive interaction between the human and the
computer where building an application based on a data-process and a GUI is a
simple task to learn and to do. Cameleon language allows conditional execution
and repetition to solve complex macro-problems. In this paper we introduce a
new model based on the extension of the petri net model for the description of
how the Cameleon language executes a composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4820</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4820</id><created>2011-10-21</created><authors><author><keyname>Lavale</keyname><forenames>Monica</forenames></author></authors><title>Analysis of Differential Phase Shift Quantum Key Distribution</title><categories>cs.CR</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the implementation of two QKD protocols (BB84 and B92) keeping in
mind that their implementations do not easily satisfy the requirement of use of
single photons. We argue that current models do not take into account issues
raised by the Uncertainty Principle related to time-location and transmission
characteristics of single photons. This indicates that security proofs of
current implementations even after the fixes for the recent successful hacks
are made will be hard to obtain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4821</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4821</id><created>2011-10-21</created><updated>2013-12-16</updated><authors><author><keyname>Dembo</keyname><forenames>Amir</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Sun</keyname><forenames>Nike</forenames></author></authors><title>Factor models on locally tree-like graphs</title><categories>math.PR cs.DM</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOP828 the Annals of
  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOP-AOP828</report-no><journal-ref>Annals of Probability 2013, Vol. 41, No. 6, 4162-4213</journal-ref><doi>10.1214/12-AOP828</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider homogeneous factor models on uniformly sparse graph sequences
converging locally to a (unimodular) random tree $T$, and study the existence
of the free energy density $\phi$, the limit of the log-partition function
divided by the number of vertices $n$ as $n$ tends to infinity. We provide a
new interpolation scheme and use it to prove existence of, and to explicitly
compute, the quantity $\phi$ subject to uniqueness of a relevant Gibbs measure
for the factor model on $T$. By way of example we compute $\phi$ for the
independent set (or hard-core) model at low fugacity, for the ferromagnetic
Ising model at all parameter values, and for the ferromagnetic Potts model with
both weak enough and strong enough interactions. Even beyond uniqueness regimes
our interpolation provides useful explicit bounds on $\phi$. In the regimes in
which we establish existence of the limit, we show that it coincides with the
Bethe free energy functional evaluated at a suitable fixed point of the belief
propagation (Bethe) recursions on $T$. In the special case that $T$ has a
Galton-Watson law, this formula coincides with the nonrigorous &quot;Bethe
prediction&quot; obtained by statistical physicists using the &quot;replica&quot; or &quot;cavity&quot;
methods. Thus our work is a rigorous generalization of these heuristic
calculations to the broader class of sparse graph sequences converging locally
to trees. We also provide a variational characterization for the Bethe
prediction in this general setting, which is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4838</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4838</id><created>2011-10-21</created><authors><author><keyname>Klein</keyname><forenames>Kyle</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author></authors><title>Capturing an Evader in Polygonal Environments: A Complete Information
  Game</title><categories>cs.GT</categories><comments>17 pages, 12 figures</comments><acm-class>I.2.11; I.2.9</acm-class><journal-ref>K. Klein and S. Suri. Complete information pursuit evasion in
  polygonal environments. In 25th Conference on Artificial Intelligence (AAAI),
  pages 1120--1125, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose an unpredictable evader is free to move around in a polygonal
environment of arbitrary complexity that is under full camera surveillance. How
many pursuers, each with the same maximum speed as the evader, are necessary
and sufficient to guarantee a successful capture of the evader? The pursuers
always know the evader's current position through the camera network, but need
to physically reach the evader to capture it. We allow the evader the knowledge
of the current positions of all the pursuers as well---this accords with the
standard worst-case analysis model, but also models a practical situation where
the evader has &quot;hacked&quot; into the surveillance system.
  Our main result is to prove that three pursuers are always sufficient and
sometimes necessary to capture the evader. The bound is independent of the
number of vertices or holes in the polygonal environment. The result should be
contrasted with the incomplete information pursuit-evasion where at least
{\Omega}(\surd h + log n) pursuers are required just for detecting the evader
in an environment with n vertices and h holes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4844</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4844</id><created>2011-10-21</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Kim</keyname><forenames>Jihie</forenames></author></authors><title>Analyzing Answers in Threaded Discussions using a Role-Based Information
  Network</title><categories>cs.SI cs.IR</categories><comments>The Third IEEE International Conference on Social Computing
  (SocialCom2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online discussion boards are an important medium for collaboration. The goal
of our work is to understand how messages and individual discussants contribute
to Q&amp;A discussions. We present a novel network model for capturing in-formation
roles of messages and discussants, and show how we identify useful answers to
the initial question. We first classify information seeking or information
providing roles of messages, such as question, answer or acknowledgement. We
also identify user intent in the discussion as an information seeker or a
provider. We capture such role information within a reply-to discussion
network, and identify messages that answer seeker questions and how answeres
are acknowledged. Message influences are analyzed using B-centrality measures.
User influences across different threads are combined with message influences.
We use the combined score in identifying the most useful answer in the thread.
The resulting ranks correlate with human provided ranks with an MRR score of
0.67.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4851</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4851</id><created>2011-10-21</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Leveraging User Diversity to Harvest Knowledge on the Social Web</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>The Third IEEE International Conference on Social Computing
  (SocialCom2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social web users are a very diverse group with varying interests, levels of
expertise, enthusiasm, and expressiveness. As a result, the quality of content
and annotations they create to organize content is also highly variable. While
several approaches have been proposed to mine social annotations, for example,
to learn folksonomies that reflect how people relate narrower concepts to
broader ones, these methods treat all users and the annotations they create
uniformly. We propose a framework to automatically identify experts, i.e.,
knowledgeable users who create high quality annotations, and use their
knowledge to guide folksonomy learning. We evaluate the approach on a large
body of social annotations extracted from the photosharing site Flickr. We show
that using expert knowledge leads to more detailed and accurate folksonomies.
Moreover, we show that including annotations from non-expert, or novice, users
leads to more comprehensive folksonomies than experts' knowledge alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4854</identifier>
 <datestamp>2011-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4854</id><created>2011-10-21</created><authors><author><keyname>Chondros</keyname><forenames>Nikos</forenames></author><author><keyname>Kokordelis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>On the Practicality of `Practical' Byzantine Fault Tolerance</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Byzantine Fault Tolerant (BFT) systems are considered by the systems research
community to be state of the art with regards to providing reliability in
distributed systems. BFT systems provide safety and liveness guarantees with
reasonable assumptions, amongst a set of nodes where at most f nodes display
arbitrarily incorrect behaviors, known as Byzantine faults. Despite this, BFT
systems are still rarely used in practice. In this paper we describe our
experience, from an application developer's perspective, trying to leverage the
publicly available and highly-tuned PBFT middleware (by Castro and Liskov), to
provide provable reliability guarantees for an electronic voting application
with high security and robustness needs. We describe several obstacles we
encountered and drawbacks we identified in the PBFT approach. These include
some that we tackled, such as lack of support for dynamic client management and
leaving state management completely up to the application. Others still
remaining include the lack of robust handling of non-determinism, lack of
support for web-based applications, lack of support for stronger cryptographic
primitives, and others. We find that, while many of the obstacles could be
overcome with a revised BFT middleware implementation that is tuned
specifically for the needs of the particular application, they require
significant engineering effort and time and their performance implications for
the end-application are unclear. An application developer is thus unlikely to
be willing to invest the time and effort to do so to leverage the BFT approach.
We conclude that the research community needs to focus on the usability of BFT
algorithms for real world applications, from the end-developer perspective, in
addition to continuing to improve the BFT middleware performance, robustness
and deployment layouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4860</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4860</id><created>2011-10-21</created><updated>2013-01-29</updated><authors><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Symmetry and approximability of submodular maximization problems</title><categories>cs.DS</categories><comments>The conference version of this paper appeared in IEEE FOCS 2009.
  Unfortunately there was an error in the main theorem of the FOCS 2009 paper.
  This long version corrects the error (see Theorem 3) and explains why the
  error does not affect the applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of recent results on optimization problems involving submodular
functions have made use of the multilinear relaxation of the problem. These
results hold typically in the value oracle model, where the objective function
is accessible via a black box returning f(S) for a given S. We present a
general approach to deriving inapproximability results in the value oracle
model, based on the notion of symmetry gap. Our main result is that for any
fixed instance that exhibits a certain symmetry gap in its multilinear
relaxation, there is a naturally related class of instances for which a better
approximation factor than the symmetry gap would require exponentially many
oracle queries. This unifies several known hardness results for submodular
maximization, and implies several new ones. In particular, we prove that there
is no constant-factor approximation for the problem of maximizing a
non-negative submodular function over the bases of a matroid. We also provide a
closely matching approximation algorithm for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4882</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4882</id><created>2011-10-21</created><updated>2013-04-12</updated><authors><author><keyname>Vegh</keyname><forenames>Laszlo A.</forenames></author></authors><title>Strongly polynomial algorithm for a class of minimum-cost flow problems
  with separable convex objectives</title><categories>cs.DS cs.DM cs.GT</categories><comments>Major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-studied nonlinear extension of the minimum-cost flow problem is to
minimize the objective \sum_{ij\in E} C_{ij}(f_{ij}) over feasible flows f,
where on each arc ij of the network, C_{ij} is a convex function. We give a
strongly polynomial algorithm for finding an exact optimal solution for a broad
class of such problems. The most important characteristic of this class is that
an optimal solution can be computed exactly provided its support.
  The class includes convex quadratic objectives and also certain market
equilibria problems, such as Fisher's market with linear or with spending
constraint utilities. Thereby we give the first strongly polynomial algorithms
for separable quadratic minimum-cost flows and for Fisher's market with
spending constraint utilities, settling open questions posed e.g. in
[Hochbaum,94] and in [Vazirani,10], respectively. The running time is O(m^4 log
m) for quadratic costs, O(n^4 + n^2(m + n log n)log n) for Fisher's markets
with linear utilities and O(m n^3 +m^2(m + n log n) log m) for spending
constraint utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4925</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4925</id><created>2011-10-21</created><updated>2011-10-26</updated><authors><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>The Similarity between Stochastic Kronecker and Chung-Lu Graph Models</title><categories>cs.SI</categories><journal-ref>SDM12: Proceedings of the Twelfth SIAM International Conference on
  Data Mining, pp. 1071-1082, April 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of massive graphs is now becoming a very important part of
science and industrial research. This has led to the construction of a large
variety of graph models, each with their own advantages. The Stochastic
Kronecker Graph (SKG) model has been chosen by the Graph500 steering committee
to create supercomputer benchmarks for graph algorithms. The major reasons for
this are its easy parallelization and ability to mirror real data. Although SKG
is easy to implement, there is little understanding of the properties and
behavior of this model.
  We show that the parallel variant of the edge-configuration model given by
Chung and Lu (referred to as CL) is notably similar to the SKG model. The graph
properties of an SKG are extremely close to those of a CL graph generated with
the appropriate parameters. Indeed, the final probability matrix used by SKG is
almost identical to that of a CL model. This implies that the graph
distribution represented by SKG is almost the same as that given by a CL model.
We also show that when it comes to fitting real data, CL performs as well as
SKG based on empirical studies of graph properties. CL has the added benefit of
a trivially simple fitting procedure and exactly matching the degree
distribution. Our results suggest that users of the SKG model should consider
the CL model because of its similar properties, simpler structure, and ability
to fit a wider range of degree distributions. At the very least, CL is a good
control model to compare against.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4929</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4929</id><created>2011-10-21</created><updated>2011-12-16</updated><authors><author><keyname>Coon</keyname><forenames>Justin</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author></authors><title>Impact of boundaries on fully connected random geometric networks</title><categories>cond-mat.dis-nn cs.NI math-ph math.MP math.PR</categories><comments>6 pages, 3 figures</comments><journal-ref>Phys Rev E 85, 011138 (2012)</journal-ref><doi>10.1103/PhysRevE.85.011138</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex networks exhibit a percolation transition involving a
macroscopic connected component, with universal features largely independent of
the microscopic model and the macroscopic domain geometry. In contrast, we show
that the transition to full connectivity is strongly influenced by details of
the boundary, but observe an alternative form of universality. Our approach
correctly distinguishes connectivity properties of networks in domains with
equal bulk contributions. It also facilitates system design to promote or avoid
full connectivity for diverse geometries in arbitrary dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4970</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4970</id><created>2011-10-22</created><authors><author><keyname>Al-Wassai</keyname><forenames>Firouz Abdullah</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author><author><keyname>Al-Zaky</keyname><forenames>Ali A.</forenames></author></authors><title>Studying Satellite Image Quality Based on the Fusion Techniques</title><categories>cs.CV</categories><journal-ref>International Journal of Advanced Research in Computer
  Science,Volume 2, No. 5, Sept-Oct 2011,www.ijarcs.info</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various and different methods can be used to produce high-resolution
multispectral images from high-resolution panchromatic image (PAN) and
low-resolution multispectral images (MS), mostly on the pixel level. However,
the jury is still out on the benefits of a fused image compared to its original
images. There is also a lack of measures for assessing the objective quality of
the spatial resolution for the fusion methods. Therefore, an objective quality
of the spatial resolution assessment for fusion images is required. So, this
study attempts to develop a new qualitative assessment to evaluate the spatial
quality of the pan sharpened images by many spatial quality metrics. Also, this
paper deals with a comparison of various image fusion techniques based on pixel
and feature fusion techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4978</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4978</id><created>2011-10-22</created><updated>2015-12-30</updated><authors><author><keyname>Drabent</keyname><forenames>Wlodzimierz</forenames></author></authors><title>Logic + control: An example of program construction and verification</title><categories>cs.LO cs.PL</categories><comments>22 pages. Substantially reworked, in particular all informal
  reasoning replaced by proofs, part of the content moved to 1412.8739 and
  1411.3015</comments><acm-class>D.1.6, F.3.1, D.2.4, D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an example of formal reasoning about the semantics of a
Prolog program of practical importance (the SAT solver of Howe and King). The
program is treated as a logic program with added control. The logic program is
constructed by means of stepwise refinement, hand in hand with its correctness
and completeness proofs. The proofs are declarative -- they do not refer to any
operational semantics. We also prove that correctness and completeness of the
logic program is preserved in the final Prolog program. Our example shows how
dealing with &quot;logic&quot; and with &quot;control&quot; can be separated. Most of reasoning
about correctness and completeness can be done at the &quot;logic&quot; level,
abstracting from any operational semantics.
  The example employs approximate specifications, they are crucial in
simplifying reasoning about logic programs. It also shows that the paradigm of
semantics preserving program transformations may be not sufficient. We suggest
considering transformations which preserve correctness and completeness with
respect to an approximate specification.
  Keywords: logic programming, declarative programming, program completeness,
program correctness, specification, program transformation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4992</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4992</id><created>2011-10-22</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author></authors><title>Welfare and Profit Maximization with Production Costs</title><categories>cs.GT</categories><comments>This is the full version of the paper that is to appear at
  Foundations of Computer Science (FOCS) 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial Auctions are a central problem in Algorithmic Mechanism Design:
pricing and allocating goods to buyers with complex preferences in order to
maximize some desired objective (e.g., social welfare, revenue, or profit). The
problem has been well-studied in the case of limited supply (one copy of each
item), and in the case of digital goods (the seller can produce additional
copies at no cost). Yet in the case of resources---oil, labor, computing
cycles, etc.---neither of these abstractions is just right: additional supplies
of these resources can be found, but at increasing difficulty (marginal cost)
as resources are depleted.
  In this work, we initiate the study of the algorithmic mechanism design
problem of combinatorial pricing under increasing marginal cost. The goal is to
sell these goods to buyers with unknown and arbitrary combinatorial valuation
functions to maximize either the social welfare, or the seller's profit;
specifically we focus on the setting of \emph{posted item prices} with buyers
arriving online. We give algorithms that achieve {\em constant factor}
approximations for a class of natural cost functions---linear, low-degree
polynomial, logarithmic---and that give logarithmic approximations for more
general increasing marginal cost functions (along with a necessary additive
loss). We show that these bounds are essentially best possible for these
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.4999</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.4999</id><created>2011-10-22</created><authors><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Capacity of the Gaussian Relay Channel with Correlated Noises to Within
  a Constant Gap</title><categories>cs.IT math.IT</categories><comments>accepted to communications letters</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper studies the relaying strategies and the approximate capacity of
the classic three-node Gaussian relay channel, but where the noises at the
relay and at the destination are correlated. It is shown that the capacity of
such a relay channel can be achieved to within a constant gap of $\hf \log_2 3
=0.7925$ bits using a modified version of the noisy network coding strategy,
where the quantization level at the relay is set in a correlation dependent
way. As a corollary, this result establishes that the conventional
compress-and-forward scheme also achieves to within a constant gap to the
capacity. In contrast, the decode-and-forward and the single-tap
amplify-and-forward relaying strategies can have an infinite gap to capacity in
the regime where the noises at the relay and at the destination are highly
correlated, and the gain of the relay-to-destination link goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5000</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5000</id><created>2011-10-22</created><updated>2011-12-09</updated><authors><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>On Noisy Network Coding for a Gaussian Relay Chain Network with
  Correlated Noises</title><categories>cs.IT math.IT</categories><comments>Proc. of CWIT '11</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Noisy network coding, which elegantly combines the conventional
compress-and-forward relaying strategy and ideas from network coding, has
recently drawn much attention for its simplicity and optimality in achieving to
within constant gap of the capacity of the multisource multicast Gaussian
network. The constant-gap result, however, applies only to Gaussian relay
networks with independent noises. This paper investigates the application of
noisy network coding to networks with correlated noises. By focusing on a
four-node Gaussian relay chain network with a particular noise correlation
structure, it is shown that noisy network coding can no longer achieve to
within constant gap to capacity with the choice of Gaussian inputs and Gaussian
quantization. The cut-set bound of the relay chain network in this particular
case, however, can be achieved to within half a bit by a simple concatenation
of a correlation-aware noisy network coding strategy and a decode-and-forward
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5015</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5015</id><created>2011-10-23</created><authors><author><keyname>Bronstein</keyname><forenames>Alexander M.</forenames></author></authors><title>Spectral descriptors for deformable shapes</title><categories>cs.CV cs.CG cs.GR math.DG</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Informative and discriminative feature descriptors play a fundamental role in
deformable shape analysis. For example, they have been successfully employed in
correspondence, registration, and retrieval tasks. In the recent years,
significant attention has been devoted to descriptors obtained from the
spectral decomposition of the Laplace-Beltrami operator associated with the
shape. Notable examples in this family are the heat kernel signature (HKS) and
the wave kernel signature (WKS). Laplacian-based descriptors achieve
state-of-the-art performance in numerous shape analysis tasks; they are
computationally efficient, isometry-invariant by construction, and can
gracefully cope with a variety of transformations. In this paper, we formulate
a generic family of parametric spectral descriptors. We argue that in order to
be optimal for a specific task, the descriptor should take into account the
statistics of the corpus of shapes to which it is applied (the &quot;signal&quot;) and
those of the class of transformations to which it is made insensitive (the
&quot;noise&quot;). While such statistics are hard to model axiomatically, they can be
learned from examples. Following the spirit of the Wiener filter in signal
processing, we show a learning scheme for the construction of optimal spectral
descriptors and relate it to Mahalanobis metric learning. The superiority of
the proposed approach is demonstrated on the SHREC'10 benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5045</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5045</id><created>2011-10-23</created><authors><author><keyname>Levenshtein</keyname><forenames>Vladimir</forenames></author><author><keyname>Siemons</keyname><forenames>Johannes</forenames></author></authors><title>Error Graphs and the Reconstruction of Elements in Groups</title><categories>math.CO cs.IT math.GR math.IT</categories><comments>Journal of Combinatorial Theory A 2009</comments><msc-class>05C12, 20B05, 20B30</msc-class><doi>10.1016/j.jcta.2008.11.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packing and covering problems for metric spaces, and graphs in particular,
are of essential interest in combinatorics and coding theory. They are
formulated in terms of metric balls of vertices. We consider a new problem in
graph theory which is also based on the consideration of metric balls of
vertices, but which is distinct from the traditional packing and covering
problems. This problem is motivated by applications in information transmission
when redundancy of messages is not sufficient for their exact reconstruction,
and applications in computational biology when one wishes to restore an
evolutionary process. It can be defined as the reconstruction, or
identification, of an unknown vertex in a given graph from a minimal number of
vertices (erroneous or distorted patterns) in a metric ball of a given radius r
around the unknown vertex. For this problem it is required to find minimum
restrictions for such a reconstruction to be possible and also to find
efficient reconstruction algorithms under such minimal restrictions.
  In this paper we define error graphs and investigate their basic properties.
A particular class of error graphs occurs when the vertices of the graph are
the elements of a group, and when the path metric is determined by a suitable
set of group elements. These are the undirected Cayley graphs. Of particular
interest is the transposition Cayley graph on the symmetric group which occurs
in connection with the analysis of transpositional mutations in molecular
biology. We obtain a complete solution of the above problems for the
transposition Cayley graph on the symmetric group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5051</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5051</id><created>2011-10-23</created><authors><author><keyname>Zhang</keyname><forenames>Dell</forenames></author></authors><title>Wikipedia Edit Number Prediction based on Temporal Dynamics Only</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we describe our approach to the Wikipedia Participation
Challenge which aims to predict the number of edits a Wikipedia editor will
make in the next 5 months. The best submission from our team, &quot;zeditor&quot;,
achieved 41.7% improvement over WMF's baseline predictive model and the final
rank of 3rd place among 96 teams. An interesting characteristic of our approach
is that only temporal dynamics features (i.e., how the number of edits changes
in recent periods, etc.) are used in a self-supervised learning framework,
which makes it easy to be generalised to other application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5057</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5057</id><created>2011-10-23</created><authors><author><keyname>Mitrovi&#x107;</keyname><forenames>Marija</forenames></author><author><keyname>Tadi&#x107;</keyname><forenames>Bosiljka</forenames></author></authors><title>Patterns of Emotional Blogging and Emergence of Communities: Agent-Based
  Model on Bipartite Networks</title><categories>cs.SI cs.HC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: We study mechanisms underlying the collective emotional behavior
of Bloggers by using the agent-based modeling and the parameters inferred from
the related empirical data.
  Methodology/Principal Findings: A bipartite network of emotional agents and
posts evolves through the addition of agents and their actions on posts. The
emotion state of an agent,quantified by the arousal and the valence, fluctuates
in time due to events on the connected posts, and in the moments of agent's
action it is transferred to a selected post. We claim that the indirect
communication of the emotion in the model rules, combined with the action-delay
time and the circadian rhythm extracted from the empirical data, can explain
the genesis of emotional bursts by users on popular Blogs and similar Web
portals. The model also identifies the parameters and how they influence the
course of the dynamics.
  Conclusions: The collective behavior is here recognized by the emergence of
communities on the network and the fractal time-series of their emotional
comments, powered by the negative emotion (critique). The evolving agents
communities leave characteristic patterns of the activity in the phase space of
the arousal--valence variables, where each segment represents a common emotion
described in psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5063</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5063</id><created>2011-10-23</created><authors><author><keyname>Weinstein</keyname><forenames>Alejandro J.</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Recovering a Clipped Signal in Sparseland</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many data acquisition systems it is common to observe signals whose
amplitudes have been clipped. We present two new algorithms for recovering a
clipped signal by leveraging the model assumption that the underlying signal is
sparse in the frequency domain. Both algorithms employ ideas commonly used in
the field of Compressive Sensing; the first is a modified version of Reweighted
$\ell_1$ minimization, and the second is a modification of a simple greedy
algorithm known as Trivial Pursuit. An empirical investigation shows that both
approaches can recover signals with significant levels of clipping
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5068</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5068</id><created>2011-10-23</created><authors><author><keyname>Testa</keyname><forenames>Claudio</forenames></author><author><keyname>Rossi</keyname><forenames>Dario</forenames></author><author><keyname>Rao</keyname><forenames>Ashwin</forenames></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames></author></authors><title>Experimental Assessment of BitTorrent Completion Time in Heterogeneous
  TCP/uTP swarms</title><categories>cs.NI</categories><comments>14 pages, under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BitTorrent, one of the most widespread used P2P application for file-sharing,
recently got rid of TCP by introducing an application-level congestion control
protocol named uTP. The aim of this new protocol is to efficiently use the
available link capacity, while minimizing its interference with the rest of
user traffic (e.g., Web, VoIP and gaming) sharing the same access bottleneck.
In this paper we perform an experimental study of the impact of uTP on the
torrent completion time, the metric that better captures the user experience.
We run BitTorrent applications in a flash crowd scenario over a dedicated
cluster platform, under both homogeneous and heterogeneous swarm population.
Experiments show that an all-uTP swarms have shorter torrent download time with
respect to all-TCP swarms. Interestingly, at the same time, we observe that
even shorter completion times can be achieved under careful mixtures of TCP and
uTP traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5091</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5091</id><created>2011-10-23</created><updated>2011-10-25</updated><authors><author><keyname>Marks</keyname><forenames>Debora S.</forenames></author><author><keyname>Colwell</keyname><forenames>Lucy J.</forenames></author><author><keyname>Sheridan</keyname><forenames>Robert</forenames></author><author><keyname>Hopf</keyname><forenames>Thomas A.</forenames></author><author><keyname>Pagnani</keyname><forenames>Andrea</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author><author><keyname>Sander</keyname><forenames>Chris</forenames></author></authors><title>3D Protein Structure Predicted from Sequence</title><categories>q-bio.BM cs.CE physics.bio-ph physics.data-an</categories><comments>Debora S Marks and Lucy J Colwell are joint first authors. Supplement
  and Appendices at: http://cbio.mskcc.org/foldingproteins. Updated version
  25-Oct-2011 with '3D' added to the title and corrections of details in the
  methods section to make it compatible with derivation of equations in the
  main text and in the supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolutionary trajectory of a protein through sequence space is
constrained by function and three-dimensional (3D) structure. Residues in
spatial proximity tend to co-evolve, yet attempts to invert the evolutionary
record to identify these constraints and use them to computationally fold
proteins have so far been unsuccessful. Here, we show that co-variation of
residue pairs, observed in a large protein family, provides sufficient
information to determine 3D protein structure. Using a data-constrained maximum
entropy model of the multiple sequence alignment, we identify pairs of
statistically coupled residue positions which are expected to be close in the
protein fold, termed contacts inferred from evolutionary information (EICs). To
assess the amount of information about the protein fold contained in these
coupled pairs, we evaluate the accuracy of predicted 3D structures for proteins
of 50-260 residues, from 15 diverse protein families, including a G-protein
coupled receptor. These structure predictions are de novo, i.e., they do not
use homology modeling or sequence-similar fragments from known structures. The
resulting low C{\alpha}-RMSD error range of 2.7-5.1{\AA}, over at least 75% of
the protein, indicates the potential for predicting essentially correct 3D
structures for the thousands of protein families that have no known structure,
provided they include a sufficiently large number of divergent sample
sequences. With the current enormous growth in sequence information based on
new sequencing technology, this opens the door to a comprehensive survey of
protein 3D structures, including many not currently accessible to the
experimental methods of structural genomics. This advance has potential
applications in many biological contexts, such as synthetic biology,
identification of functional sites in proteins and interpretation of the
functional impact of genetic variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5092</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5092</id><created>2011-10-23</created><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Cartwright</keyname><forenames>Dustin</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Geometry of the 3-user MIMO interference channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 6 figures. Appeared at the Allerton Conference, September
  2011</comments><report-no>Mittag-Leffler-2011spring</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies vector space interference alignment for the three-user
MIMO interference channel with no time or frequency diversity. The main result
is a characterization of the feasibility of interference alignment in the
symmetric case where all transmitters have M antennas and all receivers have N
antennas. If N &gt;= M and all users desire d transmit dimensions, then alignment
is feasible if and only if (2r+1)d &lt;= max(rN,(r+1)M) for all nonnegative
integers r. The analogous result holds with M and N switched if M &gt;= N.
  It turns out that, just as for the 3-user parallel interference channel
\cite{BT09}, the length of alignment paths captures the essence of the problem.
In fact, for each feasible value of M and N the maximum alignment path length
dictates both the converse and achievability arguments.
  One of the implications of our feasibility criterion is that simply counting
equations and comparing to the number of variables does not predict
feasibility. Instead, a more careful investigation of the geometry of the
alignment problem is required. The necessary condition obtained by counting
equations is implied by our new feasibility criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5097</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5097</id><created>2011-10-23</created><updated>2012-07-20</updated><authors><author><keyname>Fannjiang</keyname><forenames>Albert</forenames></author></authors><title>Absolute Uniqueness of Phase Retrieval with Random Illumination</title><categories>physics.optics cs.CV math-ph math.MP</categories><comments>21 pages, 7 figures</comments><journal-ref>Inverse Problems 28 (2012) 075008</journal-ref><doi>10.1088/0266-5611/28/7/075008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random illumination is proposed to enforce absolute uniqueness and resolve
all types of ambiguity, trivial or nontrivial, from phase retrieval. Almost
sure irreducibility is proved for any complex-valued object of a full rank
support. While the new irreducibility result can be viewed as a probabilistic
version of the classical result by Bruck, Sodin and Hayes, it provides a novel
perspective and an effective method for phase retrieval.
  In particular, almost sure uniqueness, up to a global phase, is proved for
complex-valued objects under general two-point conditions. Under a tight sector
constraint absolute uniqueness is proved to hold with probability exponentially
close to unity as the object sparsity increases. Under a magnitude constraint
with random amplitude illumination, uniqueness modulo global phase is proved to
hold with probability exponentially close to unity as object sparsity
increases. For general complex-valued objects without any constraint, almost
sure uniqueness up to global phase is established with two sets of Fourier
magnitude data under two independent illuminations. Numerical experiments
suggest that random illumination essentially alleviates most, if not all,
numerical problems commonly associated with the standard phasing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5098</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5098</id><created>2011-10-23</created><updated>2012-12-10</updated><authors><author><keyname>Angrishi</keyname><forenames>Kishore</forenames></author></authors><title>An End-to-End Stochastic Network Calculus with Effective Bandwidth and
  Effective Capacity</title><categories>cs.NI</categories><comments>17 pages</comments><doi>10.1016/j.comnet.2012.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network calculus is an elegant theory which uses envelopes to determine the
worst-case performance bounds in a network. Statistical network calculus is the
probabilistic version of network calculus, which strives to retain the
simplicity of envelope approach from network calculus and use the arguments of
statistical multiplexing to determine probabilistic performance bounds in a
network. The tightness of the determined probabilistic bounds depends on the
efficiency of modelling stochastic properties of the arrival traffic and the
service available to the traffic at a network node. The notion of effective
bandwidth from large deviations theory is a well known statistical descriptor
of arrival traffic. Similarly, the notion of effective capacity summarizes the
time varying resource availability to the arrival traffic at a network node.
The main contribution of this paper is to establish an end-to-end stochastic
network calculus with the notions of effective bandwidth and effective capacity
which provides efficient end-to-end delay and backlog bounds that grows
linearly in the number of nodes ($H$) traversed by the arrival traffic, under
the assumption of independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5102</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5102</id><created>2011-10-23</created><authors><author><keyname>Li</keyname><forenames>Congcong</forenames></author><author><keyname>Kowdle</keyname><forenames>Adarsh</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author><author><keyname>Chen</keyname><forenames>Tsuhan</forenames></author></authors><title>Towards Holistic Scene Understanding: Feedback Enabled Cascaded
  Classification Models</title><categories>cs.CV cs.AI cs.RO</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene understanding includes many related sub-tasks, such as scene
categorization, depth estimation, object detection, etc. Each of these
sub-tasks is often notoriously hard, and state-of-the-art classifiers already
exist for many of them. These classifiers operate on the same raw image and
provide correlated outputs. It is desirable to have an algorithm that can
capture such correlation without requiring any changes to the inner workings of
any classifier.
  We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that
jointly optimizes all the sub-tasks, while requiring only a `black-box'
interface to the original classifier for each sub-task. We use a two-layer
cascade of classifiers, which are repeated instantiations of the original ones,
with the output of the first layer fed into the second layer as input. Our
training method involves a feedback step that allows later classifiers to
provide earlier classifiers information about which error modes to focus on. We
show that our method significantly improves performance in all the sub-tasks in
the domain of scene understanding, where we consider depth estimation, scene
categorization, event categorization, object detection, geometric labeling and
saliency detection. Our method also improves performance in two robotic
applications: an object-grasping robot and an object-finding robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5103</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5103</id><created>2011-10-23</created><authors><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author><author><keyname>Schurch</keyname><forenames>Mark</forenames></author></authors><title>Monomer-dimer tatami tilings of square regions</title><categories>math.CO cs.DM</categories><comments>Expanded conference proceedings: A. Erickson, M. Schurch, Enumerating
  tatami mat arrangements of square grids, in: 22nd International Workshop on
  Combinatorial Al- gorithms (IWOCA), volume 7056 of Lecture Notes in Computer
  Science (LNCS), Springer Berlin / Heidelberg, 2011, p. 12 pages. More on
  Tatami tilings at
  http://alejandroerickson.com/joomla/tatami-blog/collected-resources</comments><msc-class>05B45, 05B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the number of monomer-dimer tilings of an $n\times n$ square
grid, with $m&lt;n$ monomers in which no four tiles meet at any point is
$m2^m+(m+1)2^{m+1}$, when $m$ and $n$ have the same parity. In addition, we
present a new proof of the result that there are $n2^{n-1}$ such tilings with
$n$ monomers, which divides the tilings into $n$ classes of size $2^{n-1}$. The
sum of these tilings over all monomer counts has the closed form
$2^{n-1}(3n-4)+2$ and, curiously, this is equal to the sum of the squares of
all parts in all compositions of $n$. We also describe two algorithms and a
Gray code ordering for generating the $n2^{n-1}$ tilings with $n$ monomers,
which are both based on our new proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5111</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5111</id><created>2011-10-23</created><updated>2012-08-29</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author></authors><title>Optimal antithickenings of claw-free trigraphs</title><categories>cs.DM math.CO</categories><comments>19 pages, 2 figures. Revision: Fixed statement of Corollary 2 (only
  applies to quasi-line graphs) and updated references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chudnovsky and Seymour's structure theorem for claw-free graphs has led to a
multitude of recent results that exploit two structural operations: {\em
compositions of strips} and {\em thickenings}. In this paper we consider the
latter, proving that every claw-free graph has a unique optimal {\em
antithickening}, where our definition of {\em optimal} is chosen carefully to
respect the structural foundation of the graph. Furthermore, we give an
algorithm to find the optimal antithickening in $O(m^2)$ time. For the sake of
both completeness and ease of proof, we prove stronger results in the more
general setting of trigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5156</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5156</id><created>2011-10-24</created><authors><author><keyname>Wahab</keyname><forenames>Mohd Helmy Abd</forenames></author><author><keyname>Talib</keyname><forenames>Amirul A.</forenames></author><author><keyname>Kadir</keyname><forenames>Herdawatie A.</forenames></author><author><keyname>Johari</keyname><forenames>Ayob</forenames></author><author><keyname>Noraziah</keyname><forenames>A.</forenames></author><author><keyname>Sidek</keyname><forenames>Roslina M.</forenames></author><author><keyname>Mutalib</keyname><forenames>Ariffin A.</forenames></author></authors><title>Smart Cane: Assistive Cane for Visually-impaired People</title><categories>cs.SY</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper reports on a study that helps visually-impaired people to walk
more confidently. The study hypothesizes that a smart cane that alerts
visually-impaired people over obstacles in front could help them in walking
with less accident. The aim of the paper is to address the development work of
a cane that could communicate with the users through voice alert and vibration,
which is named Smart Cane. T he development work involves coding and physical
installation. A series of tests have been carried out on the smart cane and the
results are discussed. This study found that the Smart Cane functions well as
intended, in alerting users about the obstacles in front
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5172</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5172</id><created>2011-10-24</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>INRIA Lorraine - LORIA, LHyGeS</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Quels formalismes temporels pour repr\'esenter des connaissances
  extraites de textes de recettes de cuisine ?</title><categories>cs.AI</categories><comments>Repr\'esentation et raisonnement sur le temps et l'espace (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Taaable projet goal is to create a case-based reasoning system for
retrieval and adaptation of cooking recipes. Within this framework, we are
discussing the temporal aspects of recipes and the means of representing those
in order to adapt their text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5173</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5173</id><created>2011-10-24</created><authors><author><keyname>Bazghandi</keyname><forenames>Ali</forenames></author><author><keyname>MehdiBazghandi</keyname></author></authors><title>Ad Hoc Protocols Via Multi Agent Based Tools</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is investigating behaviors of Ad Hoc protocols in
Agent-based simulation environments. First we bring brief introduction about
agents and Ad Hoc networks. We introduce some agent-based simulation tools like
NS-2. Then we focus on two protocols, which are Ad Hoc On-demand Multipath
Distance Vector (AODV) and Destination Sequenced Distance Vector (DSDV). At the
end, we bring simulation results and discuss about their reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5176</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5176</id><created>2011-10-24</created><updated>2012-10-10</updated><authors><author><keyname>Fyhn</keyname><forenames>Karsten</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author><author><keyname>Jensen</keyname><forenames>S&#xf8;ren Holdt</forenames></author></authors><title>Demodulating Subsampled Direct Sequence Spread Spectrum Signals using
  Compressive Signal Processing</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 2 figures, presented at EUSIPCO 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that to lower the sampling rate in a spread spectrum communication
system using Direct Sequence Spread Spectrum (DSSS), compressive signal
processing can be applied to demodulate the received signal. This may lead to a
decrease in the power consumption or the manufacturing price of wireless
receivers using spread spectrum technology. The main novelty of this paper is
the discovery that in spread spectrum systems it is possible to apply
compressive sensing with a much simpler hardware architecture than in other
systems, making the implementation both simpler and more energy efficient. Our
theoretical work is exemplified with a numerical experiment using the IEEE
802.15.4 standard's 2.4 GHz band specification. The numerical results support
our theoretical findings and indicate that compressive sensing may be used
successfully in spread spectrum communication systems. The results obtained
here may also be applicable in other spread spectrum technologies, such as Code
Division Multiple Access (CDMA) systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5181</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5181</id><created>2011-10-24</created><authors><author><keyname>Bergner</keyname><forenames>Steven</forenames></author><author><keyname>Sedlmair</keyname><forenames>Michael</forenames></author><author><keyname>Nabi</keyname><forenames>Sareh</forenames></author><author><keyname>Saad</keyname><forenames>Ahmed</forenames></author><author><keyname>M&#xf6;ller</keyname><forenames>Torsten</forenames></author></authors><title>Paraglide: Interactive Parameter Space Partitioning for Computer
  Simulations</title><categories>cs.SY</categories><report-no>SFU-CMPT TR 2011-06</report-no><acm-class>G.3; G.4; H.5.2; I.6; I.6.4; I.6.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce paraglide, a visualization system designed for
interactive exploration of parameter spaces of multi-variate simulation models.
To get the right parameter configuration, model developers frequently have to
go back and forth between setting parameters and qualitatively judging the
outcomes of their model. During this process, they build up a grounded
understanding of the parameter effects in order to pick the right setting.
Current state-of-the-art tools and practices, however, fail to provide a
systematic way of exploring these parameter spaces, making informed decisions
about parameter settings a tedious and workload-intensive task. Paraglide
endeavors to overcome this shortcoming by assisting the sampling of the
parameter space and the discovery of qualitatively different model outcomes.
This results in a decomposition of the model parameter space into regions of
distinct behaviour. We developed paraglide in close collaboration with experts
from three different domains, who all were involved in developing new models
for their domain. We first analyzed current practices of six domain experts and
derived a set of design requirements, then engaged in a longitudinal
user-centered design process, and finally conducted three in-depth case studies
underlining the usefulness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5183</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5183</id><created>2011-10-24</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Diffusion of Information in Robot Swarms</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to communication approaches, which spread information in
robot swarms. These mechanisms are useful for large-scale systems and also for
such cases when a limited communication equipment does not allow routing of
information packages. We focus on two approaches such as virtual fields and
epidemic algorithms, discuss several aspects of hardware implementation and
demonstrate experiments performed with microrobots &quot;Jasmine&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5186</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5186</id><created>2011-10-24</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author></authors><title>Removing spurious interactions in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 7 figures</comments><journal-ref>Phys. Rev. E 85, 036101 (2012)</journal-ref><doi>10.1103/PhysRevE.85.036101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying and removing spurious links in complex networks is a meaningful
problem for many real applications and is crucial for improving the reliability
of network data, which in turn can lead to a better understanding of the highly
interconnected nature of various social, biological and communication systems.
In this work we study the features of different simple spurious link
elimination methods, revealing that they may lead to the distortion of
networks' structural and dynamical properties. Accordingly, we propose a hybrid
method which combines similarity-based index and edge-betweenness centrality.
We show that our method can effectively eliminate the spurious interactions
while leaving the network connected and preserving the network's
functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5190</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5190</id><created>2011-10-24</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author></authors><title>Constant-factor approximation of domination number in sparse graphs</title><categories>math.CO cs.DS</categories><comments>10 pages, 0 figures</comments><msc-class>05C69</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-domination number of a graph is the minimum size of a set X such that
every vertex of G is in distance at most k from X. We give a linear time
constant-factor approximation algorithm for k-domination number in classes of
graphs with bounded expansion, which include e.g. proper minor-closed graph
classes, classes closed on topological minors or classes of graphs that can be
drawn on a fixed surface with bounded number of crossings on each edge.
  The algorithm is based on the following approximate min-max characterization.
A subset A of vertices of a graph G is d-independent if the distance between
each pair of vertices in A is greater than d. Note that the size of the largest
2k-independent set is a lower bound for the k-domination number. We show that
every graph from a fixed class with bounded expansion contains a 2k-independent
set A and a k-dominating set D such that |D|=O(|A|), and these sets can be
found in linear time. For domination number (k=1) the assumptions can be
relaxed, and the result holds for all graph classes with arrangeability bounded
by a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5222</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5222</id><created>2011-10-24</created><updated>2012-03-05</updated><authors><author><keyname>Biswas</keyname><forenames>Soumyajyoti</forenames></author><author><keyname>Ghosh</keyname><forenames>Asim</forenames></author><author><keyname>Chatterjee</keyname><forenames>Arnab</forenames></author><author><keyname>Naskar</keyname><forenames>Tapan</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Bikas K.</forenames></author></authors><title>Continuous transition of social efficiencies in the stochastic strategy
  Minority Game</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 6 figs</comments><journal-ref>Phys. Rev. E 85, 031104 (2012)</journal-ref><doi>10.1103/PhysRevE.85.031104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that in a variant of the Minority Game problem, the agents can reach
a state of maximum social efficiency, where the fluctuation between the two
choices is minimum, by following a simple stochastic strategy. By imagining a
social scenario where the agents can only guess about the number of excess
people in the majority, we show that as long as the guess value is sufficiently
close to the reality, the system can reach a state of full efficiency or
minimum fluctuation. A continuous transition to less efficient condition is
observed when the guess value becomes worse. Hence, people can optimize their
guess value for excess population to optimize the period of being in the
majority state. We also consider the situation where a finite fraction of
agents always decide completely randomly (random trader) as opposed to the rest
of the population that follow a certain strategy (chartist). For a single
random trader the system becomes fully efficient with majority-minority
crossover occurring every two-days interval on average. For just two random
traders, all the agents have equal gain with arbitrarily small fluctuations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5236</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5236</id><created>2011-10-24</created><updated>2012-09-06</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author><author><keyname>Vind</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>String Indexing for Patterns with Wildcards</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of indexing a string $t$ of length $n$ to report the
occurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.
Let $occ$ be the number of occurrences of $p$ in $t$, and $\sigma$ the size of
the alphabet. We obtain the following results.
  - A linear space index with query time $O(m+\sigma^j \log \log n + occ)$.
This significantly improves the previously best known linear space index by Lam
et al. [ISAAC 2007], which requires query time $\Theta(jn)$ in the worst case.
  - An index with query time $O(m+j+occ)$ using space $O(\sigma^{k^2} n \log^k
\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.
This is the first non-trivial bound with this query time.
  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].
  We also show that these indexes can be generalized to allow variable length
gaps in the pattern. Our results are obtained using a novel combination of
well-known and new techniques, which could be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5246</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5246</id><created>2011-10-21</created><authors><author><keyname>Stepanenko</keyname><forenames>A. S.</forenames></author><author><keyname>Yurkevich</keyname><forenames>I. V.</forenames></author><author><keyname>Constantinou</keyname><forenames>C. C.</forenames></author><author><keyname>Lerner</keyname><forenames>I. V.</forenames></author></authors><title>Fluctuation-induced traffic congestion in heterogeneous networks</title><categories>cs.NI cond-mat.stat-mech physics.soc-ph</categories><comments>4 pages, 3 figures</comments><doi>10.1209/0295-5075/100/36002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In studies of complex heterogeneous networks, particularly of the Internet,
significant attention was paid to analyzing network failures caused by hardware
faults or overload, where the network reaction was modeled as rerouting of
traffic away from failed or congested elements. Here we model another type of
the network reaction to congestion -- a sharp reduction of the input traffic
rate through congested routes which occurs on much shorter time scales. We
consider the onset of congestion in the Internet where local mismatch between
demand and capacity results in traffic losses and show that it can be described
as a phase transition characterized by strong non-Gaussian loss fluctuations at
a mesoscopic time scale. The fluctuations, caused by noise in input traffic,
are exacerbated by the heterogeneous nature of the network manifested in a
scale-free load distribution. They result in the network strongly overreacting
to the first signs of congestion by significantly reducing input traffic along
the communication paths where congestion is utterly negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5252</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5252</id><created>2011-10-24</created><authors><author><keyname>Inassaridze</keyname><forenames>Nick</forenames></author><author><keyname>Ladra</keyname><forenames>Manuel</forenames></author><author><keyname>Kandelaki</keyname><forenames>Tamaz</forenames></author></authors><title>Categorical interpretations of some key agreement protocols</title><categories>cs.CR math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give interpretations of some known key agreement protocols in the
framework of category theory and in this way we give a method of constructing
of many new key agreement protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5265</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5265</id><created>2011-10-24</created><authors><author><keyname>Werner</keyname><forenames>Eric</forenames></author></authors><title>On Programs and Genomes</title><categories>q-bio.OT cs.CE q-bio.GN</categories><comments>This a slightly extended version of Part I of a position paper
  distributed on November 18, 2007 to the participants of our Balliol Seminar
  on the Conceptual Foundations of Systems Biology. It presented my ideas on
  the global control architecture of genomes. Denis Noble and myself started
  the seminar in the Michaelmas term in the autumn of 2006 at Balliol College,
  University of Oxford</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline the global control architecture of genomes. A theory of genomic
control information is presented. The concept of a developmental control
network called a cene (for control gene) is introduced. We distinguish
parts-genes from control genes or cenes. Cenes are interpreted and executed by
the cell and, thereby, direct cell actions including communication, growth,
division, differentiation and multi-cellular development. The cenome is the
global developmental control network in the genome. The cenome is also a cene
that consists of interlinked sub-cenes that guide the ontogeny of the organism.
The complexity of organisms is linked to the complexity of the cenome. The
relevance to ontogeny and evolution is mentioned. We introduce the concept of a
universal cell and a universal genome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5280</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5280</id><created>2011-10-24</created><authors><author><keyname>Ivanova</keyname><forenames>Kristinka</forenames></author><author><keyname>Iordanov</keyname><forenames>Ivan</forenames></author></authors><title>Two-Population Dynamics in a Growing Network Model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>23 pages, 6 figures; in press in Physica A</comments><journal-ref>Physica A: Statistical Mechanics and its Applications 391(2012),
  pp. 1811-1821</journal-ref><doi>10.1016/j.physa.2011.09.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a growing network evolution model with nodal attributes. The
model describes the interactions between potentially violent V and non-violent
N agents who have different affinities in establishing connections within their
own population versus between the populations. The model is able to generate
all stable triads observed in real social systems. In the framework of rate
equations theory, we employ the mean-field approximation to derive analytical
expressions of the degree distribution and the local clustering coefficient for
each type of nodes. Analytical derivations agree well with numerical simulation
results. The assortativity of the potentially violent network qualitatively
resembles the connectivity pattern in terrorist networks that was recently
reported. The assortativity of the network driven by aggression shows clearly
different behavior than the assortativity of the networks with connections of
non-aggressive nature in agreement with recent empirical results of an online
social system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5296</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5296</id><created>2011-10-24</created><authors><author><keyname>Chowdhury</keyname><forenames>Shihabur Rahman</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahbubul</forenames></author><author><keyname>Iqbal</keyname><forenames>Sumaiya</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>Computing a Longest Common Palindromic Subsequence</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em longest common subsequence (LCS)} problem is a classic and
well-studied problem in computer science. Palindrome is a word which reads the
same forward as it does backward. The {\em longest common palindromic
subsequence (LCPS)} problem is an interesting variant of the classic LCS
problem which finds the longest common subsequence between two given strings
such that the computed subsequence is also a palindrome. In this paper, we
study the LCPS problem and give efficient algorithms to solve this problem. To
the best of our knowledge, this is the first attempt to study and solve this
interesting problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5305</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5305</id><created>2011-10-24</created><authors><author><keyname>Gittens</keyname><forenames>Alex</forenames></author></authors><title>The spectral norm error of the naive Nystrom extension</title><categories>math.NA cs.NA</categories><comments>1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The naive Nystrom extension forms a low-rank approximation to a
positive-semidefinite matrix by uniformly randomly sampling from its columns.
This paper provides the first relative-error bound on the spectral norm error
incurred in this process. This bound follows from a natural connection between
the Nystrom extension and the column subset selection problem. The main tool is
a matrix Chernoff bound for sampling without replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5342</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5342</id><created>2011-10-21</created><authors><author><keyname>Masazade</keyname><forenames>Engin</forenames></author><author><keyname>Niu</keyname><forenames>Ruixin</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Dynamic Bit Allocation for Object Tracking in Bandwidth Limited Sensor
  Networks</title><categories>stat.AP cs.IT math.IT</categories><comments>Original manusprit is submitted to IEEE Transactions on Signal
  Processing. Part of this work was presented at the Fusion'11 conference held
  at Chicago, IL, July 5-8, 2011</comments><doi>10.1109/TSP.2012.2204257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the target tracking problem in wireless sensor
networks (WSNs) using quantized sensor measurements under limited bandwidth
availability. At each time step of tracking, the available bandwidth $R$ needs
to be distributed among the $N$ sensors in the WSN for the next time step. The
optimal solution for the bandwidth allocation problem can be obtained by using
a combinatorial search which may become computationally prohibitive for large
$N$ and $R$. Therefore, we develop two new computationally efficient suboptimal
bandwidth distribution algorithms which are based on convex relaxation and
approximate dynamic programming (A-DP). We compare the mean squared error (MSE)
and computational complexity performances of convex relaxation and A-DP with
other existing suboptimal bandwidth distribution schemes based on generalized
Breiman, Friedman, Olshen, and Stone (GBFOS) algorithm and greedy search.
Simulation results show that, A-DP, convex optimization and GBFOS yield similar
MSE performance, which is very close to that based on the optimal exhaustive
search approach and they outperform greedy search and nearest neighbor based
bandwidth allocation approaches significantly. Computationally, A-DP is more
efficient than the bandwidth allocation schemes based on convex relaxation and
GBFOS, especially for a large sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5353</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5353</id><created>2011-10-24</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Quantum Copy-Protection and Quantum Money</title><categories>quant-ph cs.CC</categories><comments>14-page conference abstract; full version hasn't appeared and will
  never appear. Being posted to arXiv mostly for archaeological purposes.
  Explicit money scheme has since been broken by Lutomirski et al
  (arXiv:0912.3825). Other quantum money material has been superseded by
  results of Aaronson and Christiano (coming soon). Quantum copy-protection
  ideas will hopefully be developed in separate work</comments><journal-ref>Proceedings of IEEE Conference on Computational Complexity, pages
  229-242, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forty years ago, Wiesner proposed using quantum states to create money that
is physically impossible to counterfeit, something that cannot be done in the
classical world. However, Wiesner's scheme required a central bank to verify
the money, and the question of whether there can be unclonable quantum money
that anyone can verify has remained open since. One can also ask a related
question, which seems to be new: can quantum states be used as copy-protected
programs, which let the user evaluate some function f, but not create more
programs for f? This paper tackles both questions using the arsenal of modern
computational complexity. Our main result is that there exist quantum oracles
relative to which publicly-verifiable quantum money is possible, and any family
of functions that cannot be efficiently learned from its input-output behavior
can be quantumly copy-protected. This provides the first formal evidence that
these tasks are achievable. The technical core of our result is a
&quot;Complexity-Theoretic No-Cloning Theorem,&quot; which generalizes both the standard
No-Cloning Theorem and the optimality of Grover search, and might be of
independent interest. Our security argument also requires explicit
constructions of quantum t-designs. Moving beyond the oracle world, we also
present an explicit candidate scheme for publicly-verifiable quantum money,
based on random stabilizer states; as well as two explicit schemes for
copy-protecting the family of point functions. We do not know how to base the
security of these schemes on any existing cryptographic assumption. (Note that
without an oracle, we can only hope for security under some computational
assumption.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5355</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5355</id><created>2011-10-24</created><updated>2012-02-17</updated><authors><author><keyname>Alvarez-Hamelin</keyname><forenames>Jos&#xe9; Ignacio</forenames></author></authors><title>Is it possible to find the maximum clique in general graphs?</title><categories>cs.DS cs.CC cs.DM</categories><comments>http://hal.archives-ouvertes.fr/hal-00625917/en</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the maximum clique is a known NP-Complete problem and it is also hard
to approximate. This work proposes two efficient algorithms to obtain it.
Nevertheless, the first one is able to fins the maximum for some special cases,
while the second one has its execution time bounded by the number of cliques
that each vertex belongs to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5360</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5360</id><created>2011-10-24</created><authors><author><keyname>Weston</keyname><forenames>Stuart</forenames></author><author><keyname>Natusch</keyname><forenames>Tim</forenames></author><author><keyname>Gulyaev</keyname><forenames>Sergei</forenames></author></authors><title>New Zealand involvement in Radio Astronomical VLBI Image Processing</title><categories>astro-ph.IM cs.GR</categories><comments>6 pages, 11 figures, accepted for presentation at IVCNZ 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the establishment of the AUT University 12m radio telescope at
Warkworth, New Zealand has now become a part of the international Very Long
Baseline Interferometry (VLBI) community. A major product of VLBI observations
are images in the radio domain of astronomical objects such as Active Galactic
Nuclei (AGN). Using large geographical separations between radio antennas, very
high angular resolution can be achieved. Detailed images can be created using
the technique of VLBI Earth Rotation Aperture Synthesis. We review the current
process of VLBI radio imaging. In addition we model VLBI configurations using
the Warkworth telescope, AuScope (a new array of three 12m antennas in
Australia) and the Australian Square Kilometre Array Pathfinder (ASKAP) array
currently under construction in Western Australia, and discuss how the
configuration of these arrays affects the quality of images. Recent imaging
results that demonstrate the modeled improvements from inclusion of the AUT and
first ASKAP telescope in the Australian Long Baseline Array (LBA) are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5371</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5371</id><created>2011-10-24</created><authors><author><keyname>Mahdian</keyname><forenames>Alireza</forenames></author><author><keyname>Black</keyname><forenames>John</forenames></author><author><keyname>Han</keyname><forenames>Richard</forenames></author><author><keyname>Mishra</keyname><forenames>Shivakant</forenames></author></authors><title>MyZone: A Next-Generation Online Social Network</title><categories>cs.SI cs.CR cs.DC cs.NI physics.soc-ph</categories><report-no>CU-CS-1089-11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report considers the design of a social network that would
address the shortcomings of the current ones, and identifies user privacy,
security, and service availability as strong motivations that push the
architecture of the proposed design to be distributed. We describe our design
in detail and identify the property of resiliency as a key objective for the
overall design philosophy.
  We define the system goals, threat model, and trust model as part of the
system model, and discuss the challenges in adapting such distributed
frameworks to become highly available and highly resilient in potentially
hostile environments. We propose a distributed solution to address these
challenges based on a trust-based friendship model for replicating user
profiles and disseminating messages, and examine how this approach builds upon
prior work in distributed Peer-to-Peer (P2P) networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5383</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5383</id><created>2011-10-24</created><updated>2012-02-09</updated><authors><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative
  Attribute Graphs</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the first sub-quadratic sampling algorithm for the Multiplicative
Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close
connection between MAGM and the Kronecker Product Graph Model (KPGM) of
Leskovec et al. (2010), and show that to sample a graph from a MAGM it suffices
to sample small number of KPGM graphs and \emph{quilt} them together. Under a
restricted set of technical conditions our algorithm runs in $O((\log_2(n))^3
|E|)$ time, where $n$ is the number of nodes and $|E|$ is the number of edges
in the sampled graph. We demonstrate the scalability of our algorithm via
extensive empirical evaluation; we can sample a MAGM graph with 8 million nodes
and 20 billion edges in under 6 hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5395</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5395</id><created>2011-10-24</created><updated>2013-10-03</updated><authors><author><keyname>Danner</keyname><forenames>Norman</forenames></author><author><keyname>DeFabbia-Kane</keyname><forenames>Sam</forenames></author><author><keyname>Krizanc</keyname><forenames>Danny</forenames></author><author><keyname>Liberatore</keyname><forenames>Marc</forenames></author></authors><title>Effectiveness and detection of denial of service attacks in Tor</title><categories>cs.CR cs.NI</categories><comments>Author-prepared journal version</comments><acm-class>C.2.0; K.4.1</acm-class><journal-ref>Transactions on Information and System Security 15(3):11:1-11:25,
  2012</journal-ref><doi>10.1145/2382448.2382449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tor is currently one of the more popular systems for anonymizing near
real-time communications on the Internet. Recently, Borisov et al. proposed a
denial of service based attack on Tor (and related systems) that significantly
increases the probability of compromising the anonymity provided. In this
paper, we analyze the effectiveness of the attack using both an analytic model
and simulation. We also describe two algorithms for detecting such attacks, one
deterministic and proved correct, the other probabilistic and verified in
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5396</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5396</id><created>2011-10-24</created><authors><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>Joint Channel-Network Coding Strategies for Networks with Low Complexity
  Relays</title><categories>cs.IT math.IT</categories><comments>accepted and to appear in European Transactions on Telecommunications</comments><journal-ref>European Transactions on Telecommunications, Volume 22, Issue 7,
  pages 396-406, November 2011</journal-ref><doi>10.1002/ett.1492</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate joint network and channel coding schemes for networks when
relay nodes are not capable of performing channel coding operations. Rather,
channel encoding is performed at the source node while channel decoding is done
only at the destination nodes. We examine three different decoding strategies:
independent network-then-channel decoding, serial network and channel decoding,
and joint network and channel decoding. Furthermore, we describe how to
implement such joint network and channel decoding using iteratively decodable
error correction codes. Using simple networks as a model, we derive achievable
rate regions and use simulations to demonstrate the effectiveness of the three
decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5404</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5404</id><created>2011-10-24</created><authors><author><keyname>Le</keyname><forenames>Thai Hoang</forenames></author><author><keyname>Bui</keyname><forenames>Len</forenames></author></authors><title>Face Recognition Based on SVM and 2DPCA</title><categories>cs.CV</categories><comments>10 pages, 7 figures, 2 tables, International Journal of Signal
  Processing, Image Processing and Pattern Recognition Vol. 4, No. 3,
  September, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper will present a novel approach for solving face recognition problem.
Our method combines 2D Principal Component Analysis (2DPCA), one of the
prominent methods for extracting feature vectors, and Support Vector Machine
(SVM), the most powerful discriminative method for classification. Experiments
based on proposed method have been conducted on two public data sets FERET and
AT&amp;T; the results show that the proposed method could improve the
classification rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5419</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5419</id><created>2011-10-25</created><authors><author><keyname>Ibekwe-Sanjuan</keyname><forenames>Fidelia</forenames><affiliation>ELICO</affiliation></author><author><keyname>Sanjuan</keyname><forenames>Eric</forenames><affiliation>LIA</affiliation></author></authors><title>Knowledge Organization Research in the last two decades: 1988-2008</title><categories>cs.DL</categories><proxy>ccsd</proxy><journal-ref>Advances in Knowledge Organization 12 (2010) 115-121</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply an automatic topic mapping system to records of publications in
knowledge organization published between 1988-2008. The data was collected from
journals publishing articles in the KO field from Web of Science database
(WoS). The results showed that while topics in the first decade (1988-1997)
were more traditional, the second decade (1998-2008) was marked by a more
technological orientation and by the appearance of more specialized topics
driven by the pervasiveness of the Web environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5439</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5439</id><created>2011-10-25</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Vittorio</forenames></author></authors><title>A Unifying Tool for Bounding the Quality of Non-Cooperative Solutions in
  Weighted Congestion Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general technique, based on a primal-dual formulation, for
analyzing the quality of self-emerging solutions in weighted congestion games.
With respect to traditional combinatorial approaches, the primal-dual schema
has at least three advantages: first, it provides an analytic tool which can
always be used to prove tight upper bounds for all the cases in which we are
able to characterize exactly the polyhedron of the solutions under analysis;
secondly, in each such a case the complementary slackness conditions give us an
hint on how to construct matching lower bounding instances; thirdly, proofs
become simpler and easy to check. For the sake of exposition, we first apply
our technique to the problems of bounding the prices of anarchy and stability
of exact and approximate pure Nash equilibria, as well as the approximation
ratio of the solutions achieved after a one-round walk starting from the empty
strategy profile, in the case of affine latency functions and we show how all
the known upper bounds for these measures (and some of their generalizations)
can be easily reobtained under a unified approach. Then, we use the technique
to attack the more challenging setting of polynomial latency functions. In
particular, we obtain the first known upper bounds on the price of stability of
pure Nash equilibria and on the approximation ratio of the solutions achieved
after a one-round walk starting from the empty strategy profile for unweighted
players in the cases of quadratic and cubic latency functions. We believe that
our technique, thanks to its versatility, may prove to be a powerful tool also
in several other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5441</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5441</id><created>2011-10-25</created><updated>2012-06-27</updated><authors><author><keyname>Magierski</keyname><forenames>Piotr</forenames></author><author><keyname>Wlazlowski</keyname><forenames>Gabriel</forenames></author></authors><title>LINPRO: linear inverse problem library for data contaminated by
  statistical noise</title><categories>cs.MS hep-lat physics.data-an</categories><comments>The associated computer program is available at:
  http://tja.if.pw.edu.pl/linpro/</comments><journal-ref>Comput. Phys. Commun. 183 (2012) 2264-2271</journal-ref><doi>10.1016/j.cpc.2012.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The library LINPRO which provides solution to the linear inverse problem for
data contaminated by a statistical noise is presented. The library makes use of
two methods: Maximum Entropy Method and Singular Value Decomposition. As an
example it has been applied to perform an analytic continuation of the
imaginary time propagator obtained within the Quantum Monte Carlo method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5447</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5447</id><created>2011-10-25</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames></author></authors><title>Optimal discovery with probabilistic expert advice</title><categories>math.OC cs.LG</categories><msc-class>93E35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an original problem that arises from the issue of security
analysis of a power system and that we name optimal discovery with
probabilistic expert advice. We address it with an algorithm based on the
optimistic paradigm and the Good-Turing missing mass estimator. We show that
this strategy uniformly attains the optimal discovery rate in a macroscopic
limit sense, under some assumptions on the probabilistic experts. We also
provide numerical experiments suggesting that this optimal behavior may still
hold under weaker assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5450</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5450</id><created>2011-10-25</created><authors><author><keyname>Cespi</keyname><forenames>Roberto</forenames></author><author><keyname>Kolb</keyname><forenames>Andreas</forenames></author><author><keyname>Lindner</keyname><forenames>Marvin</forenames></author></authors><title>Hand Tracking based on Hierarchical Clustering of Range Data</title><categories>cs.CV</categories><comments>Technical Report</comments><msc-class>68U10</msc-class><acm-class>I.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast and robust hand segmentation and tracking is an essential basis for
gesture recognition and thus an important component for contact-less
human-computer interaction (HCI). Hand gesture recognition based on 2D video
data has been intensively investigated. However, in practical scenarios purely
intensity based approaches suffer from uncontrollable environmental conditions
like cluttered background colors. In this paper we present a real-time hand
segmentation and tracking algorithm using Time-of-Flight (ToF) range cameras
and intensity data. The intensity and range information is fused into one pixel
value, representing its combined intensity-depth homogeneity. The scene is
hierarchically clustered using a GPU based parallel merging algorithm, allowing
a robust identification of both hands even for inhomogeneous backgrounds. After
the detection, both hands are tracked on the CPU. Our tracking algorithm can
cope with the situation that one hand is temporarily covered by the other hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5468</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5468</id><created>2011-10-25</created><authors><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author><author><keyname>Schindelar</keyname><forenames>Kristina</forenames></author></authors><title>Fraction-free algorithm for the computation of diagonal forms matrices
  over Ore domains using Gr{\&quot;o}bner bases</title><categories>math.RA cs.SC math.OC</categories><comments>25 pages, to appear in Journal of Symbolic Computation</comments><msc-class>13P10, 16Z05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a sequel to &quot;Computing diagonal form and Jacobson normal form
of a matrix using Groebner bases&quot;, J. of Symb. Computation, 46 (5), 2011. We
present a new fraction-free algorithm for the computation of a diagonal form of
a matrix over a certain non-commutative Euclidean domain over a computable
field with the help of Gr\&quot;obner bases. This algorithm is formulated in a
general constructive framework of non-commutative Ore localizations of
$G$-algebras (OLGAs). We split the computation of a normal form of a matrix
into the diagonalization and the normalization processes. Both of them can be
made fraction-free. For a matrix $M$ over an OLGA we provide a diagonalization
algorithm to compute $U,V$ and $D$ with fraction-free entries such that $UMV=D$
holds and $D$ is diagonal. The fraction-free approach gives us more information
on the system of linear functional equations and its solutions, than the
classical setup of an operator algebra with rational functions coefficients. In
particular, one can handle distributional solutions together with, say,
meromorphic ones. We investigate Ore localizations of common operator algebras
over $K[x]$ and use them in the unimodularity analysis of transformation
matrices $U,V$. In turn, this allows to lift the isomorphism of modules over an
OLGA Euclidean domain to a polynomial subring of it. We discuss the relation of
this lifting with the solutions of the original system of equations. Moreover,
we prove some new results concerning normal forms of matrices over non-simple
domains. Our implementation in the computer algebra system {\sc
Singular:Plural} follows the fraction-free strategy and shows impressive
performance, compared with methods which directly use fractions. Since we
experience moderate swell of coefficients and obtain simple transformation
matrices, the method we propose is well suited for solving nontrivial practical
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5574</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5574</id><created>2011-10-25</created><authors><author><keyname>Cabrera</keyname><forenames>Oscar</forenames></author><author><keyname>Oriol</keyname><forenames>Marc</forenames></author><author><keyname>Franch</keyname><forenames>Xavier</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Lidia</forenames></author><author><keyname>Marco</keyname><forenames>Jordi</forenames></author><author><keyname>Fragoso</keyname><forenames>Olivia</forenames></author><author><keyname>Santaolaya</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>WeSSQoS: A Configurable SOA System for Quality-aware Web Service
  Selection</title><categories>cs.NI cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Services (WS) have become one the most used technologies nowadays in
software systems. Among the challenges when integrating WS in a given system,
requirements-driven selection occupies a prominent place. A comprehensive
selection process needs to check compliance of Non-Functional Requirements
(NFR), which can be assessed by analysing WS Quality of Service (QoS). In this
paper, we describe the WeSSQoS system that aims at ranking available WS based
on the comparison of their QoS and the stated NFRs. WeSSQoS is designed as an
open service-oriented architecture that hosts a configurable portfolio of
normalization and ranking algorithms that can be selected by the engineer when
starting a selection process. WS' QoS can be obtained either from a static,
WSDL-like description, or computed dynamically through monitoring techniques.
WeSSQoS is designed to work over multiple WS repositories and QoS sources. The
impact of having a portfolio of different normalization and ranking algorithms
is illustrated with an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5575</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5575</id><created>2011-10-25</created><authors><author><keyname>Puchala</keyname><forenames>Bernd</forenames></author><author><keyname>Rabinovich</keyname><forenames>Roman</forenames></author></authors><title>Graph Searching, Parity Games and Imperfect Information</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the interrelation between graph searching games and games with
imperfect information. As key consequence we obtain that parity games with
bounded imperfect information can be solved in PTIME on graphs of bounded
DAG-width which generalizes several results for parity games on graphs of
bounded complexity. We use a new concept of graph searching where several cops
try to catch multiple robbers instead of just a single robber. The main
technical result is that the number of cops needed to catch r robbers
monotonously is at most r times the DAG-width of the graph. We also explore
aspects of this new concept as a refinement of directed path-width which
accentuates its connection to the concept of imperfect information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5609</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5609</id><created>2011-10-25</created><updated>2011-12-04</updated><authors><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Self-similar scaling of density in complex real-world networks</title><categories>nlin.AO cs.SI physics.soc-ph</categories><journal-ref>Physica A 391(8), 2794-2802 (2012)</journal-ref><doi>10.1016/j.physa.2011.12.055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their diverse origin, networks of large real-world systems reveal a
number of common properties including small-world phenomena, scale-free degree
distributions and modularity. Recently, network self-similarity as a natural
outcome of the evolution of real-world systems has also attracted much
attention within the physics literature. Here we investigate the scaling of
density in complex networks under two classical box-covering
renormalizations-network coarse-graining-and also different community-based
renormalizations. The analysis on over 50 real-world networks reveals a
power-law scaling of network density and size under adequate renormalization
technique, yet irrespective of network type and origin. The results thus
advance a recent discovery of a universal scaling of density among different
real-world networks [Laurienti et al., Physica A 390 (20) (2011) 3608-3613.]
and imply an existence of a scale-free density also within-among different
self-similar scales of-complex real-world networks. The latter further improves
the comprehension of self-similar structure in large real-world networks with
several possible applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5657</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5657</id><created>2011-10-25</created><updated>2012-09-05</updated><authors><author><keyname>McNicholl</keyname><forenames>Timothy H.</forenames></author></authors><title>Computing links and accessing arcs</title><categories>math.LO cs.CG math.GN</categories><msc-class>03F60, 30C20, 30C30, 30C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sufficient conditions are given for the computation of accessing arcs and
arcs that links boundary components of multiply connected domains. The
existence of a not-computably-accessible but computable point on a computably
compact arc is also demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5667</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5667</id><created>2011-10-25</created><authors><author><keyname>Hwang</keyname><forenames>Irvin</forenames></author><author><keyname>Stuhlm&#xfc;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Goodman</keyname><forenames>Noah D.</forenames></author></authors><title>Inducing Probabilistic Programs by Bayesian Program Merging</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report outlines an approach to learning generative models from data. We
express models as probabilistic programs, which allows us to capture abstract
patterns within the examples. By choosing our language for programs to be an
extension of the algebraic data type of the examples, we can begin with a
program that generates all and only the examples. We then introduce greater
abstraction, and hence generalization, incrementally to the extent that it
improves the posterior probability of the examples given the program. Motivated
by previous approaches to model merging and program induction, we search for
such explanatory abstractions using program transformations. We consider two
types of transformation: Abstraction merges common subexpressions within a
program into new functions (a form of anti-unification). Deargumentation
simplifies functions by reducing the number of arguments. We demonstrate that
this approach finds key patterns in the domain of nested lists, including
parameterized sub-functions and stochastic recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5673</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5673</id><created>2011-10-25</created><updated>2012-02-28</updated><authors><author><keyname>Grabowicz</keyname><forenames>Przemyslaw A.</forenames></author><author><keyname>Eguiluz</keyname><forenames>Victor M.</forenames></author></authors><title>Heterogeneity shapes groups growth in social online communities</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 3 figure panels</comments><journal-ref>EPL 97 (2012) 28002</journal-ref><doi>10.1209/0295-5075/97/28002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex systems are characterized by broad distributions capturing, for
example, the size of firms, the population of cities or the degree distribution
of complex networks. Typically this feature is explained by means of a
preferential growth mechanism. Although heterogeneity is expected to play a
role in the evolution it is usually not considered in the modeling probably due
to a lack of empirical evidence on how it is distributed. We characterize the
intrinsic heterogeneity of groups in an online community and then show that
together with a simple linear growth and an inhomogeneous birth rate it
explains the broad distribution of group members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5684</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5684</id><created>2011-10-25</created><updated>2012-08-14</updated><authors><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>Disjoint edges in complete topological graphs</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that every complete n-vertex simple topological graph has at
least Omega(n^{1/3}) pairwise disjoint edges, and these edges can be found in
polynomial time. This proves a conjecture of Pach and T\'oth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5688</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5688</id><created>2011-10-25</created><authors><author><keyname>Ball</keyname><forenames>Nicholas M.</forenames><affiliation>Herzberg Institute of Astrophysics, Victoria, BC, Canada</affiliation></author></authors><title>Discussion on &quot;Techniques for Massive-Data Machine Learning in
  Astronomy&quot; by A. Gray</title><categories>astro-ph.IM astro-ph.CO cs.LG</categories><comments>6 pages, 1 figure. Invited commentary, Statistical Challenges in
  Modern Astronomy V, Penn State, Jun 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomy is increasingly encountering two fundamental truths: (1) The field
is faced with the task of extracting useful information from extremely large,
complex, and high dimensional datasets; (2) The techniques of astroinformatics
and astrostatistics are the only way to make this tractable, and bring the
required level of sophistication to the analysis. Thus, an approach which
provides these tools in a way that scales to these datasets is not just
desirable, it is vital. The expertise required spans not just astronomy, but
also computer science, statistics, and informatics. As a computer scientist and
expert in machine learning, Alex's contribution of expertise and a large number
of fast algorithms designed to scale to large datasets, is extremely welcome.
We focus in this discussion on the questions raised by the practical
application of these algorithms to real astronomical datasets. That is, what is
needed to maximally leverage their potential to improve the science return?
This is not a trivial task. While computing and statistical expertise are
required, so is astronomical expertise. Precedent has shown that, to-date, the
collaborations most productive in producing astronomical science results (e.g,
the Sloan Digital Sky Survey), have either involved astronomers expert in
computer science and/or statistics, or astronomers involved in close, long-term
collaborations with experts in those fields. This does not mean that the
astronomers are giving the most important input, but simply that their input is
crucial in guiding the effort in the most fruitful directions, and coping with
the issues raised by real data. Thus, the tools must be useable and
understandable by those whose primary expertise is not computing or statistics,
even though they may have quite extensive knowledge of those fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5696</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5696</id><created>2011-10-25</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Subspace Evasive Sets</title><categories>cs.CC math.AG math.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we describe an explicit, simple, construction of large subsets
of F^n, where F is a finite field, that have small intersection with every
k-dimensional affine subspace. Interest in the explicit construction of such
sets, termed subspace-evasive sets, started in the work of Pudlak and Rodl
(2004) who showed how such constructions over the binary field can be used to
construct explicit Ramsey graphs. More recently, Guruswami (2011) showed that,
over large finite fields (of size polynomial in n), subspace evasive sets can
be used to obtain explicit list-decodable codes with optimal rate and constant
list-size. In this work we construct subspace evasive sets over large fields
and use them to reduce the list size of folded Reed-Solomon codes form poly(n)
to a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5704</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5704</id><created>2011-10-26</created><authors><author><keyname>Hajirasouliha</keyname><forenames>Iman</forenames></author><author><keyname>Sch&#xf6;nhuth</keyname><forenames>Alexander</forenames></author><author><keyname>Juan</keyname><forenames>David</forenames></author><author><keyname>Valencia</keyname><forenames>Alfonso</forenames></author><author><keyname>Sahinalp</keyname><forenames>S. Cenk</forenames></author></authors><title>Mirroring co-evolving trees in the light of their topologies</title><categories>q-bio.PE cs.DS</categories><comments>13 pages, 2 figures, Iman Hajirasouliha and Alexander Sch\&quot;onhuth are
  joint first authors</comments><msc-class>62P10</msc-class><journal-ref>Bioinformatics, 28(9), 1202-1208, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the interaction partners among protein/domain families poses hard
computational problems, in particular in the presence of paralogous proteins.
Available approaches aim to identify interaction partners among protein/domain
families through maximizing the similarity between trimmed versions of their
phylogenetic trees. Since maximization of any natural similarity score is
computationally difficult, many approaches employ heuristics to maximize the
distance matrices corresponding to the tree topologies in question. In this
paper we devise an efficient deterministic algorithm which directly maximizes
the similarity between two leaf labeled trees with edge lengths, obtaining a
score-optimal alignment of the two trees in question.
  Our algorithm is significantly faster than those methods based on distance
matrix comparison: 1 minute on a single processor vs. 730 hours on a
supercomputer. Furthermore we have advantages over the current state-of-the-art
heuristic search approach in terms of precision as well as a recently suggested
overall performance measure for mirrortree approaches, while incurring only
acceptable losses in recall.
  A C implementation of the method demonstrated in this paper is available at
http://compbio.cs.sfu.ca/mirrort.htm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5710</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5710</id><created>2011-10-26</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Results on the Redundancy of Universal Compression for Finite-Length
  Sequences</title><categories>cs.IT math.IT</categories><comments>accepted in the 2011 IEEE International Symposium on Information
  Theory (ISIT 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the redundancy of universal coding schemes on
smooth parametric sources in the finite-length regime. We derive an upper bound
on the probability of the event that a sequence of length $n$, chosen using
Jeffreys' prior from the family of parametric sources with $d$ unknown
parameters, is compressed with a redundancy smaller than
$(1-\epsilon)\frac{d}{2}\log n$ for any $\epsilon&gt;0$. Our results also confirm
that for large enough $n$ and $d$, the average minimax redundancy provides a
good estimate for the redundancy of most sources. Our result may be used to
evaluate the performance of universal source coding schemes on finite-length
sequences. Additionally, we precisely characterize the minimax redundancy for
two--stage codes. We demonstrate that the two--stage assumption incurs a
negligible redundancy especially when the number of source parameters is large.
Finally, we show that the redundancy is significant in the compression of small
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5712</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5712</id><created>2011-10-26</created><updated>2012-02-09</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Mapping (USPTO) Patent Data using Overlays to Google Maps</title><categories>cs.CY physics.soc-ph</categories><comments>Journal of the American Society for Information Science and
  Technology (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique is developed using patent information available online (at the US
Patent and Trademark Office) for the generation of Google Maps. The overlays
indicate both the quantity and quality of patents at the city level. This
information is relevant for research questions in technology analysis,
innovation studies and evolutionary economics, as well as economic geography.
The resulting maps can also be relevant for technological innovation policies
and R&amp;D management, because the US market can be considered the leading market
for patenting and patent competition. In addition to the maps, the routines
provide quantitative data about the patents for statistical analysis. The
cities on the map are colored according to the results of significance tests.
The overlays are explored for the Netherlands as a &quot;national system of
innovations,&quot; and further elaborated in two cases of emerging technologies:
&quot;RNA interference&quot; and &quot;nanotechnology.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5722</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5722</id><created>2011-10-26</created><authors><author><keyname>Ibekwe-Sanjuan</keyname><forenames>Fidelia</forenames><affiliation>ELICO</affiliation></author><author><keyname>Silvia</keyname><forenames>Fernandez</forenames><affiliation>LIA</affiliation></author><author><keyname>Eric</keyname><forenames>Sanjuan</forenames><affiliation>LIA</affiliation></author><author><keyname>Eric</keyname><forenames>Charton</forenames><affiliation>LIA</affiliation></author></authors><title>Annotation of Scientific Summaries for Information Retrieval</title><categories>cs.IR</categories><comments>ECIR'08 Workshop on: Exploiting Semantic Annotations for Information
  Retrieval, Glasgow : United Kingdom (2008)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a methodology combining surface NLP and Machine Learning
techniques for ranking asbtracts and generating summaries based on annotated
corpora. The corpora were annotated with meta-semantic tags indicating the
category of information a sentence is bearing (objective, findings, newthing,
hypothesis, conclusion, future work, related work). The annotated corpus is fed
into an automatic summarizer for query-oriented abstract ranking and multi-
abstract summarization. To adapt the summarizer to these two tasks, two novel
weighting functions were devised in order to take into account the distribution
of the tags in the corpus. Results, although still preliminary, are encouraging
us to pursue this line of work and find better ways of building IR systems that
can take into account semantic annotations in a corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5741</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5741</id><created>2011-10-26</created><authors><author><keyname>Czap</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Secure Capacity Region for Erasure Broadcast Channels with Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and study a cryptographic problem relevant to wireless: a
sender, Alice, wants to transmit private messages to two receivers, Bob and
Calvin, using unreliable wireless broadcast transmissions and short public
feedback from Bob and Calvin. We ask, at what rates can we broadcast the
private messages if we also provide (information-theoretic) unconditional
security guarantees that Bob and Calvin do not learn each-other's message? We
characterize the largest transmission rates to the two receivers, for any
protocol that provides unconditional security guarantees. We design a protocol
that operates at any rate-pair within the above region, uses very simple
interactions and operations, and is robust to misbehaving users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5746</identifier>
 <datestamp>2012-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5746</id><created>2011-10-26</created><updated>2012-01-24</updated><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Private and Quantum Capacities of More Capable and Less Noisy Quantum
  Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, 1 figure, In v2, fig 1 is modified because fig 1 in v1
  incorrectly stated that the degradable class is included in the conjugate
  degradable class. Main results are unchanged. V3 is the published version. In
  v3, the title is changed by a suggestion from an editor</comments><journal-ref>Phys. Rev. A 85, 012326 (2012)</journal-ref><doi>10.1103/PhysRevA.85.012326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two new classes of quantum channels, which we call more capable and less
noisy, are introduced. The more capable class consists of channels such that
the quantum capacities of the complementary channels to the environments are
zero. The less noisy class consists of channels such that the private
capacities of the complementary channels to the environment are zero. For the
more capable class, it is clarified that the private capacity and quantum
capacity coincide. For the less noisy class, it is clarified that the private
capacity and quantum capacity can be single letter characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5753</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5753</id><created>2011-10-26</created><authors><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author></authors><title>Secondary Spectrum Auctions for Symmetric and Submodular Bidders</title><categories>cs.DS cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study truthful auctions for secondary spectrum usage in wireless networks.
In this scenario, n communication requests need to be allocated to k available
channels that are subject to interference and noise. We present the first
truthful mechanisms for secondary spectrum auctions with symmetric or
submodular valuations. Our approach to model interference uses an edge-weighted
conflict graph, and our algorithms provide asymptotically almost optimal
approximation bounds for conflict graphs with a small inductive independence
number rho &lt;&lt; n. This approach covers a large variety of interference models
such as, e.g., the protocol model or the recently popular physical model of
interference. For unweighted conflict graphs and symmetric valuations we use
LP-rounding to obtain $O(\rho)$-approximate mechanisms; for weighted conflict
graphs we get a factor of O(rho (log n + log k)). For submodular users we
combine the convex rounding framework of Dughmi et al [STOC 2011] with
randomized meta-rounding to obtain O(rho)-approximate mechanisms for
matroid-rank-sum valuations; for weighted conflict graphs we can fully drop the
dependence on k to get O(rho log n). We conclude with promising initialresults
for deterministically truthful mechanisms that allow approximation factors
based on rho.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5762</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5762</id><created>2011-10-26</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Swarmrobot.org - Open-hardware Microrobotic Project for Large-scale
  Artificial Swarms</title><categories>cs.RO cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to give an overview of the open-hardware
microrobotic project swarmrobot.org and the platform Jasmine for building
large-scale artificial swarms. The project targets an open development of
cost-effective hardware and software for a quick implementation of swarm
behavior with real robots. Detailed instructions for making the robot,
open-source simulator, software libraries and multiple publications about
performed experiments are ready for download and intend to facilitate
exploration of collective and emergent phenomena, guided self-organization and
swarm robotics in experimental way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5765</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5765</id><created>2011-10-26</created><authors><author><keyname>Anastasia</keyname><forenames>Davide</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Throughput-Distortion Computation Of Generic Matrix Multiplication:
  Toward A Computation Channel For Digital Signal Processing Systems</title><categories>cs.MS cs.CE</categories><comments>IEEE Transactions on Signal Processing (vol. 60, 2012)</comments><doi>10.1109/TSP.2011.2176337</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The generic matrix multiply (GEMM) function is the core element of
high-performance linear algebra libraries used in many
computationally-demanding digital signal processing (DSP) systems. We propose
an acceleration technique for GEMM based on dynamically adjusting the
imprecision (distortion) of computation. Our technique employs adaptive scalar
companding and rounding to input matrix blocks followed by two forms of packing
in floating-point that allow for concurrent calculation of multiple results.
Since the adaptive companding process controls the increase of concurrency (via
packing), the increase in processing throughput (and the corresponding increase
in distortion) depends on the input data statistics. To demonstrate this, we
derive the optimal throughput-distortion control framework for GEMM for the
broad class of zero-mean, independent identically distributed, input sources.
Our approach converts matrix multiplication in programmable processors into a
computation channel: when increasing the processing throughput, the output
noise (error) increases due to (i) coarser quantization and (ii) computational
errors caused by exceeding the machine-precision limitations. We show that,
under certain distortion in the GEMM computation, the proposed framework can
significantly surpass 100% of the peak performance of a given processor. The
practical benefits of our proposal are shown in a face recognition system and a
multi-layer perceptron system trained for metadata learning from a large music
feature database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5793</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5793</id><created>2011-10-26</created><authors><author><keyname>Berten</keyname><forenames>Vandy</forenames><affiliation>U.L.B</affiliation></author><author><keyname>Goossens</keyname><forenames>Jo&#xeb;l</forenames><affiliation>U.L.B</affiliation></author></authors><title>Sufficient FTP Schedulability Test for the Non-Cyclic Generalized
  Multiframe Task Model</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to provide a sufficient schedulability test -ideally polynomial-
for the scheduling of Non-Cyclic Generalized Multiframe Task Model using
Fixed-Task-Priority schedulers. We report two first results: (i) we present and
prove correct the critical instant for the Non-Cyclic Generalized Multiframe
Task Model then (ii) we propose an algorithm which provides a sufficient (but
pseudo-polynomial) schedulability test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5794</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5794</id><created>2011-10-26</created><updated>2013-01-21</updated><authors><author><keyname>Zhou</keyname><forenames>Peng</forenames></author><author><keyname>Luo</keyname><forenames>Xiapu</forenames></author><author><keyname>Chen</keyname><forenames>Ang</forenames></author><author><keyname>Chang</keyname><forenames>Rocky K. C.</forenames></author></authors><title>STor: Social Network based Anonymous Communication in Tor</title><categories>cs.CR</categories><comments>Key words: Social Network, Anonymous Communication, Tor, Fuzzy Model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymity networks hide user identities with the help of relayed anonymity
routers. However, the state-of-the-art anonymity networks do not provide an
effective trust model. As a result, users cannot circumvent malicious or
vulnerable routers, thus making them susceptible to malicious router based
attacks (e.g., correlation attacks). In this paper, we propose a novel social
network based trust model to help anonymity networks circumvent malicious
routers and obtain secure anonymity. In particular, we design an input
independent fuzzy model to determine trust relationships between friends based
on qualitative and quantitative social attributes, both of which can be readily
obtained from existing social networks. Moreover, we design an algorithm for
propagating trust over an anonymity network. We integrate these two elements in
STor, a novel social network based Tor. We have implemented STor by modifying
the Tor's source code and conducted experiments on PlanetLab to evaluate the
effectiveness of STor. Both simulation and PlanetLab experiment results have
demonstrated that STor can achieve secure anonymity by establishing trust-based
circuits in a distributed way. Although the design of STor is based on Tor
network, the social network based trust model can be adopted by other anonymity
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5805</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5805</id><created>2011-10-26</created><updated>2012-05-13</updated><authors><author><keyname>Adaricheva</keyname><forenames>Kira</forenames></author><author><keyname>Nation</keyname><forenames>J. B.</forenames></author><author><keyname>Rand</keyname><forenames>Robert</forenames></author></authors><title>Ordered direct implicational basis of a finite closure system</title><categories>math.CO cs.LO math.RA</categories><comments>25 pages, 10 figures; presented at AMS conference,
  TACL-2011,ISAIM-2012 and at RUTCOR seminar</comments><journal-ref>Discrete Applied Mathematics 161 (2013), pp. 707-723</journal-ref><doi>10.1016/j.dam.2012.08.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Closure system on a finite set is a unifying concept in logic programming,
relational data bases and knowledge systems. It can also be presented in the
terms of finite lattices, and the tools of economic description of a finite
lattice have long existed in lattice theory. We present this approach by
describing the so-called D-basis and introducing the concept of ordered direct
basis of an implicational system. A direct basis of a closure operator, or an
implicational system, is a set of implications that allows one to compute the
closure of an arbitrary set by a single iteration. This property is preserved
by the D-basis at the cost of following a prescribed order in which
implications will be attended. In particular, using an ordered direct basis
allows to optimize the forward chaining procedure in logic programming that
uses the Horn fragment of propositional logic. One can extract the D-basis from
any direct unit basis S in time polynomial in the size of S, and it takes only
linear time of the cardinality of the D-basis to put it into a proper order. We
produce examples of closure systems on a 6-element set, for which the canonical
basis of Duquenne and Guigues is not ordered direct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5813</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5813</id><created>2011-10-26</created><updated>2012-07-03</updated><authors><author><keyname>Xie</keyname><forenames>Jierui</forenames></author><author><keyname>Kelley</keyname><forenames>Stephen</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Overlapping Community Detection in Networks: the State of the Art and
  Comparative Study</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>This paper (final version) is accepted in 2012. ACM Computing
  Surveys, vol. 45, no. 4, 2013 (In press) Contact: jierui.xie@gmail.com</comments><journal-ref>ACM Computing Surveys 45(4), Article 43 (August 2013)</journal-ref><doi>10.1145/2501654.2501657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews the state of the art in overlapping community detection
algorithms, quality measures, and benchmarks. A thorough comparison of
different algorithms (a total of fourteen) is provided. In addition to
community level evaluation, we propose a framework for evaluating algorithms'
ability to detect overlapping nodes, which helps to assess over-detection and
under-detection. After considering community level detection performance
measured by Normalized Mutual Information, the Omega index, and node level
detection performance measured by F-score, we reached the following
conclusions. For low overlapping density networks, SLPA, OSLOM, Game and COPRA
offer better performance than the other tested algorithms. For networks with
high overlapping density and high overlapping diversity, both SLPA and Game
provide relatively stable performance. However, test results also suggest that
the detection in such networks is still not yet fully resolved. A common
feature observed by various algorithms in real-world networks is the relatively
small fraction of overlapping nodes (typically less than 30%), each of which
belongs to only 2 or 3 communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5825</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5825</id><created>2011-10-26</created><authors><author><keyname>Graham</keyname><forenames>Matthew</forenames></author><author><keyname>Rixon</keyname><forenames>Guy</forenames></author><author><keyname>Grid</keyname></author><author><keyname>Group</keyname><forenames>Web Services Working</forenames></author></authors><title>IVOA Recommendation: IVOA Support Interfaces</title><categories>astro-ph.IM cs.DL</categories><report-no>REC-VOSI-1.0-20110531</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes the minimum interface that a (SOAP- or REST-based)
web service requires to participate in the IVOA. Note that this is not required
of standard VO services developed prior to this specification, although uptake
is strongly encouraged on any subsequent revision. All new standard VO
services, however, must feature a VOSI-compliant interface.
  This document has been produced by the Grid and Web Services Working Group.
It has been reviewed by IVOA Members and other interested parties, and has been
endorsed by the IVOA Executive Committee as an IVOA Recommendation. It is a
stable document and may be used as reference material or cited as a normative
reference from another document. IVOA's role in making the Recommendation is to
draw attention to the specification and to promote its widespread deployment.
This enhances the functionality and interoperability inside the Astronomical
Community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5832</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5832</id><created>2011-09-23</created><authors><author><keyname>Baumann</keyname><forenames>Ringo</forenames></author><author><keyname>Herre</keyname><forenames>Heinrich</forenames></author></authors><title>The Axiomatic Foundation of Space in GFO</title><categories>cs.OH</categories><comments>36 pages, 18 figures, submitted to &quot;Applied Ontology&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space and time are basic categories of any top-level ontology. They are
fundamental assumptions for the mode of existence of those individuals which
are said to be in space and time. In the present paper the ontology of space in
the General Formal Ontology (GFO) is expounded. This ontology is represented as
a theory BT (Brentano Theory), which is specified by a set of axioms formalized
in first-order logic. This theory uses four primitive relations: SReg(x) (x is
space region), spart(x, y) (x is spatial part of y), sb(x, y) (x is spatial
boundary of y), and scoinc(x, y) (x and y spatially coincide). This ontology is
inspired by ideas of Franz Brentano. The investigation and exploration of Franz
Brentano's ideas on space and time began about twenty years ago by work of R.M.
Chisholm, B. Smith and A. Varzi. The present paper takes up this line of
research and makes a further step in establishing an ontology of space which is
based on rigorous logical methods and on principles of the new philosophical
approach of integrative realism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5844</identifier>
 <datestamp>2015-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5844</id><created>2011-10-17</created><authors><author><keyname>Bandyopadhyay</keyname><forenames>Anirban</forenames></author><author><keyname>Pati</keyname><forenames>Ranjit</forenames></author><author><keyname>Sahu</keyname><forenames>Satyajit</forenames></author><author><keyname>Peper</keyname><forenames>Ferdinand</forenames></author><author><keyname>Fujita</keyname><forenames>Daisuke</forenames></author></authors><title>Massively parallel computing on an organic molecular layer</title><categories>cs.ET physics.comp-ph</categories><comments>25 pages, 6 figures</comments><journal-ref>Nature Physics 6, 369 (2010)</journal-ref><doi>10.1038/nphys1636</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Current computers operate at enormous speeds of ~10^13 bits/s, but their
principle of sequential logic operation has remained unchanged since the 1950s.
Though our brain is much slower on a per-neuron base (~10^3 firings/s), it is
capable of remarkable decision-making based on the collective operations of
millions of neurons at a time in ever-evolving neural circuitry. Here we use
molecular switches to build an assembly where each molecule communicates-like
neurons-with many neighbors simultaneously. The assembly's ability to
reconfigure itself spontaneously for a new problem allows us to realize
conventional computing constructs like logic gates and Voronoi decompositions,
as well as to reproduce two natural phenomena: heat diffusion and the mutation
of normal cells to cancer cells. This is a shift from the current static
computing paradigm of serial bit-processing to a regime in which a large number
of bits are processed in parallel in dynamically changing hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5863</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5863</id><created>2011-10-17</created><authors><author><keyname>Martin</keyname><forenames>Owen S.</forenames></author></authors><title>A Wikipedia Literature Review</title><categories>cs.DL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper was originally designed as a literature review for a doctoral
dissertation focusing on Wikipedia. This exposition gives the structure of
Wikipedia and the latest trends in Wikipedia research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5865</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5865</id><created>2011-10-26</created><authors><author><keyname>Werner</keyname><forenames>Eric</forenames></author></authors><title>Cancer Networks: A general theoretical and computational framework for
  understanding cancer</title><categories>q-bio.MN cs.CE cs.MA q-bio.CB q-bio.GN</categories><comments>Key words: Cancer networks, cene, cenome, developmental control
  networks, stem cells, stem cell networks, cancer stem cells, stochastic stem
  cell networks, metastases hierarchy, linear networks, exponential networks,
  geometric cancer networks, cell signaling, cancer cell communication
  networks, systems biology, computational biology, multiagent systems,
  muticellular modeling, cancer modeling</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general computational theory of cancer and its developmental
dynamics. The theory is based on a theory of the architecture and function of
developmental control networks which guide the formation of multicellular
organisms. Cancer networks are special cases of developmental control networks.
Cancer results from transformations of normal developmental networks. Our
theory generates a natural classification of all possible cancers based on
their network architecture. Each cancer network has a unique topology and
semantics and developmental dynamics that result in distinct clinical tumor
phenotypes. We apply this new theory with a series of proof of concept cases
for all the basic cancer types. These cases have been computationally modeled,
their behavior simulated and mathematically described using a multicellular
systems biology approach. There are fascinating correspondences between the
dynamic developmental phenotype of computationally modeled {\em in silico}
cancers and natural {\em in vivo} cancers. The theory lays the foundation for a
new research paradigm for understanding and investigating cancer. The theory of
cancer networks implies that new diagnostic methods and new treatments to cure
cancer will become possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5867</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5867</id><created>2011-10-26</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author><author><keyname>Nadel</keyname><forenames>Alexander</forenames></author></authors><title>From Total Assignment Enumeration to Modern SAT Solver</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new framework for presenting and analyzing the functionality of a modern
DLL-based SAT solver is proposed. Our approach exploits the inherent relation
between backtracking and resolution. We show how to derive the algorithm of a
modern SAT solver from DLL step-by-step. We analyze the inference power of
Boolean Constraint Propagation, Non-Chronological Backtracking and 1UIP-based
Conflict-Directed Backjumping. Our work can serve as an introduction to a
modern SAT solver functionality and as a basis for future work on the inference
power of a modern SAT solver and on practical SAT solver design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5870</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5870</id><created>2011-10-26</created><authors><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Universal and efficient compressed sensing by spread spectrum and
  application to realistic Fourier imaging techniques</title><categories>cs.IT math.IT</categories><comments>Submitted for publication in EURASIP Journal on Advances in Signal
  Processing</comments><journal-ref>EURASIP Journal on Advances in Signal Processing 2012, 2012:6</journal-ref><doi>10.1186/1687-6180-2012-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We advocate a compressed sensing strategy that consists of multiplying the
signal of interest by a wide bandwidth modulation before projection onto
randomly selected vectors of an orthonormal basis. Firstly, in a digital
setting with random modulation, considering a whole class of sensing bases
including the Fourier basis, we prove that the technique is universal in the
sense that the required number of measurements for accurate recovery is optimal
and independent of the sparsity basis. This universality stems from a drastic
decrease of coherence between the sparsity and the sensing bases, which for a
Fourier sensing basis relates to a spread of the original signal spectrum by
the modulation (hence the name &quot;spread spectrum&quot;). The approach is also
efficient as sensing matrices with fast matrix multiplication algorithms can be
used, in particular in the case of Fourier measurements. Secondly, these
results are confirmed by a numerical analysis of the phase transition of the
l1- minimization problem. Finally, we show that the spread spectrum technique
remains effective in an analog setting with chirp modulation for application to
realistic Fourier imaging. We illustrate these findings in the context of radio
interferometry and magnetic resonance imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5886</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5886</id><created>2011-09-29</created><authors><author><keyname>Blum</keyname><forenames>B.</forenames></author><author><keyname>Koller</keyname><forenames>D.</forenames></author><author><keyname>Shelton</keyname><forenames>C. R.</forenames></author></authors><title>A Continuation Method for Nash Equilibria in Structured Games</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 25, pages
  457-502, 2006</journal-ref><doi>10.1613/jair.1947</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured game representations have recently attracted interest as models
for multi-agent artificial intelligence scenarios, with rational behavior most
commonly characterized by Nash equilibria. This paper presents efficient, exact
algorithms for computing Nash equilibria in structured game representations,
including both graphical games and multi-agent influence diagrams (MAIDs). The
algorithms are derived from a continuation method for normal-form and
extensive-form games due to Govindan and Wilson; they follow a trajectory
through a space of perturbed games and their equilibria, exploiting game
structure through fast computation of the Jacobian of the payoff function. They
are theoretically guaranteed to find at least one equilibrium of the game, and
may find more. Our approach provides the first efficient algorithm for
computing exact equilibria in graphical games with arbitrary topology, and the
first algorithm to exploit fine-grained structural properties of MAIDs.
Experimental results are presented demonstrating the effectiveness of the
algorithms and comparing them to predecessors. The running time of the
graphical game algorithm is similar to, and often better than, the running time
of previous approximate algorithms. The algorithm for MAIDs can effectively
solve games that are much larger than those solvable by previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5888</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5888</id><created>2011-10-26</created><updated>2012-03-29</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Racz</keyname><forenames>Miklos Z.</forenames></author></authors><title>A quantitative Gibbard-Satterthwaite theorem without neutrality</title><categories>math.CO cs.GT math.PR</categories><comments>46 pages; v2 has minor structural changes and adds open problems</comments><msc-class>05D40, 91B14, 68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, quantitative versions of the Gibbard-Satterthwaite theorem were
proven for $k=3$ alternatives by Friedgut, Kalai, Keller and Nisan and for
neutral functions on $k \geq 4$ alternatives by Isaksson, Kindler and Mossel.
  We prove a quantitative version of the Gibbard-Satterthwaite theorem for
general social choice functions for any number $k \geq 3$ of alternatives. In
particular we show that for a social choice function $f$ on $k \geq 3$
alternatives and $n$ voters, which is $\epsilon$-far from the family of
nonmanipulable functions, a uniformly chosen voter profile is manipulable with
probability at least inverse polynomial in $n$, $k$, and $\epsilon^{-1}$.
  Removing the neutrality assumption of previous theorems is important for
multiple reasons. For one, it is known that there is a conflict between
anonymity and neutrality, and since most common voting rules are anonymous,
they cannot always be neutral. Second, virtual elections are used in many
applications in artificial intelligence, where there are often restrictions on
the outcome of the election, and so neutrality is not a natural assumption in
these situations.
  Ours is a unified proof which in particular covers all previous cases
established before. The proof crucially uses reverse hypercontractivity in
addition to several ideas from the two previous proofs. Much of the work is
devoted to understanding functions of a single voter, and in particular we also
prove a quantitative Gibbard-Satterthwaite theorem for one voter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5889</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5889</id><created>2011-09-29</created><authors><author><keyname>Said</keyname><forenames>Hamadene</forenames></author><author><keyname>Mohammed</keyname><forenames>Hassani</forenames></author></authors><title>The Multi-player Nonzero-sum Dynkin Game in Continuous Time</title><categories>cs.GT math.PR</categories><msc-class>91A15, 91A10, 91A30, 60G40, 91A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the N-player nonzero-sum Dynkin game ($N\geq 3$) in
continuous time, which is a non-cooperative game where the strategies are
stopping times. We show that the game has a Nash equilibrium point for general
payoff processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5890</identifier>
 <datestamp>2011-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5890</id><created>2011-10-26</created><authors><author><keyname>Belanovic</keyname><forenames>Pavle</forenames></author><author><keyname>Macua</keyname><forenames>Sergio Valcarcel</forenames></author><author><keyname>Zazo</keyname><forenames>Santiago</forenames></author></authors><title>Location-aided Distributed Primary User Identification in a Cognitive
  Radio Scenario</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to IEEE ICASSP2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a cognitive radio scenario, where a number of secondary users
performs identification of which primary user, if any, is transmitting, in a
distributed way and using limited location information. We propose two fully
distributed algorithms: the first is a direct identification scheme, and in the
other a distributed sub-optimal detection based on a simplified Neyman-Pearson
energy detector precedes the identification scheme. Both algorithms are studied
analytically in a realistic transmission scenario, and the advantage obtained
by detection pre-processing is also verified via simulation. Finally, we give
details of their fully distributed implementation via consensus averaging
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5892</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5892</id><created>2011-10-26</created><authors><author><keyname>Elias</keyname><forenames>Yuval</forenames></author><author><keyname>Mor</keyname><forenames>Tal</forenames></author><author><keyname>Weinstein</keyname><forenames>Yossi</forenames></author></authors><title>Semi-optimal Practicable Algorithmic Cooling</title><categories>quant-ph cs.ET cs.IT math.IT</categories><comments>13 pages, 5 figures</comments><journal-ref>Phys. Rev. A 83 (2011) 042340</journal-ref><doi>10.1103/PhysRevA.83.042340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic Cooling (AC) of spins applies entropy manipulation algorithms in
open spin-systems in order to cool spins far beyond Shannon's entropy bound. AC
of nuclear spins was demonstrated experimentally, and may contribute to nuclear
magnetic resonance (NMR) spectroscopy. Several cooling algorithms were
suggested in recent years, including practicable algorithmic cooling (PAC) and
exhaustive AC. Practicable algorithms have simple implementations, yet their
level of cooling is far from optimal; Exhaustive algorithms, on the other hand,
cool much better, and some even reach (asymptotically) an optimal level of
cooling, but they are not practicable. We introduce here semi-optimal
practicable AC (SOPAC), wherein few cycles (typically 2-6) are performed at
each recursive level. Two classes of SOPAC algorithms are proposed and
analyzed. Both attain cooling levels significantly better than PAC, and are
much more efficient than the exhaustive algorithms. The new algorithms are
shown to bridge the gap between PAC and exhaustive AC. In addition, we
calculated the number of spins required by SOPAC in order to purify qubits for
quantum computation. As few as 12 and 7 spins are required (in an ideal
scenario) to yield a mildly pure spin (60% polarized) from initial
polarizations of 1% and 10%, respectively. In the latter case, about five more
spins are sufficient to produce a highly pure spin (99.99% polarized), which
could be relevant for fault-tolerant quantum computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5915</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5915</id><created>2011-10-26</created><updated>2012-12-01</updated><authors><author><keyname>Crowston</keyname><forenames>R.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Yeo</keyname><forenames>A.</forenames></author></authors><title>Parameterized Complexity of Satisfying Almost All Linear Equations over
  $\mathbb{F}_2$</title><categories>cs.CC cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem MaxLin2 can be stated as follows. We are given a system $S$ of
$m$ equations in variables $x_1,...,x_n$, where each equation is $\sum_{i \in
I_j}x_i = b_j$ is assigned a positive integral weight $w_j$ and $x_i,b_j \in
\mathbb{F}_2$, $I_j \subseteq \{1,2,...,n\}$ for $j=1,...,m$. We are required
to find an assignment of values to the variables in order to maximize the total
weight of the satisfied equations.
  Let $W$ be the total weight of all equations in $S$. We consider the
following parameterized version of MaxLin2: decide whether there is an
assignment satisfying equations of total weight at least $W-k$, where $k$ is a
nonnegative parameter. We prove that this parameterized problem is W[1]-hard
even if each equation of $S$ has exactly three variables and every variable
appears in exactly three equations and, moreover, each weight $w_j$ equals 1
and no two equations have the same left-hand side. We show the tightness of
this result by proving that if each equation has at most two variables then the
parameterized problem is fixed-parameter tractable. We also prove that if no
variable appears in more than two equations then we can maximize the total
weight of satisfied equations in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5942</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5942</id><created>2011-10-26</created><updated>2012-10-08</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames><affiliation>MIT CSAIL</affiliation></author><author><keyname>Zhang</keyname><forenames>Ting</forenames><affiliation>Iowa State University</affiliation></author></authors><title>Can Nondeterminism Help Complementation?</title><categories>cs.LO cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.4.3; F.4.1</acm-class><journal-ref>EPTCS 96, 2012, pp. 57-70</journal-ref><doi>10.4204/EPTCS.96.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementation and determinization are two fundamental notions in automata
theory. The close relationship between the two has been well observed in the
literature. In the case of nondeterministic finite automata on finite words
(NFA), complementation and determinization have the same state complexity,
namely Theta(2^n) where n is the state size. The same similarity between
determinization and complementation was found for Buchi automata, where both
operations were shown to have 2^\Theta(n lg n) state complexity. An intriguing
question is whether there exists a type of omega-automata whose determinization
is considerably harder than its complementation. In this paper, we show that
for all common types of omega-automata, the determinization problem has the
same state complexity as the corresponding complementation problem at the
granularity of 2^\Theta(.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5944</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5944</id><created>2011-10-26</created><updated>2011-12-22</updated><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author></authors><title>Communication cost of classically simulating a quantum channel with
  subsequent rank-1 projective measurement</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>corrected some minor typos</comments><journal-ref>Phys. Rev. A 84, 060303(R) (2011)</journal-ref><doi>10.1103/PhysRevA.84.060303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A process of preparation, transmission and subsequent projective measurement
of a qubit can be simulated by a classical model with only two bits of
communication and some amount of shared randomness. However no model for n
qubits with a finite amount of classical communication is known at present. A
lower bound for the communication cost can provide useful hints for a
generalization. It is known for example that the amount of communication must
be greater than c 2^n, where c~0.01. The proof uses a quite elaborate theorem
of communication complexity. Using a mathematical conjecture known as the
&quot;double cap conjecture&quot;, we strengthen this result by presenting a geometrical
and extremely simple derivation of the lower bound 2^n-1. Only rank-1
projective measurements are involved in the derivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5945</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5945</id><created>2011-10-26</created><authors><author><keyname>Dolui</keyname><forenames>Sudipto</forenames></author><author><keyname>Kuurstra</keyname><forenames>Alan</forenames></author><author><keyname>Patarroyo</keyname><forenames>Iv&#xe1;n C. Salgado</forenames></author><author><keyname>Michailovich</keyname><forenames>Oleg V.</forenames></author></authors><title>A New Similarity Measure for Non-Local Means Filtering of MRI Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The acquisition of MRI images offers a trade-off in terms of acquisition
time, spatial/temporal resolution and signal-to-noise ratio (SNR). Thus, for
instance, increasing the time efficiency of MRI often comes at the expense of
reduced SNR. This, in turn, necessitates the use of post-processing tools for
noise rejection, which makes image de-noising an indispensable component of
computer assistance diagnosis. In the field of MRI, a multitude of image
de-noising methods have been proposed hitherto. In this paper, the application
of a particular class of de-noising algorithms - known as non-local mean (NLM)
filters - is investigated. Such filters have been recently applied for MRI data
enhancement and they have been shown to provide more accurate results as
compared to many alternative de-noising algorithms. Unfortunately, virtually
all existing methods for NLM filtering have been derived under the assumption
of additive white Gaussian (AWG) noise contamination. Since this assumption is
known to fail at low values of SNR, an alternative formulation of NLM filtering
is required, which would take into consideration the correct Rician statistics
of MRI noise. Accordingly, the contribution of the present paper is two-fold.
First, it points out some principal disadvantages of the earlier methods of NLM
filtering of MRI images and suggests means to rectify them. Second, the paper
introduces a new similarity measure for NLM filtering of MRI Images, which is
derived under bona fide statistical assumptions and results in more accurate
reconstruction of MR scans as compared to alternative NLM approaches. Finally,
the utility and viability of the proposed method is demonstrated through a
series of numerical experiments using both in silico and in vivo MRI data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5962</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5962</id><created>2011-10-26</created><authors><author><keyname>Saavedra</keyname><forenames>Serguei</forenames></author><author><keyname>Duch</keyname><forenames>Jordi</forenames></author><author><keyname>Uzzi</keyname><forenames>Brian</forenames></author></authors><title>Tracking Traders' Understanding of the Market Using e-Communication Data</title><categories>cs.SI physics.data-an physics.soc-ph</categories><journal-ref>PLoS ONE 6(10): e26705 (2011)</journal-ref><doi>10.1371/journal.pone.0026705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking the volume of keywords in Internet searches, message boards, or
Tweets has provided an alternative for following or predicting associations
between popular interest or disease incidences. Here, we extend that research
by examining the role of e-communications among day traders and their
collective understanding of the market. Our study introduces a general method
that focuses on bundles of words that behave differently from daily
communication routines, and uses original data covering the content of instant
messages among all day traders at a trading firm over a 40-month period.
Analyses show that two word bundles convey traders' understanding of same day
market events and potential next day market events. We find that when market
volatility is high, traders' communications are dominated by same day events,
and when volatility is low, communications are dominated by next day events. We
show that the stronger the traders' attention to either same day or next day
events, the higher their collective trading performance. We conclude that
e-communication among traders is a product of mass collaboration over diverse
viewpoints that embodies unique information about their weak or strong
understanding of the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5969</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5969</id><created>2011-10-26</created><authors><author><keyname>Voorsluys</keyname><forenames>William</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Reliable Provisioning of Spot Instances for Compute-intensive
  Applications</title><categories>cs.DC</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing providers are now offering their unused resources for leasing
in the spot market, which has been considered the first step towards a
full-fledged market economy for computational resources. Spot instances are
virtual machines (VMs) available at lower prices than their standard on-demand
counterparts. These VMs will run for as long as the current price is lower than
the maximum bid price users are willing to pay per hour. Spot instances have
been increasingly used for executing compute-intensive applications. In spite
of an apparent economical advantage, due to an intermittent nature of biddable
resources, application execution times may be prolonged or they may not finish
at all. This paper proposes a resource allocation strategy that addresses the
problem of running compute-intensive jobs on a pool of intermittent virtual
machines, while also aiming to run applications in a fast and economical way.
To mitigate potential unavailability periods, a multifaceted fault-aware
resource provisioning policy is proposed. Our solution employs price and
runtime estimation mechanisms, as well as three fault tolerance techniques,
namely checkpointing, task duplication and migration. We evaluate our
strategies using trace-driven simulations, which take as input real price
variation traces, as well as an application trace from the Parallel Workload
Archive. Our results demonstrate the effectiveness of executing applications on
spot instances, respecting QoS constraints, despite occasional failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5972</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5972</id><created>2011-10-26</created><authors><author><keyname>Voorsluys</keyname><forenames>William</forenames></author><author><keyname>Garg</keyname><forenames>Saurabh Kumar</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Provisioning Spot Market Cloud Resources to Create Cost-effective
  Virtual Clusters</title><categories>cs.DC</categories><comments>14 pages, 4 figures, 11th International Conference on Algorithms and
  Architectures for Parallel Processing (ICA3PP-11); Lecture Notes in Computer
  Science, Vol. 7016, 2011</comments><doi>10.1007/978-3-642-24650-0_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infrastructure-as-a-Service providers are offering their unused resources in
the form of variable-priced virtual machines (VMs), known as &quot;spot instances&quot;,
at prices significantly lower than their standard fixed-priced resources. To
lease spot instances, users specify a maximum price they are willing to pay per
hour and VMs will run only when the current price is lower than the user's bid.
This paper proposes a resource allocation policy that addresses the problem of
running deadline-constrained compute-intensive jobs on a pool of composed
solely of spot instances, while exploiting variations in price and performance
to run applications in a fast and economical way. Our policy relies on job
runtime estimations to decide what are the best types of VMs to run each job
and when jobs should run. Several estimation methods are evaluated and
compared, using trace-based simulations, which take real price variation traces
obtained from Amazon Web Services as input, as well as an application trace
from the Parallel Workload Archive. Results demonstrate the effectiveness of
running computational jobs on spot instances, at a fraction (up to 60% lower)
of the price that would normally cost on fixed priced resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5989</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5989</id><created>2011-10-27</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Fan</keyname><forenames>Xiao</forenames></author></authors><title>A Heuristic Description of Fast Fourier Transform</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Fourier Transform (FFT) is an efficient algorithm to compute the
Discrete Fourier Transform (DFT) and its inverse. In this paper, we pay special
attention to the description of complex-data FFT. We analyze two common
descriptions of FFT and propose a new presentation. Our heuristic description
is helpful for students and programmers to grasp the algorithm entirely and
deeply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.5992</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.5992</id><created>2011-10-27</created><authors><author><keyname>Aittokoski</keyname><forenames>Timo</forenames></author><author><keyname>Tarkkanen</keyname><forenames>Suvi</forenames></author></authors><title>User preference extraction using dynamic query sliders in conjunction
  with UPS-EMO algorithm</title><categories>cs.NE cs.NA</categories><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One drawback of evolutionary multiobjective optimization algorithms (EMOA)
has traditionally been high computational cost to create an approximation of
the Pareto front: number of required objective function evaluations usually
grows high. On the other hand, for the decision maker (DM) it may be difficult
to select one of the many produced solutions as the final one, especially in
the case of more than two objectives.
  To overcome the above mentioned drawbacks number of EMOA's incorporating the
decision makers preference information have been proposed. In this case, it is
possible to save objective function evaluations by generating only the part of
the front the DM is interested in, thus also narrowing down the pool of
possible selections for the final solution.
  Unfortunately, most of the current EMO approaches utilizing preferences are
not very intuitive to use, i.e. they may require tweaking of unintuitive
parameters, and it is not always clear what kind of results one can get with
given set of parameters. In this study we propose a new approach to visually
inspect produced solutions, and to extract preference information from the DM
to further guide the search. Our approach is based on intuitive use of dynamic
query sliders, which serve as a means to extract preference information and are
part of the graphical user interface implemented for the efficient UPS-EMO
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6002</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6002</id><created>2011-10-27</created><authors><author><keyname>Tibabishev</keyname><forenames>V. N.</forenames></author></authors><title>Optimization of frequency quantization</title><categories>math.OC cs.SD</categories><comments>e.g. 6 pages</comments><msc-class>62D05, 42A38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain the functional defining the price and quality of sample readings of
the generalized velocities. It is shown that the optimal sampling frequency, in
the sense of minimizing the functional quality and price depends on the
sampling of the upper cutoff frequency of the analog signal of the order of the
generalized velocities measured by the generalized coordinates, the frequency
properties of the analog input filter and a maximum sampling rate for
analog-digital converter (ADC). An example of calculating the frequency
quantization for two-tier ADC with an input RC filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6010</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6010</id><created>2011-10-27</created><authors><author><keyname>Kamalian</keyname><forenames>Rafayel</forenames></author><author><keyname>Khachatryan</keyname><forenames>Arpine</forenames></author></authors><title>On a property of the $n$-dimensional cube</title><categories>cs.DM math.CO</categories><comments>2 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that in any subset of the vertices of $n$-dimensional cube that
contains at least $2^{n-1}+1$ vertices ($n\geq 4$), there are four vertices
that induce a claw, or there are eight vertices that induce the cycle of length
eight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6012</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6012</id><created>2011-10-27</created><updated>2012-03-13</updated><authors><author><keyname>Feulner</keyname><forenames>Thomas</forenames></author><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author></authors><title>The automorphism group of a self-dual binary [72,36,16] code does not
  contain Z7, Z3xZ3, or D10</title><categories>cs.IT math.IT</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computer calculation with Magma shows that there is no extremal self-dual
binary code C of length 72 that has an automorphism group containing D10,
Z3xZ3, or Z7. Combining this with the known results in the literature one
obtains that Aut(C) is either Z5 or has order dividing 24.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6027</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6027</id><created>2011-10-27</created><updated>2011-10-28</updated><authors><author><keyname>Smieja</keyname><forenames>Marek</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Entropy of the Mixture of Sources and Entropy Dimension</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of the entropy of the mixture of sources. There is
given an estimation of the entropy and entropy dimension of convex combination
of measures. The proof is based on our alternative definition of the entropy
based on measures instead of partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6051</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6051</id><created>2011-10-27</created><authors><author><keyname>Andersen</keyname><forenames>Jakob L.</forenames></author><author><keyname>Flamm</keyname><forenames>Christoph</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Maximizing Output and Recognizing Autocatalysis in Chemical Reaction
  Networks is NP-Complete</title><categories>q-bio.MN cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: A classical problem in metabolic design is to maximize the
production of desired compound in a given chemical reaction network by
appropriately directing the mass flow through the network. Computationally,
this problem is addressed as a linear optimization problem over the &quot;flux
cone&quot;. The prior construction of the flux cone is computationally expensive and
no polynomial-time algorithms are known. Results: Here we show that the output
maximization problem in chemical reaction networks is NP-complete. This
statement remains true even if all reactions are monomolecular or bimolecular
and if only a single molecular species is used as influx. As a corollary we
show, furthermore, that the detection of autocatalytic species, i.e., types
that can only be produced from the influx material when they are present in the
initial reaction mixture, is an NP-complete computational problem. Conclusions:
Hardness results on combinatorial problems and optimization problems are
important to guide the development of computational tools for the analysis of
metabolic networks in particular and chemical reaction networks in general. Our
results indicate that efficient heuristics and approximate algorithms need to
be employed for the analysis of large chemical networks since even conceptually
simple flow problems are provably intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6061</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6061</id><created>2011-10-27</created><updated>2011-10-31</updated><authors><author><keyname>King</keyname><forenames>Emily J.</forenames></author></authors><title>A Matricial Algorithm for Polynomial Refinement</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to have a multiresolution analysis, the scaling function must be
refinable. That is, it must be the linear combination of 2-dilation,
$\mathbb{Z}$-translates of itself. Refinable functions used in connection with
wavelets are typically compactly supported. In 2002, David Larson posed the
question in his REU site, &quot;Are all polynomials (of a single variable) finitely
refinable?&quot; That summer the author proved that the answer indeed was true using
basic linear algebra. The result was presented in a number of talks but had not
been typed up until now. The purpose of this short note is to record that
particular proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6078</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6078</id><created>2011-10-27</created><authors><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author><author><keyname>Rao</keyname><forenames>Shodhan</forenames></author><author><keyname>Jayawardhana</keyname><forenames>Bayu</forenames></author></authors><title>On the Mathematical Structure of Balanced Chemical Reaction Networks
  Governed by Mass Action Kinetics</title><categories>math.OC cs.SY math.DS physics.chem-ph q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent progress on the interplay between graph theory, dynamics,
and systems theory, we revisit the analysis of chemical reaction networks
described by mass action kinetics. For reaction networks possessing a
thermodynamic equilibrium we derive a compact formulation exhibiting at the
same time the structure of the complex graph and the stoichiometry of the
network, and which admits a direct thermodynamical interpretation. This
formulation allows us to easily characterize the set of equilibria and their
stability properties. Furthermore, we develop a framework for interconnection
of chemical reaction networks. Finally we discuss how the established framework
leads to a new approach for model reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6080</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6080</id><created>2011-10-27</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author></authors><title>Simplification paths in the Pachner graphs of closed orientable
  3-manifold triangulations</title><categories>math.GT cs.CG</categories><comments>39 pages, 15 figures, 10 tables. This is the journal version of
  arXiv:1011.4169 and contains significant new material; see the title page for
  details</comments><acm-class>F.2.2; G.2.1; G.2.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is important to have effective methods for simplifying 3-manifold
triangulations without losing any topological information. In theory this is
difficult: we might need to make a triangulation super-exponentially more
complex before we can make it smaller than its original size. Here we present
experimental work that suggests the reality is far different: for an exhaustive
census of 81,800,394 one-vertex triangulations that span 1,901 distinct closed
orientable 3-manifolds, we never need to add more than two extra tetrahedra, we
never need more than a handful of Pachner moves (or bistellar flips), and the
average number of Pachner moves decreases as the number of tetrahedra grows. If
they generalise, these extremely surprising results would have significant
implications for decision algorithms and the study of triangulations in
3-manifold topology.
  Key techniques include polynomial-time computable signatures that identify
triangulations up to isomorphism, the isomorph-free generation of non-minimal
triangulations, theoretical operations to reduce sequences of Pachner moves,
and parallel algorithms for studying finite level sets in the infinite Pachner
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6084</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6084</id><created>2011-10-27</created><updated>2013-05-24</updated><authors><author><keyname>Perchet</keyname><forenames>Vianney</forenames></author><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author></authors><title>The multi-armed bandit problem with covariates</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1101 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1101</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 2, 693-721</journal-ref><doi>10.1214/13-AOS1101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-armed bandit problem in a setting where each arm produces
a noisy reward realization which depends on an observable random covariate. As
opposed to the traditional static multi-armed bandit problem, this setting
allows for dynamically changing rewards that better describe applications where
side information is available. We adopt a nonparametric model where the
expected rewards are smooth functions of the covariate and where the hardness
of the problem is captured by a margin parameter. To maximize the expected
cumulative reward, we introduce a policy called Adaptively Binned Successive
Elimination (abse) that adaptively decomposes the global problem into suitably
&quot;localized&quot; static bandit problems. This policy constructs an adaptive
partition using a variant of the Successive Elimination (se) policy. Our
results include sharper regret bounds for the se policy in a static bandit
problem and minimax optimal regret bounds for the abse policy in the dynamic
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6089</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6089</id><created>2011-10-23</created><updated>2012-02-11</updated><authors><author><keyname>Alipour</keyname><forenames>Philip B.</forenames></author></authors><title>A Universal 4D Model for Double-Efficient Lossless Data Compressions</title><categories>cs.IT math.CO math.IT</categories><comments>Major changes have been made in this revision for a thorough and
  lucid representation of the concept following implementation, in which,
  abstract portions of it are considered for publication in Information Theory
  journals as well as a foundation work for a PhD dissertation at
  http://web.uvic.ca/~phibal12/publication.htm . Written in LaTeX: 76 pages, 56
  references, 9 figures and 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the theory, model, implementation and performance of a
combinatorial fuzzy-binary and-or (FBAR) algorithm for lossless data
compression (LDC) and decompression (LDD) on 8-bit characters. A combinatorial
pairwise flags is utilized as new zero/nonzero, impure/pure bit-pair operators,
where their combination forms a 4D hypercube to compress a sequence of bytes.
The compressed sequence is stored in a grid file of constant size.
Decompression is by using a fixed size translation table (TT) to access the
grid file during I/O data conversions. Compared to other LDC algorithms,
double-efficient (DE) entropies denoting 50% compressions with reasonable
bitrates were observed. Double-extending the usage of the TT component in code,
exhibits a Universal Predictability via its negative growth of entropy for LDCs
&gt; 87.5% compression, quite significant for scaling databases and network
communications. This algorithm is novel in encryption, binary, fuzzy and
information-theoretic methods such as probability. Therefore, information
theorists, computer scientists and engineers may find the algorithm useful for
its logic and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6097</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6097</id><created>2011-10-27</created><updated>2012-11-06</updated><authors><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author><author><keyname>Zhang</keyname><forenames>Jiang</forenames></author></authors><title>The Decentralized Structure of Collective Attention on the Web</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The collective browsing behavior of users gives rise to a flow
network transporting attention between websites. By analyzing the structure of
this network we uncovered a nontrivial scaling regularity concerning the impact
of websites.
  Methodology: We constructed three clickstreams networks, whose nodes were
websites and edges were formed by the users switching between sites. We
developed an indicator Ci as a measure of the impact of site i and investigated
its correlation with the traffic of the site Ai both on the three networks and
across the language communities within the networks.
  Conclusions: We found that the impact of websites increased slower than their
traffic. Specifically, there existed a scaling relationship between Ci and Ai
with an exponent gamma smaller than 1. We suggested that this scaling
relationship characterized the decentralized structure of the clickstream
circulation: the World Wide Web is a system that favors small sites in
reassigning the collective attention of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6105</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6105</id><created>2011-10-26</created><authors><author><keyname>Bhowmick</keyname><forenames>Santanu</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>S.</forenames></author><author><keyname>N</keyname><forenames>Nandakumar G.</forenames></author></authors><title>Generation of Test Vectors for Sequential Cell Verification</title><categories>cs.OH</categories><comments>Presented in ARM Regional Engineering Conference, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Application Specific Integrated Circuits (ASIC) and System-on-Chip (SOC)
designs, Cell - Based Design (CBD) is the most prevalent practice as it
guarantees a shorter design cycle, minimizes errors and is easier to maintain.
In modern ASIC design, standard cell methodology is practiced with sizable
libraries of cells, each containing multiple implementations of the same logic
functionality, in order to give the designer differing options based on area,
speed or power consumption. For such library cells, thorough verification of
functionality and timing is crucial for the overall success of the chip, as
even a small error can prove fatal due to the repeated use of the cell in the
design. Both formal and simulation based methods are being used in the industry
for cell verification. We propose a method using the latter approach that
generates an optimized set of test vectors for verification of sequential
cells, which are guaranteed to give complete Single Input Change transition
coverage with minimal redundancy. Knowledge of the cell functionality by means
of the State Table is the only prerequisite of this procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6126</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6126</id><created>2011-10-27</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>A Counterexample to the Generalized Linial-Nisan Conjecture</title><categories>cs.CC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In earlier work, we gave an oracle separating the relational versions of BQP
and the polynomial hierarchy, and showed that an oracle separating the decision
versions would follow from what we called the Generalized Linial-Nisan (GLN)
Conjecture: that &quot;almost k-wise independent&quot; distributions are
indistinguishable from the uniform distribution by constant-depth circuits. The
original Linial-Nisan Conjecture was recently proved by Braverman; we offered a
$200 prize for the generalized version. In this paper, we save ourselves $200
by showing that the GLN Conjecture is false, at least for circuits of depth 3
and higher. As a byproduct, our counterexample also implies that Pi2P is not
contained in P^NP relative to a random oracle with probability 1. It has been
conjectured since the 1980s that PH is infinite relative to a random oracle,
but the highest levels of PH previously proved separate were NP and coNP.
Finally, our counterexample implies that the famous results of Linial, Mansour,
and Nisan, on the structure of AC0 functions, cannot be improved in several
interesting respects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6127</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6127</id><created>2011-10-27</created><authors><author><keyname>Singh</keyname><forenames>Chandramani</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Optimal Forwarding in Delay Tolerant Networks with Multiple Destinations</title><categories>cs.NI cs.SY</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the trade-off between delivery delay and energy consumption in a
delay tolerant network in which a message (or a file) has to be delivered to
each of several destinations by epidemic relaying. In addition to the
destinations, there are several other nodes in the network that can assist in
relaying the message. We first assume that, at every instant, all the nodes
know the number of relays carrying the packet and the number of destinations
that have received the packet. We formulate the problem as a controlled
continuous time Markov chain and derive the optimal closed loop control (i.e.,
forwarding policy). However, in practice, the intermittent connectivity in the
network implies that the nodes may not have the required perfect knowledge of
the system state. To address this issue, we obtain an ODE (i.e., a
deterministic fluid) approximation for the optimally controlled Markov chain.
This fluid approximation also yields an asymptotically optimal open loop
policy. Finally, we evaluate the performance of the deterministic policy over
finite networks. Numerical results show that this policy performs close to the
optimal closed loop policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6128</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6128</id><created>2011-10-27</created><updated>2013-04-24</updated><authors><author><keyname>Campbell</keyname><forenames>Yuri</forenames></author><author><keyname>Piqueira</keyname><forenames>Jos&#xe9; Roberto Castilho</forenames></author></authors><title>Classical Hierarchical Correlation Quantification on Tripartite Qubit
  Mixed State Families</title><categories>quant-ph cs.IT math.IT nlin.CD</categories><comments>2 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are at least a number of ways to formally define complexity. Most of
them relate to some kind of minimal description of the studied object. Being
this one in form of minimal resources of minimal effort needed to generate the
object itself. This is usually achieved by detecting and taking advantage of
regularities within the object. Regularities can commonly be described in an
information-theoretic approach by quantifying the amount of correlation playing
a role in the system, this being spatial, temporal or both. This is the
approach closely related to the extent that the whole cannot be understood as
only the sum of its parts, but also by their interactions. Feature considered
to be most fundamental. Nevertheless, this irreducibility, even in the basic
quantum informational setting of composite states, is also present due to the
intrinsic structure of Hilbert spaces' tensor product. In this approach, this
irreducibility is quantified based on statistics of von Neumann measurements
forming mutually unbiased bases. Upon two different kinds of tripartite qubit
mixed state families, which hold the two possible distinct entangled states on
this space. Results show that this quantification is sensible to the different
kind of entanglement present on those families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6140</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6140</id><created>2011-10-27</created><authors><author><keyname>Kihara</keyname><forenames>Takayuki</forenames></author></authors><title>Incomputability of Simply Connected Planar Continua</title><categories>math.LO cs.LO</categories><comments>25 pages</comments><msc-class>03F60, 03D78, 54D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Le Roux and Ziegler asked whether every simply connected compact nonempty
planar co-c.e. closed set always contains a computable point. In this paper, we
solve the problem of le Roux and Ziegler by showing that there exists a
contractible planar co-c.e. dendroid without computable points. We also provide
several pathological examples of tree-like co-c.e. continua fulfilling certain
global incomputability properties: there is a computable dendrite which does
not *-include a co-c.e. tree; there is a co-c.e. dendrite which does not
*-include a computable dendrite; there is a computable dendroid which does not
*-include a co-c.e. dendrite. Here, a continuum A *-includes a member of a
class P of continua if, for every positive real, A includes a P-continuum B
such that the Hausdorff distance between A and B is smaller than the real.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6143</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6143</id><created>2011-10-27</created><authors><author><keyname>D'Alotto</keyname><forenames>Louis</forenames></author></authors><title>Cellular Automata Using Infinite Computations</title><categories>cs.DM math.DS</categories><comments>Paper Accepted for Publication in Applied Mathematics and
  Computation, Elsevier</comments><msc-class>37 (Primary) 68 (Secondary)</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an application of the Infinite Unit Axiom, introduced by
Yaroslav Sergeyev, (see [11] - [14]) to the development of one-dimensional
cellular automata. This application allows the establishment of a new and more
precise metric on the space of definition for one-dimensional cellular
automata, whereby accuracy of computations is increased. Using this new metric,
open disks are defined and the number of points in each disk is computed. The
forward dynamics of a cellular automaton map are also studied via defined
equivalence classes. Using the Infinite Unit Axiom, the number of
configurations that stay close to a given configuration under the shift
automaton map can now be computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6161</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6161</id><created>2011-10-27</created><updated>2012-02-26</updated><authors><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Sum-Rate Optimal Power Policies for Energy Harvesting Transmitters in an
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>to appear in Journal of Communications and Networks, Special Issue on
  Energy Harvesting in Wireless Networks, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a two-user Gaussian interference channel with energy
harvesting transmitters. Different than conventional battery powered wireless
nodes, energy harvesting transmitters have to adapt transmission to
availability of energy at a particular instant. In this setting, the optimal
power allocation problem to maximize the sum throughput with a given deadline
is formulated. The convergence of the proposed iterative coordinate descent
method for the problem is proved and the short-term throughput maximizing
offline power allocation policy is found. Examples for interference regions
with known sum capacities are given with directional water-filling
interpretations. Next, stochastic data arrivals are addressed. Finally online
and/or distributed near-optimal policies are proposed. Performance of the
proposed algorithms are demonstrated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6183</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6183</id><created>2011-10-27</created><updated>2012-02-24</updated><authors><author><keyname>Fogarty</keyname><forenames>Seth</forenames><affiliation>Department of Computer Science, Rice University, Houston, TX</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Department of Computer Science, Rice University, Houston, TX</affiliation></author></authors><title>B\&quot;uchi Complementation and Size-Change Termination</title><categories>cs.FL</categories><proxy>LMCS</proxy><acm-class>D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (February
  27, 2012) lmcs:1178</journal-ref><doi>10.2168/LMCS-8(1:13)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare tools for complementing nondeterministic B\&quot;uchi automata with a
recent termination-analysis algorithm. Complementation of B\&quot;uchi automata is a
key step in program verification. Early constructions using a Ramsey-based
argument have been supplanted by rank-based constructions with exponentially
better bounds. In 2001 Lee et al. presented the size-change termination (SCT)
problem, along with both a reduction to B\&quot;uchi automata and a Ramsey-based
algorithm. The Ramsey-based algorithm was presented as a more practical
alternative to the automata-theoretic approach, but strongly resembles the
initial complementation constructions for B\&quot;uchi automata. We prove that the
SCT algorithm is a specialized realization of the Ramsey-based complementation
construction. To do so, we extend the Ramsey-based complementation construction
to provide a containment-testing algorithm. Surprisingly, empirical analysis
suggests that despite the massive gap in worst-case complexity, Ramsey-based
approaches are superior over the domain of SCT problems. Upon further analysis
we discover an interesting property of the problem space that both explains
this result and provides a chance to improve rank-based tools. With these
improvements, we show that theoretical gains in efficiency of the rank-based
approach are mirrored in empirical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6188</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6188</id><created>2011-10-27</created><authors><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Ranked Sparse Signal Support Detection</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><journal-ref>IEEE Trans. on Signal Processing, vol. 60, no. 11, pp. 5919-5931,
  November 2012</journal-ref><doi>10.1109/TSP.2012.2208957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of detecting the support (sparsity pattern)
of a sparse vector from random noisy measurements. Conditional power of a
component of the sparse vector is defined as the energy conditioned on the
component being nonzero. Analysis of a simplified version of orthogonal
matching pursuit (OMP) called sequential OMP (SequOMP) demonstrates the
importance of knowledge of the rankings of conditional powers. When the simple
SequOMP algorithm is applied to components in nonincreasing order of
conditional power, the detrimental effect of dynamic range on thresholding
performance is eliminated. Furthermore, under the most favorable conditional
powers, the performance of SequOMP approaches maximum likelihood performance at
high signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6199</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6199</id><created>2011-10-27</created><authors><author><keyname>Bhatia</keyname><forenames>Aman</forenames></author><author><keyname>Iyengar</keyname><forenames>Aravind R.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Enhancing Binary Images of Non-Binary LDPC Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, to be presented at IEEE GLOBECOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the reasons behind the superior performance of belief
propagation decoding of non-binary LDPC codes over their binary images when the
transmission occurs over the binary erasure channel. We show that although
decoding over the binary image has lower complexity, it has worse performance
owing to its larger number of stopping sets relative to the original non-binary
code. We propose a method to find redundant parity-checks of the binary image
that eliminate these additional stopping sets, so that we achieve performance
comparable to that of the original non-binary LDPC code with lower decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6200</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6200</id><created>2011-10-27</created><updated>2011-11-03</updated><authors><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author><author><keyname>Chau</keyname><forenames>Duen Horng &quot;Polo&quot;</forenames></author><author><keyname>Kittur</keyname><forenames>Aniket</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>TopicViz: Semantic Navigation of Document Collections</title><categories>cs.HC cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When people explore and manage information, they think in terms of topics and
themes. However, the software that supports information exploration sees text
at only the surface level. In this paper we show how topic modeling -- a
technique for identifying latent themes across large collections of documents
-- can support semantic exploration. We present TopicViz, an interactive
environment for information exploration. TopicViz combines traditional search
and citation-graph functionality with a range of novel interactive
visualizations, centered around a force-directed layout that links documents to
the latent themes discovered by the topic model. We describe several use
scenarios in which TopicViz supports rapid sensemaking on large document
collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6221</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6221</id><created>2011-10-27</created><updated>2014-09-26</updated><authors><author><keyname>Takei</keyname><forenames>Ryo</forenames></author><author><keyname>Chen</keyname><forenames>Weiyan</forenames></author><author><keyname>Clawson</keyname><forenames>Zachary</forenames></author><author><keyname>Kirov</keyname><forenames>Slav</forenames></author><author><keyname>Vladimirsky</keyname><forenames>Alexander</forenames></author></authors><title>Optimal control with reset-renewable resources</title><categories>math.OC cs.RO</categories><comments>31 pages, 13 figures; accepted by SIAM J. on Control &amp; Optimization
  (updated to address reviewers' comments)</comments><msc-class>90C29, 49L20, 49L25, 49N90, 65N22, 65K05, 34A38</msc-class><acm-class>I.2.8; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider both discrete and continuous control problems constrained by a
fixed budget of some resource, which may be renewed upon entering a preferred
subset of the state space. In the discrete case, we consider both deterministic
and stochastic shortest path problems with full budget resets in all preferred
nodes. In the continuous case, we derive augmented PDEs of optimal control,
which are then solved numerically on the extended state space with a
full/instantaneous budget reset on the preferred subset. We introduce an
iterative algorithm for solving these problems efficiently. The method's
performance is demonstrated on a range of computational examples, including the
optimal path planning with constraints on prolonged visibility by a static
enemy observer.
  In addition, we also develop an algorithm that works on the original state
space to solve a related but simpler problem: finding the subsets of the domain
&quot;reachable-within-the-budget&quot;.
  This manuscript is an extended version of the paper accepted for publication
by SIAM J. on Control and Optimization. In the journal version, Section 3 and
the Appendix were omitted due to space limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6231</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6231</id><created>2011-10-27</created><authors><author><keyname>&#x141;upi&#x144;ska</keyname><forenames>Agnieszka</forenames></author></authors><title>Parallel implematation of flow and matching algorithms</title><categories>cs.DC</categories><comments>MSc thesis, promoter: dr Maciej \'Slusarek</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we present two parallel algorithms and their lock-free
implementations using a popular GPU environment Nvidia CUDA. The first
algorithm is the push-relabel method for the flow problem in grid graphs. The
second is the cost scaling algorithm for the assignment problem in complete
bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6251</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6251</id><created>2011-10-28</created><authors><author><keyname>Lee</keyname><forenames>Kwankyu</forenames></author><author><keyname>Bras-Amor&#xf3;s</keyname><forenames>Maria</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael E.</forenames></author></authors><title>Unique Decoding of Plane AG Codes via Interpolation</title><categories>cs.IT math.IT</categories><comments>Submitted for publication in the Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unique decoding algorithm of algebraic geometry codes on plane
curves, Hermitian codes in particular, from an interpolation point of view. The
algorithm successfully corrects errors of weight up to half of the order bound
on the minimum distance of the AG code. The decoding algorithm is the first to
combine some features of the interpolation based list decoding with the
performance of the syndrome decoding with majority voting scheme. The regular
structure of the algorithm allows a straightforward parallel implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6265</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6265</id><created>2011-10-28</created><updated>2013-06-18</updated><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Kopiczko</keyname><forenames>Pawel</forenames></author></authors><title>Understanding BitTorrent Through Real Measurements</title><categories>cs.NI cs.PF</categories><comments>11 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the results of the BitTorrent measurement study are presented.
Two sources of BitTorrent data were utilized: meta-data files that describe the
content of resources shared by BitTorrent users and the logs of one of the
currently most popular BitTorrent clients - {\mu}Torrent. {\mu}Torrent is
founded upon a rather newly released UDP-based {\mu}TP protocol that is claimed
to be more efficient than TCP-based clients. Experimental data have been
collected for fifteen days from the popular torrent-discovery site
thepiratebay.org (more than 30,000 torrents were captured and analyzed). During
this period the activity and logs of an unmodified version of {\mu}Torrent
client downloading sessions have been also captured. The obtained experimental
results are swarm-oriented (not tracker-oriented as has been previously
researched), which has allowed us to look at BitTorrent and its users from an
exchanged resources perspective. Moreover, comparative analysis of the clients'
connections with and without {\mu}TP protocol is carried out to verify to what
extent {\mu}TP improves BitTorrent transmissions. To the authors' best
knowledge, none of the previous studies have addressed these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6267</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6267</id><created>2011-10-28</created><authors><author><keyname>Hazelhurst</keyname><forenames>Scott</forenames></author><author><keyname>Johnson</keyname><forenames>Yestin</forenames></author><author><keyname>Sanders</keyname><forenames>Ian</forenames></author></authors><title>An empirical analysis of the relationship between web usage and academic
  performance in undergraduate students</title><categories>cs.SI cs.CY</categories><acm-class>K.3</acm-class><journal-ref>Proceedings of the Annual Conference of the South African Computer
  Lecturer's Association, Ballito, South Africa, July 2011, pp. 29-37</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of the internet, and in particular web browsing, offers many
potential advantages for educational institutions as students have access to a
wide range of information previously not available. However, there are
potential negative effects due to factors such as time-wasting and asocial
behaviour.
  In this study, we conducted an empirical investigation of the academic
performance and the web-usage pattern of 2153 undergraduate students. Data from
university proxy logs allows us to examine usage patterns and we compared this
data to the students' academic performance.
  The results show that there is a small but significant (both statistically
and educationally) association between heavier web browsing and poorer academic
results (lower average mark, higher failure rates). In addition, among good
students, the proportion of students who are relatively light users of the
internet is significantly greater than would be expected by chance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6271</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6271</id><created>2011-10-28</created><updated>2012-03-27</updated><authors><author><keyname>Fournier</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Malod</keyname><forenames>Guillaume</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Monomials in arithmetic circuits: Complete problems in the counting
  hierarchy</title><categories>cs.CC</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexity of two questions on polynomials given by
arithmetic circuits: testing whether a monomial is present and counting the
number of monomials. We show that these problems are complete for subclasses of
the counting hierarchy which had few or no known natural complete problems. We
also study these questions for circuits computing multilinear polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6275</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6275</id><created>2011-10-28</created><updated>2012-08-30</updated><authors><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Stability of Triangular Decomposition and Comprehensive Triangular
  Decomposition</title><categories>cs.SC math.AG</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  a proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new concept, decomposition-unstable (DU) variety of a parametric polynomial
system, is introduced in this paper and the stabilities of several triangular
decomposition methods, such as characteristic set decomposition, relatively
simplicial decomposition and regular chain decomposition, for parametric
polynomial systems are discussed in detail. The concept leads to a definition
of weakly comprehensive triangular decomposition (WCTD) and a new algorithm for
computing comprehensive triangular decomposition (CTD) which was first
introduced in [4] for computing an analogue of comprehensive Groebner systems
for parametric polynomial systems. Our algorithm takes advantage of a
hierarchical solving strategy and a self-adaptive order of parameters. The
algorithm has been implemented with Maple 15 and experimented with a number of
benchmarks from the literature. Comparison with the Maple package
RegularChains, which contains an implementation of the algorithm in [4], is
provided and the results illustrate that the time costs by our program for
computing CTDs of most examples are no more than those by RegularChains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6287</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6287</id><created>2011-10-28</created><authors><author><keyname>Cholewa</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>G&#x142;omb</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Deciding of HMM parameters based on number of critical points for
  gesture recognition from motion capture data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of choosing number of states of a HMM based on
number of critical points of the motion capture data. The choice of Hidden
Markov Models(HMM) parameters is crucial for recognizer's performance as it is
the first step of the training and cannot be corrected automatically within
HMM. In this article we define predictor of number of states based on number of
critical points of the sequence and test its effectiveness against sample data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6288</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6288</id><created>2011-10-28</created><authors><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author></authors><title>Reliability of Computational Experiments on Virtualised Hardware</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present preliminary results of an investigation into the suitability of
virtualised hardware -- in particular clouds -- for running computational
experiments. Our main concern was that the reported CPU time would not be
reliable and reproducible. The results demonstrate that while this is true in
cases where many virtual machines are running on the same physical hardware,
there is no inherent variation introduced by using virtualised hardware
compared to non-virtualised hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6290</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6290</id><created>2011-10-28</created><authors><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author><author><keyname>Jefferson</keyname><forenames>Chris</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author></authors><title>Modelling Constraint Solver Architecture Design as a Constraint Problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing component-based constraint solvers is a complex problem. Some
components are required, some are optional and there are interdependencies
between the components. Because of this, previous approaches to solver design
and modification have been ad-hoc and limited. We present a system that
transforms a description of the components and the characteristics of the
target constraint solver into a constraint problem. Solving this problem yields
the description of a valid solver. Our approach represents a significant step
towards the automated design and synthesis of constraint solvers that are
specialised for individual constraint problem classes or instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6293</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6293</id><created>2011-10-28</created><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author></authors><title>The Cubical Homology of Trace Monoids</title><categories>math.AT cs.MA math.KT</categories><comments>21 pages, International Conference &quot;Toric Topology and Automorphic
  Functions'' September 05-10, 2011 Khabarovsk</comments><msc-class>18G10, 18G35, 55U10, 68Q10, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article contains an overview of the results of the author in a field of
algebraic topology used in computer science. The relationship between the
cubical homology groups of generalized tori and homology groups of partial
trace monoid actions is described. Algorithms for computing the homology groups
of asynchronous systems, Petri nets, and Mazurkiewicz trace languages are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6296</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6296</id><created>2011-10-28</created><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Puy</keyname><forenames>G.</forenames></author><author><keyname>Thiran</keyname><forenames>J. -Ph.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>P.</forenames></author><author><keyname>Van De Ville</keyname><forenames>D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>Implications for compressed sensing of a new sampling theorem on the
  sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>1 page, 2 figures, Signal Processing with Adaptive Sparse Structured
  Representations (SPARS) 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sampling theorem on the sphere has been developed recently, requiring half
as many samples as alternative equiangular sampling theorems on the sphere. A
reduction by a factor of two in the number of samples required to represent a
band-limited signal on the sphere exactly has important implications for
compressed sensing, both in terms of the dimensionality and sparsity of
signals. We illustrate the impact of this property with an inpainting problem
on the sphere, where we show the superior reconstruction performance when
adopting the new sampling theorem compared to the alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6297</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6297</id><created>2011-10-28</created><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Puy</keyname><forenames>G.</forenames></author><author><keyname>Thiran</keyname><forenames>J. -Ph.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>P.</forenames></author><author><keyname>Van De Ville</keyname><forenames>D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>Sampling theorems and compressive sensing on the sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>9 pages, 2 figures, Proceedings of Wavelets and Sparsity XIV, SPIE
  Optics and Photonics 2011</comments><journal-ref>Proc. SPIE 8138, Wavelets and Sparsity XIV, 81381F (2011)</journal-ref><doi>10.1117/12.893481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a novel sampling theorem on the sphere developed by McEwen &amp; Wiaux
recently through an association between the sphere and the torus. To represent
a band-limited signal exactly, this new sampling theorem requires less than
half the number of samples of other equiangular sampling theorems on the
sphere, such as the canonical Driscoll &amp; Healy sampling theorem. A reduction in
the number of samples required to represent a band-limited signal on the sphere
has important implications for compressive sensing, both in terms of the
dimensionality and sparsity of signals. We illustrate the impact of this
property with an inpainting problem on the sphere, where we show superior
reconstruction performance when adopting the new sampling theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6298</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6298</id><created>2011-10-28</created><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>A novel sampling theorem on the sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>13 pages, 5 figures, accepted for publication by IEEE Trans. Sig.
  Proc.; We make our Spin Spherical Harmonic Transform (SSHT) package available
  publicly from http://www.ssht.org.uk</comments><journal-ref>IEEE Trans. Signal Process. 59 (2011) 5876-5887</journal-ref><doi>10.1109/TSP.2011.2166394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel sampling theorem on the sphere and corresponding fast
algorithms by associating the sphere with the torus through a periodic
extension. The fundamental property of any sampling theorem is the number of
samples required to represent a band-limited signal. To represent exactly a
signal on the sphere band-limited at L, all sampling theorems on the sphere
require O(L^2) samples. However, our sampling theorem requires less than half
the number of samples of other equiangular sampling theorems on the sphere and
an asymptotically identical, but smaller, number of samples than the
Gauss-Legendre sampling theorem. The complexity of our algorithms scale as
O(L^3), however, the continual use of fast Fourier transforms reduces the
constant prefactor associated with the asymptotic scaling considerably,
resulting in algorithms that are fast. Furthermore, we do not require any
precomputation and our algorithms apply to both scalar and spin functions on
the sphere without any change in computational complexity or computation time.
We make our implementation of these algorithms available publicly and perform
numerical experiments demonstrating their speed and accuracy up to very high
band-limits. Finally, we highlight the advantages of our sampling theorem in
the context of potential applications, notably in the field of compressive
sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6317</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6317</id><created>2011-10-28</created><updated>2014-01-23</updated><authors><author><keyname>Shen</keyname><forenames>Yun</forenames></author><author><keyname>Stannat</keyname><forenames>Wilhelm</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Risk-sensitive Markov control processes</title><categories>math.OC cs.CE math.DS stat.ML</categories><comments>21 pages</comments><msc-class>60J05, 93E20, 93C55, 47H07, 91B06</msc-class><journal-ref>SIAM J. Control Optim., 51(5), 3652-3672, 2013</journal-ref><doi>10.1137/120899005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general framework for measuring risk in the context of Markov
control processes with risk maps on general Borel spaces that generalize known
concepts of risk measures in mathematical finance, operations research and
behavioral economics. Within the framework, applying weighted norm spaces to
incorporate also unbounded costs, we study two types of infinite-horizon
risk-sensitive criteria, discounted total risk and average risk, and solve the
associated optimization problems by dynamic programming. For the discounted
case, we propose a new discount scheme, which is different from the
conventional form but consistent with the existing literature, while for the
average risk criterion, we state Lyapunov-like stability conditions that
generalize known conditions for Markov chains to ensure the existence of
solutions to the optimality equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6372</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6372</id><created>2011-10-28</created><updated>2012-04-23</updated><authors><author><keyname>Goyal</keyname><forenames>Sanjeev</forenames></author><author><keyname>Kearns</keyname><forenames>Michael</forenames></author></authors><title>Competitive Contagion in Networks</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a game-theoretic framework for the study of competition between
firms who have budgets to &quot;seed&quot; the initial adoption of their products by
consumers located in a social network. The payoffs to the firms are the
eventual number of adoptions of their product through a competitive stochastic
diffusion process in the network. This framework yields a rich class of
competitive strategies, which depend in subtle ways on the stochastic dynamics
of adoption, the relative budgets of the players, and the underlying structure
of the social network.
  We identify a general property of the adoption dynamics --- namely,
decreasing returns to local adoption --- for which the inefficiency of resource
use at equilibrium (the Price of Anarchy) is uniformly bounded above, across
all networks. We also show that if this property is violated the Price of
Anarchy can be unbounded, thus yielding sharp threshold behavior for a broad
class of dynamics.
  We also introduce a new notion, the Budget Multiplier, that measures the
extent that imbalances in player budgets can be amplified at equilibrium. We
again identify a general property of the adoption dynamics --- namely,
proportional local adoption between competitors --- for which the (pure
strategy) Budget Multiplier is uniformly bounded above, across all networks. We
show that a violation of this property can lead to unbounded Budget Multiplier,
again yielding sharp threshold behavior for a broad class of dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6384</identifier>
 <datestamp>2012-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6384</id><created>2011-10-28</created><updated>2012-02-21</updated><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Backdoors to Acyclic SAT</title><categories>cs.DS cs.AI cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backdoor sets, a notion introduced by Williams et al. in 2003, are certain
sets of key variables of a CNF formula F that make it easy to solve the
formula; by assigning truth values to the variables in a backdoor set, the
formula gets reduced to one or several polynomial-time solvable formulas. More
specifically, a weak backdoor set of F is a set X of variables such that there
exits a truth assignment t to X that reduces F to a satisfiable formula F[t]
that belongs to a polynomial-time decidable base class C. A strong backdoor set
is a set X of variables such that for all assignments t to X, the reduced
formula F[t] belongs to C.
  We study the problem of finding backdoor sets of size at most k with respect
to the base class of CNF formulas with acyclic incidence graphs, taking k as
the parameter. We show that
  1. the detection of weak backdoor sets is W[2]-hard in general but
fixed-parameter tractable for r-CNF formulas, for any fixed r&gt;=3, and
  2. the detection of strong backdoor sets is fixed-parameter approximable.
  Result 1 is the the first positive one for a base class that does not have a
characterization with obstructions of bounded size. Result 2 is the first
positive one for a base class for which strong backdoor sets are more powerful
than deletion backdoor sets.
  Not only SAT, but also #SAT can be solved in polynomial time for CNF formulas
with acyclic incidence graphs. Hence Result 2 establishes a new structural
parameter that makes #SAT fixed-parameter tractable and that is incomparable
with known parameters such as treewidth and clique-width.
  We obtain the algorithms by a combination of an algorithmic version of the
Erd\&quot;os-P\'osa Theorem, Courcelle's model checking for monadic second order
logic, and new combinatorial results on how disjoint cycles can interact with
the backdoor set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6387</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6387</id><created>2011-10-28</created><updated>2011-10-31</updated><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Backdoors to Satisfaction</title><categories>cs.DS cs.AI cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A backdoor set is a set of variables of a propositional formula such that
fixing the truth values of the variables in the backdoor set moves the formula
into some polynomial-time decidable class. If we know a small backdoor set we
can reduce the question of whether the given formula is satisfiable to the same
question for one or several easy formulas that belong to the tractable class
under consideration. In this survey we review parameterized complexity results
for problems that arise in the context of backdoor sets, such as the problem of
finding a backdoor set of size at most k, parameterized by k. We also discuss
recent results on backdoor sets for problems that are beyond NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6407</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6407</id><created>2011-10-28</created><authors><author><keyname>Kleiman</keyname><forenames>Elena</forenames></author></authors><title>Packing, Scheduling and Covering Problems in a Game-Theoretic
  Perspective</title><categories>cs.GT</categories><comments>PhD thesis</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Many packing, scheduling and covering problems that were previously
considered by computer science literature in the context of various
transportation and production problems, appear also suitable for describing and
modeling various fundamental aspects in networks optimization such as routing,
resource allocation, congestion control, etc. Various combinatorial problems
were already studied from the game theoretic standpoint, and we attempt to
complement to this body of research.
  Specifically, we consider the bin packing problem both in the classic and
parametric versions, the job scheduling problem and the machine covering
problem in various machine models. We suggest new interpretations of such
problems in the context of modern networks and study these problems from a game
theoretic perspective by modeling them as games, and then concerning various
game theoretic concepts in these games by combining tools from game theory and
the traditional combinatorial optimization. In the framework of this research
we introduce and study models that were not considered before, and also improve
upon previously known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6412</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6412</id><created>2011-10-28</created><updated>2012-09-05</updated><authors><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Wille</keyname><forenames>Robert</forenames></author><author><keyname>Drechsler</keyname><forenames>Rolf</forenames></author></authors><title>Synthesis of Quantum Circuits for Linear Nearest Neighbor Architectures</title><categories>quant-ph cs.ET</categories><comments>14 pages, 11 figures, 3 tables</comments><journal-ref>Quantum Information Processing, Vol. 10, No. 3, pp. 355-377, 2011</journal-ref><doi>10.1007/s11128-010-0201-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While a couple of impressive quantum technologies have been proposed, they
have several intrinsic limitations which must be considered by circuit
designers to produce realizable circuits. Limited interaction distance between
gate qubits is one of the most common limitations. In this paper, we suggest
extensions of the existing synthesis flow aimed to realize circuits for quantum
architectures with linear nearest neighbor (LNN) interaction. To this end, a
template matching optimization, an exact synthesis approach, and two reordering
strategies are introduced. The proposed methods are combined as an integrated
synthesis flow. Experiments show that by using the suggested flow, quantum cost
can be improved by more than 50% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6426</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6426</id><created>2011-10-28</created><authors><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author></authors><title>A Distributed Power Control and Transmission Rate Allocation Algorithm
  over Multiple Channels</title><categories>math.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider multiple channels and wireless nodes with multiple
transceivers. Each node assigns one transmitter at each available channel. For
each assigned transmitter the node decides the power level and data rate of
transmission in a distributed fashion, such that certain Quality of Service
(QoS) demands for the wireless node are satisfied. More specifically, we
investigate the case in which the average SINR over all channels for each
communication pair is kept above a certain threshold. A joint distributed power
and rate control algorithm for each transmitter is proposed that dynamically
adjusts the data rate to meet a target SINR at each channel, and to update the
power levels allowing for variable desired SINRs. The algorithm is fully
distributed and requires only local interference measurements. The performance
of the proposed algorithm is shown through illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6437</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6437</id><created>2011-10-28</created><updated>2011-11-21</updated><authors><author><keyname>Armstrong</keyname><forenames>Stuart</forenames></author></authors><title>Anthropic decision theory</title><categories>physics.data-an cs.AI hep-th physics.pop-ph</categories><msc-class>62C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper sets out to solve the Sleeping Beauty problem and various related
anthropic (self-locating belief) problems, not through the calculation of
anthropic probabilities, but through finding the correct decision to make.
Given certain simple assumptions, it turns out to be possible to do so without
knowing the underlying anthropic probabilities. Most common anthropic problems
are underspecified from the decision perspective, and this can explain some of
the differing intuitions in the subject: selfless and selfish agents, total and
average utilitarians, will all reach different decisions in the same problem.
These results are formalised into an anthropic decision theory, that is then
used to solve many anthropic problems and paradoxes, such as the Presumptuous
Philosopher, Adam and Eve, and Doomsday problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6473</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6473</id><created>2011-10-28</created><updated>2012-10-30</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Jansens</keyname><forenames>Dana</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Making triangulations 4-connected using flips</title><categories>cs.CG</categories><comments>22 pages, 8 figures. Accepted to CGTA special issue for CCCG 2011.
  Conference version available at
  http://2011.cccg.ca/PDFschedule/papers/paper34.pdf</comments><journal-ref>Computational Geometry: Theory and Applications, 47(2A):187-197,
  2014. Special issue for CCCG 2011</journal-ref><doi>10.1016/j.comgeo.2012.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any combinatorial triangulation on n vertices can be transformed
into a 4-connected one using at most floor((3n - 9)/5) edge flips. We also give
an example of an infinite family of triangulations that requires this many
flips to be made 4-connected, showing that our bound is tight. In addition, for
n &gt;= 19, we improve the upper bound on the number of flips required to
transform any 4-connected triangulation into the canonical triangulation (the
triangulation with two dominant vertices), matching the known lower bound of 2n
- 15. Our results imply a new upper bound on the diameter of the flip graph of
5.2n - 33.6, improving on the previous best known bound of 6n - 30.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6476</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6476</id><created>2011-10-28</created><authors><author><keyname>Chou</keyname><forenames>Tzu-Han</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Sayeed</keyname><forenames>Akbar M.</forenames></author></authors><title>Key Generation Using External Source Excitation: Capacity, Reliability,
  and Secrecy Exponent</title><categories>cs.IT math.IT</categories><comments>accepted for publication, IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental limits to secret key generation from an excited
distributed source (EDS). In an EDS a pair of terminals observe dependent
sources of randomness excited by a pre-arranged signal. We first determine the
secret key capacity for such systems with one-way public messaging. We then
characterize a tradeoff between the secret key rate and exponential bounds on
the probability of key agreement failure and on the secrecy of the key
generated. We find that there is a fundamental tradeoff between reliability and
secrecy.
  We then explore this framework within the context of reciprocal wireless
channels. In this setting, the users transmit pre-arranged excitation signals
to each other. When the fading is Rayleigh, the observations of the users are
jointly Gaussian sources. We show that an on-off excitation signal with an
SNR-dependent duty cycle achieves the secret key capacity of this system.
Furthermore, we characterize a fundamental metric -- minimum energy per key bit
for reliable key generation -- and show that in contrast to conventional AWGN
channels, there is a non-zero threshold SNR that achieves the minimum energy
per key bit. The capacity achieving on-off excitation signal achieves the
minimum energy per key bit at any SNR below the threshold. Finally, we build
off our error exponent results to investigate the energy required to generate a
key using a finite block length. Again we find that on-off excitation signals
yield an improvement when compared to constant excitation signals. In addition
to Rayleigh fading, we analyze the performance of a system based on binary
channel phase quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6483</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6483</id><created>2011-10-28</created><updated>2011-11-08</updated><authors><author><keyname>Popescu-Bodorin</keyname><forenames>N.</forenames></author><author><keyname>Balas</keyname><forenames>V. E.</forenames></author><author><keyname>Motoc</keyname><forenames>I. M.</forenames></author></authors><title>Iris Codes Classification Using Discriminant and Witness Directions</title><categories>cs.NE cs.AI cs.CV</categories><comments>6 pages, 5 figures, Proc. 5th IEEE Int. Symp. on Computational
  Intelligence and Intelligent Informatics (Floriana, Malta, September 15-17),
  ISBN: 978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print)</comments><msc-class>97R40, 68T10, 62H30</msc-class><acm-class>I.5.1; I.5.2; I.5.4</acm-class><journal-ref>Proc. 5th IEEE Int. Symp. on Computational Intelligence and
  Intelligent Informatics, pp. 143-148, 2011</journal-ref><doi>10.1109/ISCIII.2011.6069760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main topic discussed in this paper is how to use intelligence for
biometric decision defuzzification. A neural training model is proposed and
tested here as a possible solution for dealing with natural fuzzification that
appears between the intra- and inter-class distribution of scores computed
during iris recognition tests. It is shown here that the use of proposed neural
network support leads to an improvement in the artificial perception of the
separation between the intra- and inter-class score distributions by moving
them away from each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6487</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6487</id><created>2011-10-28</created><updated>2012-12-21</updated><authors><author><keyname>Mohajer</keyname><forenames>Soheil</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Feedback Capacity of the Fully Connected $K$-User Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>20 pages, 4 figures, to appear in IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The symmetric K user interference channel with fully connected topology is
considered, in which (a) each receiver suffers interference from all other
(K-1) transmitters, and (b) each transmitter has causal and noiseless feedback
from its respective receiver. The number of generalized degrees of freedom
(GDoF) is characterized in terms of \alpha, where the interference-to-noise
ratio (INR) is given by INR=SNR^\alpha. It is shown that the per-user GDoF of
this network is the same as that of the 2-user interference channel with
feedback, except for \alpha=1, for which existence of feedback does not help in
terms of GDoF. The coding scheme proposed for this network, termed cooperative
interference alignment, is based on two key ingredients, namely, interference
alignment and interference decoding. Moreover, an approximate characterization
is provided for the symmetric feedback capacity of the network, when the SNR
and INR are far apart from each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6519</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6519</id><created>2011-10-29</created><authors><author><keyname>Vincelli</keyname><forenames>Maria</forenames></author></authors><title>Un modello di struttura dinamica per ebook scolastici</title><categories>cs.DL</categories><comments>12 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a model of e-books for schools based on a graph in
which nodes represent individual subjects of a teaching program to a the
relatively low granularity, which facilitates their aggregation and
re-usability, and edges represent prerequisites between subjects (and, indeed,
between nodes). On this graph we will develop a series of simple algorithms
that allow both teachers and students to assemble an interactive and
personalized ebook that, respecting the prerequisites, it will be significant
from the point of methodological and stylistic sense. Therefore, teachers and
students do not have available a set of unrelated units neither a limited set a
few pre-packaged learning paths, as it is typical of some solutions on the Web,
but rather will have a network of topics that can be serialized in a
combinatorial vast number of alternatives, and therefore can create as many
custom ebook, but guaranteed from the point of view of the scientific
perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6544</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6544</id><created>2011-10-29</created><updated>2011-11-24</updated><authors><author><keyname>Asaduzzaman</keyname><forenames>Md</forenames></author><author><keyname>Chaussalet</keyname><forenames>Thierry J</forenames></author></authors><title>A Generalized Loss Network Model with Overflow for Capacity Planning of
  a Perinatal Network</title><categories>cs.PF</categories><comments>21 pages, 4 figures, 2 tables</comments><msc-class>90B22, 68M20, 60K25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a generalized loss network framework for capacity planning of a
perinatal network in the UK. Decomposing the network by hospitals, each unit is
analyzed with a GI/G/c/0 overflow loss network model. A two-moment
approximation is performed to obtain the steady state solution of the GI/G/c/0
loss systems, and expressions for rejection probability and overflow
probability have been derived. Using the model framework, the number of
required cots can be estimated based on the rejection probability at each level
of care of the neonatal units in a network. The generalization ensures that the
model can be applied to any perinatal network for renewal arrival and discharge
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6573</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6573</id><created>2011-10-29</created><authors><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Mor</keyname><forenames>Tal</forenames></author></authors><title>On the Security of Interferometric Quantum Key Distribution</title><categories>quant-ph cs.CR</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photonic quantum key distribution is commonly implemented using
interferometers, devices that inherently cause the addition of vacuum ancillas,
thus enlarging the quantum space in use. This enlargement sometimes exposes the
implemented protocol to new kinds of attacks that have not yet been analyzed.
  We consider several quantum key distribution implementations that use
interferometers, and analyze the enlargement of the quantum space caused by the
interferometers. While we prove that some interferometric implementations are
robust (against simple attacks), we also show that several other
implementations used in QKD experiments are totally insecure.
  This result is somewhat surprising since although we assume ideal devices and
an underlying protocol which is proven secure (e.g., the Bennett-Brassard QKD),
the realization is insecure. Our novel attack demonstrates the risks of using
practical realizations without performing an extensive security analysis
regarding the specific setup in use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6589</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6589</id><created>2011-10-30</created><authors><author><keyname>Mishra</keyname><forenames>Amit K.</forenames></author><author><keyname>Baker</keyname><forenames>Chris</forenames></author></authors><title>A cognitive diversity framework for radar target classification</title><categories>cs.AI</categories><report-no>The IET COGnitive systems with Interactive Sensors 2010</report-no><journal-ref>The IET COGnitive systems with Interactive Sensors 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification of targets by radar has proved to be notoriously difficult
with the best systems still yet to attain sufficiently high levels of
performance and reliability. In the current contribution we explore a new
design of radar based target recognition, where angular diversity is used in a
cognitive manner to attain better performance. Performance is bench- marked
against conventional classification schemes. The proposed scheme can easily be
extended to cognitive target recognition based on multiple diversity
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6590</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6590</id><created>2011-10-30</created><authors><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>New constructions of WOM codes using the Wozencraft ensemble</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give several new constructions of WOM codes. The novelty in
our constructions is the use of the so called Wozencraft ensemble of linear
codes. Specifically, we obtain the following results.
  We give an explicit construction of a two-write Write-Once-Memory (WOM for
short) code that approaches capacity, over the binary alphabet. More formally,
for every \epsilon&gt;0, 0&lt;p&lt;1 and n =(1/\epsilon)^{O(1/p\epsilon)} we give a
construction of a two-write WOM code of length n and capacity
H(p)+1-p-\epsilon. Since the capacity of a two-write WOM code is max_p
(H(p)+1-p), we get a code that is \epsilon-close to capacity. Furthermore,
encoding and decoding can be done in time O(n^2.poly(log n)) and time
O(n.poly(log n)), respectively, and in logarithmic space.
  We obtain a new encoding scheme for 3-write WOM codes over the binary
alphabet. Our scheme achieves rate 1.809-\epsilon, when the block length is
exp(1/\epsilon). This gives a better rate than what could be achieved using
previous techniques.
  We highlight a connection to linear seeded extractors for bit-fixing sources.
In particular we show that obtaining such an extractor with seed length O(log
n) can lead to improved parameters for 2-write WOM codes. We then give an
application of existing constructions of extractors to the problem of designing
encoding schemes for memory with defects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6591</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6591</id><created>2011-10-30</created><authors><author><keyname>Csorgo</keyname><forenames>Piroska</forenames></author><author><keyname>Shcherbacov</keyname><forenames>Victor</forenames></author></authors><title>On some quasigroup cryptographical primitives</title><categories>math.GR cs.CR cs.IT math.IT</categories><comments>11 pages</comments><msc-class>20N05, 20N15, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose modifications of known quasigroup based stream ciphers. Systems of
orthogonal n-ary groupoids are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6600</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6600</id><created>2011-10-30</created><updated>2013-11-27</updated><authors><author><keyname>Sitters</keyname><forenames>Rene</forenames></author></authors><title>The generalized work function algorithm is competitive for the
  generalized 2-server problem</title><categories>cs.DS</categories><msc-class>68W27, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized 2-server problem is an online optimization problem where a
sequence of requests has to be served at minimal cost. Requests arrive one by
one and need to be served instantly by at least one of two servers. We consider
the general model where the cost function of the two servers may be different.
Formally, each server moves in its own metric space and a request consists of
one point in each metric space. It is served by moving one of the two servers
to its request point. Requests have to be served without knowledge of the
future requests. The objective is to minimize the total traveled distance. The
special case where both servers move on the real line is known as the
CNN-problem. We show that the generalized work function algorithm is constant
competitive for the generalized 2-server problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6645</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6645</id><created>2011-10-30</created><authors><author><keyname>Sutherland</keyname><forenames>Alex</forenames></author><author><keyname>Sutherland</keyname><forenames>Andrew</forenames></author></authors><title>The mathematics of Spinpossible</title><categories>math.CO cs.DM</categories><comments>12 pages, preliminary set of notes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Notes on the Spinpossible puzzle game. We give a mathematical description of
the game, prove some elementary bounds on the length of optimal solutions, and
consider variations of the game which place restrictions on the set of
permitted moves. We conclude with a list of open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6647</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6647</id><created>2011-10-30</created><authors><author><keyname>Pavlo</keyname><forenames>Andrew</forenames></author><author><keyname>Jones</keyname><forenames>Evan P. C.</forenames></author><author><keyname>Zdonik</keyname><forenames>Stanley</forenames></author></authors><title>On Predictive Modeling for Optimizing Transaction Execution in Parallel
  OLTP Systems</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  85-96 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new emerging class of parallel database management systems (DBMS) is
designed to take advantage of the partitionable workloads of on-line
transaction processing (OLTP) applications. Transactions in these systems are
optimized to execute to completion on a single node in a shared-nothing cluster
without needing to coordinate with other nodes or use expensive concurrency
control measures. But some OLTP applications cannot be partitioned such that
all of their transactions execute within a single-partition in this manner.
These distributed transactions access data not stored within their local
partitions and subsequently require more heavy-weight concurrency control
protocols. Further difficulties arise when the transaction's execution
properties, such as the number of partitions it may need to access or whether
it will abort, are not known beforehand. The DBMS could mitigate these
performance issues if it is provided with additional information about
transactions. Thus, in this paper we present a Markov model-based approach for
automatically selecting which optimizations a DBMS could use, namely (1) more
efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced
undo logging, and (4) speculative execution. To evaluate our techniques, we
implemented our models and integrated them into a parallel, main-memory OLTP
DBMS to show that we can improve the performance of applications with diverse
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6648</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6648</id><created>2011-10-30</created><authors><author><keyname>Goasdou&#xe9;</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Karanasos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Leblay</keyname><forenames>Julien</forenames></author><author><keyname>Manolescu</keyname><forenames>Ioana</forenames></author></authors><title>View Selection in Semantic Web Databases</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  97-108 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting of a Semantic Web database, containing both explicit
data encoded in RDF triples, and implicit data, implied by the RDF semantics.
Based on a query workload, we address the problem of selecting a set of views
to be materialized in the database, minimizing a combination of query
processing, view storage, and view maintenance costs. Starting from an existing
relational view selection method, we devise new algorithms for recommending
view sets, and show that they scale significantly beyond the existing
relational ones when adapted to the RDF context. To account for implicit
triples in query answers, we propose a novel RDF query reformulation algorithm
and an innovative way of incorporating it into view selection in order to avoid
a combinatorial explosion in the complexity of the selection process. The
interest of our techniques is demonstrated through a set of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6649</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6649</id><created>2011-10-30</created><authors><author><keyname>Jestes</keyname><forenames>Jeffrey</forenames></author><author><keyname>Yi</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Feifei</forenames></author></authors><title>Building Wavelet Histograms on Large Data in MapReduce</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  109-120 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce is becoming the de facto framework for storing and processing
massive data, due to its excellent scalability, reliability, and elasticity. In
many MapReduce applications, obtaining a compact accurate summary of data is
essential. Among various data summarization tools, histograms have proven to be
particularly important and useful for summarizing data, and the wavelet
histogram is one of the most widely used histograms. In this paper, we
investigate the problem of building wavelet histograms efficiently on large
datasets in MapReduce. We measure the efficiency of the algorithms by both
end-to-end running time and communication cost. We demonstrate straightforward
adaptations of existing exact and approximate methods for building wavelet
histograms to MapReduce clusters are highly inefficient. To that end, we design
new algorithms for computing exact and approximate wavelet histograms and
discuss their implementation in MapReduce. We illustrate our techniques in
Hadoop, and compare to baseline solutions with extensive experiments performed
in a heterogeneous Hadoop cluster of 16 nodes, using large real and synthetic
datasets, up to hundreds of gigabytes. The results suggest significant (often
orders of magnitude) performance improvement achieved by our new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6650</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6650</id><created>2011-10-30</created><authors><author><keyname>Yang</keyname><forenames>Di</forenames></author><author><keyname>Rundensteiner</keyname><forenames>Elke A.</forenames></author><author><keyname>Ward</keyname><forenames>Matthew O.</forenames></author></authors><title>Summarization and Matching of Density-Based Clusters in Streaming
  Environments</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  121-132 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Density-based cluster mining is known to serve a broad range of applications
ranging from stock trade analysis to moving object monitoring. Although methods
for efficient extraction of density-based clusters have been studied in the
literature, the problem of summarizing and matching of such clusters with
arbitrary shapes and complex cluster structures remains unsolved. Therefore,
the goal of our work is to extend the state-of-art of density-based cluster
mining in streams from cluster extraction only to now also support analysis and
management of the extracted clusters. Our work solves three major technical
challenges. First, we propose a novel multi-resolution cluster summarization
method, called Skeletal Grid Summarization (SGS), which captures the key
features of density-based clusters, covering both their external shape and
internal cluster structures. Second, in order to summarize the extracted
clusters in real-time, we present an integrated computation strategy C-SGS,
which piggybacks the generation of cluster summarizations within the online
clustering process. Lastly, we design a mechanism to efficiently execute
cluster matching queries, which identify similar clusters for given cluster of
analyst's interest from clusters extracted earlier in the stream history. Our
experimental study using real streaming data shows the clear superiority of our
proposed methods in both efficiency and effectiveness for cluster summarization
and cluster matching queries to other potential alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6651</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6651</id><created>2011-10-30</created><authors><author><keyname>Nguyen</keyname><forenames>Thanh</forenames></author><author><keyname>Moreira</keyname><forenames>Viviane</forenames></author><author><keyname>Nguyen</keyname><forenames>Huong</forenames></author><author><keyname>Nguyen</keyname><forenames>Hoa</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author></authors><title>Multilingual Schema Matching for Wikipedia Infoboxes</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  133-144 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has taken advantage of Wikipedia's multilingualism as a
resource for cross-language information retrieval and machine translation, as
well as proposed techniques for enriching its cross-language structure. The
availability of documents in multiple languages also opens up new opportunities
for querying structured Wikipedia content, and in particular, to enable answers
that straddle different languages. As a step towards supporting such queries,
in this paper, we propose a method for identifying mappings between attributes
from infoboxes that come from pages in different languages. Our approach finds
mappings in a completely automated fashion. Because it does not require
training data, it is scalable: not only can it be used to find mappings between
many language pairs, but it is also effective for languages that are
under-represented and lack sufficient training samples. Another important
benefit of our approach is that it does not depend on syntactic similarity
between attribute names, and thus, it can be applied to language pairs that
have distinct morphologies. We have performed an extensive experimental
evaluation using a corpus consisting of pages in Portuguese, Vietnamese, and
English. The results show that not only does our approach obtain high precision
and recall, but it also outperforms state-of-the-art techniques. We also
present a case study which demonstrates that the multilingual mappings we
derive lead to substantial improvements in answer quality and coverage for
structured queries over Wikipedia content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6652</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6652</id><created>2011-10-30</created><authors><author><keyname>Liu</keyname><forenames>Guimei</forenames></author><author><keyname>Zhang</keyname><forenames>Haojun</forenames></author><author><keyname>Wong</keyname><forenames>Limsoon</forenames></author></authors><title>Controlling False Positives in Association Rule Mining</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 2, pp.
  145-156 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association rule mining is an important problem in the data mining area. It
enumerates and tests a large number of rules on a dataset and outputs rules
that satisfy user-specified constraints. Due to the large number of rules being
tested, rules that do not represent real systematic effect in the data can
satisfy the given constraints purely by random chance. Hence association rule
mining often suffers from a high risk of false positive errors. There is a lack
of comprehensive study on controlling false positives in association rule
mining. In this paper, we adopt three multiple testing correction
approaches---the direct adjustment approach, the permutation-based approach and
the holdout approach---to control false positives in association rule mining,
and conduct extensive experiments to study their performance. Our results show
that (1) Numerous spurious rules are generated if no correction is made. (2)
The three approaches can control false positives effectively. Among the three
approaches, the permutation-based approach has the highest power of detecting
real association rules, but it is very computationally expensive. We employ
several techniques to reduce its cost effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6654</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6654</id><created>2011-10-30</created><updated>2012-04-30</updated><authors><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Pointwise Relations between Information and Estimation in Gaussian Noise</title><categories>cs.IT math.IT</categories><comments>31 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the classical and recent relations between information and estimation
in the presence of Gaussian noise can be viewed as identities between
expectations of random quantities. These include the I-MMSE relationship of Guo
et al.; the relative entropy and mismatched estimation relationship of
Verd\'{u}; the relationship between causal estimation and mutual information of
Duncan, and its extension to the presence of feedback by Kadota et al.; the
relationship between causal and non-casual estimation of Guo et al., and its
mismatched version of Weissman. We dispense with the expectations and explore
the nature of the pointwise relations between the respective random quantities.
The pointwise relations that we find are as succinctly stated as - and give
considerable insight into - the original expectation identities.
  As an illustration of our results, consider Duncan's 1970 discovery that the
mutual information is equal to the causal MMSE in the AWGN channel, which can
equivalently be expressed saying that the difference between the input-output
information density and half the causal estimation error is a zero mean random
variable (regardless of the distribution of the channel input). We characterize
this random variable explicitly, rather than merely its expectation. Classical
estimation and information theoretic quantities emerge with new and surprising
roles. For example, the variance of this random variable turns out to be given
by the causal MMSE (which, in turn, is equal to the mutual information by
Duncan's result).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6685</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6685</id><created>2011-10-30</created><authors><author><keyname>Geuvers</keyname><forenames>Herman</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>Proceedings Sixth International Workshop on Logical Frameworks and
  Meta-languages: Theory and Practice</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 71, 2011</journal-ref><doi>10.4204/EPTCS.71</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume constitutes the proceedings of LFMTP 2011, the Sixth
International Workshop on Logical Frameworks and Meta-languages: Theory and
Practice. The LFMTP workshop series brings together designers, implementors,
and practitioners to discuss varied aspects of the structure of logical
frameworks and meta-languages that impinge on their use in representing,
implementing, and reasoning about a wide variety of deductive systems of
interest in logic and computer science. LFMTP 2011 was held on August 26, 2011
in Nijmegen, Netherlands, as a workshop associated with ITP 2011, the Second
International Conference on Interactive Theorem Proving. Its program consisted
of contributed and invited presentations and was integrated with that of MLPA
11, the Third Workshop on Modules and Libraries for Proof Assistants. This
proceedings contains only the contributed papers that were accepted for
presentation at the workshop. Each of these papers was accepted based on the
reviews of three members of the program committee. Authors were subsequently
given the opportunity to revise their submissions based on the comments
provided by the reviewers and the feedback obtained during their presentations
at the workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6698</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6698</id><created>2011-10-31</created><authors><author><keyname>Ali</keyname><forenames>Mortuza</forenames></author><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author></authors><title>An algebraic approach to source coding with side information using list
  decoding</title><categories>cs.IT math.IT</categories><comments>14 Pages, 7 Figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing literature on source coding with side information (SCSI) mostly uses
the state-of-the-art channel codes namely LDPC codes, turbo codes, and their
variants and assume classical unique decoding. In this paper, we present an
algebraic approach to SCSI based on the list decoding of the underlying channel
codes. We show that the theoretical limit of SCSI can be achieved in the
proposed list decoding based framework when the correlation between the source
and side information is $q$-ary symmetric. We argue that, as opposed to channel
coding, the correct sequence from the list produced by the list decoder can
effectively be recovered in case of SCSI with a few CRC symbols. The CRC
symbols, which allow the decoder to identify the correct sequence, incur
negligible overhead for large block lengths. More importantly, these CRC
symbols are not subject to noise since we are dealing with a virtual noisy
channel rather than a real noisy channel. Finally, we present a guideline for
designing constructive SCSI schemes for non-binary and binary sources using
Reed Solomon codes and BCH codes, respectively. This guideline allows us to
design a SCSI scheme for any arbitrary $q$-ary symmetric correlation without
resorting to simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6738</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6738</id><created>2011-10-31</created><updated>2011-11-16</updated><authors><author><keyname>Raut</keyname><forenames>Manoj K.</forenames></author></authors><title>An Incremental Knowledge Compilation in First Order Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm to compute the set of prime implicates of a quantifier-free
clausal formula X in first order logic had been presented in earlier work. As
the knowledge base X is dynamic, new clauses are added to the old knowledge
base. In this paper an incremental algorithm is presented to compute the prime
implicates of X and a clause C from $\pi(X)\cup C$. The correctness of the
algorithm is also proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6739</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6739</id><created>2011-10-31</created><updated>2012-06-28</updated><authors><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>Braghin</keyname><forenames>Chiara</forenames></author><author><keyname>Dondi</keyname><forenames>Riccardo</forenames></author><author><keyname>Trucco</keyname><forenames>Gabriella</forenames></author></authors><title>The Binary Perfect Phylogeny with Persistent characters</title><categories>cs.DS cs.CE</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary perfect phylogeny model is too restrictive to model biological
events such as back mutations. In this paper we consider a natural
generalization of the model that allows a special type of back mutation. We
investigate the problem of reconstructing a near perfect phylogeny over a
binary set of characters where characters are persistent: characters can be
gained and lost at most once. Based on this notion, we define the problem of
the Persistent Perfect Phylogeny (referred as P-PP). We restate the P-PP
problem as a special case of the Incomplete Directed Perfect Phylogeny, called
Incomplete Perfect Phylogeny with Persistent Completion, (refereed as IP-PP),
where the instance is an incomplete binary matrix M having some missing
entries, denoted by symbol ?, that must be determined (or completed) as 0 or 1
so that M admits a binary perfect phylogeny. We show that the IP-PP problem can
be reduced to a problem over an edge colored graph since the completion of each
column of the input matrix can be represented by a graph operation. Based on
this graph formulation, we develop an exact algorithm for solving the P-PP
problem that is exponential in the number of characters and polynomial in the
number of species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6745</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6745</id><created>2011-10-31</created><updated>2012-01-05</updated><authors><author><keyname>Kelling</keyname><forenames>Jeffrey</forenames></author><author><keyname>&#xd3;dor</keyname><forenames>G&#xe9;za</forenames></author></authors><title>Extremely large scale simulation of a Kardar-Parisi-Zhang model using
  graphics cards</title><categories>cond-mat.stat-mech cond-mat.mtrl-sci cs.DC nlin.CG physics.comp-ph</categories><comments>7 pages, 8 figures, slightly modified, accepted version for PRE</comments><journal-ref>Phys. Rev. E 84, 061150 (2011)</journal-ref><doi>10.1103/PhysRevE.84.061150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The octahedron model introduced recently has been implemented onto graphics
cards, which permits extremely large scale simulations via binary lattice gases
and bit coded algorithms. We confirm scaling behaviour belonging to the 2d
Kardar-Parisi-Zhang universality class and find a surface growth exponent:
beta=0.2415(15) on 2^17 x 2^17 systems, ruling out beta=1/4 suggested by field
theory. The maximum speed-up with respect to a single CPU is 240. The steady
state has been analysed by finite size scaling and a growth exponent
alpha=0.393(4) is found. Correction to scaling exponents are computed and the
power-spectrum density of the steady state is determined. We calculate the
universal scaling functions, cumulants and show that the limit distribution can
be obtained by the sizes considered. We provide numerical fitting for the small
and large tail behaviour of the steady state scaling function of the interface
width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6755</identifier>
 <datestamp>2012-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6755</id><created>2011-10-31</created><updated>2012-01-30</updated><authors><author><keyname>Seldin</keyname><forenames>Yevgeny</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Auer</keyname><forenames>Peter</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author></authors><title>PAC-Bayes-Bernstein Inequality for Martingales and its Application to
  Multiarmed Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new tool for data-dependent analysis of the
exploration-exploitation trade-off in learning under limited feedback. Our tool
is based on two main ingredients. The first ingredient is a new concentration
inequality that makes it possible to control the concentration of weighted
averages of multiple (possibly uncountably many) simultaneously evolving and
interdependent martingales. The second ingredient is an application of this
inequality to the exploration-exploitation trade-off via importance weighted
sampling. We apply the new tool to the stochastic multiarmed bandit problem,
however, the main importance of this paper is the development and understanding
of the new tool rather than improvement of existing algorithms for stochastic
multiarmed bandits. In the follow-up work we demonstrate that the new tool can
improve over state-of-the-art in structurally richer problems, such as
stochastic multiarmed bandits with side information (Seldin et al., 2011a).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6778</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6778</id><created>2011-10-31</created><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Towards Optimal CSI Allocation in Multicell MIMO Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the joint precoding across K transmitters (TXs),
sharing the knowledge of the user's data symbols to be transmitted towards K
single-antenna receivers (RXs). We consider a distributed channel state
information (DCSI) configuration where each TX has its own local estimate of
the overall multiuser MIMO channel. The focus of this work is on the
optimization of the allocation of the CSI feedback subject to a constraint on
the total sharing through the backhaul network. Building upon the Wyner model,
we derive a new approach to allocate the CSI feedback while making efficient
use of the pathloss structure to reduce the amount of feedback necessary. We
show that the proposed CSI allocation achieves good performance with only a
number of CSI bits per TX which does not scale with the number of cooperating
TXs, thus making the joint transmission from a large number of TXs more
practical than previously thought. Indeed, the proposed CSI allocation reduces
the cooperation to a local scale, which allows also for a reduced allocation of
the user's data symbols. We further show that the approach can be extended to a
more general class of channel: the exponentially decaying channels, which model
accuratly the cooperation of TXs located on a one dimensional space. Finally,
we verify by simulations that the proposed CSI allocation leads to very little
performance losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6787</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6787</id><created>2011-10-31</created><authors><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author></authors><title>Spatially Coupled Repeat-Accumulate Codes</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new class of spatially coupled codes based on
repeat-accumulate protographs. We show that spatially coupled repeat-accumulate
codes have several advantages over spatially coupled low-density parity-check
codes including simpler encoders and slightly higher code rates than spatially
coupled low-density parity-check codes with similar thresholds and decoding
complexity (as measured by the Tanner graph edge density).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6822</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6822</id><created>2011-10-31</created><authors><author><keyname>Boriskina</keyname><forenames>Svetlana V.</forenames></author><author><keyname>Reinhard</keyname><forenames>Bjoern M.</forenames></author></authors><title>Spectrally and Spatially Configurable Superlenses for Optoplasmonic
  Nanocircuits</title><categories>physics.optics cs.ET math-ph math.MP physics.chem-ph</categories><comments>10 pages, 3 figures + 6 pages of Suppl. Info &amp; 5 Suppl. Figures</comments><journal-ref>Proc. Natl. Acad. Sci. USA, 108(8), 3147, 2011</journal-ref><doi>10.1073/pnas.1016181108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy transfer between photons and molecules and between neighboring
molecules is ubiquitous in living nature, most prominently in photosynthesis.
While energy transfer is efficiently utilized by living systems, its adoption
to connect individual components in man-made plasmonic nanocircuits has been
challenged by low transfer efficiencies which motivate the development of
entirely new concepts for energy transfer. We introduce herein optoplasmonic
superlenses that combine the capability of optical microcavities to insulate
molecule-photon systems from decohering environmental effects with the superior
light nanoconcentration properties of nanoantennas. The proposed structures
provide significant enhancement of the emitter radiative rate and efficient
long-range transfer of emitted photons followed by subsequent re-focusing into
nanoscale volumes accessible to near- and far-field detection. Optoplasmonic
superlenses are versatile building blocks for optoplasmonic nanocircuits and
can be used to construct &quot;dark&quot; single molecule sensors, resonant amplifiers,
nanoconcentrators, frequency multiplexers, demultiplexers, energy converters
and dynamical switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6832</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6832</id><created>2011-10-31</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Raja</keyname><forenames>Adnan</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Multicommodity Flows and Cuts in Polymatroidal Networks</title><categories>cs.DS cs.DM cs.IT math.IT</categories><comments>An extended abstract will appear in Proceedings of the Innovations in
  Theoretical Computer Science Conference (ITCS), January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multicommodity flow and cut problems in {\em polymatroidal}
networks where there are submodular capacity constraints on the edges incident
to a node. Polymatroidal networks were introduced by Lawler and Martel and
Hassin in the single-commodity setting and are closely related to the
submodular flow model of Edmonds and Giles; the well-known maxflow-mincut
theorem holds in this more general setting. Polymatroidal networks for the
multicommodity case have not, as far as the authors are aware, been previously
explored. Our work is primarily motivated by applications to information flow
in wireless networks. We also consider the notion of undirected polymatroidal
networks and observe that they provide a natural way to generalize flows and
cuts in edge and node capacitated undirected networks.
  We establish poly-logarithmic flow-cut gap results in several scenarios that
have been previously considered in the standard network flow models where
capacities are on the edges or nodes. Our results have already found
aplications in wireless network information flow and we anticipate more in the
future. On the technical side our key tools are the formulation and analysis of
the dual of the flow relaxations via continuous extensions of submodular
functions, in particular the Lov\'asz extension. For directed graphs we rely on
a simple yet useful reduction from polymatroidal networks to standard networks.
For undirected graphs we rely on the interplay between the Lov\'asz extension
of a submodular function and line embeddings with low average distortion
introduced by Matousek and Rabinovich; this connection is inspired by, and
generalizes, the work of Feige, Hajiaghayi and Lee on node-capacitated
multicommodity flows and cuts. The applicability of embeddings to polymatroidal
networks is of independent mathematical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6834</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6834</id><created>2011-10-31</created><authors><author><keyname>Kalise</keyname><forenames>Dante</forenames></author><author><keyname>Lie</keyname><forenames>Ivar</forenames></author><author><keyname>Toro</keyname><forenames>Eleuterio F.</forenames></author></authors><title>High-order finite volume schemes for layered atmospheric models</title><categories>cs.NA math.NA</categories><comments>28 pages, 10 figures</comments><msc-class>65M02, 65Y02, 65Z02, 86A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a numerical scheme for the solution of a class of atmospheric
models where high horizontal resolution is required while a coarser vertical
structure is allowed. The proposed scheme considers a layering procedure for
the original set of equations, and the use of high-order ADER finite volume
schemes for the solution of the system of balance laws arising from the
dimensional reduction procedure. We present several types of layering based
upon Galerkin discretizations of the vertical structure, and we study the
effect of incrementing the order of horizontal approximation. Numerical
experiments for the computational validation of the convergence of the scheme
together with the study of physical phenomena are performed over 2D linear
advective models, including a set of equations for an isothermal atmosphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6850</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6850</id><created>2011-10-31</created><authors><author><keyname>Kujawski</keyname><forenames>Bernard</forenames></author><author><keyname>Abell</keyname><forenames>Peter</forenames></author></authors><title>Virtual communities? the middle east revolutions at the Guardian forum:
  Comment Is Free</title><categories>physics.soc-ph cs.SI</categories><doi>10.1140/epjb/e2011-20478-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the possibility of virtual community formation in an online
social network under a rapid increase of activity of members and newcomers. The
evolution is studied of the activity of online users at the Guardian - Comment
Is Free forum - covering topics related to the Middle East turmoil during the
period of 1st of January 2010 to the 28th of March 2011. Despite a threefold
upsurge of forum users and the formation of a giant component, the main network
characteristics, i.e. degree and weight distribution and clustering
coefficient, remained almost unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6864</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6864</id><created>2011-10-31</created><authors><author><keyname>Haukkanen</keyname><forenames>Pentti</forenames></author><author><keyname>Merikoski</keyname><forenames>Jorma K.</forenames></author></authors><title>Asymptotics for numbers of line segments and lines in a square grid</title><categories>math.NT cs.IT math.CO math.IT</categories><msc-class>05A99, 11N37, 11P21</msc-class><journal-ref>International Journal of Number Theory 2012</journal-ref><doi>10.1142/S1793042112500698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an asymptotic formula for the number of line segments connecting
q+1 points of an nxn square grid, and a sharper formula, assuming the Riemann
hypothesis. We also present asymptotic formulas for the number of lines through
at least q points and, respectively, through exactly q points of the grid. The
well-known case q=2 is so generalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6865</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6865</id><created>2011-10-31</created><authors><author><keyname>Vashkevich</keyname><forenames>Maxim</forenames></author><author><keyname>Parfieniuk</keyname><forenames>Marek</forenames></author><author><keyname>Petrovsky</keyname><forenames>Alexander</forenames></author></authors><title>FPGA implementation of short critical path CORDIC-based approximation of
  the eight-point DCT</title><categories>cs.AR</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient approach for multiplierless implementation
for eight-point DCT approximation, which based on coordinate rotation digital
computer (CORDIC) algorithm. The main design objective is to make critical path
of corresponding circuits shorter and reduce the combinational delay of
proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6879</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6879</id><created>2011-10-31</created><authors><author><keyname>Guha</keyname><forenames>Pritha</forenames></author><author><keyname>Shah</keyname><forenames>Kinjal</forenames></author><author><keyname>Shukla</keyname><forenames>Shiv Shankar Prasad</forenames></author><author><keyname>Singh</keyname><forenames>Shweta</forenames></author></authors><title>Incorporating Agile with MDA Case Study: Online Polling System</title><categories>cs.SE</categories><comments>14 pages,1 Figure,1 Table</comments><msc-class>68</msc-class><journal-ref>IJSEA 2011 Volume 2, Number 4, 83-96</journal-ref><doi>10.5121/ijsea.2011.2408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays agile software development is used in greater extend but for small
organizations only, whereas MDA is suitable for large organizations but yet not
standardized. In this paper the pros and cons of Model Driven Architecture
(MDA) and Extreme programming have been discussed. As both of them have some
limitations and cannot be used in both large scale and small scale
organizations a new architecture has been proposed. In this model it is tried
to opt the advantages and important values to overcome the limitations of both
the software development procedures. In support to the proposed architecture
the implementation of it on Online Polling System has been discussed and all
the phases of software development have been explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6886</identifier>
 <datestamp>2012-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6886</id><created>2011-10-31</created><updated>2012-07-30</updated><authors><author><keyname>Seldin</keyname><forenames>Yevgeny</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Auer</keyname><forenames>Peter</forenames></author></authors><title>PAC-Bayesian Inequalities for Martingales</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a set of high-probability inequalities that control the
concentration of weighted averages of multiple (possibly uncountably many)
simultaneously evolving and interdependent martingales. Our results extend the
PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales
opening the way for its application to importance weighted sampling,
reinforcement learning, and other interactive learning domains, as well as many
other domains in probability theory and statistics, where martingales are
encountered.
  We also present a comparison inequality that bounds the expectation of a
convex function of a martingale difference sequence shifted to the [0,1]
interval by the expectation of the same function of independent Bernoulli
variables. This inequality is applied to derive a tighter analog of
Hoeffding-Azuma's inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6895</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6895</id><created>2011-10-31</created><authors><author><keyname>Karaman</keyname><forenames>Svebor</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Benois-Pineau</keyname><forenames>Jenny</forenames><affiliation>LaBRI</affiliation></author><author><keyname>M&#xe9;gret</keyname><forenames>R&#xe9;mi</forenames><affiliation>IMS</affiliation></author><author><keyname>Bugeau</keyname><forenames>Aur&#xe9;lie</forenames><affiliation>LaBRI</affiliation></author></authors><title>Multi-Layer Local Graph Words for Object Recognition</title><categories>cs.MM</categories><comments>International Conference on MultiMedia Modeling, Klagenfurt :
  Autriche (2012)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-27355-1_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new multi-layer structural approach for the task
of object based image retrieval. In our work we tackle the problem of
structural organization of local features. The structural features we propose
are nested multi-layered local graphs built upon sets of SURF feature points
with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied
on these graphs, giving birth to a Bag-of-Graph-Words representation. The
multi-layer nature of the descriptors consists in scaling from trivial Delaunay
graphs - isolated feature points - by increasing the number of nodes layer by
layer up to graphs with maximal number of nodes. For each layer of graphs its
own visual dictionary is built. The experiments conducted on the SIVAL and
Caltech-101 data sets reveal that the graph features at different layers
exhibit complementary performances on the same content and perform better than
baseline BoVW approach. The combination of all existing layers, yields
significant improvement of the object recognition performance compared to
single level approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1110.6916</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1110.6916</id><created>2011-10-31</created><authors><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Asnani</keyname><forenames>Himanshu</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Multi-Terminal Source Coding With Action Dependent Side Information</title><categories>cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-terminal source coding with a single encoder and multiple
decoders where either the encoder or the decoders can take cost constrained
actions which affect the quality of the side information present at the
decoders. For the scenario where decoders take actions, we characterize the
rate-cost trade-off region for lossless source coding, and give an
achievability scheme for lossy source coding for two decoders which is optimum
for a variety of special cases of interest. For the case where the encoder
takes actions, we characterize the rate-cost trade-off for a class of lossless
source coding scenarios with multiple decoders. Finally, we also consider
extensions to other multi-terminal source coding settings with actions, and
characterize the rate -distortion-cost tradeoff for a case of successive
refinement with actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0024</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0024</id><created>2011-10-31</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Jyoti</keyname><forenames>Divya</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>Text-Independent Speaker Recognition for Low SNR Environments with
  Encryption</title><categories>cs.CV cs.CR</categories><comments>Biometrics, Pattern Recognition, Security, Speaker Individuality,
  Text-independence, Pitch Extraction, Voice Recognition, Autocorrelation;
  Published by Foundation of Computer Science, New York, USA</comments><journal-ref>International Journal of Computer Applications 31(10):43-50, 2011</journal-ref><doi>10.5120/3864-5394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition systems are commonly designed to authenticate users at the access
control levels of a system. A number of voice recognition methods have been
developed using a pitch estimation process which are very vulnerable in low
Signal to Noise Ratio (SNR) environments thus, these programs fail to provide
the desired level of accuracy and robustness. Also, most text independent
speaker recognition programs are incapable of coping with unauthorized attempts
to gain access by tampering with the samples or reference database. The
proposed text-independent voice recognition system makes use of multilevel
cryptography to preserve data integrity while in transit or storage. Encryption
and decryption follow a transform based approach layered with pseudorandom
noise addition whereas for pitch detection, a modified version of the
autocorrelation pitch extraction algorithm is used. The experimental results
show that the proposed algorithm can decrypt the signal under test with
exponentially reducing Mean Square Error over an increasing range of SNR.
Further, it outperforms the conventional algorithms in actual identification
tasks even in noisy environments. The recognition rate thus obtained using the
proposed method is compared with other conventional methods used for speaker
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0033</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0033</id><created>2011-10-31</created><updated>2011-11-23</updated><authors><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Moon</keyname><forenames>Eunyoung</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Optimized reduction of uncertainty in bursty human dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>4 pages, 1 figure</comments><journal-ref>Phys. Rev. E 85, 016102 (2012)</journal-ref><doi>10.1103/PhysRevE.85.016102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human dynamics is known to be inhomogeneous and bursty but the detailed
understanding of the role of human factors in bursty dynamics is still lacking.
In order to investigate their role we devise an agent-based model, where an
agent in an uncertain situation tries to reduce the uncertainty by
communicating with information providers while having to wait time for
responses. Here the waiting time can be considered as cost. We show that the
optimal choice of the waiting time under uncertainty gives rise to the bursty
dynamics, characterized by the heavy-tailed distribution of optimal waiting
time. We find that in all cases the efficiency for communication is relevant to
the scaling behavior of the optimal waiting time distribution. On the other
hand the cost turns out in some cases to be irrelevant depending on the degree
of uncertainty and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0034</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0034</id><created>2011-10-31</created><updated>2012-05-12</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion Adaptation Strategies for Distributed Optimization and
  Learning over Networks</title><categories>math.OC cs.IT cs.LG cs.SI math.IT physics.soc-ph</categories><comments>34 pages, 6 figures, to appear in IEEE Transactions on Signal
  Processing, 2012</comments><doi>10.1109/TSP.2012.2198470</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an adaptive diffusion mechanism to optimize a global cost function
in a distributed manner over a network of nodes. The cost function is assumed
to consist of a collection of individual components. Diffusion adaptation
allows the nodes to cooperate and diffuse information in real-time; it also
helps alleviate the effects of stochastic gradient noise and measurement noise
through a continuous learning process. We analyze the mean-square-error
performance of the algorithm in some detail, including its transient and
steady-state behavior. We also apply the diffusion algorithm to two problems:
distributed estimation with sparse parameters and distributed localization.
Compared to well-studied incremental methods, diffusion methods do not require
the use of a cyclic path over the nodes and are robust to node and link
failure. Diffusion methods also endow networks with adaptation abilities that
enable the individual nodes to continue learning even when the cost function
changes with time. Examples involving such dynamic cost functions with moving
targets are common in the context of biological networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0039</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0039</id><created>2011-10-31</created><authors><author><keyname>Horrocks</keyname><forenames>I.</forenames></author><author><keyname>Pan</keyname><forenames>J. Z.</forenames></author><author><keyname>Stamou</keyname><forenames>G.</forenames></author><author><keyname>Stoilos</keyname><forenames>G.</forenames></author><author><keyname>Tzouvaras</keyname><forenames>V.</forenames></author></authors><title>Reasoning with Very Expressive Fuzzy Description Logics</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  273-320, 2007</journal-ref><doi>10.1613/jair.2279</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely recognized today that the management of imprecision and
vagueness will yield more intelligent and realistic knowledge-based
applications. Description Logics (DLs) are a family of knowledge representation
languages that have gained considerable attention the last decade, mainly due
to their decidability and the existence of empirically high performance of
reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to
the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms
(S), inverse roles (I), role hierarchies (H) and number restrictions (N). We
illustrate why transitive role axioms are difficult to handle in the presence
of fuzzy interpretations and how to handle them properly. Then we extend these
results by adding role hierarchies and finally number restrictions. The main
contributions of the paper are the decidability proof of the fuzzy DL languages
fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base
satisfiability problem of the fuzzy-SI and fuzzy-SHIN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0040</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0040</id><created>2011-10-31</created><authors><author><keyname>Li</keyname><forenames>C. M.</forenames></author><author><keyname>Manya</keyname><forenames>F.</forenames></author><author><keyname>Planes</keyname><forenames>J.</forenames></author></authors><title>New Inference Rules for Max-SAT</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  321-359, 2007</journal-ref><doi>10.1613/jair.2215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact Max-SAT solvers, compared with SAT solvers, apply little inference at
each node of the proof tree. Commonly used SAT inference rules like unit
propagation produce a simplified formula that preserves satisfiability but,
unfortunately, solving the Max-SAT problem for the simplified formula is not
equivalent to solving it for the original formula. In this paper, we define a
number of original inference rules that, besides being applied efficiently,
transform Max-SAT instances into equivalent Max-SAT instances which are easier
to solve. The soundness of the rules, that can be seen as refinements of unit
resolution adapted to Max-SAT, are proved in a novel and simple way via an
integer programming transformation. With the aim of finding out how powerful
the inference rules are in practice, we have developed a new Max-SAT solver,
called MaxSatz, which incorporates those rules, and performed an experimental
investigation. The results provide empirical evidence that MaxSatz is very
competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph
3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation
2006.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0041</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0041</id><created>2011-10-31</created><authors><author><keyname>Bordini</keyname><forenames>R. H.</forenames></author><author><keyname>Moreira</keyname><forenames>A. F.</forenames></author><author><keyname>Vieira</keyname><forenames>R.</forenames></author><author><keyname>Wooldridge</keyname><forenames>M.</forenames></author></authors><title>On the Formal Semantics of Speech-Act Based Communication in an
  Agent-Oriented Programming Language</title><categories>cs.AI cs.MA cs.PL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  221-267, 2007</journal-ref><doi>10.1613/jair.2221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on agent communication languages has typically taken the speech acts
paradigm as its starting point. Despite their manifest attractions, speech-act
models of communication have several serious disadvantages as a foundation for
communication in artificial agent systems. In particular, it has proved to be
extremely difficult to give a satisfactory semantics to speech-act based agent
communication languages. In part, the problem is that speech-act semantics
typically make reference to the &quot;mental states&quot; of agents (their beliefs,
desires, and intentions), and there is in general no way to attribute such
attitudes to arbitrary computational agents. In addition, agent programming
languages have only had their semantics formalised for abstract, stand-alone
versions, neglecting aspects such as communication primitives. With respect to
communication, implemented agent programming languages have tended to be rather
ad hoc. This paper addresses both of these problems, by giving semantics to
speech-act based messages received by an AgentSpeak agent. AgentSpeak is a
logic-based agent programming language which incorporates the main features of
the PRS model of reactive planning systems. The paper builds upon a structural
operational semantics to AgentSpeak that we developed in previous work. The
main contributions of this paper are as follows: an extension of our earlier
work on the theoretical foundations of AgentSpeak interpreters; a
computationally grounded semantics for (the core) performatives used in
speech-act based agent communication languages; and a well-defined extension of
AgentSpeak that supports agent communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0043</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0043</id><created>2011-10-31</created><authors><author><keyname>Faltings</keyname><forenames>B.</forenames></author><author><keyname>Jurca</keyname><forenames>R.</forenames></author></authors><title>Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 29, pages
  391-419, 2007</journal-ref><doi>10.1613/jair.2243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation mechanisms offer an effective alternative to verification
authorities for building trust in electronic markets with moral hazard. Future
clients guide their business decisions by considering the feedback from past
transactions; if truthfully exposed, cheating behavior is sanctioned and thus
becomes irrational.
  It therefore becomes important to ensure that rational clients have the right
incentives to report honestly. As an alternative to side-payment schemes that
explicitly reward truthful reports, we show that honesty can emerge as a
rational behavior when clients have a repeated presence in the market. To this
end we describe a mechanism that supports an equilibrium where truthful
feedback is obtained. Then we characterize the set of pareto-optimal equilibria
of the mechanism, and derive an upper bound on the percentage of false reports
that can be recorded by the mechanism. An important role in the existence of
this bound is played by the fact that rational clients can establish a
reputation for reporting honestly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0044</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0044</id><created>2011-10-31</created><authors><author><keyname>Domshlak</keyname><forenames>C.</forenames></author><author><keyname>Hoffmann</keyname><forenames>J.</forenames></author></authors><title>Probabilistic Planning via Heuristic Forward Search and Weighted Model
  Counting</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  565-620, 2007</journal-ref><doi>10.1613/jair.2289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for probabilistic planning with no observability.
Our algorithm, called Probabilistic-FF, extends the heuristic forward-search
machinery of Conformant-FF to problems with probabilistic uncertainty about
both the initial state and action effects. Specifically, Probabilistic-FF
combines Conformant-FFs techniques with a powerful machinery for weighted model
counting in (weighted) CNFs, serving to elegantly define both the search space
and the heuristic function. Our evaluation of Probabilistic-FF shows its fine
scalability in a range of probabilistic domains, constituting a several orders
of magnitude improvement over previous results in this area. We use a
problematic case to point out the main open issue to be addressed by further
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0045</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0045</id><created>2011-10-31</created><authors><author><keyname>Bhattacharya</keyname><forenames>I.</forenames></author><author><keyname>Getoor</keyname><forenames>L.</forenames></author></authors><title>Query-time Entity Resolution</title><categories>cs.DB cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  621-657, 2007</journal-ref><doi>10.1613/jair.2290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity resolution is the problem of reconciling database references
corresponding to the same real-world entities. Given the abundance of publicly
available databases that have unresolved entities, we motivate the problem of
query-time entity resolution quick and accurate resolution for answering
queries over such unclean databases at query-time. Since collective entity
resolution approaches --- where related references are resolved jointly ---
have been shown to be more accurate than independent attribute-based resolution
for off-line entity resolution, we focus on developing new algorithms for
collective resolution for answering entity resolution queries at query-time.
For this purpose, we first formally show that, for collective resolution,
precision and recall for individual entities follow a geometric progression as
neighbors at increasing distances are considered. Unfolding this progression
leads naturally to a two stage expand and resolve query processing strategy. In
this strategy, we first extract the related records for a query using two novel
expansion operators, and then resolve the extracted records collectively. We
then show how the same strategy can be adapted for query-time entity resolution
by identifying and resolving only those database references that are the most
helpful for processing the query. We validate our approach on two large
real-world publication databases where we show the usefulness of collective
resolution and at the same time demonstrate the need for adaptive strategies
for query processing. We then show how the same queries can be answered in
real-time using our adaptive approach while preserving the gains of collective
resolution. In addition to experiments on real datasets, we use synthetically
generated data to empirically demonstrate the validity of the performance
trends predicted by our analysis of collective entity resolution over a wide
range of structural characteristics in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0046</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0046</id><created>2011-10-31</created><authors><author><keyname>Bredin</keyname><forenames>J. L.</forenames></author><author><keyname>Duong</keyname><forenames>Q.</forenames></author><author><keyname>Parkes</keyname><forenames>D. C.</forenames></author></authors><title>Chain: A Dynamic Double Auction Framework for Matching Patient Agents</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  133-179, 2007</journal-ref><doi>10.1613/jair.2303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present and evaluate a general framework for the design of
truthful auctions for matching agents in a dynamic, two-sided market. A single
commodity, such as a resource or a task, is bought and sold by multiple buyers
and sellers that arrive and depart over time. Our algorithm, Chain, provides
the first framework that allows a truthful dynamic double auction (DA) to be
constructed from a truthful, single-period (i.e. static) double-auction rule.
The pricing and matching method of the Chain construction is unique amongst
dynamic-auction rules that adopt the same building block. We examine
experimentally the allocative efficiency of Chain when instantiated on various
single-period rules, including the canonical McAfee double-auction rule. For a
baseline we also consider non-truthful double auctions populated with
zero-intelligence plus&quot;-style learning agents. Chain-based auctions perform
well in comparison with other schemes, especially as arrival intensity falls
and agent valuations become more volatile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0048</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0048</id><created>2011-10-31</created><authors><author><keyname>Mairesse</keyname><forenames>F.</forenames></author><author><keyname>Prasad</keyname><forenames>R.</forenames></author><author><keyname>Stent</keyname><forenames>A.</forenames></author><author><keyname>Walker</keyname><forenames>M. A.</forenames></author></authors><title>Individual and Domain Adaptation in Sentence Planning for Dialogue</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 30, pages
  413-456, 2007</journal-ref><doi>10.1613/jair.2329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges in the development and deployment of spoken
dialogue systems is the design of the spoken language generation module. This
challenge arises from the need for the generator to adapt to many features of
the dialogue domain, user population, and dialogue context. A promising
approach is trainable generation, which uses general-purpose linguistic
knowledge that is automatically adapted to the features of interest, such as
the application domain, individual user, or user group. In this paper we
present and evaluate a trainable sentence planner for providing restaurant
information in the MATCH dialogue system. We show that trainable sentence
planning can produce complex information presentations whose quality is
comparable to the output of a template-based generator tuned to this domain. We
also show that our method easily supports adapting the sentence planner to
individuals, and that the individualized sentence planners generally perform
better than models trained and tested on a population of individuals. Previous
work has documented and utilized individual preferences for content selection,
but to our knowledge, these results provide the first demonstration of
individual preferences for sentence planning operations, affecting the content
order, discourse structure and sentence structure of system responses. Finally,
we evaluate the contribution of different feature sets, and show that, in our
application, n-gram features often do as well as features based on higher-level
linguistic representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0049</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0049</id><created>2011-10-31</created><authors><author><keyname>Glimm</keyname><forenames>Birte</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author><author><keyname>Lutz</keyname><forenames>Carsten</forenames></author><author><keyname>Sattler</keyname><forenames>Ulrike</forenames></author></authors><title>Conjunctive Query Answering for the Description Logic SHIQ</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  157-204, 2008</journal-ref><doi>10.1613/jair.2372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conjunctive queries play an important role as an expressive query language
for Description Logics (DLs). Although modern DLs usually provide for
transitive roles, conjunctive query answering over DL knowledge bases is only
poorly understood if transitive roles are admitted in the query. In this paper,
we consider unions of conjunctive queries over knowledge bases formulated in
the prominent DL SHIQ and allow transitive roles in both the query and the
knowledge base. We show decidability of query answering in this setting and
establish two tight complexity bounds: regarding combined complexity, we prove
that there is a deterministic algorithm for query answering that needs time
single exponential in the size of the KB and double exponential in the size of
the query, which is optimal. Regarding data complexity, we prove containment in
co-NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0051</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0051</id><created>2011-10-31</created><authors><author><keyname>Coghill</keyname><forenames>George M.</forenames></author><author><keyname>King</keyname><forenames>Ross D.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Ashwin</forenames></author></authors><title>Qualitative System Identification from Imperfect Data</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  825-877, 2008</journal-ref><doi>10.1613/jair.2374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experience in the physical sciences suggests that the only realistic means of
understanding complex systems is through the use of mathematical models.
Typically, this has come to mean the identification of quantitative models
expressed as differential equations. Quantitative modelling works best when the
structure of the model (i.e., the form of the equations) is known; and the
primary concern is one of estimating the values of the parameters in the model.
For complex biological systems, the model-structure is rarely known and the
modeler has to deal with both model-identification and parameter-estimation. In
this paper we are concerned with providing automated assistance to the first of
these problems. Specifically, we examine the identification by machine of the
structural relationships between experimentally observed variables. These
relationship will be expressed in the form of qualitative abstractions of a
quantitative model. Such qualitative models may not only provide clues to the
precise quantitative model, but also assist in understanding the essence of
that model. Our position in this paper is that background knowledge
incorporating system modelling principles can be used to constrain effectively
the set of good qualitative models. Utilising the model-identification
framework provided by Inductive Logic Programming (ILP) we present empirical
support for this position using a series of increasingly complex artificial
datasets. The results are obtained with qualitative and quantitative data
subject to varying amounts of noise and different degrees of sparsity. The
results also point to the presence of a set of qualitative states, which we
term kernel subsets, that may be necessary for a qualitative model-learner to
learn correct models. We demonstrate scalability of the method to biological
system modelling by identification of the glycolysis metabolic pathway from
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0053</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0053</id><created>2011-10-31</created><authors><author><keyname>Ryan</keyname><forenames>Malcolm Ross Kinsella</forenames></author></authors><title>Exploiting Subgraph Structure in Multi-Robot Path Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  497-542, 2008</journal-ref><doi>10.1613/jair.2408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-robot path planning is difficult due to the combinatorial explosion of
the search space with every new robot added. Complete search of the combined
state-space soon becomes intractable. In this paper we present a novel form of
abstraction that allows us to plan much more efficiently. The key to this
abstraction is the partitioning of the map into subgraphs of known structure
with entry and exit restrictions which we can represent compactly. Planning
then becomes a search in the much smaller space of subgraph configurations.
Once an abstract plan is found, it can be quickly resolved into a correct (but
possibly sub-optimal) concrete plan without the need for further search. We
prove that this technique is sound and complete and demonstrate its practical
effectiveness on a real map.
  A contending solution, prioritised planning, is also evaluated and shown to
have similar performance albeit at the cost of completeness. The two approaches
are not necessarily conflicting; we demonstrate how they can be combined into a
single algorithm which outperforms either approach alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0054</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0054</id><created>2011-10-31</created><authors><author><keyname>Ding</keyname><forenames>Yulin</forenames></author><author><keyname>Ding</keyname><forenames>Y.</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Y.</forenames></author></authors><title>CTL Model Update for System Modifications</title><categories>cs.AI cs.SE</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  113-155, 2008</journal-ref><doi>10.1613/jair.2420</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking is a promising technology, which has been applied for
verification of many hardware and software systems. In this paper, we introduce
the concept of model update towards the development of an automatic system
modification tool that extends model checking functions. We define primitive
update operations on the models of Computation Tree Logic (CTL) and formalize
the principle of minimal change for CTL model update. These primitive update
operations, together with the underlying minimal change principle, serve as the
foundation for CTL model update. Essential semantic and computational
characterizations are provided for our CTL model update approach. We then
describe a formal algorithm that implements this approach. We also illustrate
two case studies of CTL model updates for the well-known microwave oven example
and the Andrew File System 1, from which we further propose a method to
optimize the update results in complex system modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0055</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0055</id><created>2011-10-31</created><authors><author><keyname>Analyti</keyname><forenames>Anastasia</forenames></author><author><keyname>Antoniou</keyname><forenames>Grigoris</forenames></author><author><keyname>Dam&#xe1;sio</keyname><forenames>Carlos Viegas</forenames></author><author><keyname>Wagner</keyname><forenames>Gerd</forenames></author></authors><title>Extended RDF as a Semantic Foundation of Rule Markup Languages</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  37-94, 2008</journal-ref><doi>10.1613/jair.2425</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontologies and automated reasoning are the building blocks of the Semantic
Web initiative. Derivation rules can be included in an ontology to define
derived concepts, based on base concepts. For example, rules allow to define
the extension of a class or property, based on a complex relation between the
extensions of the same or other classes and properties. On the other hand, the
inclusion of negative information both in the form of negation-as-failure and
explicit negative information is also needed to enable various forms of
reasoning. In this paper, we extend RDF graphs with weak and strong negation,
as well as derivation rules. The ERDF stable model semantics of the extended
framework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive
feature of our theory, which is based on Partial Logic, is that both truth and
falsity extensions of properties and classes are considered, allowing for truth
value gaps. Our framework supports both closed-world and open-world reasoning
through the explicit representation of the particular closed-world assumptions
and the ERDF ontological categories of total properties and total classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0056</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0056</id><created>2011-10-31</created><authors><author><keyname>Gim&#xe9;nez</keyname><forenames>Omer</forenames></author><author><keyname>Jonsson</keyname><forenames>Anders</forenames></author></authors><title>The Complexity of Planning Problems With Simple Causal Graphs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  319-351, 2008</journal-ref><doi>10.1613/jair.2432</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present three new complexity results for classes of planning problems with
simple causal graphs. First, we describe a polynomial-time algorithm that uses
macros to generate plans for the class 3S of planning problems with binary
state variables and acyclic causal graphs. This implies that plan generation
may be tractable even when a planning problem has an exponentially long minimal
solution. We also prove that the problem of plan existence for planning
problems with multi-valued variables and chain causal graphs is NP-hard.
Finally, we show that plan existence for planning problems with binary state
variables and polytree causal graphs is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0059</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0059</id><created>2011-10-31</created><authors><author><keyname>Briel</keyname><forenames>Menkes Hector Louis van den</forenames></author><author><keyname>Vossen</keyname><forenames>Thomas</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Loosely Coupled Formulations for Automated Planning: An Integer
  Programming Perspective</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  217-257, 2008</journal-ref><doi>10.1613/jair.2443</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We represent planning as a set of loosely coupled network flow problems,
where each network corresponds to one of the state variables in the planning
domain. The network nodes correspond to the state variable values and the
network arcs correspond to the value transitions. The planning problem is to
find a path (a sequence of actions) in each network such that, when merged,
they constitute a feasible plan. In this paper we present a number of integer
programming formulations that model these loosely coupled networks with varying
degrees of flexibility. Since merging may introduce exponentially many ordering
constraints we implement a so-called branch-and-cut algorithm, in which these
constraints are dynamically generated and added to the formulation when needed.
Our results are very promising, they improve upon previous planning as integer
programming approaches and lay the foundation for integer programming
approaches for cost optimal planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0060</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0060</id><created>2011-10-31</created><authors><author><keyname>Terekhov</keyname><forenames>Daria</forenames></author><author><keyname>Beck</keyname><forenames>J. Christopher</forenames></author></authors><title>A Constraint Programming Approach for Solving a Queueing Control Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  123-167, 2008</journal-ref><doi>10.1613/jair.2446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a facility with front room and back room operations, it is useful to
switch workers between the rooms in order to cope with changing customer
demand. Assuming stochastic customer arrival and service times, we seek a
policy for switching workers such that the expected customer waiting time is
minimized while the expected back room staffing is sufficient to perform all
work. Three novel constraint programming models and several shaving procedures
for these models are presented. Experimental results show that a model based on
closed-form expressions together with a combination of shaving procedures is
the most efficient. This model is able to find and prove optimal solutions for
many problem instances within a reasonable run-time. Previously, the only
available approach was a heuristic algorithm. Furthermore, a hybrid method
combining the heuristic and the best constraint programming method is shown to
perform as well as the heuristic in terms of solution quality over time, while
achieving the same performance in terms of proving optimality as the pure
constraint programming model. This is the first work of which we are aware that
solves such queueing-based problems with constraint programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0062</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0062</id><created>2011-10-31</created><authors><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Spaan</keyname><forenames>Matthijs T. J.</forenames></author><author><keyname>Vlassis</keyname><forenames>Nikos</forenames></author></authors><title>Optimal and Approximate Q-value Functions for Decentralized POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  289-353, 2008</journal-ref><doi>10.1613/jair.2447</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision-theoretic planning is a popular approach to sequential decision
making problems, because it treats uncertainty in sensing and acting in a
principled way. In single-agent frameworks like MDPs and POMDPs, planning can
be carried out by resorting to Q-value functions: an optimal Q-value function
Q* is computed in a recursive manner by dynamic programming, and then an
optimal policy is extracted from Q*. In this paper we study whether similar
Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs),
and how policies can be extracted from such value functions. We define two
forms of the optimal Q-value function for Dec-POMDPs: one that gives a
normative description as the Q-value function of an optimal pure joint policy
and another one that is sequentially rational and thus gives a recipe for
computation. This computation, however, is infeasible for all but the smallest
problems. Therefore, we analyze various approximate Q-value functions that
allow for efficient computation. We describe how they relate, and we prove that
they all provide an upper bound to the optimal Q-value function Q*. Finally,
unifying some previous approaches for solving Dec-POMDPs, we describe a family
of algorithms for extracting policies from such Q-value functions, and perform
an experimental evaluation on existing test problems, including a new
firefighting benchmark problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0064</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0064</id><created>2011-10-31</created><authors><author><keyname>Barnat</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Heljanko</keyname><forenames>Keijo</forenames></author></authors><title>Proceedings 10th International Workshop on Parallel and Distributed
  Methods in verifiCation</title><categories>cs.DC cs.LO cs.SE</categories><comments>EPTCS 72, 2011</comments><proxy>EPTCS</proxy><acm-class>D.1.3; D.2.4</acm-class><doi>10.4204/EPTCS.72</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 10th International Workshop on
Parallel and Distributed Methods in verifiCation (PDMC 2011) that took place in
Snowbird, Utah, on July 14, 2011. The workshop was co-located with 23rd
International Conference on Computer Aided Verification (CAV 2011). The PDMC
workshop series covers all aspects related to the verification and analysis of
very large and complex systems using, in particular, methods and techniques
that exploit contemporary, hence parallel, hardware architectures. To celebrate
the 10th anniversary of PDMC, the workshop consisted of a half day invited
session together and a half day session of regular contributed presentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0065</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0065</id><created>2011-10-31</created><authors><author><keyname>Goldman</keyname><forenames>Claudia V.</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>Communication-Based Decomposition Mechanisms for Decentralized MDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  169-202, 2008</journal-ref><doi>10.1613/jair.2466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent planning in stochastic environments can be framed formally as a
decentralized Markov decision problem. Many real-life distributed problems that
arise in manufacturing, multi-robot coordination and information gathering
scenarios can be formalized using this framework. However, finding the optimal
solution in the general case is hard, limiting the applicability of recently
developed algorithms. This paper provides a practical approach for solving
decentralized control problems when communication among the decision makers is
possible, but costly. We develop the notion of communication-based mechanism
that allows us to decompose a decentralized MDP into multiple single-agent
problems. In this framework, referred to as decentralized semi-Markov decision
process with direct communication (Dec-SMDP-Com), agents operate separately
between communications. We show that finding an optimal mechanism is equivalent
to solving optimally a Dec-SMDP-Com. We also provide a heuristic search
algorithm that converges on the optimal decomposition. Restricting the
decomposition to some specific types of local behaviors reduces significantly
the complexity of planning. In particular, we present a polynomial-time
algorithm for the case in which individual agents perform goal-oriented
behaviors between communications. The paper concludes with an additional
tractable algorithm that enables the introduction of human knowledge, thereby
reducing the overall problem to finding the best time to communicate. Empirical
results show that these approaches provide good approximate solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0067</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0067</id><created>2011-10-31</created><authors><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Culberson</keyname><forenames>Joseph</forenames></author><author><keyname>Holte</keyname><forenames>Robert</forenames></author><author><keyname>Zahavi</keyname><forenames>Uzi</forenames></author><author><keyname>Felner</keyname><forenames>Ariel</forenames></author></authors><title>A General Theory of Additive State Space Abstractions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  631-662, 2008</journal-ref><doi>10.1613/jair.2486</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Informally, a set of abstractions of a state space S is additive if the
distance between any two states in S is always greater than or equal to the sum
of the corresponding distances in the abstract spaces. The first known additive
abstractions, called disjoint pattern databases, were experimentally
demonstrated to produce state of the art performance on certain state spaces.
However, previous applications were restricted to state spaces with special
properties, which precludes disjoint pattern databases from being defined for
several commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake
puzzle. In this paper we give a general definition of additive abstractions
that can be applied to any state space and prove that heuristics based on
additive abstractions are consistent as well as admissible. We use this new
definition to create additive abstractions for these testbeds and show
experimentally that well chosen additive abstractions can reduce search time
substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude
over state of the art methods for the 17-Pancake puzzle. We also derive a way
of testing if the heuristic value returned by additive abstractions is provably
too low and show that the use of this test can reduce search time for the
15-puzzle and TopSpin by roughly a factor of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0068</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0068</id><created>2011-10-31</created><authors><author><keyname>Wang</keyname><forenames>Chenggang</forenames></author><author><keyname>Joshi</keyname><forenames>Saket</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author></authors><title>First Order Decision Diagrams for Relational MDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 31, pages
  431-472, 2008</journal-ref><doi>10.1613/jair.2489</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes capture sequential decision making under
uncertainty, where an agent must choose actions so as to optimize long term
reward. The paper studies efficient reasoning mechanisms for Relational Markov
Decision Processes (RMDP) where world states have an internal relational
structure that can be naturally described in terms of objects and relations
among them. Two contributions are presented. First, the paper develops First
Order Decision Diagrams (FODD), a new compact representation for functions over
relational structures, together with a set of operators to combine FODDs, and
novel reduction techniques to keep the representation small. Second, the paper
shows how FODDs can be used to develop solutions for RMDPs, where reasoning is
performed at the abstract level and the resulting optimal policy is independent
of domain size (number of objects) or instantiation. In particular, a variant
of the value iteration algorithm is developed by using special operations over
FODDs, and the algorithm is shown to converge to the optimal policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0073</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0073</id><created>2011-10-31</created><authors><author><keyname>Jackson</keyname><forenames>Matthew O.</forenames></author><author><keyname>Lopez-Pintado</keyname><forenames>Dunia</forenames></author></authors><title>Diffusion and Contagion in Networks with Heterogeneous Agents and
  Homophily</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how a behavior (an idea, buying a product, having a disease,
adopting a cultural fad or a technology) spreads among agents in an a social
network that exhibits segregation or homophily (the tendency of agents to
associate with others similar to themselves). Individuals are distinguished by
their types (e.g., race, gender, age, wealth, religion, profession, etc.)
which, together with biased interaction patterns, induce heterogeneous rates of
adoption. We identify the conditions under which a behavior diffuses and
becomes persistent in the population. These conditions relate to the level of
homophily in a society, the underlying proclivities of various types for
adoption or infection, as well as how each type interacts with its own type. In
particular, we show that homophily can facilitate diffusion from a small
initial seed of adopters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0084</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0084</id><created>2011-10-31</created><updated>2012-10-21</updated><authors><author><keyname>Song</keyname><forenames>Yiwei</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>Lattice codes for the Gaussian relay channel: Decode-and-Forward and
  Compress-and-Forward</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory, October 30,
  2011. Revised October 15, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice codes are known to achieve capacity in the Gaussian point-to-point
channel, achieving the same rates as independent, identically distributed
(i.i.d.) random Gaussian codebooks. Lattice codes are also known to outperform
random codes for certain channel models that are able to exploit their
linearity. In this work, we show that lattice codes may be used to achieve the
same performance as known i.i.d. Gaussian random coding techniques for the
Gaussian relay channel, and show several examples of how this may be combined
with the linearity of lattices codes in multi-source relay networks. In
particular, we present a nested lattice list decoding technique, by which,
lattice codes are shown to achieve the Decode-and-Forward (DF) rate of single
source, single destination Gaussian relay channels with one or more relays. We
next present two examples of how this DF scheme may be combined with the
linearity of lattice codes to achieve new rate regions which for some channel
conditions outperform analogous known Gaussian random coding techniques in
multi-source relay channels. That is, we derive a new achievable rate region
for the two-way relay channel with direct links and compare it to existing
schemes, and derive another achievable rate region for the multiple access
relay channel. We furthermore present a lattice Compress-and-Forward (CF)
scheme for the Gaussian relay channel which exploits a lattice Wyner-Ziv
binning scheme and achieves the same rate as the Cover-El Gamal CF rate
evaluated for Gaussian random codes. These results suggest that
structured/lattice codes may be used to mimic, and sometimes outperform, random
Gaussian codes in general Gaussian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0085</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0085</id><created>2011-10-31</created><authors><author><keyname>Abel</keyname><forenames>Andreas</forenames><affiliation>LMU Munich</affiliation></author><author><keyname>Kraus</keyname><forenames>Nicolai</forenames><affiliation>University of Nottingham</affiliation></author></authors><title>A Lambda Term Representation Inspired by Linear Ordered Logic</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 71, 2011, pp. 1-13</journal-ref><doi>10.4204/EPTCS.71.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new nameless representation of lambda terms inspired by
ordered logic. At a lambda abstraction, number and relative position of all
occurrences of the bound variable are stored, and application carries the
additional information where to cut the variable context into function and
argument part. This way, complete information about free variable occurrence is
available at each subterm without requiring a traversal, and environments can
be kept exact such that they only assign values to variables that actually
occur in the associated term. Our approach avoids space leaks in interpreters
that build function closures.
  In this article, we prove correctness of the new representation and present
an experimental evaluation of its performance in a proof checker for the
Edinburgh Logical Framework.
  Keywords: representation of binders, explicit substitutions, ordered
contexts, space leaks, Logical Framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0086</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0086</id><created>2011-10-31</created><authors><author><keyname>Beauquier</keyname><forenames>Maxime</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Sch&#xfc;rmann</keyname><forenames>Carsten</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>A Bigraph Relational Model</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 71, 2011, pp. 14-28</journal-ref><doi>10.4204/EPTCS.71.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a model based on relations for bigraphical reactive
systems [Milner09]. Its defining characteristics are that validity and reaction
relations are captured as traces in a multi-set rewriting system. The
relational model is derived from Milner's graphical definition and directly
amenable to implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0087</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0087</id><created>2011-10-31</created><authors><author><keyname>Boespflug</keyname><forenames>Mathieu</forenames><affiliation>McGill University</affiliation></author><author><keyname>Pientka</keyname><forenames>Brigitte</forenames><affiliation>McGill University</affiliation></author></authors><title>Multi-level Contextual Type Theory</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 71, 2011, pp. 29-43</journal-ref><doi>10.4204/EPTCS.71.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual type theory distinguishes between bound variables and
meta-variables to write potentially incomplete terms in the presence of
binders. It has found good use as a framework for concise explanations of
higher-order unification, characterize holes in proofs, and in developing a
foundation for programming with higher-order abstract syntax, as embodied by
the programming and reasoning environment Beluga. However, to reason about
these applications, we need to introduce meta^2-variables to characterize the
dependency on meta-variables and bound variables. In other words, we must go
beyond a two-level system granting only bound variables and meta-variables.
  In this paper we generalize contextual type theory to n levels for arbitrary
n, so as to obtain a formal system offering bound variables, meta-variables and
so on all the way to meta^n-variables. We obtain a uniform account by
collapsing all these different kinds of variables into a single notion of
variabe indexed by some level k. We give a decidable bi-directional type system
which characterizes beta-eta-normal forms together with a generalized
substitution operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0088</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0088</id><created>2011-10-31</created><authors><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author></authors><title>Nominal Logic with Equations Only</title><categories>cs.LO</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><acm-class>F.4.1; I.2.3</acm-class><journal-ref>EPTCS 71, 2011, pp. 44-57</journal-ref><doi>10.4204/EPTCS.71.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many formal systems, particularly in computer science, may be captured by
equations modulated by side conditions asserting the &quot;freshness of names&quot;;
these can be reasoned about with Nominal Equational Logic (NEL). Like most
logics of this sort NEL employs this notion of freshness as a first class
logical connective. However, this can become inconvenient when attempting to
translate results from standard equational logic to the nominal setting. This
paper presents proof rules for a logic whose only connectives are equations,
which we call Nominal Equation-only Logic (NEoL). We prove that NEoL is just as
expressive as NEL. We then give a simple description of equality in the empty
NEoL-theory, then extend that result to describe freshness in the empty
NEL-theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0089</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0089</id><created>2011-10-31</created><authors><author><keyname>Gabbay</keyname><forenames>Murdoch J.</forenames></author><author><keyname>Mulligan</keyname><forenames>Dominic P.</forenames></author></authors><title>Nominal Henkin Semantics: simply-typed lambda-calculus models in nominal
  sets</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><acm-class>F.4.1(mathematical logic); F.3.2(algebraic approaches to semantics);
  D.3.1(semantics)</acm-class><journal-ref>EPTCS 71, 2011, pp. 58-75</journal-ref><doi>10.4204/EPTCS.71.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a class of nominal algebraic Henkin-style models for the
simply typed lambda-calculus in which variables map to names in the denotation
and lambda-abstraction maps to a (non-functional) name-abstraction operation.
The resulting denotations are smaller and better-behaved, in ways we make
precise, than functional valuation-based models.
  Using these new models, we then develop a generalisation of \lambda-term
syntax enriching them with existential meta-variables, thus yielding a theory
of incomplete functions. This incompleteness is orthogonal to the usual notion
of incompleteness given by function abstraction and application, and
corresponds to holes and incomplete objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0090</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0090</id><created>2011-10-31</created><authors><author><keyname>Martin</keyname><forenames>Alan J.</forenames><affiliation>University of Ottawa</affiliation></author><author><keyname>Felty</keyname><forenames>Amy P.</forenames><affiliation>University of Ottawa</affiliation></author></authors><title>An Improved Implementation and Abstract Interface for Hybrid</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LFMTP 2011, arXiv:1110.6685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 71, 2011, pp. 76-90</journal-ref><doi>10.4204/EPTCS.71.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid is a formal theory implemented in Isabelle/HOL that provides an
interface for representing and reasoning about object languages using
higher-order abstract syntax (HOAS). This interface is built around an HOAS
variable-binding operator that is constructed definitionally from a de Bruijn
index representation. In this paper we make a variety of improvements to
Hybrid, culminating in an abstract interface that on one hand makes Hybrid a
more mathematically satisfactory theory, and on the other hand has important
practical benefits. We start with a modification of Hybrid's type of terms that
better hides its implementation in terms of de Bruijn indices, by excluding at
the type level terms with dangling indices. We present an improved set of
definitions, and a series of new lemmas that provide a complete
characterization of Hybrid's primitives in terms of properties stated at the
HOAS level. Benefits of this new package include a new proof of adequacy and
improvements to reasoning about object logics. Such proofs are carried out at
the higher level with no involvement of the lower level de Bruijn syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0094</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0094</id><created>2011-10-31</created><authors><author><keyname>Dastidar</keyname><forenames>Manosij Ghosh</forenames></author><author><keyname>Gupta</keyname><forenames>Sourav Sen</forenames></author></authors><title>Generalization of a few results in Integer Partitions</title><categories>cs.DM math.CO math.NT</categories><comments>7 pages paper, extension of http://arxiv.org/abs/1007.3459</comments><msc-class>05Axx, 11Pxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize a few important results in Integer Partitions;
namely the results known as Stanley's theorem and Elder's theorem, and the
congruence results proposed by Ramanujan for the partition function. We
generalize the results of Stanley and Elder from a fixed integer to an array of
subsequent integers, and propose an analogue of Ramanujan's congruence
relations for the `number of parts' function instead of the partition function.
We also deduce the generating function for the `number of parts', and relate
the technical results with their graphical interpretations through a novel use
of the Ferrer's diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0107</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0107</id><created>2011-10-31</created><updated>2012-03-07</updated><authors><author><keyname>Lee</keyname><forenames>Kyu-Min</forenames></author><author><keyname>Kim</keyname><forenames>Jung Yeol</forenames></author><author><keyname>Cho</keyname><forenames>Won-kuk</forenames></author><author><keyname>Goh</keyname><forenames>K. -I.</forenames></author><author><keyname>Kim</keyname><forenames>I. -M.</forenames></author></authors><title>Correlated multiplexity and connectivity of multiplex random networks</title><categories>physics.soc-ph cs.SI</categories><comments>Revised version, 12 pages, 6 figures</comments><journal-ref>New J. Phys. 14, 033027 (2012)</journal-ref><doi>10.1088/1367-2630/14/3/033027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in a complex networked system often engage in more than one type of
interactions among them; they form a multiplex network with multiple types of
links. In real-world complex systems, a node's degree for one type of links and
that for the other are not randomly distributed but correlated, which we term
correlated multiplexity. In this paper we study a simple model of multiplex
random networks and demonstrate that the correlated multiplexity can
drastically affect the properties of giant component in the network.
Specifically, when the degrees of a node for different interactions in a duplex
Erdos-Renyi network are maximally correlated, the network contains the giant
component for any nonzero link densities. In contrast, when the degrees of a
node are maximally anti-correlated, the emergence of giant component is
significantly delayed, yet the entire network becomes connected into a single
component at a finite link density. We also discuss the mixing patterns and the
cases with imperfect correlated multiplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0123</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0123</id><created>2011-11-01</created><updated>2011-11-22</updated><authors><author><keyname>Lee</keyname><forenames>Gyesik</forenames><affiliation>ROSAEC Center, Seoul National University, Korea</affiliation></author><author><keyname>Werner</keyname><forenames>Benjamin</forenames><affiliation>INRIA Saclay, France</affiliation></author></authors><title>Proof-irrelevant model of CC with predicative induction and judgmental
  equality</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (November
  23, 2011) lmcs:920</journal-ref><doi>10.2168/LMCS-7(4:5)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a set-theoretic, proof-irrelevant model for Calculus of
Constructions (CC) with predicative induction and judgmental equality in
Zermelo-Fraenkel set theory with an axiom for countably many inaccessible
cardinals. We use Aczel's trace encoding which is universally defined for any
function type, regardless of being impredicative. Direct and concrete
interpretations of simultaneous induction and mutually recursive functions are
also provided by extending Dybjer's interpretations on the basis of Aczel's
rule sets. Our model can be regarded as a higher-order generalization of the
truth-table methods. We provide a relatively simple consistency proof of type
theory, which can be used as the basis for a theorem prover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0129</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0129</id><created>2011-11-01</created><updated>2012-01-25</updated><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Output Feedback Tracking Control for a Class of Uncertain Systems
  subject to Unmodeled Dynamics and Delay at Input</title><categories>cs.SY math.OC</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides parametric uncertainties and disturbances, the unmodeled dynamics and
time delay at the input are often present in practical systems, which cannot be
ignored in some cases. This paper aims to solve output feedback tracking
control problem for a class of nonlinear uncertain systems subject to unmodeled
high-frequency gains and time delay at the input. By the additive
decomposition, the uncertain system is transformed to an uncertainty-free
system, where the uncertainties, disturbance and effect of unmodeled dynamics
plus time delay are lumped into a new disturbance at the output. Sequently,
additive decomposition is used to decompose the transformed system, which
simplifies the tracking controller design. To demonstrate the effectiveness,
the proposed control scheme is applied to three benchmark examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0156</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0156</id><created>2011-11-01</created><authors><author><keyname>Cordon-Franco</keyname><forenames>Andres</forenames></author><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>Fernandez-Duque</keyname><forenames>David</forenames></author><author><keyname>Joosten</keyname><forenames>Joost J.</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author></authors><title>A secure additive protocol for card players</title><categories>cs.DM cs.CR</categories><journal-ref>Australasian Journal of Combinatorics 54: 163-175, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider three players Alice, Bob and Cath who hold a, b and c cards,
respectively, from a deck of d=a+b+c cards. The cards are all different and
players only know their own cards. Suppose Alice and Bob wish to communicate
their cards to each other without Cath learning whether Alice or Bob holds a
specific card.
  Considering the cards as consecutive natural numbers 0,1,..., we investigate
general conditions for when Alice or Bob can safely announce the sum of the
cards they hold modulo an appropriately chosen integer. We demonstrate that
this holds whenever a,b&gt;2 and c=1. Because Cath holds a single card, this also
implies that Alice and Bob will learn the card deal from the other player's
announcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0158</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0158</id><created>2011-11-01</created><authors><author><keyname>Elyassami</keyname><forenames>Sanaa</forenames></author><author><keyname>Idri</keyname><forenames>Ali</forenames></author></authors><title>Applying Fuzzy ID3 Decision Tree for Software Effort Estimation</title><categories>cs.SE cs.AI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, 131-138 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Effort Estimation is a process of predicting the efforts and cost in
terms of money, schedule and staff for any software project system. Many
estimation models have been proposed over the last three decades and it is
believed that it is a must for the purpose of: Budgeting, risk analysis,
project planning and control, and project improvement investment analysis. In
this paper, we investigate the use of Fuzzy ID3 decision tree for software cost
estimation; it is designed by integrating the principles of ID3 decision tree
and the fuzzy set-theoretic concepts, enabling the model to handle uncertain
and imprecise data when describing the software projects, which can improve
greatly the accuracy of obtained estimates. MMRE and Pred are used as measures
of prediction accuracy for this study. A series of experiments is reported
using two different software projects datasets namely, Tukutuku and COCOMO'81
datasets. The results are compared with those produced by the crisp version of
the ID3 decision tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0194</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0194</id><created>2011-11-01</created><updated>2012-05-24</updated><authors><author><keyname>Stich</keyname><forenames>Sebastian U.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Christian L.</forenames></author><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author></authors><title>Optimization of Convex Functions with Random Pursuit</title><categories>math.OC cs.DS cs.NA math.NA</categories><comments>35 pages, 5 figures, 8 algorithms, 21 tables, submitted to journal
  The appendix contains additional supporting online material, not contained in
  the journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unconstrained randomized optimization of convex objective
functions. We analyze the Random Pursuit algorithm, which iteratively computes
an approximate solution to the optimization problem by repeated optimization
over a randomly chosen one-dimensional subspace. This randomized method only
uses zeroth-order information about the objective function and does not need
any problem-specific parametrization. We prove convergence and give convergence
rates for smooth objectives assuming that the one-dimensional optimization can
be solved exactly or approximately by an oracle. A convenient property of
Random Pursuit is its invariance under strictly monotone transformations of the
objective function. It thus enjoys identical convergence behavior on a wider
function class. To support the theoretical results we present extensive
numerical performance results of Random Pursuit, two gradient-free algorithms
recently proposed by Nesterov, and a classical adaptive step-size random search
scheme. We also present an accelerated heuristic version of the Random Pursuit
algorithm which significantly improves standard Random Pursuit on all numerical
benchmark problems. A general comparison of the experimental results reveals
that (i) standard Random Pursuit is effective on strongly convex functions with
moderate condition number, and (ii) the accelerated scheme is comparable to
Nesterov's fast gradient method and outperforms adaptive step-size strategies.
  The appendix contains additional supporting online material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0207</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0207</id><created>2011-10-31</created><authors><author><keyname>Bonato</keyname><forenames>Anthony</forenames></author><author><keyname>Janssen</keyname><forenames>Jeannette</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author></authors><title>Geometric protean graphs</title><categories>physics.soc-ph cs.SI math.CO</categories><journal-ref>Internet Mathematics 8 (2012), page 2-28</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the link structure of on-line social networks (OSNs), and introduce
a new model for such networks which may help infer their hidden underlying
reality. In the geo-protean (GEO-P) model for OSNs nodes are identified with
points in Euclidean space, and edges are stochastically generated by a mixture
of the relative distance of nodes and a ranking function. With high
probability, the GEO-P model generates graphs satisfying many observed
properties of OSNs, such as power law degree distributions, the small world
property, densification power law, and bad spectral expansion. We introduce the
dimension of an OSN based on our model, and examine this new parameter using
actual OSN data. We discuss how the geo-protean model may eventually be used as
a tool to group users with similar attributes using only the link structure of
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0219</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0219</id><created>2011-11-01</created><authors><author><keyname>Haghighi</keyname><forenames>Kasra</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>On Optimum Causal Cognitive Spectrum Reutilization Strategy</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 30, no. 10,
  pp. 1911-1921, Nov. 2012</journal-ref><doi>10.1109/JSAC.2012.121107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study opportunistic transmission strategies for cognitive
radios (CR) in which causal noisy observation from a primary user(s) (PU) state
is available. PU is assumed to be operating in a slotted manner, according to a
two-state Markov model. The objective is to maximize utilization ratio (UR),
i.e., relative number of the PU-idle slots that are used by CR, subject to
interference ratio (IR), i.e., relative number of the PU-active slots that are
used by CR, below a certain level. We introduce an a-posteriori LLR-based
cognitive transmission strategy and show that this strategy is optimum in the
sense of maximizing UR given a certain maximum allowed IR. Two methods for
calculating threshold for this strategy in practical situations are presented.
One of them performs well in higher SNRs but might have too large IR at low
SNRs and low PU activity levels, and the other is proven to never violate the
allowed IR at the price of a reduced UR. In addition, an upper-bound for the UR
of any CR strategy operating in the presence of Markovian PU is presented.
Simulation results have shown a more than 116% improvement in UR at SNR of -3dB
and IR level of 10% with PU state estimation. Thus, this opportunistic CR
mechanism possesses a high potential in practical scenarios in which there
exists no information about true states of PU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0228</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0228</id><created>2011-11-01</created><authors><author><keyname>Aguilar-Melchor</keyname><forenames>Carlos</forenames></author><author><keyname>Gaborit</keyname><forenames>Philippe</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author><author><keyname>Sok</keyname><forenames>Lin</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Classification of extremal and $s$-extremal binary self-dual codes of
  length 38</title><categories>cs.DM</categories><comments>revised version - paper submitted (4/4/2011) to IEEE trans.
  Information and accepted 20/10/2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we classify all extremal and $s$-extremal binary self-dual
codes of length 38. There are exactly 2744 extremal $[38,19,8]$ self-dual
codes, two $s$-extremal $[38,19,6]$ codes, and 1730 $s$-extremal $[38,19,8]$
codes. We obtain our results from the use of a recursive algorithm used in the
recent classification of all extremal self-dual codes of length 36, and from a
generalization of this recursive algorithm for the shadow. The classification
of $s$-extremal $[38,19,6]$ codes permits to achieve the classification of all
$s$-extremal codes with d=6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0233</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0233</id><created>2011-11-01</created><updated>2011-12-11</updated><authors><author><keyname>Talgat</keyname><forenames>Shuvatov</forenames></author><author><keyname>Batrbek</keyname><forenames>Suleimenov</forenames></author></authors><title>Gas turbine diagnostic system</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The creation of the systems models is very actual at present time, because it
allow to simulate the work of some complex equipment without any additional
spends. The given model of gas turbine is allowed to test and optimize the
software for gas turbine automation systems, study station personal, like
operators and engineers and will be useful for diagnostics and prediction tasks
to analyze the efficiency of the gas turbine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0235</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0235</id><created>2011-11-01</created><authors><author><keyname>Tucci</keyname><forenames>Gabriel H.</forenames></author><author><keyname>Wang</keyname><forenames>Ke</forenames></author></authors><title>New Methods for Handling Singular Sample Covariance Matrices</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>23 pages and 10 figures</comments><msc-class>15B52, 60B20, 62Hxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of a covariance matrix from an insufficient amount of data is
one of the most common problems in fields as diverse as multivariate
statistics, wireless communications, signal processing, biology, learning
theory and finance. In \cite{MTS}, a new approach to handle singular covariance
matrices was suggested. The main idea was to use dimensionality reduction in
conjunction with an average over the unitary matrices. In this paper we
continue with this idea and we further consider new innovative approaches that
show considerable improvements with respect to traditional methods such as
diagonal loading. One of the methods is called the \emph{Ewens} estimator and
uses a randomization of the sample covariance matrix over all the permutation
matrices with respect to the Ewens measure. The techniques used to attack this
problem are broad and run from random matrix theory to combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0242</identifier>
 <datestamp>2011-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0242</id><created>2011-11-01</created><authors><author><keyname>Sobe</keyname><forenames>Anita</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author><author><keyname>B&#xf6;sz&#xf6;rmenyi</keyname><forenames>Laszlo</forenames></author></authors><title>Storage Balancing in Self-organizing Multimedia Delivery Systems</title><categories>cs.MM cs.DC nlin.AO</categories><report-no>TR/ITEC/01/2.13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the current bio-inspired delivery networks set their focus on search,
e.g., by using artificial ants. If the network size and, therefore, the search
space gets too large, the users experience high delays until the requested
content can be consumed. In previous work, we proposed different replication
strategies to reduce the search space. In this report we further evaluate
measures for storage load balancing, because peers are most likely limited in
space. We periodically apply clean-ups if a certain storage level is reached.
For our evaluations we combine the already introduced replication measures with
least recently used (LRU), least frequently used (LFU) and a hormone-based
clean-up. The goal is to elaborate a combination that leads to low delays while
the replica utilization is high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0253</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0253</id><created>2011-11-01</created><updated>2011-11-07</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>Nearly Complete Graphs Decomposable into Large Induced Matchings and
  their Applications</title><categories>math.CO cs.DS cs.IT math.IT</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two constructions of (very) dense graphs which are edge disjoint
unions of large {\em induced} matchings. The first construction exhibits graphs
on $N$ vertices with ${N \choose 2}-o(N^2)$ edges, which can be decomposed into
pairwise disjoint induced matchings, each of size $N^{1-o(1)}$. The second
construction provides a covering of all edges of the complete graph $K_N$ by
two graphs, each being the edge disjoint union of at most $N^{2-\delta}$
induced matchings, where $\delta &gt; 0.058$. This disproves (in a strong form) a
conjecture of Meshulam, substantially improves a result of Birk, Linial and
Meshulam on communicating over a shared channel, and (slightly) extends the
analysis of H{\aa}stad and Wigderson of the graph test of Samorodnitsky and
Trevisan for linearity. Additionally, our constructions settle a combinatorial
question of Vempala regarding a candidate rounding scheme for the directed
Steiner tree problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0268</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0268</id><created>2011-11-01</created><updated>2011-11-09</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>Topology on locally finite metric spaces</title><categories>math.MG cs.CV cs.DM math.AT math.CO</categories><comments>Second preliminary version - 42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The necessity of a theory of General Topology and, most of all, of Algebraic
Topology on locally finite metric spaces comes from many areas of research in
both Applied and Pure Mathematics: Molecular Biology, Mathematical Chemistry,
Computer Science, Topological Graph Theory and Metric Geometry. In this paper
we propose the basic notions of such a theory and some applications: we replace
the classical notions of continuous function, homeomorphism and homotopic
equivalence with the notions of NPP-function, NPP-local-isomorphism and
NPP-homotopy (NPP stands for Nearest Point Preserving); we also introduce the
notion of NPP-isomorphism. We construct three invariants under NPP-isomorphisms
and, in particular, we define the fundamental group of a locally finite metric
space. As first applications, we propose the following: motivated by the
longstanding question whether there is a purely metric condition which extends
the notion of amenability of a group to any metric space, we propose the
property SN (Small Neighborhood); motivated by some applicative problems in
Computer Science, we prove the analog of the Jordan curve theorem in $\mathbb
Z^2$; motivated by a question asked during a lecture at Lausanne, we extend to
any locally finite metric space a recent inequality of P.N.Jolissaint and
Valette regarding the $\ell_p$-distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0284</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0284</id><created>2011-11-01</created><updated>2012-04-16</updated><authors><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author><author><keyname>Deza</keyname><forenames>Michel</forenames></author></authors><title>A topological interpretation of the walk distances</title><categories>math.CO cs.DM cs.SI math.MG</categories><comments>13 pages, 1 figure. Version #3</comments><msc-class>05C12, 05C50, 51K05, 15A09, 15A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The walk distances in graphs have no direct interpretation in terms of walk
weights, since they are introduced via the \emph{logarithms} of walk weights.
Only in the limiting cases where the logarithms vanish such representations
follow straightforwardly. The interpretation proposed in this paper rests on
the identity $\ln\det B=\tr\ln B$ applied to the cofactors of the matrix
$I-tA,$ where $A$ is the weighted adjacency matrix of a weighted multigraph and
$t$ is a sufficiently small positive parameter. In addition, this
interpretation is based on the power series expansion of the logarithm of a
matrix. Kasteleyn (1967) was probably the first to apply the foregoing approach
to expanding the determinant of $I-A$. We show that using a certain linear
transformation the same approach can be extended to the cofactors of $I-tA,$
which provides a topological interpretation of the walk distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0305</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0305</id><created>2011-10-28</created><authors><author><keyname>Yampolskiy</keyname><forenames>Roman V.</forenames></author></authors><title>Construction of an NP Problem with an Exponential Lower Bound</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a Hashed-Path Traveling Salesperson Problem (HPTSP),
a new type of problem which has the interesting property of having no
polynomial time solutions. Next we show that HPTSP is in the class NP by
demonstrating that local information about sub-routes is insufficient to
compute the complete value of each route. As a consequence, via Ladner's
theorem, we show that the class NPI is non-empty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0307</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0307</id><created>2011-11-01</created><updated>2011-11-04</updated><authors><author><keyname>Abbassi</keyname><forenames>Zeinab</forenames></author><author><keyname>Aperjis</keyname><forenames>Christina</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Swayed by Friends or by the Crowd?</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have conducted three empirical studies of the effects of friend
recommendations and general ratings on how online users make choices. These two
components of social influence were investigated through user studies on
Mechanical Turk. We find that for a user deciding between two choices an
additional rating star has a much larger effect than an additional friend's
recommendation on the probability of selecting an item. Equally important,
negative opinions from friends are more influential than positive opinions, and
people exhibit more random behavior in their choices when the decision involves
less cost and risk. Our results can be generalized across different
demographics, implying that individuals trade off recommendations from friends
and ratings in a similar fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0321</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0321</id><created>2011-11-01</created><updated>2012-02-15</updated><authors><author><keyname>Dieudonn&#xe9;</keyname><forenames>Yoann</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Deterministic gathering of anonymous agents in arbitrary networks</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A team consisting of an unknown number of mobile agents, starting from
different nodes of an unknown network, possibly at different times, have to
meet at the same node. Agents are anonymous (identical), execute the same
deterministic algorithm and move in synchronous rounds along links of the
network. Which configurations are gatherable and how to gather all of them
deterministically by the same algorithm?
  We give a complete solution of this gathering problem in arbitrary networks.
We characterize all gatherable configurations and give two universal
deterministic gathering algorithms, i.e., algorithms that gather all gatherable
configurations. The first algorithm works under the assumption that an upper
bound n on the size of the network is known. In this case our algorithm
guarantees gathering with detection, i.e., the existence of a round for any
gatherable configuration, such that all agents are at the same node and all
declare that gathering is accomplished. If no upper bound on the size of the
network is known, we show that a universal algorithm for gathering with
detection does not exist. Hence, for this harder scenario, we construct a
second universal gathering algorithm, which guarantees that, for any gatherable
configuration, all agents eventually get to one node and stop, although they
cannot tell if gathering is over. The time of the first algorithm is polynomial
in the upper bound n on the size of the network, and the time of the second
algorithm is polynomial in the (unknown) size itself.
  Our results have an important consequence for the leader election problem for
anonymous agents in arbitrary graphs. For anonymous agents in graphs, leader
election turns out to be equivalent to gathering with detection. Hence, as a
by-product, we obtain a complete solution of the leader election problem for
anonymous agents in arbitrary graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0337</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0337</id><created>2011-11-01</created><authors><author><keyname>Yanes</keyname><forenames>Adrian</forenames></author></authors><title>OpenWeather: a peer-to-peer weather data transmission protocol</title><categories>cs.NI</categories><comments>Available as well: http://lib.tkk.fi/Final_project/2011/urn100502.pdf</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The study of the weather is performed using instruments termed weather
stations. These weather stations are distributed around the world, collecting
the data from the different phenomena. Several weather organizations have been
deploying thousands of these instruments, creating big networks to collect
weather data. These instruments are collecting the weather data and delivering
it for later processing in the collections points. Nevertheless, all the
methodologies used to transmit the weather data are based in protocols non
adapted for this purpose. Thus, the weather stations are limited by the data
formats and protocols used in them, not taking advantage of the real-time data
available on them. We research the weather instruments, their technology and
their network capabilities, in order to provide a solution for the mentioned
problem. OpenWeather is the protocol proposed to provide a more optimum and
reliable way to transmit the weather data. We evaluate the environmental
factors, such as location or bandwidth availability, in order to design a
protocol adapted to the requirements established by the automatic weather
stations. A peer to peer architecture is proposed, providing a functional
implementation of OpenWeather protocol. The evaluation of the protocol is
executed in a real scenario, providing the hints to adapt the protocol to a
common automatic weather station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0338</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0338</id><created>2011-11-01</created><updated>2012-01-11</updated><authors><author><keyname>Almishari</keyname><forenames>Mishari</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Exploring Linkablility of Community Reviewing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large numbers of people all over the world read and contribute to various
review sites. Many contributors are understandably concerned about privacy;
specifically, about linkability of reviews (and accounts) across review sites.
In this paper, we study linkability of community reviewing and try to answer
the question: to what extent are &quot;anonymous&quot; reviews linkable, i.e., likely
authored by the same contributor? Based on a very large set of reviews from a
popular site (Yelp), we show that a high percentage of ostensibly anonymous
reviews can be linked with very high confidence. This is despite the fact that
we use very simple models and equally simple features set. Our study suggests
that contributors reliably expose their identities in reviews. This has
important implications for cross-referencing accounts between different review
sites. Also, techniques used in our study could be adopted by review sites to
give contributors feedback about privacy of their reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0352</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0352</id><created>2011-11-01</created><updated>2012-06-14</updated><authors><author><keyname>Kulis</keyname><forenames>Brian</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Revisiting k-means: New Algorithms via Bayesian Nonparametrics</title><categories>cs.LG stat.ML</categories><comments>14 pages. Updated based on the corresponding ICML paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian models offer great flexibility for clustering
applications---Bayesian nonparametrics can be used for modeling infinite
mixtures, and hierarchical Bayesian models can be utilized for sharing clusters
across multiple data sets. For the most part, such flexibility is lacking in
classical clustering methods such as k-means. In this paper, we revisit the
k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired
by the asymptotic connection between k-means and mixtures of Gaussians, we show
that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a
hard clustering algorithm in the limit, and further that the resulting
algorithm monotonically minimizes an elegant underlying k-means-like clustering
objective that includes a penalty for the number of clusters. We generalize
this analysis to the case of clustering multiple data sets through a similar
asymptotic argument with the hierarchical Dirichlet process. We also discuss
further extensions that highlight the benefits of our analysis: i) a spectral
relaxation involving thresholded eigenvectors, and ii) a normalized cut graph
clustering algorithm that does not fix the number of clusters in the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0356</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0356</id><created>2011-11-01</created><updated>2013-10-18</updated><authors><author><keyname>Soprunov</keyname><forenames>Ivan</forenames></author></authors><title>Toric complete intersection codes</title><categories>math.AG cs.IT math.IT</categories><comments>14 pages, 2 figures. Minor changes, simpler proofs, new examples</comments><msc-class>14M25, 14G50 (Primary) 52B20, 94B27 (Secondary)</msc-class><journal-ref>Journal of Symbolic Computation, Volume 50, (2013), Pages 374-385</journal-ref><doi>10.1016/j.jsc.2012.08.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give lower bounds for the minimum distance of evaluation
codes constructed from complete intersections in toric varieties. This
generalizes the results of Gold-Little-Schenck and Ballico-Fontanari who
considered evaluation codes on complete intersections in the projective space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0368</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0368</id><created>2011-11-01</created><authors><author><keyname>Brim</keyname><forenames>Lubo&#x161;</forenames><affiliation>Masaryk University</affiliation></author><author><keyname>Barnat</keyname><forenames>Ji&#x159;&#xed;</forenames><affiliation>Masaryk University</affiliation></author></authors><title>Platform Dependent Verification: On Engineering Verification Tools for
  21st Century</title><categories>cs.SE cs.LO</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.1.3</acm-class><journal-ref>EPTCS 72, 2011, pp. 1-12</journal-ref><doi>10.4204/EPTCS.72.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper overviews recent developments in platform-dependent explicit-state
LTL model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0369</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0369</id><created>2011-11-01</created><authors><author><keyname>Laarman</keyname><forenames>Alfons</forenames><affiliation>University of Twente</affiliation></author><author><keyname>van de Pol</keyname><forenames>Jaco</forenames><affiliation>University of Twente</affiliation></author></authors><title>Variations on Multi-Core Nested Depth-First Search</title><categories>cs.LO</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 13-28</journal-ref><doi>10.4204/EPTCS.72.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, two new parallel algorithms for on-the-fly model checking of LTL
properties were presented at the same conference: Automated Technology for
Verification and Analysis, 2011. Both approaches extend Swarmed NDFS, which
runs several sequential NDFS instances in parallel. While parallel random
search already speeds up detection of bugs, the workers must share some global
information in order to speed up full verification of correct models. The two
algorithms differ considerably in the global information shared between
workers, and in the way they synchronize.
  Here, we provide a thorough experimental comparison between the two
algorithms, by measuring the runtime of their implementations on a multi-core
machine. Both algorithms were implemented in the same framework of the model
checker LTSmin, using similar optimizations, and have been subjected to the
full BEEM model database.
  Because both algorithms have complementary advantages, we constructed an
algorithm that combines both ideas. This combination clearly has an improved
speedup. We also compare the results with the alternative parallel algorithm
for accepting cycle detection OWCTY-MAP. Finally, we study a simple statistical
model for input models that do contain accepting cycles. The goal is to
distinguish the speedup due to parallel random search from the speedup that can
be attributed to clever work sharing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0370</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0370</id><created>2011-11-01</created><authors><author><keyname>Bulychev</keyname><forenames>Peter</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>David</keyname><forenames>Alexandre</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim Guldstrand</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Miku&#x10d;ionis</keyname><forenames>Marius</forenames><affiliation>Aalborg University</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Aalborg University and INRIA Rennes</affiliation></author></authors><title>Distributed Parametric and Statistical Model Checking</title><categories>cs.SE cs.DC</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><acm-class>D.1.3,D.2.4,</acm-class><journal-ref>EPTCS 72, 2011, pp. 30-42</journal-ref><doi>10.4204/EPTCS.72.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical Model Checking (SMC) is a trade-off between testing and formal
verification. The core idea of the approach is to conduct some simulations of
the system and verify if they satisfy some given property. In this paper we
show that SMC is easily parallelizable on a master/slaves architecture by
introducing a series of algorithms that scale almost linearly with respect to
the number of slave computers. Our approach has been implemented in the UPPAAL
SMC toolset and applied on non-trivial case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0371</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0371</id><created>2011-11-01</created><authors><author><keyname>Hamadi</keyname><forenames>Youssef</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames><affiliation>University College Dublin</affiliation></author><author><keyname>Wintersteiger</keyname><forenames>Christoph M.</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>Lazy Decomposition for Distributed Decision Procedures</title><categories>cs.LO cs.DC cs.SE</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 43-54</journal-ref><doi>10.4204/EPTCS.72.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of automated tools for software and hardware
verification puts ever increasing demands on the underlying decision
procedures. This paper presents a framework for distributed decision procedures
(for first-order problems) based on Craig interpolation. Formulas are
distributed in a lazy fashion, i.e., without the use of costly decomposition
algorithms. Potential models which are shown to be incorrect are reconciled
through the use of Craig interpolants. Experimental results on challenging
propositional satisfiability problems indicate that our method is able to
outperform traditional solving techniques even without the use of additional
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0372</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0372</id><created>2011-11-01</created><authors><author><keyname>Kahsai</keyname><forenames>Temesghen</forenames><affiliation>The University of Iowa</affiliation></author><author><keyname>Tinelli</keyname><forenames>Cesare</forenames><affiliation>The University of Iowa</affiliation></author></authors><title>PKind: A parallel k-induction based model checker</title><categories>cs.LO cs.DC</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 55-62</journal-ref><doi>10.4204/EPTCS.72.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PKind is a novel parallel k-induction-based model checker of invariant
properties for finite- or infinite-state Lustre programs. Its architecture,
which is strictly message-based, is designed to minimize synchronization delays
and easily accommodate the incorporation of incremental invariant generators to
enhance basic k-induction. We describe PKind's functionality and main features,
and present experimental evidence that PKind significantly speeds up the
verification of safety properties and, due to incremental invariant generation,
also considerably increases the number of provable ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0373</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0373</id><created>2011-11-01</created><authors><author><keyname>Bene&#x161;</keyname><forenames>Nikola</forenames><affiliation>FI MU</affiliation></author><author><keyname>&#x10c;ern&#xe1;</keyname><forenames>Ivana</forenames><affiliation>FI MU</affiliation></author><author><keyname>K&#x159;iv&#xe1;nek</keyname><forenames>Milan</forenames></author></authors><title>CoInDiVinE: Parallel Distributed Model Checker for Component-Based
  Systems</title><categories>cs.SE cs.DC</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 63-67</journal-ref><doi>10.4204/EPTCS.72.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CoInDiVinE is a tool for parallel distributed model checking of interactions
among components in hierarchical component-based systems. The tool extends the
DiVinE framework with a new input language (component-interaction automata) and
a property specification logic (CI-LTL). As the language differs from the input
language of DiVinE, our tool employs a new state space generation algorithm
that also supports partial order reduction. Experiments indicate that the tool
has good scaling properties when run in parallel setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0374</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0374</id><created>2011-11-01</created><authors><author><keyname>Vijzelaar</keyname><forenames>Stefan</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>Verstoep</keyname><forenames>Kees</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>Fokkink</keyname><forenames>Wan</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>Bal</keyname><forenames>Henri</forenames><affiliation>VU University Amsterdam</affiliation></author></authors><title>Distributed MAP in the SpinJa Model Checker</title><categories>cs.SE cs.DC</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 84-90</journal-ref><doi>10.4204/EPTCS.72.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spin in Java (SpinJa) is an explicit state model checker for the Promela
modelling language also used by the SPIN model checker. Designed to be
extensible and reusable, the implementation of SpinJa follows a layered
approach in which each new layer extends the functionality of the previous one.
While SpinJa has preliminary support for shared-memory model checking, it did
not yet support distributed-memory model checking. This tool paper presents a
distributed implementation of a maximal accepting predecessors (MAP) search
algorithm on top of SpinJa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0375</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0375</id><created>2011-11-01</created><authors><author><keyname>Wijs</keyname><forenames>Anton</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>The HIVE Tool for Informed Swarm State Space Exploration</title><categories>cs.SE cs.DC cs.LO</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 91-98</journal-ref><doi>10.4204/EPTCS.72.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm verification and parallel randomised depth-first search are very
effective parallel techniques to hunt bugs in large state spaces. In case bugs
are absent, however, scalability of the parallelisation is completely lost. In
recent work, we proposed a mechanism to inform the workers which parts of the
state space to explore. This mechanism is compatible with any action-based
formalism, where a state space can be represented by a labelled transition
system. With this extension, each worker can be strictly bounded to explore
only a small fraction of the state space at a time. In this paper, we present
the HIVE tool together with two search algorithms which were added to the
LTSmin tool suite to both perform a preprocessing step, and execute a bounded
worker search. The new tool is used to coordinate informed swarm explorations,
and the two new LTSmin algorithms are employed for preprocessing a model and
performing the individual searches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0376</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0376</id><created>2011-11-01</created><updated>2011-11-07</updated><authors><author><keyname>Boucher</keyname><forenames>Christina</forenames></author><author><keyname>Lo</keyname><forenames>Christine</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author></authors><title>Outlier Detection for DNA Fragment Assembly</title><categories>cs.DS</categories><comments>29 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ length-$\ell$ strings $S =\{s_1, ..., s_n\}$ over a constant size
alphabet $\Sigma$ together with parameters $d$ and $k$, the objective in the
{\em Consensus String with Outliers} problem is to find a subset $S^*$ of $S$
of size $n-k$ and a string $s$ such that $\sum_{s_i \in S^*} d(s_i, s) \leq d$.
Here $d(x, y)$ denotes the Hamming distance between the two strings $x$ and
$y$. We prove
  1. a variant of {\em Consensus String with Outliers} where the number of
outliers $k$ is fixed and the objective is to minimize the total distance
$\sum_{s_i \in S^*} d(s_i, s)$ admits a simple PTAS. (ii) Under the natural
assumption that the number of outliers $k$ is small, the PTAS for the distance
minimization version of {\em Consensus String with Outliers} performs well. In
particular, as long as $k\leq cn$ for a fixed constant $c &lt; 1$, the algorithm
provides a $(1+\epsilon)$-approximate solution in time
$f(1/\epsilon)(n\ell)^{O(1)}$ and thus, is an EPTAS.
  2. In order to improve the PTAS for {\em Consensus String with Outliers} to
an EPTAS, the assumption that $k$ is small is necessary. Specifically, when $k$
is allowed to be arbitrary the {\em Consensus String with Outliers} problem
does not admit an EPTAS unless FPT=W[1]. This hardness result holds even for
binary alphabets.
  3. The decision version of {\em Consensus String with Outliers} is fixed
parameter tractable when parameterized by $\frac{d}{n-k}$. and thus, also when
parameterized by just $d$.
  To the best of our knowledge, {\em Consensus String with Outliers} is the
first problem that admits a PTAS, and is fixed parameter tractable when
parameterized by the value of the objective function but does not admit an
EPTAS under plausible complexity assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0379</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0379</id><created>2011-11-02</created><updated>2012-05-31</updated><authors><author><keyname>Brown</keyname><forenames>Daniel G.</forenames></author><author><keyname>Truszkowski</keyname><forenames>Jakub</forenames></author></authors><title>Fast reconstruction of phylogenetic trees using locality-sensitive
  hashing</title><categories>q-bio.PE cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first sub-quadratic time algorithm that with high probability
correctly reconstructs phylogenetic trees for short sequences generated by a
Markov model of evolution. Due to rapid expansion in sequence databases, such
very fast algorithms are becoming necessary. Other fast heuristics have been
developed for building trees from very large alignments (Price et al, and Brown
et al), but they lack theoretical performance guarantees. Our new algorithm
runs in $O(n^{1+\gamma(g)}\log^2n)$ time, where $\gamma$ is an increasing
function of an upper bound on the branch lengths in the phylogeny, the upper
bound $g$ must be below$1/2-\sqrt{1/8} \approx 0.15$, and $\gamma(g)&lt;1$ for all
$g$. For phylogenies with very short branches, the running time of our
algorithm is close to linear. For example, if all branch lengths correspond to
a mutation probability of less than 0.02, the running time of our algorithm is
roughly $O(n^{1.2}\log^2n)$. Via a prototype and a sequence of large-scale
experiments, we show that many large phylogenies can be reconstructed fast,
without compromising reconstruction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0380</identifier>
 <datestamp>2012-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0380</id><created>2011-11-02</created><updated>2012-06-19</updated><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author><author><keyname>Krishna</keyname><forenames>Sripad</forenames></author></authors><title>An Efficient Security Mechanism for High-Integrity Wireless Sensor
  Networks</title><categories>cs.CR cs.NI</categories><comments>withdrawn by author. arXiv admin note: v1 substantial text overlap
  with arXiv:1012.2516</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have recently attracted a lot of interest in
the research community due their wide range of applications. Unfortunately,
these networks are vulnerable to numerous security threats that can adversely
affect their proper functioning. This problem is more critical if the network
is deployed for some mission-critical applications such as in a tactical
battlefield. Random failure of nodes and intentional compromise of nodes by an
insider attack in a WSN pose particularly difficult challenges to security
engineers as these attacks cannot be defended by traditional cryptography-based
mechanisms. In this paper, a security solution is proposed for detecting
compromised and faulty nodes in a WSN. The mechanism also isolates a
compromised node from the network so that it cannot participate in any network
activity. The proposed mechanism is based on misbehavior classification,
behaviour monitoring and trust management. It involves minimum computation and
communication overhead and is ideally suited for a resource-constrained,
high-integrity WSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0382</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0382</id><created>2011-11-02</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>A Distributed Intrusion Detection System Using Cooperating Agents</title><categories>cs.CR cs.NI</categories><comments>10 pages, 4 figures, 1 table. In Proceedings of the 3rd International
  Conference on Information Processing (ICIP'09), August 7 - 9, Bangalore,
  2009, pp. 559 - 568</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current intrusion detection systems have a number of problems that limit
their configurability, scalability and efficiency. There have been some
propositions about distributed architectures based on multiple independent
agents working collectively for intrusion detection. However, these distributed
intrusion detection systems are not fully distributed as most of them centrally
analyze data collected from distributed nodes which may lead to a single point
of failure. In this paper, a distributed intrusion detection architecture is
presented that is based on autonomous and cooperating agents without any
centralized analysis components. The agents cooperate by using a hierarchical
communication of interests and data, and the analysis of intrusion data is made
by the agents at the lowest level of the hierarchy. This architecture provides
significant advantages in scalability, flexibility, extensibility, fault
tolerance, and resistance to compromise. A proof-of-concept prototype is
developed and experiments have been conducted on it. The results show the
effectiveness of the system in detecting intrusive activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0385</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0385</id><created>2011-11-02</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author><author><keyname>Chandra</keyname><forenames>M. Girish</forenames></author><author><keyname>Balamuralidhar</keyname><forenames>P.</forenames></author><author><keyname>G.</keyname><forenames>Harihara S.</forenames></author><author><keyname>Reddy</keyname><forenames>Harish</forenames></author></authors><title>A Distributed Protocol for Detection of Packet Dropping Attack in Mobile
  Ad Hoc Networks</title><categories>cs.CR cs.NI</categories><comments>7 pages, 9 figures, 1 table. In Proceedings of the International
  Conference on Telecommunications and Malaysian International Conference on
  Communications (ICT-MICC'07), May 14-17, Penang, Malaysia. Paper ID: 74,
  Track: 3: Ad Hoc Routing and Protocols. ISBN: 1-4244-1094-0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-hop mobile ad hoc networks (MANETs),mobile nodes cooperate with each
other without using any infrastructure such as access points or base stations.
Security remains a major challenge for these networks due to their features of
open medium, dynamically changing topologies, reliance on cooperative
algorithms, absence of centralized monitoring points, and lack of clear lines
of defense. Among the various attacks to which MANETs are vulnerable, malicious
packet dropping attack is very common where a malicious node can partially
degrade or completely disrupt communication in the network by consistently
dropping packets. In this paper, a mechanism for detection of packet dropping
attack is presented based on cooperative participation of the nodes in a MANET.
The redundancy of routing information in an ad hoc network is utilized to make
the scheme robust so that it works effectively even in presence of transient
network partitioning and Byzantine failure of nodes. The proposed scheme is
fully cooperative and thus more secure as the vulnerabilities of any election
algorithm used for choosing a subset of nodes for cooperation are absent.
Simulation results show the effectiveness of the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0386</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0386</id><created>2011-11-02</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author><author><keyname>Chandra</keyname><forenames>M. Girish</forenames></author><author><keyname>G.</keyname><forenames>Harihara S.</forenames></author><author><keyname>Reddy</keyname><forenames>Harish</forenames></author><author><keyname>Balamuralidhar</keyname><forenames>P.</forenames></author></authors><title>A Mechanism for Detection of Gray Hole Attack in Mobile Ad Hoc Networks</title><categories>cs.CR cs.NI</categories><comments>5 pages, 5 figures, 3 tables. In Proceedings of the 6th International
  Conference on Information, Communications and Signal Processing (ICICS '07),
  Singapore, December 10-13, 2007, Paper ID: 0458, Track: Th2.5- Network
  Security. ISBN: 1-4244-0983-7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protecting the network layer from malicious attacks is an important and
challenging security issue in mobile ad hoc networks (MANETs). In this paper, a
security mechanism is proposed to defend against a cooperative gray hole attack
on the well known AODV routing protocol in MANETs. A gray hole is a node that
selectively drops and forwards data packets after it advertises itself as
having the shortest path to the destination node in response to a route request
message from a source node. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of any node.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0387</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0387</id><created>2011-11-02</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author><author><keyname>Koilakonda</keyname><forenames>Sripad</forenames></author><author><keyname>Ukil</keyname><forenames>Arijit</forenames></author></authors><title>A Mechanism for Detection of Cooperative Black Hole Attack in Mobile Ad
  Hoc Networks</title><categories>cs.CR cs.NI</categories><comments>6 pages, 10 figures, 2 tables. In Proceedings of the 2nd
  International Conference on Intelligent Systems, Modeling and Simulation
  (ISMS'11), pp. 338-343, Phnom Penh, Cambodia, January 25-27, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms,absence of centralized
monitoring points, and lack of clear lines of defense. Most of the routing
protocols for MANETs are thus vulnerable to various types of attacks. Ad hoc
on-demand distance vector routing (AODV) is a very popular routing algorithm.
However, it is vulnerable to the well-known black hole attack, where a
malicious node falsely advertises good paths to a destination node during the
route discovery process. This attack becomes more sever when a group of
malicious nodes cooperate each other. In this paper, a defense mechanism is
presented against a coordinated attack by multiple black hole nodes in a MANET.
The simulation carried out on the proposed scheme has produced results that
demonstrate the effectiveness of the mechanism in detection of the attack while
maintaining a reasonable level of throughput in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0405</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0405</id><created>2011-11-02</created><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Hastad</keyname><forenames>Johan</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Making the long code shorter, with applications to the Unique Games
  Conjecture</title><categories>cs.CC</categories><comments>45 pages</comments><msc-class>68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long code is a central tool in hardness of approximation, especially in
questions related to the unique games conjecture. We construct a new code that
is exponentially more e?cient, but can still be used in many of these
applications. Using the new code we obtain exponential improvements over
several known results, including the following:
  1. For any eps &gt; 0, we show the existence of an n vertex graph G where every
set of o(n) vertices has expansion 1 - eps, but G's adjacency matrix has more
than exp(log^delta n) eigenvalues larger than 1 - eps, where delta depends only
on eps. This answers an open question of Arora, Barak and Steurer (FOCS 2010)
who asked whether one can improve over the noise graph on the Boolean hypercube
that has poly(log n) such eigenvalues.
  2. A gadget that reduces unique games instances with linear constraints
modulo K into instances with alphabet k with a blowup of K^polylog(K),
improving over the previously known gadget with blowup of 2^K.
  3. An n variable integrality gap for Unique Games that that survives
exp(poly(log log n)) rounds of the SDP + Sherali Adams hierarchy, improving on
the previously known bound of poly(log log n).
  We show a connection between the local testability of linear codes and small
set expansion in certain related Cayley graphs, and use this connection to
derandomize the noise graph on the Boolean hypercube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0406</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0406</id><created>2011-11-02</created><authors><author><keyname>Xie</keyname><forenames>Yingtai</forenames></author></authors><title>A Graph Invariant and 2-factorizations of a graph</title><categories>math.CO cs.DM</categories><comments>8 pages,1 figure,1 Algorism</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spanning subgraph of a graph G is called a [0,2]-factor of G, if for . is a
union of some disjoint cycles, paths and isolate vertices, that span the graph
G. It is easy to get a [0,2]-factor of G and there would be many of
[0,2]-factors for a G.A characteristic number for a [0,2]-factor, which reflect
the number of the paths and isolate vertices in it,is defineted. The
[0,2]-factor of G is called maximum if its characteristic number is minimum,
and is called characteristic number of G.It to be proved that characteristic
number of graph is a graph invariant and a polynomial time algorithm for
computing a maximum [0,2]-factor of a graph G has been given in this paper.
  A [0,2]-factor is Called a 2-factor, if its characteristic number is zero.
That is, a 2-factor is a set of some disjoint cycles, that span G.We propose a
A polynomial time algorism for computing 2-factor from a [0,2]-factor,which can
be got easily.
  A HAMILTON Cycle is a 2-factor, therefore a necessary condition of a HAMILTON
Graph is that, the graph have a 2-factor or the characteristic number of the
graph is zero. The algorism, given in this paper, make it possible to examine
the condition in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0414</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0414</id><created>2011-11-02</created><authors><author><keyname>Boskos</keyname><forenames>D.</forenames></author><author><keyname>Tsinias</keyname><forenames>J.</forenames></author></authors><title>Sufficient Conditions on the Existence of Switching Observers for
  Nonlinear Time-Varying Systems</title><categories>math.OC cs.SY</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive sufficient conditions for the solvability of the observer design
problem for a wide class of nonlinear time-varying systems, including those
having triangular structure. We establish that, under weaker assumptions than
those imposed in the existing works in the literature, it is possible to
construct a switching sequence of time-varying noncausal dynamics, exhibiting
the state determination of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0422</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0422</id><created>2011-11-02</created><authors><author><keyname>Kilpel&#xe4;inen</keyname><forenames>Pekka</forenames></author></authors><title>Inclusion of Unambiguous RE#s is NP-Hard</title><categories>cs.CC</categories><comments>2 pages; unpublished; &quot;Unambiguity&quot; is also known as &quot;weak
  determinism&quot;; The paper actually shows the co-NP-hardness of the problem</comments><acm-class>F.2.2; F.4.3; H.2.1; I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that testing inclusion between languages represented by regular
expressions with numerical occurrence indicators (RE#s) is NP-hard, even if the
expressions satisfy the requirement of &quot;unambiguity&quot;, which is required for XML
Schema content model expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0427</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0427</id><created>2011-11-02</created><updated>2011-11-02</updated><authors><author><keyname>Sarkar</keyname><forenames>Bidyut Biman</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author></authors><title>A Stochastic Net Model for Controlling Bullwhip Effect in Virtual
  Multi-Tier Retail Network</title><categories>cs.OH</categories><comments>The paper has been withdrawn as the authors have found some
  inadvertent but, serious gaps in the content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supply Chain operation is an integrated business process starting from
primary supplier to end user and the process produce products, services and
information. A successful chain will explore technology, lean operations, and
quality management by adding value for customers and stakeholders. It is a
strategic alliance among the partnering enterprises without geographical
boundary. Every chain has its own unique set of market demands and operating
challenges. Retailing is one such service domain of Supply Chain vulnerable to
bullwhip effects. Demand uncertainty is one of the root causes of Bullwhip
effects. This paper calls for modeling of a demand driven multi-tier stochastic
Retail Chain to work against the Bullwhip effect. The proposed model of the
operational chain will ensure significant return of share to the retailer
through the sophisticated transaction management, real-time inventory
management and the ability to track all inventory movements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0432</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0432</id><created>2011-11-02</created><updated>2011-11-03</updated><authors><author><keyname>Lee</keyname><forenames>Sangkyun</forenames></author><author><keyname>Wright</keyname><forenames>Stephen J.</forenames></author></authors><title>Approximate Stochastic Subgradient Estimation Training for Support
  Vector Machines</title><categories>cs.LG cs.AI</categories><comments>An extended version of the ICPRAM 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subgradient algorithms for training support vector machines have been quite
successful for solving large-scale and online learning problems. However, they
have been restricted to linear kernels and strongly convex formulations. This
paper describes efficient subgradient approaches without such limitations. Our
approaches make use of randomized low-dimensional approximations to nonlinear
kernels, and minimization of a reduced primal formulation using an algorithm
based on robust stochastic approximation, which do not require strong
convexity. Experiments illustrate that our approaches produce solutions of
comparable prediction accuracy with the solutions acquired from existing SVM
solvers, but often in much shorter time. We also suggest efficient prediction
schemes that depend only on the dimension of kernel approximation, not on the
number of support vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0434</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0434</id><created>2011-11-02</created><updated>2011-11-10</updated><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Fertin</keyname><forenames>Guillaume</forenames></author><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>Pancake Flipping is Hard</title><categories>cs.CC cs.DS</categories><comments>Corrected references</comments><doi>10.1007/978-3-642-32589-2_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pancake Flipping is the problem of sorting a stack of pancakes of different
sizes (that is, a permutation), when the only allowed operation is to insert a
spatula anywhere in the stack and to flip the pancakes above it (that is, to
perform a prefix reversal). In the burnt variant, one side of each pancake is
marked as burnt, and it is required to finish with all pancakes having the
burnt side down. Computing the optimal scenario for any stack of pancakes and
determining the worst-case stack for any stack size have been challenges over
more than three decades. Beyond being an intriguing combinatorial problem in
itself, it also yields applications, e.g. in parallel computing and
computational biology. In this paper, we show that the Pancake Flipping
problem, in its original (unburnt) variant, is NP-hard, thus answering the
long-standing question of its computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0444</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0444</id><created>2011-11-02</created><updated>2012-07-09</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Kaibel</keyname><forenames>Volker</forenames></author><author><keyname>Pashkovich</keyname><forenames>Kanstantsin</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author></authors><title>Combinatorial Bounds on Nonnegative Rank and Extended Formulations</title><categories>math.CO cs.DM</categories><comments>Revision, 25pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extended formulation of a polytope P is a polytope Q which can be
projected onto P. Extended formulations of small size (i.e., number of facets)
are of interest, as they allow to model corresponding optimization problems as
linear programs of small sizes.
  The main known lower bounds on the minimum sizes of extended formulations for
fixed polytope P (Yannakakis 1991) are closely related to the concept of
nondeterministic communication complexity. We study the relative power and
limitations of the bounds on several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0466</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0466</id><created>2011-11-02</created><authors><author><keyname>Bronstein</keyname><forenames>Michael M</forenames></author></authors><title>Kernel diff-hash</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a kernel formulation of the recently introduced diff-hash
algorithm for the construction of similarity-sensitive hash functions. Our
kernel diff-hash algorithm that shows superior performance on the problem of
image feature descriptor matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0476</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0476</id><created>2011-11-02</created><authors><author><keyname>Skrzypczak</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Equational theories of profinite structures</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a general way of constructing profinite struc-
tures based on a given framework - a countable family of objects and a
countable family of recognisers (e.g. formulas). The main theorem states:
  A subset of a family of recognisable sets is a lattice if and only if it is
definable by a family of profinite equations.
  This result extends Theorem 5.2 from [GGEP08] expressed only for finite words
and morphisms to finite monoids. One of the applications of our theorem is the
situation where objects are finite relational structures and recognisers are
first order sentences. In that setting a simple characterisation of lattices of
first order formulas arise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0492</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0492</id><created>2011-11-02</created><authors><author><keyname>Kuperberg</keyname><forenames>Greg</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Peled</keyname><forenames>Ron</forenames></author></authors><title>Probabilistic existence of rigid combinatorial structures</title><categories>math.CO cs.CC math.PR</categories><comments>Extended abstract for STOC 2012</comments><msc-class>05B30, 60C05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the existence of rigid combinatorial objects which previously were
not known to exist. Specifically, for a wide range of the underlying
parameters, we show the existence of non-trivial orthogonal arrays,
$t$-designs, and $t$-wise permutations. In all cases, the sizes of the objects
are optimal up to polynomial overhead. The proof of existence is probabilistic.
We show that a randomly chosen such object has the required properties with
positive yet tiny probability. The main technical ingredient is a special local
central limit theorem for suitable lattice random walks with finitely many
steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0499</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0499</id><created>2011-11-01</created><authors><author><keyname>Grimson</keyname><forenames>Rafael</forenames></author><author><keyname>Heintz</keyname><forenames>Joos</forenames></author><author><keyname>Kuijpers</keyname><forenames>Bart</forenames></author></authors><title>Evaluating geometric queries using few arithmetic operations</title><categories>cs.DS cs.DB math.AG</categories><msc-class>68P15, 68Q25, 14P99, 14Q99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\cp:=(P_1,...,P_s)$ be a given family of $n$-variate polynomials with
integer coefficients and suppose that the degrees and logarithmic heights of
these polynomials are bounded by $d$ and $h$, respectively. Suppose furthermore
that for each $1\leq i\leq s$ the polynomial $P_i$ can be evaluated using $L$
arithmetic operations (additions, subtractions, multiplications and the
constants 0 and 1). Assume that the family $\cp$ is in a suitable sense
\emph{generic}. We construct a database $\cal D$, supported by an algebraic
computation tree, such that for each $x\in [0,1]^n$ the query for the signs of
$P_1(x),...,P_s(x)$ can be answered using $h d^{\cO(n^2)}$ comparisons and $nL$
arithmetic operations between real numbers. The arithmetic-geometric tools
developed for the construction of $\cal D$ are then employed to exhibit example
classes of systems of $n$ polynomial equations in $n$ unknowns whose
consistency may be checked using only few arithmetic operations, admitting
however an exponential number of comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0500</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0500</id><created>2011-11-02</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author><author><keyname>Jebens</keyname><forenames>Kristof</forenames></author></authors><title>Development of a Cost-efficient Autonomous MAV for an Unstructured
  Indoor Environment</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing rescuing and surveillance operations with autonomous ground and
aerial vehicles become more and more apparent task. Involving unmanned robot
systems allows making these operations more efficient, safe and reliable
especially in hazardous areas. This work is devoted to the development of a
cost-efficient micro aerial vehicle in a quadrocopter shape for developmental
purposes within indoor scenarios. It has been constructed with off-the-shelf
components available for mini helicopters. Additional sensors and electronics
are incorporated into this aerial vehicle to stabilize its flight behavior and
to provide a capability of an autonomous navigation in a partially unstructured
indoor environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0503</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0503</id><created>2011-11-02</created><authors><author><keyname>Pantisano</keyname><forenames>Francesco</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Spectrum Leasing as an Incentive towards Uplink Macrocell and Femtocell
  Cooperation</title><categories>cs.GT</categories><comments>29 pages, 11 figures, accepted at the IEEE JSAC on Femtocell Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of femtocell access points underlaying existing communication
infrastructure has recently emerged as a key technology that can significantly
improve the coverage and performance of next-generation wireless networks. In
this paper, we propose a framework for macrocell-femtocell cooperation under a
closed access policy, in which a femtocell user may act as a relay for
macrocell users. In return, each cooperative macrocell user grants the
femtocell user a fraction of its superframe. We formulate a coalitional game
with macrocell and femtocell users being the players, which can take individual
and distributed decisions on whether to cooperate or not, while maximizing a
utility function that captures the cooperative gains, in terms of throughput
and delay.We show that the network can selforganize into a partition composed
of disjoint coalitions which constitutes the recursive core of the game
representing a key solution concept for coalition formation games in partition
form. Simulation results show that the proposed coalition formation algorithm
yields significant gains in terms of average rate per macrocell user, reaching
up to 239%, relative to the non-cooperative case. Moreover, the proposed
approach shows an improvement in terms of femtocell users' rate of up to 21%
when compared to the traditional closed access policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0508</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0508</id><created>2011-11-02</created><updated>2012-08-27</updated><authors><author><keyname>Janssen</keyname><forenames>Jeannette</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author><author><keyname>Wilson</keyname><forenames>Rory</forenames></author></authors><title>Geometric Graph Properties of the Spatial Preferred Attachment model</title><categories>cs.SI math.CO physics.soc-ph</categories><journal-ref>Advances in Applied Mathematics, published on-line, 2012</journal-ref><doi>10.1016/j.aam.2012.09.001,</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial preferred attachment (SPA) model is a model for networked
information spaces such as domains of the World Wide Web, citation graphs, and
on-line social networks. It uses a metric space to model the hidden attributes
of the vertices. Thus, vertices are elements of a metric space, and link
formation depends on the metric distance between vertices. We show, through
theoretical analysis and simulation, that for graphs formed according to the
SPA model it is possible to infer the metric distance between vertices from the
link structure of the graph. Precisely, the estimate is based on the number of
common neighbours of a pair of vertices, a measure known as {\sl co-citation}.
To be able to calculate this estimate, we derive a precise relation between the
number of common neighbours and metric distance. We also analyze the
distribution of {\sl edge lengths}, where the length of an edge is the metric
distance between its end points. We show that this distribution has three
different regimes, and that the tail of this distribution follows a power law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0524</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0524</id><created>2011-11-02</created><updated>2012-03-11</updated><authors><author><keyname>Evans</keyname><forenames>R. F. L.</forenames></author><author><keyname>Chantrell</keyname><forenames>R. W.</forenames></author><author><keyname>Nowak</keyname><forenames>U.</forenames></author><author><keyname>Lyberatos</keyname><forenames>A.</forenames></author><author><keyname>Richter</keyname><forenames>H-J.</forenames></author></authors><title>Thermally induced error: density limit for magnetic data storage</title><categories>cond-mat.mtrl-sci cs.ET physics.comp-ph</categories><comments>Improved manuscript for readability</comments><doi>10.1063/1.3691196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic data storage is pervasive in the preservation of digital information
and the rapid pace of computer development requires ever more capacity.
Increasing the storage density for magnetic hard disk drives requires a reduced
bit size, previously thought to be limited by the thermal stability of the
constituent magnetic grains. The limiting storage density in magnetic recording
is investigated treating the writing of bits as a thermodynamic process. A
'thermal writability' factor is introduced and it is shown that storage
densities will be limited to 15 to 20 TBit/in^2 unless technology can move
beyond the currently available write field magnitudes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0547</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0547</id><created>2011-11-02</created><updated>2011-12-02</updated><authors><author><keyname>Messerschmitt</keyname><forenames>David G.</forenames></author></authors><title>Interstellar Communication: The Case for Spread Spectrum</title><categories>astro-ph.IM cs.IT math.IT physics.pop-ph</categories><journal-ref>Acta Astronautica, Dec 2012, Vol 81, Issue 1, p. 227</journal-ref><doi>10.1016/j.actaastro.2012.07.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spread spectrum, widely employed in modern digital wireless terrestrial radio
systems, chooses a signal with a noise-like character and much higher bandwidth
than necessary. This paper advocates spread spectrum modulation for
interstellar communication, motivated by robust immunity to radio-frequency
interference (RFI) of technological origin in the vicinity of the receiver
while preserving full detection sensitivity in the presence of natural sources
of noise. Receiver design for noise immunity alone provides no basis for
choosing a signal with any specific character, therefore failing to reduce
ambiguity. By adding RFI to noise immunity as a design objective, the
conjunction of choice of signal (by the transmitter) together with optimum
detection for noise immunity (in the receiver) leads through simple
probabilistic argument to the conclusion that the signal should possess the
statistical properties of a burst of white noise, and also have a large
time-bandwidth product. Thus spread spectrum also provides an implicit
coordination between transmitter and receiver by reducing the ambiguity as to
the signal character. This strategy requires the receiver to guess the specific
noise-like signal, and it is contended that this is feasible if an appropriate
pseudorandom signal is generated algorithmically. For example, conceptually
simple algorithms like the binary expansion of common irrational numbers like
Pi are shown to be suitable. Due to its deliberately wider bandwidth, spread
spectrum is more susceptible to dispersion and distortion in propagation
through the interstellar medium, desirably reducing ambiguity in parameters
like bandwidth and carrier frequency. This suggests a promising new direction
in interstellar communication using spread spectrum modulation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0554</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0554</id><created>2011-11-02</created><updated>2012-06-10</updated><authors><author><keyname>Ehsani</keyname><forenames>Shayan</forenames></author><author><keyname>Fadaee</keyname><forenames>Saber Shokat</forenames></author><author><keyname>Fazli</keyname><forenames>MohammadAmin</forenames></author><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Sadeghabad</keyname><forenames>Sina Sadeghian</forenames></author><author><keyname>Safari</keyname><forenames>MohammadAli</forenames></author><author><keyname>Saghafian</keyname><forenames>Morteza</forenames></author></authors><title>On a Bounded Budget Network Creation Game</title><categories>cs.GT</categories><comments>28 pages, 3 figures, preliminary version appeared in SPAA'11</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>ACM Transactions on Algorithms (2015), 11(4), article 34</journal-ref><doi>10.1145/2701615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network creation game in which each player (vertex) has a fixed
budget to establish links to other players. In our model, each link has unit
price and each agent tries to minimize its cost, which is either its local
diameter or its total distance to other players in the (undirected) underlying
graph of the created network. Two versions of the game are studied: in the MAX
version, the cost incurred to a vertex is the maximum distance between the
vertex and other vertices, and in the SUM version, the cost incurred to a
vertex is the sum of distances between the vertex and other vertices. We prove
that in both versions pure Nash equilibria exist, but the problem of finding
the best response of a vertex is NP-hard. We take the social cost of the
created network to be its diameter, and next we study the maximum possible
diameter of an equilibrium graph with n vertices in various cases. When the sum
of players' budgets is n-1, the equilibrium graphs are always trees, and we
prove that their maximum diameter is Theta(n) and Theta(log n) in MAX and SUM
versions, respectively. When each vertex has unit budget (i.e. can establish
link to just one vertex), the diameter of any equilibrium graph in either
version is Theta(1). We give examples of equilibrium graphs in the MAX version,
such that all vertices have positive budgets and yet the diameter is
Omega(sqrt(log n)). This interesting (and perhaps counter-intuitive) result
shows that increasing the budgets may increase the diameter of equilibrium
graphs and hence deteriorate the network structure. Then we prove that every
equilibrium graph in the SUM version has diameter 2^O(sqrt(log n)). Finally, we
show that if the budget of each player is at least k, then every equilibrium
graph in the SUM version is k-connected or has diameter smaller than 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0562</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0562</id><created>2011-11-02</created><authors><author><keyname>Espa&#xf1;a</keyname><forenames>Sergio</forenames></author><author><keyname>Condori</keyname><forenames>Nelly</forenames></author><author><keyname>Wieringa</keyname><forenames>Roel</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Arturo</forenames></author><author><keyname>Pastor</keyname><forenames>&#xd3;scar</forenames></author></authors><title>Model-driven system development: Experimental design and report of the
  pilot experiment</title><categories>cs.SE</categories><comments>83 pages, 45 figures, 16 tables</comments><report-no>ProS-TR-2011-12</report-no><msc-class>68N01</msc-class><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes de design of an experiment that intends to compare two
variants of a modeldriven system development method, so as to assess the impact
of requirements engineering practice in the quality of the conceptual models.
The conceptual modelling method being assessed is the OO-Method [Pastor and
Molina 2007]. One of its variants includes Communication Analysis, a
communication-oriented requirements engineering method [Espa\~na, Gonz\'alez et
al. 2009] and a set of guidelines to derive conceptual models from requirements
models [Espa\~na, Ruiz et al. 2011; Gonz\'alez, Espa\~na et al. 2011]. The
other variant is an ad-hoc, text-based requirements practice similar to the one
that is applied in industrial projects by OO-Method practitioners. The goal of
the research, summarised according to the Goal/Question/Metric template [Basili
and Rombach 1988], is to:
  *) analyse the resulting models of two model-based information systems
analysis method variants; namely, the OO-Method (OOM) and the integration of
Communication Analysis and the OO-Method (CA+OOM),
  *) for the purpose of carrying out a comparative evaluation
  *) with respect to performance of the subject and acceptance of the method;
  *) from the viewpoint of the information systems researcher
  *) in the context of bachelor students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0567</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0567</id><created>2011-11-02</created><updated>2013-04-04</updated><authors><author><keyname>Bae</keyname><forenames>Jungyun</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>A Primal Dual Algorithm for a Heterogeneous Traveling Salesman Problem</title><categories>cs.DM cs.DS cs.RO math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surveillance applications require a collection of heterogeneous vehicles to
visit a set of targets. In this article, we consider a fundamental routing
problem that arises in these applications involving two vehicles. Specifically,
we consider a routing problem where there are two heterogeneous vehicles that
start from distinct initial locations, and a set of targets. The objective is
to find a tour for each vehicle such that each of the targets is visited at
least once by a vehicle and the sum of the distances traveled by the vehicles
is a minimum. We present a primal-dual algorithm for a variant of this routing
problem that provides an approximation ratio of 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0570</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0570</id><created>2011-11-02</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Clique cover and graph separation: New incompressibility results</title><categories>cs.DS cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of kernelization studies polynomial-time preprocessing routines for
hard problems in the framework of parameterized complexity. Although a
framework for proving kernelization lower bounds has been discovered in 2008
and successfully applied multiple times over the last three years, establishing
kernelization complexity of many important problems remains open. In this paper
we show that, unless NP is a subset of coNP/poly and the polynomial hierarchy
collapses up to its third level, the following parameterized problems do not
admit a polynomial-time preprocessing algorithm that reduces the size of an
instance to polynomial in the parameter:
  - EDGE CLIQUE COVER, parameterized by the number of cliques,
  - DIRECTED EDGE/VERTEX MULTIWAY CUT, parameterized by the size of the cutset,
even in the case of two terminals,
  - EDGE/VERTEX MULTICUT, parameterized by the size of the cutset, and
  - k-WAY CUT, parameterized by the size of the cutset.
  The existence of a polynomial kernelization for EDGE CLIQUE COVER was a
seasoned veteran in open problem sessions. Furthermore, our results complement
very recent developments in designing parameterized algorithms for cut problems
by Marx and Razgon [STOC'11], Bousquet et al. [STOC'11], Kawarabayashi and
Thorup [FOCS'11] and Chitnis et al. [SODA'12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0582</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0582</id><created>2011-11-02</created><authors><author><keyname>Agrawal</keyname><forenames>Manindra</forenames></author><author><keyname>Saha</keyname><forenames>Chandan</forenames></author><author><keyname>Saptharishi</keyname><forenames>Ramprasad</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Jacobian hits circuits: Hitting-sets, lower bounds for depth-D occur-k
  formulas &amp; depth-3 transcendence degree-k circuits</title><categories>cs.CC math.AC</categories><msc-class>68W30</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a single, common tool to strictly subsume all known cases of
polynomial time blackbox polynomial identity testing (PIT) that have been
hitherto solved using diverse tools and techniques. In particular, we show that
polynomial time hitting-set generators for identity testing of the two
seemingly different and well studied models - depth-3 circuits with bounded top
fanin, and constant-depth constant-read multilinear formulas - can be
constructed using one common algebraic-geometry theme: Jacobian captures
algebraic independence. By exploiting the Jacobian, we design the first
efficient hitting-set generators for broad generalizations of the
above-mentioned models, namely:
  (1) depth-3 (Sigma-Pi-Sigma) circuits with constant transcendence degree of
the polynomials computed by the product gates (no bounded top fanin
restriction), and (2) constant-depth constant-occur formulas (no multilinear
restriction).
  Constant-occur of a variable, as we define it, is a much more general concept
than constant-read. Also, earlier work on the latter model assumed that the
formula is multilinear. Thus, our work goes further beyond the results obtained
by Saxena &amp; Seshadhri (STOC 2011), Saraf &amp; Volkovich (STOC 2011), Anderson et
al. (CCC 2011), Beecken et al. (ICALP 2011) and Grenet et al. (FSTTCS 2011),
and brings them under one unifying technique.
  In addition, using the same Jacobian based approach, we prove exponential
lower bounds for the immanant (which includes permanent and determinant) on the
same depth-3 and depth-4 models for which we give efficient PIT algorithms. Our
results reinforce the intimate connection between identity testing and lower
bounds by exhibiting a concrete mathematical tool - the Jacobian - that is
equally effective in solving both the problems on certain interesting and
previously well-investigated (but not well understood) models of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0583</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0583</id><created>2011-11-02</created><updated>2012-02-17</updated><authors><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Information Spreading in Dynamic Graphs</title><categories>cs.DM cs.DC</categories><msc-class>68, 60</msc-class><acm-class>G.2.2; G.3; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general approach to study the flooding time (a measure of how
fast information spreads) in dynamic graphs (graphs whose topology changes with
time according to a random process).
  We consider arbitrary converging Markovian dynamic graph process, that is,
processes in which the topology of the graph at time $t$ depends only on its
topology at time $t-1$ and which have a unique stationary distribution. The
most well studied models of dynamic graphs are all Markovian and converging.
  Under general conditions, we bound the flooding time in terms of the mixing
time of the dynamic graph process. We recover, as special cases of our result,
bounds on the flooding time for the \emph{random trip} model and the
\emph{random path} models; previous analysis techniques provided bounds only in
restricted settings for such models. Our result also provides the first bound
for the \emph{random waypoint} model (which is tight for certain ranges of
parameters) whose analysis had been an important open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0594</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0594</id><created>2011-11-02</created><authors><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author></authors><title>Exploring Oracle RDBMS latches using Solaris DTrace</title><categories>cs.DB cs.DC cs.PF</categories><comments>14 pages, 6 figures, 6 tables. MEDIAS 2011 Conference. Limassol,
  Cyprus</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rise of hundreds cores technologies bring again to the first plan the problem
of interprocess synchronization in database engines. Spinlocks are widely used
in contemporary DBMS to synchronize processes at microsecond timescale. Latches
are Oracle RDBMS specific spinlocks. The latch contention is common to observe
in contemporary high concurrency OLTP environments.
  In contrast to system spinlocks used in operating systems kernels, latches
work in user context. Such user level spinlocks are influenced by context
preemption and multitasking. Until recently there were no direct methods to
measure effectiveness of user spinlocks. This became possible with the
emergence of Solaris 10 Dynamic Tracing framework. DTrace allows tracing and
profiling both OS and user applications.
  This work investigates the possibilities to diagnose and tune Oracle latches.
It explores the contemporary latch realization and spinning-blocking
strategies, analyses corresponding statistic counters.
  A mathematical model developed to estimate analytically the effect of tuning
_SPIN_COUNT value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0595</identifier>
 <datestamp>2011-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0595</id><created>2011-11-02</created><authors><author><keyname>Huang</keyname><forenames>Shurui</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>An achievable region for the double unicast problem based on a minimum
  cut analysis</title><categories>cs.IT math.IT</categories><comments>ITW, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multiple unicast problem under network coding over directed
acyclic networks when there are two source-terminal pairs, $s_1-t_1$ and
$s_2-t_2$. Current characterizations of the multiple unicast capacity region in
this setting have a large number of inequalities, which makes them hard to
explicitly evaluate. In this work we consider a slightly different problem. We
assume that we only know certain minimum cut values for the network, e.g.,
mincut$(S_i, T_j)$, where $S_i \subseteq \{s_1, s_2\}$ and $T_j \subseteq
\{t_1, t_2\}$ for different subsets $S_i$ and $T_j$. Based on these values, we
propose an achievable rate region for this problem based on linear codes.
Towards this end, we begin by defining a base region where both sources are
multicast to both the terminals. Following this we enlarge the region by
appropriately encoding the information at the source nodes, such that terminal
$t_i$ is only guaranteed to decode information from the intended source $s_i$,
while decoding a linear function of the other source. The rate region takes
different forms depending upon the relationship of the different cut values in
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0623</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0623</id><created>2011-11-02</created><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Beating Randomized Response on Incoherent Matrices</title><categories>cs.DS</categories><doi>10.1145/2213977.2214088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing accurate low rank approximations of large matrices is a fundamental
data mining task. In many applications however the matrix contains sensitive
information about individuals. In such case we would like to release a low rank
approximation that satisfies a strong privacy guarantee such as differential
privacy. Unfortunately, to date the best known algorithm for this task that
satisfies differential privacy is based on naive input perturbation or
randomized response: Each entry of the matrix is perturbed independently by a
sufficiently large random noise variable, a low rank approximation is then
computed on the resulting matrix.
  We give (the first) significant improvements in accuracy over randomized
response under the natural and necessary assumption that the matrix has low
coherence. Our algorithm is also very efficient and finds a constant rank
approximation of an m x n matrix in time O(mn). Note that even generating the
noise matrix required for randomized response already requires time O(mn).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0627</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0627</id><created>2011-11-01</created><authors><author><keyname>Barnat</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Bauch</keyname><forenames>Petr</forenames></author><author><keyname>Brim</keyname><forenames>Lubo&#x161;</forenames></author><author><keyname>&#x10c;e&#x161;ka</keyname><forenames>Milan</forenames></author></authors><title>Computing Optimal Cycle Mean in Parallel on CUDA</title><categories>cs.DC cs.DS</categories><comments>In Proceedings PDMC 2011, arXiv:1111.0064</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 72, 2011, pp. 68-83</journal-ref><doi>10.4204/EPTCS.72.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of optimal cycle mean in a directed weighted graph has many
applications in program analysis, performance verification in particular. In
this paper we propose a data-parallel algorithmic solution to the problem and
show how the computation of optimal cycle mean can be efficiently accelerated
by means of CUDA technology. We show how the problem of computation of optimal
cycle mean is decomposed into a sequence of data-parallel graph computation
primitives and show how these primitives can be implemented and optimized for
CUDA computation. Finally, we report a fivefold experimental speed up on graphs
representing models of distributed systems when compared to best sequential
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0640</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0640</id><created>2011-11-02</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Nayel</keyname><forenames>Hamada A.</forenames></author></authors><title>Partial Redundancy Elimination for Multi-threaded Programs</title><categories>cs.SE cs.LO</categories><comments>7 pages</comments><journal-ref>IJCSNS International Journal of Computer Science and Network
  Security, Vol.11 No.10, October 2011, 127-133</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-threaded programs have many applications which are widely used such as
operating systems. Analyzing multi-threaded programs differs from sequential
ones; the main feature is that many threads execute at the same time. The
effect of all other running threads must be taken in account. Partial
redundancy elimination is among the most powerful compiler optimizations: it
performs loop-invariant code motion and common subexpression elimination. We
present a type system with optimization component which performs partial
redundancy elimination for multi-threaded programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0650</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0650</id><created>2011-11-02</created><authors><author><keyname>Durand</keyname><forenames>Fabien</forenames><affiliation>LAMFA</affiliation></author></authors><title>HD0L-$\omega$-equivalence and periodicity problems in the primitive case
  (to the memory of G. Rauzy)</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper I would like to witness the mathematical inventiveness of G.
Rauzy through personnal exchanges I had with him. The objects that will emerge
will be used to treat the decidability of the HD 0 L $\omega$-equivalence and
periodicity problems in the primitive case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0654</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0654</id><created>2011-11-02</created><updated>2012-06-19</updated><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Distributed Lossy Source Coding Using Real-Number Codes</title><categories>cs.IT cs.CV cs.NI math.IT</categories><comments>5 pages, 5 figures, to appear in VTC_Fall 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how real-number codes can be used to compress correlated sources, and
establish a new framework for lossy distributed source coding, in which we
quantize compressed sources instead of compressing quantized sources. This
change in the order of binning and quantization blocks makes it possible to
model correlation between continuous-valued sources more realistically and
correct quantization error when the sources are completely correlated. The
encoding and decoding procedures are described in detail, for discrete Fourier
transform (DFT) codes. Reconstructed signal, in the mean squared error sense,
is seen to be better than that in the conventional approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0663</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0663</id><created>2011-11-02</created><authors><author><keyname>Forbes</keyname><forenames>Michael A.</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>On Identity Testing of Tensors, Low-rank Recovery and Compressed Sensing</title><categories>cs.CC cs.IT math.IT</categories><comments>55 pages</comments><journal-ref>Proceedings of the 44th Symposium on Theory of Computing (2012),
  163-172</journal-ref><doi>10.1145/2213977.2213995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of obtaining efficient, deterministic, black-box
polynomial identity testing algorithms for depth-3 set-multilinear circuits
(over arbitrary fields). This class of circuits has an efficient,
deterministic, white-box polynomial identity testing algorithm (due to Raz and
Shpilka), but has no known such black-box algorithm. We recast this problem as
a question of finding a low-dimensional subspace H, spanned by rank 1 tensors,
such that any non-zero tensor in the dual space ker(H) has high rank. We obtain
explicit constructions of essentially optimal-size hitting sets for tensors of
degree 2 (matrices), and obtain quasi-polynomial sized hitting sets for
arbitrary tensors (but this second hitting set is less explicit).
  We also show connections to the task of performing low-rank recovery of
matrices, which is studied in the field of compressed sensing. Low-rank
recovery asks (say, over the reals) to recover a matrix M from few
measurements, under the promise that M is rank &lt;=r. We also give a formal
connection between low-rank recovery and the task of sparse (vector) recovery:
any sparse-recovery algorithm that exactly recovers vectors of length n and
sparsity 2r, using m non-adaptive measurements, yields a low-rank recovery
scheme for exactly recovering nxn matrices of rank &lt;=r, making 2nm non-adaptive
measurements. Furthermore, if the sparse-recovery algorithm runs in time \tau,
then the low-rank recovery algorithm runs in time O(rn^2+n\tau). We obtain this
reduction using linear-algebraic techniques, and not using convex optimization,
which is more commonly seen in compressed sensing algorithms. By using a dual
Reed-Solomon code, we are able to (deterministically) construct low-rank
recovery schemes taking 4nr measurements over the reals, such that the
measurements can be all rank-1 matrices, or all sparse matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0670</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0670</id><created>2011-11-02</created><updated>2012-02-21</updated><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>D'Antoni</keyname><forenames>Loris</forenames></author><author><keyname>Deshmukh</keyname><forenames>Jyotirmoy V.</forenames></author><author><keyname>Raghothaman</keyname><forenames>Mukund</forenames></author><author><keyname>Yuan</keyname><forenames>Yifei</forenames></author></authors><title>Regular Functions, Cost Register Automata, and Generalized Min-Cost
  Problems</title><categories>cs.FL cs.LO</categories><comments>ICALP12 submission, technical report/extended version. 33 pages+title
  page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the successful application of the theory of regular languages to
formal verification of finite-state systems, there is a renewed interest in
developing a theory of analyzable functions from strings to numerical values
that can provide a foundation for analyzing {\em quantitative} properties of
finite-state systems. In this paper, we propose a deterministic model for
associating costs with strings that is parameterized by operations of interest
(such as addition, scaling, and $\min$), a notion of {\em regularity} that
provides a yardstick to measure expressiveness, and study decision problems and
theoretical properties of resulting classes of cost functions. Our definition
of regularity relies on the theory of string-to-tree transducers, and allows
associating costs with events that are conditional upon regular properties of
future events. Our model of {\em cost register automata} allows computation of
regular functions using multiple &quot;write-only&quot; registers whose values can be
combined using the allowed set of operations. We show that classical
shortest-path algorithms as well as algorithms designed for computing {\em
discounted costs}, can be adopted for solving the min-cost problems for the
more general classes of functions specified in our model. Cost register
automata with $\min$ and increment give a deterministic model that is
equivalent to {\em weighted automata}, an extensively studied nondeterministic
model, and this connection results in new insights and new open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0674</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0674</id><created>2011-11-02</created><updated>2014-08-20</updated><authors><author><keyname>Foniok</keyname><forenames>Jan</forenames><affiliation>Queen's University</affiliation></author></authors><title>On Ramsey properties of classes with forbidden trees</title><categories>math.CO cs.DM</categories><comments>Keywords: forbidden substructure; amalgamation; Ramsey class; partite
  method v2: changed definition of expanded class; v3: final version</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  21, 2014) lmcs:796</journal-ref><doi>10.2168/LMCS-10(3:9)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let F be a set of relational trees and let Forbh(F) be the class of all
structures that admit no homomorphism from any tree in F; all this happens over
a fixed finite relational signature $\sigma$. There is a natural way to expand
Forbh(F) by unary relations to an amalgamation class. This expanded class,
enhanced with a linear ordering, has the Ramsey property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0683</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0683</id><created>2011-11-02</created><authors><author><keyname>Nabi-Abdolyousefi</keyname><forenames>Marzieh</forenames></author><author><keyname>Mesbahi</keyname><forenames>Mehran</forenames></author></authors><title>A Sieve Method for Consensus-type Network Tomography</title><categories>math.OC cs.SY</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we examine the problem of identifying the interaction geometry
among a known number of agents, adopting a consensus-type algorithm for their
coordination. The proposed identification process is facilitated by introducing
&quot;ports&quot; for stimulating a subset of network vertices via an appropriately
defined interface and observing the network's response at another set of
vertices. It is first noted that under the assumption of controllability and
observability of corresponding steered-and-observed network, the proposed
procedure identifies a number of important features of the network using the
spectrum of the graph Laplacian. We then proceed to use degree-based graph
reconstruction methods to propose a sieve method for further characterization
of the underlying network. An example demonstrates the application of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0689</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0689</id><created>2011-11-02</created><updated>2012-12-01</updated><authors><author><keyname>Jiang</keyname><forenames>Jinjing</forenames></author><author><keyname>Marukala</keyname><forenames>Neeharika</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author></authors><title>Symmetrical Multilevel Diversity Coding and Subset Entropy Inequalities</title><categories>cs.IT math.IT</categories><comments>44 pages, 5 figures. Major revision in November 2012. Revised draft
  submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetrical multilevel diversity coding (SMDC) is a classical model for
coding over distributed storage. In this setting, a simple separate encoding
strategy known as superposition coding was shown to be optimal in terms of
achieving the minimum sum rate (Roche, Yeung, and Hau, 1997) and the entire
admissible rate region (Yeung and Zhang, 1999) of the problem. The proofs
utilized carefully constructed induction arguments, for which the classical
subset entropy inequality of Han (1978) played a key role. This paper includes
two parts. In the first part the existing optimality proofs for classical SMDC
are revisited, with a focus on their connections to subset entropy
inequalities. First, a new sliding-window subset entropy inequality is
introduced and then used to establish the optimality of superposition coding
for achieving the minimum sum rate under a weaker source-reconstruction
requirement. Second, a subset entropy inequality recently proved by Madiman and
Tetali (2010) is used to develop a new structural understanding to the proof of
Yeung and Zhang on the optimality of superposition coding for achieving the
entire admissible rate region. Building on the connections between classical
SMDC and the subset entropy inequalities developed in the first part, in the
second part the optimality of superposition coding is further extended to the
cases where there is either an additional all-access encoder (SMDC-A) or an
additional secrecy constraint (S-SMDC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0700</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0700</id><created>2011-11-02</created><updated>2013-10-09</updated><authors><author><keyname>Tarraf</keyname><forenames>Danielle C.</forenames></author><author><keyname>Bauso</keyname><forenames>Dario</forenames></author></authors><title>Finite Alphabet Control of Logistic Networks with Discrete Uncertainty</title><categories>math.OC cs.SY</categories><journal-ref>Systems &amp; Control Letters, vol.64, pp.20-26, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider logistic networks in which the control and disturbance inputs
take values in finite sets. We derive a necessary and sufficient condition for
the existence of robustly control invariant (hyperbox) sets. We show that a
stronger version of this condition is sufficient to guarantee robust global
attractivity, and we construct a counterexample demonstrating that it is not
necessary. Being constructive, our proofs of sufficiency allow us to extract
the corresponding robust control laws and to establish the invariance of
certain sets. Finally, we highlight parallels between our results and existing
results in the literature, and we conclude our study with two simple
illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0703</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0703</id><created>2011-11-02</created><authors><author><keyname>Zhang</keyname><forenames>Chuan</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Efficient Network for Non-Binary QC-LDPC Decoder</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents approaches to develop efficient network for non-binary
quasi-cyclic LDPC (QC-LDPC) decoders. By exploiting the intrinsic shifting and
symmetry properties of the check matrices, significant reduction of memory size
and routing complexity can be achieved. Two different efficient network
architectures for Class-I and Class-II non-binary QC-LDPC decoders have been
proposed, respectively. Comparison results have shown that for the code of the
64-ary (1260, 630) rate-0.5 Class-I code, the proposed scheme can save more
than 70.6% hardware required by shuffle network than the state-of-the-art
designs. The proposed decoder example for the 32-ary (992, 496) rate-0.5
Class-II code can achieve a 93.8% shuffle network reduction compared with the
conventional ones. Meanwhile, based on the similarity of Class-I and Class-II
codes, similar shuffle network is further developed to incorporate both classes
of codes at a very low cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0704</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0704</id><created>2011-11-02</created><authors><author><keyname>Zhang</keyname><forenames>Chuan</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Reduced-Latency SC Polar Decoder Architectures</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes have become one of the most favorable capacity achieving error
correction codes (ECC) along with their simple encoding method. However, among
the very few prior successive cancellation (SC) polar decoder designs, the
required long code length makes the decoding latency high. In this paper,
conventional decoding algorithm is transformed with look-ahead techniques. This
reduces the decoding latency by 50%. With pipelining and parallel processing
schemes, a parallel SC polar decoder is proposed. Sub-structure sharing
approach is employed to design the merged processing element (PE). Moreover,
inspired by the real FFT architecture, this paper presents a novel input
generating circuit (ICG) block that can generate additional input signals for
merged PEs on-the-fly. Gate-level analysis has demonstrated that the proposed
design shows advantages of 50% decoding latency and twice throughput over the
conventional one with similar hardware cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0705</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0705</id><created>2011-11-02</created><authors><author><keyname>Zhang</keyname><forenames>Chuan</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Low-Latency SC Decoder Architectures for Polar Codes</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays polar codes are becoming one of the most favorable capacity
achieving error correction codes for their low encoding and decoding
complexity. However, due to the large code length required by practical
applications, the few existing successive cancellation (SC) decoder
implementations still suffer from not only the high hardware cost but also the
long decoding latency. This paper presents novel several approaches to design
low-latency decoders for polar codes based on look-ahead techniques. Look-ahead
techniques can be employed to reschedule the decoding process of polar decoder
in numerous approaches. However, among those approaches, only well-arranged
ones can achieve good performance in terms of both latency and hardware
complexity. By revealing the recurrence property of SC decoding chart, the
authors succeed in reducing the decoding latency by 50% with look-ahead
techniques. With the help of VLSI-DSP design techniques such as pipelining,
folding, unfolding, and parallel processing, methodologies for four different
polar decoder architectures have been proposed to meet various application
demands. Sub-structure sharing scheme has been adopted to design the merged
processing element (PE) for further hardware reduction. In addition, systematic
methods for construction refined pipelining decoder (2nd design) and the input
generating circuits (ICG) block have been given. Detailed gate-level analysis
has demonstrated that the proposed designs show latency advantages over
conventional ones with similar hardware cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0706</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0706</id><created>2011-11-02</created><authors><author><keyname>Kerivin</keyname><forenames>Herve</forenames></author><author><keyname>Leblet</keyname><forenames>Jimmy</forenames></author><author><keyname>Simon</keyname><forenames>Gwendal</forenames></author><author><keyname>Zhou</keyname><forenames>Fen</forenames></author></authors><title>Maximum Bounded Rooted-Tree Packing Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph and a root, the Maximum Bounded Rooted-Tree Packing (MBRTP)
problem aims at finding K rooted-trees that span the largest subset of
vertices, when each vertex has a limited outdegree. This problem is motivated
by peer-to-peer streaming overlays in under-provisioned systems. We prove that
the MBRTP problem is NP-complete. We present two polynomial-time algorithms
that computes an optimal solution on complete graphs and trees respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0708</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0708</id><created>2011-11-02</created><updated>2011-11-29</updated><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author></authors><title>Bayesian Causal Induction</title><categories>stat.ML cs.AI</categories><comments>4 pages, 4 figures; 2011 NIPS Workshop on Philosophy and Machine
  Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering causal relationships is a hard task, often hindered by the need
for intervention, and often requiring large amounts of data to resolve
statistical uncertainty. However, humans quickly arrive at useful causal
relationships. One possible reason is that humans extrapolate from past
experience to new, unseen situations: that is, they encode beliefs over causal
invariances, allowing for sound generalization from the observations they
obtain from directly acting in the world.
  Here we outline a Bayesian model of causal induction where beliefs over
competing causal hypotheses are modeled using probability trees. Based on this
model, we illustrate why, in the general case, we need interventions plus
constraints on our causal hypotheses in order to extract causal information
from our experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0711</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0711</id><created>2011-11-02</created><authors><author><keyname>Wang</keyname><forenames>Yige</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan S.</forenames></author></authors><title>Hierarchical and High-Girth QC LDPC Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information THeory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general approach to designing capacity-approaching high-girth
low-density parity-check (LDPC) codes that are friendly to hardware
implementation. Our methodology starts by defining a new class of
&quot;hierarchical&quot; quasi-cyclic (HQC) LDPC codes that generalizes the structure of
quasi-cyclic (QC) LDPC codes. Whereas the parity check matrices of QC LDPC
codes are composed of circulant sub-matrices, those of HQC LDPC codes are
composed of a hierarchy of circulant sub-matrices that are in turn constructed
from circulant sub-matrices, and so on, through some number of levels. We show
how to map any class of codes defined using a protograph into a family of HQC
LDPC codes. Next, we present a girth-maximizing algorithm that optimizes the
degrees of freedom within the family of codes to yield a high-girth HQC LDPC
code. Finally, we discuss how certain characteristics of a code protograph will
lead to inevitable short cycles, and show that these short cycles can be
eliminated using a &quot;squashing&quot; procedure that results in a high-girth QC LDPC
code, although not a hierarchical one. We illustrate our approach with designed
examples of girth-10 QC LDPC codes obtained from protographs of one-sided
spatially-coupled codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0712</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0712</id><created>2011-11-02</created><authors><author><keyname>Shivaswamy</keyname><forenames>Pannagadatta K.</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Online Learning with Preference Feedback</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new online learning model for learning with preference feedback.
The model is especially suited for applications like web search and recommender
systems, where preference data is readily available from implicit user feedback
(e.g. clicks). In particular, at each time step a potentially structured object
(e.g. a ranking) is presented to the user in response to a context (e.g.
query), providing him or her with some unobserved amount of utility. As
feedback the algorithm receives an improved object that would have provided
higher utility. We propose a learning algorithm with provable regret bounds for
this online learning setting and demonstrate its effectiveness on a web-search
application. The new learning model also applies to many other interactive
learning problems and admits several interesting extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0727</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0727</id><created>2011-11-03</created><authors><author><keyname>Everett</keyname><forenames>Evan</forenames></author><author><keyname>Dash</keyname><forenames>Debashis</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Sabharawal</keyname><forenames>Ashutosh</forenames></author></authors><title>Self-Interference Cancellation in Multi-hop Full-Duplex Networks via
  Structured Signaling</title><categories>cs.IT math.IT</categories><comments>Draft of the paper to be presented at the Allerton Conference on Sept
  29, 2011; Proc. 49th Annual Allerton Conference on Communication, Control,
  and Computing, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses transmission strategies for dealing with the problem of
self-interference in multi-hop wireless networks in which the nodes communicate
in a full- duplex mode. An information theoretic study of the simplest such
multi-hop network: the two-hop source-relay-destination network, leads to a
novel transmission strategy called structured self-interference cancellation
(or just &quot;structured cancellation&quot; for short). In the structured cancellation
strategy the source restrains from transmitting on certain signal levels, and
the relay structures its transmit signal such that it can learn the residual
self-interference channel, and undo the self-interference, by observing the
portion of its own transmit signal that appears at the signal levels left empty
by the source. It is shown that in certain nontrivial regimes, the structured
cancellation strategy outperforms not only half-duplex but also full-duplex
schemes in which time-orthogonal training is used for estimating the residual
self-interference channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0732</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0732</id><created>2011-11-03</created><authors><author><keyname>Wu</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Liyong</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Zeng</keyname><forenames>Zhenbing</forenames></author></authors><title>Generating Loop Invariants by Computing Vanishing Ideals of Sample
  Points</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loop invariants play a very important role in proving correctness of
programs. In this paper, we address the problem of generating invariants of
polynomial loop programs. We present a new approach, for generating polynomial
equation invariants of polynomial loop programs through computing vanishing
ideals of sample points. We apply rational function interpolation, based on
early termination technique, to generate invariants of loop programs with
symbolic initial values. Our approach avoids first-order quantifier elimination
and cylindrical algebraic decomposition(CAD). An algorithm for generating
polynomial invariants is proposed and some examples are given to illustrate the
algorithm. Furthermore, we demonstrate on a set of loop programs with symbolic
initial values that our algorithm can yield polynomial invariants with degrees
high up to 15.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0735</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0735</id><created>2011-11-03</created><authors><author><keyname>Jackson</keyname><forenames>Andrew N.</forenames></author></authors><title>Using Automated Dependency Analysis To Generate Representation
  Information</title><categories>cs.DL</categories><comments>4 pages, conference paper</comments><acm-class>D.2.8; H.m</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To preserve access to digital content, we must preserve the representation
information that captures the intended interpretation of the data. In
particular, we must be able to capture performance dependency requirements,
i.e. to identify the other resources that are required in order for the
intended interpretation to be constructed successfully. Critically, we must
identify the digital objects that are only referenced in the source data, but
are embedded in the performance, such as fonts. This paper describes a new
technique for analysing the dynamic dependencies of digital media, focussing on
analysing the process that underlies the performance, rather than parsing and
deconstructing the source data. This allows the results of format-specific
characterisation tools to be verified independently, and facilitates the
generation of representation information for any digital media format, even
when no suitable characterisation tool exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0737</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0737</id><created>2011-11-03</created><updated>2012-11-18</updated><authors><author><keyname>Vashkevich</keyname><forenames>Maxim</forenames></author><author><keyname>Wan</keyname><forenames>Wanggen</forenames></author><author><keyname>Petrovsky</keyname><forenames>Alexander</forenames></author></authors><title>Practical design of multi-channel oversampled warped cosine-modulated
  filter banks</title><categories>cs.IT math.IT</categories><comments>6 pages, 9 figures. IET International Communication Conference on
  Wireless Mobile &amp; Computing 2011</comments><msc-class>65T99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical approach to optimal design of multichannel oversampled warped
cosine-modulated filter banks (CMFB) is proposed. Warped CMFB is obtained by
allpass transformation of uniform CMFB. The paper addresses the problems of
minimization amplitude distortion and suppression of aliasing components
emerged due to oversampling of filter bank channel signals. Proposed
optimization-based design considerably reduces distortions of overall filter
bank transfer function taking into account channel subsampling ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0753</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0753</id><created>2011-11-03</created><authors><author><keyname>Dutta</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author><author><keyname>Narang</keyname><forenames>Ankur</forenames></author></authors><title>Towards &quot;Intelligent Compression&quot; in Streams: A Biased Reservoir
  Sampling based Bloom Filter Approach</title><categories>cs.IR cs.DS</categories><comments>11 pages, 8 figures, 5 tables</comments><report-no>IBM TechReport RI11015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosion of information stored world-wide,data intensive computing
has become a central area of research.Efficient management and processing of
this massively exponential amount of data from diverse sources,such as
telecommunication call data records,online transaction records,etc.,has become
a necessity.Removing redundancy from such huge(multi-billion records) datasets
resulting in resource and compute efficiency for downstream processing
constitutes an important area of study. &quot;Intelligent compression&quot; or
deduplication in streaming scenarios,for precise identification and elimination
of duplicates from the unbounded datastream is a greater challenge given the
realtime nature of data arrival.Stable Bloom Filters(SBF) address this problem
to a certain extent.However,SBF suffers from a high false negative rate(FNR)
and slow convergence rate,thereby rendering it inefficient for applications
with low FNR tolerance.In this paper, we present a novel Reservoir Sampling
based Bloom Filter,(RSBF) data structure,based on the combined concepts of
reservoir sampling and Bloom filters for approximate detection of duplicates in
data streams.Using detailed theoretical analysis we prove analytical bounds on
its false positive rate(FPR),false negative rate(FNR) and convergence rates
with low memory requirements.We show that RSBF offers the currently lowest FN
and convergence rates,and are better than those of SBF while using the same
memory.Using empirical analysis on real-world datasets(3 million records) and
synthetic datasets with around 1 billion records,we demonstrate upto 2x
improvement in FNR with better convergence rates as compared to SBF,while
exhibiting comparable FPR.To the best of our knowledge,this is the first
attempt to integrate reservoir sampling method with Bloom filters for
deduplication in streaming scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0754</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0754</id><created>2011-11-03</created><updated>2014-01-10</updated><authors><author><keyname>Basu</keyname><forenames>Arnab</forenames></author><author><keyname>Basu</keyname><forenames>Samik</forenames></author><author><keyname>Mj</keyname><forenames>Mahan</forenames></author></authors><title>Nash Equilibria via Duality and Homological Selection</title><categories>math.AT cs.GT math.GT math.OC</categories><comments>20 pages, 4 figures. Final version to appear in Proceedings of the
  Indian Academy of Sciences (Math. Sci.)</comments><msc-class>55M05 (Primary), 55N45, 91A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a multifunction from $X$ to the $k-$fold symmetric product $Sym_k(X)$,
we use the Dold-Thom Theorem to establish a homological selection Theorem. This
is used to establish existence of Nash equilibria. Cost functions in problems
concerning the existence of Nash Equilibria are traditionally multilinear in
the mixed strategies. The main aim of this paper is to relax the hypothesis of
multilinearity. We use basic intersection theory, Poincar\'e Duality in
addition to the Dold-Thom Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0762</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0762</id><created>2011-11-03</created><updated>2011-11-29</updated><authors><author><keyname>Narang</keyname><forenames>Ankur</forenames></author><author><keyname>Dutta</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author></authors><title>Multidimensional Balanced Allocation for Multiple Choice &amp; (1 + Beta)
  Processes</title><categories>cs.DS</categories><report-no>RI11018</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Allocation of balls into bins is a well studied abstraction for load
balancing problems.The literature hosts numerous results for sequential(single
dimensional) allocation case when m balls are thrown into n bins. In this paper
we study the symmetric multiple choice process for both unweighted and weighted
balls as well as for both multidimensional and scalar models.Additionally,we
present the results on bounds on gap for (1+beta) choice process with
multidimensional balls and bins. We show that for the symmetric d choice
process and with m=O(n), the upper bound on the gap is O(lnln(n)) w.h.p.This
upper bound on the gap is within D=f factor of the lower bound. This is the
first such tight result.For the general case of m&gt;&gt;n the expected gap is
bounded by O(lnln(n)).For variable f and non-uniform distribution of the
populated dimensions,we obtain the upper bound on the expected gap as
O(log(n)).
  Further,for the multiple round parallel balls and bins,we show that the gap
is also bounded by O(loglog(n)) for m=O(n).The same bound holds for the
expected gap when m&gt;&gt;n. Our analysis also has strong implications in the
sequential scalar case.For the weighted balls and bins and general case m&gt;&gt;n,we
show that the upper bound on the expected gap is O(log(n)) which improves upon
the best prior bound of n^c.Moreover,we show that for the (1 + beta) choice
process and m=O(n) the upper bound(assuming uniform distribution of f populated
dimensions over D total dimensions) on the gap is O(log(n)/beta),which is
within D=f factor of the lower bound.For fixed f with non-uniform distribution
and for random f with Binomial distribution the expected gap remains
O(log(n)/beta) independent of the total number of balls thrown. This is the
first such tight result for (1 +beta) paradigm with multidimensional balls and
bins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0773</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0773</id><created>2011-11-03</created><updated>2012-03-08</updated><authors><author><keyname>Albers</keyname><forenames>Susanne</forenames></author><author><keyname>Hellwig</keyname><forenames>Matthias</forenames></author></authors><title>On the Value of Job Migration in Online Makespan Minimization</title><categories>cs.DS</categories><comments>Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Makespan minimization on identical parallel machines is a classical
scheduling problem. We consider the online scenario where a sequence of $n$
jobs has to be scheduled non-preemptively on $m$ machines so as to minimize the
maximum completion time of any job. The best competitive ratio that can be
achieved by deterministic online algorithms is in the range $[1.88,1.9201]$.
Currently no randomized online algorithm with a smaller competitiveness is
known, for general $m$.
  In this paper we explore the power of job migration, i.e.\ an online
scheduler is allowed to perform a limited number of job reassignments.
Migration is a common technique used in theory and practice to balance load in
parallel processing environments. As our main result we settle the performance
that can be achieved by deterministic online algorithms. We develop an
algorithm that is $\alpha_m$-competitive, for any $m\geq 2$, where $\alpha_m$
is the solution of a certain equation. For $m=2$, $\alpha_2 = 4/3$ and
$\lim_{m\rightarrow \infty} \alpha_m = W_{-1}(-1/e^2)/(1+ W_{-1}(-1/e^2))
\approx 1.4659$. Here $W_{-1}$ is the lower branch of the Lambert $W$ function.
For $m\geq 11$, the algorithm uses at most $7m$ migration operations. For
smaller $m$, $8m$ to $10m$ operations may be performed. We complement this
result by a matching lower bound: No online algorithm that uses $o(n)$ job
migrations can achieve a competitive ratio smaller than $\alpha_m$. We finally
trade performance for migrations. We give a family of algorithms that is
$c$-competitive, for any $5/3\leq c \leq 2$. For $c= 5/3$, the strategy uses at
most $4m$ job migrations. For $c=1.75$, at most $2.5m$ migrations are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0794</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0794</id><created>2011-11-03</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Kravaris</keyname><forenames>Costas</forenames></author></authors><title>Global Exponential Observers for Two Classes of Nonlinear Systems</title><categories>math.OC cs.SY</categories><comments>18 pages, submitted to Systems and Control Letters for possible
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops sufficient conditions for the existence of global
exponential observers for two classes of nonlinear systems: (i) the class of
systems with a globally asymptotically stable compact set, and (ii) the class
of systems that evolve on an open set. In the first class, the derived
continuous-time observer also leads to the construction of a robust global
sampled-data exponential observer, under additional conditions. Two
illustrative examples of applications of the general results are presented, one
is a system with monotone nonlinearities and the other is the chemostat system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0801</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0801</id><created>2011-11-03</created><updated>2011-12-29</updated><authors><author><keyname>Dutta</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author><author><keyname>Narang</keyname><forenames>Ankur</forenames></author></authors><title>Perfectly Balanced Allocation With Estimated Average Using Expected
  Constant Retries</title><categories>cs.DS</categories><report-no>RI11023</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balanced allocation of online balls-into-bins has long been an active area of
research for efficient load balancing and hashing applications.There exists a
large number of results in this domain for different settings, such as parallel
allocations~\cite{parallel}, multi-dimensional allocations~\cite{multi},
weighted balls~\cite{weight} etc. For sequential multi-choice allocation, where
$m$ balls are thrown into $n$ bins with each ball choosing $d$ (constant) bins
independently uniformly at random, the maximum load of a bin is $O(\log \log n)
+ m/n$ with high probability~\cite{heavily_load}. This offers the current best
known allocation scheme. However, for $d = \Theta(\log n)$, the gap reduces to
$O(1)$~\cite{soda08}.A similar constant gap bound has been established for
parallel allocations with $O(\log ^*n)$ communication rounds~\cite{lenzen}.
  In this paper we propose a novel multi-choice allocation algorithm,
\emph{Improved D-choice with Estimated Average} ($IDEA$) achieving a constant
gap with a high probability for the sequential single-dimensional online
allocation problem with constant $d$. We achieve a maximum load of $\lceil m/n
\rceil$ with high probability for constant $d$ choice scheme with
\emph{expected} constant number of retries or rounds per ball. We also show
that the bound holds even for an arbitrary large number of balls, $m&gt;&gt;n$.
Further, we generalize this result to (i)~the weighted case, where balls have
weights drawn from an arbitrary weight distribution with finite variance,
(ii)~multi-dimensional setting, where balls have $D$ dimensions with $f$
randomly and uniformly chosen filled dimension for $m=n$, and (iii)~the
parallel case, where $n$ balls arrive and are placed parallely in the bins. We
show that the gap in these case is also a constant w.h.p. (independent of $m$)
for constant value of $d$ with expected constant number of retries per ball.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0808</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0808</id><created>2011-11-03</created><updated>2012-10-04</updated><authors><author><keyname>Stefanov</keyname><forenames>Stefan Z.</forenames></author></authors><title>Quantum/Relativistic Computation of Security and Efficiency of
  Electrical Power System for a Day-Ahead</title><categories>cs.OH</categories><comments>52 pages, extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for Electric Power System (EPS) quantum/relativistic security
and efficiency computation for a day-ahead via perturbative renormalization of
the EPS, finding the computation flowcharts, verification and validation is
built in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0837</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0837</id><created>2011-11-03</created><updated>2015-03-13</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>Exponential Lower Bounds for Polytopes in Combinatorial Optimization</title><categories>math.CO cs.CC quant-ph</categories><comments>19 pages, 4 figures. This version of the paper will appear in the
  Journal of the ACM. The earlier conference version in STOC'12 had the title
  &quot;Linear vs. Semidefinite Extended Formulations: Exponential Separation and
  Strong Lower Bounds&quot;</comments><acm-class>F.2.2; G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve a 20-year old problem posed by Yannakakis and prove that there
exists no polynomial-size linear program (LP) whose associated polytope
projects to the traveling salesman polytope, even if the LP is not required to
be symmetric. Moreover, we prove that this holds also for the cut polytope and
the stable set polytope. These results were discovered through a new connection
that we make between one-way quantum communication protocols and semidefinite
programming reformulations of LPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0854</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0854</id><created>2011-11-03</created><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author></authors><title>The Homology Groups of a Partial Trace Monoid Action</title><categories>math.AT cs.MA</categories><comments>30 pages</comments><msc-class>18B40, 18G10, 18G35, 55U10, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to investigate the homology groups of mathematical
models of concurrency. We study the Baues-Wirsching homology groups of a small
category associated with a partial monoid action on a set. We prove that these
groups can be reduced to the Leech homology groups of the monoid. For a trace
monoid with an action on a set, we will build a cubical complex of free Abelian
groups with homology groups isomorphic to the integral homology groups of the
action category. It allows us to solve the problem posed by the author in 2004
of the constructing an algorithm for computing homology groups of the CE nets.
We describe the algorithm and give examples of calculating the homology groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0855</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0855</id><created>2011-11-03</created><authors><author><keyname>Amdouni</keyname><forenames>Ichrak</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Pascale</keyname><forenames>Minet</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Adjih</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>OSERENA, an Optimized Coloring Algorithm for Dense or Large Scale
  Wireless Networks</title><categories>cs.NI</categories><comments>No. RR-7785 (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this research report is to present OSERENA &quot;Optimized SchEduling
RoutEr Node Activity&quot;, a distributed coloring algorithm optimized for dense
wireless networks. Network density has an extremely reduced impact on the size
of the messages exchanged to color the network. Furthermore, the number of
colors used to color the network is not impacted by this optimization. We
describe in this research report the properties of the algorithm and prove its
correctness and termination. Simulation results point out the considerable
gains in bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0860</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0860</id><created>2011-09-26</created><authors><author><keyname>Giunchiglia</keyname><forenames>E.</forenames></author><author><keyname>Narizzano</keyname><forenames>M.</forenames></author><author><keyname>Tacchella</keyname><forenames>A.</forenames></author></authors><title>Clause/Term Resolution and Learning in the Evaluation of Quantified
  Boolean Formulas</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 26, pages
  371-416, 2006</journal-ref><doi>10.1613/jair.1959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolution is the rule of inference at the basis of most procedures for
automated reasoning. In these procedures, the input formula is first translated
into an equisatisfiable formula in conjunctive normal form (CNF) and then
represented as a set of clauses. Deduction starts by inferring new clauses by
resolution, and goes on until the empty clause is generated or satisfiability
of the set of clauses is proven, e.g., because no new clauses can be generated.
  In this paper, we restrict our attention to the problem of evaluating
Quantified Boolean Formulas (QBFs). In this setting, the above outlined
deduction process is known to be sound and complete if given a formula in CNF
and if a form of resolution, called Q-resolution, is used. We introduce
Q-resolution on terms, to be used for formulas in disjunctive normal form. We
show that the computation performed by most of the available procedures for
QBFs --based on the Davis-Logemann-Loveland procedure (DLL) for propositional
satisfiability-- corresponds to a tree in which Q-resolution on terms and
clauses alternate. This poses the theoretical bases for the introduction of
learning, corresponding to recording Q-resolution formulas associated with the
nodes of the tree. We discuss the problems related to the introduction of
learning in DLL based procedures, and present solutions extending
state-of-the-art proposals coming from the literature on propositional
satisfiability. Finally, we show that our DLL based solver extended with
learning, performs significantly better on benchmarks used in the 2003 QBF
solvers comparative evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0862</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0862</id><created>2011-09-24</created><updated>2015-09-16</updated><authors><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Gentilini</keyname><forenames>Raffaella</forenames><affiliation>Universit&#xe0; degli Studi di Perugia</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xc3;&#xa7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author></authors><title>Quantitative Languages Defined by Functional Automata</title><categories>cs.FL</categories><comments>32 pages, extended version of CONCUR'12</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:14) 2015</journal-ref><doi>10.2168/LMCS-11(3:14)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weighted automaton is functional if any two accepting runs on the same
finite word have the same value. In this paper, we investigate functional
weighted automata for four different measures: the sum, the mean, the
discounted sum of weights along edges and the ratio between rewards and costs.
On the positive side, we show that functionality is decidable for the four
measures. Furthermore, the existential and universal threshold problems, the
language inclusion problem and the equivalence problem are all decidable when
the weighted automata are functional. On the negative side, we also study the
quantitative extension of the realizability problem and show that it is
undecidable for sum, mean and ratio. We finally show how to decide whether the
language associated with a given functional automaton can be defined with a
deterministic one, for sum, mean and discounted sum. The results on
functionality and determinizability are expressed for the more general class of
functional group automata. This allows one to formulate within the same
framework new results related to discounted sum automata and known results on
sum and mean automata. Ratio automata do not fit within this general scheme and
different techniques are required to decide functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0867</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0867</id><created>2011-11-03</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Poon</keyname><forenames>Sheung-Hung</forenames></author><author><keyname>Tsai</keyname><forenames>Feng-Ren</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>The black-and-white coloring problem on distance hereditary graphs and
  strongly chordal graphs</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph G and integers b and w. The black-and-white coloring problem
asks if there exist disjoint sets of vertices B and W with |B|=b and |W|=w such
that no vertex in B is adjacent to any vertex in W. In this paper we show that
the problem is polynomial when restricted to cographs, distance-hereditary
graphs, interval graphs and strongly chordal graphs. We show that the problem
is NP-complete on splitgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0870</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0870</id><created>2011-09-29</created><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Raheja</keyname><forenames>Jagdish L.</forenames></author></authors><title>A Formal Approach for Agent Based Large Concurrent Intelligent Systems</title><categories>cs.SE</categories><comments>IJAET Vol.1, Issue 1, 2010, 95-103</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Intelligent Systems are so complex these days that an urgent need for
designing such systems in best available way is evolving. Modeling is the
useful technique to show a complex real world system into the form of
abstraction, so that analysis and implementation of the intelligent system
become easy and is useful in gathering the prior knowledge of system that is
not possible to experiment with the real world complex systems. This paper
discusses a formal approach of agent-based large systems modeling for
intelligent systems, which describes design level precautions, challenges and
techniques using autonomous agents, as its fundamental modeling abstraction. We
are discussing Ad-Hoc Network System as a case study in which we are using
mobile agents where nodes are free to relocate, as they form an Intelligent
Systems. The designing is very critical in this scenario and it can reduce the
whole cost, time duration and risk involved in the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0873</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0873</id><created>2011-11-03</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Collective Energy Foraging of Robot Swarms and Robot Organisms</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation and competition among stand-alone swarm agents increase
collective fitness of the whole system. A principally new kind of collective
systems is demonstrated by some bacteria and fungi, when they build symbiotic
organisms. Symbiotic life forms emerge new functional and self-developmental
capabilities, which allow better survival of swarm agents in different
environments. In this paper we consider energy foraging scenario for two
robotic species, swarm robots and symbiotic robot organism. It is indicated
that aggregation of microrobots into a robot organism can provide better
functional fitness for the whole group. A prototype of microrobots capable of
autonomous aggregation and disaggregation are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0875</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0875</id><created>2011-11-03</created><updated>2012-10-13</updated><authors><author><keyname>Kurve</keyname><forenames>Aditya</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Miller</keyname><forenames>David J.</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Game Theoretic Iterative Partitioning for Dynamic Load Balancing in
  Distributed Network Simulation</title><categories>cs.DC</categories><comments>Requires a more thorough study on actual simulator platform</comments><acm-class>I.6.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High fidelity simulation of large-sized complex networks can be realized on a
distributed computing platform that leverages the combined resources of
multiple processors or machines. In a discrete event driven simulation, the
assignment of logical processes (LPs) to machines is a critical step that
affects the computational and communication burden on the machines, which in
turn affects the simulation execution time of the experiment. We study a
network partitioning game wherein each node (LP) acts as a selfish player. We
derive two local node-level cost frameworks which are feasible in the sense
that the aggregate state information required to be exchanged between the
machines is independent of the size of the simulated network model. For both
cost frameworks, we prove the existence of stable Nash equilibria in pure
strategies. Using iterative partition improvements, we propose game theoretic
partitioning algorithms based on the two cost criteria and show that each
descends in a global cost. To exploit the distributed nature of the system, the
algorithm is distributed, with each node's decision based on its local
information and on a few global quantities which can be communicated
machine-to-machine. We demonstrate the performance of our partitioning
algorithm on an optimistic discrete event driven simulation platform that
models an actual parallel simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0882</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0882</id><created>2011-11-03</created><authors><author><keyname>Phe-Neau</keyname><forenames>Tiphaine</forenames></author><author><keyname>de Amorim</keyname><forenames>Marcelo Dias</forenames></author><author><keyname>Conan</keyname><forenames>Vania</forenames></author></authors><title>Using Neighborhood Beyond One Hop in Disruption-Tolerant Networks</title><categories>cs.NI</categories><comments>5 pages, 5 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Most disruption-tolerant networking (DTN) protocols available in the
literature have focused on mere contact and intercontact characteristics to
make forwarding decisions. Nevertheless, there is a world behind contacts: just
because one node is not in contact with some potential destination, it does not
mean that this node is alone. There may be interesting end-to-end transmission
opportunities through other nearby nodes. Existing protocols miss such
possibilities by maintaining a simple contact-based view of the network. In
this paper, we investigate how the vicinity of a node evolves through time and
whether such information can be useful when routing data. We observe a clear
tradeoff between routing performance and the cost for monitoring the
neighborhood. Our analyses suggest that limiting a node's neighborhood view to
three or four hops is more than enough to significantly improve forwarding
efficiency without incurring prohibitive overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0885</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0885</id><created>2011-11-03</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Khodadadzadeh</keyname><forenames>Mahdi</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Graph Regularized Nonnegative Matrix Factorization for Hyperspectral
  Data Unmixing</title><categories>cs.CV</categories><comments>4 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral unmixing is an important tool in hyperspectral data analysis for
estimating endmembers and abundance fractions in a mixed pixel. This paper
examines the applicability of a recently developed algorithm called graph
regularized nonnegative matrix factorization (GNMF) for this aim. The proposed
approach exploits the intrinsic geometrical structure of the data besides
considering positivity and full additivity constraints. Simulated data based on
the measured spectral signatures, is used for evaluating the proposed
algorithm. Results in terms of abundance angle distance (AAD) and spectral
angle distance (SAD) show that this method can effectively unmix hyperspectral
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0897</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0897</id><created>2011-11-03</created><updated>2012-04-17</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Blais</keyname><forenames>Eric</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author></authors><title>Active Property Testing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the motivations for property testing of boolean functions is the idea
that testing can serve as a preprocessing step before learning. However, in
most machine learning applications, it is not possible to request for labels of
fictitious examples constructed by the algorithm. Instead, the dominant query
paradigm in applied machine learning, called active learning, is one where the
algorithm may query for labels, but only on points in a given polynomial-sized
(unlabeled) sample, drawn from some underlying distribution D. In this work, we
bring this well-studied model in learning to the domain of testing.
  We show that for a number of important properties, testing can still yield
substantial benefits in this setting. This includes testing unions of
intervals, testing linear separators, and testing various assumptions used in
semi-supervised learning. In addition to these specific results, we also
develop a general notion of the testing dimension of a given property with
respect to a given distribution. We show this dimension characterizes (up to
constant factors) the intrinsic number of label requests needed to test that
property. We develop such notions for both the active and passive testing
models. We then use these dimensions to prove a number of lower bounds,
including for linear separators and the class of dictator functions.
  Our results show that testing can be a powerful tool in realistic models for
learning, and further that active testing exhibits an interesting and rich
structure. Our work in addition brings together tools from a range of areas
including U-statistics, noise-sensitivity, self-correction, and spectral
analysis of random matrices, and develops new tools that may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0907</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0907</id><created>2011-11-03</created><updated>2012-04-26</updated><authors><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Qian</keyname><forenames>Chao</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Towards Analyzing Crossover Operators in Evolutionary Search via General
  Markov Chain Switching Theorem</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms (EAs), simulating the evolution process of natural
species, are used to solve optimization problems. Crossover (also called
recombination), originated from simulating the chromosome exchange phenomena in
zoogamy reproduction, is widely employed in EAs to generate offspring
solutions, of which the effectiveness has been examined empirically in
applications. However, due to the irregularity of crossover operators and the
complicated interactions to mutation, crossover operators are hard to analyze
and thus have few theoretical results. Therefore, analyzing crossover not only
helps in understanding EAs, but also helps in developing novel techniques for
analyzing sophisticated metaheuristic algorithms.
  In this paper, we derive the General Markov Chain Switching Theorem (GMCST)
to facilitate theoretical studies of crossover-enabled EAs. The theorem allows
us to analyze the running time of a sophisticated EA from an easy-to-analyze
EA. Using this tool, we analyze EAs with several crossover operators on the
LeadingOnes and OneMax problems, which are noticeably two well studied problems
for mutation-only EAs but with few results for crossover-enabled EAs. We first
derive the bounds of running time of the (2+2)-EA with crossover operators;
then we study the running time gap between the mutation-only (2:2)-EA and the
(2:2)-EA with crossover operators; finally, we develop strategies that apply
crossover operators only when necessary, which improve from the mutation-only
as well as the crossover-all-the-time (2:2)-EA. The theoretical results are
verified by experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0920</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0920</id><created>2011-11-03</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author><author><keyname>Van Dooren</keyname><forenames>Paul</forenames></author></authors><title>Extracting spatial information from networks with low-order eigenvectors</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages</comments><msc-class>15A18, 92-08, 91C20, 90B18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of inferring meaningful spatial information in
networks from incomplete information on the connection intensity between the
nodes of the network. We consider two spatially distributed networks: a
population migration flow network within the US, and a network of mobile phone
calls between cities in Belgium. For both networks we use the eigenvectors of
the Laplacian matrix constructed from the link intensities to obtain
informative visualizations and capture natural geographical subdivisions. We
observe that some low order eigenvectors localize very well and seem to reveal
small geographically cohesive regions that match remarkably well with political
and administrative boundaries. We discuss possible explanations for this
observation by describing diffusion maps and localized eigenfunctions. In
addition, we discuss a possible connection with the weighted graph cut problem,
and provide numerical evidence supporting the idea that lower order
eigenvectors point out local cuts in the network. However, we do not provide a
formal and rigorous justification for our observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0922</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0922</id><created>2011-11-03</created><authors><author><keyname>Wittmann</keyname><forenames>Markus</forenames></author><author><keyname>Zeiser</keyname><forenames>Thomas</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Comparison of different Propagation Steps for the Lattice Boltzmann
  Method</title><categories>cs.DC physics.comp-ph</categories><comments>17 pages, 11 figures, 8 tables, preprint submitted to Computers &amp;
  Mathematics with Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several possibilities exist to implement the propagation step of the lattice
Boltzmann method. This paper describes common implementations which are
compared according to the number of memory transfer operations they require per
lattice node update. A memory bandwidth based performance model is then used to
obtain an estimation of the maximal reachable performance on different
machines. A subset of the discussed implementations of the propagation step
were benchmarked on different Intel and AMD-based compute nodes using the
framework of an existing flow solver which is specially adapted to simulate
flow in porous media. Finally the estimated performance is compared to the
measured one. As expected, the number of memory transfers has a significant
impact on performance. Advanced approaches for the propagation step like &quot;AA
pattern&quot; or &quot;Esoteric Twist&quot; require more implementation effort but sustain
significantly better performance than non-naive straight forward
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0934</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0934</id><created>2011-11-03</created><updated>2015-08-10</updated><authors><author><keyname>Ritt</keyname><forenames>Marcus</forenames></author><author><keyname>Costa</keyname><forenames>Alysson M.</forenames></author></authors><title>Improved integer programming models for simple assembly line balancing
  and related problems</title><categories>cs.DM cs.DS math.CO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stronger formulation of the precedence constraints and the
station limits for the simple assembly line balancing problem. The linear
relaxation of the improved integer program theoretically dominates all previous
formulations using impulse variables, and produces solutions of significantly
better quality in practice. The improved formulation can be used to strengthen
related problems with similar restrictions. We demonstrate their effectiveness
on the U-shaped assembly line balancing problem and on the bin packing problem
with precedence constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0948</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0948</id><created>2011-11-03</created><authors><author><keyname>Rao</keyname><forenames>Ashwin</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Lim</keyname><forenames>Yeon-Sup</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Barakat</keyname><forenames>Chadi</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Towsley</keyname><forenames>Don</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Dabbous</keyname><forenames>Walid</forenames><affiliation>INRIA Sophia Antipolis / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Network Characteristics of Video Streaming Traffic</title><categories>cs.NI cs.MM</categories><comments>This is the author version of the paper accepted as a full paper at
  ACM CoNEXT 2011</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video streaming represents a large fraction of Internet traffic.
Surprisingly, little is known about the network characteristics of this
traffic. In this paper, we study the network characteristics of the two most
popular video streaming services, Netflix and YouTube. We show that the
streaming strategies vary with the type of the application (Web browser or
native mobile application), and the type of container (Silverlight, Flash, or
HTML5) used for video streaming. In particular, we identify three different
streaming strategies that produce traffic patterns from non-ack clocked ON-OFF
cycles to bulk TCP transfer. We then present an analytical model to study the
potential impact of these streaming strategies on the aggregate traffic and
make recommendations accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0952</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0952</id><created>2011-11-03</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Kannan</keyname><forenames>Ravi</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>Computing a Nonnegative Matrix Factorization -- Provably</title><categories>cs.DS cs.LG</categories><comments>29 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Nonnegative Matrix Factorization (NMF) problem we are given an $n
\times m$ nonnegative matrix $M$ and an integer $r &gt; 0$. Our goal is to express
$M$ as $A W$ where $A$ and $W$ are nonnegative matrices of size $n \times r$
and $r \times m$ respectively. In some applications, it makes sense to ask
instead for the product $AW$ to approximate $M$ -- i.e. (approximately)
minimize $\norm{M - AW}_F$ where $\norm{}_F$ denotes the Frobenius norm; we
refer to this as Approximate NMF. This problem has a rich history spanning
quantum mechanics, probability theory, data analysis, polyhedral combinatorics,
communication complexity, demography, chemometrics, etc. In the past decade NMF
has become enormously popular in machine learning, where $A$ and $W$ are
computed using a variety of local search heuristics. Vavasis proved that this
problem is NP-complete. We initiate a study of when this problem is solvable in
polynomial time:
  1. We give a polynomial-time algorithm for exact and approximate NMF for
every constant $r$. Indeed NMF is most interesting in applications precisely
when $r$ is small.
  2. We complement this with a hardness result, that if exact NMF can be solved
in time $(nm)^{o(r)}$, 3-SAT has a sub-exponential time algorithm. This rules
out substantial improvements to the above algorithm.
  3. We give an algorithm that runs in time polynomial in $n$, $m$ and $r$
under the separablity condition identified by Donoho and Stodden in 2003. The
algorithm may be practical since it is simple and noise tolerant (under benign
assumptions). Separability is believed to hold in many practical settings.
  To the best of our knowledge, this last result is the first example of a
polynomial-time algorithm that provably works under a non-trivial condition on
the input and we believe that this will be an interesting and important
direction for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.0965</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.0965</id><created>2011-11-03</created><authors><author><keyname>Louis</keyname><forenames>Anand</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Many Sparse Cuts via Higher Eigenvalues</title><categories>cs.DS</categories><doi>10.1145/2213977.2214079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cheeger's fundamental inequality states that any edge-weighted graph has a
vertex subset $S$ such that its expansion (a.k.a. conductance) is bounded as
follows: \[ \phi(S) \defeq \frac{w(S,\bar{S})}{\min \set{w(S), w(\bar{S})}}
\leq 2\sqrt{\lambda_2} \] where $w$ is the total edge weight of a subset or a
cut and $\lambda_2$ is the second smallest eigenvalue of the normalized
Laplacian of the graph. Here we prove the following natural generalization: for
any integer $k \in [n]$, there exist $ck$ disjoint subsets $S_1, ..., S_{ck}$,
such that \[ \max_i \phi(S_i) \leq C \sqrt{\lambda_{k} \log k} \] where
$\lambda_i$ is the $i^{th}$ smallest eigenvalue of the normalized Laplacian and
$c&lt;1,C&gt;0$ are suitable absolute constants. Our proof is via a polynomial-time
algorithm to find such subsets, consisting of a spectral projection and a
randomized rounding. As a consequence, we get the same upper bound for the
small set expansion problem, namely for any $k$, there is a subset $S$ whose
weight is at most a $\bigO(1/k)$ fraction of the total weight and $\phi(S) \le
C \sqrt{\lambda_k \log k}$. Both results are the best possible up to constant
factors.
  The underlying algorithmic problem, namely finding $k$ subsets such that the
maximum expansion is minimized, besides extending sparse cuts to more than one
subset, appears to be a natural clustering problem in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1011</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1011</id><created>2011-11-03</created><updated>2011-11-22</updated><authors><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames><affiliation>LIAFA, CNRS and University of Paris Diderot</affiliation></author><author><keyname>Bouajjani</keyname><forenames>Ahmed</forenames><affiliation>LIAFA, CNRS and University of Paris Diderot</affiliation></author><author><keyname>Qadeer</keyname><forenames>Shaz</forenames><affiliation>Microsoft Research, Redmond</affiliation></author></authors><title>Context-Bounded Analysis For Concurrent Programs With Dynamic Creation
  of Threads</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4, D.3.1, F.4.3, I.2.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (November
  23, 2011) lmcs:708</journal-ref><doi>10.2168/LMCS-7(4:4)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-bounded analysis has been shown to be both efficient and effective at
finding bugs in concurrent programs. According to its original definition,
context-bounded analysis explores all behaviors of a concurrent program up to
some fixed number of context switches between threads. This definition is
inadequate for programs that create threads dynamically because bounding the
number of context switches in a computation also bounds the number of threads
involved in the computation. In this paper, we propose a more general
definition of context-bounded analysis useful for programs with dynamic thread
creation. The idea is to bound the number of context switches for each thread
instead of bounding the number of switches of all threads. We consider several
variants based on this new definition, and we establish decidability and
complexity results for the analysis induced by them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1014</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1014</id><created>2011-11-03</created><authors><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Ganesh</keyname><forenames>Arvind</forenames></author><author><keyname>Yang</keyname><forenames>Allen</forenames></author><author><keyname>Zhou</keyname><forenames>Zihan</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Sparsity and Robustness in Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report concerns the use of techniques for sparse signal representation
and sparse error correction for automatic face recognition. Much of the recent
interest in these techniques comes from the paper &quot;Robust Face Recognition via
Sparse Representation&quot; by Wright et al. (2009), which showed how, under certain
technical conditions, one could cast the face recognition problem as one of
seeking a sparse representation of a given input face image in terms of a
&quot;dictionary&quot; of training images and images of individual pixels. In this
report, we have attempted to clarify some frequently encountered questions
about this work and particularly, on the validity of using sparse
representation techniques for face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1020</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1020</id><created>2011-11-03</created><updated>2012-05-24</updated><authors><author><keyname>Noorshams</keyname><forenames>Nima</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Stochastic Belief Propagation: A Low-Complexity Alternative to the
  Sum-Product Algorithm</title><categories>cs.IT math.IT stat.ML</categories><comments>Portions of the results were initially reported at the Allerton
  Conference on Communications, Control, and Computing (September 2011). The
  work was also submitted to IEEE Transaction on Information Theory in November
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum-product or belief propagation (BP) algorithm is a widely-used
message-passing algorithm for computing marginal distributions in graphical
models with discrete variables. At the core of the BP message updates, when
applied to a graphical model with pairwise interactions, lies a matrix-vector
product with complexity that is quadratic in the state dimension $d$, and
requires transmission of a $(d-1)$-dimensional vector of real numbers
(messages) to its neighbors. Since various applications involve very large
state dimensions, such computation and communication complexities can be
prohibitively complex. In this paper, we propose a low-complexity variant of
BP, referred to as stochastic belief propagation (SBP). As suggested by the
name, it is an adaptively randomized version of the BP message updates in which
each node passes randomly chosen information to each of its neighbors. The SBP
message updates reduce the computational complexity (per iteration) from
quadratic to linear in $d$, without assuming any particular structure of the
potentials, and also reduce the communication complexity significantly,
requiring only $\log{d}$ bits transmission per edge. Moreover, we establish a
number of theoretical guarantees for the performance of SBP, showing that it
converges almost surely to the BP fixed point for any tree-structured graph,
and for graphs with cycles satisfying a contractivity condition. In addition,
for these graphical models, we provide non-asymptotic upper bounds on the
convergence rate, showing that the $\ell_{\infty}$ norm of the error vector
decays no slower than $O(1/\sqrt{t})$ with the number of iterations $t$ on
trees and the mean square error decays as $O(1/t)$ for general graphs. These
analysis show that SBP can provably yield reductions in computational and
communication complexities for various classes of graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1022</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1022</id><created>2011-11-03</created><authors><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author><author><keyname>Jos&#xe9;</keyname><forenames>Mart&#xed;n Jos&#xe9;</forenames></author></authors><title>Towards the integration of formal specification in the \'Ancora
  methodology</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are some non-formal methodologies such as RUP, OpenUP, agile
methodologies such as SCRUP, XP and techniques like those proposed by UML,
which allow the development of software. The software industry has struggled to
generate quality software, as importance has not been given to the engineering
requirements, resulting in a poor specification of requirements and software of
poor quality. In order to generate a contribution to the specification of
requirements, this article describes a methodological proposal, implementing
formal methods to the results of the process of requirements analysis of the
methodology \'Ancora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1041</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1041</id><created>2011-11-04</created><updated>2013-01-07</updated><authors><author><keyname>Donoho</keyname><forenames>David</forenames></author><author><keyname>Johnstone</keyname><forenames>Iain</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Accurate Prediction of Phase Transitions in Compressed Sensing via a
  Connection to Minimax Denoising</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>71 pages, 32 pdf figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing posits that, within limits, one can undersample a sparse
signal and yet reconstruct it accurately. Knowing the precise limits to such
undersampling is important both for theory and practice. We present a formula
that characterizes the allowed undersampling of generalized sparse objects. The
formula applies to Approximate Message Passing (AMP) algorithms for compressed
sensing, which are here generalized to employ denoising operators besides the
traditional scalar soft thresholding denoiser. This paper gives several
examples including scalar denoisers not derived from convex penalization -- the
firm shrinkage nonlinearity and the minimax nonlinearity -- and also nonscalar
denoisers -- block thresholding, monotone regression, and total variation
minimization.
  Let the variables eps = k/N and delta = n/N denote the generalized sparsity
and undersampling fractions for sampling the k-generalized-sparse N-vector x_0
according to y=Ax_0. Here A is an n\times N measurement matrix whose entries
are iid standard Gaussian. The formula states that the phase transition curve
delta = delta(eps) separating successful from unsuccessful reconstruction of
x_0 by AMP is given by: delta = M(eps| Denoiser), where M(eps| Denoiser)
denotes the per-coordinate minimax mean squared error (MSE) of the specified,
optimally-tuned denoiser in the directly observed problem y = x + z. In short,
the phase transition of a noiseless undersampling problem is identical to the
minimax MSE in a denoising problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1043</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1043</id><created>2011-11-04</created><authors><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author></authors><title>The MOR cryptosystem and extra-special $p$-groups</title><categories>math.GR cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the MOR cryptosystem, using the automorphism group of the
extra-special $p$-group of exponent $p$, for an odd prime $p$. Similar results
can be obtained for extra-special $p$-groups of exponent $p^2$ and for the even
prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1048</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1048</id><created>2011-11-04</created><updated>2011-12-14</updated><authors><author><keyname>Charafeddine</keyname><forenames>Mohamad Awad</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Paulraj</keyname><forenames>Arogyaswami</forenames></author></authors><title>Achievable and Crystallized Rate Regions of the Interference Channel
  with Interference as Noise</title><categories>cs.IT math.IT</categories><comments>28 pages, 12 figures, to appear in IEEE Transactions of Wireless
  Communication</comments><journal-ref>IEEE Transactions of Wireless Communications, March 2012, Volume
  11, Issue 3, Pages 1100-1111</journal-ref><doi>10.1109/TWC.2012.010312.110497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interference channel achievable rate region is presented when the
interference is treated as noise. The formulation starts with the 2-user
channel, and then extends the results to the n-user case. The rate region is
found to be the convex hull of the union of n power control rate regions, where
each power control rate region is upperbounded by a (n-1)-dimensional
hyper-surface characterized by having one of the transmitters transmitting at
full power. The convex hull operation lends itself to a time-sharing operation
depending on the convexity behavior of those hyper-surfaces. In order to know
when to use time-sharing rather than power control, the paper studies the
hyper-surfaces convexity behavior in details for the 2-user channel with
specific results pertaining to the symmetric channel. It is observed that most
of the achievable rate region can be covered by using simple On/Off binary
power control in conjunction with time-sharing. The binary power control
creates several corner points in the n-dimensional space. The crystallized rate
region, named after its resulting crystal shape, is hence presented as the
time-sharing convex hull imposed onto those corner points; thereby offering a
viable new perspective of looking at the achievable rate region of the
interference channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1051</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1051</id><created>2011-11-04</created><updated>2013-09-02</updated><authors><author><keyname>Lee</keyname><forenames>Jung Hoon</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Multiuser Diversity in Interfering Broadcast Channels: Achievable
  Degrees of Freedom and User Scaling Law</title><categories>cs.IT math.IT</categories><comments>To appear, IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2013.100313.130255</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how multiuser dimensions can effectively be exploited
for target degrees of freedom (DoF) in interfering broadcast channels (IBC)
consisting of K-transmitters and their user groups. First, each transmitter is
assumed to have a single antenna and serve a singe user in its user group where
each user has receive antennas less than K. In this case, a K-transmitter
single-input multiple-output (SIMO) interference channel (IC) is constituted
after user selection. Without help of multiuser diversity, K-1 interfering
signals cannot be perfectly removed at each user since the number of receive
antennas is smaller than or equal to the number of interferers. Only with
proper user selection, non-zero DoF per transmitter is achievable as the number
of users increases. Through geometric interpretation of interfering channels,
we show that the multiuser dimensions have to be used first for reducing the
DoF loss caused by the interfering signals, and then have to be used for
increasing the DoF gain from its own signal. The sufficient number of users for
the target DoF is derived. We also discuss how the optimal strategy of
exploiting multiuser diversity can be realized by practical user selection
schemes. Finally, the single transmit antenna case is extended to the
multiple-input multiple-output (MIMO) IBC where each transmitter with multiple
antennas serves multiple users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1053</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1053</id><created>2011-11-04</created><authors><author><keyname>Skvortsov</keyname><forenames>Alex</forenames></author><author><keyname>Ristic</keyname><forenames>Branko</forenames></author></authors><title>Modelling and Performance analysis of a Network of Chemical Sensors with
  Dynamic Collaboration</title><categories>cs.SI physics.soc-ph</categories><comments>21 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of environmental monitoring using a wireless network of chemical
sensors with a limited energy supply is considered. Since the conventional
chemical sensors in active mode consume vast amounts of energy, an optimisation
problem arises in the context of a balance between the energy consumption and
the detection capabilities of such a network. A protocol based on &quot;dynamic
sensor collaboration&quot; is employed: in the absence of any pollutant, majority of
sensors are in the sleep (passive) mode; a sensor is invoked (activated) by
wake-up messages from its neighbors only when more information is required. The
paper proposes a mathematical model of a network of chemical sensors using this
protocol. The model provides valuable insights into the network behavior and
near optimal capacity design (energy consumption against detection). An
analytical model of the environment, using turbulent mixing to capture chaotic
fluctuations, intermittency and non-homogeneity of the pollutant distribution,
is employed in the study. A binary model of a chemical sensor is assumed (a
device with threshold detection). The outcome of the study is a set of simple
analytical tools for sensor network design, optimisation, and performance
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1055</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1055</id><created>2011-11-04</created><updated>2014-11-21</updated><authors><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Multi-way spectral partitioning and higher-order Cheeger inequalities</title><categories>math.MG cs.DS math.SP</categories><comments>Misc. edits, added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic fact in spectral graph theory is that the number of connected
components in an undirected graph is equal to the multiplicity of the
eigenvalue zero in the Laplacian matrix of the graph. In particular, the graph
is disconnected if and only if there are at least two eigenvalues equal to
zero. Cheeger's inequality and its variants provide an approximate version of
the latter fact; they state that a graph has a sparse cut if and only if there
are at least two eigenvalues that are close to zero.
  It has been conjectured that an analogous characterization holds for higher
multiplicities, i.e., there are $k$ eigenvalues close to zero if and only if
the vertex set can be partitioned into $k$ subsets, each defining a sparse cut.
We resolve this conjecture. Our result provides a theoretical justification for
clustering algorithms that use the bottom $k$ eigenvectors to embed the
vertices into $\mathbb R^k$, and then apply geometric considerations to the
embedding.
  We also show that these techniques yield a nearly optimal tradeoff between
the expansion of sets of size $\approx n/k$, and the $k$th smallest eigenvalue
of the normalized Laplacian matrix, denoted $\lambda_k$. In particular, we show
that in every graph there is a set of size at most $2n/k$ which has expansion
at most $O(\sqrt{\lambda_k \log k})$. This bound is tight, up to constant
factors, for the &quot;noisy hypercube&quot; graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1060</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1060</id><created>2011-11-04</created><authors><author><keyname>Nakajima</keyname><forenames>Chihiro H.</forenames></author><author><keyname>Sakaue</keyname><forenames>Takahiro</forenames></author></authors><title>Computing a Knot Invariant as a Constraint Satisfaction Problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>6 pages, 3 figures, submitted to short note in Journal of Physical
  Society of Japan</comments><doi>10.1143/JPSJ.81.035001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We point out the connection between mathematical knot theory and spin
glass/search problem. In particular, we present a statistical mechanical
formulation of the problem of computing a knot invariant; p-colorability
problem, which provides an algorithm to find the solution. The method also
allows one to get some deeper insight into the structural complexity of knots,
which is expected to be related with the landscape structure of constraint
satisfaction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1084</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1084</id><created>2011-11-04</created><updated>2012-06-16</updated><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Yuan</keyname><forenames>Chun-Ming</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>Sparse Differential Resultant for Laurent Differential Polynomials</title><categories>cs.SC math.AG</categories><comments>70 pages</comments><msc-class>Primary 12H05, Secondary 14M25, 14Q99, 68W30</msc-class><acm-class>I.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first introduce the concept of Laurent differentially
essential systems and give a criterion for Laurent differentially essential
systems in terms of their supports. Then the sparse differential resultant for
a Laurent differentially essential system is defined and its basic properties
are proved. In particular, order and degree bounds for the sparse differential
resultant are given. Based on these bounds, an algorithm to compute the sparse
differential resultant is proposed, which is single exponential in terms of the
number of indeterminates, the Jacobi number of the system, and the size of the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1086</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1086</id><created>2011-11-04</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Jyoti</keyname><forenames>Divya</forenames></author><author><keyname>Bhatia</keyname><forenames>M. G.</forenames></author></authors><title>Design and Simulation of an 8-bit Dedicated Processor for calculating
  the Sine and Cosine of an Angle using the CORDIC Algorithm</title><categories>cs.AR cs.DS</categories><comments>CORDIC, VHDL, dedicated processor, datapath, finite state machine</comments><journal-ref>Proceedings of the 2011 IEEE International Conference on
  Computational Intelligence and Computing Research (ICCIC); IEEE Xplore:
  CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN:
  978-1-61284-766-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design and simulation of an 8-bit dedicated
processor for calculating the Sine and Cosine of an Angle using CORDIC
Algorithm (COordinate Rotation DIgital Computer), a simple and efficient
algorithm to calculate hyperbolic and trigonometric functions. We have proposed
a dedicated processor system, modeled by writing appropriate programs in VHDL,
for calculating the Sine and Cosine of an angle. System simulation was carried
out using ModelSim 6.3f and Xilinx ISE Design Suite 12.3. A maximum frequency
of 81.353 MHz was reached with a minimum period of 12.292 ns. 126 (3%) slices
were used. This paper attempts to survey the existing CORDIC algorithm with an
eye towards implementation in Field Programmable Gate Arrays (FPGAs). A brief
description of the theory behind the algorithm and the derivation of the Sine
and Cosine of an angle using the CORDIC algorithm has been presented. The
system can be implemented using Spartan3 XC3S400 with Xilinx ISE 12.3 and VHDL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1090</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1090</id><created>2011-11-04</created><authors><author><keyname>Jyoti</keyname><forenames>Divya</forenames></author><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Vaidya</keyname><forenames>Pallavi</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>A robust, low-cost approach to Face Detection and Face Recognition</title><categories>cs.CV</categories><comments>discrete wavelet transform, face detection, face recognition, person
  identification</comments><journal-ref>CiiT International Journal of Digital Image Processing, Vol. 15,
  No. 10, October 2011, ISSN 0974 - 9691 (Print) &amp; ISSN 0974 - 9586 (Online)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the domain of Biometrics, recognition systems based on iris, fingerprint
or palm print scans etc. are often considered more dependable due to extremely
low variance in the properties of these entities with respect to time. However,
over the last decade data processing capability of computers has increased
manifold, which has made real-time video content analysis possible. This shows
that the need of the hour is a robust and highly automated Face Detection and
Recognition algorithm with credible accuracy rate. The proposed Face Detection
and Recognition system using Discrete Wavelet Transform (DWT) accepts face
frames as input from a database containing images from low cost devices such as
VGA cameras, webcams or even CCTV's, where image quality is inferior. Face
region is then detected using properties of L*a*b* color space and only Frontal
Face is extracted such that all additional background is eliminated. Further,
this extracted image is converted to grayscale and its dimensions are resized
to 128 x 128 pixels. DWT is then applied to entire image to obtain the
coefficients. Recognition is carried out by comparison of the DWT coefficients
belonging to the test image with those of the registered reference image. On
comparison, Euclidean distance classifier is deployed to validate the test
image from the database. Accuracy for various levels of DWT Decomposition is
obtained and hence, compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1093</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1093</id><created>2011-11-04</created><authors><author><keyname>Thampi</keyname><forenames>Sabu M.</forenames></author><author><keyname>Jacob</keyname><forenames>Ann Jisma</forenames></author></authors><title>Securing Biometric Images using Reversible Watermarking</title><categories>cs.CV cs.IR</categories><comments>8 pages, 7 figures</comments><journal-ref>International Journal of Image Processing (IJIP), Volume:
  5,Issue:4, September/October 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric security is a fast growing area. Protecting biometric data is very
important since it can be misused by attackers. In order to increase security
of biometric data there are different methods in which watermarking is widely
accepted. A more acceptable, new important development in this area is
reversible watermarking in which the original image can be completely restored
and the watermark can be retrieved. But reversible watermarking in biometrics
is an understudied area. Reversible watermarking maintains high quality of
biometric data. This paper proposes Rotational Replacement of LSB as a
reversible watermarking scheme for biometric images. PSNR is the regular method
used for quality measurement of biometric data. In this paper we also show that
SSIM Index is a better alternate for effective quality assessment for
reversible watermarked biometric data by comparing with the well known
reversible watermarking scheme using Difference Expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1094</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1094</id><created>2011-11-04</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>On Three Challenges of Artificial Living Systems and Embodied Evolution</title><categories>cs.RO cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating autonomous, self-supporting, self-replicating, sustainable systems
is a great challenge. To some extent, understanding life means not only being
able to create it from scratch, but also improving, supporting, saving it, or
even making it even more advanced. This can be thought of as a long-term goal
of living technologies and embodied evolution. Current research agenda targets
several short- and middle-term steps towards achieving such a vision:
connection of ICT and bio-/chemo- developments, advances in &quot;soft&quot; and &quot;wet&quot;
robotics, integration of material science into developmental robotics, and
potentially, addressing the self-replication in autonomous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1103</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1103</id><created>2011-11-04</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Hengst</keyname><forenames>Stefan</forenames></author><author><keyname>Arias</keyname><forenames>Antonia P&#xe9;rez</forenames></author><author><keyname>Friedberger</keyname><forenames>Simon</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Using a Telepresence System to Investigate Route Choice Behavior</title><categories>cs.HC</categories><comments>Preprint of TGF11 (Traffic and Granular Flow, Moscow, September 2011)
  conference proceedings contribution</comments><doi>10.1007/978-3-642-39669-4_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combination of a telepresence system and a microscopic traffic simulator is
introduced. It is evaluated using a hotel evacuation scenario. Four different
kinds of supporting information are compared, standard exit signs, floor plans
with indicated exit routes, guiding lines on the floor and simulated agents
leading the way. The results indicate that guiding lines are the most efficient
way to support an evacuation but the natural behavior of following others comes
very close. On another level the results are consistent with previously
performed real and virtual experiments and validate the use of a telepresence
system in evacuation studies. It is shown that using a microscopic traffic
simulator extends the possibilities for evaluation, e.g. by adding simulated
humans to the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1109</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1109</id><created>2011-11-04</created><updated>2014-11-13</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Structure Theorem and Isomorphism Test for Graphs with Excluded
  Topological Subgraphs</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the structure theorem of Robertson and Seymour for graphs
excluding a fixed graph $H$ as a minor to graphs excluding $H$ as a topological
subgraph. We prove that for a fixed $H$, every graph excluding $H$ as a
topological subgraph has a tree decomposition where each part is either &quot;almost
embeddable&quot; to a fixed surface or has bounded degree with the exception of a
bounded number of vertices. Furthermore, we prove that such a decomposition is
computable by an algorithm that is fixed-parameter tractable with parameter
$|H|$.
  We present two algorithmic applications of our structure theorem. To
illustrate the mechanics of a &quot;typical&quot; application of the structure theorem,
we show that on graphs excluding $H$ as a topological subgraph, Partial
Dominating Set (find $k$ vertices whose closed neighborhood has maximum size)
can be solved in time $f(H,k)\cdot n^{O(1)}$ time. More significantly, we show
that on graphs excluding $H$ as a topological subgraph, Graph Isomorphism can
be solved in time $n^{f(H)}$. This result unifies and generalizes two
previously known important polynomial-time solvable cases of Graph Isomorphism:
bounded-degree graphs and $H$-minor free graphs. The proof of this result needs
a generalization of our structure theorem to the context of invariant treelike
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1124</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1124</id><created>2011-11-04</created><authors><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>Kletenik</keyname><forenames>Devorah</forenames></author><author><keyname>Sellie</keyname><forenames>Linda</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco</forenames></author></authors><title>Tight Bounds on Proper Equivalence Query Learning of DNF</title><categories>cs.LG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a new structural lemma for partial Boolean functions $f$, which we
call the seed lemma for DNF. Using the lemma, we give the first subexponential
algorithm for proper learning of DNF in Angluin's Equivalence Query (EQ) model.
The algorithm has time and query complexity $2^{(\tilde{O}{\sqrt{n}})}$, which
is optimal. We also give a new result on certificates for DNF-size, a simple
algorithm for properly PAC-learning DNF, and new results on EQ-learning $\log
n$-term DNF and decision trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1129</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1129</id><created>2011-11-04</created><authors><author><keyname>Wittmann</keyname><forenames>Markus</forenames></author><author><keyname>Zeiser</keyname><forenames>Thomas</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Domain decomposition and locality optimization for large-scale lattice
  Boltzmann simulations</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple, parallel and distributed algorithm for setting up and
partitioning a sparse representation of a regular discretized simulation
domain. This method is scalable for a large number of processes even for
complex geometries and ensures load balance between the domains, reasonable
communication interfaces, and good data locality within the domain. Applying
this scheme to a list-based lattice Boltzmann flow solver can achieve similar
or even higher flow solver performance than widely used standard graph
partition based tools such as METIS and PT-SCOTCH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1136</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1136</id><created>2011-11-04</created><updated>2011-11-14</updated><authors><author><keyname>Garber</keyname><forenames>Dan</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Universal MMSE Filtering With Logarithmic Adaptive Regret</title><categories>cs.LG cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online estimation of a real-valued signal
corrupted by oblivious zero-mean noise using linear estimators. The estimator
is required to iteratively predict the underlying signal based on the current
and several last noisy observations, and its performance is measured by the
mean-square-error. We describe and analyze an algorithm for this task which: 1.
Achieves logarithmic adaptive regret against the best linear filter in
hindsight. This bound is assyptotically tight, and resolves the question of
Moon and Weissman [1]. 2. Runs in linear time in terms of the number of filter
coefficients. Previous constructions required at least quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1144</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1144</id><created>2011-11-04</created><updated>2012-11-13</updated><authors><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author><author><keyname>Wang</keyname><forenames>Ligong</forenames></author></authors><title>The State-Dependent Semideterministic Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the capacity region of the state-dependent semideterministic
broadcast channel with noncausal state-information at the transmitter. One of
the two outputs of this channel is a deterministic function of the channel
input and the channel state, and the state is assumed to be known noncausally
to the transmitter but not to the receivers. We show that appending the state
to the deterministic output does not increase capacity.
  We also derive an outer bound on the capacity of general (not necessarily
semideterministic) state-dependent broadcast channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1162</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1162</id><created>2011-11-04</created><updated>2012-05-28</updated><authors><author><keyname>Dossal</keyname><forenames>Charles</forenames></author><author><keyname>Kachour</keyname><forenames>Maher</forenames></author><author><keyname>Fadili</keyname><forenames>Jalal M.</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames></author><author><keyname>Chesneau</keyname><forenames>Christophe</forenames></author></authors><title>The degrees of freedom of the Lasso for general design matrix</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>A short version appeared in SPARS'11, June 2011 Previously entitled
  &quot;The degrees of freedom of penalized l1 minimization&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the degrees of freedom ($\dof$) of penalized
$\ell_1$ minimization (also known as the Lasso) for linear regression models.
We give a closed-form expression of the $\dof$ of the Lasso response. Namely,
we show that for any given Lasso regularization parameter $\lambda$ and any
observed data $y$ belonging to a set of full (Lebesgue) measure, the
cardinality of the support of a particular solution of the Lasso problem is an
unbiased estimator of the degrees of freedom. This is achieved without the need
of uniqueness of the Lasso solution. Thus, our result holds true for both the
underdetermined and the overdetermined case, where the latter was originally
studied in \cite{zou}. We also show, by providing a simple counterexample, that
although the $\dof$ theorem of \cite{zou} is correct, their proof contains a
flaw since their divergence formula holds on a different set of a full measure
than the one that they claim. An effective estimator of the number of degrees
of freedom may have several applications including an objectively guided choice
of the regularization parameter in the Lasso through the $\sure$ framework. Our
theoretical findings are illustrated through several numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1170</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1170</id><created>2011-11-04</created><updated>2012-12-21</updated><authors><author><keyname>Morandi</keyname><forenames>Benjamin</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Record-replay debugging for the SCOOP concurrency model</title><categories>cs.DC</categories><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support developers in writing reliable and efficient concurrent programs,
novel concurrent programming abstractions have been proposed in recent years.
Programming with such abstractions requires new analysis tools because the
execution semantics often differs considerably from established models. We
present a record-replay technique for programs written in SCOOP, an
object-oriented programming model for concurrency. The resulting tool enables
developers to reproduce the nondeterministic execution of a concurrent program,
a necessary prerequisite for debugging and testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1191</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1191</id><created>2011-11-04</created><authors><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Constant Envelope Precoding for Power-Efficient Downlink Wireless
  Communication in Multi-User MIMO Systems Using Large Antenna Arrays</title><categories>cs.IT math.IT</categories><comments>Submitted to 2012 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP' 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider downlink cellular multi-user communication between a base station
(BS) having N antennas and M single-antenna users, i.e., an N X M Gaussian
Broadcast Channel (GBC). Under an average only total transmit power constraint
(APC), large antenna arrays at the BS (having tens to a few hundred antennas)
have been recently shown to achieve remarkable multi-user interference (MUI)
suppression with simple precoding techniques. However, building large arrays in
practice, would require cheap/power-efficient Radio-Frequency(RF) electronic
components. The type of transmitted signal that facilitates the use of most
power-efficient RF components is a constant envelope (CE) signal. Under certain
mild channel conditions (including i.i.d. fading), we analytically show that,
even under the stringent per-antenna CE transmission constraint (compared to
APC), MUI suppression can still be achieved with large antenna arrays. Our
analysis also reveals that, with a fixed M and increasing N, the total
transmitted power can be reduced while maintaining a constant
signal-to-interference-noise-ratio (SINR) level at each user. We also propose a
novel low-complexity CE precoding scheme, using which, we confirm our
analytical observations for the i.i.d. Rayleigh fading channel, through
Monte-Carlo simulations. Simulation of the information sum-rate under the
per-antenna CE constraint, shows that, for a fixed M and a fixed desired
sum-rate, the required total transmit power decreases linearly with increasing
N, i.e., an O(N) array power gain. Also, in terms of the total transmit power
required to achieve a fixed desired information sum-rate, despite the stringent
per-antenna CE constraint, the proposed CE precoding scheme performs close to
the GBC sum-capacity (under APC) achieving scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1224</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1224</id><created>2011-11-04</created><authors><author><keyname>Cheng</keyname><forenames>Qi</forenames></author><author><keyname>Hill</keyname><forenames>Joshua E.</forenames></author><author><keyname>Wan</keyname><forenames>Daqing</forenames></author></authors><title>Counting Value Sets: Algorithm and Complexity</title><categories>math.NT cs.CC</categories><msc-class>11Yxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be a prime. Given a polynomial in $\F_{p^m}[x]$ of degree $d$ over
the finite field $\F_{p^m}$, one can view it as a map from $\F_{p^m}$ to
$\F_{p^m}$, and examine the image of this map, also known as the value set. In
this paper, we present the first non-trivial algorithm and the first complexity
result on computing the cardinality of this value set. We show an elementary
connection between this cardinality and the number of points on a family of
varieties in affine space. We then apply Lauder and Wan's $p$-adic
point-counting algorithm to count these points, resulting in a non-trivial
algorithm for calculating the cardinality of the value set. The running time of
our algorithm is $(pmd)^{O(d)}$. In particular, this is a polynomial time
algorithm for fixed $d$ if $p$ is reasonably small. We also show that the
problem is #P-hard when the polynomial is given in a sparse representation,
$p=2$, and $m$ is allowed to vary, or when the polynomial is given as a
straight-line program, $m=1$ and $p$ is allowed to vary. Additionally, we prove
that it is NP-hard to decide whether a polynomial represented by a
straight-line program has a root in a prime-order finite field, thus resolving
an open problem proposed by Kaltofen and Koiran in
\cite{Kaltofen03,KaltofenKo05}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1227</identifier>
 <datestamp>2011-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1227</id><created>2011-11-04</created><authors><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>More Voices Than Ever? Quantifying Media Bias in Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 Pages, 7 figures, appeared in ICWSM 2011</comments><journal-ref>Proceedings of the Fifth International AAAI Conference on Weblogs
  and Social Media (ICWSM 2011), 17-21 July 2011, Barcelona, Spain</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media, such as blogs, are often seen as democratic entities that allow
more voices to be heard than the conventional mass or elite media. Some also
feel that social media exhibits a balancing force against the arguably slanted
elite media. A systematic comparison between social and mainstream media is
necessary but challenging due to the scale and dynamic nature of modern
communication. Here we propose empirical measures to quantify the extent and
dynamics of social (blog) and mainstream (news) media bias. We focus on a
particular form of bias---coverage quantity---as applied to stories about the
111th US Congress. We compare observed coverage of Members of Congress against
a null model of unbiased coverage, testing for biases with respect to political
party, popular front runners, regions of the country, and more. Our measures
suggest distinct characteristics in news and blog media. A simple generative
model, in agreement with data, reveals differences in the process of coverage
selection between the two media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1250</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1250</id><created>2011-11-04</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szaga</keyname><forenames>Pawel</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Using Transcoding for Hidden Communication in IP Telephony</title><categories>cs.CR cs.MM</categories><comments>17 pages, 16 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new steganographic method for IP telephony called
TranSteg (Transcoding Steganography). Typically, in steganographic
communication it is advised for covert data to be compressed in order to limit
its size. In TranSteg it is the overt data that is compressed to make space for
the steganogram. The main innovation of TranSteg is to, for a chosen voice
stream, find a codec that will result in a similar voice quality but smaller
voice payload size than the originally selected. Then, the voice stream is
transcoded. At this step the original voice payload size is intentionally
unaltered and the change of the codec is not indicated. Instead, after placing
the transcoded voice payload, the remaining free space is filled with hidden
data. TranSteg proof of concept implementation was designed and developed. The
obtained experimental results are enclosed in this paper. They prove that the
proposed method is feasible and offers a high steganographic bandwidth.
TranSteg detection is difficult to perform when performing inspection in a
single network localisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1261</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1261</id><created>2011-11-04</created><authors><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>A Casual Tour Around a Circuit Complexity Bound</title><categories>cs.CC</categories><comments>21 pages, 2 figures. An earlier version appeared in SIGACT News,
  September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I will discuss the recent proof that the complexity class NEXP
(nondeterministic exponential time) lacks nonuniform ACC circuits of polynomial
size. The proof will be described from the perspective of someone trying to
discover it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1274</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1274</id><created>2011-11-04</created><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Panagiotou</keyname><forenames>Konstantinos</forenames></author></authors><title>Catching the k-NAESAT Threshold</title><categories>cs.DM math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best current estimates of the thresholds for the existence of solutions
in random constraint satisfaction problems ('CSPs') mostly derive from the
first and the second moment method. Yet apart from a very few exceptional cases
these methods do not quite yield matching upper and lower bounds. According to
deep but non-rigorous arguments from statistical mechanics, this discrepancy is
due to a change in the geometry of the set of solutions called condensation
that occurs shortly before the actual threshold for the existence of solutions
(Krzakala, Montanari, Ricci-Tersenghi, Semerjian, Zdeborova: PNAS 2007). To
cope with condensation, physicists have developed a sophisticated but
non-rigorous formalism called Survey Propagation (Mezard, Parisi, Zecchina:
Science 2002). This formalism yields precise conjectures on the threshold
values of many random CSPs. Here we develop a new Survey Propagation inspired
second moment method for the random k-NAESAT problem, which is one of the
standard benchmark problems in the theory of random CSPs. This new technique
allows us to overcome the barrier posed by condensation rigorously. We prove
that the threshold for the existence of solutions in random $k$-NAESAT is
$2^{k-1}\ln2-(\frac{\ln2}2+\frac14)+\eps_k$, where $|\eps_k| \le
2^{-(1-o_k(1))k}$, thereby verifying the statistical mechanics conjecture for
this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1278</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1278</id><created>2011-11-04</created><authors><author><keyname>Chum</keyname><forenames>Chi Sing</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaowen</forenames></author></authors><title>Hash function based secret sharing scheme designs</title><categories>cs.CR</categories><comments>Submitted to Security and Communication Networks (Wiley)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret sharing schemes create an effective method to safeguard a secret by
dividing it among several participants. By using hash functions and the herding
hashes technique, we first set up a (t+1, n) threshold scheme which is perfect
and ideal, and then extend it to schemes for any general access structure. The
schemes can be further set up as proactive or verifiable if necessary. The
setup and recovery of the secret is efficient due to the fast calculation of
the hash function. The proposed scheme is flexible because of the use of
existing hash functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1301</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1301</id><created>2011-11-05</created><authors><author><keyname>Patnaikuni</keyname><forenames>P. Shrinivasan. R.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Raj. B.</forenames></author></authors><title>An architecture for &quot;Web Of Things&quot; using SOCKS protocol based IPv6/IPv4
  gatewaying for heterogeneous communication</title><categories>cs.NI</categories><comments>10 pages 5 Figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  &quot;Web Of Things&quot; evolved from &quot;Internet Of Things&quot;. Lot of research has been
done in designing architecture for &quot;Web Of Things&quot;. Two main architectures are
Smart gateway based architecture and embedded Web Server based architecture.
These architectures address some of the basic and essential issues relating to
Service Oriented Architecture for &quot;Web Of Things&quot;. Taking into consideration
the period of coexistence of IPv4 and IPv6 we propose an architecture using
SOCKS protocol based IPv6/IPv4 gatewaying and refinements which facilitates
smooth heterogeneous communications between the IPv6 and IPv4 enabled embedded
nodes and can potentially be used to prevent security threats like
Denial-of-Service (DoS) attacks on embedded devices attached to the web and
increase its performance. Our architecture provides a way for caching responses
from device and thereby increasing its efficiency and performance and yielding
quick response times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1311</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1311</id><created>2011-11-05</created><authors><author><keyname>Herrmann</keyname><forenames>Richard</forenames></author></authors><title>Covariant fractional extension of the modified Laplace-operator used in
  3D-shape recovery</title><categories>cs.CV</categories><comments>5 pages, 3 figures, draft for proceedings IFAC FDA12 in Nanjing,
  China</comments><report-no>gigahedron1111.01</report-no><journal-ref>Fract. Calc. Appl. Anal. (2012) Vol. 15 Num. 2, 332--343</journal-ref><doi>10.2478/s13540-012-0024-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extending the Liouville-Caputo definition of a fractional derivative to a
nonlocal covariant generalization of arbitrary bound operators acting on
multidimensional Riemannian spaces an appropriate approach for the 3D shape
recovery of aperture afflicted 2D slide sequences is proposed. We demonstrate,
that the step from a local to a nonlocal algorithm yields an order of magnitude
in accuracy and by using the specific fractional approach an additional factor
2 in accuracy of the derived results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1315</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1315</id><created>2011-11-05</created><updated>2012-03-06</updated><authors><author><keyname>Wang</keyname><forenames>Yuyang</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author></authors><title>Nonparametric Bayesian Estimation of Periodic Functions</title><categories>cs.LG astro-ph.IM</categories><doi>10.1088/0004-637X/756/1/67</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world problems exhibit patterns that have periodic behavior. For
example, in astrophysics, periodic variable stars play a pivotal role in
understanding our universe. An important step when analyzing data from such
processes is the problem of identifying the period: estimating the period of a
periodic function based on noisy observations made at irregularly spaced time
points. This problem is still a difficult challenge despite extensive study in
different disciplines. The paper makes several contributions toward solving
this problem. First, we present a nonparametric Bayesian model for period
finding, based on Gaussian Processes (GP), that does not make strong
assumptions on the shape of the periodic function. As our experiments
demonstrate, the new model leads to significantly better results in period
estimation when the target function is non-sinusoidal. Second, we develop a new
algorithm for parameter optimization for GP which is useful when the likelihood
function is very sensitive to the setting of the hyper-parameters with numerous
local minima, as in the case of period estimation. The algorithm combines
gradient optimization with grid search and incorporates several mechanisms to
overcome the high complexity of inference with GP. Third, we develop a novel
approach for using domain knowledge, in the form of a probabilistic generative
model, and incorporate it into the period estimation algorithm. Experimental
results on astrophysics data validate our approach showing significant
improvement over the state of the art in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1321</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1321</id><created>2011-11-05</created><authors><author><keyname>Varlamov</keyname><forenames>Oleg O.</forenames></author></authors><title>MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and
  Practical Realization of Automated Constructor of Algorithms Handling More
  than Three Million Production Rules</title><categories>cs.AI</categories><comments>23 pages, 21 figures</comments><msc-class>68T01, 68T27, 68T20, 68T30</msc-class><acm-class>I.2; D.1.6; I.2.1; I.2.2; I.2.4; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretical transition from the graphs of production systems to the
bipartite graphs of the MIVAR nets is shown. Examples of the implementation of
the MIVAR nets in the formalisms of matrixes and graphs are given. The linear
computational complexity of algorithms for automated building of objects and
rules of the MIVAR nets is theoretically proved. On the basis of the MIVAR nets
the UDAV software complex is developed, handling more than 1.17 million objects
and more than 3.5 million rules on ordinary computers. The results of
experiments that confirm a linear computational complexity of the MIVAR method
of information processing are given.
  Keywords: MIVAR, MIVAR net, logical inference, computational complexity,
artificial intelligence, intelligent systems, expert systems, General Problem
Solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1328</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1328</id><created>2011-11-05</created><authors><author><keyname>Duan</keyname><forenames>Xueying</forenames></author><author><keyname>Wang</keyname><forenames>Qichun</forenames></author></authors><title>Two Classes of Crooked Multinomials Inequivalent to Power Functions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that crooked functions can be used to construct many interesting
combinatorial objects, and a quadratic function is crooked if and only if it is
almost perfect nonlinear (APN). In this paper, we introduce two infinite
classes of quadratic crooked multinomials on fields of order $2^{2m}$. One
class of APN functions constructed in [7] is a particular case of the one we
construct in Theorem 1. Moreover, we prove that the two classes of crooked
functions constructed in this paper are EA inequivalent to power functions and
conjecture that CCZ inequivalence between them also holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1347</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1347</id><created>2011-11-05</created><authors><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Gao</keyname><forenames>Su</forenames></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author></authors><title>Wyner-Ziv Coding Based on Multidimensional Nested Lattices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed source coding (DSC) addresses the compression of correlated
sources without communication links among them. This paper is concerned with
the Wyner-Ziv problem: coding of an information source with side information
available only at the decoder in the form of a noisy version of the source.
Both the theoretical analysis and code design are addressed in the framework of
multi-dimensional nested lattice coding (NLC). For theoretical analysis,
accurate computation of the rate-distortion function is given under the
high-resolution assumption, and a new upper bound using the derivative of the
theta series is derived. For practical code design, several techniques with low
complexity are proposed. Compared to the existing Slepian-Wolf coded nested
quantization (SWC-NQ) for Wyner-Ziv coding based on one or two-dimensional
lattices, our proposed multi-dimensional NLC can offer better performance at
arguably lower complexity, since it does not require the second stage of
Slepian-Wolf coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1353</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1353</id><created>2011-11-05</created><authors><author><keyname>Paul</keyname><forenames>Gerald</forenames></author></authors><title>An efficient implementation of the simulated annealing heuristic for the
  quadratic assignment problem</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. One of the most powerful and commonly used
heuristics to obtain approximations to the optimal solution of the QAP is
simulated annealing (SA). We present an efficient implementation of the SA
heuristic which performs more than 100 times faster then existing
implementations for large problem sizes and a large number of SA iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1355</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1355</id><created>2011-11-05</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>A Compressed Self-Index for Genomic Databases</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in DNA sequencing technology will soon result in databases of
thousands of genomes. Within a species, individuals' genomes are almost exact
copies of each other; e.g., any two human genomes are 99.9% the same. Relative
Lempel-Ziv (RLZ) compression takes advantage of this property: it stores the
first genome uncompressed or as an FM-index, then compresses the other genomes
with a variant of LZ77 that copies phrases only from the first genome. RLZ
achieves good compression and supports fast random access; in this paper we
show how to support fast search as well, thus obtaining an efficient compressed
self-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1365</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1365</id><created>2011-11-05</created><updated>2012-01-18</updated><authors><author><keyname>Zhang</keyname><forenames>Shihua</forenames></author><author><keyname>Zhao</keyname><forenames>Junfei</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang-Sun</forenames></author></authors><title>Co-community Structure in Time-varying Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>5 pages, 6 figures</comments><journal-ref>Phys. Rev. E 2012, 85, 056110</journal-ref><doi>10.1103/PhysRevE.85.056110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we introduce the concept of co-community structure in
time-varying networks. We propose a novel optimization algorithm to rapidly
detect co-community structure in these networks. Both theoretical and numerical
results show that the proposed method not only can resolve detailed
co-communities, but also can effectively identify the dynamical phenomena in
these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1373</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1373</id><created>2011-11-06</created><authors><author><keyname>Spencer</keyname><forenames>Jason</forenames></author></authors><title>Speculative Parallel Evaluation Of Classification Trees On GPGPU Compute
  Engines</title><categories>cs.DC cs.CV</categories><comments>14 pages, 4 figures, 5 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the problem of optimizing classification tree evaluation for
on-line and real-time applications by using GPUs. Looking at trees with
continuous attributes often used in image segmentation, we first put the
existing algorithms for serial and data-parallel evaluation on solid footings.
We then introduce a speculative parallel algorithm designed for single
instruction, multiple data (SIMD) architectures commonly found in GPUs. A
theoretical analysis shows how the run times of data and speculative
decompositions compare assuming independent processors. To compare the
algorithms in the SIMD environment, we implement both on a CUDA 2.0
architecture machine and compare timings to a serial CPU implementation.
Various optimizations and their effects are discussed, and results are given
for all algorithms. Our specific tests show a speculative algorithm improves
run time by 25% compared to a data decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1378</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1378</id><created>2011-11-06</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Specimens: &quot;most of&quot; generic NPs in a contextually flexible type theory</title><categories>math.LO cs.LO</categories><comments>Genius III (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to compute the meanings associated to sentences with
generic NPs corresponding to the most of generalized quantifier. We call these
generics specimens and they resemble stereotypes or prototypes in lexical
semantics. The meanings are viewed as logical formulae that can be thereafter
interpreted in your favorite models. We rather depart from the dominant Fregean
single untyped universe and go for type theory with hints from Hilbert epsilon
calculus and from medieval philosophy. Our type theoretic analysis bears some
resemblance with on going work in lexical semantics. Our model also applies to
classical examples involving a class (or a generic element of this class) which
is provided by the context. An outcome of this study is that, in the
minimalism-contextualism debate, if one adopts a type theoretical view, terms
encode the purely semantic meaning component while their typing is
pragmatically determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1386</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1386</id><created>2011-11-06</created><authors><author><keyname>Mejer</keyname><forenames>Avihai</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Confidence Estimation in Structured Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured classification tasks such as sequence labeling and dependency
parsing have seen much interest by the Natural Language Processing and the
machine learning communities. Several online learning algorithms were adapted
for structured tasks such as Perceptron, Passive- Aggressive and the recently
introduced Confidence-Weighted learning . These online algorithms are easy to
implement, fast to train and yield state-of-the-art performance. However,
unlike probabilistic models like Hidden Markov Model and Conditional random
fields, these methods generate models that output merely a prediction with no
additional information regarding confidence in the correctness of the output.
In this work we fill the gap proposing few alternatives to compute the
confidence in the output of non-probabilistic algorithms.We show how to compute
confidence estimates in the prediction such that the confidence reflects the
probability that the word is labeled correctly. We then show how to use our
methods to detect mislabeled words, trade recall for precision and active
learning. We evaluate our methods on four noun-phrase chunking and named entity
recognition sequence labeling tasks, and on dependency parsing for 14
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1396</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1396</id><created>2011-11-06</created><authors><author><keyname>Khajehnejad</keyname><forenames>M. Amin</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Improving the Thresholds of Sparse Recovery: An Analysis of a Two-Step
  Reweighted Basis Pursuit Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that $\ell_1$ minimization can be used to recover
sufficiently sparse unknown signals from compressed linear measurements. In
fact, exact thresholds on the sparsity, as a function of the ratio between the
system dimensions, so that with high probability almost all sparse signals can
be recovered from i.i.d. Gaussian measurements, have been computed and are
referred to as &quot;weak thresholds&quot; \cite{D}. In this paper, we introduce a
reweighted $\ell_1$ recovery algorithm composed of two steps: a standard
$\ell_1$ minimization step to identify a set of entries where the signal is
likely to reside, and a weighted $\ell_1$ minimization step where entries
outside this set are penalized. For signals where the non-sparse component
entries are independent and identically drawn from certain classes of
distributions, (including most well known continuous distributions), we prove a
\emph{strict} improvement in the weak recovery threshold. Our analysis suggests
that the level of improvement in the weak threshold depends on the behavior of
the distribution at the origin. Numerical simulations verify the distribution
dependence of the threshold improvement very well, and suggest that in the case
of i.i.d. Gaussian nonzero entries, the improvement can be quite
impressive---over 20% in the example we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1400</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1400</id><created>2011-11-06</created><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Styer</keyname><forenames>Michael</forenames></author><author><keyname>Moratto</keyname><forenames>Zachary</forenames></author><author><keyname>Nefian</keyname><forenames>Ara</forenames></author><author><keyname>Broxton</keyname><forenames>Michael</forenames></author></authors><title>Student's T Robust Bundle Adjustment Algorithm</title><categories>stat.CO cs.GR math.OC</categories><comments>8 pages. Originally written in November 2009. Describes
  implementation of Robust Bundle Adjustment in NASA's VisionWorkbench package,
  available at https://github.com/visionworkbench/visionworkbench</comments><msc-class>62F35, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bundle adjustment (BA) is the problem of refining a visual reconstruction to
produce better structure and viewing parameter estimates. This problem is often
formulated as a nonlinear least squares problem, where data arises from
interest point matching. Mismatched interest points cause serious problems in
this approach, as a single mismatch will affect the entire reconstruction. In
this paper, we propose a novel robust Student's t BA algorithm (RST-BA). We
model reprojection errors using the heavy tailed Student's t-distribution, and
use an implicit trust region method to compute the maximum a posteriori (MAP)
estimate of the camera and viewing parameters in this model. The resulting
algorithm exploits the sparse structure essential for reconstructing
multi-image scenarios, has the same time complexity as standard L2 bundle
adjustment (L2-BA), and can be implemented with minimal changes to the standard
least squares framework. We show that the RST-BA is more accurate than either
L2-BA or L2-BA with a sigma-edit rule for outlier removal for a range of
simulated error generation scenarios. The new method has also been used to
reconstruct lunar topography using data from the NASA Apollo 15 orbiter, and we
present visual and quantitative comparisons of RST-BA and L2-BA methods for
this application. In particular, using the RST-BA algorithm we were able to
reconstruct a DEM from unprocessed data with many outliers and no ground
control points, which was not possible with the L2-BA method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1410</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1410</id><created>2011-11-06</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Li</keyname><forenames>Qin</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Shu</keyname><forenames>Shi</forenames></author></authors><title>Breaking a chaotic image encryption algorithm based on perceptron model</title><categories>cs.CR</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a chaotic image encryption algorithm based on perceptron model was
proposed. The present paper analyzes security of the algorithm and finds that
the equivalent secret key can be reconstructed with only one pair of
known-plaintext/ciphertext, which is supported by both mathematical proof and
experiment results. In addition, some other security defects are also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1414</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1414</id><created>2011-11-06</created><updated>2012-08-17</updated><authors><author><keyname>Raeth</keyname><forenames>C.</forenames></author><author><keyname>Gliozzi</keyname><forenames>M.</forenames></author><author><keyname>Papadakis</keyname><forenames>I. E.</forenames></author><author><keyname>Brinkmann</keyname><forenames>W.</forenames></author></authors><title>Revisiting algorithms for generating surrogate time series</title><categories>physics.data-an astro-ph.HE cs.CE nlin.CD</categories><comments>5 pages, 4 figures, accepted for publication in PRL</comments><doi>10.1103/PhysRevLett.109.144101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of surrogates is one of the key concepts of nonlinear data
analysis. Here, we demonstrate that commonly used algorithms for generating
surrogates often fail to generate truly linear time series. Rather, they create
surrogate realizations with Fourier phase correlations leading to
non-detections of nonlinearities. We argue that reliable surrogates can only be
generated, if one tests separately for static and dynamic nonlinearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1418</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1418</id><created>2011-11-06</created><authors><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Robins</keyname><forenames>James</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Efficient Nonparametric Conformal Prediction Regions</title><categories>math.ST cs.LG stat.TH</categories><msc-class>62G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate and extend the conformal prediction method due to
Vovk,Gammerman and Shafer (2005) to construct nonparametric prediction regions.
These regions have guaranteed distribution free, finite sample coverage,
without any assumptions on the distribution or the bandwidth. Explicit
convergence rates of the loss function are established for such regions under
standard regularity conditions. Approximations for simplifying implementation
and data driven bandwidth selection methods are also discussed. The theoretical
properties of our method are demonstrated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1422</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1422</id><created>2011-11-06</created><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author></authors><title>Robust Interactive Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and study a generalization of the standard
active-learning model where a more general type of query, class conditional
query, is allowed. Such queries have been quite useful in applications, but
have been lacking theoretical understanding. In this work, we characterize the
power of such queries under two well-known noise models. We give nearly tight
upper and lower bounds on the number of queries needed to learn both for the
general agnostic setting and for the bounded noise model. We further show that
our methods can be made adaptive to the (unknown) noise rate, with only
negligible loss in query complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1423</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1423</id><created>2011-11-06</created><authors><author><keyname>Chadha</keyname><forenames>Aman R.</forenames></author><author><keyname>Vaidya</keyname><forenames>Pallavi P.</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>Face Recognition Using Discrete Cosine Transform for Global and Local
  Features</title><categories>cs.CV cs.CR</categories><comments>face recognition; biometrics; person identification; authentication;
  discrete cosine transform; DCT; global local features; Proceedings of the
  2011 International Conference on Recent Advancements in Electrical,
  Electronics and Control Engineering (IConRAEeCE) IEEE Xplore: CFP1153R-ART;
  ISBN: 978-1-4577-2149-6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face Recognition using Discrete Cosine Transform (DCT) for Local and Global
Features involves recognizing the corresponding face image from the database.
The face image obtained from the user is cropped such that only the frontal
face image is extracted, eliminating the background. The image is restricted to
a size of 128 x 128 pixels. All images in the database are gray level images.
DCT is applied to the entire image. This gives DCT coefficients, which are
global features. Local features such as eyes, nose and mouth are also extracted
and DCT is applied to these features. Depending upon the recognition rate
obtained for each feature, they are given weightage and then combined. Both
local and global features are used for comparison. By comparing the ranks for
global and local features, the false acceptance rate for DCT can be minimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1426</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1426</id><created>2011-11-06</created><updated>2011-11-09</updated><authors><author><keyname>Roy</keyname><forenames>Rajat S.</forenames></author><author><keyname>Chen</keyname><forenames>Kevin C.</forenames></author><author><keyname>Sengupta</keyname><forenames>Anirvan M.</forenames></author><author><keyname>Schliep</keyname><forenames>Alexander</forenames></author></authors><title>SLIQ: Simple Linear Inequalities for Efficient Contig Scaffolding</title><categories>q-bio.GN cs.CE</categories><comments>16 pages, 6 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scaffolding is an important subproblem in &quot;de novo&quot; genome assembly in which
mate pair data are used to construct a linear sequence of contigs separated by
gaps. Here we present SLIQ, a set of simple linear inequalities derived from
the geometry of contigs on the line that can be used to predict the relative
positions and orientations of contigs from individual mate pair reads and thus
produce a contig digraph. The SLIQ inequalities can also filter out unreliable
mate pairs and can be used as a preprocessing step for any scaffolding
algorithm. We tested the SLIQ inequalities on five real data sets ranging in
complexity from simple bacterial genomes to complex mammalian genomes and
compared the results to the majority voting procedure used by many other
scaffolding algorithms. SLIQ predicted the relative positions and orientations
of the contigs with high accuracy in all cases and gave more accurate position
predictions than majority voting for complex genomes, in particular the human
genome. Finally, we present a simple scaffolding algorithm that produces linear
scaffolds given a contig digraph. We show that our algorithm is very efficient
compared to other scaffolding algorithms while maintaining high accuracy in
predicting both contig positions and orientations for real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1429</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1429</id><created>2011-11-06</created><authors><author><keyname>Afuwoqi</keyname><forenames>Anthony</forenames></author><author><keyname>Wu</keyname><forenames>Hongyou</forenames></author></authors><title>Promoting Industry-University Partnership in Information Technology</title><categories>cs.CY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is becoming increasingly difficult for Nigerian universities to go it
alone in terms of serving as a citadel of learning, coping with the huge wage
bill and competing with their peers in other parts of the world, due to
competitive, economic and other pressures. As a consequence, Nigerian
universities are left with no option than to carry their industrial partners
along in terms of research and development through the formation of
partnerships for their mutual benefit. Since the industries are established for
profit making and the universities for knowledge enhancement, such partnerships
would help in spreading the costs in terms of provision of knowledge and costs
of research. This paper discusses the various types of partnerships involving
industries and universities, the benefits derived and a possible model for the
working of such a partnership which could be adapted to other sectors and
countries in sub-Saharan Africa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1432</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1432</id><created>2011-11-06</created><authors><author><keyname>Kieffer</keyname><forenames>J.</forenames></author><author><keyname>Flajolet</keyname><forenames>P.</forenames></author><author><keyname>Yang</keyname><forenames>E. -h.</forenames></author></authors><title>Universal Lossless Data Compression Via Binary Decision Diagrams</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary string of length $2^k$ induces the Boolean function of $k$ variables
whose Shannon expansion is the given binary string. This Boolean function then
is representable via a unique reduced ordered binary decision diagram (ROBDD).
The given binary string is fully recoverable from this ROBDD. We exhibit a
lossless data compression algorithm in which a binary string of length a power
of two is compressed via compression of the ROBDD associated to it as described
above.
  We show that when binary strings of length $n$ a power of two are compressed
via this algorithm, the maximal pointwise redundancy/sample with respect to any
s-state binary information source has the upper bound
$(4\log_2s+16+o(1))/\log_2n $. To establish this result, we exploit a result of
Liaw and Lin stating that the ROBDD representation of a Boolean function of $k$
variables contains a number of vertices on the order of $(2+o(1))2^{k}/k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1453</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1453</id><created>2011-11-06</created><updated>2015-01-13</updated><authors><author><keyname>Abhishek</keyname><forenames>Vineet</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Williams</keyname><forenames>Steven R.</forenames></author></authors><title>On Bidding with Securities: Risk Aversion and Positive Dependence</title><categories>cs.GT</categories><comments>21 pages, 3 figures. This is a work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DeMarzo et al. (2005) consider auctions in which bids are selected from a
completely ordered family of securities whose values are tied to the resource
being auctioned. The paper defines a notion of relative steepness of families
of securities and shows that a steeper family provides greater expected revenue
to the seller. Two assumptions are: the buyers are risk-neutral; the random
variables through which values and signals of the buyers are realized are
affiliated. We show that this revenue ranking holds for the second price
auction in the case of risk-aversion. However, it does not hold if affiliation
is relaxed to a less restrictive form of positive dependence, namely first
order stochastic dominance (FOSD). We define the relative strong steepness of
families of securities and show that it provides a necessary and sufficient
condition for comparing two families in the FOSD case. All results extend to
the English auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1461</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1461</id><created>2011-11-06</created><authors><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author></authors><title>Multimodal diff-hash</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require comparing multimodal data with different structure
and dimensionality that cannot be compared directly. Recently, there has been
increasing interest in methods for learning and efficiently representing such
multimodal similarity. In this paper, we present a simple algorithm for
multimodal similarity-preserving hashing, trying to map multimodal data into
the Hamming space while preserving the intra- and inter-modal similarities. We
show that our method significantly outperforms the state-of-the-art method in
the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1462</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1462</id><created>2011-11-06</created><updated>2013-09-23</updated><authors><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Rosenbaum</keyname><forenames>David J.</forenames></author></authors><title>Uselessness for an Oracle Model with Internal Randomness</title><categories>quant-ph cs.CC</categories><comments>18 pages. v2. shortened, presentation improved, same results</comments><journal-ref>Q. Inf. Comput. vol. 14, no. 7&amp;8, pp. 608-624 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of the standard oracle model in which the oracle
acts on the target with a permutation selected according to internal random
coins. We describe several problems that are impossible to solve classically
but can be solved by a quantum algorithm using a single query; we show that
such infinity-vs-one separations between classical and quantum query
complexities can be constructed from much weaker separations.
  We also give conditions to determine when oracle problems---either in the
standard model, or in any of the generalizations we consider---cannot be solved
with success probability better than random guessing would achieve. In the
oracle model with internal randomness where the goal is to gain any nonzero
advantage over guessing, we prove (roughly speaking) that $k$ quantum queries
are equivalent in power to $2k$ classical queries, thus extending results of
Meyer and Pommersheim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1464</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1464</id><created>2011-11-06</created><updated>2013-06-22</updated><authors><author><keyname>Brazil</keyname><forenames>Marcus N.</forenames></author><author><keyname>Ras</keyname><forenames>Charl J.</forenames></author><author><keyname>Swanepoel</keyname><forenames>Konrad J.</forenames></author><author><keyname>Thomas</keyname><forenames>Doreen A.</forenames></author></authors><title>Generalised k-Steiner Tree Problems in Normed Planes</title><categories>math.CO cs.DS</categories><journal-ref>Algorithmica: Volume 71, Issue 1 (2015), Page 66-86</journal-ref><doi>10.1007/s00453-013-9780-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 1-Steiner tree problem, the problem of constructing a Steiner minimum
tree containing at most one Steiner point, has been solved in the Euclidean
plane by Georgakopoulos and Papadimitriou using plane subdivisions called
oriented Dirichlet cell partitions. Their algorithm produces an optimal
solution within $O(n^2)$ time. In this paper we generalise their approach in
order to solve the $k$-Steiner tree problem, in which the Steiner minimum tree
may contain up to $k$ Steiner points for a given constant $k$. We also extend
their approach further to encompass other normed planes, and to solve a much
wider class of problems, including the $k$-bottleneck Steiner tree problem and
other generalised $k$-Steiner tree problems. We show that, for any fixed $k$,
such problems can be solved in $O(n^{2k})$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1486</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1486</id><created>2011-11-06</created><authors><author><keyname>Wang</keyname><forenames>Yisong</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author><author><keyname>Yuan</keyname><forenames>Li Yan</forenames></author><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author></authors><title>Embedding Description Logic Programs into Default Logic</title><categories>cs.AI</categories><comments>53 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description logic programs (dl-programs) under the answer set semantics
formulated by Eiter {\em et al.} have been considered as a prominent formalism
for integrating rules and ontology knowledge bases. A question of interest has
been whether dl-programs can be captured in a general formalism of nonmonotonic
logic. In this paper, we study the possibility of embedding dl-programs into
default logic. We show that dl-programs under the strong and weak answer set
semantics can be embedded in default logic by combining two translations, one
of which eliminates the constraint operator from nonmonotonic dl-atoms and the
other translates a dl-program into a default theory. For dl-programs without
nonmonotonic dl-atoms but with the negation-as-failure operator, our embedding
is polynomial, faithful, and modular. In addition, our default logic encoding
can be extended in a simple way to capture recently proposed weakly
well-supported answer set semantics, for arbitrary dl-programs. These results
reinforce the argument that default logic can serve as a fruitful foundation
for query-based approaches to integrating ontology and rules. With its simple
syntax and intuitive semantics, plus available computational results, default
logic can be considered an attractive approach to integration of ontology and
rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1491</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1491</id><created>2011-11-07</created><authors><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>Approximating the Exponential, the Lanczos Method and an
  \tilde{O}(m)-Time Spectral Algorithm for Balanced Separator</title><categories>cs.DS cs.NA math.CA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a novel spectral approximation algorithm for the balanced separator
problem that, given a graph G, a constant balance b \in (0,1/2], and a
parameter \gamma, either finds an \Omega(b)-balanced cut of conductance
O(\sqrt(\gamma)) in G, or outputs a certificate that all b-balanced cuts in G
have conductance at least \gamma, and runs in time \tilde{O}(m). This settles
the question of designing asymptotically optimal spectral algorithms for
balanced separator. Our algorithm relies on a variant of the heat kernel random
walk and requires, as a subroutine, an algorithm to compute \exp(-L)v where L
is the Laplacian of a graph related to G and v is a vector. Algorithms for
computing the matrix-exponential-vector product efficiently comprise our next
set of results. Our main result here is a new algorithm which computes a good
approximation to \exp(-A)v for a class of PSD matrices A and a given vector u,
in time roughly \tilde{O}(m_A), where m_A is the number of non-zero entries of
A. This uses, in a non-trivial way, the result of Spielman and Teng on
inverting SDD matrices in \tilde{O}(m_A) time. Finally, we prove e^{-x} can be
uniformly approximated up to a small additive error, in a non-negative interval
[a,b] with a polynomial of degree roughly \sqrt{b-a}. While this result is of
independent interest in approximation theory, we show that, via the Lanczos
method from numerical analysis, it yields a simple algorithm to compute
\exp(-A)v for PSD matrices that runs in time roughly O(t_A \sqrt{||A||}), where
t_A is the time required for computation of the vector Aw for given vector w.
As an application, we obtain a simple and practical algorithm, with output
conductance O(\sqrt(\gamma)), for balanced separator that runs in time
\tilde{O}(m/\sqrt(\gamma)). This latter algorithm matches the running time, but
improves on the approximation guarantee of the algorithm by Andersen and Peres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1492</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1492</id><created>2011-11-07</created><authors><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author><author><keyname>Souissi</keyname><forenames>Samia</forenames></author><author><keyname>Katayama</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Inuzuka</keyname><forenames>Nobuhiro</forenames></author><author><keyname>D&#xe9;fago</keyname><forenames>Xavier</forenames></author><author><keyname>Wada</keyname><forenames>Koichi</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>The Gathering Problem for Two Oblivious Robots with Unreliable Compasses</title><categories>cs.DC cs.DS cs.RO</categories><comments>23 pages, 10 figures, to appear at SIAM Journal on Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous mobile robots are often classified into synchronous,
semi-synchronous and asynchronous robots when discussing the pattern formation
problem. For semi-synchronous robots, all patterns formable with memory are
also formable without memory, with the single exception of forming a point
(i.e., the gathering) by two robots. However, the gathering problem for two
semi-synchronous robots without memory is trivially solvable when their local
coordinate systems are consistent, and the impossibility proof essentially uses
the inconsistencies in their coordinate systems. Motivated by this, this paper
investigates the magnitude of consistency between the local coordinate systems
necessary and sufficient to solve the gathering problem for two oblivious
robots under semi-synchronous and asynchronous models. To discuss the magnitude
of consistency, we assume that each robot is equipped with an unreliable
compass, the bearings of which may deviate from an absolute reference
direction, and that the local coordinate system of each robot is determined by
its compass. We consider two families of unreliable compasses, namely,static
compasses with constant bearings, and dynamic compasses the bearings of which
can change arbitrarily.
  For each of the combinations of robot and compass models, we establish the
condition on deviation \phi that allows an algorithm to solve the gathering
problem, where the deviation is measured by the largest angle formed between
the x-axis of a compass and the reference direction of the global coordinate
system: \phi &lt; \pi/2 for semi-synchronous and asynchronous robots with static
compasses, \phi &lt; \pi/4 for semi-synchronous robots with dynamic compasses, and
\phi &lt; \pi/6 for asynchronous robots with dynamic compasses. Except for
asynchronous robots with dynamic compasses, these sufficient conditions are
also necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1497</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1497</id><created>2011-11-07</created><updated>2012-09-17</updated><authors><author><keyname>Roy</keyname><forenames>Rishiraj Saha</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Choudhury</keyname><forenames>Monojit</forenames></author><author><keyname>Laxman</keyname><forenames>Srivatsan</forenames></author></authors><title>An IR-based Evaluation Framework for Web Search Query Segmentation</title><categories>cs.IR</categories><acm-class>H.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the first evaluation framework for Web search query
segmentation based directly on IR performance. In the past, segmentation
strategies were mainly validated against manual annotations. Our work shows
that the goodness of a segmentation algorithm as judged through evaluation
against a handful of human annotated segmentations hardly reflects its
effectiveness in an IR-based setup. In fact, state-of the-art algorithms are
shown to perform as good as, and sometimes even better than human annotations
-- a fact masked by previous validations. The proposed framework also provides
us an objective understanding of the gap between the present best and the best
possible segmentation algorithm. We draw these conclusions based on an
extensive evaluation of six segmentation strategies, including three most
recent algorithms, vis-a-vis segmentations from three human annotators. The
evaluation framework also gives insights about which segments should be
necessarily detected by an algorithm for achieving the best retrieval results.
The meticulously constructed dataset used in our experiments has been made
public for use by the research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1498</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1498</id><created>2011-11-07</created><authors><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>H_2-Optimal Decentralized Control over Posets: A State-Space Solution
  for State-Feedback</title><categories>math.OC cs.SY</categories><comments>39 pages, 2 figures, submitted to IEEE Transactions on Automatic
  Control</comments><journal-ref>IEEE Transactions on Automatic Control, Vol. 58, No. 12, pp.
  3084-3096, 2013</journal-ref><doi>10.1109/TAC.2013.2281881</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a complete state-space solution to H_2-optimal decentralized
control of poset-causal systems with state-feedback. Our solution is based on
the exploitation of a key separability property of the problem, that enables an
efficient computation of the optimal controller by solving a small number of
uncoupled standard Riccati equations. Our approach gives important insight into
the structure of optimal controllers, such as controller degree bounds that
depend on the structure of the poset. A novel element in our state-space
characterization of the controller is a remarkable pair of transfer functions,
that belong to the incidence algebra of the poset, are inverses of each other,
and are intimately related to prediction of the state along the different paths
on the poset. The results are illustrated by a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1504</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1504</id><created>2011-11-07</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM</affiliation></author><author><keyname>Todorcevic</keyname><forenames>Stevo</forenames><affiliation>ELM</affiliation></author></authors><title>A Hierarchy of Tree-Automatic Structures</title><categories>math.LO cs.LO</categories><comments>To appear in The Journal of Symbolic Logic. arXiv admin note:
  substantial text overlap with arXiv:1007.0822</comments><proxy>ccsd</proxy><journal-ref>Journal of Symbolic Logic 77, 1 (2012) 350-368</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider $\omega^n$-automatic structures which are relational structures
whose domain and relations are accepted by automata reading ordinal words of
length $\omega^n$ for some integer $n\geq 1$. We show that all these structures
are $\omega$-tree-automatic structures presentable by Muller or Rabin tree
automata. We prove that the isomorphism relation for $\omega^2$-automatic
(resp. $\omega^n$-automatic for $n&gt;2$) boolean algebras (respectively, partial
orders, rings, commutative rings, non commutative rings, non commutative
groups) is not determined by the axiomatic system ZFC. We infer from the proof
of the above result that the isomorphism problem for $\omega^n$-automatic
boolean algebras, $n &gt; 1$, (respectively, rings, commutative rings, non
commutative rings, non commutative groups) is neither a $\Sigma_2^1$-set nor a
$\Pi_2^1$-set. We obtain that there exist infinitely many $\omega^n$-automatic,
hence also $\omega$-tree-automatic, atomless boolean algebras $B_n$, $n\geq 1$,
which are pairwise isomorphic under the continuum hypothesis CH and pairwise
non isomorphic under an alternate axiom AT, strengthening a result of [FT10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1514</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1514</id><created>2011-11-07</created><authors><author><keyname>Tang</keyname><forenames>Linpeng</forenames></author><author><keyname>Liu</keyname><forenames>Qin</forenames></author></authors><title>A Survey on Distance Vector Routing Protocols</title><categories>cs.NI</categories><comments>15 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we give a brief introduction to five different distance vector
routing protocols (RIP, AODV, EIGRP, RIP-MTI and Babel) and give some of our
thoughts on how to solve the count to infinity problem. Our focus is how
distance vector routing protocols, based on limited information, can prevent
routing loops and the count to infinity problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1546</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1546</id><created>2011-11-07</created><updated>2015-01-15</updated><authors><author><keyname>Brunsch</keyname><forenames>Tobias</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author></authors><title>Improved Smoothed Analysis of Multiobjective Optimization</title><categories>cs.DS</categories><comments>to appear in JACM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several new results about smoothed analysis of multiobjective
optimization problems. Motivated by the discrepancy between worst-case analysis
and practical experience, this line of research has gained a lot of attention
in the last decade. We consider problems in which d linear and one arbitrary
objective function are to be optimized over a subset S of {0,1}^n of feasible
solutions. We improve the previously best known bound for the smoothed number
of Pareto-optimal solutions to O(n^{2d} phi^d), where phi denotes the
perturbation parameter. Additionally, we show that for any constant c the c-th
moment of the smoothed number of Pareto-optimal solutions is bounded by
O((n^{2d} phi^d)^c). This improves the previously best known bounds
significantly. Furthermore, we address the criticism that the perturbations in
smoothed analysis destroy the zero-structure of problems by showing that the
smoothed number of Pareto-optimal solutions remains polynomially bounded even
for zero-preserving perturbations. This broadens the class of problems captured
by smoothed analysis and it has consequences for non-linear objective
functions. One corollary of our result is that the smoothed number of
Pareto-optimal solutions is polynomially bounded for polynomial objective
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1555</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1555</id><created>2011-11-07</created><updated>2012-01-04</updated><authors><author><keyname>Santos</keyname><forenames>Gilson O. dos</forenames></author><author><keyname>de Assis</keyname><forenames>Francisco M.</forenames></author></authors><title>A scheme to protect against multiple quantum erasures</title><categories>cs.IT math.IT quant-ph</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scheme able to protect k &gt;= 3 qubits of information against the
occurrence of multiple erasures, based on the code proposed by Yang et al.
(2004 JETP Letters 79 236). In this scheme redundant blocks are used and we
restrict to the case that each erasure must occur in distinct blocks. We
explicitly characterize the encoding operation and the restoring operation
required to implement this scheme. The operators used in these operations can
be adjusted to construct different quantum erasure-correcting codes. A special
feature of this scheme is that no measurement is required. To illustrate our
scheme, we present an example in which five-qubits of information are protected
against the occurrence of two erasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1562</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1562</id><created>2011-11-07</created><authors><author><keyname>Shams</keyname><forenames>M. Y.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Nomir</keyname><forenames>O.</forenames></author><author><keyname>El-Awady</keyname><forenames>R. M.</forenames></author></authors><title>Iris Recognition Based on LBP and Combined LVQ Classifier</title><categories>cs.CV</categories><comments>12 Pages, 12 Figures</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 5, Oct 2011</journal-ref><doi>10.5121/ijcsit.2011.3506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iris recognition is considered as one of the best biometric methods used for
human identification and verification, this is because of its unique features
that differ from one person to another, and its importance in the security
field. This paper proposes an algorithm for iris recognition and classification
using a system based on Local Binary Pattern and histogram properties as a
statistical approaches for feature extraction, and Combined Learning Vector
Quantization Classifier as Neural Network approach for classification, in order
to build a hybrid model depends on both features. The localization and
segmentation techniques are presented using both Canny edge detection and Hough
Circular Transform in order to isolate an iris from the whole eye image and for
noise detection .Feature vectors results from LBP is applied to a Combined LVQ
classifier with different classes to determine the minimum acceptable
performance, and the result is based on majority voting among several LVQ
classifier. Different iris datasets CASIA, MMU1, MMU2, and LEI with different
extensions and size are presented. Since LBP is working on a grayscale level so
colored iris images should be transformed into a grayscale level. The proposed
system gives a high recognition rate 99.87 % on different iris datasets
compared with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1564</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1564</id><created>2011-11-07</created><authors><author><keyname>Singh</keyname><forenames>Balwnder</forenames></author><author><keyname>Narang</keyname><forenames>Sukhleen Bindra</forenames></author><author><keyname>Khosla</keyname><forenames>Arun</forenames></author></authors><title>Particle Swarm Optimization Framework for Low Power Testing of VLSI
  Circuits</title><categories>cs.NE</categories><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.2, No.3, July 2011</journal-ref><doi>10.5121/ijaia.2011.2302</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Power dissipation in sequential circuits is due to increased toggling count
of Circuit under Test, which depends upon test vectors applied. If successive
test vectors sequences have more toggling nature then it is sure that toggling
rate of flip flops is higher. Higher toggling for flip flops results more power
dissipation. To overcome this problem, one method is to use GA to have test
vectors of high fault coverage in short interval, followed by Hamming distance
management on test patterns. This approach is time consuming and needs more
efforts. Another method which is purposed in this paper is a PSO based Frame
Work to optimize power dissipation. Here target is to set the entire test
vector in a frame for time period 'T', so that the frame consists of all those
vectors strings which not only provide high fault coverage but also arrange
vectors in frame to produce minimum toggling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1570</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1570</id><created>2011-11-07</created><authors><author><keyname>Durao</keyname><forenames>Frederico</forenames></author><author><keyname>Dolog</keyname><forenames>Peter</forenames></author></authors><title>Semantic Grounding Strategies for Tagbased Recommender Systems</title><categories>cs.IR cs.SI</categories><comments>13 pages, 5 figures</comments><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.2,
  No.4, 2011, 67-79</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recommender systems usually operate on similarities between recommended items
or users. Tag based recommender systems utilize similarities on tags. The tags
are however mostly free user entered phrases. Therefore, similarities computed
without their semantic groundings might lead to less relevant recommendations.
In this paper, we study a semantic grounding used for tag similarity calculus.
We show a comprehensive analysis of semantic grounding given by 20 ontologies
from different domains. The study besides other things reveals that currently
available OWL ontologies are very narrow and the percentage of the similarity
expansions is rather small. WordNet scores slightly better as it is broader but
not much as it does not support several semantic relationships. Furthermore,
the study reveals that even with such number of expansions, the recommendations
change considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1585</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1585</id><created>2011-11-07</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Steinberg</keyname><forenames>Benjamin</forenames></author></authors><title>The Krohn-Rhodes Theorem and Local Divisors</title><categories>math.GR cs.FL</categories><msc-class>20M10, 20M20, 20M35, 68Q45, 68Q70</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of the Krohn-Rhodes Theorem using local divisors. The
proof provides nearly as good a decomposition in terms of size as the holonomy
decomposition of Eilenberg, avoids induction on the size of the state set, and
works exclusively with monoids with the base case of the induction being that
of a group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1586</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1586</id><created>2011-11-07</created><authors><author><keyname>M</keyname><forenames>Thirumaran.</forenames></author><author><keyname>P</keyname><forenames>Dhavachelvan.</forenames></author><author><keyname>G</keyname><forenames>Aranganayagi.</forenames></author><author><keyname>Abarna</keyname><forenames>S.</forenames></author></authors><title>Evaluation of Computability Criterions for Runtime Web Service
  Integration</title><categories>cs.SE</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's competitive environment drives the enterprises to extend their focus
and collaborate with their business partners to carry out the necessities.
Tight coordination among business partners assists to share and integrate the
service logic globally. But integrating service logics across diverse
enterprises leads to exponential problem which stipulates developers to
comprehend the whole service and must resolve suitable method to integrate the
services. It is complex and time-consuming task. So the present focus is to
have a mechanized system to analyze the Business logics and convey the proper
mode to integrate them. There is no standard model to undertake these issues
and one such a framework proposed in this paper examines the Business logics
individually and suggests proper structure to integrate them. One of the
innovative concepts of proposed model is Property Evaluation System which
scrutinizes the service logics and generates Business Logic Property Schema
(BLPS) for the required services. BLPS holds necessary information to recognize
the correct structure for integrating the service logics. At the time of
integration, System consumes this BLPS schema and suggests the feasible ways to
integrate the service logics. Also if the service logics are attempted to
integrate in invalid structure or attempted to violate accessibility levels,
system will throw exception with necessary information. This helps developers
to ascertain the efficient structure to integrate the services with least
effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1596</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1596</id><created>2011-11-07</created><updated>2013-02-22</updated><authors><author><keyname>Melnik</keyname><forenames>Sergey</forenames></author><author><keyname>Ward</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Gleeson</keyname><forenames>James P.</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Multi-Stage Complex Contagions</title><categories>cs.SI math.DS nlin.AO physics.soc-ph</categories><comments>12 pages, 10 figures. This version is accepted to appear in Chaos</comments><journal-ref>Chaos 23, 013124 (2013)</journal-ref><doi>10.1063/1.4790836</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of ideas across a social network can be studied using complex
contagion models, in which agents are activated by contact with multiple
activated neighbors. The investigation of complex contagions can provide
crucial insights into social influence and behavior-adoption cascades on
networks. In this paper, we introduce a model of a multi-stage complex
contagion on networks. Agents at different stages --- which could, for example,
represent differing levels of support for a social movement or differing levels
of commitment to a certain product or idea --- exert different amounts of
influence on their neighbors. We demonstrate that the presence of even one
additional stage introduces novel dynamical behavior, including interplay
between multiple cascades, that cannot occur in single-stage contagion models.
We find that cascades --- and hence collective action --- can be driven not
only by high-stage influencers but also by low-stage influencers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1598</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1598</id><created>2011-11-07</created><authors><author><keyname>Aghav</keyname><forenames>Jagannath</forenames></author><author><keyname>Tumma</keyname><forenames>Ashwin</forenames></author></authors><title>Design and Validation of Safety Cruise Control System for Automobiles</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In light of the recent humongous growth of the human population worldwide,
there has also been a voluminous and uncontrolled growth of vehicles, which has
consequently increased the number of road accidents to a large extent. In lieu
of a solution to the above mentioned issue, our system is an attempt to
mitigate the same using synchronous programming language. The aim is to develop
a safety crash warning system that will address the rear end crashes and also
take over the controlling of the vehicle when the threat is at a very high
level. Adapting according to the environmental conditions is also a prominent
feature of the system. Safety System provides warnings to drivers to assist in
avoiding rear-end crashes with other vehicles. Initially the system provides a
low level alarm and as the severity of the threat increases the level of
warnings or alerts also rises. At the highest level of threat, the system
enters in a Cruise Control Mode, wherein the system controls the speed of the
vehicle by controlling the engine throttle and if permitted, the brake system
of the vehicle. We focus on this crash area as it has a very high percentage of
the crash-related fatalities. To prove the feasibility, robustness and
reliability of the system, we have also proved some of the properties of the
system using temporal logic along with a reference implementation in ESTEREL.
To bolster the same, we have formally verified various properties of the system
along with their proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1599</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1599</id><created>2011-11-07</created><authors><author><keyname>Lea</keyname><forenames>Colin S.</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Efficient Hierarchical Markov Random Fields for Object Detection on a
  Mobile Robot</title><categories>cs.CV</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Object detection and classification using video is necessary for intelligent
planning and navigation on a mobile robot. However, current methods can be too
slow or not sufficient for distinguishing multiple classes. Techniques that
rely on binary (foreground/background) labels incorrectly identify areas with
multiple overlapping objects as single segment. We propose two Hierarchical
Markov Random Field models in efforts to distinguish connected objects using
tiered, binary label sets. Near-realtime performance has been achieved using
efficient optimization methods which runs up to 11 frames per second on a dual
core 2.2 Ghz processor. Evaluation of both models is done using footage taken
from a robot obstacle course at the 2010 Intelligent Ground Vehicle
Competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1605</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1605</id><created>2011-11-03</created><authors><author><keyname>Kumar</keyname><forenames>Bimal Aklesh</forenames></author></authors><title>Solar Power Systems Web Monitoring</title><categories>cs.OH</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All over the world the peak demand load is increasing and the load factor is
decreasing year-by-year. The fossil fuel is considered insufficient thus solar
energy systems are becoming more and more useful, not only in terms of
installation but monitoring of these systems is very crucial. Monitoring
becomes very important when there are a large number of solar panels.
Monitoring would allow early detection if the output falls below required level
or one of the solar panel out of 1000 goes down. In this study the target is to
monitor and control a developed solar panel by using available internet
foundation. This web-enabled software will provide more flexibility over the
system such as transmitting data from panel to the host computer and
disseminating information to relevant stake holders barring any geographical
barrier. The software would be built around web server with dynamic HTML and
JAVA, this paper presents the preliminary design of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="25000" completeListSize="102538">1122234|26001</resumptionToken>
</ListRecords>
</OAI-PMH>
