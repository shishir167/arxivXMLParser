<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:04:40Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|55001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2474</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2474</id><created>2014-01-10</created><authors><author><keyname>Hurley</keyname><forenames>Barry</forenames></author><author><keyname>Kadioglu</keyname><forenames>Serdar</forenames></author><author><keyname>Malitsky</keyname><forenames>Yuri</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author></authors><title>Transformation-based Feature Computation for Algorithm Portfolios</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instance-specific algorithm configuration and algorithm portfolios have been
shown to offer significant improvements over single algorithm approaches in a
variety of application domains. In the SAT and CSP domains algorithm portfolios
have consistently dominated the main competitions in these fields for the past
five years. For a portfolio approach to be effective there are two crucial
conditions that must be met. First, there needs to be a collection of
complementary solvers with which to make a portfolio. Second, there must be a
collection of problem features that can accurately identify structural
differences between instances. This paper focuses on the latter issue: feature
representation, because, unlike SAT, not every problem has well-studied
features. We employ the well-known SATzilla feature set, but compute
alternative sets on different SAT encodings of CSPs. We show that regardless of
what encoding is used to convert the instances, adequate structural information
is maintained to differentiate between problem instances, and that this can be
exploited to make an effective portfolio-based CSP solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2482</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2482</id><created>2014-01-10</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Bogunovi&#x107;</keyname><forenames>Nikola</forenames></author><author><keyname>&#x106;osi&#x107;</keyname><forenames>Kre&#x161;imir</forenames></author></authors><title>STIMONT: A core ontology for multimedia stimuli description</title><categories>cs.MM cs.AI</categories><comments>27 pages, 13 figures</comments><journal-ref>Multimedia tools and applications, 11042, July 2013</journal-ref><doi>10.1007/s11042-013-1624-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affective multimedia documents such as images, sounds or videos elicit
emotional responses in exposed human subjects. These stimuli are stored in
affective multimedia databases and successfully used for a wide variety of
research in psychology and neuroscience in areas related to attention and
emotion processing. Although important all affective multimedia databases have
numerous deficiencies which impair their applicability. These problems, which
are brought forward in the paper, result in low recall and precision of
multimedia stimuli retrieval which makes creating emotion elicitation
procedures difficult and labor-intensive. To address these issues a new core
ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and
extends W3C EmotionML format with an expressive and formal representation of
affective concepts, high-level semantics, stimuli document metadata and the
elicited physiology. The advantages of ontology in description of affective
multimedia stimuli are demonstrated in a document retrieval experiment and
compared against contemporary keyword-based querying methods. Also, a software
tool Intelligent Stimulus Generator for retrieval of affective multimedia and
construction of stimuli sequences is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2483</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2483</id><created>2014-01-10</created><authors><author><keyname>Maseleno</keyname><forenames>Andino</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahmud</forenames></author></authors><title>Dempster-Shafer Theory for Move Prediction in Start Kicking of The
  Bicycle Kick of Sepak Takraw Game</title><categories>cs.AI</categories><comments>Middle-East Journal of Scientific Research, Vol. 16, No. 7, 2013.
  ISSN 1990-9233, pp. 896 - 903</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents Dempster-Shafer theory for move prediction in start
kicking of the bicycle kick of sepak takraw game. Sepak takraw is a highly
complex net-barrier kicking sport that involves dazzling displays of quick
reflexes, acrobatic twists, turns and swerves of the agile human body movement.
A Bicycle kick or Scissor kick is a physical move made by throwing the body up
into the air, making a shearing movement with the legs to get one leg in front
of the other without holding on to the ground. Specifically, this paper
considers bicycle kick of sepak takraw game in start kicking of the ball with
uncertainty where player has different awareness regarding the contingencies.
We have chosen Dempster-Shafer theory because the advantages of the
Dempster-Shafer theory which include the ability to model information in a
flexible way without requiring a probability to be assigned to each element in
a set, providing a convenient and simple mechanism for combining two or more
pieces of evidence under certain conditions, it can model ignorance explicitly,
rejection of the law of additivity for belief in disjoint propositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2489</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2489</id><created>2014-01-10</created><authors><author><keyname>Alkadeki</keyname><forenames>Hatm</forenames></author><author><keyname>Wang</keyname><forenames>Xingang</forenames></author><author><keyname>Odetayo</keyname><forenames>Michael</forenames></author></authors><title>Estimation of Medium Access Control Layer Packet Delay Distribution for
  IEEE 802.11</title><categories>cs.NI</categories><comments>10pages, 4figures, journal publication</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 6, December 2013</journal-ref><doi>10.5121/ijwmn.2013.5608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important standard in wireless local area networks is IEEE 802.11.
This is why much of the research work for the enhancement of wireless network
is usually based on the behavior of IEEE 802.11 protocol. However, some of the
ways in which IEEE 802.11 medium access control layer behaves is still
unreliable to guarantee quality of service. For instance, medium access control
layer packet delay, jitter and packet loss rate still remain a challenge. The
main objective of this research is to propose an accurate estimation of the
medium access control layer packet delay distribution for IEEE 802.11. This
estimation considers the differences between busy probability and collision
probability. These differences are employed to achieve a more accurate
estimation. Finally, the proposed model and simulation are implemented and
validated - using MATLAB program for the purpose of simulation, and Maple
program to undertake the calculation of the equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2490</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2490</id><created>2014-01-10</created><authors><author><keyname>Yildirim</keyname><forenames>Sinan</forenames></author><author><keyname>Cemgil</keyname><forenames>A. Taylan</forenames></author><author><keyname>Singh</keyname><forenames>Sumeetpal S.</forenames></author></authors><title>An Online Expectation-Maximisation Algorithm for Nonnegative Matrix
  Factorisation Models</title><categories>cs.LG stat.CO stat.ML</categories><comments>6 pages, 3 figures</comments><journal-ref>16th IFAC Symposium on System Identification, 2012, Volume 16,
  Part 1,</journal-ref><doi>10.3182/20120711-3-BE-2027.00312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we formulate the nonnegative matrix factorisation (NMF) problem
as a maximum likelihood estimation problem for hidden Markov models and propose
online expectation-maximisation (EM) algorithms to estimate the NMF and the
other unknown static parameters. We also propose a sequential Monte Carlo
approximation of our online EM algorithm. We show the performance of the
proposed method with two numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2493</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2493</id><created>2014-01-10</created><authors><author><keyname>Mendes</keyname><forenames>Anthony</forenames></author><author><keyname>Morrison</keyname><forenames>Kent E.</forenames></author></authors><title>Guessing games</title><categories>cs.GT math.PR</categories><comments>12 pages, 3 figures</comments><msc-class>91A06 (Primary) 91A60 (Secondary)</msc-class><journal-ref>Amer. Math. Monthly 121 (2014) 33-44</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a guessing game, players guess the value of a random real number selected
using some probability density function. The winner may be determined in
various ways; for example, a winner can be a player whose guess is closest in
magnitude to the target or a winner can be a player coming closest without
guessing higher than the target. We study optimal strategies for players in
these games and determine some of them for two, three, and four players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2496</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2496</id><created>2014-01-10</created><authors><author><keyname>Tajima</keyname><forenames>Masato</forenames></author><author><keyname>Okino</keyname><forenames>Koji</forenames></author><author><keyname>Murayama</keyname><forenames>Tatsuto</forenames></author></authors><title>Reduction of Error-Trellises for Tail-Biting Convolutional Codes Using
  Shifted Error-Subsequences</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to the 2014 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the reduction of error-trellises for tail-biting
convolutional codes. In the case where some column of a parity-check matrix has
a monomial factor (with indeterminate D), we show that the associated
tail-biting error-trellis can be reduced by cyclically shifting the
corresponding error-subsequence by l (the power of D) time units. We see that
the resulting reduced error-trellis is again tail-biting. Moreover, we show
that reduction is also possible using backward-shifted error-subsequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2503</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2503</id><created>2014-01-11</created><authors><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>Does Restraining End Effect Matter in EMD-Based Modeling Framework for
  Time Series Prediction? Some Experimental Evidences</title><categories>cs.AI stat.AP</categories><comments>28 pages</comments><journal-ref>Neurocomputing. 123, 2013: 174-184</journal-ref><doi>10.1016/j.neucom.2013.07.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the &quot;decomposition-and-ensemble&quot; principle, the empirical mode
decomposition (EMD)-based modeling framework has been widely used as a
promising alternative for nonlinear and nonstationary time series modeling and
prediction. The end effect, which occurs during the sifting process of EMD and
is apt to distort the decomposed sub-series and hurt the modeling process
followed, however, has been ignored in previous studies. Addressing the end
effect issue, this study proposes to incorporate end condition methods into
EMD-based decomposition and ensemble modeling framework for one- and multi-step
ahead time series prediction. Four well-established end condition methods,
Mirror method, Coughlin's method, Slope-based method, and Rato's method, are
selected, and support vector regression (SVR) is employed as the modeling
technique. For the purpose of justification and comparison, well-known NN3
competition data sets are used and four well-established prediction models are
selected as benchmarks. The experimental results demonstrated that significant
improvement can be achieved by the proposed EMD-based SVR models with end
condition methods. The EMD-SBM-SVR model and EMD-Rato-SVR model, in particular,
achieved the best prediction performances in terms of goodness of forecast
measures and equality of accuracy of competing forecasts test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2504</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2504</id><created>2014-01-11</created><authors><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>Multi-Step-Ahead Time Series Prediction using Multiple-Output Support
  Vector Regression</title><categories>cs.LG stat.ML</categories><comments>26 pages</comments><doi>10.1016/j.neucom.2013.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate time series prediction over long future horizons is challenging and
of great interest to both practitioners and academics. As a well-known
intelligent algorithm, the standard formulation of Support Vector Regression
(SVR) could be taken for multi-step-ahead time series prediction, only relying
either on iterated strategy or direct strategy. This study proposes a novel
multiple-step-ahead time series prediction approach which employs
multiple-output support vector regression (M-SVR) with multiple-input
multiple-output (MIMO) prediction strategy. In addition, the rank of three
leading prediction strategies with SVR is comparatively examined, providing
practical implications on the selection of the prediction strategy for
multi-step-ahead forecasting while taking SVR as modeling technique. The
proposed approach is validated with the simulated and real datasets. The
quantitative and comprehensive assessments are performed on the basis of the
prediction accuracy and computational cost. The results indicate that: 1) the
M-SVR using MIMO strategy achieves the best accurate forecasts with accredited
computational load, 2) the standard SVR using direct strategy achieves the
second best accurate forecasts, but with the most expensive computational cost,
and 3) the standard SVR using iterated strategy is the worst in terms of
prediction accuracy, but with the least computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2507</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2507</id><created>2014-01-11</created><authors><author><keyname>Dougherty</keyname><forenames>Randall</forenames></author><author><keyname>Freiling</keyname><forenames>Eric</forenames></author><author><keyname>Zeger</keyname><forenames>Kenneth</forenames></author></authors><title>Characteristic-Dependent Linear Rank Inequalities with Applications to
  Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two characteristic-dependent linear rank inequalities are given for eight
variables. Specifically, the first inequality holds for all finite fields whose
characteristic is not three and does not in general hold over characteristic
three. The second inequality holds for all finite fields whose characteristic
is three and does not in general hold over characteristics other than three.
Applications of these inequalities to the computation of capacity upper bounds
in network coding are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2514</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2514</id><created>2014-01-11</created><authors><author><keyname>Bhattacharya</keyname><forenames>Abhijit</forenames></author><author><keyname>Rao</keyname><forenames>Akhila</forenames></author><author><keyname>Naveen</keyname><forenames>K. P.</forenames></author><author><keyname>Nishanth</keyname><forenames>P. P.</forenames></author><author><keyname>Anand</keyname><forenames>S. V. R.</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>QoS Constrained Optimal Sink and Relay Placement in Planned Wireless
  Sensor Networks</title><categories>cs.NI cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are given a set of sensors at given locations, a set of potential
locations for placing base stations (BSs, or sinks), and another set of
potential locations for placing wireless relay nodes. There is a cost for
placing a BS and a cost for placing a relay. The problem we consider is to
select a set of BS locations, a set of relay locations, and an association of
sensor nodes with the selected BS locations, so that number of hops in the path
from each sensor to its BS is bounded by hmax, and among all such feasible
networks, the cost of the selected network is the minimum. The hop count bound
suffices to ensure a certain probability of the data being delivered to the BS
within a given maximum delay under a light traffic model. We observe that the
problem is NP-Hard, and is hard to even approximate within a constant factor.
For this problem, we propose a polynomial time approximation algorithm
(SmartSelect) based on a relay placement algorithm proposed in our earlier
work, along with a modification of the greedy algorithm for weighted set cover.
We have analyzed the worst case approximation guarantee for this algorithm. We
have also proposed a polynomial time heuristic to improve upon the solution
provided by SmartSelect. Our numerical results demonstrate that the algorithms
provide good quality solutions using very little computation time in various
randomly generated network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2515</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2515</id><created>2014-01-11</created><authors><author><keyname>Berardi</keyname><forenames>Stefano</forenames><affiliation>Universita di Torino</affiliation></author></authors><title>An intuitionistic version of Ramsey Theorem (italian version)</title><categories>cs.LO</categories><comments>in Italian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ramsey Theorem [6] for pairs is intuitionistically but not classically
provable: it is equivalent to a subclassical principle [2]. In this note we
show that Ramsey may be restated in an intuitionistically provable form, which
is informative (or at least without negations), and classically equivalent to
the original. With respect to previous works of the same kind, we do not use no
counterexample as in [1], [5], nor we add a new principle to the intuitionism
as in [4]. We claim that this intuitionistic version of Ramsey could be use to
replace Ramsey Theorem in the convergence proof of programs included in [3].
  [1] Gianluigi Bellin. Ramsey interpreted: a parametric version of Ramsey
Theorem. In AMS, editor, Logic and Computation: Proceedings of a Symposium held
at Carnegie Mellon University, volume 106.
  [2] Stefano Berardi, Silvia Steila, Ramsey Theorem for pairs as a classical
principle in Intuitionistic Arithmetic, Submitted to the proceedings of Types
2013 in Toulouse.
  [3] Byron Cook, Abigail See, Florian Zuleger, Ramsey vs. Lexicographic
Termination Proving, LNCS 7795, 2013, Springer Berlin Heidelberg.
  [4] Thierry Coquand. A direct proof of Ramsey Theorem.
  [5] Paulo Oliva and Thomas Powell. A Constructive Interpretation of Ramsey
Theorem via the Product of Selection Functions. CoRR, arXiv:1204.5631, 2012.
  [6] F. P. Ramsey. On a problem in formal logic. Proc. London Math. Soc.,
1930.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2516</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2516</id><created>2014-01-11</created><authors><author><keyname>Nagavi</keyname><forenames>Trisiladevi C.</forenames></author><author><keyname>Bhajantri</keyname><forenames>Nagappa U.</forenames></author></authors><title>Progressive Filtering Using Multiresolution Histograms for Query by
  Humming System</title><categories>cs.IR</categories><comments>12 Pages, 6 Figures, Full version of the paper published at
  ICMCCA-2012 with the same title,
  Link:http://link.springer.com/chapter/10.1007/978-81-322-1143-3_21</comments><journal-ref>Proc. of the First International Conference on Multimedia
  Processing,Communication and Computing Applications(ICMCCA) 13-15 December
  2012,Bangalore India,Lecture Notes in Electrical Engineering,Volume 213,Pages
  253-265,Springer India,2013</journal-ref><doi>10.1007/978-81-322-1143-3_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rising availability of digital music stipulates effective categorization
and retrieval methods. Real world scenarios are characterized by mammoth music
collections through pertinent and non-pertinent songs with reference to the
user input. The primary goal of the research work is to counter balance the
perilous impact of non-relevant songs through Progressive Filtering (PF) for
Query by Humming (QBH) system. PF is a technique of problem solving through
reduced space. This paper presents the concept of PF and its efficient design
based on Multi-Resolution Histograms (MRH) to accomplish searching in
manifolds. Initially the entire music database is searched to obtain high
recall rate and narrowed search space. Later steps accomplish slow search in
the reduced periphery and achieve additional accuracy.
  Experimentation on large music database using recursive programming
substantiates the potential of the method. The outcome of proposed strategy
glimpses that MRH effectively locate the patterns. Distances of MRH at lower
level are the lower bounds of the distances at higher level, which guarantees
evasion of false dismissals during PF. In due course, proposed method helps to
strike a balance between efficiency and effectiveness. The system is scalable
for large music retrieval systems and also data driven for performance
optimization as an added advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2517</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2517</id><created>2014-01-11</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author><author><keyname>Bertolotto</keyname><forenames>Michela</forenames></author><author><keyname>Wilson</keyname><forenames>David C.</forenames></author></authors><title>The semantic similarity ensemble</title><categories>cs.CL</categories><comments>Special feature on Semantic and Conceptual Issues in GIS (SeCoGIS)</comments><journal-ref>Journal of Spatial Information Science (JOSIS), Number 7 (2013),
  pp. 27-44</journal-ref><doi>10.5311/JOSIS.2013.7.128</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Computational measures of semantic similarity between geographic terms
provide valuable support across geographic information retrieval, data mining,
and information integration. To date, a wide variety of approaches to
geo-semantic similarity have been devised. A judgment of similarity is not
intrinsically right or wrong, but obtains a certain degree of cognitive
plausibility, depending on how closely it mimics human behavior. Thus selecting
the most appropriate measure for a specific task is a significant challenge. To
address this issue, we make an analogy between computational similarity
measures and soliciting domain expert opinions, which incorporate a subjective
set of beliefs, perceptions, hypotheses, and epistemic biases. Following this
analogy, we define the semantic similarity ensemble (SSE) as a composition of
different similarity measures, acting as a panel of experts having to reach a
decision on the semantic similarity of a set of geographic terms. The approach
is evaluated in comparison to human judgments, and results indicate that an SSE
performs better than the average of its parts. Although the best member tends
to outperform the ensemble, all ensembles outperform the average performance of
each ensemble's member. Hence, in contexts where the best measure is unknown,
the ensemble provides a more cognitively plausible approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2518</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2518</id><created>2014-01-11</created><updated>2015-07-14</updated><authors><author><keyname>Vyavahare</keyname><forenames>Pooja</forenames></author><author><keyname>Limaye</keyname><forenames>Nutan</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>Optimal Embedding of Functions for In-Network Computation: Complexity
  Analysis and Algorithms</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider optimal distributed computation of a given function of
distributed data. The input (data) nodes and the sink node that receives the
function form a connected network that is described by an undirected weighted
network graph. The algorithm to compute the given function is described by a
weighted directed acyclic graph and is called the computation graph. An
embedding defines the computation communication sequence that obtains the
function at the sink. Two kinds of optimal embeddings are sought, the embedding
that---(1)~minimizes delay in obtaining function at sink, and (2)~minimizes
cost of one instance of computation of function. This abstraction is motivated
by three applications---in-network computation over sensor networks, operator
placement in distributed databases, and module placement in distributed
computing.
  We first show that obtaining minimum-delay and minimum-cost embeddings are
both NP-complete problems and that cost minimization is actually MAX SNP-hard.
Next, we consider specific forms of the computation graph for which polynomial
time solutions are possible. When the computation graph is a tree, a polynomial
time algorithm to obtain the minimum delay embedding is described. Next, for
the case when the function is described by a layered graph we describe an
algorithm that obtains the minimum cost embedding in polynomial time. This
algorithm can also be used to obtain an approximation for delay minimization.
We then consider bounded treewidth computation graphs and give an algorithm to
obtain the minimum cost embedding in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2529</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2529</id><created>2014-01-11</created><updated>2014-09-30</updated><authors><author><keyname>Vural</keyname><forenames>Elif</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>A Study of Image Analysis with Tangent Distance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of the geometric transformation between a reference and a
target image, known as registration or alignment, corresponds to the projection
of the target image onto the transformation manifold of the reference image
(the set of images generated by its geometric transformations). It, however,
often takes a nontrivial form such that the exact computation of projections on
the manifold is difficult. The tangent distance method is an effective
algorithm to solve this problem by exploiting a linear approximation of the
manifold. As theoretical studies about the tangent distance algorithm have been
largely overlooked, we present in this work a detailed performance analysis of
this useful algorithm, which can eventually help its implementation. We
consider a popular image registration setting using a multiscale pyramid of
lowpass filtered versions of the (possibly noisy) reference and target images,
which is particularly useful for recovering large transformations. We first
show that the alignment error has a nonmonotonic variation with the filter
size, due to the opposing effects of filtering on both manifold nonlinearity
and image noise. We then study the convergence of the multiscale tangent
distance method to the optimal solution. We finally examine the performance of
the tangent distance method in image classification applications. Our
theoretical findings are confirmed by experiments on image transformation
models involving translations, rotations and scalings. Our study is the first
detailed study of the tangent distance algorithm that leads to a better
understanding of its efficacy and to the proper selection of its design
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2530</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2530</id><created>2014-01-11</created><authors><author><keyname>Yan</keyname><forenames>Tongjiang</forenames></author><author><keyname>Chen</keyname><forenames>Zhixiong</forenames></author><author><keyname>Li</keyname><forenames>Bao</forenames></author></authors><title>A General Construction of Binary Sequences with Optimal Autocorrelation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general construction of binary sequences with low autocorrelation are
considered in the paper. Based on recent progresses about this topic and this
construction, several classes of binary sequences with optimal autocorrelation
and other low autocorrelation are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2532</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2532</id><created>2014-01-11</created><authors><author><keyname>Guo</keyname><forenames>Jiong</forenames></author><author><keyname>Shrestha</keyname><forenames>Yash Raj</forenames></author></authors><title>Parameterized Complexity of Edge Interdiction Problems</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of interdiction problems in graphs. For
an optimization problem on graphs, one can formulate an interdiction problem as
a game consisting of two players, namely, an interdictor and an evader, who
compete on an objective with opposing interests. In edge interdiction problems,
every edge of the input graph has an interdiction cost associated with it and
the interdictor interdicts the graph by modifying the edges in the graph, and
the number of such modifications is constrained by the interdictor's budget.
The evader then solves the given optimization problem on the modified graph.
The action of the interdictor must impede the evader as much as possible. We
focus on edge interdiction problems related to minimum spanning tree, maximum
matching and shortest paths. These problems arise in different real world
scenarios. We derive several fixed-parameter tractability and W[1]-hardness
results for these interdiction problems with respect to various parameters.
Next, we show close relation between interdiction problems and partial cover
problems on bipartite graphs where the goal is not to cover all elements but to
minimize/maximize the number of covered elements with specific number of sets.
Hereby, we investigate the parameterized complexity of several partial cover
problems on bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2538</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2538</id><created>2014-01-11</created><authors><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Linear-Time Compression of Bounded-Genus Graphs into
  Information-Theoretically Optimal Number of Bits</title><categories>cs.DS</categories><comments>26 pages, 9 figures, accepted to SIAM Journal on Computing</comments><journal-ref>SIAM Journal on Computing, 43(2):477-496, 2014</journal-ref><doi>10.1137/120879142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\textit{compression scheme}$ $A$ for a class $\mathbb{G}$ of graphs
consists of an encoding algorithm $\textit{Encode}_A$ that computes a binary
string $\textit{Code}_A(G)$ for any given graph $G$ in $\mathbb{G}$ and a
decoding algorithm $\textit{Decode}_A$ that recovers $G$ from
$\textit{Code}_A(G)$. A compression scheme $A$ for $\mathbb{G}$ is
$\textit{optimal}$ if both $\textit{Encode}_A$ and $\textit{Decode}_A$ run in
linear time and the number of bits of $\textit{Code}_A(G)$ for any $n$-node
graph $G$ in $\mathbb{G}$ is information-theoretically optimal to within
lower-order terms. Trees and plane triangulations were the only known
nontrivial graph classes that admit optimal compression schemes. Based upon
Goodrich's separator decomposition for planar graphs and Djidjev and
Venkatesan's planarizers for bounded-genus graphs, we give an optimal
compression scheme for any hereditary (i.e., closed under taking subgraphs)
class $\mathbb{G}$ under the premise that any $n$-node graph of $\mathbb{G}$ to
be encoded comes with a genus-$o(\frac{n}{\log^2 n})$ embedding. By Mohar's
linear-time algorithm that embeds a bounded-genus graph on a genus-$O(1)$
surface, our result implies that any hereditary class of genus-$O(1)$ graphs
admits an optimal compression scheme. For instance, our result yields the
first-known optimal compression schemes for planar graphs, plane graphs, graphs
embedded on genus-$1$ surfaces, graphs with genus $2$ or less, $3$-colorable
directed plane graphs, $4$-outerplanar graphs, and forests with degree at most
$5$. For non-hereditary graph classes, we also give a methodology for obtaining
optimal compression schemes. From this methodology, we give the first known
optimal compression schemes for triangulations of genus-$O(1)$ surfaces and
floorplans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2540</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2540</id><created>2014-01-11</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Soni</keyname><forenames>Ankita</forenames></author><author><keyname>Batra</keyname><forenames>Nikhil</forenames></author></authors><title>Reliability Analysis to overcome Black Hole Attack in Wireless Sensor
  Network</title><categories>cs.NI</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are vulnerable to several attacks, one of them being
the black hole attack. A black hole is a malicious node that attracts all the
traffic in the network by advertising that it has the shortest path in the
network. Once it receives the packet from other nodes, it drops all the packets
causing loss of critical information. In this paper we propose a reliability
analysis mechanism. The proposed reliability analysis scheme overcomes the
shortcomings of existing cooperative black hole attack using AODV routing
protocol. As soon as there is a path available for routing, its reliability is
checked using the proposed scheme. The proposed reliability analysis scheme
helps in achieving maximum reliability by minimizing the complexity of the
system. The final path available after the reliability analysis using the
proposed scheme will make the path secure enough to minimize the packet loss,
end-to-end delay and the energy utilization of the network as well as maximize
the network lifetime in return.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2541</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2541</id><created>2014-01-11</created><authors><author><keyname>Virmani</keyname><forenames>Dr. Deepali</forenames></author><author><keyname>Hemrajani</keyname><forenames>Manas</forenames></author><author><keyname>Chandel</keyname><forenames>Shringarica</forenames></author></authors><title>Exponential Trust Based Mechanism to Detect Black Hole attack in
  Wireless Sensor Network</title><categories>cs.NI</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is a key feature in Wireless Sensor Networks but they are prone to
many kinds of attacks and one of them is Black Hole Attack. In a black hole
attack all the packets are consecutively dropped which leads to the decrease in
the efficiency of the network and unnecessary wastage of battery life. In this
paper, we propose an exponential trust based mechanism to detect the malicious
node. In the proposed method a Streak counter is deployed to store the
consecutive number of packets dropped and a trust factor is maintained for each
node. The trust factor drops exponentially with each consecutive packet dropped
which helps in detecting the malicious node. The proposed method show a drastic
decrease in the number of packets dropped before the node being detected as a
malicious node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2542</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2542</id><created>2014-01-11</created><authors><author><keyname>Hamodi</keyname><forenames>Jamil</forenames></author><author><keyname>Thool</keyname><forenames>Ravindra</forenames></author><author><keyname>Salah</keyname><forenames>Khaled</forenames></author><author><keyname>Alsagaf</keyname><forenames>Anwar</forenames></author><author><keyname>Holba</keyname><forenames>Yousef</forenames></author></authors><title>Performance Study of Mobile TV over Mobile WiMAX Considering Different
  Modulation and Coding Techniques</title><categories>cs.NI</categories><comments>12 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1312.7442; and text overlap with arXiv:1005.0976 by other authors</comments><journal-ref>Int. J. Communications,Network and System Sciences, 2014, 7, 10-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of the wide-spread use of smart phones, video streaming over
mobile wireless networks has suddenly taken a huge surge in recent years.
Considering its enormous potential, mobile WiMAX is emerging as a viable
technology for mobile TV which is expected to become of key importance in the
future of mobile indus- try. In this paper, a simulation performance study of
Mobile TV over mobile WiMAX is conducted with different types of adaptive
modulation and coding taking into account key system and environment parameters
which include the variation in the speed of the mobile, path-loss, scheduling
service classes with the fixed type of mod- ulations. Our simulation has been
conducted using OPNET simulation. Simulation results show that dynamic
adaptation of modulation and coding schemes based onchannel conditions can
offer considerably more en- hanced QoS and at the same time reduce the overall
bandwidthof the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2545</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2545</id><created>2014-01-11</created><authors><author><keyname>Santhalia</keyname><forenames>Vikram</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Design and Development of a User Specific Dynamic E-Magazine</title><categories>cs.IR</categories><comments>19 pages, 6 figures</comments><report-no>MUITTR-2014-0001</report-no><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet and electronic media gaining more popularity due to ease and speed,
the count of Internet users has increased tremendously. The world is moving
faster each day with several events taking place at once and the Internet is
flooded with information in every field. There are categories of information
ranging from most relevant to user, to the information totally irrelevant or
less relevant to specific users. In such a scenario getting the information
which is most relevant to the user is indispensable to save time. The
motivation of our solution is based on the idea of optimizing the search for
information automatically. This information is delivered to user in the form of
an interactive GUI. The optimization of the contents or information served to
him is based on his social networking profiles and on his reading habits on the
proposed solution. The aim is to get the user's profile information based on
his social networking profile considering that almost every Internet user has
one. This helps us personalize the contents delivered to the user in order to
produce what is most relevant to him, in the form of a personalized e-magazine.
Further the proposed solution learns user's reading habits for example the news
he saves or clicks the most and makes a decision to provide him with the best
contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2548</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2548</id><created>2014-01-11</created><authors><author><keyname>Fiedor</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Mutual Information Rate-Based Networks in Financial Markets</title><categories>q-fin.ST cs.IT math.IT</categories><comments>12 pages, 12 figures, 2 tables, submitted to PRE</comments><msc-class>05C05, 05C10, 05C12, 28D20, 91G70</msc-class><acm-class>F.1.3; G.2.2; E.4</acm-class><journal-ref>Phys. Rev. E 89, 2014, 052801</journal-ref><doi>10.1103/PhysRevE.89.052801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years efforts in econophysics have been shifted to study how
network theory can facilitate understanding of complex financial markets. Main
part of these efforts is the study of correlation-based hierarchical networks.
This is somewhat surprising as the underlying assumptions of research looking
at financial markets is that they behave chaotically. In fact it's common for
econophysicists to estimate maximal Lyapunov exponent for log returns of a
given financial asset to confirm that prices behave chaotically. Chaotic
behaviour is only displayed by dynamical systems which are either non-linear or
infinite-dimensional. Therefore it seems that non-linearity is an important
part of financial markets, which is proved by numerous studies confirming
financial markets display significant non-linear behaviour, yet network theory
is used to study them using almost exclusively correlations and partial
correlations, which are inherently dealing with linear dependencies only. In
this paper we introduce a way to incorporate non-linear dynamics and
dependencies into hierarchical networks to study financial markets using mutual
information and its dynamical extension: the mutual information rate. We
estimate it using multidimensional Lempel-Ziv complexity and then convert it
into an Euclidean metric in order to find appropriate topological structure of
networks modelling financial markets. We show that this approach leads to
different results than correlation-based approach used in most studies, on the
basis of 15 biggest companies listed on Warsaw Stock Exchange in the period of
2009-2012 and 91 companies listed on NYSE100 between 2003 and 2013, using
minimal spanning trees and planar maximally filtered graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2553</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2553</id><created>2014-01-11</created><updated>2014-05-02</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>Testing for Synchronization</title><categories>cs.FL</categories><comments>minor corrections</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the first problem that appears in any application of
synchronizing automata, namely, the problem of deciding whether or not a given
$n$-state $k$-letter automaton is synchronizing. First we generalize results
from \cite{RandSynch},\cite{On2Problems} for the case of strongly connected
partial automata. Specifically, for $k&gt;1$ we show that an automaton is
synchronizing with probability $1-O(\frac{1}{n^{0.5k}})$ and present an
algorithm with linear in $n$ expected time, while the best known algorithm is
quadratic on each instance. This results are interesting due to their
applications in synchronization of finite state information sources.
  After that we consider the synchronization of reachable partial automata that
has application for splicing systems in computational biology. For this case we
prove that the problem of testing a given automaton for synchronization is
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2560</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2560</id><created>2014-01-11</created><authors><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Millimeter Wave Cellular Wireless Networks: Potentials and Challenges</title><categories>cs.NI</categories><comments>17 pages, 15 figures. arXiv admin note: text overlap with
  arXiv:1312.4921</comments><journal-ref>Proceedings of the IEEE , vol.102, no.3, pp.366,385, March 2014</journal-ref><doi>10.1109/JPROC.2014.2299397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmW) frequencies between 30 and 300 GHz are a new frontier
for cellular communication that offers the promise of orders of magnitude
greater bandwidths combined with further gains via beamforming and spatial
multiplexing from multi-element antenna arrays. This paper surveys measurements
and capacity studies to assess this technology with a focus on small cell
deployments in urban environments. The conclusions are extremely encouraging;
measurements in New York City at 28 and 73 GHz demonstrate that, even in an
urban canyon environment, significant non-line-of-sight (NLOS) outdoor,
street-level coverage is possible up to approximately 200 m from a potential
low power micro- or picocell base station. In addition, based on statistical
channel models from these measurements, it is shown that mmW systems can offer
more than an order of magnitude increase in capacity over current
state-of-the-art 4G cellular networks at current cell densities. Cellular
systems, however, will need to be significantly redesigned to fully achieve
these gains. Specifically, the requirement of highly directional and adaptive
transmissions, directional isolation between links and significant
possibilities of outage have strong implications on multiple access, channel
structure, synchronization and receiver design. To address these challenges,
the paper discusses how various technologies including adaptive beamforming,
multihop relaying, heterogeneous network architectures and carrier aggregation
can be leveraged in the mmW context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2566</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2566</id><created>2014-01-11</created><updated>2014-08-13</updated><authors><author><keyname>Melikyan</keyname><forenames>Hayk</forenames></author><author><keyname>Zusmanovich</keyname><forenames>Pasha</forenames></author></authors><title>Melikyan algebra is a deformation of a Poisson algebra</title><categories>math.RA cs.SC</categories><comments>v.2: minor corrections in English and formatting</comments><msc-class>17B50, 17B55, 17B63, 17-04</msc-class><journal-ref>Journal of Physics: Conference Series 532 (2014), 012019</journal-ref><doi>10.1088/1742-6596/532/1/012019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove, using computer, that the restricted Melikyan algebra of dimension
125 is a deformation of a Poisson algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2567</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2567</id><created>2014-01-11</created><updated>2014-01-20</updated><authors><author><keyname>Ilik</keyname><forenames>Danko</forenames></author></authors><title>Axioms and Decidability for Type Isomorphism in the Presence of Sums</title><categories>cs.LO cs.PL math.LO</categories><doi>10.1145/2603088.2603115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of characterizing isomorphisms of types, or,
equivalently, constructive cardinality of sets, in the simultaneous presence of
disjoint unions, Cartesian products, and exponentials. Mostly relying on
results about polynomials with exponentiation that have not been used in our
context, we derive: that the usual finite axiomatization known as High-School
Identities (HSI) is complete for a significant subclass of types; that it is
decidable for that subclass when two types are isomorphic; that, for the whole
of the set of types, a recursive extension of the axioms of HSI exists that is
complete; and that, for the whole of the set of types, the question as to
whether two types are isomorphic is decidable when base types are to be
interpreted as finite sets. We also point out certain related open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2568</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2568</id><created>2014-01-11</created><authors><author><keyname>Floor</keyname><forenames>P&#xe5;l Anders</forenames></author><author><keyname>Kim</keyname><forenames>Anna N.</forenames></author><author><keyname>Ramstad</keyname><forenames>Tor A.</forenames></author><author><keyname>Balasingham</keyname><forenames>Ilangko</forenames></author><author><keyname>Wernersson</keyname><forenames>Niklas</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Zero-Delay Joint Source-Channel Coding for a Multivariate Gaussian on a
  Gaussian MAC</title><categories>cs.IT math.IT</categories><comments>11 page draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, communication of a Multivariate Gaussian over a Gaussian
Multiple Access Channel is studied. Distributed zero-delay joint source-channel
coding (JSCC) solutions to the problem are given. Both nonlinear and linear
approaches are discussed. The performance upper bound (signal-to-distortion
ratio) for arbitrary code length is also derived and Zero-delay cooperative
JSCC is briefly addressed in order to provide an approximate bound on the
performance of zero-delay schemes. The main contribution is a nonlinear hybrid
discrete-analog JSSC scheme based on distributed quantization and a linear
continuous mapping named Distributed Quantizer Linear Coder (DQLC). The DQLC
has promising performance which improves with increasing correlation, and is
robust against variations in noise level. The DQLC exhibits a constant gap to
the performance upper bound as the signal-to-noise ratio (SNR) becomes large
for any number of sources and values of correlation. Therefore it outperforms a
linear solution (uncoded transmission) in any case when the SNR gets
sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2569</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2569</id><created>2014-01-11</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author></authors><title>Multi Terminal Probabilistic Compressed Sensing</title><categories>cs.IT math.IT stat.ML</categories><comments>11 pages, 13 figures. arXiv admin note: text overlap with
  arXiv:1112.0708 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the `Approximate Message Passing' (AMP) algorithm, initially
developed for compressed sensing of signals under i.i.d. Gaussian measurement
matrices, has been extended to a multi-terminal setting (MAMP algorithm). It
has been shown that similar to its single terminal counterpart, the behavior of
MAMP algorithm is fully characterized by a `State Evolution' (SE) equation for
large block-lengths. This equation has been used to obtain the rate-distortion
curve of a multi-terminal memoryless source. It is observed that by spatially
coupling the measurement matrices, the rate-distortion curve of MAMP algorithm
undergoes a phase transition, where the measurement rate region corresponding
to a low distortion (approximately zero distortion) regime is fully
characterized by the joint and conditional Renyi information dimension (RID) of
the multi-terminal source. This measurement rate region is very similar to the
rate region of the Slepian-Wolf distributed source coding problem where the RID
plays a role similar to the discrete entropy.
  Simulations have been done to investigate the empirical behavior of MAMP
algorithm. It is observed that simulation results match very well with
predictions of SE equation for reasonably large block-lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2571</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2571</id><created>2014-01-11</created><authors><author><keyname>Rashid</keyname><forenames>Mahmood A.</forenames></author><author><keyname>Hoque</keyname><forenames>Md Tamjidul</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>Association Rules Mining Based Clinical Observations</title><categories>cs.DB cs.CE</categories><comments>5 pages, MEDINFO 2010, C. Safran et al. (Eds.), IOS Press</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Healthcare institutes enrich the repository of patients' disease related
information in an increasing manner which could have been more useful by
carrying out relational analysis. Data mining algorithms are proven to be quite
useful in exploring useful correlations from larger data repositories. In this
paper we have implemented Association Rules mining based a novel idea for
finding co-occurrences of diseases carried by a patient using the healthcare
repository. We have developed a system-prototype for Clinical State Correlation
Prediction (CSCP) which extracts data from patients' healthcare database,
transforms the OLTP data into a Data Warehouse by generating association rules.
The CSCP system helps reveal relations among the diseases. The CSCP system
predicts the correlation(s) among primary disease (the disease for which the
patient visits the doctor) and secondary disease/s (which is/are other
associated disease/s carried by the same patient having the primary disease).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2580</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2580</id><created>2014-01-11</created><updated>2014-02-15</updated><authors><author><keyname>Rabinovich</keyname><forenames>Alexander</forenames><affiliation>Tel Aviv University</affiliation></author></authors><title>A Proof of Kamp's theorem</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  18, 2014) lmcs:730</journal-ref><doi>10.2168/LMCS-10(1:14)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple proof of Kamp's theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2592</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2592</id><created>2014-01-12</created><authors><author><keyname>Geng</keyname><forenames>Chunhua</forenames></author><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>On the Optimality of Treating Interference as Noise: General Message
  Sets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a K-user Gaussian interference channel, it has been shown that if for each
user the desired signal strength is no less than the sum of the strengths of
the strongest interference from this user and the strongest interference to
this user (all values in dB scale), then treating interference as noise (TIN)
is optimal from the perspective of generalized degrees-of-freedom (GDoF) and
achieves the entire channel capacity region to within a constant gap. In this
work, we show that for such TIN-optimal interference channels, even if the
message set is expanded to include an independent message from each transmitter
to each receiver, operating the new channel as the original interference
channel and treating interference as noise is still optimal for the sum
capacity up to a constant gap. Furthermore, we extend the result to the
sum-GDoF optimality of TIN in the general setting of X channels with arbitrary
numbers of transmitters and receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2594</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2594</id><created>2014-01-12</created><authors><author><keyname>Nie</keyname><forenames>You-Qi</forenames></author><author><keyname>Zhang</keyname><forenames>Hong-Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Ma</keyname><forenames>Xiongfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Pan</keyname><forenames>Jian-Wei</forenames></author></authors><title>Practical and fast quantum random number generation based on photon
  arrival time relative to external reference</title><categories>quant-ph cs.CR</categories><comments>9 pages, 3 figures, 1 table. Accepted for publication in Applied
  Physics Letters</comments><journal-ref>Appl. Phys. Lett. 104, 051110 (2014)</journal-ref><doi>10.1063/1.4863224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a practical high-speed quantum random number generator, where the
timing of single-photon detection relative to an external time reference is
measured as the raw data. The bias of the raw data can be substantially reduced
compared with the previous realizations. The raw random bit rate of our
generator can reach 109 Mbps. We develop a model for the generator and evaluate
the min-entropy of the raw data. Toeplitz matrix hashing is applied for
randomness extraction, after which the final random bits are able to pass the
standard randomness tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2596</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2596</id><created>2014-01-12</created><authors><author><keyname>Huang</keyname><forenames>Zhenqi</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Differentially Private Distributed Optimization</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed optimization and iterative consensus literature, a standard
problem is for $N$ agents to minimize a function $f$ over a subset of Euclidean
space, where the cost function is expressed as a sum $\sum f_i$. In this paper,
we study the private distributed optimization (PDOP) problem with the
additional requirement that the cost function of the individual agents should
remain differentially private. The adversary attempts to infer information
about the private cost functions from the messages that the agents exchange.
Achieving differential privacy requires that any change of an individual's cost
function only results in unsubstantial changes in the statistics of the
messages. We propose a class of iterative algorithms for solving PDOP, which
achieves differential privacy and convergence to the optimal value. Our
analysis reveals the dependence of the achieved accuracy and the privacy levels
on the the parameters of the algorithm. We observe that to achieve
$\epsilon$-differential privacy the accuracy of the algorithm has the order of
$O(\frac{1}{\epsilon^2})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2607</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2607</id><created>2014-01-12</created><authors><author><keyname>Wang</keyname><forenames>Anyu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhifang</forenames></author></authors><title>Repair Locality From a Combinatorial Perspective</title><categories>cs.IT math.IT</categories><doi>10.1109/ISIT.2014.6875178</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repair locality is a desirable property for erasure codes in distributed
storage systems. Recently, different structures of local repair groups have
been proposed in the definitions of repair locality. In this paper, the concept
of regenerating set is introduced to characterize the local repair groups. A
definition of locality $r^{(\delta -1)}$ (i.e., locality $r$ with repair
tolerance $\delta -1$) under the most general structure of regenerating sets is
given. All previously studied locality turns out to be special cases of this
definition. Furthermore, three representative concepts of locality proposed
before are reinvestigated under the framework of regenerating sets, and their
respective upper bounds on the minimum distance are reproved in a uniform and
brief form. Additionally, a more precise distance bound is derived for the
square code which is a class of linear codes with locality $r^{(2)}$ and high
information rate, and an explicit code construction attaining the optimal
distance bound is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2610</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2610</id><created>2014-01-12</created><authors><author><keyname>Ballatore</keyname><forenames>Andrea</forenames></author><author><keyname>Wilson</keyname><forenames>David C.</forenames></author><author><keyname>Bertolotto</keyname><forenames>Michela</forenames></author></authors><title>A Survey of Volunteered Open Geo-Knowledge Bases in the Semantic Web</title><categories>cs.DL cs.CY cs.IR</categories><journal-ref>in Quality Issues in the Management of Web Information, ISRL 50,
  pp. 93-120, Springer, 2013</journal-ref><doi>10.1007/978-3-642-37688-7_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, rapid advances in web technologies, coupled with
innovative models of spatial data collection and consumption, have generated a
robust growth in geo-referenced information, resulting in spatial information
overload. Increasing 'geographic intelligence' in traditional text-based
information retrieval has become a prominent approach to respond to this issue
and to fulfill users' spatial information needs. Numerous efforts in the
Semantic Geospatial Web, Volunteered Geographic Information (VGI), and the
Linking Open Data initiative have converged in a constellation of open
knowledge bases, freely available online. In this article, we survey these open
knowledge bases, focusing on their geospatial dimension. Particular attention
is devoted to the crucial issue of the quality of geo-knowledge bases, as well
as of crowdsourced data. A new knowledge base, the OpenStreetMap Semantic
Network, is outlined as our contribution to this area. Research directions in
information integration and Geographic Information Retrieval (GIR) are then
reviewed, with a critical discussion of their current limitations and future
prospects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2612</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2612</id><created>2014-01-12</created><updated>2015-01-21</updated><authors><author><keyname>Elishco</keyname><forenames>Ohad</forenames></author><author><keyname>Meyerovitch</keyname><forenames>Tom</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author></authors><title>Semi-constrained Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When transmitting information over a noisy channel, two approaches, dating
back to Shannon's work, are common: assuming the channel errors are independent
of the transmitted content and devising an error-correcting code, or assuming
the errors are data dependent and devising a constrained-coding scheme that
eliminates all offending data patterns. In this paper we analyze a middle road,
which we call a semiconstrained system. In such a system, which is an extension
of the channel with cost constraints model, we do not eliminate the
error-causing sequences entirely, but rather restrict the frequency in which
they appear.
  We address several key issues in this study. The first is proving closed-form
bounds on the capacity which allow us to bound the asymptotics of the capacity.
In particular, we bound the rate at which the capacity of the semiconstrained
$(0,k)$-RLL tends to $1$ as $k$ grows. The second key issue is devising
efficient encoding and decoding procedures that asymptotically achieve capacity
with vanishing error. Finally, we consider delicate issues involving the
continuity of the capacity and a relaxation of the definition of
semiconstrained systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2617</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2617</id><created>2014-01-12</created><authors><author><keyname>Mayer</keyname><forenames>Robert V.</forenames></author></authors><title>Computer model of teaching with the varied coefficient of forgetting</title><categories>cs.CY</categories><comments>4 pages, 5 figures, 3 programs (on russian).
  http://injoit.org/index.php/j1</comments><msc-class>68U20</msc-class><acm-class>I.6.3</acm-class><journal-ref>International Journal of Open Information Technologies Vol 2, No
  1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At computer modeling of process of training it is usually supposed that all
elements of a training material are forgotten with an identical speed. But in
practice that knowledge which are included in educational activity of the pupil
are remembered much more strongly and forgotten more slowly then knowledge
which he doesn't use. For the purpose of more exact research of didactic
systems is offered the model of training, in which consider that in case
increasing the number of applications of this element of a learning material:
1) duration of its use by the pupil decreases; 2) the coefficient of forgetting
decreases. The computer model is considered, programs in the Pascal language
are submitted, results of modeling are given and analyzed. Keywords: didactics,
information and cybernetic approach, computer modeling of process of training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2618</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2618</id><created>2014-01-12</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Malhotra</keyname><forenames>Vikrant</forenames></author><author><keyname>Tyagi</keyname><forenames>Ridhi</forenames></author></authors><title>Sentiment Analysis Using Collaborated Opinion Mining</title><categories>cs.IR cs.CL</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion mining and Sentiment analysis have emerged as a field of study since
the widespread of World Wide Web and internet. Opinion refers to extraction of
those lines or phrase in the raw and huge data which express an opinion.
Sentiment analysis on the other hand identifies the polarity of the opinion
being extracted. In this paper we propose the sentiment analysis in
collaboration with opinion extraction, summarization, and tracking the records
of the students. The paper modifies the existing algorithm in order to obtain
the collaborated opinion about the students. The resultant opinion is
represented as very high, high, moderate, low and very low. The paper is based
on a case study where teachers give their remarks about the students and by
applying the proposed sentiment analysis algorithm the opinion is extracted and
represented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2619</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2619</id><created>2014-01-12</created><authors><author><keyname>Friedkin</keyname><forenames>Noah E.</forenames></author></authors><title>Scale-free interpersonal influences on opinions in complex systems</title><categories>cs.SI cs.MA physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important side effect of the evolution of the human brain is an increased
capacity to form opinions in a very large domain of issues, which become points
of aggressive interpersonal disputes. Remarkably, such disputes are often no
less vigorous on small differences of opinion than large differences. Opinion
differences that may be measured on the real number line may not directly
correspond to the subjective importance of an issue and extent of resistance to
opinion change. This is a hard problem for field of opinion dynamics, a field
that has become increasingly prominent as it has attracted more contributions
to it from investigators in the natural and engineering sciences. The paper
contributes a scale-free approach to assessing the extents to which
individuals, with unknown heterogeneous resistances to influence, have been
influenced by the opinions of others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2641</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2641</id><created>2014-01-12</created><authors><author><keyname>Ismaili</keyname><forenames>Imdad Ali</forenames></author><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Shah</keyname><forenames>Azhar Ali</forenames></author></authors><title>Towards a Generic Framework for the Development of Unicode Based Digital
  Sindhi Dictionaries</title><categories>cs.CL</categories><journal-ref>Mehran University Research Journal of Engineering &amp; Technology
  Volume 31, No. 1, January 2012 [ISSN 0254-7821]</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Dictionaries are essence of any language providing vital linguistic recourse
for the language learners, researchers and scholars. This paper focuses on the
methodology and techniques used in developing software architecture for a
UBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The
proposed system provides an accurate solution for construction and
representation of Unicode based Sindhi characters in a dictionary implementing
Hash Structure algorithm and a custom java Object as its internal data
structure saved in a file. The System provides facilities for Insertion,
Deletion and Editing of new records of Sindhi. Through this framework any type
of Sindhi to English and English to Sindhi Dictionary (belonging to different
domains of knowledge, e.g. engineering, medicine, computer, biology etc.) could
be developed easily with accurate representation of Unicode Characters in font
independent manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2651</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2651</id><created>2014-01-12</created><authors><author><keyname>White</keyname><forenames>David</forenames></author></authors><title>An Overview of Schema Theory</title><categories>cs.NE</categories><comments>27 pages. Originally written in 2009 and hosted on my website, I've
  decided to put it on the arXiv as a more permanent home. The paper is
  primarily expository, so I don't really know where to submit it, but perhaps
  one day I will find an appropriate journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to give an introduction to the field of Schema
Theory written by a mathematician and for mathematicians. In particular, we
endeavor to to highlight areas of the field which might be of interest to a
mathematician, to point out some related open problems, and to suggest some
large-scale projects. Schema theory seeks to give a theoretical justification
for the efficacy of the field of genetic algorithms, so readers who have
studied genetic algorithms stand to gain the most from this paper. However,
nothing beyond basic probability theory is assumed of the reader, and for this
reason we write in a fairly informal style.
  Because the mathematics behind the theorems in schema theory is relatively
elementary, we focus more on the motivation and philosophy. Many of these
results have been proven elsewhere, so this paper is designed to serve a
primarily expository role. We attempt to cast known results in a new light,
which makes the suggested future directions natural. This involves devoting a
substantial amount of time to the history of the field.
  We hope that this exposition will entice some mathematicians to do research
in this area, that it will serve as a road map for researchers new to the
field, and that it will help explain how schema theory developed. Furthermore,
we hope that the results collected in this document will serve as a useful
reference. Finally, as far as the author knows, the questions raised in the
final section are new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2657</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2657</id><created>2014-01-12</created><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Gui</keyname><forenames>Ning</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>The Missing Ones: Key Ingredients Towards Effective Ambient Assisted
  Living Systems</title><categories>cs.CY cs.AI</categories><journal-ref>Journal of Ambient Intelligence and Smart Environments, Volume 2
  Issue 2, April 2010 Pages 109-120 IOS Press Amsterdam, The Netherlands, The
  Netherlands</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The population of elderly people keeps increasing rapidly, which becomes a
predominant aspect of our societies. As such, solutions both efficacious and
cost-effective need to be sought. Ambient Assisted Living (AAL) is a new
approach which promises to address the needs from elderly people. In this
paper, we claim that human participation is a key ingredient towards effective
AAL systems, which not only saves social resources, but also has positive
relapses on the psychological health of the elderly people. Challenges in
increasing the human participation in ambient assisted living are discussed in
this paper and solutions to meet those challenges are also proposed. We use our
proposed mutual assistance community, which is built with service oriented
approach, as an example to demonstrate how to integrate human tasks in AAL
systems. Our preliminary simulation results are presented, which support the
effectiveness of human participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2662</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2662</id><created>2014-01-12</created><authors><author><keyname>Kintali</keyname><forenames>Shiva</forenames></author></authors><title>Directed Width Parameters and Circumference of Digraphs</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the directed treewidth, DAG-width and Kelly-width of a digraph
are bounded above by its circumference plus one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2663</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2663</id><created>2014-01-12</created><authors><author><keyname>Ayd&#x131;n</keyname><forenames>Cem R&#x131;fk&#x131;</forenames></author><author><keyname>Erkan</keyname><forenames>Ali</forenames></author><author><keyname>G&#xfc;ng&#xf6;r</keyname><forenames>Tunga</forenames></author><author><keyname>Tak&#xe7;&#x131;</keyname><forenames>Hidayet</forenames></author></authors><title>Dictionary-Based Concept Mining: An Application for Turkish</title><categories>cs.CL</categories><comments>12 pages with 3 figures, to be published in &quot;International Conference
  on Foundations of Computer Science &amp; Technology (CST 2014), Zurich,
  Switzerland - January 2014 Proceedings, AIRCC&quot;</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a dictionary-based method is used to extract expressive
concepts from documents. So far, there have been many studies concerning
concept mining in English, but this area of study for Turkish, an agglutinative
language, is still immature. We used dictionary instead of WordNet, a lexical
database grouping words into synsets that is widely used for concept
extraction. The dictionaries are rarely used in the domain of concept mining,
but taking into account that dictionary entries have synonyms, hypernyms,
hyponyms and other relationships in their meaning texts, the success rate has
been high for determining concepts. This concept extraction method is
implemented on documents, that are collected from different corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2668</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2668</id><created>2014-01-12</created><updated>2014-01-14</updated><authors><author><keyname>Ma</keyname><forenames>Jianzhu</forenames></author><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>MRFalign: Protein Homology Detection through Alignment of Markov Random
  Fields</title><categories>q-bio.QM cs.CE cs.LG</categories><comments>Accepted by both RECOMB 2014 and PLOS Computational Biology</comments><doi>10.1371/journal.pcbi.1003500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-based protein homology detection has been extensively studied and so
far the most sensitive method is based upon comparison of protein sequence
profiles, which are derived from multiple sequence alignment (MSA) of sequence
homologs in a protein family. A sequence profile is usually represented as a
position-specific scoring matrix (PSSM) or an HMM (Hidden Markov Model) and
accordingly PSSM-PSSM or HMM-HMM comparison is used for homolog detection. This
paper presents a new homology detection method MRFalign, consisting of three
key components: 1) a Markov Random Fields (MRF) representation of a protein
family; 2) a scoring function measuring similarity of two MRFs; and 3) an
efficient ADMM (Alternating Direction Method of Multipliers) algorithm aligning
two MRFs. Compared to HMM that can only model very short-range residue
correlation, MRFs can model long-range residue interaction pattern and thus,
encode information for the global 3D structure of a protein family.
Consequently, MRF-MRF comparison for remote homology detection shall be much
more sensitive than HMM-HMM or PSSM-PSSM comparison. Experiments confirm that
MRFalign outperforms several popular HMM or PSSM-based methods in terms of both
alignment accuracy and remote homology detection and that MRFalign works
particularly well for mainly beta proteins. For example, tested on the
benchmark SCOP40 (8353 proteins) for homology detection, PSSM-PSSM and HMM-HMM
succeed on 48% and 52% of proteins, respectively, at superfamily level, and on
15% and 27% of proteins, respectively, at fold level. In contrast, MRFalign
succeeds on 57.3% and 42.5% of proteins at superfamily and fold level,
respectively. This study implies that long-range residue interaction patterns
are very helpful for sequence-based homology detection. The software is
available for download at http://raptorx.uchicago.edu/download/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2672</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2672</id><created>2014-01-12</created><updated>2014-03-10</updated><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>On a Duality Between Recoverable Distributed Storage and Index Coding</title><categories>cs.IT math.IT</categories><comments>A small new section and new references added. A minor error corrected
  from the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a model of a single-failure locally recoverable
distributed storage system. This model appears to give rise to a problem
seemingly dual of the well-studied index coding problem. The relation between
the dimensions of an optimal index code and optimal distributed storage code of
our model has been established in this paper. We also show some extensions to
vector codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2684</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2684</id><created>2014-01-12</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Improving Quality of Clustering using Cellular Automata for Information
  retrieval</title><categories>cs.IR</categories><comments>Journal of Computer Science 4 (2): 167-171, 2008,ISSN 1549-3636, 2008
  Science Publications</comments><journal-ref>Journal of Computer Science 4 (2): 167-171, 2008,ISSN 1549-3636,
  2008 Science Publications</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering has been widely applied to Information Retrieval (IR) on the
grounds of its potential improved effectiveness over inverted file search.
Clustering is a mostly unsupervised procedure and the majority of the
clustering algorithms depend on certain assumptions in order to define the
subgroups present in a data set .A clustering quality measure is a function
that, given a data set and its partition into clusters, returns a non-negative
real number representing the quality of that clustering. Moreover, they may
behave in a different way depending on the features of the data set and their
input parameters values. Therefore, in most applications the resulting
clustering scheme requires some sort of evaluation as regards its validity. The
quality of clustering can be enhanced by using a Cellular Automata Classifier
for information retrieval. In this study we take the view that if cellular
automata with clustering is applied to search results (query-specific
clustering), then it has the potential to increase the retrieval effectiveness
compared both to that of static clustering and of conventional inverted file
search. We conducted a number of experiments using ten document collections and
eight hierarchic clustering methods. Our results show that the effectiveness of
query-specific clustering with cellular automata is indeed higher and suggest
that there is scope for its application to IR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2686</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2686</id><created>2014-01-12</created><authors><author><keyname>Gilles</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Heal</keyname><forenames>Kathryn</forenames></author></authors><title>A parameterless scale-space approach to find meaningful modes in
  histograms - Application to image and spectrum segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm to automatically detect meaningful
modes in a histogram. The proposed method is based on the behavior of local
minima in a scale-space representation. We show that the detection of such
meaningful modes is equivalent in a two classes clustering problem on the
length of minima scale-space curves. The algorithm is easy to implement, fast,
and does not require any parameters. We present several results on histogram
and spectrum segmentation, grayscale image segmentation and color image
reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2688</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2688</id><created>2014-01-12</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inamupudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>PSMACA: An Automated Protein Structure Prediction Using MACA (Multiple
  Attractor Cellular Automata)</title><categories>cs.CE cs.LG</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1310.4342, arXiv:1310.4495</comments><journal-ref>Journal of Bioinformatics and Intelligent Control Vol 2, pp
  211--215, 2013</journal-ref><doi>10.1166/jbic.2013.1052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein Structure Predication from sequences of amino acid has gained a
remarkable attention in recent years. Even though there are some prediction
techniques addressing this problem, the approximate accuracy in predicting the
protein structure is closely 75%. An automated procedure was evolved with MACA
(Multiple Attractor Cellular Automata) for predicting the structure of the
protein. Most of the existing approaches are sequential which will classify the
input into four major classes and these are designed for similar sequences.
PSMACA is designed to identify ten classes from the sequences that share
twilight zone similarity and identity with the training sequences. This method
also predicts three states (helix, strand, and coil) for the structure. Our
comprehensive design considers 10 feature selection methods and 4 classifiers
to develop MACA (Multiple Attractor Cellular Automata) based classifiers that
are build for each of the ten classes. We have tested the proposed classifier
with twilight-zone and 1-high-similarity benchmark datasets with over three
dozens of modern competing predictors shows that PSMACA provides the best
overall accuracy that ranges between 77% and 88.7% depending on the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2690</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2690</id><created>2014-01-12</created><authors><author><keyname>Ma</keyname><forenames>Shuai</forenames></author><author><keyname>Feng</keyname><forenames>Kaiyu</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author><author><keyname>Li</keyname><forenames>Jianxin</forenames></author><author><keyname>Huai</keyname><forenames>Jinpeng</forenames></author></authors><title>Distance Landmarks Revisited for Road Graphs</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computing shortest distances is one of the fundamental problems on graphs,
and remains a {\em challenging} task today. {\em Distance} {\em landmarks} have
been recently studied for shortest distance queries with an auxiliary data
structure, referred to as {\em landmark} {\em covers}. This paper studies how
to apply distance landmarks for fast {\em exact} shortest distance query
answering on large road graphs. However, the {\em direct} application of
distance landmarks is {\em impractical} due to the high space and time cost. To
rectify this problem, we investigate novel techniques that can be seamlessly
combined with distance landmarks. We first propose a notion of {\em hybrid
landmark covers}, a revision of landmark covers. Second, we propose a notion of
{\em agents}, each of which represents a small subgraph and holds good
properties for fast distance query answering. We also show that agents can be
computed in {\em linear time}. Third, we introduce graph partitions to deal
with the remaining subgraph that cannot be captured by agents. Fourth, we
develop a unified framework that seamlessly integrates our proposed techniques
and existing optimization techniques, for fast shortest distance query
answering. Finally, we experimentally verify that our techniques significantly
improve the efficiency of shortest distance queries, using real-life road
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2692</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2692</id><created>2014-01-12</created><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>On the Optimality of Treating Interference as Noise for $K$ user
  Parallel Gaussian Interference Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown recently by Geng et al. that in a $K$ user Gaussian
interference network, if for each user the desired signal strength is no less
than the sum of the strengths of the strongest interference from this user and
the strongest interference to this user (all signal strengths measured in dB
scale), then power control and treating interference as noise (TIN) is
sufficient to achieve the entire generalized degrees of freedom (GDoF) region.
Motivated by the intuition that the deterministic model of Avestimehr et al.
(ADT deterministic model) is particularly suited for exploring the optimality
of TIN, the results of Geng et al. are first re-visited under the ADT
deterministic model, and are shown to directly translate between the Gaussian
and deterministic settings. Next, we focus on the extension of these results to
parallel interference networks, from a sum-capacity/sum-GDoF perspective. To
this end, we interpret the explicit characterization of the
sum-capacity/sum-GDoF of a TIN optimal network (without parallel channels) as a
minimum weighted matching problem in combinatorial optimization, and obtain a
simple characterization in terms of a partition of the interference network
into vertex-disjoint cycles. Aided by insights from the cyclic partition, the
sum-capacity optimality of TIN for $K$ user parallel interference networks is
characterized for the ADT deterministic model, leading ultimately to
corresponding GDoF results for the Gaussian setting. In both cases, subject to
a mild invertibility condition the optimality of TIN is shown to extend to
parallel networks in a separable fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2693</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2693</id><created>2014-01-12</created><updated>2014-01-23</updated><authors><author><keyname>Ding</keyname><forenames>Yang</forenames></author></authors><title>On List-decodability of Random Rank Metric Codes</title><categories>cs.IT math.IT</categories><comments>8 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we consider list decoding for both random rank metric
codes and random linear rank metric codes. Firstly, we show that, for arbitrary
$0&lt;R&lt;1$ and $\epsilon&gt;0$ ($\epsilon$ and $R$ are independent), if
$0&lt;\frac{n}{m}\leq \epsilon$, then with high probability a random rank metric
code in $F_{q}^{m\times n}$ of rate $R$ can be list-decoded up to a fraction
$(1-R-\epsilon)$ of rank errors with constant list size $L$ satisfying $L\leq
O(1/\epsilon)$. Moreover, if $\frac{n}{m}\geq\Theta_R(\epsilon)$, any rank
metric code in $F_{q}^{m\times n}$ with rate $R$ and decoding radius
$\rho=1-R-\epsilon$ can not be list decoded in ${\rm poly}(n)$ time. Secondly,
we show that if $\frac{n}{m}$ tends to a constant $b\leq 1$, then every
$F_q$-linear rank metric code in $F_{q}^{m\times n}$ with rate $R$ and list
decoding radius $\rho$ satisfies the Gilbert-Varsharmov bound, i.e., $R\leq
(1-\rho)(1-b\rho)$. Furthermore, for arbitrary $\epsilon&gt;0$ and any $0&lt;\rho&lt;1$,
with high probability a random $F_q$-linear rank metric codes with rate
$R=(1-\rho)(1-b\rho)-\epsilon$ can be list decoded up to a fraction $\rho$ of
rank errors with constant list size $L$ satisfying $L\leq O(\exp(1/\epsilon))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2702</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2702</id><created>2014-01-12</created><updated>2014-01-17</updated><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Clearing Markets via Bundles</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for combinatorial market design problems, where a set of
heterogeneous and indivisible objects are priced and sold to potential buyers
subject to equilibrium constraints. Extending the CWE notion introduced by
Feldman et al. [STOC 2013], we introduce the concept of a Market-Clearing
Combinatorial Walrasian Equilibium (MC-CWE) as a natural relaxation of the
classical Walrasian equilibrium (WE) solution concept. The only difference
between a MC-CWE and a WE is the ability for the seller to bundle the items
prior to sale. This innocuous and natural bundling operation imposes a plethora
of algorithmic and economic challenges and opportunities. Unlike WE, which is
guaranteed to exist only for (gross) substitutes valuations, a MC-CWE always
exists. The main algorithmic challenge, therefore, is to design computationally
efficient mechanisms that generate MC-CWE outcomes that approximately maximize
social welfare. For a variety of valuation classes encompassing substitutes and
complements (including super-additive, single-minded and budget-additive
valuations), we design polynomial-time MC-CWE mechanisms that provide tight
welfare approximation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2713</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2713</id><created>2014-01-13</created><authors><author><keyname>Harper</keyname><forenames>Marc</forenames></author></authors><title>Entropy Rates of the Multidimensional Moran Processes and
  Generalizations</title><categories>math.DS cs.IT math.IT q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interrelationships of the fundamental biological processes natural
selection, mutation, and stochastic drift are quantified by the entropy rate of
Moran processes with mutation, measuring the long-run variation of a Markov
process. The entropy rate is shown to behave intuitively with respect to
evolutionary parameters such as monotonicity with respect to mutation
probability (for the neutral landscape), relative fitness, and strength of
selection. Strict upper bounds, depending only on the number of replicating
types, for the entropy rate are given and the neutral fitness landscape attains
the maximum in the large population limit. Various additional limits are
computed including small mutation, weak and strong selection, and large
population holding the other parameters constant, revealing the individual
contributions and dependences of each evolutionary parameter on the long-run
outcomes of the processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2714</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2714</id><created>2014-01-13</created><authors><author><keyname>Pandya</keyname><forenames>Paritosh K.</forenames></author><author><keyname>Shah</keyname><forenames>Simoni S.</forenames></author></authors><title>Deterministic Logics for UL</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of Unambiguous Star-Free Regular Languages (UL) was defined by
Schutzenberger as the class of languages defined by Unambiguous Polynomials. UL
has been variously characterized (over finite words) by logics such as
TL[X_a,Y_a], UITL, TL[F,P], FO2[&lt;], the variety DA of monoids, as well as
partially-ordered two-way DFA (po2DFA). We revisit this language class with
emphasis on notion of unambiguity and develop on the concept of Deterministic
Logics for UL. The formulas of deterministic logics uniquely parse a word in
order to evaluate satisfaction. We show that several deterministic logics
robustly characterize UL. Moreover, we derive constructive reductions from
these logics to the po2DFA automata. These reductions also allow us to show
NP-complete satisfaction complexity for the deterministic logics considered.
  Logics such as TL[F,P], FO2[&lt;] are not deterministic and have been shown to
characterize UL using algebraic methods. However there has been no known
constructive reduction from these logics to po2DFA. We use deterministic logics
to bridge this gap. The language-equivalent po2DFA for a given TL[F,P] formula
is constructed and we analyze its size relative to the size of the TL[F,P]
formula. This is an efficient reduction which gives an alternate proof to
NP-complete satisfiability complexity of TL[F,P] formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2716</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2716</id><created>2014-01-13</created><authors><author><keyname>Ding</keyname><forenames>Yang</forenames></author><author><keyname>Jin</keyname><forenames>Lingfei</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Erasure List-Decodable Codes from Random and Algebraic Geometry Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure list decoding was introduced to correct a larger number of erasures
with output of a list of possible candidates. In the present paper, we consider
both random linear codes and algebraic geometry codes for list decoding erasure
errors. The contributions of this paper are two-fold. Firstly, we show that,
for arbitrary $0&lt;R&lt;1$ and $\epsilon&gt;0$ ($R$ and $\epsilon$ are independent),
with high probability a random linear code is an erasure list decodable code
with constant list size $2^{O(1/\epsilon)}$ that can correct a fraction
$1-R-\epsilon$ of erasures, i.e., a random linear code achieves the
information-theoretic optimal trade-off between information rate and fraction
of erasure errors. Secondly, we show that algebraic geometry codes are good
erasure list-decodable codes. Precisely speaking, for any $0&lt;R&lt;1$ and
$\epsilon&gt;0$, a $q$-ary algebraic geometry code of rate $R$ from the
Garcia-Stichtenoth tower can correct
$1-R-\frac{1}{\sqrt{q}-1}+\frac{1}{q}-\epsilon$ fraction of erasure errors with
list size $O(1/\epsilon)$. This improves the Johnson bound applied to algebraic
geometry codes. Furthermore, list decoding of these algebraic geometry codes
can be implemented in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2720</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2720</id><created>2014-01-13</created><updated>2014-09-27</updated><authors><author><keyname>Novakovi&#x107;</keyname><forenames>Vedran</forenames></author></authors><title>A hierarchically blocked Jacobi SVD algorithm for single and multiple
  graphics processing units</title><categories>cs.NA cs.DC cs.MS</categories><comments>Accepted for publication in SIAM Journal on Scientific Computing</comments><msc-class>65Y05 (Primary) 65Y10, 65F15 (Secondary)</msc-class><acm-class>G.1.0; G.1.3; G.4</acm-class><journal-ref>SIAM J. Sci. Comput. 37 (2015), C1-C30</journal-ref><doi>10.1137/140952429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hierarchically blocked one-sided Jacobi algorithm for the
singular value decomposition (SVD), targeting both single and multiple graphics
processing units (GPUs). The blocking structure reflects the levels of GPU's
memory hierarchy. The algorithm may outperform MAGMA's dgesvd, while retaining
high relative accuracy. To this end, we developed a family of parallel pivot
strategies on GPU's shared address space, but applicable also to inter-GPU
communication. Unlike common hybrid approaches, our algorithm in a single GPU
setting needs a CPU for the controlling purposes only, while utilizing GPU's
resources to the fullest extent permitted by the hardware. When required by the
problem size, the algorithm, in principle, scales to an arbitrary number of GPU
nodes. The scalability is demonstrated by more than twofold speedup for
sufficiently large matrices on a Tesla S2050 system with four GPUs vs. a single
Fermi card.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2724</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2724</id><created>2014-01-13</created><authors><author><keyname>Bella</keyname><forenames>Fabio</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author></authors><title>Capturing Evidence From Wireless Internet Services Development</title><categories>cs.SE cs.NI</categories><comments>6 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1372130</comments><journal-ref>Proceedings of The Eleventh Annual International Workshop on
  Software Technology and Engineering Practice (STEP), pp.33,39, 19-21 Sept.
  2003</journal-ref><doi>10.1109/STEP.2003.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The merging of the Internet with the Wireless services domain has created a
potential market whose characteristics are new technologies and time-to-market
pressure. The lack of knowledge about new technologies and the need to be
competitive in a short time demand that software organizations learn quickly
about this domain and its characteristics. Additionally, the effects of
development techniques in this context need to be understood. Learning from
previous experiences in such a changing environment demands a clear
understanding of the evidence to be captured, and how it could be used in the
future. This article presents definitions of quantitative and qualitative
evidence, and templates for capturing such evidence in a systematic way. Such
templates were used in the context of two pilot projects dealing with the
development of Wireless Internet Services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2727</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2727</id><created>2014-01-13</created><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>Hardware Implementation of four byte per clock RC4 algorithm</title><categories>cs.AR</categories><comments>This is an unpublished draft version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of cryptography till date the 2-byte in 1-clock is the best
known RC4 hardware design [1], while 1-byte in 1-clock [2], and the 1-byte in 3
clocks [3][4] are the best known implementation. The design algorithm in[2]
considers two consecutive bytes together and processes them in 2 clocks. The
design [1] is a pipelining architecture of [2]. The design of 1-byte in
3-clocks is too much modular and clock hungry. In this paper considering the
RC4 algorithm, as it is, a simpler RC4 hardware design providing higher
throughput is proposed in which 6 different architecture has been proposed. In
design 1, 1-byte is processed in 1-clock, design 2 is a dynamic KSA-PRGA
architecture of Design 1. Design 3 can process 2 byte in a single clock, where
as Design 4 is Dynamic KSA-PRGA architecture of Design 3. Design 5 and Design 6
are parallelization architecture design 2 and design 4 which can compute 4 byte
in a single clock. The maturity in terms of throughput, power consumption and
resource usage, has been achieved from design 1 to design 6. The RC4 encryption
and decryption designs are respectively embedded on two FPGA boards as
co-processor hardware, the communication between the two boards performed using
Ethernet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2730</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2730</id><created>2014-01-13</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Torre</keyname><forenames>Alicia Fern&#xe1;ndez-del Viso</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Carlos Rebate</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author></authors><title>Estimating the Effort Overhead in Global Software Development</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=5581517</comments><journal-ref>Proceedings of the IEEE International Conference on Global
  Software Engineering (ICGSE 2010), Princeton, USA, August 23-26 2010</journal-ref><doi>10.1109/ICGSE.2010.38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models for effort and cost estimation are important for distributed software
development as well as for collocated software and system development. Standard
cost models only insufficiently consider the characteristics of distributed
development such as dissimilar abilities at the different sites or significant
overhead due to remote collaboration. Therefore, explicit cost models for
distributed development are needed. In this article, we present the initial
development of a cost overhead model for a Spanish global software development
organization. The model was developed using the CoBRA approach for cost
estimation. As a result, cost drivers for the specific distributed development
context were identified and their impact was quantified on an empirical basis.
The article presents related work, an overview of the approach, and its
application in the industrial context. Finally, we sketch the inclusion of the
model in an approach for systematic task allocation and give an overview of
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2731</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2731</id><created>2014-01-13</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Torre</keyname><forenames>Alicia Fern&#xe1;ndez-del Viso</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Carlos Rebate</forenames></author><author><keyname>Heinz</keyname><forenames>Markus</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author></authors><title>A Rule-based Model for Customized Risk Identification in Distributed
  Software Development Projects</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=5581511</comments><journal-ref>Proceedings of the IEEE International Conference on Global
  Software Engineering (ICGSE 2010), Princeton, USA, August 23-26 2010</journal-ref><doi>10.1109/ICGSE.2010.32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many project risks in distributed software development are very different
from the ones in collocated development and therefore are often overlooked. At
the same time, they depend to a large extent on project-specific
characteristics. This article presents a model for identifying risks early in a
project. This model systematically captures experiences from past projects and
is based on a set of logical rules describing how project characteristics
influence typical risks in distributed development. Thus, the model is able to
assess risks individually for each project. It was developed by applying
qualitative content analysis to 19 interviews with practitioners. An evaluation
using expert interviews showed that the risks identified by the model matched
the actual experiences in 81% of the cases; of these, 40% have not been
regarded yet at project start. The article describes the concepts of the model,
its instantiation and evaluation, followed by a conclusion and future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2732</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2732</id><created>2014-01-13</created><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>Fault Detection for RC4 Algorithm and its Implementation on FPGA
  Platform</title><categories>cs.AR</categories><comments>Published Book Title: Elsevier Science and Technology, ICCN 2013,
  Bangalore, Page(s): 224 - 232, Volume 3, DOI-03.elsevierst.2013.3.ICCN25,
  ISBN :9789351071044</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hardware implementation of a cryptographic algorithm, one may achieve
leakage of secret information by creating scopes to introduce controlled faulty
bit(s) even though the algorithm is mathematically a secured one. The technique
is very e?ective in respect of crypto processors embedded in smart cards. In
this paper few fault detecting architectures for RC4 algorithm are designed and
implemented on Virtex5(ML505, LX110t) FPGA board. The results indicate that the
proposed architectures can handle most of the faults without loss of throughput
consuming marginally additional hardware and power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2751</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2751</id><created>2014-01-13</created><authors><author><keyname>Duval</keyname><forenames>Dominique</forenames></author><author><keyname>Echahed</keyname><forenames>Rachid</forenames></author><author><keyname>Prost</keyname><forenames>Frederic</forenames></author><author><keyname>Ribeiro</keyname><forenames>Leila</forenames></author></authors><title>Transformation of Attributed Structures with Cloning (Long Version)</title><categories>cs.SE</categories><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Copying, or cloning, is a basic operation used in the specification of many
applications in computer science. However, when dealing with complex
structures, like graphs, cloning is not a straightforward operation since a
copy of a single vertex may involve (implicitly)copying many edges. Therefore,
most graph transformation approaches forbid the possibility of cloning. We
tackle this problem by providing a framework for graph transformations with
cloning. We use attributed graphs and allow rules to change attributes. These
two features (cloning/changing attributes) together give rise to a powerful
formal specification approach. In order to handle different kinds of graphs and
attributes, we first define the notion of attributed structures in an abstract
way. Then we generalise the sesqui-pushout approach of graph transformation in
the proposed general framework and give appropriate conditions under which
attributed structures can be transformed. Finally, we instantiate our general
framework with different examples, showing that many structures can be handled
and that the proposed framework allows one to specify complex operations in a
natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2753</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2753</id><created>2014-01-13</created><updated>2015-01-02</updated><authors><author><keyname>Zhao</keyname><forenames>Peilin</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Stochastic Optimization with Importance Sampling</title><categories>stat.ML cs.LG</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uniform sampling of training data has been commonly used in traditional
stochastic optimization algorithms such as Proximal Stochastic Gradient Descent
(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although
uniform sampling can guarantee that the sampled stochastic quantity is an
unbiased estimate of the corresponding true quantity, the resulting estimator
may have a rather high variance, which negatively affects the convergence of
the underlying optimization procedure. In this paper we study stochastic
optimization with importance sampling, which improves the convergence rate by
reducing the stochastic variance. Specifically, we study prox-SGD (actually,
stochastic mirror descent) with importance sampling and prox-SDCA with
importance sampling. For prox-SGD, instead of adopting uniform sampling
throughout the training process, the proposed algorithm employs importance
sampling to minimize the variance of the stochastic gradient. For prox-SDCA,
the proposed importance sampling scheme aims to achieve higher expected dual
value at each dual coordinate ascent step. We provide extensive theoretical
analysis to show that the convergence rates with the proposed importance
sampling methods can be significantly improved under suitable conditions both
for prox-SGD and for prox-SDCA. Experiments are provided to verify the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2757</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2757</id><created>2014-01-13</created><authors><author><keyname>Kl&#xe4;s</keyname><forenames>Michael</forenames></author><author><keyname>Nakao</keyname><forenames>Haruka</forenames></author><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Predicting Defect Content and Quality Assurance Effectiveness by
  Combining Expert Judgment and Defect Data - A Case Study</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=4700306</comments><journal-ref>Proceedings of the 2008 19th International Symposium on Software
  Reliability Engineering, ISSRE '08, page 17-26, Washington, DC, USA, 2008</journal-ref><doi>10.1109/ISSRE.2008.43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning quality assurance (QA) activities in a systematic way and
controlling their execution are challenging tasks for companies that develop
software or software-intensive systems. Both require estimation capabilities
regarding the effectiveness of the applied QA techniques and the defect content
of the checked artifacts. Existing approaches for these purposes need extensive
measurement data from historical projects. Due to the fact that many companies
do not collect enough data for applying these approaches (especially for the
early project lifecycle), they typically base their QA planning and controlling
solely on expert opinion. This article presents a hybrid method that combines
commonly available measurement data and context-specific expert knowledge. To
evaluate the method's applicability and usefulness, we conducted a case study
in the context of independent verification and validation activities for
critical software in the space domain. A hybrid defect content and
effectiveness model was developed for the software requirements analysis phase
and evaluated with available legacy data. One major result is that the hybrid
model provides improved estimation accuracy when compared to applicable models
based solely on data. The mean magnitude of relative error (MMRE) determined by
cross-validation is 29.6% compared to 76.5% obtained by the most accurate
data-based model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2768</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2768</id><created>2014-01-13</created><authors><author><keyname>Hanumantharaju</keyname><forenames>M. C.</forenames></author><author><keyname>Gopalakrishna</keyname><forenames>M. T.</forenames></author></authors><title>Design of novel architectures and field programmable gate arrays
  implementation of two dimensional gaussian surround function</title><categories>cs.AR</categories><comments>10 Pages, 14 figures (including authors photo), International
  Conference on Image &amp; Signal Processing (ICIP2013)</comments><journal-ref>International Journal of Information Processing, ISSN : 0973-8215,
  7(4), 66-75, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A new design and novel architecture suitable for FPGA/ASIC implementation of
a 2D Gaussian surround function for image processing application is presented
in this paper. The proposed scheme results in enormous savings of memory
normally required for 2D Gaussian function implementation. In the present work,
the Gaussian symmetric characteristics which quickly falls off toward
plus/minus infinity has been used in order to save the memory. The 2D Gaussian
function implementation is presented for use in applications such as image
enhancement, smoothing, edge detection and filtering etc. The FPGA
implementation of the proposed 2D Gaussian function is capable of processing
(blurring, smoothing, and convolution) high resolution color pictures of size
up to $1600\times1200$ pixels at the real time video rate of 30 frames/sec. The
Gaussian design exploited here has been used in the core part of retinex based
color image enhancement. Therefore, the design presented produces Gaussian
output with three different scales, namely, 16, 64 and 128. The design was
coded in Verilog, a popular hardware design language used in industries,
conforming to RTL coding guidelines and fits onto a single chip with a gate
count utilization of 89,213 gates. Experimental results presented confirms that
the proposed method offers a new approach for development of large sized
Gaussian pyramid while reducing the on-chip memory utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2774</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2774</id><created>2014-01-13</created><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author></authors><title>Exact Optimized-cost Repair in Multi-hop Distributed Storage Networks</title><categories>cs.IT math.IT</categories><comments>(To appear in ICC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of exact repair of a failed node in multi-hop networked
distributed storage systems is considered. Contrary to the most of the current
studies which model the repair process by the direct links from surviving nodes
to the new node, the repair is modeled by considering the multi-hop network
structure, and taking into account that there might not exist direct links from
all the surviving nodes to the new node. In the repair problem of these
systems, surviving nodes may cooperate to transmit the repair traffic to the
new node. In this setting, we define the total number of packets transmitted
between nodes as repair-cost. A lower bound of the repaircost can thus be found
by cut-set bound analysis. In this paper, we show that the lower bound of the
repair-cost is achievable for the exact repair of MDS codes in tandem and grid
networks, thus resulting in the minimum-cost exact MDS codes. Further, two
suboptimal (achievable) bounds for the large scale grid networks are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2778</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2778</id><created>2014-01-13</created><updated>2014-09-10</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Alkemade</keyname><forenames>Floortje</forenames></author><author><keyname>Heimeriks</keyname><forenames>Gaston</forenames></author><author><keyname>Hoekstra</keyname><forenames>Rinke</forenames></author></authors><title>Patents as Instruments for Exploring Innovation Dynamics: Geographic and
  Technological Perspectives on &quot;Photovoltaic Cells&quot;</title><categories>cs.CY cs.DL</categories><comments>accepted for publication in Scientometrics on September 10, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of innovation are nonlinear and complex: geographical,
technological, and economic selection environments can be expected to interact.
Can patents provide an analytical lens to this process in terms of different
attributes such as inventor addresses, classification codes, backward and
forward citations, etc.? Two recently developed patent maps with interactive
overlay techniques--Google Maps and maps based on citation relations among
International Patent Classifications (IPC)--are elaborated into dynamic
versions that allow for online animations and comparisons by using split
screens. Various forms of animation are explored. The recently developed
Cooperative Patent Classifications (CPC) of the U.S. Patent and Trade Office
(USPTO) and the European Patent Office (EPO) provide new options for a precise
delineation of samples in both USPTO data and the Worldwide Patent Statistics
Database (PatStat) of EPO. Among the &quot;technologies for the mitigation of
climate change&quot; (class Y02), we zoom in on nine material technologies for
photovoltaic cells; and focus on one of them (CuInSe2) as a lead case. The
longitudinal development of Rao-Stirling diversity in the IPC-based maps
provides a heuristics for studying technological generations during the period
under study (1975-2012). The sequencing of generations prevails in USPTO data
more than in PatStat data because PatStat aggregates patent information from
countries in different stages of technological development, whereas one can
expect USPTO patents to be competitive at the technological edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2782</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2782</id><created>2014-01-13</created><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Gui</keyname><forenames>Ning</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Participant: A New Concept for Optimally Assisting the Elder People</title><categories>cs.CY</categories><comments>Twentieth IEEE International Symposium on Computer-Based Medical
  Systems, 2007. CBMS '07</comments><doi>10.1109/CBMS.2007.82</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elder people are becoming a predominant aspect of our societies. As such,
solutions both efficacious and cost-effective need to be sought. The approach
pursued so far to solve this problem used to increase the number of people
working in the health sector, e.g. doctors, nurses, etc. This increases the
costs, which is becoming a big burden for countries. In this paper we propose a
new concept in the health management of elder people, which we name as
&quot;participant&quot;. We propose the &quot;participant&quot; concept to encourage elder people
to participate in those group activities that they are able to. Their roles in
these activities are not passively requesting help, but actively participating
to some healthcare processes. Characteristics of the participant approach are
that medical resources are efficiently spared with this model, and the social
network of the elder people is kept. A &quot;virtual community&quot; for mutual
assistance is set up in this paper, and the simulations demonstrate that the
&quot;participant&quot; model could fully utilize the community resources. Furthermore,
the psychological health of the elder people will be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2794</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2794</id><created>2014-01-13</created><authors><author><keyname>D&#xfc;ck</keyname><forenames>N.</forenames></author><author><keyname>Zimmermann</keyname><forenames>K. -H.</forenames></author></authors><title>On Binomial Ideals associated to Linear Codes</title><categories>math.AC cs.IT math.IT</categories><msc-class>94B05 (Primary), 13P10, 94B35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was shown that a binary linear code can be associated to a
binomial ideal given as the sum of a toric ideal and a non-prime ideal. Since
then two different generalizations have been provided which coincide for the
binary case. In this paper, we establish some connections between the two
approaches. In particular, we show that the corresponding code ideals are
related by elimination. Finally, a new heuristic decoding method for linear
codes over prime fields is discussed using Gr\&quot;obner bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2804</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2804</id><created>2014-01-13</created><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Ranftl</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>Insights into analysis operator learning: From patch-based sparse models
  to higher-order MRFs</title><categories>cs.CV</categories><comments>13 pages, 10 figures, accepted to IEEE Image Processing</comments><doi>10.1109/TIP.2014.2299065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a new learning algorithm for the recently introduced
co-sparse analysis model. First, we give new insights into the co-sparse
analysis model by establishing connections to filter-based MRF models, such as
the Field of Experts (FoE) model of Roth and Black. For training, we introduce
a technique called bi-level optimization to learn the analysis operators.
Compared to existing analysis operator learning approaches, our training
procedure has the advantage that it is unconstrained with respect to the
analysis operator. We investigate the effect of different aspects of the
co-sparse analysis model and show that the sparsity promoting function (also
called penalty function) is the most important factor in the model. In order to
demonstrate the effectiveness of our training approach, we apply our trained
models to various classical image restoration problems. Numerical experiments
show that our trained models clearly outperform existing analysis operator
learning approaches and are on par with state-of-the-art image denoising
algorithms. Our approach develops a framework that is intuitive to understand
and easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2815</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2815</id><created>2014-01-13</created><updated>2014-05-19</updated><authors><author><keyname>Sun</keyname><forenames>Lijun</forenames></author><author><keyname>Axhausen</keyname><forenames>Kay W.</forenames></author><author><keyname>Lee</keyname><forenames>Der-Horng</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Efficient detection of contagious outbreaks in massive metropolitan
  encounter networks</title><categories>physics.soc-ph cs.SI</categories><comments>4 figures</comments><journal-ref>Sci. Rep. 4, 5099 (2014)</journal-ref><doi>10.1038/srep05099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical contact remains difficult to trace in large metropolitan networks,
though it is a key vehicle for the transmission of contagious outbreaks.
Co-presence encounters during daily transit use provide us with a city-scale
time-resolved physical contact network, consisting of 1 billion contacts among
3 million transit users. Here, we study the advantage that knowledge of such
co-presence structures may provide for early detection of contagious outbreaks.
We first examine the &quot;friend sensor&quot; scheme --- a simple, but universal
strategy requiring only local information --- and demonstrate that it provides
significant early detection of simulated outbreaks. Taking advantage of the
full network structure, we then identify advanced &quot;global sensor sets&quot;,
obtaining substantial early warning times savings over the friends sensor
scheme. Individuals with highest number of encounters are the most efficient
sensors, with performance comparable to individuals with the highest travel
frequency, exploratory behavior and structural centrality. An efficiency
balance emerges when testing the dependency on sensor size and evaluating
sensor reliability; we find that substantial and reliable lead-time could be
attained by monitoring only 0.01% of the population with the highest degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2818</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2818</id><created>2014-01-13</created><updated>2014-07-01</updated><authors><author><keyname>Brunton</keyname><forenames>Alan</forenames></author><author><keyname>Bolkart</keyname><forenames>Timo</forenames></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author></authors><title>Multilinear Wavelets: A Statistical Shape Space for Human Faces</title><categories>cs.CV cs.GR</categories><comments>10 pages, 7 figures; accepted to ECCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a statistical model for $3$D human faces in varying expression,
which decomposes the surface of the face using a wavelet transform, and learns
many localized, decorrelated multilinear models on the resulting coefficients.
Using this model we are able to reconstruct faces from noisy and occluded $3$D
face scans, and facial motion sequences. Accurate reconstruction of face shape
is important for applications such as tele-presence and gaming. The localized
and multi-scale nature of our model allows for recovery of fine-scale detail
while retaining robustness to severe noise and occlusion, and is
computationally efficient and scalable. We validate these properties
experimentally on challenging data in the form of static scans and motion
sequences. We show that in comparison to a global multilinear model, our model
better preserves fine detail and is computationally faster, while in comparison
to a localized PCA model, our model better handles variation in expression, is
faster, and allows us to fix identity parameters for a given subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2819</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2819</id><created>2014-01-13</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>A notion of graph homeomorphism</title><categories>math.GN cs.DM</categories><comments>11 figures, 33 pages</comments><msc-class>05C75, 54A99, 57M15, 55M10, 55M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a notion of graph homeomorphisms which uses the concept of
dimension and homotopy for graphs. It preserves the dimension of a subbasis,
cohomology and Euler characteristic. Connectivity and homotopy look as in
classical topology. The Brouwer-Lefshetz fixed point leads to the following
discretiszation of the Kakutani fixed point theorem: any graph homeomorphism T
with nonzero Lefschetz number has a nontrivial invariant open set which is
fixed by T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2838</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2838</id><created>2014-01-13</created><authors><author><keyname>Meeds</keyname><forenames>Edward</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation</title><categories>cs.LG q-bio.QM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists often express their understanding of the world through a
computationally demanding simulation program. Analyzing the posterior
distribution of the parameters given observations (the inverse problem) can be
extremely challenging. The Approximate Bayesian Computation (ABC) framework is
the standard statistical tool to handle these likelihood free problems, but
they require a very large number of simulations. In this work we develop two
new ABC sampling algorithms that significantly reduce the number of simulations
necessary for posterior inference. Both algorithms use confidence estimates for
the accept probability in the Metropolis Hastings step to adaptively choose the
number of necessary simulations. Our GPS-ABC algorithm stores the information
obtained from every simulation in a Gaussian process which acts as a surrogate
function for the simulated statistics. Experiments on a challenging realistic
biological problem illustrate the potential of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2844</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2844</id><created>2014-01-13</created><authors><author><keyname>Kalinovsky</keyname><forenames>Yakiv O.</forenames></author><author><keyname>Lande</keyname><forenames>Dmitry V.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yuliya E.</forenames></author><author><keyname>Khitsko</keyname><forenames>Iana V.</forenames></author></authors><title>Inifnite hypercomplex number system factorization methods</title><categories>cs.NA cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of obtaining the set of noncanonical hypercomplex number systems
by conversion of infinite hypercomplex number system to finite hypercomplex
number system depending on multiplication rules and factorization method is
described. Systems obtained by this method starting from the 3rddimension are
noncanonical. The obtained systems of even dimension can be re-factorized. As a
result of it hypercomplex number system of two times less dimension are got.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2851</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2851</id><created>2014-01-09</created><authors><author><keyname>Chowdhury</keyname><forenames>Md. Naseef-Ur-Rahman</forenames></author><author><keyname>Paul</keyname><forenames>Suvankar</forenames></author><author><keyname>Sultana</keyname><forenames>Kazi Zakia</forenames></author></authors><title>Statistical Analysis based Hypothesis Testing Method in Biological
  Knowledge Discovery</title><categories>cs.IR cs.CL</categories><comments>9 pages, published on International Journal on Computational Sciences
  &amp; Applications (IJCSA) Vol.3, No.6, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The correlation and interactions among different biological entities comprise
the biological system. Although already revealed interactions contribute to the
understanding of different existing systems, researchers face many questions
everyday regarding inter-relationships among entities. Their queries have
potential role in exploring new relations which may open up a new area of
investigation. In this paper, we introduce a text mining based method for
answering the biological queries in terms of statistical computation such that
researchers can come up with new knowledge discovery. It facilitates user to
submit their query in natural linguistic form which can be treated as
hypothesis. Our proposed approach analyzes the hypothesis and measures the
p-value of the hypothesis with respect to the existing literature. Based on the
measured value, the system either accepts or rejects the hypothesis from
statistical point of view. Moreover, even it does not find any direct
relationship among the entities of the hypothesis, it presents a network to
give an integral overview of all the entities through which the entities might
be related. This is also congenial for the researchers to widen their view and
thus think of new hypothesis for further investigation. It assists researcher
to get a quantitative evaluation of their assumptions such that they can reach
a logical conclusion and thus aids in relevant re-searches of biological
knowledge discovery. The system also provides the researchers a graphical
interactive interface to submit their hypothesis for assessment in a more
convenient way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2853</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2853</id><created>2014-01-13</created><updated>2014-01-27</updated><authors><author><keyname>Sarkar</keyname><forenames>Chayan</forenames></author><author><keyname>Rao</keyname><forenames>Vijay S.</forenames></author><author><keyname>Prasad</keyname><forenames>R. Venkatesha</forenames></author></authors><title>Sleep-Route: Routing through Sleeping Sensors</title><categories>cs.NI</categories><comments>An extended version of this article has been submitted to 12th Intl.
  Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless
  Networks (WiOpt-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose an energy-efficient data gathering scheme for
wireless sensor network called Sleep-Route, which splits the sensor nodes into
two sets - active and dormant (low-power sleep). Only the active set of sensor
nodes participate in data collection. The sensing values of the dormant sensor
nodes are predicted with the help of an active sensor node. Virtual Sensing
Framework (VSF) provides the mechanism to predict the sensing values by
exploiting the data correlation among the sensor nodes. If the number of active
sensor nodes can be minimized, a lot of energy can be saved. The active nodes'
selection must fulfill the following constraints - (i) the set of active nodes
are sufficient to predict the sensing values of the dormant nodes, (ii) each
active sensor nodes can report their data to the sink node (directly or through
some other active node(s)). The goal is to select a minimal number of active
sensor nodes so that energy savings can be maximized.
  The optimal set of active node selection raise a combinatorial optimization
problem, which we refer as Sleep-Route problem. We show that Sleep-Route
problem is NP-hard. Then, we formulate an integer linear program (ILP) to solve
the problem optimally. To solve the problem in polynomial time, we also propose
a heuristic algorithm that performs near optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2854</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2854</id><created>2014-01-13</created><updated>2014-01-17</updated><authors><author><keyname>Baelde</keyname><forenames>David</forenames></author><author><keyname>Delaune</keyname><forenames>St&#xe9;phanie</forenames></author><author><keyname>Hirschi</keyname><forenames>Lucca</forenames></author></authors><title>A reduced semantics for deciding trace equivalence using constraint
  systems</title><categories>cs.CR cs.LO</categories><comments>Accepted for publication at POST'14</comments><acm-class>C.2.2; D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many privacy-type properties of security protocols can be modelled using
trace equivalence properties in suitable process algebras. It has been shown
that such properties can be decided for interesting classes of finite processes
(i.e., without replication) by means of symbolic execution and constraint
solving. However, this does not suffice to obtain practical tools. Current
prototypes suffer from a classical combinatorial explosion problem caused by
the exploration of many interleavings in the behaviour of processes.
M\&quot;odersheim et al. have tackled this problem for reachability properties using
partial order reduction techniques. We revisit their work, generalize it and
adapt it for equivalence checking. We obtain an optimization in the form of a
reduced symbolic semantics that eliminates redundant interleavings on the fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2861</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2861</id><created>2014-01-13</created><updated>2015-10-27</updated><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Function spaces for second-order polynomial time</title><categories>cs.CC cs.LO math.LO</categories><msc-class>68Q15, 03D30, 03D65</msc-class><acm-class>F.1.3; I.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of second-order polynomial-time computability, we prove that
there is no general function space construction. We proceed to identify
restrictions on the domain or the codomain that do provide a function space
with polynomial-time function evaluation containing all polynomial-time
computable functions of that type.
  As side results we show that a polynomial-time counterpart to admissibility
of a representation is not a suitable criterion for natural representations,
and that the Weihrauch degrees embed into the polynomial-time Weihrauch
degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2866</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2866</id><created>2014-01-13</created><updated>2014-04-16</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Stefaner</keyname><forenames>Moritz</forenames></author><author><keyname>Anegon</keyname><forenames>Felix de Moya</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>What is the effect of country-specific characteristics on the research
  performance of scientific institutions? Using multi-level statistical models
  to rank and map universities and research-focused institutions worldwide</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bornmann, Stefaner, de Moya Anegon, and Mutz (in press) have introduced a web
application (www.excellencemapping.net) which is linked to both academic
ranking lists published hitherto (e.g. the Academic Ranking of World
Universities) as well as spatial visualization approaches. The web application
visualizes institutional performance within specific subject areas as ranking
lists and on custom tile-based maps. The new, substantially enhanced version of
the web application and the multilevel logistic regression on which it is based
are described in this paper. Scopus data were used which have been collected
for the SCImago Institutions Ranking. Only those universities and
research-focused institutions are considered that have published at least 500
articles, reviews and conference papers in the period 2006 to 2010 in a certain
Scopus subject area. In the enhanced version, the effect of single covariates
(such as the per capita GDP of a country in which an institution is located) on
two performance metrics (best paper rate and best journal rate) is examined and
visualized. A covariate-adjusted ranking and mapping of the institutions is
produced in which the single covariates are held constant. The results on the
performance of institutions can then be interpreted as if the institutions all
had the same value (reference point) for the covariate in question. For
example, those institutions can be identified worldwide showing a very good
performance despite a bad financial situation in the corresponding country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2871</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2871</id><created>2014-01-13</created><authors><author><keyname>Zhang</keyname><forenames>Lefei</forenames></author></authors><title>Tensor Representation and Manifold Learning Methods for Remote Sensing
  Images</title><categories>cs.CV</categories><comments>7 pages</comments><msc-class>68</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One of the main purposes of earth observation is to extract interested
information and knowledge from remote sensing (RS) images with high efficiency
and accuracy. However, with the development of RS technologies, RS system
provide images with higher spatial and temporal resolution and more spectral
channels than before, and it is inefficient and almost impossible to manually
interpret these images. Thus, it is of great interests to explore automatic and
intelligent algorithms to quickly process such massive RS data with high
accuracy. This thesis targets to develop some efficient information extraction
algorithms for RS images, by relying on the advanced technologies in machine
learning. More precisely, we adopt the manifold learning algorithms as the
mainline and unify the regularization theory, tensor-based method, sparse
learning and transfer learning into the same framework. The main contributions
of this thesis are as follows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2874</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2874</id><created>2014-01-13</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author></authors><title>Constant Factor Approximation for Capacitated k-Center with Outliers</title><categories>cs.DS</categories><comments>15 pages, 3 figures, accepted to STACS 2014</comments><msc-class>68W25, 68W05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-center problem is a classic facility location problem, where given an
edge-weighted graph $G = (V,E)$ one is to find a subset of $k$ vertices $S$,
such that each vertex in $V$ is &quot;close&quot; to some vertex in $S$. The
approximation status of this basic problem is well understood, as a simple
2-approximation algorithm is known to be tight. Consequently different
extensions were studied.
  In the capacitated version of the problem each vertex is assigned a capacity,
which is a strict upper bound on the number of clients a facility can serve,
when located at this vertex. A constant factor approximation for the
capacitated $k$-center was obtained last year by Cygan, Hajiaghayi and Khuller
[FOCS'12], which was recently improved to a 9-approximation by An, Bhaskara and
Svensson [arXiv'13].
  In a different generalization of the problem some clients (denoted as
outliers) may be disregarded. Here we are additionally given an integer $p$ and
the goal is to serve exactly $p$ clients, which the algorithm is free to
choose. In 2001 Charikar et al. [SODA'01] presented a 3-approximation for the
$k$-center problem with outliers.
  In this paper we consider a common generalization of the two extensions
previously studied separately, i.e. we work with the capacitated $k$-center
with outliers. We present the first constant factor approximation algorithm
with approximation ratio of 25 even for the case of non-uniform hard
capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2880</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2880</id><created>2014-01-13</created><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>Blanco</keyname><forenames>Victor H.</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author></authors><title>Impact of contrarians and intransigents in a kinetic model of opinion
  dynamics</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>19 pages, 8 figures, to appear in PRE</comments><journal-ref>Phys. Rev. E 89, 013310 (2014)</journal-ref><doi>10.1103/PhysRevE.89.013310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study opinion formation on a fully-connected population
participating of a public debate with two distinct choices, where the agents
may adopt three different attitudes (favorable to either one choice or to the
other, or undecided). The interactions between agents occur by pairs and are
competitive, with couplings that are either negative with probability $p$ or
positive with probability $1-p$. This bimodal probability distribution of
couplings produces a behavior similar to the one resulting from the
introduction of Galam's contrarians in the population. In addition, we consider
that a fraction $d$ of the individuals are intransigent, that is, reluctant to
change their opinions. The consequences of the presence of contrarians and
intransigents are studied by means of computer simulations. Our results suggest
that the presence of inflexible agents affects the critical behavior of the
system, causing either the shift of the critical point or the suppression of
the ordering phase transition, depending on the groups of opinions
intransigents belong to. We also discuss the relevance of the model for real
social systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2899</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2899</id><created>2014-01-08</created><authors><author><keyname>Malamou</keyname><forenames>A.</forenames></author><author><keyname>Pandis</keyname><forenames>C.</forenames></author><author><keyname>Frangos</keyname><forenames>P.</forenames></author><author><keyname>Stefaneas</keyname><forenames>P.</forenames></author><author><keyname>Karakasiliotis</keyname><forenames>A.</forenames></author><author><keyname>Kodokostas</keyname><forenames>D.</forenames></author></authors><title>Application of the Modified Fractal Signature Method for Terrain
  Classification from Synthetic Aperture Radar Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the Modified Fractal Signature method is applied to real
Synthetic Aperture Radar images provided to our research group by SET 163
Working Group on SAR radar techniques. This method uses the blanket technique
to provide useful information for SAR image classification. It is based on the
calculation of the volume of a blanket, corresponding to the image to be
classified, and then on the calculation of the corresponding Fractal Area curve
and Fractal Dimension curve of the image. The main idea concerning this
proposed technique is the fact that different terrain types encountered in SAR
images yield different values of Fractal Area curves and Fractal Dimension
curves, upon which classification of different types of terrain is possible. As
a result, a classification technique for five different terrain types, i.e.
urban, suburban, rural, mountain and sea, is presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2902</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2902</id><created>2013-11-28</created><authors><author><keyname>Sinha</keyname><forenames>Sukanta</forenames></author><author><keyname>Dattagupta</keyname><forenames>Rana</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>An Alternate Approach for Designing a Domain Specific Image Search
  Prototype Using Histogram</title><categories>cs.CV cs.IR</categories><comments>10 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Everyone knows that thousand of words are represented by a single image. As a
result image search has become a very popular mechanism for the Web searchers.
Image search means, the search results are produced by the search engine should
be a set of images along with their Web page Unified Resource Locator. Now Web
searcher can perform two types of image search, they are Text to Image and
Image to Image search. In Text to Image search, search query should be a text.
Based on the input text data system will generate a set of images along with
their Web page URL as an output. On the other hand, in Image to Image search,
search query should be an image and based on this image system will generate a
set of images along with their Web page URL as an output. According to the
current scenarios, Text to Image search mechanism always not returns perfect
result. It matches the text data and then displays the corresponding images as
an output, which is not always perfect. To resolve this problem, Web
researchers have introduced the Image to Image search mechanism. In this paper,
we have also proposed an alternate approach of Image to Image search mechanism
using Histogram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2911</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2911</id><created>2014-01-09</created><authors><author><keyname>Dara</keyname><forenames>Raju</forenames></author><author><keyname>Satyanarayana</keyname><forenames>Dr. Ch.</forenames></author><author><keyname>Govardhan</keyname><forenames>Dr. A.</forenames></author></authors><title>Front End Data Cleaning And Transformation In Standard Printed Form
  Using Neural Models</title><categories>cs.DB</categories><comments>11 pages, 13 figures, International Journal on Computational Sciences
  &amp; Applications (IJCSA) Vol.3, No.6, December 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Front end of data collection and loading into database manually may cause
potential errors in data sets and a very time consuming process. Scanning of a
data document in the form of an image and recognition of corresponding
information in that image can be considered as a possible solution of this
challenge. This paper presents an automated solution for the problem of data
cleansing and recognition of user written data to transform into standard
printed format with the help of artificial neural networks. Three different
neural models namely direct, correlation based and hierarchical have been
developed to handle this issue. In a very hostile input environment, the
solution is developed to justify the proposed logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2912</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2912</id><created>2014-01-13</created><updated>2014-01-13</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Anup</forenames></author><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Ailon</keyname><forenames>Nir</forenames></author></authors><title>A tight lower bound instance for k-means++ in constant dimension</title><categories>cs.DS</categories><comments>To appear in TAMC 2014. arXiv admin note: text overlap with
  arXiv:1306.4207</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-means++ seeding algorithm is one of the most popular algorithms that is
used for finding the initial $k$ centers when using the k-means heuristic. The
algorithm is a simple sampling procedure and can be described as follows: Pick
the first center randomly from the given points. For $i &gt; 1$, pick a point to
be the $i^{th}$ center with probability proportional to the square of the
Euclidean distance of this point to the closest previously $(i-1)$ chosen
centers.
  The k-means++ seeding algorithm is not only simple and fast but also gives an
$O(\log{k})$ approximation in expectation as shown by Arthur and Vassilvitskii.
There are datasets on which this seeding algorithm gives an approximation
factor of $\Omega(\log{k})$ in expectation. However, it is not clear from these
results if the algorithm achieves good approximation factor with reasonably
high probability (say $1/poly(k)$). Brunsch and R\&quot;{o}glin gave a dataset where
the k-means++ seeding algorithm achieves an $O(\log{k})$ approximation ratio
with probability that is exponentially small in $k$. However, this and all
other known lower-bound examples are high dimensional. So, an open problem was
to understand the behavior of the algorithm on low dimensional datasets. In
this work, we give a simple two dimensional dataset on which the seeding
algorithm achieves an $O(\log{k})$ approximation ratio with probability
exponentially small in $k$. This solves open problems posed by Mahajan et al.
and by Brunsch and R\&quot;{o}glin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2920</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2920</id><created>2014-01-13</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>Greet</forenames></author><author><keyname>Lauwereins</keyname><forenames>Rudy</forenames></author></authors><title>The EFTOS Voting Farm: A Software Tool for Fault Masking in Message
  Passing Parallel Environments</title><categories>cs.DC</categories><comments>Proc. of the 24th EUROMICRO Conf. on Engineering Systems and Software
  for the next decade, Vaesteras, Sweden, August 25-27, 1998; pp. 379-386</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a set of C functions implementing a distributed software voting
mechanism for EPX or similar message passing environments, and we place it
within the EFTOS framework (Embedded Fault-Tolerant Supercomputing, ESPRIT-IV
Project 21012) of software tools for enhancing the dependability of a user
application. The described mechanism can be used for instance to implement
restoring organs i.e., N-modular redundancy systems with N-replicated voters.
We show that, besides structural design goals like fault transparency, this
tool achieves replication transparency, a high degree of flexibility and
ease-of-use, and good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2921</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2921</id><created>2014-01-13</created><authors><author><keyname>Fradkov</keyname><forenames>Alexander L.</forenames></author><author><keyname>Shalymov</keyname><forenames>Dmitry S.</forenames></author></authors><title>Information Entropy Dynamics and Maximum Entropy Production Principle</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asymptotic convergence of probability density function (pdf) and
convergence of differential entropy are examined for the non-stationary
processes that follow the maximum entropy principle (MaxEnt) and maximum
entropy production principle (MEPP). Asymptotic convergence of pdf provides new
justification of MEPP while convergence of differential entropy is important in
asymptotic analysis of communication systems. A set of equations describing the
dynamics of pdf under mass conservation and energy conservation constraints is
derived. It is shown that for pdfs with compact carrier the limit pdf is unique
and can be obtained from Jaynes's MaxEnt principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2929</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2929</id><created>2014-01-13</created><updated>2014-08-15</updated><authors><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Interactive Proofs with Quantum Finite Automata</title><categories>quant-ph cs.CC cs.FL</categories><comments>A4, 10pt, 2 figures, 20 pages. This paper is a complete version of
  the second half part of the extended abstract appearing in the Proc. of CIAA
  2004, LNCS vol.3317, pp.225-236. A complete version of the first half had
  already appeared in JCSS, 2009. (*) Please note that there is a substantial
  text overlap with arXiv:quant-ph/0410040 as explained as above</comments><journal-ref>Theoretical Computer Science, vol. 568, pp. 1-18, 2015</journal-ref><doi>10.1016/j.tcs.2014.11.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following an early work of Dwork and Stockmeyer on interactive proof systems
whose verifiers are two-way probabilistic finite automata, the authors
initiated in 2004 a study on the computational power of quantum interactive
proof systems whose verifiers are particularly limited to quantum finite
automata. As a follow-up to the authors' early journal publication [J. Comput.
System Sci., vol.75, pp.255-269, 2009], we further investigate the quantum
nature of interactions between provers and verifiers by studying how various
restrictions on quantum interactive proof systems affect the language
recognition power of the proof systems. In particular, we examine three
intriguing restrictions that (i) provers always behave in a classical fashion,
(ii) verifiers always reveal to provers the information on next moves, and
(iii) the number of interactions between provers and verifiers is bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2937</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2937</id><created>2014-01-13</created><authors><author><keyname>Steinberger</keyname><forenames>Ralf</forenames></author></authors><title>A survey of methods to ease the development of highly multilingual text
  mining applications</title><categories>cs.CL</categories><comments>22 pages. Published online on 12 October 2011</comments><acm-class>I.2.7; H.3.1; H.3.3; H.3.6</acm-class><journal-ref>Language Resources and Evaluation, Volume 46, Issue 2, pp 155-176,
  June 2012</journal-ref><doi>10.1007/s10579-011-9165-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilingual text processing is useful because the information content found
in different languages is complementary, both regarding facts and opinions.
While Information Extraction and other text mining software can, in principle,
be developed for many languages, most text analysis tools have only been
applied to small sets of languages because the development effort per language
is large. Self-training tools obviously alleviate the problem, but even the
effort of providing training data and of manually tuning the results is usually
considerable. In this paper, we gather insights by various multilingual system
developers on how to minimise the effort of developing natural language
processing applications for many languages. We also explain the main guidelines
underlying our own effort to develop complex text mining software for tens of
languages. While these guidelines - most of all: extreme simplicity - can be
very restrictive and limiting, we believe to have shown the feasibility of the
approach through the development of the Europe Media Monitor (EMM) family of
applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex
media monitoring tools that process and analyse up to 100,000 online news
articles per day in between twenty and fifty languages. We will also touch upon
the kind of language resources that would make it easier for all to develop
highly multilingual text mining applications. We will argue that - to achieve
this - the most needed resources would be freely available, simple, parallel
and uniform multilingual dictionaries, corpora and software tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2943</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2943</id><created>2014-01-13</created><authors><author><keyname>Turchi</keyname><forenames>Marco</forenames></author><author><keyname>Atkinson</keyname><forenames>Martin</forenames></author><author><keyname>Wilcox</keyname><forenames>Alastair</forenames></author><author><keyname>Crawley</keyname><forenames>Brett</forenames></author><author><keyname>Bucci</keyname><forenames>Stefano</forenames></author><author><keyname>Steinberger</keyname><forenames>Ralf</forenames></author><author><keyname>Van der Goot</keyname><forenames>Erik</forenames></author></authors><title>ONTS: &quot;Optima&quot; News Translation System</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.3; H.3.6</acm-class><journal-ref>Proceedings of the 13th Conference of the European Chapter of the
  Association for Computational Linguistics, pages 25-30, Avignon, France,
  April 23 - 27 2012. Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a real-time machine translation system that allows users to select
a news category and to translate the related live news articles from Arabic,
Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and
Turkish into English. The Moses-based system was optimised for the news domain
and differs from other available systems in four ways: (1) News items are
automatically categorised on the source side, before translation; (2) Named
entity translation is optimised by recognising and extracting them on the
source side and by re-inserting their translation in the target language,
making use of a separate entity repository; (3) News titles are translated with
a separate translation system which is optimised for the specific style of news
titles; (4) The system was optimised for speed in order to cope with the large
volume of daily news articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2949</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2949</id><created>2014-01-10</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Exploiting generalisation symmetries in accuracy-based learning
  classifier systems: An initial study</title><categories>cs.NE cs.LG</categories><comments>6 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern learning classifier systems typically exploit a niched genetic
algorithm to facilitate rule discovery. When used for reinforcement learning,
such rules represent generalisations over the state-action-reward space. Whilst
encouraging maximal generality, the niching can potentially hinder the
formation of generalisations in the state space which are symmetrical, or very
similar, over different actions. This paper introduces the use of rules which
contain multiple actions, maintaining accuracy and reward metrics for each
action. It is shown that problem symmetries can be exploited, improving
performance, whilst not degrading performance when symmetries are reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2952</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2952</id><created>2014-01-13</created><authors><author><keyname>Ying</keyname><forenames>Dawei</forenames></author><author><keyname>Vook</keyname><forenames>Frederick W.</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy A.</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author></authors><title>Kronecker Product Correlation Model and Limited Feedback Codebook Design
  in a 3D Channel Model</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, to appear at IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 2D antenna array introduces a new level of control and additional degrees
of freedom in multiple-input-multiple-output (MIMO) systems particularly for
the so-called &quot;massive MIMO&quot; systems. To accurately assess the performance
gains of these large arrays, existing azimuth-only channel models have been
extended to handle 3D channels by modeling both the elevation and azimuth
dimensions. In this paper, we study the channel correlation matrix of a generic
ray-based 3D channel model, and our analysis and simulation results demonstrate
that the 3D correlation matrix can be well approximated by a Kronecker
production of azimuth and elevation correlations. This finding lays the
theoretical support for the usage of a product codebook for reduced complexity
feedback from the receiver to the transmitter. We also present the design of a
product codebook based on Grassmannian line packing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2955</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2955</id><created>2014-01-13</created><authors><author><keyname>Naeini</keyname><forenames>Mahdi Pakdaman</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>Binary Classifier Calibration: Bayesian Non-Parametric Approach</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of probabilistic predictions is well calibrated if the events that are
predicted to occur with probability p do in fact occur about p fraction of the
time. Well calibrated predictions are particularly important when machine
learning models are used in decision analysis. This paper presents two new
non-parametric methods for calibrating outputs of binary classification models:
a method based on the Bayes optimal selection and a method based on the
Bayesian model averaging. The advantage of these methods is that they are
independent of the algorithm used to learn a predictive model, and they can be
applied in a post-processing step, after the model is learned. This makes them
applicable to a wide variety of machine learning models and methods. These
calibration methods, as well as other methods, are tested on a variety of
datasets in terms of both discrimination and calibration performance. The
results show the methods either outperform or are comparable in performance to
the state-of-the-art calibration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2960</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2960</id><created>2014-01-13</created><updated>2014-10-19</updated><authors><author><keyname>Ames</keyname><forenames>Brendan</forenames></author><author><keyname>Beveridge</keyname><forenames>Andrew</forenames></author><author><keyname>Carlson</keyname><forenames>Rosalie</forenames></author><author><keyname>Djang</keyname><forenames>Claire</forenames></author><author><keyname>Isler</keyname><forenames>Volkan</forenames></author><author><keyname>Ragain</keyname><forenames>Stephen</forenames></author><author><keyname>Savage</keyname><forenames>Maxray</forenames></author></authors><title>A Leapfrog Strategy for Pursuit-Evasion in a Polygonal Environment</title><categories>cs.CG</categories><comments>25 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study pursuit-evasion in a polygonal environment with polygonal obstacles.
In this turn based game, an evader $e$ is chased by pursuers $p_1, p_2, ...,
p_{\ell}$. The players have full information about the environment and the
location of the other players. The pursuers are allowed to coordinate their
actions. On the pursuer turn, each $p_i$ can move to any point at distance at
most 1 from his current location. On the evader turn, he moves similarly. The
pursuers win if some pursuer becomes co-located with the evader in finite time.
The evader wins if he can evade capture forever.
  It is known that one pursuer can capture the evader in any simply-connected
polygonal environment, and that three pursuers are always sufficient in any
polygonal environment (possibly with polygonal obstacles). We contribute two
new results to this field. First, we fully characterize when an environment
with a single obstacles is one-pursuer-win or two-pursuer-win. Second, we give
sufficient (but not necessary) conditions for an environment to have a winning
strategy for two pursuers. Such environments can be swept by a \emph{leapfrog
strategy} in which the two cops alternately guard/increase the currently
controlled area. The running time of this algorithm is $O(n \cdot h \cdot
{diam}(P))$ where $n$ is the number of vertices, $h$ is the number of obstacles
and ${diam}(P)$ is the diameter of $P$.
  More concretely, for an environment with $n$ vertices, we describe an
$O(n^2)$ algorithm that (1) determines whether the obstacles are
well-separated, and if so, (2) constructs the required partition for a leapfrog
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2965</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2965</id><created>2014-01-13</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>G.</forenames></author><author><keyname>Truyens</keyname><forenames>M.</forenames></author><author><keyname>Rosseel</keyname><forenames>W.</forenames></author><author><keyname>Lauwereins</keyname><forenames>Rudy</forenames></author></authors><title>A Hypermedia Distributed Application for Monitoring and Fault-Injection
  in Embedded Fault-tolerant Parallel Programs</title><categories>cs.DC</categories><comments>Proc. of the 6th Conf. on Parallel and Distributed Processing (PDP98)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a distributed, multimedia application which is being developed in
the framework of the ESPRIT-IV Project 21012 EFTOS (Embedded Fault-Tolerant
Supercomputing). The application dynamically sets up a hierarchy of HTML pages
reflecting the current status of an EFTOS-compliant dependable application
running on a Parsytec CC system. These pages are fed to a World-Wide Web
browser playing the role of a hypermedia monitor. The adopted approach allows
the user to concentrate on the high-level aspects of his/her application so to
quickly assess the quality of its current fault-tolerance design. This view of
the system lends itself well for being coupled with a tool to interactively
inject software faults in the user application; this tool is currently under
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2973</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2973</id><created>2014-01-13</created><authors><author><keyname>Hegde</keyname><forenames>Rajneesh</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Non-Embeddable Extensions of Embedded Minors</title><categories>math.CO cs.DM</categories><comments>30 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is weakly 4-connected if it is 3-connected, has at least five
vertices, and for every pair of sets (A,B) with union V(G) and intersection of
size three such that no edge has one end in A-B and the other in B-A, one of
the induced subgraphs G[A], G[B] has at most four edges. We describe a set of
constructions that starting from a weakly 4-connected planar graph G produce a
finite list of non-planar weakly 4-connected graphs, each having a minor
isomorphic to G, such that every non-planar weakly 4-connected graph H that has
a minor isomorphic to G has a minor isomorphic to one of the graphs in the
list. Our main result is more general and applies in particular to polyhedral
embeddings in any surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.2974</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.2974</id><created>2014-01-13</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>G.</forenames></author><author><keyname>Lauwereins</keyname><forenames>Rudy</forenames></author></authors><title>Software Tool Combining Fault Masking with User-Defined Recovery
  Strategies</title><categories>cs.DC</categories><journal-ref>IEE Proc. Software, Vol. 145, No. 6 (December 1998)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the voting farm, a tool which implements a distributed software
voting mechanism for a number of parallel message passing systems. The tool,
developed in the framework of EFTOS (Embedded Fault-Tolerant Supercomputing),
can be used in stand-alone mode or in conjunction with other EFTOS fault
tolerance tools. In the former case, we describe how the mechanism can be
exploited, e.g., to implement restoring organs ($N\!$-modular redundancy
systems with $N\!$-replicated voters); in the latter case, we show how it is
possible for the user to implement in an easy and effective way a number of
different recovery strategies via a custom, high-level language. Combining such
strategies with the basic fault masking capabilities of the voting tool makes
it possible to set up complex fault-tolerant systems such as, for instance,
$N$-and-$M$-spare systems or gracefully degrading voting farms. We also report
about the impact that our tool can have on reliability, and we show how,
besides structural design goals like fault transparency, our tool achieves
replication transparency, a high degree of flexibility and ease-of-use, and
good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3013</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3013</id><created>2014-01-13</created><authors><author><keyname>Elliott</keyname><forenames>James</forenames></author><author><keyname>Hoemmen</keyname><forenames>Mark</forenames></author><author><keyname>Mueller</keyname><forenames>Frank</forenames></author></authors><title>Resilience in Numerical Methods: A Position on Fault Models and
  Methodologies</title><categories>cs.MS cs.ET math.NA</categories><comments>Position Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future extreme-scale computer systems may expose silent data corruption (SDC)
to applications, in order to save energy or increase performance. However,
resilience research struggles to come up with useful abstract programming
models for reasoning about SDC. Existing work randomly flips bits in running
applications, but this only shows average-case behavior for a low-level,
artificial hardware model. Algorithm developers need to understand worst-case
behavior with the higher-level data types they actually use, in order to make
their algorithms more resilient. Also, we know so little about how SDC may
manifest in future hardware, that it seems premature to draw conclusions about
the average case. We argue instead that numerical algorithms can benefit from a
numerical unreliability fault model, where faults manifest as unbounded
perturbations to floating-point data. Algorithms can use inexpensive &quot;sanity&quot;
checks that bound or exclude error in the results of computations. Given a
selective reliability programming model that requires reliability only when and
where needed, such checks can make algorithms reliable despite unbounded
faults. Sanity checks, and in general a healthy skepticism about the
correctness of subroutines, are wise even if hardware is perfectly reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3038</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3038</id><created>2014-01-13</created><updated>2014-03-17</updated><authors><author><keyname>Formato</keyname><forenames>Richard A.</forenames></author></authors><title>Pi Fractions for Generating Uniformly Distributed Sampling Points in
  Global Search and Optimization Algorithms</title><categories>cs.OH</categories><comments>Discussion of bidimensional correlation has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pi Fractions are used to create deterministic uniformly distributed
pseudorandom decision space sample points for a global search and optimization
algorithm. These fractions appear to be uniformly distributed on [0,1] and can
be used in any stochastic algorithm rendering it effectively deterministic
without compromising its ability to explore the decision space. Pi Fractions
are generated using the BBP Pi digit extraction algorithm. The Pi Fraction
approach is tested using genetic algorithm Pi-GASR with very good results. A Pi
Fraction data file is available upon request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3041</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3041</id><created>2014-01-13</created><authors><author><keyname>Chevalier-Boisvert</keyname><forenames>Maxime</forenames></author><author><keyname>Feeley</keyname><forenames>Marc</forenames></author></authors><title>Removing Dynamic Type Tests with Context-Driven Basic Block Versioning</title><categories>cs.PL cs.PF</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic typing is an important feature of dynamic programming languages.
Primitive operators such as those for performing arithmetic and comparisons
typically operate on a wide variety of in put value types, and as such, must
internally implement some form of dynamic type dispatch and type checking.
Removing such type tests is important for an efficient implementation.
  In this paper, we examine the effectiveness of a novel approach to reducing
the number of dynamically executed type tests called context-driven basic block
versioning. This simple technique clones and specializes basic blocks in such a
way as to allow the compiler to accumulate type information while machine code
is generated, without a separate type analysis pass. The accumulated
information allows the removal of some redundant type tests, particularly in
performance-critical paths.
  We have implemented intraprocedural context-driven basic block versioning in
a JavaScript JIT compiler. For comparison, we have also implemented a classical
flow-based type analysis operating on the same concrete types. Our results show
that basic block versioning performs better on most benchmarks and removes a
large fraction of type tests at the expense of a moderate code size increase.
We believe that this technique offers a good tradeoff between implementation
complexity and performance, and is suitable for integration in production JIT
compilers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3046</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3046</id><created>2014-01-13</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Investigating Cellular Automata Based Network Intrusion Detection System
  For Fixed Networks (NIDWCA)</title><categories>cs.NI</categories><comments>2008 International Conference on Advanced Computer Theory and
  Engineering</comments><doi>10.1109/ICACTE.2008.159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Intrusion Detection Systems (NIDS) are computer systems which monitor
a network with the aim of discerning malicious from benign activity on that
network. With the recent growth of the Internet such security limitations are
becoming more and more pressing. Most of the current network intrusion
detection systems relay on labeled training data. An Unsupervised CA based
anomaly detection technique that was trained with unlabelled data is capable of
detecting previously unseen attacks. This new approach, based on the Cellular
Automata classifier (CAC) with Genetic Algorithms (GA), is used to classify
program behavior as normal or intrusive. Parameters and evolution process for
CAC with GA are discussed in detail. This implementation considers both
temporal and spatial information of network connections in encoding the network
connection information into rules in NIDS. Preliminary experiments with KDD Cup
data set show that the CAC classifier with Genetic Algorithms can effectively
detect intrusive attacks and achieve a low false positive rate. Training a
NIDWCA (Network Intrusion Detection with Cellular Automata) classifier takes
significantly shorter time than any other conventional techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3049</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3049</id><created>2014-01-13</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>On the Secrecy Outage Capacity of Physical Layer Security in Large-Scale
  MIMO Relaying Systems with Imperfect CSI</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of physical layer security in a
large-scale multiple-input multiple-output (LS-MIMO) relaying system. The
advantage of LS-MIMO relaying systems is exploited to enhance both wireless
security and spectral efficiency. In particular, the challenging issue incurred
by short interception distance is well addressed. Under very practical
assumptions, i.e., no eavesdropper's channel state information (CSI) and
imperfect legitimate channel CSI, this paper gives a thorough investigation of
the impact of imperfect CSI in two classic relaying systems, i.e.,
amplify-and-forward (AF) and decode-and-forward (DF) systems, and obtain
explicit expressions of secrecy outage capacities for both cases. Finally, our
theoretical claims are validated by the numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3056</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3056</id><created>2014-01-13</created><authors><author><keyname>Pan</keyname><forenames>Yujian</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Power of individuals -- Controlling centrality of temporal networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal networks are such networks where nodes and interactions may appear
and disappear at various time scales. With the evidence of ubiquity of temporal
networks in our economy, nature and society, it's urgent and significant to
focus on structural controllability of temporal networks, which nowadays is
still an untouched topic. We develop graphic tools to study the structural
controllability of temporal networks, identifying the intrinsic mechanism of
the ability of individuals in controlling a dynamic and large-scale temporal
network. Classifying temporal trees of a temporal network into different types,
we give (both upper and lower) analytical bounds of the controlling centrality,
which are verified by numerical simulations of both artificial and empirical
temporal networks. We find that the scale-free distribution of node's
controlling centrality is virtually independent of the time scale and types of
datasets, meaning the inherent heterogeneity and robustness of the controlling
centrality of temporal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3063</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3063</id><created>2014-01-13</created><updated>2014-07-23</updated><authors><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Lutz</keyname><forenames>Neil</forenames></author></authors><title>Lines Missing Every Random Point</title><categories>cs.CC</categories><comments>Added a section: &quot;Betting in Doubly Exponential Time.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there is, in every direction in Euclidean space, a line that
misses every computably random point. We also prove that there exist, in every
direction in Euclidean space, arbitrarily long line segments missing every
double exponential time random point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3069</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3069</id><created>2014-01-14</created><updated>2014-01-15</updated><authors><author><keyname>Satapathy</keyname><forenames>Shashank Mouli</forenames></author><author><keyname>Rath</keyname><forenames>Santanu Kumar</forenames></author></authors><title>Use Case Point Approach Based Software Effort Estimation using Various
  Support Vector Regression Kernel Methods</title><categories>cs.SE cs.LG</categories><comments>13 pages, 6 figures, 11 Tables, International Journal of Information
  Processing (IJIP)</comments><journal-ref>International Journal of Information Processing,7(4),2013,87-101</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The job of software effort estimation is a critical one in the early stages
of the software development life cycle when the details of requirements are
usually not clearly identified. Various optimization techniques help in
improving the accuracy of effort estimation. The Support Vector Regression
(SVR) is one of several different soft-computing techniques that help in
getting optimal estimated values. The idea of SVR is based upon the computation
of a linear regression function in a high dimensional feature space where the
input data are mapped via a nonlinear function. Further, the SVR kernel methods
can be applied in transforming the input data and then based on these
transformations, an optimal boundary between the possible outputs can be
obtained. The main objective of the research work carried out in this paper is
to estimate the software effort using use case point approach. The use case
point approach relies on the use case diagram to estimate the size and effort
of software projects. Then, an attempt has been made to optimize the results
obtained from use case point analysis using various SVR kernel methods to
achieve better prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3071</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3071</id><created>2014-01-14</created><updated>2014-10-05</updated><authors><author><keyname>Yang</keyname><forenames>Ang</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author><author><keyname>Zhu</keyname><forenames>Dalin</forenames></author><author><keyname>Lei</keyname><forenames>Ming</forenames></author></authors><title>A Framework of Performance Analysis for Distributed Antenna Systems
  Based on Random Matrix Theory</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors due to an error in
  Appendix I</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Future communications systems will definitely be built on green
infrastructures. To realize such a goal, recently a new network infrastructure
named cloud radio access network (C-RAN) is proposed by China Mobile to enhance
network coverage and save energy simultaneously. In C-RANs, to order to save
more energy the radio front ends are separated from the colocated baseband
units and distributively located in physical positions. C-RAN can be recognized
as a variant of distributed antenna systems (DASs). In this paper we analyze
the performance of C-RANS using random matrix theory. Due to the fact that the
antennas are distributed geographically instead of being installed nearby, the
variances of the entries in the considered channel matrix are different from
each other. To the best of the authors' knowledge, the work on random matrices
with elements having different variances is largely open, which is of great
importance for DASs. In our work, some fundamental results on the eigenvalue
distributions of the random matrices with different variances are derived
first. Then based on these fundamental conclusions the outage probability of
the considered DAS is derived. Finally, the accuracy of our analytical results
is assessed by some numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3075</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3075</id><created>2014-01-14</created><updated>2015-02-13</updated><authors><author><keyname>Qifu</keyname><affiliation>Tyler</affiliation></author><author><keyname>Sun</keyname></author><author><keyname>Yin</keyname><forenames>Xunrui</forenames></author><author><keyname>Li</keyname><forenames>Zongpeng</forenames></author><author><keyname>Long</keyname><forenames>Keping</forenames></author></authors><title>Multicast Network Coding and Field Sizes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an acyclic multicast network, it is well known that a linear network
coding solution over GF($q$) exists when $q$ is sufficiently large. In
particular, for each prime power $q$ no smaller than the number of receivers, a
linear solution over GF($q$) can be efficiently constructed. In this work, we
reveal that a linear solution over a given finite field does \emph{not}
necessarily imply the existence of a linear solution over all larger finite
fields. Specifically, we prove by construction that: (i) For every source
dimension no smaller than 3, there is a multicast network linearly solvable
over GF(7) but not over GF(8), and another multicast network linearly solvable
over GF(16) but not over GF(17); (ii) There is a multicast network linearly
solvable over GF(5) but not over such GF($q$) that $q &gt; 5$ is a Mersenne prime
plus 1, which can be extremely large; (iii) A multicast network linearly
solvable over GF($q^{m_1}$) and over GF($q^{m_2}$) is \emph{not} necessarily
linearly solvable over GF($q^{m_1+m_2}$); (iv) There exists a class of
multicast networks with a set $T$ of receivers such that the minimum field size
$q_{min}$ for a linear solution over GF($q_{min}$) is lower bounded by
$\Theta(\sqrt{|T|})$, but not every larger field than GF($q_{min}$) suffices to
yield a linear solution. The insight brought from this work is that not only
the field size, but also the order of subgroups in the multiplicative group of
a finite field affects the linear solvability of a multicast network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3088</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3088</id><created>2014-01-14</created><authors><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author></authors><title>Secret Message Transmission by HARQ with Multiple Encoding</title><categories>cs.CR cs.IT math.IT</categories><comments>Proc. Int. Conference on Communications (ICC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure transmission between two agents, Alice and Bob, over block fading
channels can be achieved similarly to conventional hybrid automatic repeat
request (HARQ) by letting Alice transmit multiple blocks, each containing an
encoded version of the secret message, until Bob informs Alice about successful
decoding by a public error-free return channel. In existing literature each
block is a differently punctured version of a single codeword generated with a
Wyner code that uses a common randomness for all blocks. In this paper instead
we propose a more general approach where multiple codewords are generated from
independent randomnesses. The class of channels for which decodability and
secrecy is ensured is characterized, with derivations for the existence of
secret codes. We show in particular that the classes are not a trivial subset
(or superset) of those of existing schemes, thus highlighting the novelty of
the proposed solution. The result is further confirmed by deriving the average
achievable secrecy throughput, thus taking into account both decoding and
secrecy outage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3090</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3090</id><created>2014-01-14</created><authors><author><keyname>Guo</keyname><forenames>Bin</forenames></author><author><keyname>Yu</keyname><forenames>Zhiwen</forenames></author><author><keyname>Zhang</keyname><forenames>Daqing</forenames></author><author><keyname>Zhou</keyname><forenames>Xingshe</forenames></author></authors><title>From Participatory Sensing to Mobile Crowd Sensing</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research on the efforts of combining human and machine intelligence has a
long history. With the development of mobile sensing and mobile Internet
techniques, a new sensing paradigm called Mobile Crowd Sensing (MCS), which
leverages the power of citizens for large-scale sensing has become popular in
recent years. As an evolution of participatory sensing, MCS has two unique
features: (1) it involves both implicit and explicit participation; (2) MCS
collects data from two user-participant data sources: mobile social networks
and mobile sensing. This paper presents the literary history of MCS and its
unique issues. A reference framework for MCS systems is also proposed. We
further clarify the potential fusion of human and machine intelligence in MCS.
Finally, we discuss the future research trends as well as our efforts to MCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3093</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3093</id><created>2014-01-14</created><authors><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Rate-Distortion for Ranking with Incomplete Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the rate-distortion relationship in the set of permutations endowed
with the Kendall Tau metric and the Chebyshev metric. Our study is motivated by
the application of permutation rate-distortion to the average-case and
worst-case analysis of algorithms for ranking with incomplete information and
approximate sorting algorithms. For the Kendall Tau metric we provide bounds
for small, medium, and large distortion regimes, while for the Chebyshev metric
we present bounds that are valid for all distortions and are especially
accurate for small distortions. In addition, for the Chebyshev metric, we
provide a construction for covering codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3098</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3098</id><created>2014-01-14</created><updated>2014-03-16</updated><authors><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author></authors><title>Interference Alignment (IA) and Coordinated Multi-Point (CoMP) overheads
  and RF impairments: testbed results</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the network MIMO techniques of interference
alignment (IA) and fully adaptive joint transmission coordinated multipoint
(CoMP) in an indoor very small cell environment. Our focus is on the overheads
in a system with quantized channel state feedback from the receiver to the
transmitter (based on the 802.11ac standard) and on the impact of non-ideal
hardware. The indoor office scenario should be the most favourable case in
terms of the required feedback rates due to the large coherence bandwidth and
coherence time of the channel. The evaluations are done using a real-world
wireless testbed with three BSs and three MSs all having two antennas. The
signal to noise ratio in the measurements is very high, 35-60dB, due to the
short transmission range. Under such conditions radio hardware impairments
becomes a major limitation on the performance. We quantify the impact of these
impairments. For a 23ms update interval the overhead is 2.5% and IA and CoMP
improves the sum throughput 27% and 47% in average (over the reference schemes
e.g. TDMA MIMO), under stationary conditions. When two people are walking in
the measurement area the throughput improvements drops to 16% and 45%,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3112</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3112</id><created>2014-01-14</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames><affiliation>IETR</affiliation></author><author><keyname>Crussi&#xe8;re</keyname><forenames>Matthieu</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames><affiliation>IETR</affiliation></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>IETR</affiliation></author></authors><title>Achieving Low-Complexity Maximum-Likelihood Detection for the 3D MIMO
  Code</title><categories>cs.IT cs.NI math.IT</categories><proxy>ccsd</proxy><journal-ref>EURASIP Journal on Wireless Communications and Networking (EURASIP
  JWCN) (2014) 1-27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D MIMO code is a robust and efficient space-time block code (STBC) for
the distributed MIMO broadcasting but suffers from high maximum-likelihood (ML)
decoding complexity. In this paper, we first analyze some properties of the 3D
MIMO code to show that the 3D MIMO code is fast-decodable. It is proved that
the ML decoding performance can be achieved with a complexity of O(M^{4.5})
instead of O(M^8) in quasi static channel with M-ary square QAM modulations.
Consequently, we propose a simplified ML decoder exploiting the unique
properties of 3D MIMO code. Simulation results show that the proposed
simplified ML decoder can achieve much lower processing time latency compared
to the classical sphere decoder with Schnorr-Euchner enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3126</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3126</id><created>2014-01-14</created><authors><author><keyname>Zignani</keyname><forenames>Matteo</forenames></author><author><keyname>Quadri</keyname><forenames>Christian</forenames></author><author><keyname>Gaitto</keyname><forenames>Sabrina</forenames></author><author><keyname>Rossi</keyname><forenames>Gian Paolo</forenames></author></authors><title>Exploiting all phone media? A multidimensional network analysis of phone
  users' sociality</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing awareness that human communications and social interactions are
assuming a stratified structure, due to the availability of multiple
techno-communication channels, including online social networks, mobile phone
calls, short messages (SMS) and e-mails, has recently led to the study of
multidimensional networks, as a step further the classical Social Network
Analysis. A few papers have been dedicated to develop the theoretical framework
to deal with such multiplex networks and to analyze some example of
multidimensional social networks. In this context we perform the first study of
the multiplex mobile social network, gathered from the records of both call and
text message activities of millions of users of a large mobile phone operator
over a period of 12 weeks. While social networks constructed from mobile phone
datasets have drawn great attention in recent years, so far studies have dealt
with text message and call data, separately, providing a very partial view of
people sociality expressed on phone. Here we analyze how the call and the text
message dimensions overlap showing how many information about links and nodes
could be lost only accounting for a single layer and how users adopt different
media channels to interact with their neighborhood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3127</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3127</id><created>2014-01-14</created><updated>2014-09-03</updated><authors><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>From Polar to Reed-Muller Codes: a Technique to Improve the
  Finite-Length Performance</title><categories>cs.IT math.IT</categories><comments>8 pages, 7 figures, in IEEE Transactions on Communications, 2014 and
  in ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the relationship between polar and RM codes and we describe a
coding scheme which improves upon the performance of the standard polar code at
practical block lengths. Our starting point is the experimental observation
that RM codes have a smaller error probability than polar codes under MAP
decoding. This motivates us to introduce a family of codes that &quot;interpolates&quot;
between RM and polar codes, call this family ${\mathcal C}_{\rm inter} =
\{C_{\alpha} : \alpha \in [0, 1]\}$, where $C_{\alpha} \big |_{\alpha = 1}$ is
the original polar code, and $C_{\alpha} \big |_{\alpha = 0}$ is an RM code.
Based on numerical observations, we remark that the error probability under MAP
decoding is an increasing function of $\alpha$. MAP decoding has in general
exponential complexity, but empirically the performance of polar codes at
finite block lengths is boosted by moving along the family ${\mathcal C}_{\rm
inter}$ even under low-complexity decoding schemes such as, for instance,
belief propagation or successive cancellation list decoder. We demonstrate the
performance gain via numerical simulations for transmission over the erasure
channel as well as the Gaussian channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3129</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3129</id><created>2014-01-14</created><authors><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Syrj&#xe4;l&#xe4;</keyname><forenames>Ville</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Cancellation of Power Amplifier Induced Nonlinear Self-Interference in
  Full-Duplex Transceivers</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in proceedings of the 2013 Asilomar Conference on Signals,
  Systems &amp; Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, full-duplex (FD) communications with simultaneous transmission and
reception on the same channel has been proposed. The FD receiver, however,
suffers from inevitable self-interference (SI) from the much more powerful
transmit signal. Analogue radio-frequency (RF) and baseband, as well as digital
baseband, cancellation techniques have been proposed for suppressing the SI,
but so far most of the studies have failed to take into account the inherent
nonlinearities of the transmitter and receiver front-ends. To fill this gap,
this article proposes a novel digital nonlinear interference cancellation
technique to mitigate the power amplifier (PA) induced nonlinear SI in a FD
transceiver. The technique is based on modeling the nonlinear SI channel, which
is comprised of the nonlinear PA, the linear multipath SI channel, and the RF
SI canceller, with a parallel Hammerstein nonlinearity. Stemming from the
modeling, and appropriate parameter estimation, the known transmit data is then
processed with the developed nonlinear parallel Hammerstein structure and
suppressed from the receiver path at digital baseband. The results illustrate
that with a given IIP3 figure for the PA, the proposed technique enables higher
transmit power to be used compared to existing linear SI cancellation methods.
Alternatively, for a given maximum transmit power level, a lower-quality PA
(i.e., lower IIP3) can be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3145</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3145</id><created>2014-01-14</created><updated>2015-08-09</updated><authors><author><keyname>Nasini</keyname><forenames>Stefano</forenames></author><author><keyname>Castro</keyname><forenames>Jordi</forenames></author><author><keyname>Casas</keyname><forenames>Pau Fonseca i</forenames></author></authors><title>Bartering integer commodities with exogenous prices</title><categories>q-fin.GN cs.GT math.OC</categories><comments>30 pages, 5 sections, 10 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of markets with indivisible goods and fixed exogenous prices has
played an important role in economic models, especially in relation to wage
rigidity and unemployment. This research report provides a mathematical and
computational details associated to the mathematical programming based
approaches proposed by Nasini et al. (accepted 2014) to study pure exchange
economies where discrete amounts of commodities are exchanged at fixed prices.
Barter processes, consisting in sequences of elementary reallocations of couple
of commodities among couples of agents, are formalized as local searches
converging to equilibrium allocations. A direct application of the analyzed
processes in the context of computational economics is provided, along with a
Java implementation of the approaches described in this research report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3146</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3146</id><created>2014-01-14</created><authors><author><keyname>Bertschinger</keyname><forenames>Nils</forenames></author><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author></authors><title>The Blackwell relation defines no lattice</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages, 1 figure</comments><msc-class>62B15, 62C05, 94A40</msc-class><doi>10.1109/ISIT.2014.6875280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blackwell's theorem shows the equivalence of two preorders on the set of
information channels. Here, we restate, and slightly generalize, his result in
terms of random variables. Furthermore, we prove that the corresponding partial
order is not a lattice; that is, least upper bounds and greatest lower bounds
do not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3148</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3148</id><created>2014-01-14</created><authors><author><keyname>Xu</keyname><forenames>S.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Dynamic Topology Adaptation and Distributed Estimation for Smart Grids</title><categories>cs.IT cs.LG math.IT</categories><comments>4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents new dynamic topology adaptation strategies for
distributed estimation in smart grids systems. We propose a dynamic exhaustive
search--based topology adaptation algorithm and a dynamic sparsity--inspired
topology adaptation algorithm, which can exploit the topology of smart grids
with poor--quality links and obtain performance gains. We incorporate an
optimized combining rule, named Hastings rule into our proposed dynamic
topology adaptation algorithms. Compared with the existing works in the
literature on distributed estimation, the proposed algorithms have a better
convergence rate and significantly improve the system performance. The
performance of the proposed algorithms is compared with that of existing
algorithms in the IEEE 14--bus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3168</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3168</id><created>2014-01-14</created><updated>2014-05-20</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Design of Relay--Assisted Primary--Secondary Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of $N$ cognitive relays to assist primary and secondary transmissions
in a time-slotted cognitive setting with one primary user (PU) and one
secondary user (SU) is investigated. An overlapped spectrum sensing strategy is
proposed for channel sensing, where the SU senses the channel for $\tau$
seconds from the beginning of the time slot and the cognitive relays sense the
channel for $2 \tau$ seconds from the beginning of the time slot, thus
providing the SU with an intrinsic priority over the relays. The relays sense
the channel over the interval $[0,\tau]$ to detect primary activity and over
the interval $[\tau,2\tau]$ to detect secondary activity. The relays help both
the PU and SU to deliver their undelivered packets and transmit when both are
idle. Two optimization-based formulations with quality of service constraints
involving queueing delay are studied. Both cases of perfect and imperfect
spectrum sensing are investigated. These results show the benefits of relaying
and its ability to enhance both primary and secondary performance, especially
in the case of no direct link between the PU and the SU transmitters and their
respective receivers. Three packet decoding strategies at the relays are also
investigated and their performance is compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3172</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3172</id><created>2014-01-14</created><authors><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Ji</keyname><forenames>Pengli</forenames></author><author><keyname>Li</keyname><forenames>Chumin</forenames></author></authors><title>An iterative merging placement algorithm for the fixed-outline
  floorplanning</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of rectangular modules with fixed area and variable dimensions,
and a fixed rectangular circuit. The placement of Fixed-Outline Floorplanning
with Soft Modules (FOFSM) aims to determine the dimensions and position of each
module on the circuit. We present a two-stage Iterative Merging Placement (IMP)
algorithm for the FOFSM with zero deadspace constraint. The first stage
iteratively merges two modules with the least area into a composite module to
achieve a final composite module, and builds up a slicing tree in a bottom-up
hierarchy. The second stage recursively determines the relative relationship
(left-right or top-bottom) of the sibling modules in the slicing tree in a
top-down hierarchy, and the dimensions and position of each leaf module are
determined automatically. Compared with zero-dead-space (ZDS) algorithm, the
only algorithm guarantees a feasible layout under some condition, we prove that
the proposed IMP could construct a feasible layout under a more relaxed
condition. Besides, IMP is more scalable in handling FOFSM considering the
wirelength or without the zero deadspace constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3173</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3173</id><created>2014-01-14</created><authors><author><keyname>Noureddine</keyname><forenames>Mohamad</forenames></author><author><keyname>Zaraket</keyname><forenames>Fadi A.</forenames></author><author><keyname>Elzein</keyname><forenames>Ali S.</forenames></author></authors><title>Synthesis of Sequential Extended Regular Expressions for Verification</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis techniques take realizable Linear Temporal Logic specifications and
produce correct cir- cuits that implement the specifications. The generated
circuits can be used directly, or as miters that check the correctness of a
logic design. Typically, those techniques generate non-deterministic finite
state automata, which can be determinized at a possibly exponential cost.
Recent results show multiple advantages of using deterministic automata in
symbolic and bounded model checking of LTL safety properties. In this paper, we
present a technique with a supporting tool that takes a sequential extended
regular expression specification {\Phi}, and a logic design implementation S,
and generates a sequential circuit C, expressed as an And-Inverted-Graph, that
checks whether S satisfies {\Phi}. The technique passes the generated circuit C
to ABC, a bounded model checker, to validate correctness. We use free input
variables to encode the non- determinism in {\Phi} and we obtain a number of
states in miter linear in the size of {\Phi}. Our technique succeeds to
generate the input to the model checker while other techniques fail because of
the exponential blowup, and in most cases, ABC succeeds to either find defects
in the design that was otherwise uncheckable, or validate the design. We
evaluated our technique against several industrial benchmarks including the IBM
arbiter, a load balancer, and a traffic light system, and compared our results
with the NuSMV framework. Our method found defects and validated systems NuSMV
could not validate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3174</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3174</id><created>2014-01-14</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author></authors><title>Comments on &quot;Optimal Utilization of a Cognitive Shared Channel with a
  Rechargeable Primary Source Node&quot;</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to JCN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper [1], the authors investigated the maximum stable throughput
region of a network composed of a rechargeable primary user and a secondary
user plugged to a reliable power supply. The authors studied the cases of an
infinite and a finite energy queue at the primary transmitter. However, the
results of the finite case are incorrect. We show that under the proposed
energy queue model (a decoupled ${\rm M/D/1}$ queueing system with Bernoulli
arrivals and the consumption of one energy packet per time slot), the energy
queue capacity does not affect the stability region of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3189</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3189</id><created>2014-01-14</created><updated>2015-02-01</updated><authors><author><keyname>Zhu</keyname><forenames>Jingge</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Asymmetric Compute-and-Forward with CSIT</title><categories>cs.IT math.IT</categories><comments>in International Zurich Seminar on Communications, 2014; minor update
  on examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a modified compute-and-forward scheme which utilizes Channel State
Information at the Transmitters (CSIT) in a natural way. The modified scheme
allows different users to have different coding rates, and use CSIT to achieve
larger rate region. This idea is applicable to all systems which use the
compute-and-forward technique and can be arbitrarily better than the regular
scheme in some settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3198</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3198</id><created>2014-01-14</created><authors><author><keyname>Guan</keyname><forenames>Peng</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Online Markov decision processes with Kullback-Leibler control cost</title><categories>math.OC cs.LG cs.SY</categories><comments>to appear in IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers an online (real-time) control problem that involves an
agent performing a discrete-time random walk over a finite state space. The
agent's action at each time step is to specify the probability distribution for
the next state given the current state. Following the set-up of Todorov, the
state-action cost at each time step is a sum of a state cost and a control cost
given by the Kullback-Leibler (KL) divergence between the agent's next-state
distribution and that determined by some fixed passive dynamics. The online
aspect of the problem is due to the fact that the state cost functions are
generated by a dynamic environment, and the agent learns the current state cost
only after selecting an action. An explicit construction of a computationally
efficient strategy with small regret (i.e., expected difference between its
actual total cost and the smallest cost attainable using noncausal knowledge of
the state costs) under mild regularity conditions is presented, along with a
demonstration of the performance of the proposed strategy on a simulated target
tracking problem. A number of new results on Markov decision processes with KL
control cost are also obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3201</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3201</id><created>2013-10-10</created><authors><author><keyname>Sun</keyname><forenames>Chongjing</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author></authors><title>Privacy Preserving Social Network Publication Against Mutual Friend
  Attacks</title><categories>cs.DB cs.CR cs.SI</categories><comments>10 pages, 11 figures, extended version of a paper in the 4th IEEE
  International Workshop on Privacy Aspects of Data Mining(PADM2013)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Publishing social network data for research purposes has raised serious
concerns for individual privacy. There exist many privacy-preserving works that
can deal with different attack models. In this paper, we introduce a novel
privacy attack model and refer it as a mutual friend attack. In this model, the
adversary can re-identify a pair of friends by using their number of mutual
friends. To address this issue, we propose a new anonymity concept, called
k-NMF anonymity, i.e., k-anonymity on the number of mutual friends, which
ensures that there exist at least k-1 other friend pairs in the graph that
share the same number of mutual friends. We devise algorithms to achieve the
k-NMF anonymity while preserving the original vertex set in the sense that we
allow the occasional addition but no deletion of vertices. Further we give an
algorithm to ensure the k-degree anonymity in addition to the k-NMF anonymity.
The experimental results on real-word datasets demonstrate that our approach
can preserve the privacy and utility of social networks effectively against
mutual friend attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3202</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3202</id><created>2014-01-14</created><authors><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tarable</keyname><forenames>Alberto</forenames></author><author><keyname>Camarda</keyname><forenames>Christian</forenames></author><author><keyname>Devassy</keyname><forenames>Rahul</forenames></author><author><keyname>Montorsi</keyname><forenames>Guido</forenames></author></authors><title>Capacity bounds for MIMO microwave backhaul links affected by phase
  noise</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present bounds and a closed-form high-SNR expression for the capacity of
multiple-antenna systems affected by Wiener phase noise. Our results are
developed for the scenario where a single oscillator drives all the
radio-frequency circuitries at each transceiver (common oscillator setup), the
input signal is subject to a peak-power constraint, and the channel matrix is
deterministic. This scenario is relevant for line-of-sight multiple-antenna
microwave backhaul links with sufficiently small antenna spacing at the
transceivers. For the 2 by 2 multiple-antenna case, for a Wiener phase-noise
process with standard deviation equal to 6 degrees, and at the medium/high SNR
values at which microwave backhaul links operate, the upper bound reported in
the paper exhibits a 3 dB gap from a lower bound obtained using 64-QAM.
Furthermore, in this SNR regime the closed-form high-SNR expression is shown to
be accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3214</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3214</id><created>2014-01-14</created><updated>2014-02-10</updated><authors><author><keyname>Skrzypczak</keyname><forenames>Micha&#x142;</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Separation Property for wB- and wS-regular Languages</title><categories>cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  12, 2014) lmcs:1224</journal-ref><doi>10.2168/LMCS-10(1:8)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that {\omega}B- and {\omega}S-regular languages satisfy
the following separation-type theorem If L1,L2 are disjoint languages of
{\omega}-words both recognised by {\omega}B- (resp. {\omega}S)-automata then
there exists an {\omega}-regular language Lsep that contains L1, and whose
complement contains L2. In particular, if a language and its complement are
recognised by {\omega}B- (resp. {\omega}S)-automata then the language is
{\omega}-regular. The result is especially interesting because, as shown by
Boja\'nczyk and Colcombet, {\omega}B-regular languages are complements of
{\omega}S-regular languages. Therefore, the above theorem shows that these are
two mutually dual classes that both have the separation property. Usually (e.g.
in descriptive set theory or recursion theory) exactly one class from a pair C,
Cc has the separation property. The proof technique reduces the separation
property for {\omega}-word languages to profinite languages using Ramsey's
theorem and topological methods. After that reduction, the analysis of the
separation property in the profinite monoid is relatively simple. The whole
construction is technically not complicated, moreover it seems to be quite
extensible. The paper uses a framework for the analysis of B- and S-regular
languages in the context of the profinite monoid that was proposed by
Toru\'nczyk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3215</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3215</id><created>2014-01-14</created><updated>2014-01-16</updated><authors><author><keyname>Fan</keyname><forenames>Jihao</forenames></author><author><keyname>Chen</keyname><forenames>Hanwu</forenames></author></authors><title>Constructions of Pure Asymmetric Quantum Alternant Codes Based on
  Subclasses of Alternant Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we construct asymmetric quantum error-correcting codes(AQCs)
based on subclasses of Alternant codes. Firstly, We propose a new subclass of
Alternant codes which can attain the classical Gilbert-Varshamov bound to
construct AQCs. It is shown that when $d_x=2$, $Z$-parts of the AQCs can attain
the classical Gilbert-Varshamov bound. Then we construct AQCs based on a famous
subclass of Alternant codes called Goppa codes. As an illustrative example, we
get three $[[55,6,19/4]],[[55,10,19/3]],[[55,15,19/2]]$ AQCs from the well
known $[55,16,19]$ binary Goppa code. At last, we get asymptotically good
binary expansions of asymmetric quantum GRS codes, which are quantum
generalizations of Retter's classical results. All the AQCs constructed in this
paper are pure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3222</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3222</id><created>2014-01-14</created><authors><author><keyname>Mantzaris</keyname><forenames>Alexander V.</forenames></author></authors><title>Uncovering nodes that spread information between communities in social
  networks</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  From many datasets gathered in online social networks, well defined community
structures have been observed. A large number of users participate in these
networks and the size of the resulting graphs poses computational challenges.
There is a particular demand in identifying the nodes responsible for
information flow between communities; for example, in temporal Twitter networks
edges between communities play a key role in propagating spikes of activity
when the connectivity between communities is sparse and few edges exist between
different clusters of nodes. The new algorithm proposed here is aimed at
revealing these key connections by measuring a node's vicinity to nodes of
another community. We look at the nodes which have edges in more than one
community and the locality of nodes around them which influence the information
received and broadcasted to them. The method relies on independent random walks
of a chosen fixed number of steps, originating from nodes with edges in more
than one community. For the large networks that we have in mind, existing
measures such as betweenness centrality are difficult to compute, even with
recent methods that approximate the large number of operations required. We
therefore design an algorithm that scales up to the demand of current big data
requirements and has the ability to harness parallel processing capabilities.
The new algorithm is illustrated on synthetic data, where results can be judged
carefully, and also on a real, large scale Twitter activity data, where new
insights can be gained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3225</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3225</id><created>2014-01-14</created><authors><author><keyname>Maier</keyname><forenames>Henning</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>Cyclic Interference Alignment and Cancellation in 3-User X-Networks with
  Minimal Backhaul</title><categories>cs.IT math.IT</categories><comments>8 pages, short version submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Cyclic Interference Alignment (IA) on the 3-user
X-network and show that it is infeasible to exactly achieve the upper bound of
$\frac{K^2}{2K-1}=\frac{9}{5}$ degrees of freedom for the lower bound of n=5
signalling dimensions and K=3 user-pairs. This infeasibility goes beyond the
problem of common eigenvectors in invariant subspaces within spatial IA.
  In order to gain non-asymptotic feasibility with minimal intervention, we
first investigate an alignment strategy that enables IA by feedforwarding a
subset of messages with minimal rate. In a second step, we replace the proposed
feedforward strategy by an analogous Cyclic Interference Alignment and
Cancellation scheme with a backhaul network on the receiver side and also by a
dual Cyclic Interference Neutralization scheme with a backhaul network on the
transmitter side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3230</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3230</id><created>2013-10-08</created><authors><author><keyname>Paramesha</keyname><forenames>K</forenames></author><author><keyname>Ravishankar</keyname><forenames>K C</forenames></author></authors><title>Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet</title><categories>cs.CL cs.IR</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 3, No.5, September 2013</journal-ref><doi>10.5121/ijfcst.2013.3504</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The task of sentiment analysis of reviews is carried out using manually built
/ automatically generated lexicon resources of their own with which terms are
matched with lexicon to compute the term count for positive and negative
polarity. On the other hand the Sentiwordnet, which is quite different from
other lexicon resources that gives scores (weights) of the positive and
negative polarity for each word. The polarity of a word namely positive,
negative and neutral have the score ranging between 0 to 1 indicates the
strength/weight of the word with that sentiment orientation. In this paper, we
show that using the Sentiwordnet, how we could enhance the performance of the
classification at both sentence and document level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3232</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3232</id><created>2014-01-14</created><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author></authors><title>Hierarchies in independence and inclusion logic with strict semantics</title><categories>math.LO cs.LO</categories><msc-class>03C80</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expressive power of fragments of inclusion and independence
logic defined by restricting the number k of universal quantifiers in formulas.
Assuming the so-called strict semantics for these logics, we relate these
fragments of inclusion and independence logic to sublogics ESO_f(k\forall) of
existential second-order logic, which in turn are known to capture the
complexity classes NTIME_{RAM}(n^k).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3235</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3235</id><created>2014-01-14</created><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author></authors><title>Hierarchies in inclusion logic with lax semantics</title><categories>math.LO cs.LO</categories><msc-class>03C80</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expressive power of fragments of inclusion logic under the
so-called lax team semantics. The fragments are defined either by restricting
the number of universal quantifiers or the arity of inclusion atoms in
formulae. In case of universal quantifiers, the corresponding hierarchy
collapses at the first level. Arity hierarchy is shown to be strict by relating
the question to the study of arity hierarchies in fixed point logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3250</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3250</id><created>2014-01-14</created><authors><author><keyname>Lei</keyname><forenames>Ming</forenames></author><author><keyname>Soleymani</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Half-Duplex Relaying for the Multiuser Channel</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on studying the half-duplex (HD) relaying in the Multiple
Access Relay Channel (MARC) and the Compound Multiple Access Channel with a
Relay (cMACr). A generalized Quantize-and-Forward (GQF) has been proposed to
establish the achievable rate regions. Such scheme is developed based on the
variation of the Quantize-and-Forward (QF) scheme and single block with two
slots coding structure. The results in this paper can also be considered as a
significant extension of the achievable rate region of Half-Duplex Relay
Channel (HDRC). Furthermore, the rate regions based on GQF scheme is extended
to the Gaussian channel case. The scheme performance is shown through some
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3258</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3258</id><created>2014-01-14</created><authors><author><keyname>Caceres</keyname><forenames>Rajmonda</forenames></author><author><keyname>Carter</keyname><forenames>Kevin</forenames></author><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author></authors><title>A Boosting Approach to Learning Graph Representations</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the right graph representation from noisy, multisource data has
garnered significant interest in recent years. A central tenet of this problem
is relational learning. Here the objective is to incorporate the partial
information each data source gives us in a way that captures the true
underlying relationships. To address this challenge, we present a general,
boosting-inspired framework for combining weak evidence of entity associations
into a robust similarity metric. We explore the extent to which different
quality measurements yield graph representations that are suitable for
community detection. We then present empirical results on both synthetic and
real datasets demonstrating the utility of this framework. Our framework leads
to suitable global graph representations from quality measurements local to
each edge. Finally, we discuss future extensions and theoretical considerations
of learning useful graph representations from weak feedback in general
application settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3269</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3269</id><created>2014-01-14</created><authors><author><keyname>Horton</keyname><forenames>Nicholas J</forenames></author><author><keyname>Baumer</keyname><forenames>Benjamin S</forenames></author><author><keyname>Wickham</keyname><forenames>Hadley</forenames></author></authors><title>Teaching precursors to data science in introductory and second courses
  in statistics</title><categories>stat.CO cs.CY stat.OT</categories><msc-class>62-07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistics students need to develop the capacity to make sense of the
staggering amount of information collected in our increasingly data-centered
world. Data science is an important part of modern statistics, but our
introductory and second statistics courses often neglect this fact. This paper
discusses ways to provide a practical foundation for students to learn to
&quot;compute with data&quot; as defined by Nolan and Temple Lang (2010), as well as
develop &quot;data habits of mind&quot; (Finzer, 2013). We describe how introductory and
second courses can integrate two key precursors to data science: the use of
reproducible analysis tools and access to large databases. By introducing
students to commonplace tools for data management, visualization, and
reproducible analysis in data science and applying these to real-world
scenarios, we prepare them to think statistically in the era of big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3274</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3274</id><created>2014-01-14</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Hidden Attacks on Power Grid: Optimal Attack Strategies and Mitigation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real time operation of the power grid and synchronism of its different
elements require accurate estimation of its state variables. Errors in state
estimation will lead to sub-optimal Optimal Power Flow (OPF) solutions and
subsequent increase in the price of electricity in the market or, potentially
overload and create line outages. This paper studies hidden data attacks on
power systems by an adversary trying to manipulate state estimators. The
adversary gains control of a few meters, and is able to introduce spurious
measurements in them. The paper presents a polynomial time algorithm using
min-cut calculations to determine the minimum number of measurements an
adversary needs to manipulate in order to perform a hidden attack. Greedy
techniques are presented to aid the system operator in identifying critical
measurements for protection to prevent such hidden data attacks. Secure PMU
placement against data attacks is also discussed and an algorithm for placing
PMUs for this purpose is developed. The performances of the proposed algorithms
are shown through simulations on IEEE test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3277</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3277</id><created>2014-01-14</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>A Novel Rate Control Algorithm for Onboard Predictive Coding of
  Multispectral and Hyperspectral Images</title><categories>cs.IT math.IT</categories><doi>10.1109/TGRS.2013.2296329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictive coding is attractive for compression onboard of spacecrafts thanks
to its low computational complexity, modest memory requirements and the ability
to accurately control quality on a pixel-by-pixel basis. Traditionally,
predictive compression focused on the lossless and near-lossless modes of
operation where the maximum error can be bounded but the rate of the compressed
image is variable. Rate control is considered a challenging problem for
predictive encoders due to the dependencies between quantization and prediction
in the feedback loop, and the lack of a signal representation that packs the
signal's energy into few coefficients. In this paper, we show that it is
possible to design a rate control scheme intended for onboard implementation.
In particular, we propose a general framework to select quantizers in each
spatial and spectral region of an image so as to achieve the desired target
rate while minimizing distortion. The rate control algorithm allows to achieve
lossy, near-lossless compression, and any in-between type of compression, e.g.,
lossy compression with a near-lossless constraint. While this framework is
independent of the specific predictor used, in order to show its performance,
in this paper we tailor it to the predictor adopted by the CCSDS-123 lossless
compression standard, obtaining an extension that allows to perform lossless,
near-lossless and lossy compression in a single package. We show that the rate
controller has excellent performance in terms of accuracy in the output rate,
rate-distortion characteristics and is extremely competitive with respect to
state-of-the-art transform coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3280</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3280</id><created>2014-01-14</created><updated>2014-01-16</updated><authors><author><keyname>Bar</keyname><forenames>Krzysztof</forenames></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames></author></authors><title>Groupoid Semantics for Thermal Computing</title><categories>cs.LO</categories><comments>We describe a groupoid model for thermodynamic computation, and a
  quantization procedure that turns encrypted communication into quantum
  teleportation. Everything is done using higher category theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A groupoid semantics is presented for systems with both logical and thermal
degrees of freedom. We apply this to a syntactic model for encryption, and
obtain an algebraic characterization of the heat produced by the encryption
function, as predicted by Landauer's principle. Our model has a linear
representation theory that reveals an underlying quantum semantics, giving for
the first time a functorial classical model for quantum teleportation and other
quantum phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3289</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3289</id><created>2014-01-14</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Nain</keyname><forenames>Sumit</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>The Complexity of Partial-observation Stochastic Parity Games With
  Finite-memory Strategies</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player partial-observation stochastic games on finite-state
graphs where player 1 has partial observation and player 2 has perfect
observation. The winning condition we study are \omega-regular conditions
specified as parity objectives. The qualitative-analysis problem given a
partial-observation stochastic game and a parity objective asks whether there
is a strategy to ensure that the objective is satisfied with probability~1
(resp. positive probability). These qualtitative-analysis problems are known to
be undecidable. However in many applications the relevant question is the
existence of finite-memory strategies, and the qualitative-analysis problems
under finite-memory strategies was recently shown to be decidable in 2EXPTIME.
We improve the complexity and show that the qualitative-analysis problems for
partial-observation stochastic parity games under finite-memory strategies are
EXPTIME-complete; and also establish optimal (exponential) memory bounds for
finite-memory strategies required for qualitative analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3301</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3301</id><created>2014-01-14</created><updated>2015-06-19</updated><authors><author><keyname>Cuvelier</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Japhet</keyname><forenames>Caroline</forenames></author><author><keyname>Scarella</keyname><forenames>Gilles</forenames></author></authors><title>An efficient way to assemble finite element matrices in vector languages</title><categories>cs.MS cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient Matlab codes in 2D and 3D have been proposed recently to assemble
finite element matrices. In this paper we present simple, compact and efficient
vectorized algorithms, which are variants of these codes, in arbitrary
dimension, without the use of any lower level language. They can be easily
implemented in many vector languages (e.g. Matlab, Octave, Python, Scilab, R,
Julia, C++ with STL,...). The principle of these techniques is general, we
present it for the assembly of several finite element matrices in arbitrary
dimension, in the P1 finite element case. We also provide an extension of the
algorithms to the case of a system of PDE's. Then we give an extension to
piecewise polynomials of higher order. We compare numerically the performance
of these algorithms in Matlab, Octave and Python, with that in FreeFEM++ and in
a compiled language such as C. Examples show that, unlike what is commonly
believed, the performance is not radically worse than that of C : in the
best/worst cases, selected vector languages are respectively 2.3/3.5 and
2.9/4.1 times slower than C in the scalar and vector cases. We also present
numerical results which illustrate the computational costs of these algorithms
compared to standard algorithms and to other recent ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3307</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3307</id><created>2014-01-14</created><authors><author><keyname>Wang</keyname><forenames>Yongge</forenames></author></authors><title>On the Design of LIL Tests for (Pseudo) Random Generators and Some
  Experimental Results</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NIST SP800-22 (2010) proposes the state of art testing suite for (pseudo)
random generators to detect deviations of a binary sequence from randomness. On
the one hand, as a counter example to NIST SP800-22 test suite, it is easy to
construct functions that are considered as GOOD pseudorandom generators by NIST
SP800-22 test suite though the output of these functions are easily
distinguishable from the uniform distribution. Thus these functions are not
pseudorandom generators by definition. On the other hand, NIST SP800-22 does
not cover some of the important laws for randomness. Two fundamental limit
theorems about random binary strings are the central limit theorem and the law
of the iterated logarithm (LIL). Several frequency related tests in NIST
SP800-22 cover the central limit theorem while no NIST SP800-22 test covers
LIL.
  This paper proposes techniques to address the above challenges that NIST
SP800-22 testing suite faces. Firstly, we propose statistical distance based
testing techniques for (pseudo) random generators to reduce the above mentioned
Type II errors in NIST SP800-22 test suite. Secondly, we propose LIL based
statistical testing techniques, calculate the probabilities, and carry out
experimental tests on widely used pseudorandom generators by generating around
30TB of pseudorandom sequences. The experimental results show that for a sample
size of 1000 sequences (2TB), the statistical distance between the generated
sequences and the uniform distribution is around 0.07 (with $0$ for
statistically indistinguishable and $1$ for completely distinguishable) and the
root-mean-square deviation is around 0.005.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3308</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3308</id><created>2014-01-14</created><authors><author><keyname>Ochem</keyname><forenames>Pascal</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author></authors><title>Homomorphisms of signed planar graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signed graphs are studied since the middle of the last century. Recently, the
notion of homomorphism of signed graphs has been introduced since this notion
captures a number of well known conjectures which can be reformulated using the
definitions of signed homomorphism.
  In this paper, we introduce and study the properties of some target graphs
for signed homomorphism. Using these properties, we obtain upper bounds on the
signed chromatic numbers of graphs with bounded acyclic chromatic number and of
signed planar graphs with given girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3309</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3309</id><created>2014-01-14</created><updated>2015-10-07</updated><authors><author><keyname>Backman</keyname><forenames>Spencer</forenames></author></authors><title>Riemann-Roch Theory for Graph Orientations</title><categories>math.CO cs.DM math.AG</categories><comments>36 pages, 9 figures, substantial revisions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new framework for investigating linear equivalence of divisors
on graphs using a generalization of Gioan's cycle-cocycle reversal system for
partial orientations. An oriented version of Dhar's burning algorithm is
introduced and employed in the study of acyclicity for partial orientations. We
then show that the Baker-Norine rank of a partially orientable divisor is one
less than the minimum number of directed paths which need to be reversed in the
generalized cycle-cocycle reversal system to produce an acyclic partial
orientation. These results are applied in providing new proofs of the
Riemann-Roch theorem for graphs as well as Luo's topological characterization
of rank-determining sets. We prove that the max-flow min-cut theorem is
equivalent to the Euler characteristic description of orientable divisors and
extend this characterization to the setting of partial orientations.
Furthermore, we demonstrate that $Pic^{g-1}(G)$ is canonically isomorphic as a
$Pic^{0}(G)$-torsor to the equivalence classes of full orientations in the
cycle-cocycle reversal system acted on my directed path reversals. Efficient
algorithms for computing break divisors and constructing partial orientations
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3322</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3322</id><created>2013-12-24</created><authors><author><keyname>Yousafzai</keyname><forenames>Jibran</forenames></author><author><keyname>Cvetkovic</keyname><forenames>Zoran</forenames></author><author><keyname>Sollich</keyname><forenames>Peter</forenames></author><author><keyname>Ager</keyname><forenames>Matthew</forenames></author></authors><title>A Subband-Based SVM Front-End for Robust ASR</title><categories>cs.CL cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a novel support vector machine (SVM) based robust
automatic speech recognition (ASR) front-end that operates on an ensemble of
the subband components of high-dimensional acoustic waveforms. The key issues
of selecting the appropriate SVM kernels for classification in frequency
subbands and the combination of individual subband classifiers using ensemble
methods are addressed. The proposed front-end is compared with state-of-the-art
ASR front-ends in terms of robustness to additive noise and linear filtering.
Experiments performed on the TIMIT phoneme classification task demonstrate the
benefits of the proposed subband based SVM front-end: it outperforms the
standard cepstral front-end in the presence of noise and linear filtering for
signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed
front-end with a conventional front-end such as MFCC yields further
improvements over the individual front ends across the full range of noise
levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3324</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3324</id><created>2014-01-14</created><authors><author><keyname>Heebl</keyname><forenames>Jason D.</forenames></author><author><keyname>Thomas</keyname><forenames>Erin M.</forenames></author><author><keyname>Penno</keyname><forenames>Robert P.</forenames></author><author><keyname>Grbic</keyname><forenames>Anthony</forenames></author></authors><title>Comprehensive Analysis and Measurement of Frequency-Tuned and
  Impedance-Tuned Wireless Non-Radiative Power Transfer Systems</title><categories>cs.OH</categories><comments>11 pages, 16 figures. This article has been accepted into IEEE
  Antennas and Propagation Magazine</comments><doi>10.1109/MAP.2014.6971924</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper theoretically and experimentally investigates frequency-tuned and
impedance-tuned wireless non-radiative power transfer (WNPT) systems.
Closed-form expressions for the efficiencies of both systems, as a function of
frequency and system (circuit) parameters, are presented. In the
frequency-tuned system, the operating frequency is adjusted to compensate for
changes in mutual inductance that occur for variations of transmitter and
receiver loop positions. Frequency-tuning is employed for a range of distances
over which the loops are strongly coupled. In contrast, the impedance-tuned
system employs varactor-based matching networks to compensate for changes in
mutual inductance and achieve a simultaneous conjugate impedance match over a
range of distances. The frequency-tuned system is simpler to implement, while
the impedance-tuned system is more complex but can achieve higher efficiencies.
Both of the experimental WNPT systems studied employ resonant shielded loops as
transmitting and receiving devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3325</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3325</id><created>2014-01-14</created><updated>2015-11-18</updated><authors><author><keyname>Roux</keyname><forenames>St&#xe9;phane Le</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Equilibria in multi-player multi-outcome infinite sequential games</title><categories>cs.LO cs.GT math.GN</categories><comments>Draft version; Section 8 of Version 1 is no longer part of Version 2,
  and will in time be made available elsewhere</comments><msc-class>91A13, 91A18, 03E15</msc-class><acm-class>F.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the existence of certain types of equilibria (Nash,
$\varepsilon$-Nash, subgame perfect, $\varepsilon$-subgame perfect,
Pareto-optimal) in multi-player multi-outcome infinite sequential games. We use
two fundamental approach: One requires strong topological restrictions on the
games, but produces very strong existence results. The other merely requires
some very basic determinacy properties to still obtain some existence results.
Both results are transfer results: Starting with the existence of some
equilibria for a small class of games, they allow us to conclude the existence
of some type of equilibria for a larger class.
  To make the abstract results more concrete, we investigate as a special case
infinite sequential games with real-valued payoff functions. Depending on the
class of payoff functions (continuous, upper semi-continuous, Borel) and
whether the game is zero-sum, we obtain varying existence results for
equilibria.
  In the context of infinite-duration games played on finite graphs we provide
similar results showing that if the two-player win/lose versions are finite
memory determined, this can be transferred to the general case under some
conditions.
  Our results hold for games with two or up to countably many players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3331</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3331</id><created>2014-01-14</created><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Sathya</forenames></author><author><keyname>Riihonen</keyname><forenames>Taneli</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Otewa</keyname><forenames>Strasdosky</forenames></author><author><keyname>Icheln</keyname><forenames>Clemens</forenames></author><author><keyname>Haneda</keyname><forenames>Katsuyuki</forenames></author><author><keyname>Tretyakov</keyname><forenames>Sergei</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author></authors><title>Advanced Self-interference Cancellation and Multiantenna Techniques for
  Full-Duplex Radios</title><categories>cs.OH</categories><comments>Presented in 47th Annual Asilomar Conference on Signals, Systems, and
  Computers, 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In an in-band full-duplex system, radios transmit and receive simultaneously
in the same frequency band at the same time, providing a radical improvement in
spectral efficiency over a half-duplex system. However, in order to design such
a system, it is necessary to mitigate the self-interference due to simultaneous
transmission and reception, which seriously limits the maximum transmit power
of the full-duplex device. Especially, large differences in power levels in the
receiver front-end sets stringent requirements for the linearity of the
transceiver electronics. We present an advanced architecture for a compact
full-duplex multiantenna transceiver combining antenna design with analog and
digital cancellation, including both linear and nonlinear signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3357</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3357</id><created>2013-12-09</created><updated>2014-03-31</updated><authors><author><keyname>Gregoire</keyname><forenames>Jean</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>de La Fortelle</keyname><forenames>Arnaud</forenames></author><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author></authors><title>Back-pressure traffic signal control with unknown routing rates</title><categories>cs.SY</categories><comments>accepted for presentation at IFAC 2014, 6 pages. arXiv admin note:
  text overlap with arXiv:1309.6484</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The control of a network of signalized intersections is considered. Previous
works proposed a feedback control belonging to the family of the so-called
back-pressure controls that ensures provably maximum stability given
pre-specified routing probabilities. However, this optimal back-pressure
controller (BP*) requires routing rates and a measure of the number of vehicles
queuing at a node for each possible routing decision. It is an idealistic
assumption for our application since vehicles (going straight, turning
left/right) are all gathered in the same lane apart from the proximity of the
intersection and cameras can only give estimations of the aggregated queue
length. In this paper, we present a back-pressure traffic signal controller
(BP) that does not require routing rates, it requires only aggregated queue
lengths estimation (without direction information) and loop detectors at the
stop line for each possible direction. A theoretical result on the Lyapunov
drift in heavy load conditions under BP control is provided and tends to
indicate that BP should have good stability properties. Simulations confirm
this and show that BP stabilizes the queuing network in a significant part of
the capacity region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3370</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3370</id><created>2013-12-19</created><authors><author><keyname>Li</keyname><forenames>J.</forenames></author><author><keyname>Peters</keyname><forenames>T. J.</forenames></author><author><keyname>Jordan</keyname><forenames>K. E.</forenames></author></authors><title>Computational Topology for Approximations of Knots</title><categories>cs.CG cs.DM math.GT math.MG</categories><msc-class>57Q37, 57M50, 57Q55, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preservation of ambient isotopic equivalence under piecewise linear (PL)
approximation for smooth knots are prominent in molecular modeling and
simulation. Sufficient conditions are given regarding: (1) Hausdorff distance,
and (2) a sum of total curvature and derivative. High degree Bezier curves are
often used as smooth representations, where computational efficiency is a
practical concern. Subdivision can produce PL approximations for a given
B\'ezier curve, fulfilling the above two conditions. The primary contributions
are: (i) a priori bounds on the number of subdivision iterations sufficient to
achieve a PL approximation that is ambient isotopic to the original Bezier
curve, and (ii) improved iteration bounds over those previously established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3372</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3372</id><created>2014-01-14</created><authors><author><keyname>Vepstas</keyname><forenames>Linas</forenames></author><author><keyname>Goertzel</keyname><forenames>Ben</forenames></author></authors><title>Learning Language from a Large (Unannotated) Corpus</title><categories>cs.CL cs.LG</categories><comments>29 pages, 5 figures, research proposal</comments><acm-class>I.2.6; I.2.0; I.2.7; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach to the fully automated, unsupervised extraction of
dependency grammars and associated syntax-to-semantic-relationship mappings
from large text corpora is described. The suggested approach builds on the
authors' prior work with the Link Grammar, RelEx and OpenCog systems, as well
as on a number of prior papers and approaches from the statistical language
learning literature. If successful, this approach would enable the mining of
all the information needed to power a natural language comprehension and
generation system, directly from a large, unannotated corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3373</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3373</id><created>2014-01-14</created><authors><author><keyname>Daoud</keyname><forenames>Ashraf Al</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Kesidis</keyname><forenames>George</forenames><affiliation>The Pennsylvania State University</affiliation></author><author><keyname>Liebeherr</keyname><forenames>J&#xf6;rg</forenames><affiliation>University of Toronto</affiliation></author></authors><title>An Iterated Game of Uncoordinated Sharing of Licensed Spectrum Using
  Zero-Determinant Strategies</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider private commons for secondary sharing of licensed spectrum bands
with no access coordination provided by the primary license holder. In such
environments, heterogeneity in demand patterns of the secondary users can lead
to constant changes in the interference levels, and thus can be a source of
volatility to the utilities of the users. In this paper, we consider secondary
users to be service providers that provide downlink services. We formulate the
spectrum sharing problem as a non-cooperative iterated game of power control
where service providers change their power levels to fix their long-term
average rates at utility-maximizing values. First, we show that in any iterated
2x2 game, the structure of the single-stage game dictates the degree of control
that a service provider can exert on the long-term outcome of the game. Then we
show that if service providers use binary actions either to access or not to
access the channel at any round of the game, then the long-term rate can be
fixed regardless of the strategy of the opponent. We identify these rates and
show that they can be achieved using mixed Markovian strategies that will be
clearly identified in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3375</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3375</id><created>2014-01-14</created><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Flexible Backhaul Design and Degrees of Freedom for Linear Interference
  Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Symposium on Information Theory (ISIT
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The considered problem is that of maximizing the degrees of freedom (DoF) in
cellular downlink, under a backhaul load constraint that limits the number of
messages that can be delivered from a centralized controller to the base
station transmitters. A linear interference channel model is considered, where
each transmitter is connected to the receiver having the same index as well as
one succeeding receiver. The backhaul load is defined as the sum of all the
messages available at all the transmitters normalized by the number of users.
When the backhaul load is constrained to an integer level B, the asymptotic per
user DoF is shown to equal (4B-1)/(4B), and it is shown that the optimal
assignment of messages to transmitters is asymmetric and satisfies a local
cooperation constraint and that the optimal coding scheme relies only on
zero-forcing transmit beamforming. Finally, an extension of the presented
coding scheme is shown to apply for more general locally connected and
two-dimensional networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3376</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3376</id><created>2014-01-14</created><updated>2015-10-19</updated><authors><author><keyname>Wu</keyname><forenames>Guohua</forenames></author></authors><title>Across neighbourhood search for numerical optimization</title><categories>cs.NE</categories><journal-ref>Guohua Wu , Across neighbourhood search for numerical
  optimization, Information Sciences (2015), doi: 10.1016/j.ins.2015.09.051</journal-ref><doi>10.1016/j.ins.2015.09.051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population-based search algorithms (PBSAs), including swarm intelligence
algorithms (SIAs) and evolutionary algorithms (EAs), are competitive
alternatives for solving complex optimization problems and they have been
widely applied to real-world optimization problems in different fields. In this
study, a novel population-based across neighbourhood search (ANS) is proposed
for numerical optimization. ANS is motivated by two straightforward assumptions
and three important issues raised in improving and designing efficient PBSAs.
In ANS, a group of individuals collaboratively search the solution space for an
optimal solution of the optimization problem considered. A collection of
superior solutions found by individuals so far is maintained and updated
dynamically. At each generation, an individual directly searches across the
neighbourhoods of multiple superior solutions with the guidance of a Gaussian
distribution. This search manner is referred to as across neighbourhood search.
The characteristics of ANS are discussed and the concept comparisons with other
PBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for
implementation and application with three parameters being required to tune.
Extensive experiments on 18 benchmark optimization functions of different types
show that ANS has well balanced exploration and exploitation capabilities and
performs competitively compared with many efficient PBSAs (Related Matlab codes
used in the experiments are available from
http://guohuawunudt.gotoip2.com/publications.html).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3381</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3381</id><created>2014-01-14</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Burgess</keyname><forenames>Mark</forenames></author></authors><title>Promises, Impositions, and other Directionals</title><categories>cs.MA</categories><comments>55 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Promises, impositions, proposals, predictions, and suggestions are
categorized as voluntary co-operational methods. The class of voluntary
co-operational methods is included in the class of so-called directionals.
Directionals are mechanisms supporting the mutual coordination of autonomous
agents.
  Notations are provided capable of expressing residual fragments of
directionals. An extensive example, involving promises about the suitability of
programs for tasks imposed on the promisee is presented. The example
illustrates the dynamics of promises and more specifically the corresponding
mechanism of trust updating and credibility updating. Trust levels and
credibility levels then determine the way certain promises and impositions are
handled.
  The ubiquity of promises and impositions is further demonstrated with two
extensive examples involving human behaviour: an artificial example about an
agent planning a purchase, and a realistic example describing technology
mediated interaction concerning the solution of pay station failure related
problems arising for an agent intending to leave the parking area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3385</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3385</id><created>2014-01-14</created><authors><author><keyname>Fabris</keyname><forenames>Antonio Elias</forenames></author><author><keyname>Batista</keyname><forenames>Val&#xe9;rio Ramos</forenames></author></authors><title>A programme to determine the exact interior of any connected digital
  picture</title><categories>cs.CG cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region filling is one of the most important and fundamental operations in
computer graphics and image processing. Many filling algorithms and their
implementations are based on the Euclidean geometry, which are then translated
into computational models moving carelessly from the continuous to the finite
discrete space of the computer. The consequences of this approach is that most
implementations fail when tested for challenging degenerate and nearly
degenerate regions. We present a correct integer-only procedure that works for
all connected digital pictures. It finds all possible interior points, which
are then displayed and stored in a locating matrix. Namely, we present a
filling and locating procedure that can be used in computer graphics and image
processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3387</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3387</id><created>2014-01-14</created><updated>2014-07-08</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Maximum Throughput of a Cooperative Energy Harvesting Cognitive Radio
  User</title><categories>cs.IT cs.NI math.IT</categories><comments>Part of this paper was accepted for publication in PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the maximum throughput of a saturated
rechargeable secondary user (SU) sharing the spectrum with a primary user (PU).
The SU harvests energy packets (tokens) from the environment with a certain
harvesting rate. All transmitters are assumed to have data buffers to store the
incoming data packets. In addition to its own traffic buffer, the SU has a
buffer for storing the admitted primary packets for relaying; and a buffer for
storing the energy tokens harvested from the environment. We propose a new
cooperative cognitive relaying protocol that allows the SU to relay a fraction
of the undelivered primary packets. We consider an interference channel model
(or a multipacket reception (MPR) channel model), where concurrent
transmissions can survive from interference with certain probability
characterized by the complement of channel outages. The proposed protocol
exploits the primary queue burstiness and receivers' MPR capability. In
addition, it efficiently expends the secondary energy tokens under the
objective of secondary throughput maximization. Our numerical results show the
benefits of cooperation, receivers' MPR capability, and secondary energy queue
arrival rate on the system performance from a network layer standpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3390</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3390</id><created>2014-01-14</created><authors><author><keyname>Naeini</keyname><forenames>Mahdi Pakdaman</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>Binary Classifier Calibration: Non-parametric approach</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate calibration of probabilistic predictive models learned is critical
for many practical prediction and decision-making tasks. There are two main
categories of methods for building calibrated classifiers. One approach is to
develop methods for learning probabilistic models that are well-calibrated, ab
initio. The other approach is to use some post-processing methods for
transforming the output of a classifier to be well calibrated, as for example
histogram binning, Platt scaling, and isotonic regression. One advantage of the
post-processing approach is that it can be applied to any existing
probabilistic classification model that was constructed using any
machine-learning method.
  In this paper, we first introduce two measures for evaluating how well a
classifier is calibrated. We prove three theorems showing that using a simple
histogram binning post-processing method, it is possible to make a classifier
be well calibrated while retaining its discrimination capability. Also, by
casting the histogram binning method as a density-based non-parametric binary
classifier, we can extend it using two simple non-parametric density estimation
methods. We demonstrate the performance of the proposed calibration methods on
synthetic and real datasets. Experimental results show that the proposed
methods either outperform or are comparable to existing calibration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3409</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3409</id><created>2014-01-14</created><updated>2014-10-22</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Zhao</keyname><forenames>Hongyu</forenames></author><author><keyname>Yu</keyname><forenames>Weichuan</forenames></author></authors><title>Low-Rank Modeling and Its Applications in Image Analysis</title><categories>cs.CV cs.LG stat.ML</categories><comments>To appear in ACM Computing Surveys</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank modeling generally refers to a class of methods that solve problems
by representing variables of interest as low-rank matrices. It has achieved
great success in various fields including computer vision, data mining, signal
processing and bioinformatics. Recently, much progress has been made in
theories, algorithms and applications of low-rank modeling, such as exact
low-rank matrix recovery via convex programming and matrix completion applied
to collaborative filtering. These advances have brought more and more
attentions to this topic. In this paper, we review the recent advance of
low-rank modeling, the state-of-the-art algorithms, and related applications in
image analysis. We first give an overview to the concept of low-rank modeling
and challenging problems in this area. Then, we summarize the models and
algorithms for low-rank matrix recovery and illustrate their advantages and
limitations with numerical experiments. Next, we introduce a few applications
of low-rank modeling in the context of image analysis. Finally, we conclude
this paper with some discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3410</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3410</id><created>2014-01-14</created><authors><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Kim</keyname><forenames>Na-Rae</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Effect of ISI Mitigation on Modulation Techniques in Communication via
  Diffusion</title><categories>cs.ET cs.IT math.IT</categories><comments>9 pages, 10 figures, 27 references, conference, two-column format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication via diffusion (CvD) is an effective and energy efficient method
for transmitting information in nanonetworks. In this work, we focus on a
diffusion-based communication system where the reception process is an
absorption via receptors. Whenever a molecule hits to the receiver it is
removed from the environment. This kind of reception process is called first
passage process and it is more complicated compared to diffusion process only.
In 3-D environments, obtaining analytical solution for hitting time
distribution for realistic cases is complicated, hence we develop an end-to-end
simulator for he diffusion-based communication system that sends consecutive
symbols.
  In CvD, each symbol is modulated and demodulated in a time slot called symbol
duration, however the long tail distribution of hitting time is the main
challenge that affects the symbol detection error. The molecules arriving in
the following slots become an interference source when detection takes place.
End-to-end simulator enables us to analyze the effect of inter symbol
interference (ISI) without making any assumptions on the ISI. We propose an ISI
cancellation technique that utilizes decision feedback for compensating the
effect of previously demodulated symbol. Three different modulation types are
considered with pulse, square, and cosine carrier waves. In case of constraints
on transmitter or receiver node it may not be possible to use pulse as a
carrier, and peak-to-average messenger molecule metric is defined for this
purpose. Results show that, the proposed ISI mitigation technique improves the
symbol detection performance and the amplitude-based modulations are improved
more than frequency-based modulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3413</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3413</id><created>2014-01-14</created><authors><author><keyname>Saluja</keyname><forenames>Avneesh</forenames></author><author><keyname>Pakdaman</keyname><forenames>Mahdi</forenames></author><author><keyname>Piao</keyname><forenames>Dongzhen</forenames></author><author><keyname>Parikh</keyname><forenames>Ankur P.</forenames></author></authors><title>Infinite Mixed Membership Matrix Factorization</title><categories>cs.LG cs.IR</categories><comments>For ICDM 2013 Workshop Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rating and recommendation systems have become a popular application area for
applying a suite of machine learning techniques. Current approaches rely
primarily on probabilistic interpretations and extensions of matrix
factorization, which factorizes a user-item ratings matrix into latent user and
item vectors. Most of these methods fail to model significant variations in
item ratings from otherwise similar users, a phenomenon known as the &quot;Napoleon
Dynamite&quot; effect. Recent efforts have addressed this problem by adding a
contextual bias term to the rating, which captures the mood under which a user
rates an item or the context in which an item is rated by a user. In this work,
we extend this model in a nonparametric sense by learning the optimal number of
moods or contexts from the data, and derive Gibbs sampling inference procedures
for our model. We evaluate our approach on the MovieLens 1M dataset, and show
significant improvements over the optimal parametric baseline, more than twice
the improvements previously encountered for this task. We also extract and
evaluate a DBLP dataset, wherein we predict the number of papers co-authored by
two authors, and present improvements over the parametric baseline on this
alternative domain as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3420</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3420</id><created>2014-01-14</created><updated>2015-04-21</updated><authors><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Democratic Representations</title><categories>cs.IT math.IT</categories><comments>Submitted to a Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimization of the $\ell_{\infty}$ (or maximum) norm subject to a constraint
that imposes consistency to an underdetermined system of linear equations finds
use in a large number of practical applications, including vector quantization,
approximate nearest neighbor search, peak-to-average power ratio (or &quot;crest
factor&quot;) reduction in communication systems, and peak force minimization in
robotics and control. This paper analyzes the fundamental properties of signal
representations obtained by solving such a convex optimization problem. We
develop bounds on the maximum magnitude of such representations using the
uncertainty principle (UP) introduced by Lyubarskii and Vershynin, and study
the efficacy of $\ell_{\infty}$-norm-based dynamic range reduction. Our
analysis shows that matrices satisfying the UP, such as randomly subsampled
Fourier or i.i.d. Gaussian matrices, enable the computation of what we call
democratic representations, whose entries all have small and similar magnitude,
as well as low dynamic range. To compute democratic representations at low
computational complexity, we present two new, efficient convex optimization
algorithms. We finally demonstrate the efficacy of democratic representations
for dynamic range reduction in a DVB-T2-based broadcast system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3421</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3421</id><created>2014-01-14</created><authors><author><keyname>Agarwal</keyname><forenames>Sruti</forenames></author><author><keyname>Saha</keyname><forenames>Sangeet</forenames></author><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Performance Evaluation of ECC in Single and Multi Processor
  Architectures on FPGA Based Embedded System</title><categories>cs.AR cs.CR</categories><comments>Published Book Title: Elsevier Science and Technology, ICCN 2013,
  Bangalore, Page(s): 140 - 147, Volume 3, 03.elsevierst.2013.3.ICCN16, ISBN
  :9789351071044, Paper
  link:-http://searchdl.org/index.php/book_series/view/917</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographic algorithms are computationally costly and the challenge is more
if we need to execute them in resource constrained embedded systems. Field
Programmable Gate Arrays (FPGAs) having programmable logic de- vices and
processing cores, have proven to be highly feasible implementation platforms
for embedded systems providing lesser design time and reconfig- urability.
Design parameters like throughput, resource utilization and power requirements
are the key issues. The popular Elliptic Curve Cryptography (ECC), which is
superior over other public-key crypto-systems like RSA in many ways, such as
providing greater security for a smaller key size, is cho- sen in this work and
the possibilities of its implementation in FPGA based embedded systems for both
single and dual processor core architectures in- volving task parallelization
have been explored. This exploration, which is first of its kind considering
the other existing works, is a needed activity for evaluating the best possible
architectural environment for ECC implementa- tion on FPGA (Virtex4 XC4VFX12,
FF668, -10) based embedded platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3426</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3426</id><created>2014-01-14</created><authors><author><keyname>Gal</keyname><forenames>Yaakov</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Networks of Influence Diagrams: A Formalism for Representing Agents'
  Beliefs and Decision-Making Processes</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  109-147, 2008</journal-ref><doi>10.1613/jair.2503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Networks of Influence Diagrams (NID), a compact, natural
and highly expressive language for reasoning about agents beliefs and
decision-making processes. NIDs are graphical structures in which agents mental
models are represented as nodes in a network; a mental model for an agent may
itself use descriptions of the mental models of other agents. NIDs are
demonstrated by examples, showing how they can be used to describe conflicting
and cyclic belief structures, and certain forms of bounded rationality. In an
opponent modeling domain, NIDs were able to outperform other computational
agents whose strategies were not known in advance. NIDs are equivalent in
representation to Bayesian games but they are more compact and structured than
this formalism. In particular, the equilibrium definition for NIDs makes an
explicit distinction between agents optimal strategies, and how they actually
behave in reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3427</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3427</id><created>2014-01-14</created><authors><author><keyname>Miclet</keyname><forenames>Laurent</forenames></author><author><keyname>Bayoudh</keyname><forenames>Sabri</forenames></author><author><keyname>Delhay</keyname><forenames>Arnaud</forenames></author></authors><title>Analogical Dissimilarity: Definition, Algorithms and Two Experiments in
  Machine Learning</title><categories>cs.LG cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  793-824, 2008</journal-ref><doi>10.1613/jair.2519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines the notion of analogical dissimilarity between four
objects, with a special focus on objects structured as sequences. Firstly, it
studies the case where the four objects have a null analogical dissimilarity,
i.e. are in analogical proportion. Secondly, when one of these objects is
unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of
defining analogical dissimilarity, which is a measure of how far four objects
are from being in analogical proportion. In particular, when objects are
sequences, it gives a definition and an algorithm based on an optimal alignment
of the four sequences. It gives also learning algorithms, i.e. methods to find
the triple of objects in a learning sample which has the least analogical
dissimilarity with a given object. Two practical experiments are described: the
first is a classification problem on benchmarks of binary and nominal data, the
second shows how the generation of sequences by solving analogical equations
enables a handwritten character recognition system to rapidly be adapted to a
new writer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3428</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3428</id><created>2014-01-14</created><authors><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Benazera</keyname><forenames>Emmanuel</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author><author><keyname>Mausam</keyname></author></authors><title>A Heuristic Search Approach to Planning with Continuous Resources in
  Stochastic Domains</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  27-59, 2009</journal-ref><doi>10.1613/jair.2529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal planning in stochastic domains with
resource constraints, where the resources are continuous and the choice of
action at each step depends on resource availability. We introduce the HAO*
algorithm, a generalization of the AO* algorithm that performs search in a
hybrid state space that is modeled using both discrete and continuous state
variables, where the continuous variables represent monotonic resources. Like
other heuristic search algorithms, HAO* leverages knowledge of the start state
and an admissible heuristic to focus computational effort on those parts of the
state space that could be reached from the start state by following an optimal
policy. We show that this approach is especially effective when resource
constraints limit how much of the state space is reachable. Experimental
results demonstrate its effectiveness in the domain that motivates our
research: automated planning for planetary exploration rovers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3429</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3429</id><created>2014-01-14</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin L.</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author></authors><title>Latent Tree Models and Approximate Inference in Bayesian Networks</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  879-900, 2008</journal-ref><doi>10.1613/jair.2530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for approximate inference in Bayesian networks
(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)
from the data offline, and when online, make inference with the LTM instead of
the original BN. Because LTMs are tree-structured, inference takes linear time.
In the meantime, they can represent complex relationship among leaf nodes and
hence the approximation accuracy is often good. Empirical evidence shows that
our method can achieve good approximation accuracy at low online computational
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3430</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3430</id><created>2014-01-14</created><authors><author><keyname>Bordeaux</keyname><forenames>Lucas</forenames></author><author><keyname>Cadoli</keyname><forenames>Marco</forenames></author><author><keyname>Mancini</keyname><forenames>Toni</forenames></author></authors><title>A Unifying Framework for Structural Properties of CSPs: Definitions,
  Complexity, Tractability</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  607-629, 2008</journal-ref><doi>10.1613/jair.2538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Literature on Constraint Satisfaction exhibits the definition of several
structural properties that can be possessed by CSPs, like (in)consistency,
substitutability or interchangeability. Current tools for constraint solving
typically detect such properties efficiently by means of incomplete yet
effective algorithms, and use them to reduce the search space and boost search.
  In this paper, we provide a unifying framework encompassing most of the
properties known so far, both in CSP and other fields literature, and shed
light on the semantical relationships among them. This gives a unified and
comprehensive view of the topic, allows new, unknown, properties to emerge, and
clarifies the computational complexity of the various detection problems.
  In particular, among the others, two new concepts, fixability and
removability emerge, that come out to be the ideal characterisations of values
that may be safely assigned or removed from a variables domain, while
preserving problem satisfiability. These two notions subsume a large number of
known properties, including inconsistency, substitutability and others.
  Because of the computational intractability of all the property-detection
problems, by following the CSP approach we then determine a number of
relaxations which provide sufficient conditions for their tractability. In
particular, we exploit forms of language restrictions and local reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3431</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3431</id><created>2014-01-14</created><authors><author><keyname>Delgrande</keyname><forenames>James</forenames></author><author><keyname>Jin</keyname><forenames>Yi</forenames></author><author><keyname>Pelletier</keyname><forenames>Francis Jeffry</forenames></author></authors><title>Compositional Belief Update</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  757-791, 2008</journal-ref><doi>10.1613/jair.2539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore a class of belief update operators, in which the
definition of the operator is compositional with respect to the sentence to be
added. The goal is to provide an update operator that is intuitive, in that its
definition is based on a recursive decomposition of the update sentences
structure, and that may be reasonably implemented. In addressing update, we
first provide a definition phrased in terms of the models of a knowledge base.
While this operator satisfies a core group of the benchmark Katsuno-Mendelzon
update postulates, not all of the postulates are satisfied. Other
Katsuno-Mendelzon postulates can be obtained by suitably restricting the
syntactic form of the sentence for update, as we show. In restricting the
syntactic form of the sentence for update, we also obtain a hierarchy of update
operators with Winsletts standard semantics as the most basic interesting
approach captured. We subsequently give an algorithm which captures this
approach; in the general case the algorithm is exponential, but with some
not-unreasonable assumptions we obtain an algorithm that is linear in the size
of the knowledge base. Hence the resulting approach has much better complexity
characteristics than other operators in some situations. We also explore other
compositional belief change operators: erasure is developed as a dual operator
to update; we show that a forget operator is definable in terms of update; and
we give a definition of the compositional revision operator. We obtain that
compositional revision, under the most natural definition, yields the Satoh
revision operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3432</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3432</id><created>2014-01-14</created><authors><author><keyname>De Laet</keyname><forenames>Tinne</forenames></author><author><keyname>De Schutter</keyname><forenames>Joris</forenames></author><author><keyname>Bruyninckx</keyname><forenames>Herman</forenames></author></authors><title>A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for
  Range Finders in Dynamic Environments</title><categories>cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  179-222, 2008</journal-ref><doi>10.1613/jair.2540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and experimentally validates a Bayesian network model of
a range finder adapted to dynamic environments. All modeling assumptions are
rigorously explained, and all model parameters have a physical interpretation.
This approach results in a transparent and intuitive model. With respect to the
state of the art beam model this paper: (i) proposes a different functional
form for the probability of range measurements caused by unmodeled objects,
(ii) intuitively explains the discontinuity encountered in te state of the art
beam model, and (iii) reduces the number of model parameters, while maintaining
the same representational power for experimental data. The proposed beam model
is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood
and a variational Bayesian estimator (both based on expectation-maximization)
are proposed to learn the model parameters.
  Furthermore, the RBBM is extended to a full scan model in two steps: first,
to a full scan model for static environments and next, to a full scan model for
general, dynamic environments. The full scan model accounts for the dependency
between beams and adapts to the local sample density when using a particle
filter. In contrast to Gaussian-based state of the art models, the proposed
full scan model uses a sample-based approximation. This sample-based
approximation enables handling dynamic environments and capturing
multi-modality, which occurs even in simple static environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3433</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3433</id><created>2014-01-14</created><authors><author><keyname>Gerding</keyname><forenames>Enrico H.</forenames></author><author><keyname>Dash</keyname><forenames>Rajdeep Kumar</forenames></author><author><keyname>Byde</keyname><forenames>Andrew</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas Robert</forenames></author></authors><title>Optimal Strategies for Simultaneous Vickrey Auctions with Perfect
  Substitutes</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  939-982, 2008</journal-ref><doi>10.1613/jair.2544</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive optimal strategies for a bidding agent that participates in
multiple, simultaneous second-price auctions with perfect substitutes. We prove
that, if everyone else bids locally in a single auction, the global bidder
should always place non-zero bids in all available auctions, provided there are
no budget constraints. With a budget, however, the optimal strategy is to bid
locally if this budget is equal or less than the valuation. Furthermore, for a
wide range of valuation distributions, we prove that the problem of finding the
optimal bids reduces to two dimensions if all auctions are identical. Finally,
we address markets with both sequential and simultaneous auctions,
non-identical auctions, and the allocative efficiency of the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3434</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3434</id><created>2014-01-14</created><authors><author><keyname>Cs&#xe1;ji</keyname><forenames>Bal&#xe1;zs Csan&#xe1;d</forenames></author><author><keyname>Monostori</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Adaptive Stochastic Resource Control: A Machine Learning Approach</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  453-486, 2008</journal-ref><doi>10.1613/jair.2548</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates stochastic resource allocation problems with scarce,
reusable resources and non-preemtive, time-dependent, interconnected tasks.
This approach is a natural generalization of several standard resource
management problems, such as scheduling and transportation problems. First,
reactive solutions are considered and defined as control policies of suitably
reformulated Markov decision processes (MDPs). We argue that this reformulation
has several favorable properties, such as it has finite state and action
spaces, it is aperiodic, hence all policies are proper and the space of control
policies can be safely restricted. Next, approximate dynamic programming (ADP)
methods, such as fitted Q-learning, are suggested for computing an efficient
control policy. In order to compactly maintain the cost-to-go function, two
representations are studied: hash tables and support vector regression (SVR),
particularly, nu-SVRs. Several additional improvements, such as the application
of limited-lookahead rollout algorithms in the initial phases, action space
decomposition, task clustering and distributed sampling are investigated, too.
Finally, experimental results on both benchmark and industry-related data are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3436</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3436</id><created>2014-01-14</created><authors><author><keyname>Ross</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author><author><keyname>Paquet</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Chaib-draa</keyname><forenames>Brahim</forenames></author></authors><title>Online Planning Algorithms for POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  663-704, 2008</journal-ref><doi>10.1613/jair.2567</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially Observable Markov Decision Processes (POMDPs) provide a rich
framework for sequential decision-making under uncertainty in stochastic
domains. However, solving a POMDP is often intractable except for small
problems due to their complexity. Here, we focus on online approaches that
alleviate the computational complexity by computing good local policies at each
decision step during the execution. Online algorithms generally consist of a
lookahead search to find the best action to execute at each time step in an
environment. Our objectives here are to survey the various existing online
POMDP methods, analyze their properties and discuss their advantages and
disadvantages; and to thoroughly evaluate these online approaches in different
environments under various metrics (return, error bound reduction, lower bound
improvement). Our experimental results indicate that state-of-the-art online
heuristic search methods can handle large POMDP domains efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3437</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3437</id><created>2014-01-14</created><authors><author><keyname>Amir</keyname><forenames>Eyal</forenames></author><author><keyname>Chang</keyname><forenames>Allen</forenames></author></authors><title>Learning Partially Observable Deterministic Action Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  349-402, 2008</journal-ref><doi>10.1613/jair.2575</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present exact algorithms for identifying deterministic-actions effects and
preconditions in dynamic partially observable domains. They apply when one does
not know the action model(the way actions affect the world) of a domain and
must learn it from partial observations over time. Such scenarios are common in
real world applications. They are challenging for AI tasks because traditional
domain structures that underly tractability (e.g., conditional independence)
fail there (e.g., world features become correlated). Our work departs from
traditional assumptions about partial observations and action models. In
particular, it focuses on problems in which actions are deterministic of simple
logical structure and observation models have all features observed with some
frequency. We yield tractable algorithms for the modified problem for such
domains.
  Our algorithms take sequences of partial observations over time as input, and
output deterministic action models that could have lead to those observations.
The algorithms output all or one of those models (depending on our choice), and
are exact in that no model is misclassified given the observations. Our
algorithms take polynomial time in the number of time steps and state features
for some traditional action classes examined in the AI-planning literature,
e.g., STRIPS actions. In contrast, traditional approaches for HMMs and
Reinforcement Learning are inexact and exponentially intractable for such
domains. Our experiments verify the theoretical tractability guarantees, and
show that we identify action models exactly. Several applications in planning,
autonomous exploration, and adventure-game playing already use these results.
They are also promising for probabilistic settings, partially observable
reinforcement learning, and diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3438</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3438</id><created>2014-01-14</created><authors><author><keyname>Moore</keyname><forenames>Neil C. A.</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>The Ultrametric Constraint and its Application to Phylogenetics</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  901-938, 2008</journal-ref><doi>10.1613/jair.2580</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A phylogenetic tree shows the evolutionary relationships among species.
Internal nodes of the tree represent speciation events and leaf nodes
correspond to species. A goal of phylogenetics is to combine such trees into
larger trees, called supertrees, whilst respecting the relationships in the
original trees. A rooted tree exhibits an ultrametric property; that is, for
any three leaves of the tree it must be that one pair has a deeper most recent
common ancestor than the other pairs, or that all three have the same most
recent common ancestor. This inspires a constraint programming encoding for
rooted trees. We present an efficient constraint that enforces the ultrametric
property over a symmetric array of constrained integer variables, with the
inevitable property that the lower bounds of any three variables are mutually
supportive. We show that this allows an efficient constraint-based solution to
the supertree construction problem. We demonstrate that the versatility of
constraint programming can be exploited to allow solutions to variants of the
supertree construction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3439</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3439</id><created>2014-01-14</created><authors><author><keyname>Chernova</keyname><forenames>Sonia</forenames></author><author><keyname>Veloso</keyname><forenames>Manuela</forenames></author></authors><title>Interactive Policy Learning through Confidence-Based Autonomy</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  1-25, 2009</journal-ref><doi>10.1613/jair.2584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Confidence-Based Autonomy (CBA), an interactive algorithm for
policy learning from demonstration. The CBA algorithm consists of two
components which take advantage of the complimentary abilities of humans and
computer agents. The first component, Confident Execution, enables the agent to
identify states in which demonstration is required, to request a demonstration
from the human teacher and to learn a policy based on the acquired data. The
algorithm selects demonstrations based on a measure of action selection
confidence, and our results show that using Confident Execution the agent
requires fewer demonstrations to learn the policy than when demonstrations are
selected by a human teacher. The second algorithmic component, Corrective
Demonstration, enables the teacher to correct any mistakes made by the agent
through additional demonstrations in order to improve the policy and future
task performance. CBA and its individual components are compared and evaluated
in a complex simulated driving domain. The complete CBA algorithm results in
the best overall learning performance, successfully reproducing the behavior of
the teacher while balancing the tradeoff between number of demonstrations and
number of incorrect actions during learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3441</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3441</id><created>2014-01-14</created><authors><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author><author><keyname>Pechyony</keyname><forenames>Dmitry</forenames></author></authors><title>Transductive Rademacher Complexity and its Applications</title><categories>cs.LG cs.AI stat.ML</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  193-234, 2009</journal-ref><doi>10.1613/jair.2587</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a technique for deriving data-dependent error bounds for
transductive learning algorithms based on transductive Rademacher complexity.
Our technique is based on a novel general error bound for transduction in terms
of transductive Rademacher complexity, together with a novel bounding technique
for Rademacher averages for particular algorithms, in terms of their
&quot;unlabeled-labeled&quot; representation. This technique is relevant to many advanced
graph-based transductive algorithms and we demonstrate its effectiveness by
deriving error bounds to three well known algorithms. Finally, we present a new
PAC-Bayesian bound for mixtures of transductive algorithms based on our
Rademacher bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3442</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3442</id><created>2014-01-14</created><authors><author><keyname>Gershman</keyname><forenames>Amir</forenames></author><author><keyname>Meisels</keyname><forenames>Amnon</forenames></author><author><keyname>Zivan</keyname><forenames>Roie</forenames></author></authors><title>Asynchronous Forward Bounding for Distributed COPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  61-88, 2009</journal-ref><doi>10.1613/jair.2591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new search algorithm for solving distributed constraint optimization
problems (DisCOPs) is presented. Agents assign variables sequentially and
compute bounds on partial assignments asynchronously. The asynchronous bounds
computation is based on the propagation of partial assignments. The
asynchronous forward-bounding algorithm (AFB) is a distributed optimization
search algorithm that keeps one consistent partial assignment at all times. The
algorithm is described in detail and its correctness proven. Experimental
evaluation shows that AFB outperforms synchronous branch and bound by many
orders of magnitude, and produces a phase transition as the tightness of the
problem increases. This is an analogous effect to the phase transition that has
been observed when local consistency maintenance is applied to MaxCSPs. The AFB
algorithm is further enhanced by the addition of a backjumping mechanism,
resulting in the AFB-BJ algorithm. Distributed backjumping is based on
accumulated information on bounds of all values and on processing concurrently
a queue of candidate goals for the next move back. The AFB-BJ algorithm is
compared experimentally to other DisCOP algorithms (ADOPT, DPOP, OptAPO) and is
shown to be a very efficient algorithm for DisCOPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3443</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3443</id><created>2014-01-14</created><authors><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author><author><keyname>Mancarella</keyname><forenames>Paolo</forenames></author><author><keyname>Sadri</keyname><forenames>Fariba</forenames></author><author><keyname>Stathis</keyname><forenames>Kostas</forenames></author><author><keyname>Toni</keyname><forenames>Francesca</forenames></author></authors><title>Computational Logic Foundations of KGP Agents</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  285-348, 2008</journal-ref><doi>10.1613/jair.2596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the computational logic foundations of a model of agency
called the KGP (Knowledge, Goals and Plan model. This model allows the
specification of heterogeneous agents that can interact with each other, and
can exhibit both proactive and reactive behaviour allowing them to function in
dynamic environments by adjusting their goals and plans when changes happen in
such environments. KGP provides a highly modular agent architecture that
integrates a collection of reasoning and physical capabilities, synthesised
within transitions that update the agents state in response to reasoning,
sensing and acting. Transitions are orchestrated by cycle theories that specify
the order in which transitions are executed while taking into account the
dynamic context and agent preferences, as well as selection operators for
providing inputs to transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3444</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3444</id><created>2014-01-14</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Fargier</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author><author><keyname>Bonnefon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>On the Qualitative Comparison of Decisions Having Positive and Negative
  Features</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  385-417, 2008</journal-ref><doi>10.1613/jair.2520</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making a decision is often a matter of listing and comparing positive and
negative arguments. In such cases, the evaluation scale for decisions should be
considered bipolar, that is, negative and positive values should be explicitly
distinguished. That is what is done, for example, in Cumulative Prospect
Theory. However, contraryto the latter framework that presupposes genuine
numerical assessments, human agents often decide on the basis of an ordinal
ranking of the pros and the cons, and by focusing on the most salient
arguments. In other terms, the decision process is qualitative as well as
bipolar. In this article, based on a bipolar extension of possibility theory,
we define and axiomatically characterize several decision rules tailored for
the joint handling of positive and negative arguments in an ordinal setting.
The simplest rules can be viewed as extensions of the maximin and maximax
criteria to the bipolar case, and consequently suffer from poor decisive power.
More decisive rules that refine the former are also proposed. These refinements
agree both with principles of efficiency and with the spirit of
order-of-magnitude reasoning, that prevails in qualitative decision theory. The
most refined decision rule uses leximin rankings of the pros and the cons, and
the ideas of counting arguments of equal strength and cancelling pros by cons.
It is shown to come down to a special case of Cumulative Prospect Theory, and
to subsume the Take the Best heuristic studied by cognitive psychologists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3446</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3446</id><created>2014-01-14</created><authors><author><keyname>Hawlader</keyname><forenames>Md. Shiplu</forenames></author><author><keyname>Tareeq</keyname><forenames>Saifuddin Md.</forenames></author></authors><title>Amino Acid Interaction Network Prediction using Multi-objective
  Optimization</title><categories>cs.CE cs.NE</categories><doi>10.5121/csit.2014.4113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein can be represented by amino acid interaction network. This network is
a graph whose vertices are the proteins amino acids and whose edges are the
interactions between them. This interaction network is the first step of
proteins three-dimensional structure prediction. In this paper we present a
multi-objective evolutionary algorithm for interaction prediction and ant
colony probabilistic optimization algorithm is used to confirm the interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3447</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3447</id><created>2014-01-15</created><authors><author><keyname>Esmeir</keyname><forenames>Saher</forenames></author><author><keyname>Markovitch</keyname><forenames>Shaul</forenames></author></authors><title>Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based
  Approach</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  1-31, 2008</journal-ref><doi>10.1613/jair.2602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning techniques are gaining prevalence in the production of a
wide range of classifiers for complex real-world applications with nonuniform
testing and misclassification costs. The increasing complexity of these
applications poses a real challenge to resource management during learning and
classification. In this work we introduce ACT (anytime cost-sensitive tree
learner), a novel framework for operating in such complex environments. ACT is
an anytime algorithm that allows learning time to be increased in return for
lower classification costs. It builds a tree top-down and exploits additional
time resources to obtain better estimations for the utility of the different
candidate splits. Using sampling techniques, ACT approximates the cost of the
subtree under each candidate split and favors the one with a minimal cost. As a
stochastic algorithm, ACT is expected to be able to escape local minima, into
which greedy methods may be trapped. Experiments with a variety of datasets
were conducted to compare ACT to the state-of-the-art cost-sensitive tree
learners. The results show that for the majority of domains ACT produces
significantly less costly trees. ACT also exhibits good anytime behavior with
diminishing returns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3448</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3448</id><created>2014-01-15</created><authors><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Marinescu</keyname><forenames>Radu</forenames></author></authors><title>AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  465-519, 2008</journal-ref><doi>10.1613/jair.2605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recently introduced framework of AND/OR search spaces for
graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD)
with AND nodes, in order to capture function decomposition structure and to
extend these compiled data structures to general weighted graphical models
(e.g., probabilistic models). We present the AND/OR Multi-Valued Decision
Diagram (AOMDD) which compiles a graphical model into a canonical form that
supports polynomial (e.g., solution counting, belief updating) or constant time
(e.g. equivalence of graphical models) queries. We provide two algorithms for
compiling the AOMDD of a graphical model. The first is search-based, and works
by applying reduction rules to the trace of the memory intensive AND/OR search
algorithm. The second is inference-based and uses a Bucket Elimination schedule
to combine the AOMDDs of the input functions via the the APPLY operator. For
both algorithms, the compilation time and the size of the AOMDD are, in the
worst case, exponential in the treewidth of the graphical model, rather than
pathwidth as is known for ordered binary decision diagrams (OBDDs). We
introduce the concept of semantic treewidth, which helps explain why the size
of a decision diagram is often much smaller than the worst case bound. We
provide an experimental evaluation that demonstrates the potential of AOMDDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3449</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3449</id><created>2014-01-15</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>Eliciting Single-Peaked Preferences Using Comparison Queries</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  161-191, 2009</journal-ref><doi>10.1613/jair.2606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting is a general method for aggregating the preferences of multiple
agents. Each agent ranks all the possible alternatives, and based on this, an
aggregate ranking of the alternatives (or at least a winning alternative) is
produced. However, when there are many alternatives, it is impractical to
simply ask agents to report their complete preferences. Rather, the agents
preferences, or at least the relevant parts thereof, need to be elicited. This
is done by asking the agents a (hopefully small) number of simple queries about
their preferences, such as comparison queries, which ask an agent to compare
two of the alternatives. Prior work on preference elicitation in voting has
focused on the case of unrestricted preferences. It has been shown that in this
setting, it is sometimes necessary to ask each agent (almost) as many queries
as would be required to determine an arbitrary ranking of the alternatives. In
contrast, in this paper, we focus on single-peaked preferences. We show that
such preferences can be elicited using only a linear number of comparison
queries, if either the order with respect to which preferences are
single-peaked is known, or at least one other agents complete preferences are
known. We show that using a sublinear number of queries does not suffice. We
also consider the case of cardinally single-peaked preferences. For this case,
we show that if the alternatives cardinal positions are known, then an agents
preferences can be elicited using only a logarithmic number of queries;
however, we also show that if the cardinal positions are not known, then a
sublinear number of queries does not suffice. We present experimental results
for all elicitation algorithms. We also consider the problem of only eliciting
enough information to determine the aggregate ranking, and show that even for
this more modest objective, a sublinear number of queries per agent does not
suffice for known ordinal or unknown cardinal positions. Finally, we discuss
whether and how these techniques can be applied when preferences are almost
single-peaked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3450</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3450</id><created>2014-01-15</created><authors><author><keyname>Grinshpoun</keyname><forenames>Tal</forenames></author><author><keyname>Meisels</keyname><forenames>Amnon</forenames></author></authors><title>Completeness and Performance Of The APO Algorithm</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1109.6052 by
  other authors</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  223-258, 2008</journal-ref><doi>10.1613/jair.2611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous Partial Overlay (APO) is a search algorithm that uses
cooperative mediation to solve Distributed Constraint Satisfaction Problems
(DisCSPs). The algorithm partitions the search into different subproblems of
the DisCSP. The original proof of completeness of the APO algorithm is based on
the growth of the size of the subproblems. The present paper demonstrates that
this expected growth of subproblems does not occur in some situations, leading
to a termination problem of the algorithm. The problematic parts in the APO
algorithm that interfere with its completeness are identified and necessary
modifications to the algorithm that fix these problematic parts are given. The
resulting version of the algorithm, Complete Asynchronous Partial Overlay
(CompAPO), ensures its completeness. Formal proofs for the soundness and
completeness of CompAPO are given. A detailed performance evaluation of CompAPO
comparing it to other DisCSP algorithms is presented, along with an extensive
experimental evaluation of the algorithm's unique behavior. Additionally, an
optimization version of the algorithm, CompOptAPO, is presented, discussed, and
evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3451</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3451</id><created>2014-01-15</created><authors><author><keyname>Jurca</keyname><forenames>Radu</forenames></author><author><keyname>Faltings</keyname><forenames>Boi</forenames></author></authors><title>Mechanisms for Making Crowds Truthful</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  209-253, 2009</journal-ref><doi>10.1613/jair.2621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider schemes for obtaining truthful reports on a common but hidden
signal from large groups of rational, self-interested agents. One example are
online feedback mechanisms, where users provide observations about the quality
of a product or service so that other users can have an accurate idea of what
quality they can expect. However, (i) providing such feedback is costly, and
(ii) there are many motivations for providing incorrect feedback.
  Both problems can be addressed by reward schemes which (i) cover the cost of
obtaining and reporting feedback, and (ii) maximize the expected reward of a
rational agent who reports truthfully. We address the design of such
incentive-compatible rewards for feedback generated in environments with pure
adverse selection. Here, the correlation between the true knowledge of an agent
and her beliefs regarding the likelihoods of reports of other agents can be
exploited to make honest reporting a Nash equilibrium.
  In this paper we extend existing methods for designing incentive-compatible
rewards by also considering collusion. We analyze different scenarios, where,
for example, some or all of the agents collude. For each scenario we
investigate whether a collusion-resistant, incentive-compatible reward scheme
exists, and use automated mechanism design to specify an algorithm for deriving
an efficient reward mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3453</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3453</id><created>2014-01-15</created><authors><author><keyname>Goldsmith</keyname><forenames>Judy</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Truszczyski</keyname><forenames>Miroslaw</forenames></author><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>The Computational Complexity of Dominance and Consistency in CP-Nets</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  403-432, 2008</journal-ref><doi>10.1613/jair.2627</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of testing dominance and
consistency in CP-nets. Previously, the complexity of dominance has been
determined for restricted classes in which the dependency graph of the CP-net
is acyclic. However, there are preferences of interest that define cyclic
dependency graphs; these are modeled with general CP-nets. In our main results,
we show here that both dominance and consistency for general CP-nets are
PSPACE-complete. We then consider the concept of strong dominance, dominance
equivalence and dominance incomparability, and several notions of optimality,
and identify the complexity of the corresponding decision problems. The
reductions used in the proofs are from STRIPS planning, and thus reinforce the
earlier established connections between both areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3454</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3454</id><created>2014-01-15</created><authors><author><keyname>Abdallah</keyname><forenames>Sherief</forenames></author><author><keyname>Lesser</keyname><forenames>Victor</forenames></author></authors><title>A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics</title><categories>cs.LG cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  521-549, 2008</journal-ref><doi>10.1613/jair.2628</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several multiagent reinforcement learning (MARL) algorithms have been
proposed to optimize agents decisions. Due to the complexity of the problem,
the majority of the previously developed MARL algorithms assumed agents either
had some knowledge of the underlying game (such as Nash equilibria) and/or
observed other agents actions and the rewards they received.
  We introduce a new MARL algorithm called the Weighted Policy Learner (WPL),
which allows agents to reach a Nash Equilibrium (NE) in benchmark
2-player-2-action games with minimum knowledge. Using WPL, the only feedback an
agent needs is its own local reward (the agent does not observe other agents
actions or rewards). Furthermore, WPL does not assume that agents know the
underlying game or the corresponding Nash Equilibrium a priori. We
experimentally show that our algorithm converges in benchmark
two-player-two-action games. We also show that our algorithm converges in the
challenging Shapleys game where previous MARL algorithms failed to converge
without knowing the underlying game or the NE. Furthermore, we show that WPL
outperforms the state-of-the-art algorithms in a more realistic setting of 100
agents interacting and learning concurrently.
  An important aspect of understanding the behavior of a MARL algorithm is
analyzing the dynamics of the algorithm: how the policies of multiple learning
agents evolve over time as agents interact with one another. Such an analysis
not only verifies whether agents using a given MARL algorithm will eventually
converge, but also reveals the behavior of the MARL algorithm prior to
convergence. We analyze our algorithm in two-player-two-action games and show
that symbolically proving WPLs convergence is difficult, because of the
non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had
either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs
dynamics differential equations and compare the solution to the dynamics of
previous MARL algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3455</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3455</id><created>2014-01-15</created><authors><author><keyname>Doshi</keyname><forenames>Prashant</forenames></author><author><keyname>Gmytrasiewicz</keyname><forenames>Piotr J.</forenames></author></authors><title>Monte Carlo Sampling Methods for Approximating Interactive POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  297-337, 2009</journal-ref><doi>10.1613/jair.2630</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially observable Markov decision processes (POMDPs) provide a principled
framework for sequential planning in uncertain single agent settings. An
extension of POMDPs to multiagent settings, called interactive POMDPs
(I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief
systems which represent an agent's belief about the physical world, about
beliefs of other agents, and about their beliefs about others' beliefs. This
modification makes the difficulties of obtaining solutions due to complexity of
the belief and policy spaces even more acute. We describe a general method for
obtaining approximate solutions of I-POMDPs based on particle filtering (PF).
We introduce the interactive PF, which descends the levels of the interactive
belief hierarchies and samples and propagates beliefs at each level. The
interactive PF is able to mitigate the belief space complexity, but it does not
address the policy space complexity. To mitigate the policy space complexity --
sometimes also called the curse of history -- we utilize a complementary method
based on sampling likely observations while building the look ahead
reachability tree. While this approach does not completely address the curse of
history, it beats back the curse's impact substantially. We provide
experimental results and chart future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3457</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3457</id><created>2014-01-15</created><authors><author><keyname>Branavan</keyname><forenames>S. R. K.</forenames></author><author><keyname>Chen</keyname><forenames>Harr</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author></authors><title>Learning Document-Level Semantic Properties from Free-Text Annotations</title><categories>cs.CL cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  569-603, 2009</journal-ref><doi>10.1613/jair.2633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method for inferring the semantic properties of
documents by leveraging free-text keyphrase annotations. Such annotations are
becoming increasingly abundant due to the recent dramatic growth in
semi-structured, user-generated online content. One especially relevant domain
is product reviews, which are often annotated by their authors with pros/cons
keyphrases such as a real bargain or good value. These annotations are
representative of the underlying semantic properties; however, unlike expert
annotations, they are noisy: lay authors may use different labels to denote the
same property, and some labels may be missing. To learn using such noisy
annotations, we find a hidden paraphrase structure which clusters the
keyphrases. The paraphrase structure is linked with a latent topic model of the
review texts, enabling the system to predict the properties of unannotated
documents and to effectively aggregate the semantic properties of multiple
reviews. Our approach is implemented as a hierarchical Bayesian model with
joint inference. We find that joint inference increases the robustness of the
keyphrase clustering and encourages the latent topics to correlate with
semantically meaningful properties. Multiple evaluations demonstrate that our
model substantially outperforms alternative approaches for summarizing single
and multiple documents into a set of semantically salient keyphrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3458</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3458</id><created>2014-01-15</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Dalmao</keyname><forenames>Shannon</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author></authors><title>Solving #SAT and Bayesian Inference with Backtracking Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  391-442, 2009</journal-ref><doi>10.1613/jair.2648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference in Bayes Nets (BAYES) is an important problem with numerous
applications in probabilistic reasoning. Counting the number of satisfying
assignments of a propositional formula (#SAT) is a closely related problem of
fundamental theoretical importance. Both these problems, and others, are
members of the class of sum-of-products (SUMPROD) problems. In this paper we
show that standard backtracking search when augmented with a simple memoization
scheme (caching) can solve any sum-of-products problem with time complexity
that is at least as good any other state-of-the-art exact algorithm, and that
it can also achieve the best known time-space tradeoff. Furthermore,
backtracking's ability to utilize more flexible variable orderings allows us to
prove that it can achieve an exponential speedup over other standard algorithms
for SUMPROD on some instances.
  The ideas presented here have been utilized in a number of solvers that have
been applied to various types of sum-of-product problems. These system's have
exploited the fact that backtracking can naturally exploit more of the
problem's structure to achieve improved performance on a range of
probleminstances. Empirical evidence of this performance gain has appeared in
published works describing these solvers, and we provide references to these
works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3459</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3459</id><created>2014-01-15</created><authors><author><keyname>Binshtok</keyname><forenames>Maxim</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Generic Preferences over Subsets of Structured Objects</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  133-164, 2009</journal-ref><doi>10.1613/jair.2653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various tasks in decision making and decision support systems require
selecting a preferred subset of a given set of items. Here we focus on problems
where the individual items are described using a set of characterizing
attributes, and a generic preference specification is required, that is, a
specification that can work with an arbitrary set of items. For example,
preferences over the content of an online newspaper should have this form: At
each viewing, the newspaper contains a subset of the set of articles currently
available. Our preference specification over this subset should be provided
offline, but we should be able to use it to select a subset of any currently
available set of articles, e.g., based on their tags. We present a general
approach for lifting formalisms for specifying preferences over objects with
multiple attributes into ones that specify preferences over subsets of such
objects. We also show how we can compute an optimal subset given such a
specification in a relatively efficient manner. We provide an empirical
evaluation of the approach as well as some worst-case complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3460</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3460</id><created>2014-01-15</created><authors><author><keyname>Bernstein</keyname><forenames>Daniel S.</forenames></author><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>Policy Iteration for Decentralized Control of Markov Decision Processes</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  89-132, 2009</journal-ref><doi>10.1613/jair.2667</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordination of distributed agents is required for problems arising in many
areas, including multi-robot systems, networking and e-commerce. As a formal
framework for such problems, we use the decentralized partially observable
Markov decision process (DEC-POMDP). Though much work has been done on optimal
dynamic programming algorithms for the single-agent version of the problem,
optimal algorithms for the multiagent case have been elusive. The main
contribution of this paper is an optimal policy iteration algorithm for solving
DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent
policies. The solution can include a correlation device, which allows agents to
correlate their actions without communicating. This approach alternates between
expanding the controller and performing value-preserving transformations, which
modify the controller without sacrificing value. We present two efficient
value-preserving transformations: one can reduce the size of the controller and
the other can improve its value while keeping the size fixed. Empirical results
demonstrate the usefulness of value-preserving transformations in increasing
value while keeping controller size to a minimum. To broaden the applicability
of the approach, we also present a heuristic version of the policy iteration
algorithm, which sacrifices convergence to optimality. This algorithm further
reduces the size of the controllers at each step by assuming that probability
distributions over the other agents actions are known. While this assumption
may not hold in general, it helps produce higher quality solutions in our test
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3461</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3461</id><created>2014-01-15</created><authors><author><keyname>Petrik</keyname><forenames>Marek</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>A Bilinear Programming Approach for Multiagent Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  235-274, 2009</journal-ref><doi>10.1613/jair.2673</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiagent planning and coordination problems are common and known to be
computationally hard. We show that a wide range of two-agent problems can be
formulated as bilinear programs. We present a successive approximation
algorithm that significantly outperforms the coverage set algorithm, which is
the state-of-the-art method for this class of multiagent problems. Because the
algorithm is formulated for bilinear programs, it is more general and simpler
to implement. The new algorithm can be terminated at any time and-unlike the
coverage set algorithm-it facilitates the derivation of a useful online
performance bound. It is also much more efficient, on average reducing the
computation time of the optimal solution by about four orders of magnitude.
Finally, we introduce an automatic dimensionality reduction method that
improves the effectiveness of the algorithm, extending its applicability to new
domains and providing a new way to analyze a subclass of bilinear programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3462</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3462</id><created>2014-01-15</created><authors><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author><author><keyname>Kaiser</keyname><forenames>William J.</forenames></author></authors><title>Efficient Informative Sensing using Multiple Robots</title><categories>cs.RO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  707-755, 2009</journal-ref><doi>10.1613/jair.2674</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for efficient monitoring of spatio-temporal dynamics in large
environmental applications, such as the water quality monitoring in rivers and
lakes, motivates the use of robotic sensors in order to achieve sufficient
spatial coverage. Typically, these robots have bounded resources, such as
limited battery or limited amounts of time to obtain measurements. Thus,
careful coordination of their paths is required in order to maximize the amount
of information collected, while respecting the resource constraints. In this
paper, we present an efficient approach for near-optimally solving the NP-hard
optimization problem of planning such informative paths. In particular, we
first develop eSIP (efficient Single-robot Informative Path planning), an
approximation algorithm for optimizing the path of a single robot. Hereby, we
use a Gaussian Process to model the underlying phenomenon, and use the mutual
information between the visited locations and remainder of the space to
quantify the amount of information collected. We prove that the mutual
information collected using paths obtained by using eSIP is close to the
information obtained by an optimal solution. We then provide a general
technique, sequential allocation, which can be used to extend any single robot
planning algorithm, such as eSIP, for the multi-robot problem. This procedure
approximately generalizes any guarantees for the single-robot problem to the
multi-robot case. We extensively evaluate the effectiveness of our approach on
several experiments performed in-field for two important environmental sensing
applications, lake and river monitoring, and simulation experiments performed
using several real world sensor network data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3463</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3463</id><created>2014-01-15</created><authors><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Vescovi</keyname><forenames>Michele</forenames></author></authors><title>Automated Reasoning in Modal and Description Logics via SAT Encoding:
  the Case Study of K(m)/ALC-Satisfiability</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  343-389, 2009</journal-ref><doi>10.1613/jair.2675</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two decades, modal and description logics have been applied to
numerous areas of computer science, including knowledge representation, formal
verification, database theory, distributed computing and, more recently,
semantic web and ontologies. For this reason, the problem of automated
reasoning in modal and description logics has been thoroughly investigated. In
particular, many approaches have been proposed for efficiently handling the
satisfiability of the core normal modal logic K(m), and of its notational
variant, the description logic ALC. Although simple in structure, K(m)/ALC is
computationally very hard to reason on, its satisfiability being
PSPACE-complete.
  In this paper we start exploring the idea of performing automated reasoning
tasks in modal and description logics by encoding them into SAT, so that to be
handled by state-of-the-art SAT tools; as with most previous approaches, we
begin our investigation from the satisfiability in K(m). We propose an
efficient encoding, and we test it on an extensive set of benchmarks, comparing
the approach with the main state-of-the-art tools available. Although the
encoding is necessarily worst-case exponential, from our experiments we notice
that, in practice, this approach can handle most or all the problems which are
at the reach of the other approaches, with performances which are comparable
with, or even better than, those of the current state-of-the-art tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3464</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3464</id><created>2014-01-15</created><authors><author><keyname>Daly</keyname><forenames>R&#xf3;n&#xe1;n</forenames></author><author><keyname>Shen</keyname><forenames>Qiang</forenames></author></authors><title>Learning Bayesian Network Equivalence Classes with Ant Colony
  Optimization</title><categories>cs.NE cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  391-447, 2009</journal-ref><doi>10.1613/jair.2681</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks are a useful tool in the representation of uncertain
knowledge. This paper proposes a new algorithm called ACO-E, to learn the
structure of a Bayesian network. It does this by conducting a search through
the space of equivalence classes of Bayesian networks using Ant Colony
Optimization (ACO). To this end, two novel extensions of traditional ACO
techniques are proposed and implemented. Firstly, multiple types of moves are
allowed. Secondly, moves can be given in terms of indices that are not based on
construction graph nodes. The results of testing show that ACO-E performs
better than a greedy search and other state-of-the-art and metaheuristic
algorithms whilst searching in the space of equivalence classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3465</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3465</id><created>2014-01-15</created><authors><author><keyname>de Jong</keyname><forenames>Steven</forenames></author><author><keyname>Uyttendaele</keyname><forenames>Simon</forenames></author><author><keyname>Tuyls</keyname><forenames>Karl</forenames></author></authors><title>Learning to Reach Agreement in a Continuous Ultimatum Game</title><categories>cs.GT cs.SI physics.soc-ph</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  551-574, 2008</journal-ref><doi>10.1613/jair.2685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that acting in an individually rational manner, according to
the principles of classical game theory, may lead to sub-optimal solutions in a
class of problems named social dilemmas. In contrast, humans generally do not
have much difficulty with social dilemmas, as they are able to balance personal
benefit and group benefit. As agents in multi-agent systems are regularly
confronted with social dilemmas, for instance in tasks such as resource
allocation, these agents may benefit from the inclusion of mechanisms thought
to facilitate human fairness. Although many of such mechanisms have already
been implemented in a multi-agent systems context, their application is usually
limited to rather abstract social dilemmas with a discrete set of available
strategies (usually two). Given that many real-world examples of social
dilemmas are actually continuous in nature, we extend this previous work to
more general dilemmas, in which agents operate in a continuous strategy space.
The social dilemma under study here is the well-known Ultimatum Game, in which
an optimal solution is achieved if agents agree on a common strategy. We
investigate whether a scale-free interaction network facilitates agents to
reach agreement, especially in the presence of fixed-strategy agents that
represent a desired (e.g. human) outcome. Moreover, we study the influence of
rewiring in the interaction network. The agents are equipped with
continuous-action learning automata and play a large number of random pairwise
games in order to establish a common strategy. From our experiments, we may
conclude that results obtained in discrete-strategy games can be generalized to
continuous-strategy games to a certain extent: a scale-free interaction network
structure allows agents to achieve agreement on a common strategy, and rewiring
in the interaction network greatly enhances the agents ability to reach
agreement. However, it also becomes clear that some alternative mechanisms,
such as reputation and volunteering, have many subtleties involved and do not
have convincing beneficial effects in the continuous case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3466</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3466</id><created>2014-01-15</created><authors><author><keyname>Rahwan</keyname><forenames>Talal</forenames></author><author><keyname>Ramchurn</keyname><forenames>Sarvapali Dyanand</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas Robert</forenames></author><author><keyname>Giovannucci</keyname><forenames>Andrea</forenames></author></authors><title>An Anytime Algorithm for Optimal Coalition Structure Generation</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  521-567, 2009</journal-ref><doi>10.1613/jair.2695</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coalition formation is a fundamental type of interaction that involves the
creation of coherent groupings of distinct, autonomous, agents in order to
efficiently achieve their individual or collective goals. Forming effective
coalitions is a major research challenge in the field of multi-agent systems.
Central to this endeavour is the problem of determining which of the many
possible coalitions to form in order to achieve some goal. This usually
requires calculating a value for every possible coalition, known as the
coalition value, which indicates how beneficial that coalition would be if it
was formed. Once these values are calculated, the agents usually need to find a
combination of coalitions, in which every agent belongs to exactly one
coalition, and by which the overall outcome of the system is maximized.
However, this coalition structure generation problem is extremely challenging
due to the number of possible solutions that need to be examined, which grows
exponentially with the number of agents involved. To date, therefore, many
algorithms have been proposed to solve this problem using different techniques
ranging from dynamic programming, to integer programming, to stochastic search
all of which suffer from major limitations relating to execution time, solution
quality, and memory requirements.
  With this in mind, we develop an anytime algorithm to solve the coalition
structure generation problem. Specifically, the algorithm uses a novel
representation of the search space, which partitions the space of possible
solutions into sub-spaces such that it is possible to compute upper and lower
bounds on the values of the best coalition structures in them. These bounds are
then used to identify the sub-spaces that have no potential of containing the
optimal solution so that they can be pruned. The algorithm, then, searches
through the remaining sub-spaces very efficiently using a branch-and-bound
technique to avoid examining all the solutions within the searched subspace(s).
In this setting, we prove that our algorithm enumerates all coalition
structures efficiently by avoiding redundant and invalid solutions
automatically. Moreover, in order to effectively test our algorithm we develop
a new type of input distribution which allows us to generate more reliable
benchmarks compared to the input distributions previously used in the field.
Given this new distribution, we show that for 27 agents our algorithm is able
to find solutions that are optimal in 0.175% of the time required by the
fastest available algorithm in the literature. The algorithm is anytime, and if
interrupted before it would have normally terminated, it can still provide a
solution that is guaranteed to be within a bound from the optimal one.
Moreover, the guarantees we provide on the quality of the solution are
significantly better than those provided by the previous state of the art
algorithms designed for this purpose. For example, for the worst case
distribution given 25 agents, our algorithm is able to find a 90% efficient
solution in around 10% of time it takes to find the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3467</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3467</id><created>2014-01-15</created><authors><author><keyname>Gim&#xe9;nez</keyname><forenames>Omer</forenames></author><author><keyname>Jonsson</keyname><forenames>Anders</forenames></author></authors><title>Planning over Chain Causal Graphs for Variables with Domains of Size 5
  Is NP-Hard</title><categories>cs.AI cs.CC</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  675-706, 2009</journal-ref><doi>10.1613/jair.2742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, considerable focus has been given to the problem of determining the
boundary between tractable and intractable planning problems. In this paper, we
study the complexity of planning in the class C_n of planning problems,
characterized by unary operators and directed path causal graphs. Although this
is one of the simplest forms of causal graphs a planning problem can have, we
show that planning is intractable for C_n (unless P = NP), even if the domains
of state variables have bounded size. In particular, we show that plan
existence for C_n^k is NP-hard for k&gt;=5 by reduction from CNFSAT. Here, k
denotes the upper bound on the size of the state variable domains. Our result
reduces the complexity gap for the class C_n^k to cases k=3 and k=4 only, since
C_n^2 is known to be tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3468</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3468</id><created>2014-01-15</created><authors><author><keyname>Palacios</keyname><forenames>Hector</forenames></author><author><keyname>Geffner</keyname><forenames>Hector</forenames></author></authors><title>Compiling Uncertainty Away in Conformant Planning Problems with Bounded
  Width</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  623-675, 2009</journal-ref><doi>10.1613/jair.2708</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformant planning is the problem of finding a sequence of actions for
achieving a goal in the presence of uncertainty in the initial state or action
effects. The problem has been approached as a path-finding problem in belief
space where good belief representations and heuristics are critical for scaling
up. In this work, a different formulation is introduced for conformant problems
with deterministic actions where they are automatically converted into
classical ones and solved by an off-the-shelf classical planner. The
translation maps literals L and sets of assumptions t about the initial
situation, into new literals KL/t that represent that L must be true if t is
initially true. We lay out a general translation scheme that is sound and
establish the conditions under which the translation is also complete. We show
that the complexity of the complete translation is exponential in a parameter
of the problem called the conformant width, which for most benchmarks is
bounded. The planner based on this translation exhibits good performance in
comparison with existing planners, and is the basis for T0, the best performing
planner in the Conformant Track of the 2006 International Planning Competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3469</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3469</id><created>2014-01-15</created><authors><author><keyname>de Angulo</keyname><forenames>Vicente Ruiz</forenames></author><author><keyname>Torras</keyname><forenames>Carme</forenames></author></authors><title>Exploiting Single-Cycle Symmetries in Continuous Constraint Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  499-520, 2009</journal-ref><doi>10.1613/jair.2711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetries in discrete constraint satisfaction problems have been explored
and exploited in the last years, but symmetries in continuous constraint
problems have not received the same attention. Here we focus on permutations of
the variables consisting of one single cycle. We propose a procedure that takes
advantage of these symmetries by interacting with a continuous constraint
solver without interfering with it. A key concept in this procedure are the
classes of symmetric boxes formed by bisecting a n-dimensional cube at the same
point in all dimensions at the same time. We analyze these classes and quantify
them as a function of the cube dimensionality. Moreover, we propose a simple
algorithm to generate the representatives of all these classes for any number
of variables at very high rates. A problem example from the chemical
and#64257;eld and the cyclic n-roots problem are used to show the performance
of the approach in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3470</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3470</id><created>2014-01-15</created><authors><author><keyname>Hoffmann</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Bertoli</keyname><forenames>Piergiorgio</forenames></author><author><keyname>Helmert</keyname><forenames>Malte</forenames></author><author><keyname>Pistore</keyname><forenames>Marco</forenames></author></authors><title>Message-Based Web Service Composition, Integrity Constraints, and
  Planning under Uncertainty: A New Connection</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  49-117, 2009</journal-ref><doi>10.1613/jair.2716</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to recent advances, AI Planning has become the underlying technique
for several applications. Figuring prominently among these is automated Web
Service Composition (WSC) at the &quot;capability&quot; level, where services are
described in terms of preconditions and effects over ontological concepts. A
key issue in addressing WSC as planning is that ontologies are not only formal
vocabularies; they also axiomatize the possible relationships between concepts.
Such axioms correspond to what has been termed &quot;integrity constraints&quot; in the
actions and change literature, and applying a web service is essentially a
belief update operation. The reasoning required for belief update is known to
be harder than reasoning in the ontology itself. The support for belief update
is severely limited in current planning tools.
  Our first contribution consists in identifying an interesting special case of
WSC which is both significant and more tractable. The special case, which we
term &quot;forward effects&quot;, is characterized by the fact that every ramification of
a web service application involves at least one new constant generated as
output by the web service. We show that, in this setting, the reasoning
required for belief update simplifies to standard reasoning in the ontology
itself. This relates to, and extends, current notions of &quot;message-based&quot; WSC,
where the need for belief update is removed by a strong (often implicit or
informal) assumption of &quot;locality&quot; of the individual messages. We clarify the
computational properties of the forward effects case, and point out a strong
relation to standard notions of planning under uncertainty, suggesting that
effective tools for the latter can be successfully adapted to address the
former.
  Furthermore, we identify a significant sub-case, named &quot;strictly forward
effects&quot;, where an actual compilation into planning under uncertainty exists.
This enables us to exploit off-the-shelf planning tools to solve message-based
WSC in a general form that involves powerful ontologies, and requires reasoning
about partial matches between concepts. We provide empirical evidence that this
approach may be quite effective, using Conformant-FF as the underlying planner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3471</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3471</id><created>2014-01-15</created><authors><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author><author><keyname>Miranda</keyname><forenames>Enrique</forenames></author></authors><title>Conservative Inference Rule for Uncertain Reasoning under Incompleteness</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  757-821, 2009</journal-ref><doi>10.1613/jair.2736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we formulate the problem of inference under incomplete
information in very general terms. This includes modelling the process
responsible for the incompleteness, which we call the incompleteness process.
We allow the process behaviour to be partly unknown. Then we use Walleys theory
of coherent lower previsions, a generalisation of the Bayesian theory to
imprecision, to derive the rule to update beliefs under incompleteness that
logically follows from our assumptions, and that we call conservative inference
rule. This rule has some remarkable properties: it is an abstract rule to
update beliefs that can be applied in any situation or domain; it gives us the
opportunity to be neither too optimistic nor too pessimistic about the
incompleteness process, which is a necessary condition to draw reliable while
strong enough conclusions; and it is a coherent rule, in the sense that it
cannot lead to inconsistencies. We give examples to show how the new rule can
be applied in expert systems, in parametric statistical inference, and in
pattern classification, and discuss more generally the view of incompleteness
processes defended here as well as some of its consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3472</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3472</id><created>2014-01-15</created><authors><author><keyname>Su</keyname><forenames>Kaile</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author><author><keyname>Lv</keyname><forenames>Guanfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Variable Forgetting in Reasoning about Knowledge</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  677-716, 2009</journal-ref><doi>10.1613/jair.2750</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate knowledge reasoning within a simple framework
called knowledge structure. We use variable forgetting as a basic operation for
one agent to reason about its own or other agents\ knowledge. In our framework,
two notions namely agents\ observable variables and the weakest sufficient
condition play important roles in knowledge reasoning. Given a background
knowledge base and a set of observable variables for each agent, we show that
the notion of an agent knowing a formula can be defined as a weakest sufficient
condition of the formula under background knowledge base. Moreover, we show how
to capture the notion of common knowledge by using a generalized notion of
weakest sufficient condition. Also, we show that public announcement operator
can be conveniently dealt with via our notion of knowledge structure. Further,
we explore the computational complexity of the problem whether an epistemic
formula is realized in a knowledge structure. In the general case, this problem
is PSPACE-hard; however, for some interesting subcases, it can be reduced to
co-NP. Finally, we discuss possible applications of our framework in some
interesting domains such as the automated analysis of the well-known muddy
children puzzle and the verification of the revised Needham-Schroeder protocol.
We believe that there are many scenarios where the natural presentation of the
available information about knowledge is under the form of a knowledge
structure. What makes it valuable compared with the corresponding multi-agent
S5 Kripke structure is that it can be much more succinct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3473</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3473</id><created>2014-01-15</created><authors><author><keyname>Ramchurn</keyname><forenames>Sarvapali Dyanand</forenames></author><author><keyname>Mezzetti</keyname><forenames>Claudio</forenames></author><author><keyname>Giovannucci</keyname><forenames>Andrea</forenames></author><author><keyname>Rodriguez-Aguilar</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Dash</keyname><forenames>Rajdeep Kumar</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas Robert</forenames></author></authors><title>Trust-Based Mechanisms for Robust and Efficient Task Allocation in the
  Presence of Execution Uncertainty</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  119-159, 2009</journal-ref><doi>10.1613/jair.2751</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vickrey-Clarke-Groves (VCG) mechanisms are often used to allocate tasks to
selfish and rational agents. VCG mechanisms are incentive compatible, direct
mechanisms that are efficient (i.e., maximise social utility) and individually
rational (i.e., agents prefer to join rather than opt out). However, an
important assumption of these mechanisms is that the agents will &quot;always&quot;
successfully complete their allocated tasks. Clearly, this assumption is
unrealistic in many real-world applications, where agents can, and often do,
fail in their endeavours. Moreover, whether an agent is deemed to have failed
may be perceived differently by different agents. Such subjective perceptions
about an agents probability of succeeding at a given task are often captured
and reasoned about using the notion of &quot;trust&quot;. Given this background, in this
paper we investigate the design of novel mechanisms that take into account the
trust between agents when allocating tasks.
  Specifically, we develop a new class of mechanisms, called &quot;trust-based
mechanisms&quot;, that can take into account multiple subjective measures of the
probability of an agent succeeding at a given task and produce allocations that
maximise social utility, whilst ensuring that no agent obtains a negative
utility. We then show that such mechanisms pose a challenging new combinatorial
optimisation problem (that is NP-complete), devise a novel representation for
solving the problem, and develop an effective integer programming solution
(that can solve instances with about 2x10^5 possible allocations in 40
seconds).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3474</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3474</id><created>2014-01-15</created><authors><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>Optimal Value of Information in Graphical Models</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  557-591, 2009</journal-ref><doi>10.1613/jair.2737</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world decision making tasks require us to choose among several
expensive observations. In a sensor network, for example, it is important to
select the subset of sensors that is expected to provide the strongest
reduction in uncertainty. In medical decision making tasks, one needs to select
which tests to administer before deciding on the most effective treatment. It
has been general practice to use heuristic-guided procedures for selecting
observations. In this paper, we present the first efficient optimal algorithms
for selecting observations for a class of probabilistic graphical models. For
example, our algorithms allow to optimally label hidden variables in Hidden
Markov Models (HMMs). We provide results for both selecting the optimal subset
of observations, and for obtaining an optimal conditional observation plan.
  Furthermore we prove a surprising result: In most graphical models tasks, if
one designs an efficient algorithm for chain graphs, such as HMMs, this
procedure can be generalized to polytree graphical models. We prove that the
optimizing value of information is $NP^{PP}$-hard even for polytrees. It also
follows from our results that just computing decision theoretic value of
information objective functions, which are commonly used in practice, is a
#P-complete problem even on Naive Bayes models (a simple special case of
polytrees).
  In addition, we consider several extensions, such as using our algorithms for
scheduling observation selection for multiple sensors. We demonstrate the
effectiveness of our approach on several real-world datasets, including a
prototype sensor network deployment for energy conservation in buildings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3475</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3475</id><created>2014-01-15</created><authors><author><keyname>Bienvenu</keyname><forenames>Meghyn</forenames></author></authors><title>Prime Implicates and Prime Implicants: From Propositional to Modal Logic</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  71-128, 2009</journal-ref><doi>10.1613/jair.2754</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prime implicates and prime implicants have proven relevant to a number of
areas of artificial intelligence, most notably abductive reasoning and
knowledge compilation. The purpose of this paper is to examine how these
notions might be appropriately extended from propositional logic to the modal
logic K. We begin the paper by considering a number of potential definitions of
clauses and terms for K. The different definitions are evaluated with respect
to a set of syntactic, semantic, and complexity-theoretic properties
characteristic of the propositional definition. We then compare the definitions
with respect to the properties of the notions of prime implicates and prime
implicants that they induce. While there is no definition that perfectly
generalizes the propositional notions, we show that there does exist one
definition which satisfies many of the desirable properties of the
propositional case. In the second half of the paper, we consider the
computational properties of the selected definition. To this end, we provide
sound and complete algorithms for generating and recognizing prime implicates,
and we show the prime implicate recognition task to be PSPACE-complete. We also
prove upper and lower bounds on the size and number of prime implicates. While
the paper focuses on the logic K, all of our results hold equally well for
multi-modal K and for concept expressions in the description logic ALC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3476</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3476</id><created>2014-01-15</created><authors><author><keyname>Bonatti</keyname><forenames>Piero A.</forenames></author><author><keyname>Lutz</keyname><forenames>Carsten</forenames></author><author><keyname>Wolter</keyname><forenames>Frank</forenames></author></authors><title>The Complexity of Circumscription in DLs</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  717-773, 2009</journal-ref><doi>10.1613/jair.2763</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As fragments of first-order logic, Description logics (DLs) do not provide
nonmonotonic features such as defeasible inheritance and default rules. Since
many applications would benefit from the availability of such features, several
families of nonmonotonic DLs have been developed that are mostly based on
default logic and autoepistemic logic. In this paper, we consider
circumscription as an interesting alternative approach to nonmonotonic DLs
that, in particular, supports defeasible inheritance in a natural way. We study
DLs extended with circumscription under different language restrictions and
under different constraints on the sets of minimized, fixed, and varying
predicates, and pinpoint the exact computational complexity of reasoning for
DLs ranging from ALC to ALCIO and ALCQO. When the minimized and fixed
predicates include only concept names but no role names, then reasoning is
complete for NExpTime^NP. It becomes complete for NP^NExpTime when the number
of minimized and fixed predicates is bounded by a constant. If roles can be
minimized or fixed, then complexity ranges from NExpTime^NP to undecidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3477</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3477</id><created>2014-01-15</created><authors><author><keyname>Gallardo</keyname><forenames>Jos&#xe9; Enrique</forenames></author><author><keyname>Cotta</keyname><forenames>Carlos</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Antonio Jos&#xe9;</forenames></author></authors><title>Solving Weighted Constraint Satisfaction Problems with Memetic/Exact
  Hybrid Algorithms</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:0812.4170</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  533-555, 2009</journal-ref><doi>10.1613/jair.2770</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weighted constraint satisfaction problem (WCSP) is a constraint
satisfaction problem in which preferences among solutions can be expressed.
Bucket elimination is a complete technique commonly used to solve this kind of
constraint satisfaction problem. When the memory required to apply bucket
elimination is too high, a heuristic method based on it (denominated
mini-buckets) can be used to calculate bounds for the optimal solution.
Nevertheless, the curse of dimensionality makes these techniques impractical on
large scale problems. In response to this situation, we present a memetic
algorithm for WCSPs in which bucket elimination is used as a mechanism for
recombining solutions, providing the best possible child from the parental set.
Subsequently, a multi-level model in which this exact/metaheuristic hybrid is
further hybridized with branch-and-bound techniques and mini-buckets is
studied. As a case study, we have applied these algorithms to the resolution of
the maximum density still life problem, a hard constraint optimization problem
based on Conways game of life. The resulting algorithm consistently finds
optimal patterns for up to date solved instances in less time than current
approaches. Moreover, it is shown that this proposal provides new best known
solutions for very large instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3478</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3478</id><created>2014-01-15</created><authors><author><keyname>Bromberg</keyname><forenames>Facundo</forenames></author><author><keyname>Margaritis</keyname><forenames>Dimitris</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant</forenames></author></authors><title>Efficient Markov Network Structure Discovery Using Independence Tests</title><categories>cs.LG cs.AI stat.ML</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  449-484, 2009</journal-ref><doi>10.1613/jair.2773</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two algorithms for learning the structure of a Markov network from
data: GSMN* and GSIMN. Both algorithms use statistical independence tests to
infer the structure by successively constraining the set of structures
consistent with the results of these tests. Until very recently, algorithms for
structure learning were based on maximum likelihood estimation, which has been
proved to be NP-hard for Markov networks due to the difficulty of estimating
the parameters of the network, needed for the computation of the data
likelihood. The independence-based approach does not require the computation of
the likelihood, and thus both GSMN* and GSIMN can compute the structure
efficiently (as shown in our experiments). GSMN* is an adaptation of the
Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of
Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls
well-known properties of the conditional independence relation to infer novel
independences from known ones, thus avoiding the performance of statistical
tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle
theorem, also introduced in this work, which is a simplified version of the set
of Markov axioms. Experimental comparisons on artificial and real-world data
sets show GSIMN can yield significant savings with respect to GSMN*, while
generating a Markov network with comparable or in some cases improved quality.
We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,
that produces all possible conditional independences resulting from repeatedly
applying Pearls theorems on the known conditional independence tests. The
results of this comparison show that GSIMN, by the sole use of the Triangle
theorem, is nearly optimal in terms of the set of independences tests that it
infers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3479</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3479</id><created>2014-01-15</created><authors><author><keyname>Chali</keyname><forenames>Yllias</forenames></author><author><keyname>Joty</keyname><forenames>Shafiq Rayhan</forenames></author><author><keyname>Hasan</keyname><forenames>Sadid A.</forenames></author></authors><title>Complex Question Answering: Unsupervised Learning Approaches and
  Experiments</title><categories>cs.CL cs.IR cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  1-47, 2009</journal-ref><doi>10.1613/jair.2784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex questions that require inferencing and synthesizing information from
multiple documents can be seen as a kind of topic-oriented, informative
multi-document summarization where the goal is to produce a single text as a
compressed version of a set of documents with a minimum loss of relevant
information. In this paper, we experiment with one empirical method and two
unsupervised statistical machine learning techniques: K-means and Expectation
Maximization (EM), for computing relative importance of the sentences. We
compare the results of these approaches. Our experiments show that the
empirical approach outperforms the other two techniques and EM performs better
than K-means. However, the performance of these approaches depends entirely on
the feature set used and the weighting of these features. In order to measure
the importance and relevance to the user query we extract different kinds of
features (i.e. lexical, lexical semantic, cosine similarity, basic element,
tree kernel based syntactic and shallow-semantic) for each of the document
sentences. We use a local search technique to learn the weights of the
features. To the best of our knowledge, no study has used tree kernel functions
to encode syntactic/semantic information for more complex tasks such as
computing the relatedness between the query sentences and the document
sentences in order to generate query-focused summaries (or answers to complex
questions). For each of our methods of generating summaries (i.e. empirical,
K-means and EM) we show the effects of syntactic and shallow-semantic features
over the bag-of-words (BOW) features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3481</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3481</id><created>2014-01-15</created><authors><author><keyname>Zytnicki</keyname><forenames>Matthias</forenames></author><author><keyname>Gaspin</keyname><forenames>Christine</forenames></author><author><keyname>de Givry</keyname><forenames>Simon</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author></authors><title>Bounds Arc Consistency for Weighted CSPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  593-621, 2009</journal-ref><doi>10.1613/jair.2797</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Weighted Constraint Satisfaction Problem (WCSP) framework allows
representing and solving problems involving both hard constraints and cost
functions. It has been applied to various problems, including resource
allocation, bioinformatics, scheduling, etc. To solve such problems, solvers
usually rely on branch-and-bound algorithms equipped with local consistency
filtering, mostly soft arc consistency. However, these techniques are not well
suited to solve problems with very large domains. Motivated by the resolution
of an RNA gene localization problem inside large genomic sequences, and in the
spirit of bounds consistency for large domains in crisp CSPs, we introduce soft
bounds arc consistency, a new weighted local consistency specifically designed
for WCSP with very large domains. Compared to soft arc consistency, BAC
provides significantly improved time and space asymptotic complexity. In this
paper, we show how the semantics of cost functions can be exploited to further
improve the time complexity of BAC. We also compare both in theory and in
practice the efficiency of BAC on a WCSP with bounds consistency enforced on a
crisp CSP using cost variables. On two different real problems modeled as WCSP,
including our RNA gene localization problem, we observe that maintaining bounds
arc consistency outperforms arc consistency and also improves over bounds
consistency enforced on a constraint model with cost variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3482</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3482</id><created>2014-01-15</created><authors><author><keyname>Saquete</keyname><forenames>Estela</forenames></author><author><keyname>Vicedo</keyname><forenames>Jose Luis</forenames></author><author><keyname>Mart&#xed;nez-Barco</keyname><forenames>Patricio</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Rafael</forenames></author><author><keyname>Llorens</keyname><forenames>Hector</forenames></author></authors><title>Enhancing QA Systems with Complex Temporal Question Processing
  Capabilities</title><categories>cs.CL cs.AI cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  775-811, 2009</journal-ref><doi>10.1613/jair.2805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a multilayered architecture that enhances the
capabilities of current QA systems and allows different types of complex
questions or queries to be processed. The answers to these questions need to be
gathered from factual information scattered throughout different documents.
Specifically, we designed a specialized layer to process the different types of
temporal questions. Complex temporal questions are first decomposed into simple
questions, according to the temporal relations expressed in the original
question. In the same way, the answers to the resulting simple questions are
recomposed, fulfilling the temporal restrictions of the original complex
question. A novel aspect of this approach resides in the decomposition which
uses a minimal quantity of resources, with the final aim of obtaining a
portable platform that is easily extensible to other languages. In this paper
we also present a methodology for evaluation of the decomposition of the
questions as well as the ability of the implemented temporal layer to perform
at a multilingual level. The temporal layer was first performed for English,
then evaluated and compared with: a) a general purpose QA system (F-measure
65.47% for QA plus English temporal layer vs. 38.01% for the general QA
system), and b) a well-known QA system. Much better results were obtained for
temporal questions with the multilayered system. This system was therefore
extended to Spanish and very good results were again obtained in the evaluation
(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general
QA system).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3483</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3483</id><created>2014-01-15</created><authors><author><keyname>Chieu</keyname><forenames>Hai Leong</forenames></author><author><keyname>Lee</keyname><forenames>Wee Sun Sun</forenames></author></authors><title>Relaxed Survey Propagation for The Weighted Maximum Satisfiability
  Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  229-266, 2009</journal-ref><doi>10.1613/jair.2808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The survey propagation (SP) algorithm has been shown to work well on large
instances of the random 3-SAT problem near its phase transition. It was shown
that SP estimates marginals over covers that represent clusters of solutions.
The SP-y algorithm generalizes SP to work on the maximum satisfiability
(Max-SAT) problem, but the cover interpretation of SP does not generalize to
SP-y. In this paper, we formulate the relaxed survey propagation (RSP)
algorithm, which extends the SP algorithm to apply to the weighted Max-SAT
problem. We show that RSP has an interpretation of estimating marginals over
covers violating a set of clauses with minimal weight. This naturally
generalizes the cover interpretation of SP. Empirically, we show that RSP
outperforms SP-y and other state-of-the-art Max-SAT solvers on random Max-SAT
instances. RSP also outperforms state-of-the-art weighted Max-SAT solvers on
random weighted Max-SAT instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3484</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3484</id><created>2014-01-15</created><authors><author><keyname>Janhunen</keyname><forenames>Tomi</forenames></author><author><keyname>Oikarinen</keyname><forenames>Emilia</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Modularity Aspects of Disjunctive Stable Models</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 35, pages
  813-857, 2009</journal-ref><doi>10.1613/jair.2810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practically all programming languages allow the programmer to split a program
into several modules which brings along several advantages in software
development. In this paper, we are interested in the area of answer-set
programming where fully declarative and nonmonotonic languages are applied. In
this context, obtaining a modular structure for programs is by no means
straightforward since the output of an entire program cannot in general be
composed from the output of its components. To better understand the effects of
disjunctive information on modularity we restrict the scope of analysis to the
case of disjunctive logic programs (DLPs) subject to stable-model semantics. We
define the notion of a DLP-function, where a well-defined input/output
interface is provided, and establish a novel module theorem which indicates the
compositionality of stable-model semantics for DLP-functions. The module
theorem extends the well-known splitting-set theorem and enables the
decomposition of DLP-functions given their strongly connected components based
on positive dependencies induced by rules. In this setting, it is also possible
to split shared disjunctive rules among components using a generalized shifting
technique. The concept of modular equivalence is introduced for the mutual
comparison of DLP-functions using a generalization of a translation-based
verification method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3485</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3485</id><created>2014-01-15</created><authors><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Shearer</keyname><forenames>Rob</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Hypertableau Reasoning for Description Logics</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  165-228, 2009</journal-ref><doi>10.1613/jair.2811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel reasoning calculus for the description logic SHOIQ^+---a
knowledge representation formalism with applications in areas such as the
Semantic Web. Unnecessary nondeterminism and the construction of large models
are two primary sources of inefficiency in the tableau-based reasoning calculi
used in state-of-the-art reasoners. In order to reduce nondeterminism, we base
our calculus on hypertableau and hyperresolution calculi, which we extend with
a blocking condition to ensure termination. In order to reduce the size of the
constructed models, we introduce anywhere pairwise blocking. We also present an
improved nominal introduction rule that ensures termination in the presence of
nominals, inverse roles, and number restrictions---a combination of DL
constructs that has proven notoriously difficult to handle. Our implementation
shows significant performance improvements over state-of-the-art reasoners on
several well-known ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3486</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3486</id><created>2014-01-15</created><authors><author><keyname>Jonsson</keyname><forenames>Anders</forenames></author></authors><title>The Role of Macros in Tractable Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  471-511, 2009</journal-ref><doi>10.1613/jair.2891</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents several new tractability results for planning based on
macros. We describe an algorithm that optimally solves planning problems in a
class that we call inverted tree reducible, and is provably tractable for
several subclasses of this class. By using macros to store partial plans that
recur frequently in the solution, the algorithm is polynomial in time and space
even for exponentially long plans. We generalize the inverted tree reducible
class in several ways and describe modifications of the algorithm to deal with
these new classes. Theoretical results are validated in experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3487</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3487</id><created>2014-01-15</created><authors><author><keyname>Artale</keyname><forenames>Alessandro</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Kontchakov</keyname><forenames>Roman</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>Michael</forenames></author></authors><title>The DL-Lite Family and Relations</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  1-69, 2009</journal-ref><doi>10.1613/jair.2820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced series of description logics under the common moniker
DL-Lite has attracted attention of the description logic and semantic web
communities due to the low computational complexity of inference, on the one
hand, and the ability to represent conceptual modeling formalisms, on the
other. The main aim of this article is to carry out a thorough and systematic
investigation of inference in extensions of the original DL-Lite logics along
five axes: by (i) adding the Boolean connectives and (ii) number restrictions
to concept constructs, (iii) allowing role hierarchies, (iv) allowing role
disjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity
constraints, and (v) adopting or dropping the unique same assumption. We
analyze the combined complexity of satisfiability for the resulting logics, as
well as the data complexity of instance checking and answering positive
existential queries. Our approach is based on embedding DL-Lite logics in
suitable fragments of the one-variable first-order logic, which provides useful
insights into their properties and, in particular, computational behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3488</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3488</id><created>2014-01-15</created><authors><author><keyname>Chen</keyname><forenames>Harr</forenames></author><author><keyname>Branavan</keyname><forenames>S. R. K.</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Karger</keyname><forenames>David R.</forenames></author></authors><title>Content Modeling Using Latent Permutations</title><categories>cs.IR cs.CL cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  129-163, 2009</journal-ref><doi>10.1613/jair.2830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel Bayesian topic model for learning discourse-level document
structure. Our model leverages insights from discourse theory to constrain
latent topic assignments in a way that reflects the underlying organization of
document topics. We propose a global model in which both topic selection and
ordering are biased to be similar across a collection of related documents. We
show that this space of orderings can be effectively represented using a
distribution over permutations called the Generalized Mallows Model. We apply
our method to three complementary discourse-level tasks: cross-document
alignment, document segmentation, and information ordering. Our experiments
show that incorporating our permutation-based model in these applications
yields substantial improvements in performance over previously proposed
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3489</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3489</id><created>2014-01-15</created><authors><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Gogate</keyname><forenames>Vibhav</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Join-Graph Propagation Algorithms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  279-328, 2010</journal-ref><doi>10.1613/jair.2842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates parameterized approximate message-passing schemes that
are based on bounded inference and are inspired by Pearl's belief propagation
algorithm (BP). We start with the bounded inference mini-clustering algorithm
and then move to the iterative scheme called Iterative Join-Graph Propagation
(IJGP), that combines both iteration and bounded inference. Algorithm IJGP
belongs to the class of Generalized Belief Propagation algorithms, a framework
that allowed connections with approximate algorithms from statistical physics
and is shown empirically to surpass the performance of mini-clustering and
belief propagation, as well as a number of other state-of-the-art algorithms on
several classes of networks. We also provide insight into the accuracy of
iterative BP and IJGP by relating these algorithms to well known classes of
constraint propagation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3490</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3490</id><created>2014-01-15</created><authors><author><keyname>Yeoh</keyname><forenames>William</forenames></author><author><keyname>Felner</keyname><forenames>Ariel</forenames></author><author><keyname>Koenig</keyname><forenames>Sven</forenames></author></authors><title>BnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  85-133, 2010</journal-ref><doi>10.1613/jair.2849</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed constraint optimization (DCOP) problems are a popular way of
formulating and solving agent-coordination problems. A DCOP problem is a
problem where several agents coordinate their values such that the sum of the
resulting constraint costs is minimal. It is often desirable to solve DCOP
problems with memory-bounded and asynchronous algorithms. We introduce
Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search
algorithm that uses the message-passing and communication framework of ADOPT
(Modi, Shen, Tambe, and Yokoo, 2005), a well known memory-bounded asynchronous
DCOP search algorithm, but changes the search strategy of ADOPT from best-first
search to depth-first branch-and-bound search. Our experimental results show
that BnB-ADOPT finds cost-minimal solutions up to one order of magnitude faster
than ADOPT for a variety of large DCOP problems and is as fast as NCBB, a
memory-bounded synchronous DCOP search algorithm, for most of these DCOP
problems. Additionally, it is often desirable to find bounded-error solutions
for DCOP problems within a reasonable amount of time since finding cost-minimal
solutions is NP-hard. The existing bounded-error approximation mechanism allows
users only to specify an absolute error bound on the solution cost but a
relative error bound is often more intuitive. Thus, we present two new
bounded-error approximation mechanisms that allow for relative error bounds and
implement them on top of BnB-ADOPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3491</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3491</id><created>2014-01-15</created><authors><author><keyname>Keyder</keyname><forenames>Emil</forenames></author><author><keyname>Geffner</keyname><forenames>Hector</forenames></author></authors><title>Soft Goals Can Be Compiled Away</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  547-556, 2009</journal-ref><doi>10.1613/jair.2857</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft goals extend the classical model of planning with a simple model of
preferences. The best plans are then not the ones with least cost but the ones
with maximum utility, where the utility of a plan is the sum of the utilities
of the soft goals achieved minus the plan cost. Finding plans with high utility
appears to involve two linked problems: choosing a subset of soft goals to
achieve and finding a low-cost plan to achieve them. New search algorithms and
heuristics have been developed for planning with soft goals, and a new track
has been introduced in the International Planning Competition (IPC) to test
their performance. In this note, we show however that these extensions are not
needed: soft goals do not increase the expressive power of the basic model of
planning with action costs, as they can easily be compiled away. We apply this
compilation to the problems of the net-benefit track of the most recent IPC,
and show that optimal and satisficing cost-based planners do better on the
compiled problems than optimal and satisficing net-benefit planners on the
original problems with explicit soft goals. Furthermore, we show that
penalties, or negative preferences expressing conditions to avoid, can also be
compiled away using a similar idea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3492</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3492</id><created>2014-01-15</created><authors><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Stuetzle</keyname><forenames>Thomas</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author></authors><title>ParamILS: An Automatic Algorithm Configuration Framework</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  267-306, 2009</journal-ref><doi>10.1613/jair.2861</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of performance-optimizing parameter settings is an
important part of the development and application of algorithms. We describe an
automatic framework for this algorithm configuration problem. More formally, we
provide methods for optimizing a target algorithm's performance on a given
class of problem instances by varying a set of ordinal and/or categorical
parameters. We review a family of local-search-based algorithm configuration
procedures and present novel techniques for accelerating them by adaptively
limiting the time spent for evaluating individual configurations. We describe
the results of a comprehensive experimental evaluation of our methods, based on
the configuration of prominent complete and incomplete algorithms for SAT. We
also present what is, to our knowledge, the first published work on
automatically configuring the CPLEX mixed integer programming solver. All the
algorithms we considered had default parameter settings that were manually
identified with considerable effort. Nevertheless, using our automated
algorithm configuration procedures, we achieved substantial and consistent
performance improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3493</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3493</id><created>2014-01-15</created><authors><author><keyname>Zahavi</keyname><forenames>Uzi</forenames></author><author><keyname>Felner</keyname><forenames>Ariel</forenames></author><author><keyname>Burch</keyname><forenames>Neil</forenames></author><author><keyname>Holte</keyname><forenames>Robert C.</forenames></author></authors><title>Predicting the Performance of IDA* using Conditional Distributions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  41-83, 2010</journal-ref><doi>10.1613/jair.2890</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes
IDA* will expand on a single iteration for a given consistent heuristic, and
experimentally demonstrated that it could make very accurate predictions. In
this paper we show that, in addition to requiring the heuristic to be
consistent, their formulas predictions are accurate only at levels of the
brute-force search tree where the heuristic values obey the unconditional
distribution that they defined and then used in their formula. We then propose
a new formula that works well without these requirements, i.e., it can make
accurate predictions of IDA*s performance for inconsistent heuristics and if
the heuristic values in any level do not obey the unconditional distribution.
In order to achieve this we introduce the conditional distribution of heuristic
values which is a generalization of their unconditional heuristic distribution.
We also provide extensions of our formula that handle individual start states
and the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for
propagating heuristic values when inconsistent heuristics are used.
Experimental results demonstrate the accuracy of our new method and all its
variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3494</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3494</id><created>2014-01-15</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Tamir</keyname><forenames>Tami</forenames></author></authors><title>Approximate Strong Equilibrium in Job Scheduling Games</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  387-414, 2009</journal-ref><doi>10.1613/jair.2892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Nash Equilibrium (NE) is a strategy profile resilient to unilateral
deviations, and is predominantly used in the analysis of multiagent systems. A
downside of NE is that it is not necessarily stable against deviations by
coalitions. Yet, as we show in this paper, in some cases, NE does exhibit
stability against coalitional deviations, in that the benefits from a joint
deviation are bounded. In this sense, NE approximates strong equilibrium.
  Coalition formation is a key issue in multiagent systems. We provide a
framework for quantifying the stability and the performance of various
assignment policies and solution concepts in the face of coalitional
deviations. Within this framework we evaluate a given configuration according
to three measures: (i) IR_min: the maximal number alpha, such that there exists
a coalition in which the minimal improvement ratio among the coalition members
is alpha, (ii) IR_max: the maximal number alpha, such that there exists a
coalition in which the maximal improvement ratio among the coalition members is
alpha, and (iii) DR_max: the maximal possible damage ratio of an agent outside
the coalition.
  We analyze these measures in job scheduling games on identical machines. In
particular, we provide upper and lower bounds for the above three measures for
both NE and the well-known assignment rule Longest Processing Time (LPT).
  Our results indicate that LPT performs better than a general NE. However, LPT
is not the best possible approximation. In particular, we present a polynomial
time approximation scheme (PTAS) for the makespan minimization problem which
provides a schedule with IR_min of 1+epsilon for any given epsilon. With
respect to computational complexity, we show that given an NE on m &gt;= 3
identical machines or m &gt;= 2 unrelated machines, it is NP-hard to determine
whether a given coalition can deviate such that every member decreases its
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3502</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3502</id><created>2014-01-15</created><updated>2014-04-18</updated><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Dusit</keyname><forenames>Niyato</forenames></author><author><keyname>Ekram</keyname><forenames>Hossain</forenames></author></authors><title>Dynamic Spectrum Access in Cognitive Radio Networks with RF Energy
  Harvesting</title><categories>cs.NI</categories><comments>To appear in IEEE Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum efficiency and energy efficiency are two critical issues in
designing wireless networks. Through dynamic spectrum access, cognitive radios
can improve the spectrum efficiency and capacity of wireless networks. On the
other hand, radio frequency (RF) energy harvesting has emerged as a promising
technique to supply energy to wireless networks and thereby increase their
energy efficiency. Therefore, to achieve both spectrum and energy efficiencies,
the secondary users in a cognitive radio network (CRN) can be equipped with the
RF energy harvesting capability and such a network can be referred to as an
RF-powered cognitive radio network. In this article, we provide an overview of
the RF-powered CRNs and discuss the challenges that arise for dynamic spectrum
access in these networks. Focusing on the tradeoff among spectrum sensing, data
transmission, and RF energy harvesting, then we discuss the dynamic channel
selection problem in a multi-channel RF-powered CRN. In the RF-powered CRN, a
secondary user can adaptively select a channel to transmit data when the
channel is not occupied by any primary user. Alternatively, the secondary user
can harvest RF energy for data transmission if the channel is occupied. The
optimal channel selection policy of the secondary user can be obtained by
formulating a Markov decision process (MDP) problem. We present some numerical
results obtained by solving this MDP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3506</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3506</id><created>2014-01-15</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>A Layered Coalitional Game Framework of Wireless Relay Network</title><categories>cs.NI cs.GT</categories><comments>Accepted for publication in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wireless relay network (WRN) has recently emerged as an effective way to
increase communication capacity and extend a coverage area with a low cost. In
the WRN, multiple service providers (SPs) can cooperate to share their
resources (e.g., relay nodes and spectrum), to achieve higher utility in terms
of revenue. Such cooperation can improve the capacity of the WRN, and thus
throughput for terminal devices (TDs). However, this cooperation can be
realized only if fair allocation of aggregated utility, which is the sum of the
utility of all the cooperative SPs, can be achieved. In this paper, we
investigate the WRN consisting of SPs at the upper layer and TDs at the lower
layer and present a game theoretic framework to address the cooperation
decision making problem in the WRN. Specifically, the cooperation of SPs is
modeled as an overlapping coalition formation game, in which SPs should form a
stable coalitional structure and obtain a fair share of the aggregated utility.
We also study the problem of allocating aggregated utility based on the concept
of Shapley value, which stabilizes the cooperation of SPs in the WRN. The
cooperation of TDs is modeled as a network formation game, in which TDs
establish links among each other to form a stable network structure. Numerical
results demonstrate that the proposed distributed algorithm obtains the
aggregated utility approximating the optimal solutions and achieves good
convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3510</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3510</id><created>2014-01-15</created><authors><author><keyname>Varshney</keyname><forenames>Saurabh</forenames></author><author><keyname>Bajpai</keyname><forenames>Jyoti</forenames></author></authors><title>Improving Performance Of English-Hindi Cross Language Information
  Retrieval Using Transliteration Of Query Terms</title><categories>cs.IR cs.CL</categories><comments>International Journal on Natural Language Computing (IJNLC) Vol. 2,
  No.6, December 2013 http://airccse.org/journal/ijnlc/index.html</comments><doi>10.5121/ijnlc.2013.2604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main issue in Cross Language Information Retrieval (CLIR) is the poor
performance of retrieval in terms of average precision when compared to
monolingual retrieval performance. The main reasons behind poor performance of
CLIR are mismatching of query terms, lexical ambiguity and un-translated query
terms. The existing problems of CLIR are needed to be addressed in order to
increase the performance of the CLIR system. In this paper, we are putting our
effort to solve the given problem by proposed an algorithm for improving the
performance of English-Hindi CLIR system. We used all possible combination of
Hindi translated query using transliteration of English query terms and
choosing the best query among them for retrieval of documents. The experiment
is performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.
The experimental result show that the proposed approach gives better
performance of English-Hindi CLIR system and also helps in overcoming existing
problems and outperforms the existing English-Hindi CLIR system in terms of
average precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3511</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3511</id><created>2014-01-15</created><authors><author><keyname>Li</keyname><forenames>Hongxing</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Optimal CSMA-based Wireless Communication with Worst-case Delay and
  Non-uniform Sizes</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carrier Sense Multiple Access (CSMA) protocols have been shown to reach the
full capacity region for data communication in wireless networks, with
polynomial complexity. However, current literature achieves the throughput
optimality with an exponential delay scaling with the network size, even in a
simplified scenario for transmission jobs with uniform sizes. Although CSMA
protocols with order-optimal average delay have been proposed for specific
topologies, no existing work can provide worst-case delay guarantee for each
job in general network settings, not to mention the case when the jobs have
non-uniform lengths while the throughput optimality is still targeted. In this
paper, we tackle on this issue by proposing a two-timescale CSMA-based data
communication protocol with dynamic decisions on rate control, link scheduling,
job transmission and dropping in polynomial complexity. Through rigorous
analysis, we demonstrate that the proposed protocol can achieve a throughput
utility arbitrarily close to its offline optima for jobs with non-uniform sizes
and worst-case delay guarantees, with a tradeoff of longer maximum allowable
delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3516</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3516</id><created>2014-01-15</created><authors><author><keyname>Hartmann</keyname><forenames>Tanja</forenames></author><author><keyname>Kappes</keyname><forenames>Andrea</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Clustering Evolving Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>46 pages (including 9 pages of references), 4 figures, submission
  version, to appear in a collection of surveys associated with the DFG
  Priority Programme &quot;Algorithm Engineering&quot; (SPP 1307)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roughly speaking, clustering evolving networks aims at detecting structurally
dense subgroups in networks that evolve over time. This implies that the
subgroups we seek for also evolve, which results in many additional tasks
compared to clustering static networks. We discuss these additional tasks and
difficulties resulting thereof and present an overview on current approaches to
solve these problems. We focus on clustering approaches in online scenarios,
i.e., approaches that incrementally use structural information from previous
time steps in order to incorporate temporal smoothness or to achieve low
running time. Moreover, we describe a collection of real world networks and
generators for synthetic data that are often used for evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3519</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3519</id><created>2014-01-15</created><authors><author><keyname>Sharma</keyname><forenames>Kamlesh</forenames></author><author><keyname>Prasad</keyname><forenames>Dr. T. V</forenames></author></authors><title>Swar The Voice Operated PC</title><categories>cs.HC</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyboard, although a popular medium, is not very convenient as it requires a
certain amount of skill for effective usage. A mouse on the other hand requires
a good hand-eye co-ordination. Also current computer interfaces also assume a
certain level of literacy from the user. It also expect the user to have
certain level of proficiency in English. In our country where the literacy
level is as low as 50% in some states, if information technology has to reach
the grass root level; these constraints have to be eliminated. As a solution
for these, Speech Recognition and hence the concept of Voice operated computer
system comes into picture. In this paper we propose a technique to develop a
voice recognition system which will be used for controlling computer via speech
input from any user i.e. without the use of mouse and / or keyboard. Once
developed this system would be of great benefit to physically handicapped
people as Instead of scrolling through written procedures on a laptop or
handheld computer, they can wear a headset and have their hands and eyes free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3520</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3520</id><created>2014-01-15</created><updated>2014-02-05</updated><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Adaptive Mode Selection for Bidirectional Relay Networks -- Fixed Rate
  Transmission</title><categories>cs.IT math.IT</categories><comments>IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of sum throughput maximization for
bidirectional relay networks with block fading. Thereby, user 1 and user 2
exchange information only via a relay node, i.e., a direct link between both
users is not present. We assume that channel state information at the
transmitter (CSIT) is not available and/or only one coding and modulation
scheme is used at the transmitters due to complexity constraints. Thus, the
nodes transmit with a fixed predefined rate regardless of the channel state
information (CSI). In general, the nodes in the network can assume one of three
possible states in each time slot, namely the transmit, receive, and silent
state. Most of the existing protocols assume a fixed schedule for the sequence
of the states of the nodes. In this paper, we abandon the restriction of having
a fixed and predefined schedule and propose a new protocol which, based on the
CSI at the receiver (CSIR), selects the optimal states of the nodes in each
time slot such that the sum throughput is maximized. To this end, the relay has
to be equipped with two buffers for storage of the information received from
the two users. Numerical results show that the proposed protocol significantly
outperforms the existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3521</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3521</id><created>2014-01-15</created><authors><author><keyname>Syrjala</keyname><forenames>Ville</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Riihonen</keyname><forenames>Taneli</forenames></author><author><keyname>Korpi</keyname><forenames>Dani</forenames></author></authors><title>Analysis of Oscillator Phase-Noise Effects on Self-Interference
  Cancellation in Full-Duplex OFDM Radio Transceivers</title><categories>cs.IT math.IT</categories><comments>Revised manuscript for IEEE Transactions on Wireless Communications,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the analysis of oscillator phase-noise effects on the
self-interference cancellation capability of full-duplex direct-conversion
radio transceivers. Closed-form solutions are derived for the power of the
residual self-interference stemming from phase noise in two alternative cases
of having either independent oscillators or the same oscillator at the
transmitter and receiver chains of the full-duplex transceiver. The results
show that phase noise has a severe effect on self-interference cancellation in
both of the considered cases, and that by using the common oscillator in
upconversion and downconversion results in clearly lower residual
self-interference levels. The results also show that it is in general vital to
use high quality oscillators in full-duplex transceivers, or have some means
for phase noise estimation and mitigation in order to suppress its effects. One
of the main findings is that in practical scenarios the subcarrier-wise
phase-noise spread of the multipath components of the self-interference channel
causes most of the residual phase-noise effect when high amounts of
self-interference cancellation is desired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3525</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3525</id><created>2014-01-15</created><authors><author><keyname>Roehner</keyname><forenames>Bertrand M.</forenames></author></authors><title>Is the month of Ramadan marked by a reduction in the number of suicides?</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Muslims the month of Ramadan is a time of fasting but during the evenings
after sunset it is also an occasion for family and social gatherings.
Therefore, according to the Bertillon-Durkheim conception of suicide (that is
based on the strength of social ties), one would expect a fall in suicide rates
during Ramadan. Is this conjecture confirmed by observation? That is the
question addressed in the present paper. Surprisingly, the most tricky part of
the investigation was to find reliable monthly suicide data. In the Islamic
world Turkey seems to be the only country whose statistical institute publishes
such observations. The data reveal indeed a fall of about $15\%$ in suicide
numbers during the month of Ramadan (with respect to same-non-Ramadan months).
As the standard deviation is only $4.7\%$ this effect has a high degree of
significance. This observation, along with the fact that other occasions of
social gathering such as Thanksgiving or Christmas are also marked by a drop in
suicides, adds further credence to the B-D thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3527</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3527</id><created>2014-01-15</created><authors><author><keyname>Han</keyname><forenames>Guangyue</forenames></author><author><keyname>Song</keyname><forenames>Jian</forenames></author></authors><title>Extensions of the I-MMSE Relation</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unveiling a fundamental link between information theory and estimation
theory, the I-MMSE relation by Guo, Shamai and Verdu has great theoretical
significance and numerous practical applications. On the other hand, its
influences to date have been restricted to channels without feedback and
memory, due to the lack of extensions of the I-MMSE relation to such channels.
In this paper, we propose extensions of the I-MMSE relation for discrete and
continuous-time Gaussian channels with feedback or memory. Our approach is
based on a very simple observation, which can be applied to other scenarios,
such as a simple and direct proof of the classical de Bruijn's identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3529</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3529</id><created>2014-01-15</created><updated>2015-06-27</updated><authors><author><keyname>Liu</keyname><forenames>Xianming</forenames></author><author><keyname>Han</keyname><forenames>Guangyue</forenames></author></authors><title>Capacity Regions of Families of Continuous-Time Multi-User Gaussian
  Channels</title><categories>cs.IT math.IT</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to use Brownian motions to model families of
continuous-time multiuser Gaussian channels without bandwidth limit. It turns
out that such a formulation allows parallel translation of many fundamental
notions and techniques from the discrete-time setting to the continuous-time
regime, which enables us to derive the capacity regions of a continuous-time
white Gaussian multiple access channel with/without feedback, a continuous-time
white Gaussian interference channel without feedback and a continuous-time
white Gaussian broadcast channel without feedback. In theory, these capacity
results give the fundamental transmission limit modulation/coding schemes can
achieve for families of continuous-time Gaussian one-hop channels without
bandwidth limit; in practice, the explicit capacity regions derived and
capacity achieving modulation/coding scheme proposed may provide engineering
insights on designing multi-user communication systems operating on an
ultra-wideband regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3531</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3531</id><created>2014-01-15</created><updated>2014-05-08</updated><authors><author><keyname>Fulcher</keyname><forenames>Ben D.</forenames></author><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author></authors><title>Highly comparative feature-based time-series classification</title><categories>cs.LG cs.AI cs.DB physics.data-an q-bio.QM</categories><doi>10.1109/TKDE.2014.2316504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A highly comparative, feature-based approach to time series classification is
introduced that uses an extensive database of algorithms to extract thousands
of interpretable features from time series. These features are derived from
across the scientific time-series analysis literature, and include summaries of
time series in terms of their correlation structure, distribution, entropy,
stationarity, scaling properties, and fits to a range of time-series models.
After computing thousands of features for each time series in a training set,
those that are most informative of the class structure are selected using
greedy forward feature selection with a linear classifier. The resulting
feature-based classifiers automatically learn the differences between classes
using a reduced number of time-series properties, and circumvent the need to
calculate distances between time series. Representing time series in this way
results in orders of magnitude of dimensionality reduction, allowing the method
to perform well on very large datasets containing long time series or time
series of different lengths. For many of the datasets studied, classification
performance exceeded that of conventional instance-based classifiers, including
one nearest neighbor classifiers using Euclidean distances and dynamic time
warping and, most importantly, the features selected provide an understanding
of the properties of the dataset, insight that can guide further scientific
investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3538</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3538</id><created>2014-01-15</created><updated>2014-08-04</updated><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Riihonen</keyname><forenames>Taneli</forenames></author><author><keyname>Syrj&#xe4;l&#xe4;</keyname><forenames>Ville</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author></authors><title>Full-Duplex Transceiver System Calculations: Analysis of ADC and
  Linearity Challenges</title><categories>cs.IT cs.ET math.IT</categories><comments>16 pages, 12 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 13, no. 7, pp.
  3821-3836, July 2014</journal-ref><doi>10.1109/TWC.2014.2315213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the intensive recent research on wireless single-channel full-duplex
communications, relatively little is known about the transceiver chain
nonidealities of full-duplex devices. In this paper, the effect of nonlinear
distortion occurring in the transmitter power amplifier (PA) and the receiver
chain is analyzed, alongside with the dynamic range requirements of
analog-to-digital converters (ADCs). This is done with detailed system
calculations, which combine the properties of the individual electronics
components to jointly model the complete transceiver chain, including
self-interference cancellation. They also quantify the decrease in the dynamic
range for the signal of interest caused by self-interference at the
analog-to-digital interface. Using these system calculations, we provide
comprehensive numerical results for typical transceiver parameters. The
analytical results are also confirmed with full waveform simulations. We
observe that the nonlinear distortion produced by the transmitter PA is a
significant issue in a full-duplex transceiver and, when using cheaper and less
linear components, also the receiver chain nonlinearities become considerable.
It is also shown that, with digitally-intensive self-interference cancellation,
the quantization noise of the ADCs is another significant problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3541</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3541</id><created>2014-01-15</created><authors><author><keyname>Sidi</keyname><forenames>Habib B. A.</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Tall</keyname><forenames>Abdoulaye</forenames></author></authors><title>Self-Optimizing Mechanisms for EMF Reduction in Heterogeneous Networks</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the exposure to Radio Frequency (RF) Electromagnetic
Fields (EMF) and on optimization methods to reduce it. Within the FP7 LEXNET
project, an Exposure Index (EI) has been defined that aggregates the essential
components that impact exposure to EMF. The EI includes, among other, downlink
(DL) exposure induced by the base stations (BSs) and access points, the uplink
(UL) exposure induced by the devices in communication, and the corresponding
exposure time. Motivated by the EI definition, this paper develops stochastic
approximation based self-optimizing algorithm that dynamically adapts the
network to reduce the EI in a heterogeneous network with macro- and small
cells. It is argued that the increase of the small cells' coverage can, to a
certain extent, reduce the EI, but above a certain limit, will deteriorate DL
QoS. A load balancing algorithm is formulated that adapts the small cell'
coverage based on UL loads and a DL QoS indicator. The proof of convergence of
the algorithm is provided and its performance in terms of EI reduction is
illustrated through extensive numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3554</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3554</id><created>2014-01-15</created><authors><author><keyname>Jain</keyname><forenames>Abhishek</forenames></author><author><keyname>Gupta</keyname><forenames>Piyush Kumar</forenames></author><author><keyname>Gupta</keyname><forenames>Dr. Hima</forenames></author><author><keyname>Dhar</keyname><forenames>Sachish</forenames></author></authors><title>Accelerating SystemVerilog UVM Based VIP to Improve Methodology for
  Verification of Image Signal Processing Designs Using HW Emulator</title><categories>cs.OH</categories><comments>International Journal of VLSI design &amp; Communication Systems (VLSICS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the development of Acceleratable UVCs from standard
UVCs in SystemVerilog and their usage in UVM based Verification Environment of
Image Signal Processing designs to increase run time performance. This paper
covers development of Acceleratable UVCs from standard UVCs for internal
control and data buses of ST imaging group by partitioning of transaction-level
components and cycle-accurate signal-level components between the software
simulator and hardware accelerator respectively. Standard Co-Emulation API:
Modeling Interface (SCE-MI) compliant, transaction-level communications link
between test benches running on a host system and Emulation machine is
established. Accelerated Verification IPs are used at UVM based Verification
Environment of Image Signal Processing designs both with simulator and emulator
as UVM acceleration is an extension of the standard simulation-only UVM and is
fully backward compatible with it. Acceleratable UVCs significantly reduces
development schedule risks while leveraging transaction models used during
simulation.
  In this paper, we discuss our experiences on UVM based methodology adoption
on TestBench-Xpress(TBX) based technology step by step. We are also doing
comparison between the run time performance results from earlier simulator-only
environment and the new, hardware-accelerated environment. Although this paper
focuses on Acceleratable UVCs development and their usage for image signal
processing designs, Same concept can be extended for non-image signal
processing designs.
  KEYWORDS- SystemVerilog, Universal Verification Methodology (UVM),
TestBench-Xpress (TBX), Universal Verification Component (UVC), Standard
Co-Emulation API: Modelling Interface (SCE-MI), Acceleratable UVC, Emulator,
XRTL Tasks/Functions (xtf), Transactor interface (tif), Verification IP (VIP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3556</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3556</id><created>2014-01-15</created><authors><author><keyname>Geyer</keyname><forenames>Alex E.</forenames></author><author><keyname>Nikjah</keyname><forenames>Reza</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Beaulieu</keyname><forenames>Norman C.</forenames></author></authors><title>Equivalent Codes, Optimality, and Performance Analysis of OSTBC:
  Textbook Study</title><categories>cs.IT math.IT</categories><comments>33 pages, 12 figures, 5 tables, full size journal paper, Finished in
  Oct. 2009, Unpublished</comments><journal-ref>IEEE Trans. Communications, vol. 63, no. 8, pp. 2912-2923, Aug.
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equivalent model for a multi-input multi-output (MIMO) communication
system with orthogonal space-time block codes (OSTBCs) is proposed based on a
newly revealed connection between OSTBCs and Euclidean codes. Examples of
distance spectra, signal constellations, and signal coordinate diagrams of
Euclidean codes equivalent to simplest OSTBCs are given. A new asymptotic upper
bound for the symbol error rate (SER) of OSTBCs, based on the distance spectra
of the introduced equivalent Euclidean codes is derived, and new general design
criteria for signal constellations of the optimal OSTBC are proposed. Some
bounds relating distance properties, dimensionality, and cardinality of OSTBCs
with constituent signals of equal energy are given, and new optimal signal
constellations with cardinalities M = 8 and M = 16 for Alamouti's code are
designed. Using the new model for MIMO communication systems with OSTBCs, a
general methodology for performance analysis of OSTBCs is developed. As an
example of the application of this methodology, an exact evaluation of the SER
of any OSTBC is given. Namely, a new expression for the SER of Alamouti's OSTBC
with binary phase shift keying (BPSK) signals is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3566</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3566</id><created>2014-01-15</created><authors><author><keyname>Taheri</keyname><forenames>Omid</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Reweighted l1-norm Penalized LMS for Sparse Channel Estimation and Its
  Analysis</title><categories>cs.IT math.IT</categories><comments>28 pages, 4 figures, 1 table, Submitted to Signal Processing on June
  2013</comments><journal-ref>O. Taheri and S.A. Vorobyov, &quot;Reweighted l1-norm penalized LMS for
  sparse channel estimation and its analysis,&quot; Signal Processing, vol. 104, pp.
  70-79, Nov. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new reweighted l1-norm penalized least mean square (LMS) algorithm for
sparse channel estimation is proposed and studied in this paper. Since standard
LMS algorithm does not take into account the sparsity information about the
channel impulse response (CIR), sparsity-aware modifications of the LMS
algorithm aim at outperforming the standard LMS by introducing a penalty term
to the standard LMS cost function which forces the solution to be sparse. Our
reweighted l1-norm penalized LMS algorithm introduces in addition a reweighting
of the CIR coefficient estimates to promote a sparse solution even more and
approximate l0-pseudo-norm closer. We provide in depth quantitative analysis of
the reweighted l1-norm penalized LMS algorithm. An expression for the excess
mean square error (MSE) of the algorithm is also derived which suggests that
under the right conditions, the reweighted l1-norm penalized LMS algorithm
outperforms the standard LMS, which is expected. However, our quantitative
analysis also answers the question of what is the maximum sparsity level in the
channel for which the reweighted l1-norm penalized LMS algorithm is better than
the standard LMS. Simulation results showing the better performance of the
reweighted l1-norm penalized LMS algorithm compared to other existing LMS-type
algorithms are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3567</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3567</id><created>2014-01-15</created><authors><author><keyname>Khmou</keyname><forenames>Y.</forenames></author><author><keyname>Safi</keyname><forenames>S.</forenames></author></authors><title>2D Direction Of Arrival Estimation with Modified Propagator</title><categories>cs.IT math.IT stat.AP</categories><comments>4 pages, one latex file, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fast algorithm for the Direction Of Arrival (DOA) estimation
of radiating sources, based on partial covariance matrix and without eigende-
composition of incoming signals is extended to two dimensional problem of joint
azimuth and elevation estimation angles using Uniform Circular Array (UCA) in
case of non coherent narrowband signals. Simulation results are presented with
both Additive White Gaussian Noise (AWGN) and real symmetric Toeplitz noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3569</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3569</id><created>2014-01-15</created><authors><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author></authors><title>Efficient Strategies for Single/Multi-Target Jamming on MIMO Gaussian
  Channels</title><categories>cs.IT math.IT</categories><comments>24 pages, 5 figures, Submitted to IEEE Trans. Signal Processing on
  December 2013</comments><journal-ref>IEEE Trans. Signal Processing, vol. 63, no. 21, pp. 5821-5836,
  Nov. 2015</journal-ref><doi>10.1109/TSP.2015.2457398</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of jamming on multiple-input multiple-output (MIMO) Gaussian
channels is investigated in this paper. In the case of a single target
legitimate signal, we show that the existing result based on the simplification
of the system model by neglecting the jamming channel leads to losing important
insights regarding the effect of jamming power and jamming channel on the
jamming strategy. We find a closed-form optimal solution for the problem under
a positive semi-definite (PSD) condition without considering simplifications in
the model. If the condition is not satisfied and the optimal solution may not
exist in closed-form, we find the optimal solution using a numerical method and
also propose a suboptimal solution in closed-form as a close approximation of
the optimal solution. Then, the possibility of extending the results to solve
the problem of multi-target jamming is investigated for four scenarios, i.e.,
multiple access channel, broadcasting channel, multiple transceiver pairs with
orthogonal transmissions, and multiple transceiver pairs with interference,
respectively. It is shown that the proposed numerical method can be extended to
all scenarios while the proposed closed-form solutions for jamming may be
applied in the scenarios of the multiple access channel and multiple
transceiver pairs with orthogonal transmissions. Simulation results verify the
effectiveness of the proposed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3579</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3579</id><created>2013-12-20</created><updated>2014-02-22</updated><authors><author><keyname>Yahya</keyname><forenames>Keyvan</forenames></author></authors><title>A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An
  Actor-Critic Approach</title><categories>cs.GT cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to find an algorithmic structure that affords to predict and
explain economical choice behaviour particularly under uncertainty(random
policies) by manipulating the prevalent Actor-Critic learning method to comply
with the requirements we have been entrusted ever since the field of
neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that
seem relevant to our discussion, we will try to outline some of the important
works which have so far been done to simulate choice making processes.
Concerning neurological findings that suggest the existence of two specific
functions that are executed through Basal Ganglia all the way up to sub-
cortical areas, namely 'rewards' and 'beliefs', we will offer a modified
version of actor/critic algorithm to shed a light on the relation between these
functions and most importantly resolve what is referred to as a challenge for
actor-critic algorithms, that is, the lack of inheritance or hierarchy which
avoids the system being evolved in continuous time tasks whence the convergence
might not be emerged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3580</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3580</id><created>2013-12-18</created><authors><author><keyname>Tavan</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Yates</keyname><forenames>Roy D.</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author></authors><title>Bits Through Bufferless Queues</title><categories>cs.IT math.IT</categories><comments>8 pages, 3 figures, accepted in 51st Annual Allerton Conference on
  Communication, Control, and Computing, University of Illinois, Monticello,
  Illinois, Oct 2-4, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity of a channel in which information is
conveyed by the timing of consecutive packets passing through a queue with
independent and identically distributed service times. Such timing channels are
commonly studied under the assumption of a work-conserving queue. In contrast,
this paper studies the case of a bufferless queue that drops arriving packets
while a packet is in service. Under this bufferless model, the paper provides
upper bounds on the capacity of timing channels and establishes achievable
rates for the case of bufferless M/M/1 and M/G/1 queues. In particular, it is
shown that a bufferless M/M/1 queue at worst suffers less than 10% reduction in
capacity when compared to an M/M/1 work-conserving queue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3582</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3582</id><created>2013-12-23</created><updated>2014-02-08</updated><authors><author><keyname>Bao</keyname><forenames>Xiaomin</forenames></author></authors><title>The equivalent identities of the MacWilliams identity for linear codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use derivatives to prove the equivalences between MacWilliams identity and
its four equivalent forms, and present new interpretations for the four
equivalent forms. Our results explicitly give out the relationships between
MacWilliams identity and its four equivalent forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3584</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3584</id><created>2013-11-20</created><authors><author><keyname>Kadir</keyname><forenames>Abdul</forenames></author><author><keyname>Nugroho</keyname><forenames>Lukito Edi</forenames></author><author><keyname>Susanto</keyname><forenames>Adhi</forenames></author><author><keyname>Santosa</keyname><forenames>Paulus Insap</forenames></author></authors><title>Experiments of Distance Measurements in a Foliage Plant Retrieval System</title><categories>cs.CV</categories><comments>14 pages, International Journal of Signal Processing, Image
  Processing and Pattern Recognition Vol. 5, No. 2, June, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of important components in an image retrieval system is selecting a
distance measure to compute rank between two objects. In this paper, several
distance measures were researched to implement a foliage plant retrieval
system. Sixty kinds of foliage plants with various leaf color and shape were
used to test the performance of 7 different kinds of distance measures: city
block distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2
statistics, Jensen Shannon divergence and Kullback Leibler divergence. The
results show that city block and Euclidean distance measures gave the best
performance among the others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3588</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3588</id><created>2014-01-15</created><updated>2014-02-12</updated><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames><affiliation>Graz University of Technology</affiliation></author><author><keyname>Bloem</keyname><forenames>Roderick</forenames><affiliation>Graz University of Technology</affiliation></author></authors><title>Parameterized Synthesis</title><categories>cs.LO</categories><comments>Extended version of TACAS 2012 paper, 29 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  13, 2014) lmcs:736</journal-ref><doi>10.2168/LMCS-10(1:12)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the synthesis problem for distributed architectures with a
parametric number of finite-state components. Parameterized specifications
arise naturally in a synthesis setting, but thus far it was unclear how to
detect realizability and how to perform synthesis in a parameterized setting.
Using a classical result from verification, we show that for a class of
specifications in indexed LTL\X, parameterized synthesis in token ring networks
is equivalent to distributed synthesis in a network consisting of a few copies
of a single process. Adapting a well-known result from distributed synthesis,
we show that the latter problem is undecidable. We describe a semi-decision
procedure for the parameterized synthesis problem in token rings, based on
bounded synthesis. We extend the approach to parameterized synthesis in
token-passing networks with arbitrary topologies, and show applicability on a
simple case study. Finally, we sketch a general framework for parameterized
synthesis based on cutoffs and other parameterized verification techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3590</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3590</id><created>2014-01-14</created><updated>2014-04-30</updated><authors><author><keyname>Mahmoud</keyname><forenames>Karim M.</forenames></author></authors><title>An Enhanced Method For Evaluating Automatic Video Summaries</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of automatic video summaries is a challenging problem. In the past
years, some evaluation methods are presented that utilize only a single feature
like color feature to detect similarity between automatic video summaries and
ground-truth user summaries. One of the drawbacks of using a single feature is
that sometimes it gives a false similarity detection which makes the assessment
of the quality of the generated video summary less perceptual and not accurate.
In this paper, a novel method for evaluating automatic video summaries is
presented. This method is based on comparing automatic video summaries
generated by video summarization techniques with ground-truth user summaries.
The objective of this evaluation method is to quantify the quality of video
summaries, and allow comparing different video summarization techniques
utilizing both color and texture features of the video frames and using the
Bhattacharya distance as a dissimilarity measure due to its advantages. Our
Experiments show that the proposed evaluation method overcomes the drawbacks of
other methods and gives a more perceptual evaluation of the quality of the
automatic video summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3592</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3592</id><created>2014-01-15</created><authors><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author></authors><title>Intelligent Systems for Information Security</title><categories>cs.NE cs.CR</categories><comments>PhD Thesis. Ain Shams University, Faculty of Engineering. Computer
  and Systems Eng. Dept., 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis aims to use intelligent systems to extend and improve performance
and security of cryptographic techniques. Genetic algorithms framework for
cryptanalysis problem is addressed. A novel extension to the differential
cryptanalysis using genetic algorithm is proposed and a fitness measure based
on the differential characteristics of the cipher being attacked is also
proposed. The complexity of the proposed attack is shown to be less than
quarter of normal differential cryptanalysis of the same cipher by applying the
proposed attack to both the basic Substitution Permutation Network and the
Feistel Network. The basic models of modern block ciphers are attacked instead
of actual cipher to prove that the attack is applicable to other ciphers
vulnerable to differential cryptanalysis. A new attack for block cipher based
on the ability of neural networks to perform an approximation of mapping is
proposed. A complete problem formulation is explained and implementation of the
attack on some hypothetical Feistel cipher not vulnerable to differential or
linear attacks is presented. A new block cipher based on the neural networks is
proposed. A complete cipher structure is given and a key scheduling is also
shown. The main properties of neural network being able to perform mapping
between large dimension domains in a very fast and a very small memory compared
to S-Boxes is used as a base for the cipher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3607</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3607</id><created>2014-01-15</created><updated>2014-02-07</updated><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>A Brief History of Learning Classifier Systems: From CS-1 to XCS</title><categories>cs.NE cs.LG</categories><comments>37 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Learning Classifier Systems can be characterized by their use of rule
accuracy as the utility metric for the search algorithm(s) discovering useful
rules. Such searching typically takes place within the restricted space of
co-active rules for efficiency. This paper gives an historical overview of the
evolution of such systems up to XCS, and then some of the subsequent
developments of XCS to different types of learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3613</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3613</id><created>2014-01-15</created><updated>2014-07-30</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Turing Minimalism and the Emergence of Complexity</title><categories>cs.CC cs.IT math.IT</categories><comments>As accepted The Rutherford Journal - The New Zealand Journal for the
  History and Philosophy of Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not only did Turing help found one of the most exciting areas of modern
science (computer science), but it may be that his contribution to our
understanding of our physical reality is greater than we had hitherto supposed.
Here I explore the path that Alan Turing would have certainly liked to follow,
that of complexity science, which was launched in the wake of his seminal work
on computability and structure formation. In particular, I will explain how the
theory of algorithmic probability based on Turing's universal machine can also
explain how structure emerges at the most basic level, hence reconnecting two
of Turing's most cherished topics: computation and pattern formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3614</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3614</id><created>2014-01-15</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Reflective and Refractive Variables: A Model for Effective and
  Maintainable Adaptive-and-Dependable Software</title><categories>cs.SE</categories><comments>Proc. of the 33rd Euromicro Conference on Software Engineering and
  Advanced Applications (SEEA 2007), L\&quot;ubeck, Germany, August 27-31 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple and effective tool for the expression of tasks such as
cross-layer optimization strategies or sensors-related applications. The
approach is based on what we refer to as &quot;reflective and refractive variables&quot;.
Both types of variables are associated with external entities, e.g. sensors or
actuators. A reflective variable is a volatile variable, that is, a variable
that might be concurrently modified by multiple threads. A library of threads
is made available, each of which interfaces a set of sensors and continuously
update the value of a corresponding set of sensors. One such thread is &quot;cpu&quot;,
which exports the current level of usage of the local CPU as an integer between
0 and 100. This integer is reflected into the integer reflective variable cpu.
A refractive variable is a reflective variable that can be modified. Each
modification is caught and interpreted as a request to change the value of an
actuator. For instance, setting variable &quot;tcp_sendrate&quot; would request a
cross-layer adjustment to the thread interfacing the local TCP layer entity.
This allows express in an easy way complex operations in the application layer
of any programming language, e.g. plain old C. We describe our translator and
the work we are carrying out within PATS to build simple and powerful libraries
of scripts based on reflective and refractive variables, including robotics
applications and RFID tags processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3615</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3615</id><created>2013-12-17</created><authors><author><keyname>Hofmann</keyname><forenames>Johannes</forenames></author><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Performance Engineering for a Medical Imaging Application on the Intel
  Xeon Phi Accelerator</title><categories>cs.DC cs.CV cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the Xeon Phi, which is based on Intel's Many Integrated Cores
architecture, for its suitability to run the FDK algorithm--the most commonly
used algorithm to perform the 3D image reconstruction in cone-beam computed
tomography. We study the challenges of efficiently parallelizing the
application and means to enable sensible data sharing between threads despite
the lack of a shared last level cache. Apart from parallelization, SIMD
vectorization is critical for good performance on the Xeon Phi; we perform
various micro-benchmarks to investigate the platform's new set of vector
instructions and put a special emphasis on the newly introduced vector gather
capability. We refine a previous performance model for the application and
adapt it for the Xeon Phi to validate the performance of our optimized
hand-written assembly implementation, as well as the performance of several
different auto-vectorization approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3617</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3617</id><created>2014-01-15</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Power Allocation in MIMO Wiretap Channel with Statistical CSI and
  Finite-Alphabet Input</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of power allocation in MIMO wiretap
channel for secrecy in the presence of multiple eavesdroppers. Perfect
knowledge of the destination channel state information (CSI) and only the
statistical knowledge of the eavesdroppers CSI are assumed. We first consider
the MIMO wiretap channel with Gaussian input. Using Jensen's inequality, we
transform the secrecy rate max-min optimization problem to a single
maximization problem. We use generalized singular value decomposition and
transform the problem to a concave maximization problem which maximizes the sum
secrecy rate of scalar wiretap channels subject to linear constraints on the
transmit covariance matrix. We then consider the MIMO wiretap channel with
finite-alphabet input. We show that the transmit covariance matrix obtained for
the case of Gaussian input, when used in the MIMO wiretap channel with
finite-alphabet input, can lead to zero secrecy rate at high transmit powers.
We then propose a power allocation scheme with an additional power constraint
which alleviates this secrecy rate loss problem, and gives non-zero secrecy
rates at high transmit powers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3621</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3621</id><created>2014-01-15</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Service-oriented Communities: Visions and Contributions towards Social
  Organizations</title><categories>cs.CY</categories><comments>Proc. of the Fifth Int.l Workshop on MObile and NEtworking
  Technologies for social applications (MONET 2010), Lecture Notes in Computer
  Science, Volume 6428/2010, pp. 319-328, October 25-29, 2010, Crete, Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase of the populations, resources are becoming scarcer, and a
smarter way to make use of them becomes a vital necessity of our societies. On
the other hand, resource management is traditionally carried out through well
established organizations, policies, and regulations that are often considered
as impossible to restructure. Our position is that merely expanding the
traditional approaches might not be enough. Systems must be radically rethought
in order to achieve a truly effective and rational use of the available
resources. Classical concepts such as demand and supply need to be rethought as
well, as they operate artificial classifications that limit the true potential
of systems and organizations. In what follows we propose our vision to future,
&quot;smarter&quot; systems able to overcome the limitations of the status quo. Such
systems require what Boulding called &quot;gestalts,&quot; namely concepts able to
&quot;directing research towards the gaps which they reveal&quot;. In this paper we
elaborate on this and show how such gestalts can pave the way towards novel
reformulations of traditional services able to reach a better and more sensible
management of the available resources and cope with their scarcity. Our vision
of a Service-oriented Community is also introduced. We believe that such
communities---in heterarchical coexistence with traditional systems---provide
the necessary diversity and innovation orientation to prevent societal lock-ins
such as the ones we are experiencing in assisting our elderly ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3623</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3623</id><created>2014-01-15</created><authors><author><keyname>Mundra</keyname><forenames>Ankit</forenames></author><author><keyname>Rathee</keyname><forenames>Geetanjali</forenames></author><author><keyname>Chawla</keyname><forenames>Meenu</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author><author><keyname>Soni</keyname><forenames>Ashsutosh</forenames></author></authors><title>Transport Information System using Query Centric Cyber Physical Systems
  (QCPS)</title><categories>cs.OH</categories><comments>5 pages, 4 Figures</comments><journal-ref>International Journal of Computer Applications 85(3):12-16,
  January 2014. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/14820-3050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To incorporate the computation and communication with the physical world,
next generation architecture i.e. CPS is viewed as a new technology. To improve
the better interaction with the physical world or to perk up the electricity
delivery usage, various CPS based approaches have been introduced. Recently
several GPS equipped smart phones and sensor based frameworks have been
proposed which provide various services i.e. environment estimation, road
safety improvement but encounter certain limitations like elevated energy
consumption and high computation cost. To meet the high reliability and safety
requirements, this paper introduces a novel approach based on QCPS model which
provides several users services (discussed in this paper). Further, this paper
proposed a Transport Information System (TIS), which provide the communication
with lower cost overhead by arranging the similar sensors in the form of grids.
Each grid has a coordinator which interacts with cloud to process the user
query. In order to evaluate the performance of proposed approach we have
implemented a test bed of 16 wireless sensor nodes and have shown the
performance in terms of computation and communication cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3626</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3626</id><created>2014-01-15</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Modeling Concept Combinations in a Quantum-theoretic Framework</title><categories>cs.AI quant-ph</categories><comments>5 pages. arXiv admin note: substantial text overlap with
  arXiv:1311.6050</comments><journal-ref>Advances in Cognitive Neurodynamics (IV), pp 393-399, 2014</journal-ref><doi>10.1007/978-94-017-9548-7_55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present modeling for conceptual combinations which uses the mathematical
formalism of quantum theory. Our model faithfully describes a large amount of
experimental data collected by different scholars on concept conjunctions and
disjunctions. Furthermore, our approach sheds a new light on long standing
drawbacks connected with vagueness, or fuzziness, of concepts, and puts forward
a completely novel possible solution to the 'combination problem' in concept
theory. Additionally, we introduce an explanation for the occurrence of quantum
structures in the mechanisms and dynamics of concepts and, more generally, in
cognitive and decision processes, according to which human thought is a well
structured superposition of a 'logical thought' and a 'conceptual thought', and
the latter usually prevails over the former, at variance with some widespread
beliefs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3627</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3627</id><created>2014-01-15</created><authors><author><keyname>Gui</keyname><forenames>Ning</forenames></author><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>A Service-oriented Infrastructure Approach for Mutual Assistance
  Communities</title><categories>cs.CY</categories><comments>Proc. of the First IEEE WoWMoM Workshop on Adaptive and DependAble
  Mission- and bUsiness-critical mobile Systems (ADAMUS'07), Helsinki, Finland,
  June 18, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elder people are becoming a predominant aspect of our societies. As such,
solutions both efficacious and cost-effective need to be sought. This paper
proposes a service-oriented infrastructure approach to this problem. We propose
an open and integrated service infrastructure to orchestrate the available
resources (smart devices, professional carers, informal carers) to help elder
or disabled people. Main characteristic of our design is the explicitly support
of dynamically available service providers such as informal carers. By modeling
the service description as Semantic Web Services, the service request can
automatically be discovered, reasoned about and mapped onto the pool of
heterogeneous service providers. We expect our approach to be able to
efficiently utilize the available service resources, enrich the service
options, and best match the requirements of the requesters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3632</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3632</id><created>2014-01-15</created><updated>2015-09-22</updated><authors><author><keyname>Qamar</keyname><forenames>Shaan</forenames></author><author><keyname>Guhaniyogi</keyname><forenames>Rajarshi</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Bayesian Conditional Density Filtering</title><categories>stat.ML cs.LG stat.CO</categories><comments>41 pages, 7 figures, 12 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Conditional Density Filtering (C-DF) algorithm for efficient
online Bayesian inference. C-DF adapts MCMC sampling to the online setting,
sampling from approximations to conditional posterior distributions obtained by
propagating surrogate conditional sufficient statistics (a function of data and
parameter estimates) as new data arrive. These quantities eliminate the need to
store or process the entire dataset simultaneously and offer a number of
desirable features. Often, these include a reduction in memory requirements and
runtime and improved mixing, along with state-of-the-art parameter inference
and prediction. These improvements are demonstrated through several
illustrative examples including an application to high dimensional compressed
regression. Finally, we show that C-DF samples converge to the target posterior
distribution asymptotically as sampling proceeds and more data arrives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3654</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3654</id><created>2014-01-15</created><authors><author><keyname>Birgin</keyname><forenames>Ernesto G.</forenames></author><author><keyname>Feofiloff</keyname><forenames>Paulo</forenames></author><author><keyname>Fernandes</keyname><forenames>Cristina G.</forenames></author><author><keyname>de Melo</keyname><forenames>Everton L.</forenames></author><author><keyname>Oshiro</keyname><forenames>Marcio T. I.</forenames></author><author><keyname>Ronconi</keyname><forenames>D&#xe9;bora P.</forenames></author></authors><title>A MILP model for an extended version of the Flexible Job Shop Problem</title><categories>cs.DS cs.DM</categories><comments>15 pages, 2 figures, 4 tables. Optimization Letters, 2013</comments><doi>10.1007/S11590-013-0669-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A MILP model for an extended version of the Flexible Job Shop Scheduling
problem is proposed. The extension allows the precedences between operations of
a job to be given by an arbitrary directed acyclic graph rather than a linear
order. The goal is the minimization of the makespan. Theoretical and practical
advantages of the proposed model are discussed. Numerical experiments show the
performance of a commercial exact solver when applied to the proposed model.
The new model is also compared with a simple extension of the model described
by \&quot;Ozg\&quot;uven, \&quot;Ozbakir, and Yavuz (Mathematical models for job-shop
scheduling problems with routing and process plan flexibility, Applied
Mathematical Modelling, 34:1539--1548, 2010), using instances from the
literature and instances inspired by real data from the printing industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3659</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3659</id><created>2014-01-15</created><authors><author><keyname>Ahmadi</keyname><forenames>Hadi</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>Multipath Private Communication: An Information Theoretic Approach</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sending private messages over communication environments under surveillance
is an important challenge in communication security and has attracted
attentions of cryptographers through time. We believe that resources other than
cryptographic keys can be used for communication privacy. We consider private
message transmission (PMT) in an abstract multipath communication model between
two communicants, Alice and Bob, in the presence of an eavesdropper, Eve. Alice
and Bob have pre-shared keys and Eve is computationally unbounded. There are a
total of $n$ paths and the three parties can have simultaneous access to at
most $t_a$, $t_b$, and $t_e$ paths. The parties can switch their paths after
every $\lambda$ bits of communication. We study perfect (P)-PMT versus
asymptotically-perfect (AP)-PMT protocols. The former has zero tolerance of
transmission error and leakage, whereas the latter allows for positive error
and leakage that tend to zero as the message length increases. We derive the
necessary and sufficient conditions under which P-PMT and AP-PMT are possible.
We also introduce explicit P-PMT and AP-PMT constructions. Our results show
AP-PMT protocols attain much higher information rates than P-PMT ones.
Interestingly, AP-PMT is possible even in poorest condition where $t_a=t_b=1$
and $t_e=n-1$. It remains however an open question whether the derived rates
can be improved by more sophisticated AP-PMT protocols.
  We study applications of our results to private communication over the
real-life scenarios of multiple-frequency links and multiple-route networks. We
show practical examples of such scenarios that can be abstracted by the
multipath setting: Our results prove the possibility of keyless
information-theoretic private message transmission at rates $17\%$ and $20\%$
for the two example scenarios, respectively. We discuss open problems and
future work at the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3660</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3660</id><created>2014-01-15</created><authors><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Berioli</keyname><forenames>Matteo</forenames></author></authors><title>The Throughput of Slotted Aloha with Diversity</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for publications at 51st Annual Allerton Conference on
  Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a simple variation of classical Slotted Aloha is introduced
and analyzed. The enhancement relies on adding multiple receivers that gather
different observations of the packets transmitted by a user population in one
slot. For each observation, the packets transmitted in one slot are assumed to
be subject to independent on-off fading, so that each of them is either
completely faded, and then does not bring any power or interference at the
receiver, or it arrives unfaded, and then may or may not, collide with other
unfaded transmissions. With this model, a novel type of diversity is introduced
to the conventional SA scheme, leading to relevant throughput gains already for
moderate number of receivers. The analytical framework that we introduce allows
to derive closed-form expression of both throughput and packet loss rate an
arbitrary number of receivers, providing interesting hints on the key
trade-offs that characterize the system. We then focus on the problem of having
receivers forward the full set of collected packets to a final gateway using
the minimum possible amount of resources, i.e., avoiding delivery of duplicate
packets, without allowing any exchange of information among them. We derive
what is the minimum amount of resources needed and propose a scheme based on
random linear network coding that achieves asymptotically this bound without
the need for the receivers to coordinate among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3667</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3667</id><created>2014-01-15</created><updated>2014-09-30</updated><authors><author><keyname>Li</keyname><forenames>Tongxin</forenames></author><author><keyname>Chan</keyname><forenames>Chun Lam</forenames></author><author><keyname>Huang</keyname><forenames>Wenhao</forenames></author><author><keyname>Kaced</keyname><forenames>Tarik</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>Group Testing with Prior Statistics</title><categories>cs.IT math.IT</categories><comments>23 pages, 12 figures, presented in ISIT 2014, Honolulu</comments><journal-ref>Information Theory (ISIT), 2014 IEEE International Symposium on,
  2346-2350</journal-ref><doi>10.1109/ISIT.2014.6875253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new group testing model wherein each item is a binary random
variable defined by an a priori probability of being defective. We assume that
each probability is small and that items are independent, but not necessarily
identically distributed. The goal of group testing algorithms is to identify
with high probability the subset of defectives via non-linear (disjunctive)
binary measurements. Our main contributions are two classes of algorithms: (1)
adaptive algorithms with tests based either on a maximum entropy principle, or
on a Shannon-Fano/Huffman code; (2) non-adaptive algorithms. Under loose
assumptions and with high probability, our algorithms only need a number of
measurements that is close to the information-theoretic lower bound, up to an
explicitly-calculated universal constant factor. We provide simulations to
support our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3669</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3669</id><created>2014-01-15</created><authors><author><keyname>Tatar</keyname><forenames>D.</forenames></author><author><keyname>Lupea</keyname><forenames>M.</forenames></author><author><keyname>Kapetanios</keyname><forenames>E.</forenames></author></authors><title>Hrebs and Cohesion Chains as similar tools for semantic text properties
  research</title><categories>cs.CL</categories><comments>13 pages, KNOWLEDGE ENGINEERING: PRINCIPLES AND TECHNIQUES
  Proceedings of the International Conference on Knowledge Engineering,
  Principles and Techniques, KEPT 2013 Cluj-Napoca (Romania)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study it is proven that the Hrebs used in Denotation analysis of
texts and Cohesion Chains (de?ned as a fusion between Lexical Chains and
Coreference Chains) represent similar linguistic tools. This result gives us
the possibility to extend to Cohesion Chains (CCs) some important indicators
as, for example the Kernel of CCs, the topicality of a CC, text concentration,
CC-di?useness and mean di?useness of the text. Let us mention that nowhere in
the Lexical Chains or Coreference Chains literature these kinds of indicators
are introduced and used since now. Similarly, some applications of CCs in the
study of a text (as for example segmentation or summarization of a text) could
be realized starting from hrebs. As an illustration of the similarity between
Hrebs and CCs a detailed analyze of the poem &quot;Lacul&quot; by Mihai Eminescu is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3670</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3670</id><created>2014-01-15</created><authors><author><keyname>Paluch</keyname><forenames>Katarzyna</forenames></author></authors><title>Better Approximation Algorithms for Maximum Asymmetric Traveling
  Salesman and Shortest Superstring</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the maximum asymmetric traveling salesman problem (Max ATSP) we are given
a complete directed graph with nonnegative weights on the edges and we wish to
compute a traveling salesman tour of maximum weight. In this paper we give a
fast combinatorial $\frac 34$-approximation algorithm for Max ATSP. It is based
on a novel use of {\it half-edges}, matchings and a new method of edge
coloring. (A {\it half-edge} of edge $(u,v)$ is informally speaking &quot;either a
head or a tail of $(u,v)$&quot;.) The current best approximation algorithms for Max
ATSP, achieving the approximation guarantee of $\frac 23$, are due to Kaplan,
Lewenstein, Shafrir and Sviridenko and Elbassioni, Paluch, van Zuylen. Using a
recent result by Mucha, which states that an $\alpha$-approximation algorithm
for Max ATSP implies a $(2+\frac{11(1-\alpha)}{9-2\alpha})$-approximation
algorithm for the shortest superstring problem (SSP), we obtain also a $(2
\frac{11}{30} \approx 2,3667)$-approximation algorithm for SSP, beating the
previously best known (having approximation factor equal to $2 \frac{11}{23}
\approx 2,4782$.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3674</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3674</id><created>2014-01-15</created><authors><author><keyname>Guo</keyname><forenames>Zhili</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author></authors><title>Wireless Video Multicast with Cooperative and Incremental Transmission
  of Parity Packets</title><categories>cs.MM cs.IT cs.NI math.IT</categories><comments>11 pages in double-column IEEE journal style. Submitted to IEEE
  JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a cooperative multicast scheme that uses Randomized
Distributed Space Time Codes (R-DSTC), along with packet level Forward Error
Correction (FEC), is studied. Instead of sending source packets and/or parity
packets through two hops using R-DSTC as proposed in our prior work, the new
scheme delivers both source packets and parity packets using only one hop.
After the source station (access point, AP) first sends all the source packets,
the AP as well as all nodes that have received all source packets together send
the parity packets using R-DSTC. As more parity packets are transmitted, more
nodes can recover all source packets and join the parity packet transmission.
The process continues until all nodes acknowledge the receipt of enough packets
for recovering the source packets. For each given node distribution, the
optimum transmission rates for source and parity packets are determined such
that the video rate that can be sustained at all nodes is maximized. This new
scheme can support significantly higher video rates, and correspondingly higher
PSNR of decoded video, than the prior approaches. Three suboptimal approaches,
which do not require full information about user distribution or the feedback,
and hence are more feasible in practice are also presented. The proposed
suboptimal scheme with only the node count information and without feedback
still outperforms our prior approach that assumes full channel information and
no feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3675</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3675</id><created>2014-01-15</created><updated>2014-05-27</updated><authors><author><keyname>Mennle</keyname><forenames>Timo</forenames></author><author><keyname>Seuken</keyname><forenames>Sven</forenames></author></authors><title>An Axiomatic Approach to Characterizing and Relaxing Strategyproofness
  of One-sided Matching Mechanisms</title><categories>cs.GT</categories><comments>Working Paper. arXiv admin note: text overlap with arXiv:1303.2558</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study one-sided matching mechanisms where agents have vNM utility
functions and report ordinal preferences. We first show that in this domain
strategyproof mechanisms are characterized by three intuitive axioms: swap
consistency, weak invariance, and lower invariance. Our second result is that
dropping lower invariance leads to an interesting new relaxation of
strategyproofness, which we call partial strategyproofness. In particular, we
show that mechanisms are swap consistent and weakly invariant if and only if
they are strategyproof on a restricted domain where agents have sufficiently
different valuations for different objects. Furthermore, we show that this
domain restriction is maximal and use it to define a single-parameter measure
for the degree of strategyproofness of a manipulable mechanism. We also provide
an algorithm that computes this measure. Our new partial strategyproofness
concept finds applications in the incentive analysis of non-strategyproof
mechanisms, such as the Probabilistic Serial mechanism, different variants of
the Boston mechanism, and the construction of new hybrid mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3677</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3677</id><created>2014-01-15</created><authors><author><keyname>Deng</keyname><forenames>Na</forenames></author><author><keyname>Zhou</keyname><forenames>Wuyang</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>The Ginibre Point Process as a Model for Wireless Networks with
  Repulsion</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>29 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial structure of transmitters in wireless networks plays a key role
in evaluating the mutual interference and hence the performance. Although the
Poisson point process (PPP) has been widely used to model the spatial
configuration of wireless networks, it is not suitable for networks with
repulsion. The Ginibre point process (GPP) is one of the main examples of
determinantal point processes that can be used to model random phenomena where
repulsion is observed. Considering the accuracy, tractability and
practicability tradeoffs, we introduce and promote the $\beta$-GPP, an
intermediate class between the PPP and the GPP, as a model for wireless
networks when the nodes exhibit repulsion. To show that the model leads to
analytically tractable results in several cases of interest, we derive the mean
and variance of the interference using two different approaches: the Palm
measure approach and the reduced second moment approach, and then provide
approximations of the interference distribution by three known probability
density functions. Besides, to show that the model is relevant for cellular
systems, we derive the coverage probability of the typical user and also find
that the fitted $\beta$-GPP can closely model the deployment of actual base
stations in terms of the coverage probability and other statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3682</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3682</id><created>2014-01-15</created><updated>2015-09-28</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Cai</keyname><forenames>Minglai</forenames></author><author><keyname>Deppe</keyname><forenames>Christian</forenames></author></authors><title>Broadcast Classical-Quantum Capacity Region of Two-Phase Bidirectional
  Relaying Channel</title><categories>cs.IT math.IT math.QA quant-ph</categories><journal-ref>Quantum Information Processing: Volume 14, Issue 10 (2015), Page
  3879-3897</journal-ref><doi>10.1007/s11128-015-1065-2</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study a three-node quantum network which enables bidirectional
communication between two nodes with a half-duplex relay node. A
decode-and-forward protocol is used to perform the communication in two phases.
In the first phase, the messages of two nodes are transmitted to the relay
node. In the second phase, the relay node broadcasts a re-encoded composition
to the two nodes. We determine the capacity region of the broadcast phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3683</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3683</id><created>2014-01-15</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>G.</forenames></author></authors><title>$\mathcal R\!\raise2pt\hbox{$\varepsilon$}\!\hbox{$\mathcal L$}$: A
  Fault Tolerance Linguistic Structure for Distributed Applications</title><categories>cs.DC cs.SE</categories><comments>Proc. of the 9th IEEE Conf. and Workshop on Engineering of
  Computer-Based Systems (ECBS-2002), Lund, Sweden, 8-11 April, 2002; pp. 51-58</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The embedding of fault tolerance provisions into the application layer of a
programming language is a non-trivial task that has not found a satisfactory
solution yet. Such a solution is very important, and the lack of a simple,
coherent and effective structuring technique for fault tolerance has been
termed by researchers in this field as the &quot;software bottleneck of system
development&quot;. The aim of this paper is to report on the current status of a
novel fault tolerance linguistic structure for distributed applications
characterized by soft real-time requirements. A compliant prototype
architecture is also described. The key aspect of this structure is that it
allows to decompose the target fault-tolerant application into three distinct
components, respectively responsible for (1) the functional service, (2) the
management of the fault tolerance provisions, and (3) the adaptation to the
current environmental conditions. The paper also briefly mentions a few case
studies and preliminary results obtained exercising the prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3685</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3685</id><created>2014-01-15</created><authors><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Kumar</keyname><forenames>Mehul</forenames></author><author><keyname>Yadav</keyname><forenames>Pulkit</forenames></author></authors><title>Improved analysis of D2-sampling based PTAS for k-means and other
  Clustering problems</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1201.4206</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an improved analysis of the simple $D^2$-sampling based PTAS for the
$k$-means clustering problem given by Jaiswal, Kumar, and Sen (Algorithmica,
2013). The improvement on the running time is from $O\left(nd \cdot
2^{\tilde{O}(k^2/\epsilon)}\right)$ to $O\left(nd \cdot
2^{\tilde{O}(k/\epsilon)}\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3687</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3687</id><created>2014-01-15</created><updated>2014-12-29</updated><authors><author><keyname>Burke</keyname><forenames>Kyle</forenames></author></authors><title>$2^3$ Quantified Boolean Formula Games and Their Complexities</title><categories>cs.CC</categories><comments>14 pages, 0 figures, for Integers 2013 Conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider QBF, the Quantified Boolean Formula problem, as a combinatorial game
ruleset. The problem is rephrased as determining the winner of the game where
two opposing players take turns assigning values to boolean variables. In this
paper, three common variations of games are applied to create seven new games:
whether each player is restricted to where they may play, which values they may
set variables to, or the condition they are shooting for at the end of the
game. The complexity for determining which player can win is analyzed for all
games. Of the seven, two are trivially in P and the other five are
PSPACE-complete. These varying properties are common for combinatorial games;
reductions from these five hard games can simplify the process for showing the
PSPACE-hardness of other games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3690</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3690</id><created>2014-01-15</created><authors><author><keyname>Berg</keyname><forenames>Chris</forenames></author><author><keyname>Pons</keyname><forenames>Viviane</forenames></author><author><keyname>Scrimshaw</keyname><forenames>Travis</forenames></author><author><keyname>Striker</keyname><forenames>Jessica</forenames></author><author><keyname>Stump</keyname><forenames>Christian</forenames></author></authors><title>FindStat - the combinatorial statistics database</title><categories>math.CO cs.DB</categories><comments>2 pages, project description</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FindStat project at www.FindStat.org provides an online platform for
mathematicians, particularly for combinatorialists, to gather information about
combinatorial statistics and their relations. This outline provides an overview
over the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3700</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3700</id><created>2014-01-15</created><updated>2014-04-06</updated><authors><author><keyname>Horowitz</keyname><forenames>Matanya B.</forenames></author><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author><author><keyname>Burdick</keyname><forenames>Joel W.</forenames></author></authors><title>Convex Relaxations of SE(2) and SE(3) for Visual Pose Estimation</title><categories>cs.CV</categories><comments>ICRA 2014 Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method for rigid body pose estimation based on
spectrahedral representations of the tautological orbitopes of $SE(2)$ and
$SE(3)$. The approach can use dense point cloud data from stereo vision or an
RGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data.
The method is a convex relaxation of the classical pose estimation problem, and
is based on explicit linear matrix inequality (LMI) representations for the
convex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxed
pose estimation problem can be framed as a robust least squares problem with
the optimization variable constrained to these convex sets. Although this
formulation is a relaxation of the original problem, numerical experiments
indicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or
$SE(3)$ - in many interesting settings. We additionally show that this method
is guaranteed to be exact for a large class of pose estimation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3714</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3714</id><created>2014-01-15</created><updated>2014-02-19</updated><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Oliveira</keyname><forenames>Rafael</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Testing Equivalence of Polynomials under Shifts</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two polynomials $f, g \in \mathbb{F}[x_1, \ldots, x_n]$ are called
shift-equivalent if there exists a vector $(a_1, \ldots, a_n) \in \mathbb{F}^n$
such that the polynomial identity $f(x_1+a_1, \ldots, x_n+a_n) \equiv
g(x_1,\ldots,x_n)$ holds. Our main result is a new randomized algorithm that
tests whether two given polynomials are shift equivalent. Our algorithm runs in
time polynomial in the circuit size of the polynomials, to which it is given
black box access. This complements a previous work of Grigoriev (Theoretical
Computer Science, 1997) who gave a deterministic algorithm running in time
$n^{O(d)}$ for degree $d$ polynomials.
  Our algorithm uses randomness only to solve instances of the Polynomial
Identity Testing (PIT) problem. Hence, if one could de-randomize PIT (a
long-standing open problem in complexity) a de-randomization of our algorithm
would follow. This establishes an equivalence between de-randomizing
shift-equivalence testing and de-randomizing PIT (both in the black-box and the
white-box setting). For certain restricted models, such as Read Once Branching
Programs, we already obtain a deterministic algorithm using existing PIT
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3717</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3717</id><created>2014-01-15</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Physical Realizability and Mean Square Performance of Translation
  Invariant Networks of Interacting Linear Quantum Stochastic Systems</title><categories>cs.SY math.PR quant-ph</categories><comments>18 pages, 7 figures, submitted to MTNS 2014 on 29 November 2013</comments><msc-class>93A15, 81Q93, 81S25, 93E15, 82C10, 82C20, 82B10, 34L20, 37A30,
  42B10, 15B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with translation invariant networks of linear quantum
stochastic systems with nearest neighbour interaction mediated by boson fields.
The systems are associated with sites of a one-dimensional chain or a
multidimensional lattice and are governed by coupled linear quantum stochastic
differential equations (QSDEs). Such interconnections of open quantum systems
are relevant, for example, to the phonon theory of crystalline solids, atom
trapping in optical lattices and quantum metamaterials. In order to represent a
large-scale open quantum harmonic oscillator, the coefficients of the coupled
QSDEs must satisfy certain physical realizability conditions. These are
established in the form of matrix algebraic equations for the parameters of an
individual building block of the network and its interaction with the
neighbours and external fields. We also discuss the computation of mean square
performance functionals with block Toeplitz weighting matrices for such systems
in the thermodynamic limit per site for unboundedly increasing fragments of the
lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3733</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3733</id><created>2014-01-15</created><authors><author><keyname>Bennett</keyname><forenames>Ed</forenames></author><author><keyname>Del Debbio</keyname><forenames>Luigi</forenames></author><author><keyname>Jordan</keyname><forenames>Kirk</forenames></author><author><keyname>Lucini</keyname><forenames>Biagio</forenames></author><author><keyname>Patella</keyname><forenames>Agostino</forenames></author><author><keyname>Pica</keyname><forenames>Claudio</forenames></author><author><keyname>Rago</keyname><forenames>Antonio</forenames></author></authors><title>BSMBench: a flexible and scalable supercomputer benchmark from
  computational particle physics</title><categories>cs.DC hep-lat</categories><comments>16 pages, 11 figures</comments><report-no>CP3-Origins-2014-001 DNRF90 &amp; DIAS-2014-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benchmarking plays a central role in the evaluation of High Performance
Computing architectures. Several benchmarks have been designed that allow users
to stress various components of supercomputers. In order for the figures they
provide to be useful, benchmarks need to be representative of the most common
real-world scenarios. In this work, we introduce BSMBench, a benchmarking suite
derived from Monte Carlo code used in computational particle physics. The
advantage of this suite (which can be freely downloaded from
http://www.bsmbench.org/) over others is the capacity to vary the relative
importance of computation and communication. This enables the tests to simulate
various practical situations. To showcase BSMBench, we perform a wide range of
tests on various architectures, from desktop computers to state-of-the-art
supercomputers, and discuss the corresponding results. Possible future
directions of development of the benchmark are also outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3737</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3737</id><created>2014-01-15</created><authors><author><keyname>Glasmachers</keyname><forenames>Tobias</forenames></author><author><keyname>Dogan</keyname><forenames>&#xdc;r&#xfc;n</forenames></author></authors><title>Coordinate Descent with Online Adaptation of Coordinate Frequencies</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinate descent (CD) algorithms have become the method of choice for
solving a number of optimization problems in machine learning. They are
particularly popular for training linear models, including linear support
vector machine classification, LASSO regression, and logistic regression.
  We consider general CD with non-uniform selection of coordinates. Instead of
fixing selection frequencies beforehand we propose an online adaptation
mechanism for this important parameter, called the adaptive coordinate
frequencies (ACF) method. This mechanism removes the need to estimate optimal
coordinate frequencies beforehand, and it automatically reacts to changing
requirements during an optimization run.
  We demonstrate the usefulness of our ACF-CD approach for a variety of
optimization problems arising in machine learning contexts. Our algorithm
offers significant speed-ups over state-of-the-art training methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3753</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3753</id><created>2013-10-31</created><updated>2015-03-06</updated><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Parizi</keyname><forenames>Mani Bastani</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>LLR-based Successive Cancellation List Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing in September 2014
  -- Revised in March 2015. An earlier version of this work has been presented
  at the 39th International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP'2014)</comments><doi>10.1109/TSP.2015.2439211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that successive cancellation list decoding can be formulated
exclusively using log-likelihood ratios. In addition to numerical stability,
the log-likelihood ratio based formulation has useful properties which simplify
the sorting step involved in successive cancellation list decoding. We propose
a hardware architecture of the successive cancellation list decoder in the
log-likelihood ratio domain which, compared to a log-likelihood domain
implementation, requires less irregular and smaller memories. This
simplification together with the gains in the metric sorter, lead to $56\%$ to
$137\%$ higher throughput per unit area than other recently proposed
architectures. We then evaluate the empirical performance of the CRC-aided
successive cancellation list decoder at different list sizes using different
CRCs and conclude that it is important to adapt the CRC length to the list size
in order to achieve the best error-rate performance of concatenated polar
codes. Finally, we synthesize conventional successive cancellation decoders at
large block-lengths with the same block-error probability as our proposed
CRC-aided successive cancellation list decoders to demonstrate that, while our
decoders have slightly lower throughput and larger area, they have a
significantly smaller decoding latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3758</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3758</id><created>2014-01-15</created><authors><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Luk&#xe1;&#x161;</forenames></author></authors><title>Decidability of the extension problem for maps into odd-dimensional
  spheres</title><categories>math.AT cs.CG</categories><comments>6 pages</comments><msc-class>Primary 55Q05, Secondary 55S35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, it was shown that the problem of existence of a continuous
map $X \to Y$ extending a given map $A \to Y$ defined on a subspace $A
\subseteq X$ is undecidable, even for $Y$ an even-dimensional sphere. In the
present paper, we prove that the same problem for $Y$ an odd-dimensional sphere
is decidable. More generally, the same holds for any $d$-connected target space
$Y$ whose homotopy groups $\pi_k Y$ are finite for $k&gt;2d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3760</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3760</id><created>2014-01-15</created><authors><author><keyname>Yang</keyname><forenames>Xiao</forenames></author><author><keyname>Barron</keyname><forenames>Andrew R.</forenames></author></authors><title>Large Alphabet Compression and Predictive Distributions through
  Poissonization and Tilting</title><categories>cs.IT math.IT stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a convenient strategy for coding and predicting
sequences of independent, identically distributed random variables generated
from a large alphabet of size $m$. In particular, the size of the sample is
allowed to be variable. The employment of a Poisson model and tilting method
simplifies the implementation and analysis through independence. The resulting
strategy is optimal within the class of distributions satisfying a moment
condition, and is close to optimal for the class of all i.i.d distributions on
strings of a given length. Moreover, the method can be used to code and predict
strings with a condition on the tail of the ordered counts. It can also be
applied to distributions in an envelope class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3762</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3762</id><created>2014-01-15</created><updated>2014-06-20</updated><authors><author><keyname>Ju</keyname><forenames>Andrew</forenames></author><author><keyname>Healy</keyname><forenames>Patrick</forenames></author></authors><title>An Experimental Evaluation of List Coloring Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The list coloring problem is a variant of vertex coloring where a vertex may
be colored only a color from a prescribed set. Several applications of vertex
coloring are more appropriately modelled as instances of list coloring and thus
we argue that it is an important problem to consider. Regardless of the
importance of list coloring, few published algorithms exist for it. In this
paper we review the only two existing ones we could find and propose an exact
branch and bound one. We conduct an experimental evaluation of the three
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3766</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3766</id><created>2014-01-15</created><updated>2014-01-29</updated><authors><author><keyname>Crubille</keyname><forenames>Raphaelle</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author></authors><title>On Probabilistic Applicative Bisimulation and Call-by-Value
  $\lambda$-Calculi (Long Version)</title><categories>cs.LO</categories><comments>30 pages</comments><acm-class>F.3.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic applicative bisimulation is a recently introduced coinductive
methodology for program equivalence in a probabilistic, higher-order, setting.
In this paper, the technique is applied to a typed, call-by-value,
lambda-calculus. Surprisingly, the obtained relation coincides with context
equivalence, contrary to what happens when call-by-name evaluation is
considered. Even more surprisingly, full-abstraction only holds in a symmetric
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3768</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3768</id><created>2014-01-15</created><authors><author><keyname>Samanthula</keyname><forenames>Bharath K.</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author></authors><title>Lightweight and Secure Two-Party Range Queries over Outsourced Encrypted
  Databases</title><categories>cs.CR</categories><comments>25 pages, 1 figure</comments><acm-class>D.4.6; E.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the many benefits of cloud computing, an entity may want to outsource
its data and their related analytics tasks to a cloud. When data are sensitive,
it is in the interest of the entity to outsource encrypted data to the cloud;
however, this limits the types of operations that can be performed on the cloud
side. Especially, evaluating queries over the encrypted data stored on the
cloud without the entity performing any computation and without ever decrypting
the data become a very challenging problem. In this paper, we propose solutions
to conduct range queries over outsourced encrypted data. The existing methods
leak valuable information to the cloud which can violate the security guarantee
of the underlying encryption schemes. In general, the main security primitive
used to evaluate range queries is secure comparison (SC) of encrypted integers.
However, we observe that the existing SC protocols are not very efficient. To
this end, we first propose a novel SC scheme that takes encrypted integers and
outputs encrypted comparison result. We empirically show its practical
advantage over the current state-of-the-art. We then utilize the proposed SC
scheme to construct two new secure range query protocols. Our protocols protect
data confidentiality, privacy of user's query, and also preserve the semantic
security of the encrypted data; therefore, they are more secure than the
existing protocols. Furthermore, our second protocol is lightweight at the user
end, and it can allow an authorized user to use any device with limited storage
and computing capability to perform the range queries over outsourced encrypted
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3781</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3781</id><created>2014-01-15</created><updated>2014-10-06</updated><authors><author><keyname>Kumagai</keyname><forenames>Wataru</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Random Number Conversion and LOCC Conversion via Restricted Storage</title><categories>quant-ph cs.IT math.IT</categories><comments>41 pages, 12 figures. Since the form of the function $A_{1,s_2,t_2}$
  was incorrect in the previous manuscript, it was modified. Some proofs were
  modified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider random number conversion (RNC) through random number storage with
restricted size. We clarify the relation between the performance of RNC and the
size of storage in the framework of first- and second-order asymptotics, and
derive their rate regions. Then, we show that the results for RNC with
restricted storage recover those for conventional RNC without storage in the
limit of storage size. To treat RNC via restricted storage, we introduce a new
kind of probability distributions named generalized Rayleigh-normal
distributions. Using the generalized Rayleigh-normal distributions, we can
describe the second-order asymptotic behaviour of RNC via restricted storage in
a unified manner. As an application to quantum information theory, we analyze
LOCC conversion via entanglement storage with restricted size. Moreover, we
derive the optimal LOCC compression rate under a constraint of conversion
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3785</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3785</id><created>2014-01-15</created><authors><author><keyname>Xu</keyname><forenames>Songcen</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Adaptive Link Selection Strategies for Distributed Estimation in
  Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose adaptive link selection strategies for distributed
estimation in diffusion-type wireless networks. We develop an exhaustive
search-based link selection algorithm and a sparsity-inspired link selection
algorithm that can exploit the topology of networks with poor-quality links. In
the exhaustive search-based algorithm, we choose the set of neighbors that
results in the smallest excess mean square error (EMSE) for a specific node. In
the sparsity-inspired link selection algorithm, a convex regularization is
introduced to devise a sparsity-inspired link selection algorithm. The proposed
algorithms have the ability to equip diffusion-type wireless networks and to
significantly improve their performance. Simulation results illustrate that the
proposed algorithms have lower EMSE values, a better convergence rate and
significantly improve the network performance when compared with existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3794</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3794</id><created>2014-01-15</created><updated>2014-07-26</updated><authors><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author><author><keyname>Maculan</keyname><forenames>Nelson</forenames></author><author><keyname>Penna</keyname><forenames>Puca Huachi Vaz</forenames></author><author><keyname>Ochi</keyname><forenames>Luis Satoru</forenames></author></authors><title>Large neighborhoods with implicit customer selection for vehicle routing
  problems with profits</title><categories>cs.DS</categories><comments>Working Paper -- MIT, Revised, 34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider several Vehicle Routing Problems (VRP) with profits, which seek
to select a subset of customers, each one being associated with a profit, and
to design service itineraries. When the sum of profits is maximized under
distance constraints, the problem is usually called team orienteering problem.
The capacitated profitable tour problem seeks to maximize profits minus travel
costs under capacity constraints. Finally, in the VRP with private fleet and
common carrier, some customers can be delegated to an external carrier subject
to a cost. Three families of combined decisions must be taken: customers
selection, assignment to vehicles, and sequencing of deliveries for each route.
  We propose a new neighborhood search for these problems which explores an
exponential number of solutions in pseudo polynomial time. The search is
conducted with standard VRP neighborhoods on an &quot;exhaustive&quot; solution
representation, visiting all customers. Since visiting all customers is usually
infeasible or sub-optimal, an efficient &quot;Select&quot; algorithm, based on resource
constrained shortest paths, is repeatedly used on any new route to find the
optimal subsequence of visits to customers. The good performance of these
neighborhood structures is demonstrated by extensive computational experiments
with a local search, an iterated local search and a hybrid genetic algorithm.
Intriguingly, even a local-improvement method to the first local optimum of
this neighborhood achieves an average gap of 0.09% on classic team orienteering
benchmark instances, rivaling with the current state-of-the-art metaheuristics.
Promising research avenues on hybridizations with more standard routing
neighborhoods are also open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3801</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3801</id><created>2014-01-15</created><updated>2015-03-14</updated><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Finite-length Analysis on Tail probability for Markov Chain and
  Application to Simple Hypothesis Testing</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using terminologies of information geometry, we derive upper and lower bounds
of the tail probability of the sample mean. Employing these bounds, we obtain
upper and lower bounds of the minimum error probability of the 2nd kind of
error under the exponential constraint for the error probability of the 1st
kind of error in a simple hypothesis testing for a finite-length Markov chain,
which yields the Hoeffding type bound. For these derivations, we derive upper
and lower bounds of cumulant generating function for Markov chain. As a
byproduct, we obtain another simple proof of central limit theorem for Markov
chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3807</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3807</id><created>2014-01-15</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>On the Existence of MDS Codes Over Small Fields With Constrained
  Generator Matrices</title><categories>cs.IT cs.DM math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence over small fields of Maximum Distance Separable (MDS)
codes with generator matrices having specified supports (i.e. having specified
locations of zero entries). This problem unifies and simplifies the problems
posed in recent works of Yan and Sprintson (NetCod'13) on weakly secure
cooperative data exchange, of Halbawi et al. (arxiv'13) on distributed
Reed-Solomon codes for simple multiple access networks, and of Dau et al.
(ISIT'13) on MDS codes with balanced and sparse generator matrices. We
conjecture that there exist such $[n,k]_q$ MDS codes as long as $q \geq n + k -
1$, if the specified supports of the generator matrices satisfy the so-called
MDS condition, which can be verified in polynomial time. We propose a
combinatorial approach to tackle the conjecture, and prove that the conjecture
holds for a special case when the sets of zero coordinates of rows of the
generator matrix share with each other (pairwise) at most one common element.
Based on our numerical result, the conjecture is also verified for all $k \leq
7$. Our approach is based on a novel generalization of the well-known Hall's
marriage theorem, which allows (overlapping) multiple representatives instead
of a single representative for each subset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3809</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3809</id><created>2014-01-15</created><updated>2014-04-07</updated><authors><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>An Information-Spectrum Approach to Weak Variable-Length Source Coding
  with Side-Information</title><categories>cs.IT math.IT</categories><comments>54 pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies variable-length (VL) source coding of general sources with
side-information. Novel one-shot coding theorems for coding with common
side-information available at the encoder and the decoder and Slepian- Wolf
(SW) coding (i.e., with side-information only at the decoder) are given, and
then, are applied to asymptotic analyses of these coding problems. Especially,
a general formula for the infimum of the coding rate asymptotically achievable
by weak VL-SW coding (i.e., VL-SW coding with vanishing error probability) is
derived. Further, the general formula is applied to investigating weak VL-SW
coding of mixed sources. Our results derive and extend several known results on
SW coding and weak VL coding, e.g., the optimal achievable rate of VL-SW coding
for mixture of i.i.d. sources is given for countably infinite alphabet case
with mild condition. In addition, the usefulness of the encoder
side-information is investigated. Our result shows that if the encoder
side-information is useless in weak VL coding then it is also useless even in
the case where the error probability may be positive asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3814</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3814</id><created>2014-01-15</created><updated>2015-05-05</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Information Geometry Approach to Parameter Estimation in Markov Chains</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>Appendix D is added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the parameter estimation of Markov chain when the unknown
transition matrix belongs to an exponential family of transition matrices.
Then, we show that the sample mean of the generator of the exponential family
is an asymptotically efficient estimator. Further, we also define a curved
exponential family of transition matrices. Using a transition matrix version of
the Pythagorean theorem, we give an asymptotically efficient estimator for a
curved exponential family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3815</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3815</id><created>2014-01-15</created><authors><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>Khan</keyname><forenames>M. Junaid</forenames></author></authors><title>On Swarm Stability of Linear Time-Invariant Descriptor Compartmental
  Networks</title><categories>cs.SY</categories><report-no>2015</report-no><doi>10.1049/iet-cta.2014.0130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm stability is concerned for descriptor compartmental networks with
linear time-invariant protocol. Compartmental network is a specific type of
dynamical multi-agent system. Necessary and sufficient conditions for both
consensus and critical swarm stability are presented, which require a joint
matching between the interactive dynamics of nearest neighboring vertices and
the Laplacian spectrum of the overall network topology. Three numerical
instances are illustrated to verify the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3818</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3818</id><created>2014-01-15</created><authors><author><keyname>Sun</keyname><forenames>Xiaoxia</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Structured Priors for Sparse-Representation-Based Hyperspectral Image
  Classification</title><categories>cs.CV cs.LG stat.ML</categories><comments>IEEE Geoscience and Remote Sensing Letter</comments><doi>10.1109/LGRS.2013.2290531</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Pixel-wise classification, where each pixel is assigned to a predefined
class, is one of the most important procedures in hyperspectral image (HSI)
analysis. By representing a test pixel as a linear combination of a small
subset of labeled pixels, a sparse representation classifier (SRC) gives rather
plausible results compared with that of traditional classifiers such as the
support vector machine (SVM). Recently, by incorporating additional structured
sparsity priors, the second generation SRCs have appeared in the literature and
are reported to further improve the performance of HSI. These priors are based
on exploiting the spatial dependencies between the neighboring pixels, the
inherent structure of the dictionary, or both. In this paper, we review and
compare several structured priors for sparse-representation-based HSI
classification. We also propose a new structured prior called the low rank
group prior, which can be considered as a modification of the low rank prior.
Furthermore, we will investigate how different structured priors improve the
result for the HSI classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3824</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3824</id><created>2014-01-15</created><authors><author><keyname>Wei</keyname><forenames>Xiaohan</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Power Aware Wireless File Downloading: A Constrained Restless Bandit
  Approach</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper treats power-aware throughput maximization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy and its effectiveness is shown via
simulation. For simple 3-user cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3825</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3825</id><created>2014-01-15</created><authors><author><keyname>van der Hoek</keyname><forenames>Wiebe</forenames></author><author><keyname>Walther</keyname><forenames>Dirk</forenames></author><author><keyname>Wooldridge</keyname><forenames>Michael</forenames></author></authors><title>Reasoning About the Transfer of Control</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  437-477, 2010</journal-ref><doi>10.1613/jair.2901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DCL-PC: a logic for reasoning about how the abilities of agents
and coalitions of agents are altered by transferring control from one agent to
another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about
cooperation in which the abilities of agents and coalitions of agents stem from
a distribution of atomic Boolean variables to individual agents -- the choices
available to a coalition correspond to assignments to the variables the
coalition controls. The basic modal constructs of DCL-PC are of the form
coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic
logic modalities in which atomic programs are of the form agent i gives control
of variable p to agent j; as usual in dynamic logic, these atomic programs may
be combined using sequence, iteration, choice, and test operators to form
complex programs. By combining such dynamic transfer programs with cooperation
modalities, it becomes possible to reason about how the power of agents and
coalitions is affected by the transfer of control. We give two alternative
semantics for the logic: a direct semantics, in which we capture the
distributions of Boolean variables to agents; and a more conventional Kripke
semantics. We prove that these semantics are equivalent, and then present an
axiomatization for the logic. We investigate the computational complexity of
model checking and satisfiability for DCL-PC, and show that both problems are
PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally,
we investigate the characterisation of control in DCL-PC. We distinguish
between first-order control -- the ability of an agent or coalition to control
some state of affairs through the assignment of values to the variables under
the control of the agent or coalition -- and second-order control -- the
ability of an agent to exert control over the control that other agents have by
transferring variables to other agents. We give a logical characterisation of
second-order control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3827</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3827</id><created>2014-01-15</created><authors><author><keyname>He</keyname><forenames>Ruijie</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author><author><keyname>Roy</keyname><forenames>Nicholas</forenames></author></authors><title>Efficient Planning under Uncertainty with Macro-actions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  523-570, 2011</journal-ref><doi>10.1613/jair.3171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding how to act in partially observable environments remains an active
area of research. Identifying good sequences of decisions is particularly
challenging when good control performance requires planning multiple steps into
the future in domains with many states. Towards addressing this challenge, we
present an online, forward-search algorithm called the Posterior Belief
Distribution (PBD). PBD leverages a novel method for calculating the posterior
distribution over beliefs that result after a sequence of actions is taken,
given the set of observation sequences that could be received during this
process. This method allows us to efficiently evaluate the expected reward of a
sequence of primitive actions, which we refer to as macro-actions. We present a
formal analysis of our approach, and examine its performance on two very large
simulation experiments: scientific exploration and a target monitoring domain.
We also demonstrate our algorithm being used to control a real robotic
helicopter in a target monitoring experiment, which suggests that our approach
has practical potential for planning in real-world, large partially observable
domains where a multi-step lookahead is required to achieve good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3829</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3829</id><created>2014-01-15</created><authors><author><keyname>Greenwald</keyname><forenames>Amy</forenames></author><author><keyname>Lee</keyname><forenames>Seong Jae</forenames></author><author><keyname>Naroditskiy</keyname><forenames>Victor</forenames></author></authors><title>RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel</title><categories>cs.GT cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  513-546, 2009</journal-ref><doi>10.1613/jair.2904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged
victorious in the travel division of the 2006 Trading Agent Competition in a
photo finish. At a high level, the design of many successful trading agents can
be summarized as follows: (i) price prediction: build a model of market prices;
and (ii) optimization: solve for an approximately optimal set of bids, given
this model. To predict, RoxyBot builds a stochastic model of market prices by
simulating simultaneous ascending auctions. To optimize, RoxyBot relies on the
sample average approximation method, a stochastic optimization technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3830</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3830</id><created>2014-01-15</created><authors><author><keyname>Andersen</keyname><forenames>Henrik Reif</forenames></author><author><keyname>Hadzic</keyname><forenames>Tarik</forenames></author><author><keyname>Pisinger</keyname><forenames>David</forenames></author></authors><title>Interactive Cost Configuration Over Decision Diagrams</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  99-139, 2010</journal-ref><doi>10.1613/jair.2905</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many AI domains such as product configuration, a user should interactively
specify a solution that must satisfy a set of constraints. In such scenarios,
offline compilation of feasible solutions into a tractable representation is an
important approach to delivering efficient backtrack-free user interaction
online. In particular,binary decision diagrams (BDDs) have been successfully
used as a compilation target for product and service configuration. In this
paper we discuss how to extend BDD-based configuration to scenarios involving
cost functions which express user preferences.
  We first show that an efficient, robust and easy to implement extension is
possible if the cost function is additive, and feasible solutions are
represented using multi-valued decision diagrams (MDDs). We also discuss the
effect on MDD size if the cost function is non-additive or if it is encoded
explicitly into MDD. We then discuss interactive configuration in the presence
of multiple cost functions. We prove that even in its simplest form,
multiple-cost configuration is NP-hard in the input MDD. However, for solving
two-cost configuration we develop a pseudo-polynomial scheme and a fully
polynomial approximation scheme. The applicability of our approach is
demonstrated through experiments over real-world configuration models and
product-catalogue datasets. Response times are generally within a fraction of a
second even for very large instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3831</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3831</id><created>2014-01-15</created><authors><author><keyname>Aras</keyname><forenames>Raghav</forenames></author><author><keyname>Dutech</keyname><forenames>Alain</forenames></author></authors><title>An Investigation into Mathematical Programming for Finite Horizon
  Decentralized POMDPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  329-396, 2010</journal-ref><doi>10.1613/jair.2915</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized planning in uncertain environments is a complex task generally
dealt with by using a decision-theoretic approach, mainly through the framework
of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs).
Although DEC-POMDPS are a general and powerful modeling tool, solving them is a
task with an overwhelming complexity that can be doubly exponential. In this
paper, we study an alternate formulation of DEC-POMDPs relying on a
sequence-form representation of policies. From this formulation, we show how to
derive Mixed Integer Linear Programming (MILP) problems that, once solved, give
exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be
derived either by using some combinatorial characteristics of the optimal
solutions of the DEC-POMDPs or by using concepts borrowed from game theory.
Through an experimental validation on classical test problems from the
DEC-POMDP literature, we compare our approach to existing algorithms. Results
show that mathematical programming outperforms dynamic programming but is less
efficient than forward search, except for some particular problems. The main
contributions of this work are the use of mathematical programming for
DEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions.
Besides, we argue that our alternate representation of DEC-POMDPs could be
helpful for designing novel algorithms looking for approximate solutions to
DEC-POMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3832</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3832</id><created>2014-01-15</created><authors><author><keyname>Michelson</keyname><forenames>Matthew</forenames></author><author><keyname>Knoblock</keyname><forenames>Craig A.</forenames></author></authors><title>Constructing Reference Sets from Unstructured, Ungrammatical Text</title><categories>cs.CL cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  189-221, 2010</journal-ref><doi>10.1613/jair.2937</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vast amounts of text on the Web are unstructured and ungrammatical, such as
classified ads, auction listings, forum postings, etc. We call such text
&quot;posts.&quot; Despite their inconsistent structure and lack of grammar, posts are
full of useful information. This paper presents work on semi-automatically
building tables of relational information, called &quot;reference sets,&quot; by
analyzing such posts directly. Reference sets can be applied to a number of
tasks such as ontology maintenance and information extraction. Our
reference-set construction method starts with just a small amount of background
knowledge, and constructs tuples representing the entities in the posts to form
a reference set. We also describe an extension to this approach for the special
case where even this small amount of background knowledge is impossible to
discover and use. To evaluate the utility of the machine-constructed reference
sets, we compare them to manually constructed reference sets in the context of
reference-set-based information extraction. Our results show the reference sets
constructed by our method outperform manually constructed reference sets. We
also compare the reference-set-based extraction approach using the
machine-constructed reference set to supervised extraction approaches using
generic features. These results demonstrate that using machine-constructed
reference sets outperforms the supervised methods, even though the supervised
methods require training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3833</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3833</id><created>2014-01-15</created><authors><author><keyname>Bidyuk</keyname><forenames>Bozhena</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Rollon</keyname><forenames>Emma</forenames></author></authors><title>Active Tuples-based Scheme for Bounding Posterior Beliefs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  335-371, 2010</journal-ref><doi>10.1613/jair.2945</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a scheme for computing lower and upper bounds on the
posterior marginals in Bayesian networks with discrete variables. Its power
lies in its ability to use any available scheme that bounds the probability of
evidence or posterior marginals and enhance its performance in an anytime
manner. The scheme uses the cutset conditioning principle to tighten existing
bounding schemes and to facilitate anytime behavior, utilizing a fixed number
of cutset tuples. The accuracy of the bounds improves as the number of used
cutset tuples increases and so does the computation time. We demonstrate
empirically the value of our scheme for bounding posterior marginals and
probability of evidence using a variant of the bound propagation algorithm as a
plug-in scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3834</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3834</id><created>2014-01-15</created><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Mechanisms for Multi-Unit Auctions</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  85-98, 2010</journal-ref><doi>10.1613/jair.2950</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an incentive-compatible polynomial-time approximation scheme for
multi-unit auctions with general k-minded player valuations. The mechanism
fully optimizes over an appropriately chosen sub-range of possible allocations
and then uses VCG payments over this sub-range. We show that obtaining a fully
polynomial-time incentive-compatible approximation scheme, at least using VCG
payments, is NP-hard. For the case of valuations given by black boxes, we give
a polynomial-time incentive-compatible 2-approximation mechanism and show that
no better is possible, at least using VCG payments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3835</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3835</id><created>2014-01-15</created><authors><author><keyname>Varzinczak</keyname><forenames>Ivan Jos&#xe9;</forenames></author></authors><title>On Action Theory Change</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  189-246, 2010</journal-ref><doi>10.1613/jair.2959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As historically acknowledged in the Reasoning about Actions and Change
community, intuitiveness of a logical domain description cannot be fully
automated. Moreover, like any other logical theory, action theories may also
evolve, and thus knowledge engineers need revision methods to help in
accommodating new incoming information about the behavior of actions in an
adequate manner. The present work is about changing action domain descriptions
in multimodal logic. Its contribution is threefold: first we revisit the
semantics of action theory contraction proposed in previous work, giving more
robust operators that express minimal change based on a notion of distance
between Kripke-models. Second we give algorithms for syntactical action theory
contraction and establish their correctness with respect to our semantics for
those action theories that satisfy a principle of modularity investigated in
previous work. Since modularity can be ensured for every action theory and, as
we show here, needs to be computed at most once during the evolution of a
domain description, it does not represent a limitation at all to the method
here studied. Finally we state AGM-like postulates for action theory
contraction and assess the behavior of our operators with respect to them.
Moreover, we also address the revision counterpart of action theory change,
showing that it benefits from our semantics for contraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3836</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3836</id><created>2014-01-15</created><authors><author><keyname>Zhao</keyname><forenames>Liyue</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Sukthankar</keyname><forenames>Gita</forenames></author></authors><title>An Active Learning Approach for Jointly Estimating Worker Performance
  and Annotation Reliability with Crowdsourced Data</title><categories>cs.LG cs.HC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing platforms offer a practical solution to the problem of
affordably annotating large datasets for training supervised classifiers.
Unfortunately, poor worker performance frequently threatens to compromise
annotation reliability, and requesting multiple labels for every instance can
lead to large cost increases without guaranteeing good results. Minimizing the
required training samples using an active learning selection procedure reduces
the labeling requirement but can jeopardize classifier training by focusing on
erroneous annotations. This paper presents an active learning approach in which
worker performance, task difficulty, and annotation reliability are jointly
estimated and used to compute the risk function guiding the sample selection
procedure. We demonstrate that the proposed approach, which employs active
learning with Bayesian networks, significantly improves training accuracy and
correctly ranks the expertise of unknown labelers in the presence of annotation
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3837</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3837</id><created>2014-01-15</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Mixed Strategies in Combinatorial Agency</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  339-369, 2010</journal-ref><doi>10.1613/jair.2961</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many multiagent domains a set of agents exert effort towards a joint
outcome, yet the individual effort levels cannot be easily observed. A typical
example for such a scenario is routing in communication networks, where the
sender can only observe whether the packet reached its destination, but often
has no information about the actions of the intermediate routers, which
influences the final outcome. We study a setting where a principal needs to
motivate a team of agents whose combination of hidden efforts stochastically
determines an outcome. In a companion paper we devise and study a basic
combinatorial agency model for this setting, where the principal is restricted
to inducing a pure Nash equilibrium. Here we study various implications of this
restriction. First, we show that, in contrast to the case of observable
efforts, inducing a mixed-strategies equilibrium may be beneficial for the
principal. Second, we present a sufficient condition for technologies for which
no gain can be generated. Third, we bound the principals gain for various
families of technologies. Finally, we study the robustness of mixed equilibria
to coalitional deviations and the computational hardness of the optimal mixed
equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3838</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3838</id><created>2014-01-15</created><authors><author><keyname>Cayrol</keyname><forenames>Claudette</forenames></author><author><keyname>de Saint-Cyr</keyname><forenames>Florence Dupin</forenames></author><author><keyname>Lagasquie-Schiex</keyname><forenames>Marie-Christine</forenames></author></authors><title>Change in Abstract Argumentation Frameworks: Adding an Argument</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  49-84, 2010</journal-ref><doi>10.1613/jair.2965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of change in an abstract argumentation
system. We focus on a particular change: the addition of a new argument which
interacts with previous arguments. We study the impact of such an addition on
the outcome of the argumentation system, more particularly on the set of its
extensions. Several properties for this change operation are defined by
comparing the new set of extensions to the initial one, these properties are
called structural when the comparisons are based on set-cardinality or
set-inclusion relations. Several other properties are proposed where
comparisons are based on the status of some particular arguments: the accepted
arguments; these properties refer to the evolution of this status during the
change, e.g., Monotony and Priority to Recency. All these properties may be
more or less desirable according to specific applications. They are studied
under two particular semantics: the grounded and preferred semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3839</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3839</id><created>2014-01-15</created><authors><author><keyname>Richter</keyname><forenames>Silvia</forenames></author><author><keyname>Westphal</keyname><forenames>Matthias</forenames></author></authors><title>The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  127-177, 2010</journal-ref><doi>10.1613/jair.2972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LAMA is a classical planning system based on heuristic forward search. Its
core feature is the use of a pseudo-heuristic derived from landmarks,
propositional formulas that must be true in every solution of a planning task.
LAMA builds on the Fast Downward planning system, using finite-domain rather
than binary state variables and multi-heuristic search. The latter is employed
to combine the landmark heuristic with a variant of the well-known FF
heuristic. Both heuristics are cost-sensitive, focusing on high-quality
solutions in the case where actions have non-uniform cost. A weighted A* search
is used with iteratively decreasing weights, so that the planner continues to
search for plans of better quality until the search is terminated. LAMA showed
best performance among all planners in the sequential satisficing track of the
International Planning Competition 2008. In this paper we present the system in
detail and investigate which features of LAMA are crucial for its performance.
We present individual results for some of the domains used at the competition,
demonstrating good and bad cases for the techniques implemented in LAMA.
Overall, we find that using landmarks improves performance, whereas the
incorporation of action costs into the heuristic estimators proves not to be
beneficial. We show that in some domains a search that ignores cost solves far
more problems, raising the question of how to deal with action costs more
effectively in the future. The iterated weighted A* search greatly improves
results, and shows synergy effects with the use of landmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3840</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3840</id><created>2014-01-15</created><authors><author><keyname>Wittocx</keyname><forenames>Johan</forenames></author><author><keyname>Mari&#xeb;n</keyname><forenames>Maarten</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Grounding FO and FO(ID) with Bounds</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  223-269, 2010</journal-ref><doi>10.1613/jair.2980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grounding is the task of reducing a first-order theory and finite domain to
an equivalent propositional theory. It is used as preprocessing phase in many
logic-based reasoning systems. Such systems provide a rich first-order input
language to a user and can rely on efficient propositional solvers to perform
the actual reasoning. Besides a first-order theory and finite domain, the input
for grounders contains in many applications also additional data. By exploiting
this data, the size of the grounders output can often be reduced significantly.
A common practice to improve the efficiency of a grounder in this context is by
manually adding semantically redundant information to the input theory,
indicating where and when the grounder should exploit the data. In this paper
we present a method to compute and add such redundant information
automatically. Our method therefore simplifies the task of writing input
theories that can be grounded efficiently by current systems. We first present
our method for classical first-order logic (FO) theories. Then we extend it to
FO(ID), the extension of FO with inductive definitions, which allows for more
concise and comprehensive input theories. We discuss implementation issues and
experimentally validate the practical applicability of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3841</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3841</id><created>2014-01-15</created><authors><author><keyname>Riedl</keyname><forenames>Mark Owen</forenames></author><author><keyname>Young</keyname><forenames>Robert Michael</forenames></author></authors><title>Narrative Planning: Balancing Plot and Character</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  217-268, 2010</journal-ref><doi>10.1613/jair.2989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Narrative, and in particular storytelling, is an important part of the human
experience. Consequently, computational systems that can reason about narrative
can be more effective communicators, entertainers, educators, and trainers. One
of the central challenges in computational narrative reasoning is narrative
generation, the automated creation of meaningful event sequences. There are
many factors -- logical and aesthetic -- that contribute to the success of a
narrative artifact. Central to this success is its understandability. We argue
that the following two attributes of narratives are universal: (a) the logical
causal progression of plot, and (b) character believability. Character
believability is the perception by the audience that the actions performed by
characters do not negatively impact the audiences suspension of disbelief.
Specifically, characters must be perceived by the audience to be intentional
agents. In this article, we explore the use of refinement search as a technique
for solving the narrative generation problem -- to find a sound and believable
sequence of character actions that transforms an initial world state into a
world state in which goal propositions hold. We describe a novel refinement
search planning algorithm -- the Intent-based Partial Order Causal Link (IPOCL)
planner -- that, in addition to creating causally sound plot progression,
reasons about character intentionality by identifying possible character goals
that explain their actions and creating plan structures that explain why those
characters commit to their goals. We present the results of an empirical
evaluation that demonstrates that narrative plans generated by the IPOCL
algorithm support audience comprehension of character intentions better than
plans generated by conventional partial-order planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3842</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3842</id><created>2014-01-15</created><authors><author><keyname>Lesaint</keyname><forenames>David</forenames></author><author><keyname>Mehta</keyname><forenames>Deepak</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>Developing Approaches for Solving a Telecommunications Feature
  Subscription Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  271-305, 2010</journal-ref><doi>10.1613/jair.2992</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Call control features (e.g., call-divert, voice-mail) are primitive options
to which users can subscribe off-line to personalise their service. The
configuration of a feature subscription involves choosing and sequencing
features from a catalogue and is subject to constraints that prevent
undesirable feature interactions at run-time. When the subscription requested
by a user is inconsistent, one problem is to find an optimal relaxation, which
is a generalisation of the feedback vertex set problem on directed graphs, and
thus it is an NP-hard task. We present several constraint programming
formulations of the problem. We also present formulations using partial
weighted maximum Boolean satisfiability and mixed integer linear programming.
We study all these formulations by experimentally comparing them on a variety
of randomly generated instances of the feature subscription problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3843</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3843</id><created>2014-01-15</created><authors><author><keyname>Daniel</keyname><forenames>Kenny</forenames></author><author><keyname>Nash</keyname><forenames>Alex</forenames></author><author><keyname>Koenig</keyname><forenames>Sven</forenames></author><author><keyname>Felner</keyname><forenames>Ariel</forenames></author></authors><title>Theta*: Any-Angle Path Planning on Grids</title><categories>cs.CG cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  533-579, 2010</journal-ref><doi>10.1613/jair.2994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grids with blocked and unblocked cells are often used to represent terrain in
robotics and video games. However, paths formed by grid edges can be longer
than true shortest paths in the terrain since their headings are artificially
constrained. We present two new correct and complete any-angle path-planning
algorithms that avoid this shortcoming. Basic Theta* and Angle-Propagation
Theta* are both variants of A* that propagate information along grid edges
without constraining paths to grid edges. Basic Theta* is simple to understand
and implement, fast and finds short paths. However, it is not guaranteed to
find true shortest paths. Angle-Propagation Theta* achieves a better worst-case
complexity per vertex expansion than Basic Theta* by propagating angle ranges
when it expands vertices, but is more complex, not as fast and finds slightly
longer paths. We refer to Basic Theta* and Angle-Propagation Theta*
collectively as Theta*. Theta* has unique properties, which we analyze in
detail. We show experimentally that it finds shorter paths than both A* with
post-smoothed paths and Field D* (the only other version of A* we know of that
propagates information along grid edges without constraining paths to grid
edges) with a runtime comparable to that of A* on grids. Finally, we extend
Theta* to grids that contain unblocked cells with non-uniform traversal costs
and introduce variants of Theta* which provide different tradeoffs between path
length and runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3844</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3844</id><created>2014-01-15</created><authors><author><keyname>Engel</keyname><forenames>Yagil</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Multiattribute Auctions Based on Generalized Additive Independence</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  479-525, 2010</journal-ref><doi>10.1613/jair.3002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop multiattribute auctions that accommodate generalized additive
independent (GAI) preferences. We propose an iterative auction mechanism that
maintains prices on potentially overlapping GAI clusters of attributes, thus
decreases elicitation and computational burden, and creates an open competition
among suppliers over a multidimensional domain. Most significantly, the auction
is guaranteed to achieve surplus which approximates optimal welfare up to a
small additive factor, under reasonable equilibrium strategies of traders. The
main departure of GAI auctions from previous literature is to accommodate
non-additive trader preferences, hence allowing traders to condition their
evaluation of specific attributes on the value of other attributes. At the same
time, the GAI structure supports a compact representation of prices, enabling a
tractable auction process. We perform a simulation study, demonstrating and
quantifying the significant efficiency advantage of more expressive preference
modeling. We draw random GAI-structured utility functions with various internal
structures, generate additive functions that approximate the GAI utility, and
compare the performance of the auctions using the two representations. We find
that allowing traders to express existing dependencies among attributes
improves the economic efficiency of multiattribute auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3845</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3845</id><created>2014-01-15</created><authors><author><keyname>Wu</keyname><forenames>Jianhui</forenames></author><author><keyname>Durfee</keyname><forenames>Edmund H.</forenames></author></authors><title>Resource-Driven Mission-Phasing Techniques for Constrained Agents in
  Stochastic Environments</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  415-473, 2010</journal-ref><doi>10.1613/jair.3004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because an agents resources dictate what actions it can possibly take, it
should plan which resources it holds over time carefully, considering its
inherent limitations (such as power or payload restrictions), the competing
needs of other agents for the same resources, and the stochastic nature of the
environment. Such agents can, in general, achieve more of their objectives if
they can use --- and even create --- opportunities to change which resources
they hold at various times. Driven by resource constraints, the agents could
break their overall missions into an optimal series of phases, optimally
reconfiguring their resources at each phase, and optimally using their assigned
resources in each phase, given their knowledge of the stochastic environment.
In this paper, we formally define and analyze this constrained, sequential
optimization problem in both the single-agent and multi-agent contexts. We
present a family of mixed integer linear programming (MILP) formulations of
this problem that can optimally create phases (when phases are not predefined)
accounting for costs and limitations in phase creation. Because our
formulations multaneously also find the optimal allocations of resources at
each phase and the optimal policies for using the allocated resources at each
phase, they exploit structure across these coupled problems. This allows them
to find solutions significantly faster(orders of magnitude faster in larger
problems) than alternative solution techniques, as we demonstrate empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3846</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3846</id><created>2014-01-15</created><authors><author><keyname>Gange</keyname><forenames>Graeme</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter James</forenames></author><author><keyname>Lagoon</keyname><forenames>Vitaly</forenames></author></authors><title>Fast Set Bounds Propagation Using a BDD-SAT Hybrid</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  307-338, 2010</journal-ref><doi>10.1613/jair.3014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary Decision Diagram (BDD) based set bounds propagation is a powerful
approach to solving set-constraint satisfaction problems. However, prior BDD
based techniques in- cur the significant overhead of constructing and
manipulating graphs during search. We present a set-constraint solver which
combines BDD-based set-bounds propagators with the learning abilities of a
modern SAT solver. Together with a number of improvements beyond the basic
algorithm, this solver is highly competitive with existing propagation based
set constraint solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3847</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3847</id><created>2014-01-15</created><authors><author><keyname>Wu</keyname><forenames>Jia-Hong</forenames></author><author><keyname>Givan</keyname><forenames>Robert</forenames></author></authors><title>Automatic Induction of Bellman-Error Features for Probabilistic Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  687-755, 2010</journal-ref><doi>10.1613/jair.3021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain-specific features are important in representing problem structure
throughout machine learning and decision-theoretic planning. In planning, once
state features are provided, domain-independent algorithms such as approximate
value iteration can learn weighted combinations of those features that often
perform well as heuristic estimates of state value (e.g., distance to the
goal). Successful applications in real-world domains often require features
crafted by human experts. Here, we propose automatic processes for learning
useful domain-specific feature sets with little or no human intervention. Our
methods select and add features that describe state-space regions of high
inconsistency in the Bellman equation (statewise Bellman error) during
approximate value iteration. Our method can be applied using any
real-valued-feature hypothesis space and corresponding learning method for
selecting features from training sets of state-value pairs. We evaluate the
method with hypothesis spaces defined by both relational and propositional
feature languages, using nine probabilistic planning domains. We show that
approximate value iteration using a relational feature space performs at the
state-of-the-art in domain-independent stochastic relational planning. Our
method provides the first domain-independent approach that plays Tetris
successfully (without human-engineered features).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3848</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3848</id><created>2014-01-15</created><authors><author><keyname>Feldman</keyname><forenames>Alexander</forenames></author><author><keyname>Provan</keyname><forenames>Gregory</forenames></author><author><keyname>van Gemund</keyname><forenames>Arjan</forenames></author></authors><title>Approximate Model-Based Diagnosis Using Greedy Stochastic Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  371-413, 2010</journal-ref><doi>10.1613/jair.3025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which
trades off guarantees of computing minimal diagnoses for computational
efficiency. We empirically demonstrate, using the 74XXX and ISCAS-85 suites of
benchmark combinatorial circuits, that SAFARI achieves several
orders-of-magnitude speedup over two well-known deterministic algorithms, CDA*
and HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of
multiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is
optimal for a range of propositional fault models, such as the widely-used
weak-fault models (models with ignorance of abnormal behavior). We discuss the
optimality of SAFARI in a class of strong-fault circuit models with stuck-at
failure modes. By modeling the algorithm itself as a Markov chain, we provide
exact bounds on the minimality of the diagnosis computed. SAFARI also displays
strong anytime behavior, and will return a diagnosis after any non-trivial
inference time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3849</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3849</id><created>2014-01-15</created><authors><author><keyname>Rudolph</keyname><forenames>Sebastian</forenames></author><author><keyname>Glimm</keyname><forenames>Birte</forenames></author></authors><title>Nominals, Inverses, Counting, and Conjunctive Queries or: Why Infinity
  is your Friend!</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  429-481, 2010</journal-ref><doi>10.1613/jair.3029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description Logics are knowledge representation formalisms that provide, for
example, the logical underpinning of the W3C OWL standards. Conjunctive
queries, the standard query language in databases, have recently gained
significant attention as an expressive formalism for querying Description Logic
knowledge bases. Several different techniques for deciding conjunctive query
entailment are available for a wide range of DLs. Nevertheless, the combination
of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL
causes unsolvable problems for the techniques hitherto available. We tackle
this problem and present a decidability result for entailment of unions of
conjunctive queries in the DL ALCHOIQb that contains all three problematic
constructors simultaneously. Provided that queries contain only simple roles,
our result also shows decidability of entailment of (unions of) conjunctive
queries in the logic that underpins OWL 1 DL and we believe that the presented
results will pave the way for further progress towards conjunctive query
entailment decision procedures for the Description Logics underlying the OWL
standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3850</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3850</id><created>2014-01-15</created><authors><author><keyname>Feldman</keyname><forenames>Alexander</forenames></author><author><keyname>Provan</keyname><forenames>Gregory</forenames></author><author><keyname>van Gemund</keyname><forenames>Arjan</forenames></author></authors><title>A Model-Based Active Testing Approach to Sequential Diagnosis</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  301-334, 2010</journal-ref><doi>10.1613/jair.3031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based diagnostic reasoning often leads to a large number of diagnostic
hypotheses. The set of diagnoses can be reduced by taking into account extra
observations (passive monitoring), measuring additional variables (probing) or
executing additional tests (sequential diagnosis/test sequencing). In this
paper we combine the above approaches with techniques from Automated Test
Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework
called FRACTAL (FRamework for ACtive Testing ALgorithms). Apart from the inputs
and outputs that connect a system to its environment, in active testing we
consider additional input variables to which a sequence of test vectors can be
supplied. We address the computationally hard problem of computing optimal
control assignments (as defined in FRACTAL) in terms of a greedy approximation
algorithm called FRACTAL-G. We compare the decrease in the number of remaining
minimal cardinality diagnoses of FRACTAL-G to that of two more FRACTAL
algorithms: FRACTAL-ATPG and FRACTAL-P. FRACTAL-ATPG is based on ATPG and
sequential diagnosis while FRACTAL-P is based on probing and, although not an
active testing algorithm, provides a baseline for comparing the lower bound on
the number of reachable diagnoses for the FRACTAL algorithms. We empirically
evaluate the trade-offs of the three FRACTAL algorithms by performing extensive
experimentation on the ISCAS85/74XXX benchmark of combinational circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3851</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3851</id><created>2014-01-15</created><authors><author><keyname>Xu</keyname><forenames>Jing</forenames></author><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author></authors><title>Intrusion Detection using Continuous Time Bayesian Networks</title><categories>cs.AI cs.CR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  745-774, 2010</journal-ref><doi>10.1613/jair.3050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion detection systems (IDSs) fall into two high-level categories:
network-based systems (NIDS) that monitor network behaviors, and host-based
systems (HIDS) that monitor system calls. In this work, we present a general
technique for both systems. We use anomaly detection, which identifies patterns
not conforming to a historic norm. In both types of systems, the rates of
change vary dramatically over time (due to burstiness) and over components (due
to service difference). To efficiently model such systems, we use continuous
time Bayesian networks (CTBNs) and avoid specifying a fixed update interval
common to discrete-time models. We build generative models from the normal
training data, and abnormal behaviors are flagged based on their likelihood
under this norm. For NIDS, we construct a hierarchical CTBN model for the
network packet traces and use Rao-Blackwellized particle filtering to learn the
parameters. We illustrate the power of our method through experiments on
detecting real worms and identifying hosts on two publicly available network
traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel
learning method to deal with the finite resolution of system log file time
stamps, without losing the benefits of our continuous time model. We
demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3852</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3852</id><created>2014-01-15</created><authors><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Malizia</keyname><forenames>Enrico</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Non-Transferable Utility Coalitional Games via Mixed-Integer Linear
  Constraints</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  633-685, 2010</journal-ref><doi>10.1613/jair.3060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coalitional games serve the purpose of modeling payoff distribution problems
in scenarios where agents can collaborate by forming coalitions in order to
obtain higher worths than by acting in isolation. In the classical Transferable
Utility (TU) setting, coalition worths can be freely distributed amongst
agents. However, in several application scenarios, this is not the case and the
Non-Transferable Utility setting (NTU) must be considered, where additional
application-oriented constraints are imposed on the possible worth
distributions. In this paper, an approach to define NTU games is proposed which
is based on describing allowed distributions via a set of mixed-integer linear
constraints applied to an underlying TU game. It is shown that such games allow
non-transferable conditions on worth distributions to be specified in a natural
and succinct way. The properties and the relationships among the most prominent
solution concepts for NTU games that hold when they are applied on
(mixed-integer) constrained games are investigated. Finally, a thorough
analysis is carried out to assess the impact of issuing constraints on the
computational complexity of some of these solution concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3853</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3853</id><created>2014-01-15</created><authors><author><keyname>Katz</keyname><forenames>Michael</forenames></author><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author></authors><title>Implicit Abstraction Heuristics</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  51-126, 2010</journal-ref><doi>10.1613/jair.3063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-space search with explicit abstraction heuristics is at the state of
the art of cost-optimal planning. These heuristics are inherently limited,
nonetheless, because the size of the abstract space must be bounded by some,
even if a very large, constant. Targeting this shortcoming, we introduce the
notion of (additive) implicit abstractions, in which the planning task is
abstracted by instances of tractable fragments of optimal planning. We then
introduce a concrete setting of this framework, called fork-decomposition, that
is based on two novel fragments of tractable cost-optimal planning. The induced
admissible heuristics are then studied formally and empirically. This study
testifies for the accuracy of the fork decomposition heuristics, yet our
empirical evaluation also stresses the tradeoff between their accuracy and the
runtime complexity of computing them. Indeed, some of the power of the explicit
abstraction heuristics comes from precomputing the heuristic function offline
and then determining h(s) for each evaluated state s by a very fast lookup in a
database. By contrast, while fork-decomposition heuristics can be calculated in
polynomial time, computing them is far from being fast. To address this
problem, we show that the time-per-node complexity bottleneck of the
fork-decomposition heuristics can be successfully overcome. We demonstrate that
an equivalent of the explicit abstraction notion of a database exists for the
fork-decomposition abstractions as well, despite their exponential-size
abstract spaces. We then verify empirically that heuristic search with the
databased&quot; fork-decomposition heuristics favorably competes with the state of
the art of cost-optimal planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3854</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3854</id><created>2014-01-16</created><authors><author><keyname>Banerjee</keyname><forenames>Bonny</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>B.</forenames></author></authors><title>A Constraint Satisfaction Framework for Executing Perceptions and
  Actions in Diagrammatic Reasoning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  373-427, 2010</journal-ref><doi>10.1613/jair.3069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagrammatic reasoning (DR) is pervasive in human problem solving as a
powerful adjunct to symbolic reasoning based on language-like representations.
The research reported in this paper is a contribution to building a general
purpose DR system as an extension to a SOAR-like problem solving architecture.
The work is in a framework in which DR is modeled as a process where subtasks
are solved, as appropriate, either by inference from symbolic representations
or by interaction with a diagram, i.e., perceiving specified information from a
diagram or modifying/creating objects in a diagram in specified ways according
to problem solving needs. The perceptions and actions in most DR systems built
so far are hand-coded for the specific application, even when the rest of the
system is built using the general architecture. The absence of a general
framework for executing perceptions/actions poses as a major hindrance to using
them opportunistically -- the essence of open-ended search in problem solving.
Our goal is to develop a framework for executing a wide variety of specified
perceptions and actions across tasks/domains without human intervention. We
observe that the domain/task-specific visual perceptions/actions can be
transformed into domain/task-independent spatial problems. We specify a spatial
problem as a quantified constraint satisfaction problem in the real domain
using an open-ended vocabulary of properties, relations and actions involving
three kinds of diagrammatic objects -- points, curves, regions. Solving a
spatial problem from this specification requires computing the equivalent
simplified quantifier-free expression, the complexity of which is inherently
doubly exponential. We represent objects as configuration of simple elements to
facilitate decomposition of complex problems into simpler and similar
subproblems. We show that, if the symbolic solution to a subproblem can be
expressed concisely, quantifiers can be eliminated from spatial problems in
low-order polynomial time using similar previously solved subproblems. This
requires determining the similarity of two problems, the existence of a mapping
between them computable in polynomial time, and designing a memory for storing
previously solved problems so as to facilitate search. The efficacy of the idea
is shown by time complexity analysis. We demonstrate the proposed approach by
executing perceptions and actions involved in DR tasks in two army
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3855</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3855</id><created>2014-01-16</created><authors><author><keyname>Benisch</keyname><forenames>Michael</forenames></author><author><keyname>Davis</keyname><forenames>George B.</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Algorithms for Closed Under Rational Behavior (CURB) Sets</title><categories>cs.GT cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  513-534, 2010</journal-ref><doi>10.1613/jair.3070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a series of algorithms demonstrating that solutions according to
the fundamental game-theoretic solution concept of closed under rational
behavior (CURB) sets in two-player, normal-form games can be computed in
polynomial time (we also discuss extensions to n-player games). First, we
describe an algorithm that identifies all of a player's best responses
conditioned on the belief that the other player will play from within a given
subset of its strategy space. This algorithm serves as a subroutine in a series
of polynomial-time algorithms for finding all minimal CURB sets, one minimal
CURB set, and the smallest minimal CURB set in a game. We then show that the
complexity of finding a Nash equilibrium can be exponential only in the size of
a game's smallest CURB set. Related to this, we show that the smallest CURB set
can be an arbitrarily small portion of the game, but it can also be arbitrarily
larger than the supports of its only enclosed Nash equilibrium. We test our
algorithms empirically and find that most commonly studied academic games tend
to have either very large or very small minimal CURB sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3856</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3856</id><created>2014-01-16</created><authors><author><keyname>Chalkiadakis</keyname><forenames>Georgios</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Polukarov</keyname><forenames>Maria</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas Robert</forenames></author></authors><title>Cooperative Games with Overlapping Coalitions</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  179-216, 2010</journal-ref><doi>10.1613/jair.3075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the usual models of cooperative game theory, the outcome of a coalition
formation process is either the grand coalition or a coalition structure that
consists of disjoint coalitions. However, in many domains where coalitions are
associated with tasks, an agent may be involved in executing more than one
task, and thus may distribute his resources among several coalitions. To tackle
such scenarios, we introduce a model for cooperative games with overlapping
coalitions--or overlapping coalition formation (OCF) games. We then explore the
issue of stability in this setting. In particular, we introduce a notion of the
core, which generalizes the corresponding notion in the traditional
(non-overlapping) scenario. Then, under some quite general conditions, we
characterize the elements of the core, and show that any element of the core
maximizes the social welfare. We also introduce a concept of balancedness for
overlapping coalitional games, and use it to characterize coalition structures
that can be extended to elements of the core. Finally, we generalize the notion
of convexity to our setting, and show that under some natural assumptions
convex games have a non-empty core. Moreover, we introduce two alternative
notions of stability in OCF that allow a wider range of deviations, and explore
the relationships among the corresponding definitions of the core, as well as
the classic (non-overlapping) core and the Aubin core. We illustrate the
general properties of the three cores, and also study them from a computational
perspective, thus obtaining additional insights into their fundamental
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3857</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3857</id><created>2014-01-16</created><authors><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author><author><keyname>Bj&#xf6;rnsson</keyname><forenames>Yngvi</forenames></author><author><keyname>Lawrence</keyname><forenames>Ramon</forenames></author></authors><title>Case-Based Subgoaling in Real-Time Heuristic Search for Video Game
  Pathfinding</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  269-300, 2010</journal-ref><doi>10.1613/jair.3076</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time heuristic search algorithms satisfy a constant bound on the amount
of planning per action, independent of problem size. As a result, they scale up
well as problems become larger. This property would make them well suited for
video games where Artificial Intelligence controlled agents must react quickly
to user commands and to other agents actions. On the downside, real-time search
algorithms employ learning methods that frequently lead to poor solution
quality and cause the agent to appear irrational by re-visiting the same
problem states repeatedly. The situation changed recently with a new algorithm,
D LRTA*, which attempted to eliminate learning by automatically selecting
subgoals. D LRTA* is well poised for video games, except it has a complex and
memory-demanding pre-computation phase during which it builds a database of
subgoals. In this paper, we propose a simpler and more memory-efficient way of
pre-computing subgoals thereby eliminating the main obstacle to applying
state-of-the-art real-time search methods in video games. The new algorithm
solves a number of randomly chosen problems off-line, compresses the solutions
into a series of subgoals and stores them in a database. When presented with a
novel problem on-line, it queries the database for the most similar previously
solved case and uses its subgoals to solve the problem. In the domain of
pathfinding on four large video game maps, the new algorithm delivers solutions
eight times better while using 57 times less memory and requiring 14% less
pre-computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3858</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3858</id><created>2014-01-16</created><authors><author><keyname>de Bruijn</keyname><forenames>Jos</forenames></author><author><keyname>Heymans</keyname><forenames>Stijn</forenames></author></authors><title>Logical Foundations of RDF(S) with Datatypes</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  535-568, 2010</journal-ref><doi>10.1613/jair.3088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Resource Description Framework (RDF) is a Semantic Web standard that
provides a data language, simply called RDF, as well as a lightweight ontology
language, called RDF Schema. We investigate embeddings of RDF in logic and show
how standard logic programming and description logic technology can be used for
reasoning with RDF. We subsequently consider extensions of RDF with datatype
support, considering D entailment, defined in the RDF semantics specification,
and D* entailment, a semantic weakening of D entailment, introduced by ter
Horst. We use the embeddings and properties of the logics to establish novel
upper bounds for the complexity of deciding entailment. We subsequently
establish two novel lower bounds, establishing that RDFS entailment is
PTime-complete and that simple-D entailment is coNP-hard, when considering
arbitrary datatypes, both in the size of the entailing graph. The results
indicate that RDFS may not be as lightweight as one may expect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3859</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3859</id><created>2014-01-16</created><authors><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author></authors><title>A Utility-Theoretic Approach to Privacy in Online Services</title><categories>cs.AI cs.CR cs.CY</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  633-662, 2010</journal-ref><doi>10.1613/jair.3089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online offerings such as web search, news portals, and e-commerce
applications face the challenge of providing high-quality service to a large,
heterogeneous user base. Recent efforts have highlighted the potential to
improve performance by introducing methods to personalize services based on
special knowledge about users and their context. For example, a users
demographics, location, and past search and browsing may be useful in enhancing
the results offered in response to web search queries. However, reasonable
concerns about privacy by both users, providers, and government agencies acting
on behalf of citizens, may limit access by services to such information. We
introduce and explore an economics of privacy in personalization, where people
can opt to share personal information, in a standing or on-demand manner, in
return for expected enhancements in the quality of an online service. We focus
on the example of web search and formulate realistic objective functions for
search efficacy and privacy. We demonstrate how we can find a provably
near-optimal optimization of the utility-privacy tradeoff in an efficient
manner. We evaluate our methodology on data drawn from a log of the search
activity of volunteer participants. We separately assess users' preferences
about privacy and utility via a large-scale survey, aimed at eliciting
preferences about peoples' willingness to trade the sharing of personal data in
returns for gains in search efficiency. We show that a significant level of
personalization can be achieved using a relatively small amount of information
about users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3860</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3860</id><created>2014-01-16</created><authors><author><keyname>Lang</keyname><forenames>Tobias</forenames></author><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>Planning with Noisy Probabilistic Relational Rules</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  1-49, 2010</journal-ref><doi>10.1613/jair.3093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noisy probabilistic relational rules are a promising world model
representation for several reasons. They are compact and generalize over world
instantiations. They are usually interpretable and they can be learned
effectively from the action experiences in complex worlds. We investigate
reasoning with such rules in grounded relational domains. Our algorithms
exploit the compactness of rules for efficient and flexible decision-theoretic
planning. As a first approach, we combine these rules with the Upper Confidence
Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second
approach converts these rules into a structured dynamic Bayesian network
representation and predicts the effects of action sequences using approximate
inference and beliefs over world states. We evaluate the effectiveness of our
approaches for planning in a simulated complex 3D robot manipulation scenario
with an articulated manipulator and realistic physics and in domains of the
probabilistic planning competition. Empirical results show that our methods can
solve problems where existing methods fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3861</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3861</id><created>2014-01-16</created><authors><author><keyname>Burns</keyname><forenames>Ethan</forenames></author><author><keyname>Lemons</keyname><forenames>Sofia</forenames></author><author><keyname>Ruml</keyname><forenames>Wheeler</forenames></author><author><keyname>Zhou</keyname><forenames>Rong</forenames></author></authors><title>Best-First Heuristic Search for Multicore Machines</title><categories>cs.AI cs.DC</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  689-743, 2010</journal-ref><doi>10.1613/jair.3094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To harness modern multicore processors, it is imperative to develop parallel
versions of fundamental algorithms. In this paper, we compare different
approaches to parallel best-first search in a shared-memory setting. We present
a new method, PBNF, that uses abstraction to partition the state space and to
detect duplicate states without requiring frequent locking. PBNF allows
speculative expansions when necessary to keep threads busy. We identify and fix
potential livelock conditions in our approach, proving its correctness using
temporal logic. Our approach is general, allowing it to extend easily to
suboptimal and anytime heuristic search. In an empirical comparison on STRIPS
planning, grid pathfinding, and sliding tile puzzle problems using 8-core
machines, we show that A*, weighted A* and Anytime weighted A* implemented
using PBNF yield faster search than improved versions of previous parallel
search proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3862</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3862</id><created>2014-01-16</created><authors><author><keyname>Wang</keyname><forenames>Yonghong</forenames></author><author><keyname>Hang</keyname><forenames>Chung-Wei</forenames></author><author><keyname>Singh</keyname><forenames>Munindar P.</forenames></author></authors><title>A Probabilistic Approach for Maintaining Trust Based on Evidence</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  221-267, 2011</journal-ref><doi>10.1613/jair.3108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leading agent-based trust models address two important needs. First, they
show how an agent may estimate the trustworthiness of another agent based on
prior interactions. Second, they show how agents may share their knowledge in
order to cooperatively assess the trustworthiness of others. However, in
real-life settings, information relevant to trust is usually obtained
piecemeal, not all at once. Unfortunately, the problem of maintaining trust has
drawn little attention. Existing approaches handle trust updates in a
heuristic, not a principled, manner. This paper builds on a formal model that
considers probability and certainty as two dimensions of trust. It proposes a
mechanism using which an agent can update the amount of trust it places in
other agents on an ongoing basis. This paper shows via simulation that the
proposed approach (a) provides accurate estimates of the trustworthiness of
agents that change behavior frequently; and (b) captures the dynamic behavior
of the agents. This paper includes an evaluation based on a real dataset drawn
from Amazon Marketplace, a leading e-commerce site.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3863</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3863</id><created>2014-01-16</created><authors><author><keyname>J&#xe4;ger</keyname><forenames>Gerold</forenames></author><author><keyname>Zhang</keyname><forenames>Weixiong</forenames></author></authors><title>An Effective Algorithm for and Phase Transitions of the Directed
  Hamiltonian Cycle Problem</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  663-687, 2010</journal-ref><doi>10.1613/jair.3109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hamiltonian cycle problem (HCP) is an important combinatorial problem
with applications in many areas. It is among the first problems used for
studying intrinsic properties, including phase transitions, of combinatorial
problems. While thorough theoretical and experimental analyses have been made
on the HCP in undirected graphs, a limited amount of work has been done for the
HCP in directed graphs (DHCP). The main contribution of this work is an
effective algorithm for the DHCP. Our algorithm explores and exploits the close
relationship between the DHCP and the Assignment Problem (AP) and utilizes a
technique based on Boolean satisfiability (SAT). By combining effective
algorithms for the AP and SAT, our algorithm significantly outperforms previous
exact DHCP algorithms, including an algorithm based on the award-winning
Concorde TSP algorithm. The second result of the current study is an
experimental analysis of phase transitions of the DHCP, verifying and refining
a known phase transition of the DHCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3864</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3864</id><created>2014-01-16</created><authors><author><keyname>Zhou</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>A Logical Study of Partial Entailment</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  25-56, 2011</journal-ref><doi>10.1613/jair.3117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel logical notion--partial entailment--to propositional
logic. In contrast with classical entailment, that a formula P partially
entails another formula Q with respect to a background formula set \Gamma
intuitively means that under the circumstance of \Gamma, if P is true then some
&quot;part&quot; of Q will also be true. We distinguish three different kinds of partial
entailments and formalize them by using an extended notion of prime implicant.
We study their semantic properties, which show that, surprisingly, partial
entailments fail for many simple inference rules. Then, we study the related
computational properties, which indicate that partial entailments are
relatively difficult to be computed. Finally, we consider a potential
application of partial entailments in reasoning about rational agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3865</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3865</id><created>2014-01-16</created><authors><author><keyname>Tannier</keyname><forenames>Xavier</forenames></author><author><keyname>Muller</keyname><forenames>Philippe</forenames></author></authors><title>Evaluating Temporal Graphs Built from Texts via Transitive Reduction</title><categories>cs.CL cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  375-413, 2011</journal-ref><doi>10.1613/jair.3118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal information has been the focus of recent attention in information
extraction, leading to some standardization effort, in particular for the task
of relating events in a text. This task raises the problem of comparing two
annotations of a given text, because relations between events in a story are
intrinsically interdependent and cannot be evaluated separately. A proper
evaluation measure is also crucial in the context of a machine learning
approach to the problem. Finding a common comparison referent at the text level
is not obvious, and we argue here in favor of a shift from event-based measures
to measures on a unique textual object, a minimal underlying temporal graph, or
more formally the transitive reduction of the graph of relations between event
boundaries. We support it by an investigation of its properties on synthetic
data and on a well-know temporal corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3866</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3866</id><created>2014-01-16</created><authors><author><keyname>Geist</keyname><forenames>Christian</forenames></author><author><keyname>Endriss</keyname><forenames>Ulle</forenames></author></authors><title>Automated Search for Impossibility Theorems in Social Choice Theory:
  Ranking Sets of Objects</title><categories>cs.AI cs.LO cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  143-174, 2011</journal-ref><doi>10.1613/jair.3126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for using standard techniques from satisfiability
checking to automatically verify and discover theorems in an area of economic
theory known as ranking sets of objects. The key question in this area, which
has important applications in social choice theory and decision making under
uncertainty, is how to extend an agents preferences over a number of objects to
a preference relation over nonempty sets of such objects. Certain combinations
of seemingly natural principles for this kind of preference extension can
result in logical inconsistencies, which has led to a number of important
impossibility theorems. We first prove a general result that shows that for a
wide range of such principles, characterised by their syntactic form when
expressed in a many-sorted first-order logic, any impossibility exhibited at a
fixed (small) domain size will necessarily extend to the general case. We then
show how to formulate candidates for impossibility theorems at a fixed domain
size in propositional logic, which in turn enables us to automatically search
for (general) impossibility theorems using a SAT solver. When applied to a
space of 20 principles for preference extension familiar from the literature,
this method yields a total of 84 impossibility theorems, including both known
and nontrivial new results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3867</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3867</id><created>2014-01-16</created><authors><author><keyname>Hunter</keyname><forenames>Aaron</forenames></author><author><keyname>Delgrande</keyname><forenames>James P.</forenames></author></authors><title>Iterated Belief Change Due to Actions and Observations</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  269-304, 2011</journal-ref><doi>10.1613/jair.3132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In action domains where agents may have erroneous beliefs, reasoning about
the effects of actions involves reasoning about belief change. In this paper,
we use a transition system approach to reason about the evolution of an agents
beliefs as actions are executed. Some actions cause an agent to perform belief
revision while others cause an agent to perform belief update, but the
interaction between revision and update can be non-elementary. We present a set
of rationality properties describing the interaction between revision and
update, and we introduce a new class of belief change operators for reasoning
about alternating sequences of revisions and updates. Our belief change
operators can be characterized in terms of a natural shifting operation on
total pre-orderings over interpretations. We compare our approach with related
work on iterated belief change due to action, and we conclude with some
directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3868</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3868</id><created>2014-01-16</created><authors><author><keyname>Atserias</keyname><forenames>Albert</forenames></author><author><keyname>Fichte</keyname><forenames>Johannes Klaus</forenames></author><author><keyname>Thurley</keyname><forenames>Marc</forenames></author></authors><title>Clause-Learning Algorithms with Many Restarts and Bounded-Width
  Resolution</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  353-373, 2011</journal-ref><doi>10.1613/jair.3152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We offer a new understanding of some aspects of practical SAT-solvers that
are based on DPLL with unit-clause propagation, clause-learning, and restarts.
We do so by analyzing a concrete algorithm which we claim is faithful to what
practical solvers do. In particular, before making any new decision or restart,
the solver repeatedly applies the unit-resolution rule until saturation, and
leaves no component to the mercy of non-determinism except for some internal
randomness. We prove the perhaps surprising fact that, although the solver is
not explicitly designed for it, with high probability it ends up behaving as
width-k resolution after no more than O(n^2k+2) conflicts and restarts, where n
is the number of variables. In other words, width-k resolution can be thought
of as O(n^2k+2) restarts of the unit-resolution rule with learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3869</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3869</id><created>2014-01-16</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Paterson</keyname><forenames>Mike</forenames></author></authors><title>False-Name Manipulations in Weighted Voting Games</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  57-93, 2011</journal-ref><doi>10.1613/jair.3166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted voting is a classic model of cooperation among agents in
decision-making domains. In such games, each player has a weight, and a
coalition of players wins the game if its total weight meets or exceeds a given
quota. A players power in such games is usually not directly proportional to
his weight, and is measured by a power index, the most prominent among which
are the Shapley-Shubik index and the Banzhaf index.In this paper, we
investigate by how much a player can change his power, as measured by the
Shapley-Shubik index or the Banzhaf index, by means of a false-name
manipulation, i.e., splitting his weight among two or more identities. For both
indices, we provide upper and lower bounds on the effect of weight-splitting.
We then show that checking whether a beneficial split exists is NP-hard, and
discuss efficient algorithms for restricted cases of this problem, as well as
randomized algorithms for the general case. We also provide an experimental
evaluation of these algorithms. Finally, we examine related forms of
manipulative behavior, such as annexation, where a player subsumes other
players, or merging, where several players unite into one. We characterize the
computational complexity of such manipulations and provide limits on their
effects. For the Banzhaf index, we describe a new paradox, which we term the
Annexation Non-monotonicity Paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3870</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3870</id><created>2014-01-16</created><authors><author><keyname>Talvitie</keyname><forenames>Erik</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Learning to Make Predictions In Partially Observable Environments
  Without a Generative Model</title><categories>cs.LG cs.AI stat.ML</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  353-392, 2011</journal-ref><doi>10.1613/jair.3396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When faced with the problem of learning a model of a high-dimensional
environment, a common approach is to limit the model to make only a restricted
set of predictions, thereby simplifying the learning problem. These partial
models may be directly useful for making decisions or may be combined together
to form a more complete, structured model. However, in partially observable
(non-Markov) environments, standard model-learning methods learn generative
models, i.e. models that provide a probability distribution over all possible
futures (such as POMDPs). It is not straightforward to restrict such models to
make only certain predictions, and doing so does not always simplify the
learning problem. In this paper we present prediction profile models:
non-generative partial models for partially observable systems that make only a
given set of predictions, and are therefore far simpler than generative models
in some cases. We formalize the problem of learning a prediction profile model
as a transformation of the original model-learning problem, and show
empirically that one can learn prediction profile models that make a small set
of important predictions even in systems that are too complex for standard
generative models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3871</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3871</id><created>2014-01-16</created><authors><author><keyname>Fard</keyname><forenames>Mahdi Milani</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Non-Deterministic Policies in Markovian Decision Processes</title><categories>cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  1-24, 2011</journal-ref><doi>10.1613/jair.3175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markovian processes have long been used to model stochastic environments.
Reinforcement learning has emerged as a framework to solve sequential planning
and decision-making problems in such environments. In recent years, attempts
were made to apply methods from reinforcement learning to construct decision
support systems for action selection in Markovian environments. Although
conventional methods in reinforcement learning have proved to be useful in
problems concerning sequential decision-making, they cannot be applied in their
current form to decision support systems, such as those in medical domains, as
they suggest policies that are often highly prescriptive and leave little room
for the users input. Without the ability to provide flexible guidelines, it is
unlikely that these methods can gain ground with users of such systems. This
paper introduces the new concept of non-deterministic policies to allow more
flexibility in the users decision-making process, while constraining decisions
to remain near optimal solutions. We provide two algorithms to compute
non-deterministic policies in discrete domains. We study the output and running
time of these method on a set of synthetic and real-world problems. In an
experiment with human subjects, we show that humans assisted by hints based on
non-deterministic policies outperform both human-only and computer-only agents
in a web navigation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3872</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3872</id><created>2014-01-16</created><authors><author><keyname>Lecoutre</keyname><forenames>Christophe</forenames></author><author><keyname>Cardon</keyname><forenames>Stephane</forenames></author><author><keyname>Vion</keyname><forenames>Julien</forenames></author></authors><title>Second-Order Consistencies</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  175-219, 2011</journal-ref><doi>10.1613/jair.3180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a comprehensive study of second-order consistencies
(i.e., consistencies identifying inconsistent pairs of values) for constraint
satisfaction. We build a full picture of the relationships existing between
four basic second-order consistencies, namely path consistency (PC),
3-consistency (3C), dual consistency (DC) and 2-singleton arc consistency
(2SAC), as well as their conservative and strong variants. Interestingly, dual
consistency is an original property that can be established by using the
outcome of the enforcement of generalized arc consistency (GAC), which makes it
rather easy to obtain since constraint solvers typically maintain GAC during
search. On binary constraint networks, DC is equivalent to PC, but its
restriction to existing constraints, called conservative dual consistency
(CDC), is strictly stronger than traditional conservative consistencies derived
from path consistency, namely partial path consistency (PPC) and conservative
path consistency (CPC). After introducing a general algorithm to enforce strong
(C)DC, we present the results of an experimentation over a wide range of
benchmarks that demonstrate the interest of (conservative) dual consistency. In
particular, we show that enforcing (C)DC before search clearly improves the
performance of MAC (the algorithm that maintains GAC during search) on several
binary and non-binary structured problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3874</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3874</id><created>2014-01-16</created><authors><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Madhavan</keyname><forenames>Jayant</forenames></author><author><keyname>Halevy</keyname><forenames>Alon</forenames></author></authors><title>Identifying Aspects for Web-Search Queries</title><categories>cs.IR cs.DB</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  677-700, 2011</journal-ref><doi>10.1613/jair.3182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many web-search queries serve as the beginning of an exploration of an
unknown space of information, rather than looking for a specific web page. To
answer such queries effec- tively, the search engine should attempt to organize
the space of relevant information in a way that facilitates exploration. We
describe the Aspector system that computes aspects for a given query. Each
aspect is a set of search queries that together represent a distinct
information need relevant to the original search query. To serve as an
effective means to explore the space, Aspector computes aspects that are
orthogonal to each other and to have high combined coverage. Aspector combines
two sources of information to compute aspects. We discover candidate aspects by
analyzing query logs, and cluster them to eliminate redundancies. We then use a
mass-collaboration knowledge base (e.g., Wikipedia) to compute candidate
aspects for queries that occur less frequently and to group together aspects
that are likely to be &quot;semantically&quot; related. We present a user study that
indicates that the aspects we compute are rated favorably against three
competing alternatives -related searches proposed by Google, cluster labels
assigned by the Clusty search engine, and navigational searches proposed by
Bing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3875</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3875</id><created>2014-01-16</created><authors><author><keyname>Ruml</keyname><forenames>Wheeler</forenames></author><author><keyname>Do</keyname><forenames>Minh Binh</forenames></author><author><keyname>Zhou</keyname><forenames>Rong</forenames></author><author><keyname>Fromherz</keyname><forenames>Markus P. J.</forenames></author></authors><title>On-line Planning and Scheduling: An Application to Controlling Modular
  Printers</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  415-468, 2011</journal-ref><doi>10.1613/jair.3184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a case study of artificial intelligence techniques applied to the
control of production printing equipment. Like many other real-world
applications, this complex domain requires high-speed autonomous
decision-making and robust continual operation. To our knowledge, this work
represents the first successful industrial application of embedded
domain-independent temporal planning. Our system handles execution failures and
multi-objective preferences. At its heart is an on-line algorithm that combines
techniques from state-space planning and partial-order scheduling. We suggest
that this general architecture may prove useful in other applications as more
intelligent systems operate in continual, on-line settings. Our system has been
used to drive several commercial prototypes and has enabled a new product
architecture for our industrial partner. When compared with state-of-the-art
off-line planners, our system is hundreds of times faster and often finds
better plans. Our experience demonstrates that domain-independent AI planning
based on heuristic search can flexibly handle time, resources, replanning, and
multiple objectives in a high-speed practical application without requiring
hand-coded control knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3876</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3876</id><created>2014-01-16</created><authors><author><keyname>Xia</keyname><forenames>Lirong</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>Determining Possible and Necessary Winners Given Partial Orders</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  25-67, 2011</journal-ref><doi>10.1613/jair.3186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually a voting rule requires agents to give their preferences as linear
orders. However, in some cases it is impractical for an agent to give a linear
order over all the alternatives. It has been suggested to let agents submit
partial orders instead. Then, given a voting rule, a profile of partial orders,
and an alternative (candidate) c, two important questions arise: first, is it
still possible for c to win, and second, is c guaranteed to win? These are the
possible winner and necessary winner problems, respectively. Each of these two
problems is further divided into two sub-problems: determining whether c is a
unique winner (that is, c is the only winner), or determining whether c is a
co-winner (that is, c is in the set of winners). We consider the setting where
the number of alternatives is unbounded and the votes are unweighted. We
completely characterize the complexity of possible/necessary winner problems
for the following common voting rules: a class of positional scoring rules
(including Borda), Copeland, maximin, Bucklin, ranked pairs, voting trees, and
plurality with runoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3877</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3877</id><created>2014-01-16</created><authors><author><keyname>Cseke</keyname><forenames>Botond</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>Properties of Bethe Free Energies and Message Passing in Gaussian Models</title><categories>cs.LG cs.AI stat.ML</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  1-24, 2011</journal-ref><doi>10.1613/jair.3195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of computing approximate marginals in Gaussian
probabilistic models by using mean field and fractional Bethe approximations.
We define the Gaussian fractional Bethe free energy in terms of the moment
parameters of the approximate marginals, derive a lower and an upper bound on
the fractional Bethe free energy and establish a necessary condition for the
lower bound to be bounded from below. It turns out that the condition is
identical to the pairwise normalizability condition, which is known to be a
sufficient condition for the convergence of the message passing algorithm. We
show that stable fixed points of the Gaussian message passing algorithm are
local minima of the Gaussian Bethe free energy. By a counterexample, we
disprove the conjecture stating that the unboundedness of the free energy
implies the divergence of the message passing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3878</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3878</id><created>2014-01-16</created><authors><author><keyname>Cimatti</keyname><forenames>Alessandro</forenames></author><author><keyname>Griggio</keyname><forenames>Alberto</forenames></author><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author></authors><title>Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  701-728, 2011</journal-ref><doi>10.1613/jair.3196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding small unsatisfiable cores for SAT formulas has
recently received a lot of interest, mostly for its applications in formal
verification. However, propositional logic is often not expressive enough for
representing many interesting verification problems, which can be more
naturally addressed in the framework of Satisfiability Modulo Theories, SMT.
Surprisingly, the problem of finding unsatisfiable cores in SMT has received
very little attention in the literature. In this paper we present a novel
approach to this problem, called the Lemma-Lifting approach. The main idea is
to combine an SMT solver with an external propositional core extractor. The SMT
solver produces the theory lemmas found during the search, dynamically lifting
the suitable amount of theory information to the Boolean level. The core
extractor is then called on the Boolean abstraction of the original SMT problem
and of the theory lemmas. This results in an unsatisfiable core for the
original SMT problem, once the remaining theory lemmas are removed. The
approach is conceptually interesting, and has several advantages in practice.
In fact, it is extremely simple to implement and to update, and it can be
interfaced with every propositional core extractor in a plug-and-play manner,
so as to benefit for free of all unsat-core reduction techniques which have
been or will be made available.
  We have evaluated our algorithm with a very extensive empirical test on
SMT-LIB benchmarks, which confirms the validity and potential of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3879</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3879</id><created>2014-01-16</created><authors><author><keyname>Hebrard</keyname><forenames>Emmanuel</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>Soft Constraints of Difference and Equality</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  97-130, 2011</journal-ref><doi>10.1613/jair.3197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many combinatorial problems one may need to model the diversity or
similarity of assignments in a solution. For example, one may wish to maximise
or minimise the number of distinct values in a solution. To formulate problems
of this type, we can use soft variants of the well known AllDifferent and
AllEqual constraints. We present a taxonomy of six soft global constraints,
generated by combining the two latter ones and the two standard cost functions,
which are either maximised or minimised. We characterise the complexity of
achieving arc and bounds consistency on these constraints, resolving those
cases for which NP-hardness was neither proven nor disproven. In particular, we
explore in depth the constraint ensuring that at least k pairs of variables
have a common value. We show that achieving arc consistency is NP-hard, however
achieving bounds consistency can be done in polynomial time through dynamic
programming. Moreover, we show that the maximum number of pairs of equal
variables can be approximated by a factor 1/2 with a linear time greedy
algorithm. Finally, we provide a fixed parameter tractable algorithm with
respect to the number of values appearing in more than two distinct domains.
Interestingly, this taxonomy shows that enforcing equality is harder than
enforcing difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3880</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3880</id><created>2014-01-16</created><authors><author><keyname>Papadopoulos</keyname><forenames>Harris</forenames></author><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author><author><keyname>Gammerman</keyname><forenames>Alex</forenames></author></authors><title>Regression Conformal Prediction with Nearest Neighbours</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  815-840, 2011</journal-ref><doi>10.1613/jair.3198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours
Regression (k-NNR) algorithm and propose ways of extending the typical
nonconformity measure used for regression so far. Unlike traditional regression
methods which produce point predictions, Conformal Predictors output predictive
regions that satisfy a given confidence level. The regions produced by any
Conformal Predictor are automatically valid, however their tightness and
therefore usefulness depends on the nonconformity measure used by each CP. In
effect a nonconformity measure evaluates how strange a given example is
compared to a set of other examples based on some traditional machine learning
algorithm. We define six novel nonconformity measures based on the k-Nearest
Neighbours Regression algorithm and develop the corresponding CPs following
both the original (transductive) and the inductive CP approaches. A comparison
of the predictive regions produced by our measures with those of the typical
regression measure suggests that a major improvement in terms of predictive
region tightness is achieved by the new measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3881</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3881</id><created>2014-01-16</created><authors><author><keyname>Bilgic</keyname><forenames>Mustafa</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Value of Information Lattice: Exploiting Probabilistic Independence for
  Effective Feature Subset Acquisition</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  69-95, 2011</journal-ref><doi>10.1613/jair.3200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the cost-sensitive feature acquisition problem, where
misclassifying an instance is costly but the expected misclassification cost
can be reduced by acquiring the values of the missing features. Because
acquiring the features is costly as well, the objective is to acquire the right
set of features so that the sum of the feature acquisition cost and
misclassification cost is minimized. We describe the Value of Information
Lattice (VOILA), an optimal and efficient feature subset acquisition framework.
Unlike the common practice, which is to acquire features greedily, VOILA can
reason with subsets of features. VOILA efficiently searches the space of
possible feature subsets by discovering and exploiting conditional independence
properties between the features and it reuses probabilistic inference
computations to further speed up the process. Through empirical evaluation on
five medical datasets, we show that the greedy strategy is often reluctant to
acquire features, as it cannot forecast the benefit of acquiring multiple
features in combination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3882</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3882</id><created>2014-01-16</created><authors><author><keyname>Joshi</keyname><forenames>Saket</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author></authors><title>Probabilistic Relational Planning with First Order Decision Diagrams</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  231-266, 2011</journal-ref><doi>10.1613/jair.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic programming algorithms have been successfully applied to
propositional stochastic planning problems by using compact representations, in
particular algebraic decision diagrams, to capture domain dynamics and value
functions. Work on symbolic dynamic programming lifted these ideas to first
order logic using several representation schemes. Recent work introduced a
first order variant of decision diagrams (FODD) and developed a value iteration
algorithm for this representation. This paper develops several improvements to
the FODD algorithm that make the approach practical. These include, new
reduction operators that decrease the size of the representation, several
speedup techniques, and techniques for value approximation. Incorporating
these, the paper presents a planning system, FODD-Planner, for solving
relational stochastic planning problems. The system is evaluated on several
domains, including problems from the recent international planning competition,
and shows competitive performance with top ranking systems. This is the first
demonstration of feasibility of this approach and it shows that abstraction
through compact representation is a promising approach to stochastic planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3883</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3883</id><created>2014-01-16</created><authors><author><keyname>Kozorovitsky</keyname><forenames>Anna Khudyak</forenames></author><author><keyname>Kurland</keyname><forenames>Oren</forenames></author></authors><title>From &quot;Identical&quot; to &quot;Similar&quot;: Fusing Retrieved Lists Based on
  Inter-Document Similarities</title><categories>cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  267-296, 2011</journal-ref><doi>10.1613/jair.3214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for fusing document lists that were retrieved in response to a query
often utilize the retrieval scores and/or ranks of documents in the lists. We
present a novel fusion approach that is based on using, in addition,
information induced from inter-document similarities. Specifically, our methods
let similar documents from different lists provide relevance-status support to
each other. We use a graph-based method to model relevance-status propagation
between documents. The propagation is governed by inter-document-similarities
and by retrieval scores of documents in the lists. Empirical evaluation
demonstrates the effectiveness of our methods in fusing TREC runs. The
performance of our most effective methods transcends that of effective fusion
methods that utilize only retrieval scores or ranks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3884</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3884</id><created>2014-01-16</created><authors><author><keyname>Gujar</keyname><forenames>Sujit</forenames></author><author><keyname>Narahari</keyname><forenames>Yadati</forenames></author></authors><title>Redistribution Mechanisms for Assignment of Heterogeneous Objects</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  131-154, 2011</journal-ref><doi>10.1613/jair.3225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are p heterogeneous objects to be assigned to n competing agents (n &gt;
p) each with unit demand. It is required to design a Groves mechanism for this
assignment problem satisfying weak budget balance, individual rationality, and
minimizing the budget imbalance. This calls for designing an appropriate rebate
function. When the objects are identical, this problem has been solved which we
refer as WCO mechanism. We measure the performance of such mechanisms by the
redistribution index. We first prove an impossibility theorem which rules out
linear rebate functions with non-zero redistribution index in heterogeneous
object assignment. Motivated by this theorem, we explore two approaches to get
around this impossibility. In the first approach, we show that linear rebate
functions with non-zero redistribution index are possible when the valuations
for the objects have a certain type of relationship and we design a mechanism
with linear rebate function that is worst case optimal. In the second approach,
we show that rebate functions with non-zero efficiency are possible if
linearity is relaxed. We extend the rebate functions of the WCO mechanism to
heterogeneous objects assignment and conjecture them to be worst case optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3885</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3885</id><created>2014-01-16</created><authors><author><keyname>De la Rosa</keyname><forenames>Tomas</forenames></author><author><keyname>Jimenez</keyname><forenames>Sergio</forenames></author><author><keyname>Fuentetaja</keyname><forenames>Raquel</forenames></author><author><keyname>Borrajo</keyname><forenames>Daniel</forenames></author></authors><title>Scaling up Heuristic Planning with Relational Decision Trees</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  767-813, 2011</journal-ref><doi>10.1613/jair.3231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current evaluation functions for heuristic planning are expensive to compute.
In numerous planning problems these functions provide good guidance to the
solution, so they are worth the expense. However, when evaluation functions are
misguiding or when planning problems are large enough, lots of node evaluations
must be computed, which severely limits the scalability of heuristic planners.
In this paper, we present a novel solution for reducing node evaluations in
heuristic planning based on machine learning. Particularly, we define the task
of learning search control for heuristic planning as a relational
classification task, and we use an off-the-shelf relational classification tool
to address this learning task. Our relational classification task captures the
preferred action to select in the different planning contexts of a specific
planning domain. These planning contexts are defined by the set of helpful
actions of the current state, the goals remaining to be achieved, and the
static predicates of the planning task. This paper shows two methods for
guiding the search of a heuristic planner with the learned classifiers. The
first one consists of using the resulting classifier as an action policy. The
second one consists of applying the classifier to generate lookahead states
within a Best First Search algorithm. Experiments over a variety of domains
reveal that our heuristic planner using the learned classifiers solves larger
problems than state-of-the-art planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3886</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3886</id><created>2014-01-16</created><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author><author><keyname>van Beek</keyname><forenames>Peter</forenames></author></authors><title>Exploiting Structure in Weighted Model Counting Approaches to
  Probabilistic Inference</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  729-765, 2011</journal-ref><doi>10.1613/jair.3232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies have demonstrated that encoding a Bayesian network into a
SAT formula and then performing weighted model counting using a backtracking
search algorithm can be an effective method for exact inference. In this paper,
we present techniques for improving this approach for Bayesian networks with
noisy-OR and noisy-MAX relations---two relations that are widely used in
practice as they can dramatically reduce the number of probabilities one needs
to specify. In particular, we present two SAT encodings for noisy-OR and two
encodings for noisy-MAX that exploit the structure or semantics of the
relations to improve both time and space efficiency, and we prove the
correctness of the encodings. We experimentally evaluated our techniques on
large-scale real and randomly generated Bayesian networks. On these benchmarks,
our techniques gave speedups of up to two orders of magnitude over the best
previous approaches for networks with noisy-OR/MAX relations and scaled up to
larger networks. As well, our techniques extend the weighted model counting
approach for exact inference to networks that were previously intractable for
the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3887</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3887</id><created>2014-01-16</created><authors><author><keyname>Bordeaux</keyname><forenames>Lucas</forenames></author><author><keyname>Katsirelos</keyname><forenames>George</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>The Complexity of Integer Bound Propagation</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  657-676, 2011</journal-ref><doi>10.1613/jair.3248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bound propagation is an important Artificial Intelligence technique used in
Constraint Programming tools to deal with numerical constraints. It is
typically embedded within a search procedure (&quot;branch and prune&quot;) and used at
every node of the search tree to narrow down the search space, so it is
critical that it be fast. The procedure invokes constraint propagators until a
common fixpoint is reached, but the known algorithms for this have a
pseudo-polynomial worst-case time complexity: they are fast indeed when the
variables have a small numerical range, but they have the well-known problem of
being prohibitively slow when these ranges are large. An important question is
therefore whether strongly-polynomial algorithms exist that compute the common
bound consistent fixpoint of a set of constraints. This paper answers this
question. In particular we show that this fixpoint computation is in fact
NP-complete, even when restricted to binary linear constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3888</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3888</id><created>2014-01-16</created><authors><author><keyname>Korzhyk</keyname><forenames>Dmytro</forenames></author><author><keyname>Yin</keyname><forenames>Zhengyu</forenames></author><author><keyname>Kiekintveld</keyname><forenames>Christopher</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Tambe</keyname><forenames>Milind</forenames></author></authors><title>Stackelberg vs. Nash in Security Games: An Extended Investigation of
  Interchangeability, Equivalence, and Uniqueness</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  297-327, 2011</journal-ref><doi>10.1613/jair.3269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant recent interest in game-theoretic approaches to
security, with much of the recent research focused on utilizing the
leader-follower Stackelberg game model. Among the major applications are the
ARMOR program deployed at LAX Airport and the IRIS program in use by the US
Federal Air Marshals (FAMS). The foundational assumption for using Stackelberg
games is that security forces (leaders), acting first, commit to a randomized
strategy; while their adversaries (followers) choose their best response after
surveillance of this randomized strategy. Yet, in many situations, a leader may
face uncertainty about the follower's surveillance capability. Previous work
fails to address how a leader should compute her strategy given such
uncertainty. We provide five contributions in the context of a general class of
security games. First, we show that the Nash equilibria in security games are
interchangeable, thus alleviating the equilibrium selection problem. Second,
under a natural restriction on security games, any Stackelberg strategy is also
a Nash equilibrium strategy; and furthermore, the solution is unique in a class
of security games of which ARMOR is a key exemplar. Third, when faced with a
follower that can attack multiple targets, many of these properties no longer
hold. Fourth, we show experimentally that in most (but not all) games where the
restriction does not hold, the Stackelberg strategy is still a Nash equilibrium
strategy, but this is no longer true when the attacker can attack multiple
targets. Finally, as a possible direction for future research, we propose an
extensive-form game model that makes the defender's uncertainty about the
attacker's ability to observe explicit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3890</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3890</id><created>2014-01-16</created><authors><author><keyname>Hoffmann</keyname><forenames>Joerg</forenames></author></authors><title>Analyzing Search Topology Without Running Any Search: On the Connection
  Between Causal Graphs and h+</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  155-229, 2011</journal-ref><doi>10.1613/jair.3276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ignoring delete lists relaxation is of paramount importance for both
satisficing and optimal planning. In earlier work, it was observed that the
optimal relaxation heuristic h+ has amazing qualities in many classical
planning benchmarks, in particular pertaining to the complete absence of local
minima. The proofs of this are hand-made, raising the question whether such
proofs can be lead automatically by domain analysis techniques. In contrast to
earlier disappointing results -- the analysis method has exponential runtime
and succeeds only in two extremely simple benchmark domains -- we herein answer
this question in the affirmative. We establish connections between causal graph
structure and h+ topology. This results in low-order polynomial time analysis
methods, implemented in a tool we call TorchLight. Of the 12 domains where the
absence of local minima has been proved, TorchLight gives strong success
guarantees in 8 domains. Empirically, its analysis exhibits strong performance
in a further 2 of these domains, plus in 4 more domains where local minima may
exist but are rare. In this way, TorchLight can distinguish easy domains from
hard ones. By summarizing structural reasons for analysis failure, TorchLight
also provides diagnostic output indicating domain aspects that may cause local
minima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3892</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3892</id><created>2014-01-16</created><authors><author><keyname>Siddiqi</keyname><forenames>Sajjad Ahmed</forenames></author><author><keyname>Huang</keyname><forenames>Jinbo</forenames></author></authors><title>Sequential Diagnosis by Abstraction</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  329-365, 2011</journal-ref><doi>10.1613/jair.3296</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a system behaves abnormally, sequential diagnosis takes a sequence of
measurements of the system until the faults causing the abnormality are
identified, and the goal is to reduce the diagnostic cost, defined here as the
number of measurements. To propose measurement points, previous work employs a
heuristic based on reducing the entropy over a computed set of diagnoses. This
approach generally has good performance in terms of diagnostic cost, but can
fail to diagnose large systems when the set of diagnoses is too large. Focusing
on a smaller set of probable diagnoses scales the approach but generally leads
to increased average diagnostic costs. In this paper, we propose a new
diagnostic framework employing four new techniques, which scales to much larger
systems with good performance in terms of diagnostic cost. First, we propose a
new heuristic for measurement point selection that can be computed efficiently,
without requiring the set of diagnoses, once the system is modeled as a
Bayesian network and compiled into a logical form known as d-DNNF. Second, we
extend hierarchical diagnosis, a technique based on system abstraction from our
previous work, to handle probabilities so that it can be applied to sequential
diagnosis to allow larger systems to be diagnosed. Third, for the largest
systems where even hierarchical diagnosis fails, we propose a novel method that
converts the system into one that has a smaller abstraction and whose diagnoses
form a superset of those of the original system; the new system can then be
diagnosed and the result mapped back to the original system. Finally, we
propose a novel cost estimation function which can be used to choose an
abstraction of the system that is more likely to provide optimal average cost.
Experiments with ISCAS-85 benchmark circuits indicate that our approach scales
to all circuits in the suite except one that has a flat structure not
susceptible to useful abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3893</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3893</id><created>2014-01-16</created><authors><author><keyname>Yuan</keyname><forenames>Changhe</forenames></author><author><keyname>Lim</keyname><forenames>Heejin</forenames></author><author><keyname>Lu</keyname><forenames>Tsai-Ching</forenames></author></authors><title>Most Relevant Explanation in Bayesian Networks</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  309-352, 2011</journal-ref><doi>10.1613/jair.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major inference task in Bayesian networks is explaining why some variables
are observed in their particular states using a set of target variables.
Existing methods for solving this problem often generate explanations that are
either too simple (underspecified) or too complex (overspecified). In this
paper, we introduce a method called Most Relevant Explanation (MRE) which finds
a partial instantiation of the target variables that maximizes the generalized
Bayes factor (GBF) as the best explanation for the given evidence. Our study
shows that GBF has several theoretical properties that enable MRE to
automatically identify the most relevant target variables in forming its
explanation. In particular, conditional Bayes factor (CBF), defined as the GBF
of a new explanation conditioned on an existing explanation, provides a soft
measure on the degree of relevance of the variables in the new explanation in
explaining the evidence given the existing explanation. As a result, MRE is
able to automatically prune less relevant variables from its explanation. We
also show that CBF is able to capture well the explaining-away phenomenon that
is often represented in Bayesian networks. Moreover, we define two dominance
relations between the candidate solutions and use the relations to generalize
MRE to find a set of top explanations that is both diverse and representative.
Case studies on several benchmark diagnostic Bayesian networks show that MRE is
often able to find explanatory hypotheses that are not only precise but also
concise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3894</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3894</id><created>2014-01-16</created><authors><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Kocsis</keyname><forenames>Levente</forenames></author></authors><title>Efficient Multi-Start Strategies for Local Search Algorithms</title><categories>cs.LG cs.AI stat.ML</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  407-444, 2011</journal-ref><doi>10.1613/jair.3313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local search algorithms applied to optimization problems often suffer from
getting trapped in a local optimum. The common solution for this deficiency is
to restart the algorithm when no progress is observed. Alternatively, one can
start multiple instances of a local search algorithm, and allocate
computational resources (in particular, processing time) to the instances
depending on their behavior. Hence, a multi-start strategy has to decide
(dynamically) when to allocate additional resources to a particular instance
and when to start new instances. In this paper we propose multi-start
strategies motivated by works on multi-armed bandit problems and Lipschitz
optimization with an unknown constant. The strategies continuously estimate the
potential performance of each algorithm instance by supposing a convergence
rate of the local search algorithm up to an unknown constant, and in every
phase allocate resources to those instances that could converge to the optimum
for a particular range of the constant. Asymptotic bounds are given on the
performance of the strategies. In particular, we prove that at most a quadratic
increase in the number of times the target function is evaluated is needed to
achieve the performance of a local search algorithm started from the attraction
region of the optimum. Experiments are provided using SPSA (Simultaneous
Perturbation Stochastic Approximation) and k-means as local search algorithms,
and the results indicate that the proposed strategies work well in practice,
and, in all cases studied, need only logarithmically more evaluations of the
target function as opposed to the theoretically suggested quadratic increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3895</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3895</id><created>2014-01-16</created><authors><author><keyname>Dvorak</keyname><forenames>Wolfgang</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>On the Intertranslatability of Argumentation Semantics</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  445-475, 2011</journal-ref><doi>10.1613/jair.3318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Translations between different nonmonotonic formalisms always have been an
important topic in the field, in particular to understand the
knowledge-representation capabilities those formalisms offer. We provide such
an investigation in terms of different semantics proposed for abstract
argumentation frameworks, a nonmonotonic yet simple formalism which received
increasing interest within the last decade. Although the properties of these
different semantics are nowadays well understood, there are no explicit results
about intertranslatability. We provide such translations wrt. different
properties and also give a few novel complexity results which underlie some
negative results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3896</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3896</id><created>2014-01-16</created><authors><author><keyname>Kurland</keyname><forenames>Oren</forenames></author><author><keyname>Krikon</keyname><forenames>Eyal</forenames></author></authors><title>The Opposite of Smoothing: A Language Model Approach to Ranking
  Query-Specific Document Clusters</title><categories>cs.IR</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  367-395, 2011</journal-ref><doi>10.1613/jair.3327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting information induced from (query-specific) clustering of
top-retrieved documents has long been proposed as a means for improving
precision at the very top ranks of the returned results. We present a novel
language model approach to ranking query-specific clusters by the presumed
percentage of relevant documents that they contain. While most previous cluster
ranking approaches focus on the cluster as a whole, our model utilizes also
information induced from documents associated with the cluster. Our model
substantially outperforms previous approaches for identifying clusters
containing a high relevant-document percentage. Furthermore, using the model to
produce document ranking yields precision-at-top-ranks performance that is
consistently better than that of the initial ranking upon which clustering is
performed. The performance also favorably compares with that of a
state-of-the-art pseudo-feedback-based retrieval method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3897</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3897</id><created>2014-01-16</created><authors><author><keyname>Gabbay</keyname><forenames>Dov</forenames></author><author><keyname>Pearce</keyname><forenames>David</forenames></author><author><keyname>Valverde</keyname><forenames>Agust&#xed;n</forenames></author></authors><title>Interpolable Formulas in Equilibrium Logic and Answer Set Programming</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  917-943, 2011</journal-ref><doi>10.1613/jair.3329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpolation is an important property of classical and many non-classical
logics that has been shown to have interesting applications in computer science
and AI. Here we study the Interpolation Property for the the non-monotonic
system of equilibrium logic, establishing weaker or stronger forms of
interpolation depending on the precise interpretation of the inference
relation. These results also yield a form of interpolation for ground logic
programs under the answer sets semantics. For disjunctive logic programs we
also study the property of uniform interpolation that is closely related to the
concept of variable forgetting. The first-order version of equilibrium logic
has analogous Interpolation properties whenever the collection of equilibrium
models is (first-order) definable. Since this is the case for so-called safe
programs and theories, it applies to the usual situations that arise in
practical answer set programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3898</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3898</id><created>2014-01-16</created><authors><author><keyname>Lee</keyname><forenames>Joohyung</forenames></author><author><keyname>Meng</keyname><forenames>Yunsong</forenames></author></authors><title>First-Order Stable Model Semantics and First-Order Loop Formulas</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  125-180, 2011</journal-ref><doi>10.1613/jair.3337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lin and Zhaos theorem on loop formulas states that in the propositional case
the stable model semantics of a logic program can be completely characterized
by propositional loop formulas, but this result does not fully carry over to
the first-order case. We investigate the precise relationship between the
first-order stable model semantics and first-order loop formulas, and study
conditions under which the former can be represented by the latter. In order to
facilitate the comparison, we extend the definition of a first-order loop
formula which was limited to a nondisjunctive program, to a disjunctive program
and to an arbitrary first-order theory. Based on the studied relationship we
extend the syntax of a logic program with explicit quantifiers, which allows us
to do reasoning involving non-Herbrand stable models using first-order
reasoners. Such programs can be viewed as a special class of first-order
theories under the stable model semantics, which yields more succinct loop
formulas than the general language due to their restricted syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3899</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3899</id><created>2014-01-16</created><authors><author><keyname>Santhanam</keyname><forenames>Ganesh Ram</forenames></author><author><keyname>Basu</keyname><forenames>Samik</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant</forenames></author></authors><title>Representing and Reasoning with Qualitative Preferences for
  Compositional Systems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  211-274, 2011</journal-ref><doi>10.1613/jair.3339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications, e.g., Web service composition, complex system design, team
formation, etc., rely on methods for identifying collections of objects or
entities satisfying some functional requirement. Among the collections that
satisfy the functional requirement, it is often necessary to identify one or
more collections that are optimal with respect to user preferences over a set
of attributes that describe the non-functional properties of the collection.
  We develop a formalism that lets users express the relative importance among
attributes and qualitative preferences over the valuations of each attribute.
We define a dominance relation that allows us to compare collections of objects
in terms of preferences over attributes of the objects that make up the
collection. We establish some key properties of the dominance relation. In
particular, we show that the dominance relation is a strict partial order when
the intra-attribute preference relations are strict partial orders and the
relative importance preference relation is an interval order.
  We provide algorithms that use this dominance relation to identify the set of
most preferred collections. We show that under certain conditions, the
algorithms are guaranteed to return only (sound), all (complete), or at least
one (weakly complete) of the most preferred collections. We present results of
simulation experiments comparing the proposed algorithms with respect to (a)
the quality of solutions (number of most preferred solutions) produced by the
algorithms, and (b) their performance and efficiency. We also explore some
interesting conjectures suggested by the results of our experiments that relate
the properties of the user preferences, the dominance relation, and the
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3900</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3900</id><created>2014-01-16</created><authors><author><keyname>Aravantinos</keyname><forenames>Vincent</forenames></author><author><keyname>Caferra</keyname><forenames>Ricardo</forenames></author><author><keyname>Peltier</keyname><forenames>Nicolas</forenames></author></authors><title>Decidability and Undecidability Results for Propositional Schemata</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  599-656, 2011</journal-ref><doi>10.1613/jair.3351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a logic of propositional formula schemata adding to the syntax of
propositional logic indexed propositions and iterated connectives ranging over
intervals parameterized by arithmetic variables. The satisfiability problem is
shown to be undecidable for this new logic, but we introduce a very general
class of schemata, called bound-linear, for which this problem becomes
decidable. This result is obtained by reduction to a particular class of
schemata called regular, for which we provide a sound and complete terminating
proof procedure. This schemata calculus allows one to capture proof patterns
corresponding to a large class of problems specified in propositional logic. We
also show that the satisfiability problem becomes again undecidable for slight
extensions of this class, thus demonstrating that bound-linear schemata
represent a good compromise between expressivity and decidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3901</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3901</id><created>2014-01-16</created><authors><author><keyname>Bonatti</keyname><forenames>Piero A.</forenames></author><author><keyname>Faella</keyname><forenames>Marco</forenames></author><author><keyname>Sauro</keyname><forenames>Luigi</forenames></author></authors><title>Defeasible Inclusions in Low-Complexity DLs</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  719-764, 2011</journal-ref><doi>10.1613/jair.3360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some of the applications of OWL and RDF (e.g. biomedical knowledge
representation and semantic policy formulation) call for extensions of these
languages with nonmonotonic constructs such as inheritance with overriding.
Nonmonotonic description logics have been studied for many years, however no
practical such knowledge representation languages exist, due to a combination
of semantic difficulties and high computational complexity. Independently,
low-complexity description logics such as DL-lite and EL have been introduced
and incorporated in the OWL standard. Therefore, it is interesting to see
whether the syntactic restrictions characterizing DL-lite and EL bring
computational benefits to their nonmonotonic versions, too. In this paper we
extensively investigate the computational complexity of Circumscription when
knowledge bases are formulated in DL-lite_R, EL, and fragments thereof. We
identify fragments whose complexity ranges from P to the second level of the
polynomial hierarchy, as well as fragments whose complexity raises to PSPACE
and beyond.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3902</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3902</id><created>2014-01-16</created><authors><author><keyname>Booth</keyname><forenames>Richard</forenames></author><author><keyname>Meyer</keyname><forenames>Thomas</forenames></author><author><keyname>Varzinczak</keyname><forenames>Ivan</forenames></author><author><keyname>Wassermann</keyname><forenames>Renata</forenames></author></authors><title>On the Link between Partial Meet, Kernel, and Infra Contraction and its
  Application to Horn Logic</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  31-53, 2011</journal-ref><doi>10.1613/jair.3364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard belief change assumes an underlying logic containing full classical
propositional logic. However, there are good reasons for considering belief
change in less expressive logics as well. In this paper we build on recent
investigations by Delgrande on contraction for Horn logic. We show that the
standard basic form of contraction, partial meet, is too strong in the Horn
case. This result stands in contrast to Delgrande's conjecture that orderly
maxichoice is the appropriate form of contraction for Horn logic. We then
define a more appropriate notion of basic contraction for the Horn case,
influenced by the convexity property holding for full propositional logic and
which we refer to as infra contraction. The main contribution of this work is a
result which shows that the construction method for Horn contraction for belief
sets based on our infra remainder sets corresponds exactly to Hansson's
classical kernel contraction for belief sets, when restricted to Horn logic.
This result is obtained via a detour through contraction for belief bases. We
prove that kernel contraction for belief bases produces precisely the same
results as the belief base version of infra contraction. The use of belief
bases to obtain this result provides evidence for the conjecture that Horn
belief change is best viewed as a hybrid version of belief set change and
belief base change. One of the consequences of the link with base contraction
is the provision of a representation result for Horn contraction for belief
sets in which a version of the Core-retainment postulate features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3903</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3903</id><created>2014-01-16</created><authors><author><keyname>Agmon</keyname><forenames>Noa</forenames></author><author><keyname>Kaminka</keyname><forenames>Gal A.</forenames></author><author><keyname>Kraus</keyname><forenames>Sarit</forenames></author></authors><title>Multi-Robot Adversarial Patrolling: Facing a Full-Knowledge Opponent</title><categories>cs.MA cs.RO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  887-916, 2011</journal-ref><doi>10.1613/jair.3365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of adversarial multi-robot patrol has gained interest in recent
years, mainly due to its immediate relevance to various security applications.
In this problem, robots are required to repeatedly visit a target area in a way
that maximizes their chances of detecting an adversary trying to penetrate
through the patrol path. When facing a strong adversary that knows the patrol
strategy of the robots, if the robots use a deterministic patrol algorithm,
then in many cases it is easy for the adversary to penetrate undetected (in
fact, in some of those cases the adversary can guarantee penetration).
Therefore this paper presents a non-deterministic patrol framework for the
robots. Assuming that the strong adversary will take advantage of its knowledge
and try to penetrate through the patrols weakest spot, hence an optimal
algorithm is one that maximizes the chances of detection in that point. We
therefore present a polynomial-time algorithm for determining an optimal patrol
under the Markovian strategy assumption for the robots, such that the
probability of detecting the adversary in the patrols weakest spot is
maximized. We build upon this framework and describe an optimal patrol strategy
for several robotic models based on their movement abilities (directed or
undirected) and sensing abilities (perfect or imperfect), and in different
environment models - either patrol around a perimeter (closed polygon) or an
open fence (open polyline).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3905</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3905</id><created>2014-01-16</created><authors><author><keyname>Wang</keyname><forenames>Ko-Hsin Cindy</forenames></author><author><keyname>Botea</keyname><forenames>Adi</forenames></author></authors><title>MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability
  and Completeness Guarantees</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  55-90, 2011</journal-ref><doi>10.1613/jair.3370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent path planning is a challenging problem with numerous real-life
applications. Running a centralized search such as A* in the combined state
space of all units is complete and cost-optimal, but scales poorly, as the
state space size is exponential in the number of mobile units. Traditional
decentralized approaches, such as FAR and WHCA*, are faster and more scalable,
being based on problem decomposition. However, such methods are incomplete and
provide no guarantees with respect to the running time or the solution quality.
They are not necessarily able to tell in a reasonable time whether they would
succeed in finding a solution to a given instance. We introduce MAPP, a
tractable algorithm for multi-agent path planning on undirected graphs. We
present a basic version and several extensions. They have low-polynomial
worst-case upper bounds for the running time, the memory requirements, and the
length of solutions. Even though all algorithmic versions are incomplete in the
general case, each provides formal guarantees on problems it can solve. For
each version, we discuss the algorithms completeness with respect to clearly
defined subclasses of instances. Experiments were run on realistic game grid
maps. MAPP solved 99.86% of all mobile units, which is 18--22% better than the
percentage of FAR and WHCA*. MAPP marked 98.82% of all units as provably
solvable during the first stage of plan computation. Parts of MAPPs computation
can be re-used across instances on the same map. Speed-wise, MAPP is
competitive or significantly faster than WHCA*, depending on whether MAPP
performs all computations from scratch. When data that MAPP can re-use are
preprocessed offline and readily available, MAPP is slower than the very fast
FAR algorithm by a factor of 2.18 on average. MAPPs solutions are on average
20% longer than FARs solutions and 7--31% longer than WHCA*s solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3906</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3906</id><created>2014-01-16</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y</forenames></author></authors><title>Making Decisions Using Sets of Probabilities: Updating, Time
  Consistency, and Calibration</title><categories>cs.AI cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  393-426, 2011</journal-ref><doi>10.1613/jair.3374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how an agent should update her beliefs when her beliefs are
represented by a set P of probability distributions, given that the agent makes
decisions using the minimax criterion, perhaps the best-studied and most
commonly-used criterion in the literature. We adopt a game-theoretic framework,
where the agent plays against a bookie, who chooses some distribution from P.
We consider two reasonable games that differ in what the bookie knows when he
makes his choice. Anomalies that have been observed before, like time
inconsistency, can be understood as arising because different games are being
played, against bookies with different information. We characterize the
important special cases in which the optimal decision rules according to the
minimax criterion amount to either conditioning or simply ignoring the
information. Finally, we consider the relationship between updating and
calibration when uncertainty is described by sets of probabilities. Our results
emphasize the key role of the rectangularity condition of Epstein and
Schneider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3907</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3907</id><created>2014-01-16</created><authors><author><keyname>Lu</keyname><forenames>Xiaosong</forenames></author><author><keyname>Schwartz</keyname><forenames>Howard M.</forenames></author><author><keyname>Givigi</keyname><forenames>Sidney N.</forenames><suffix>Jr</suffix></author></authors><title>Policy Invariance under Reward Transformations for General-Sum
  Stochastic Games</title><categories>cs.GT cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  397-406, 2011</journal-ref><doi>10.1613/jair.3384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the potential-based shaping method from Markov decision processes
to multi-player general-sum stochastic games. We prove that the Nash equilibria
in a stochastic game remains unchanged after potential-based shaping is applied
to the environment. The property of policy invariance provides a possible way
of speeding convergence when learning to play a stochastic game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3908</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3908</id><created>2014-01-16</created><authors><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author></authors><title>Centrality-as-Relevance: Support Sets and Similarity as Geometric
  Proximity</title><categories>cs.IR cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  275-308, 2011</journal-ref><doi>10.1613/jair.3387</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In automatic summarization, centrality-as-relevance means that the most
important content of an information source, or a collection of information
sources, corresponds to the most central passages, considering a representation
where such notion makes sense (graph, spatial, etc.). We assess the main
paradigms, and introduce a new centrality-based relevance model for automatic
summarization that relies on the use of support sets to better estimate the
relevant content. Geometric proximity is used to compute semantic relatedness.
Centrality (relevance) is determined by considering the whole input source (and
not only local information), and by taking into account the existence of minor
topics or lateral subjects in the information sources to be summarized. The
method consists in creating, for each passage of the input source, a support
set consisting only of the most semantically related passages. Then, the
determination of the most relevant content is achieved by selecting the
passages that occur in the largest number of support sets. This model produces
extractive summaries that are generic, and language- and domain-independent.
Thorough automatic evaluation shows that the method achieves state-of-the-art
performance, both in written text, and automatically transcribed speech
summarization, including when compared to considerably more complex approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3909</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3909</id><created>2014-01-16</created><authors><author><keyname>Hoshino</keyname><forenames>Richard</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Scheduling Bipartite Tournaments to Minimize Total Travel Distance</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  91-124, 2011</journal-ref><doi>10.1613/jair.3388</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many professional sports leagues, teams from opposing leagues/conferences
compete against one another, playing inter-league games. This is an example of
a bipartite tournament. In this paper, we consider the problem of reducing the
total travel distance of bipartite tournaments, by analyzing inter-league
scheduling from the perspective of discrete optimization. This research has
natural applications to sports scheduling, especially for leagues such as the
National Basketball Association (NBA) where teams must travel long distances
across North America to play all their games, thus consuming much time, money,
and greenhouse gas emissions. We introduce the Bipartite Traveling Tournament
Problem (BTTP), the inter-league variant of the well-studied Traveling
Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for
small values of n, a distance-optimal inter-league schedule can be generated
from an algorithm based on minimum-weight 4-cycle-covers. We apply our
theoretical results to the 12-team Nippon Professional Baseball (NPB) league in
Japan, producing a provably-optimal schedule requiring 42950 kilometres of
total team travel, a 16% reduction compared to the actual distance traveled by
these teams during the 2010 NPB season. We also develop a nearly-optimal
inter-league tournament for the 30-team NBA league, just 3.8% higher than the
trivial theoretical lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3910</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3910</id><created>2014-01-16</created><authors><author><keyname>Dai</keyname><forenames>Peng</forenames></author><author><keyname>Mausam</keyname></author><author><keyname>Weld</keyname><forenames>Daniel Sabby</forenames></author><author><keyname>Goldsmith</keyname><forenames>Judy</forenames></author></authors><title>Topological Value Iteration Algorithms</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  181-209, 2011</journal-ref><doi>10.1613/jair.3390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Value iteration is a powerful yet inefficient algorithm for Markov decision
processes (MDPs) because it puts the majority of its effort into backing up the
entire state space, which turns out to be unnecessary in many cases. In order
to overcome this problem, many approaches have been proposed. Among them, ILAO*
and variants of RTDP are state-of-the-art ones. These methods use reachability
analysis and heuristic search to avoid some unnecessary backups. However, none
of these approaches build the graphical structure of the state transitions in a
pre-processing step or use the structural information to systematically
decompose a problem, whereby generating an intelligent backup sequence of the
state space. In this paper, we present two optimal MDP algorithms. The first
algorithm, topological value iteration (TVI), detects the structure of MDPs and
backs up states based on topological sequences. It (1) divides an MDP into
strongly-connected components (SCCs), and (2) solves these components
sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly
when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm,
focused topological value iteration (FTVI), is an extension of TVI. FTVI
restricts its attention to connected components that are relevant for solving
the MDP. Specifically, it uses a small amount of heuristic search to eliminate
provably sub-optimal actions; this pruning allows FTVI to find smaller
connected components, thus running faster. We demonstrate that FTVI outperforms
TVI by an order of magnitude, averaged across several domains. Surprisingly,
FTVI also significantly outperforms popular heuristically-informed MDP
algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains,
sometimes by as much as two orders of magnitude. Finally, we characterize the
type of domains where FTVI excels --- suggesting a way to an informed choice of
solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3915</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3915</id><created>2014-01-16</created><updated>2014-01-24</updated><authors><author><keyname>Bhattacharyya</keyname><forenames>Sharmodeep</forenames></author><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author></authors><title>Community Detection in Networks using Graph Distance</title><categories>stat.ML cs.SI</categories><comments>Presented in Networks with Community Structure Workshop, Eurandom,
  January, 2014. arXiv admin note: text overlap with arXiv:math/0504589 by
  other authors</comments><msc-class>62F12, 05C82, 62Q87, 62J80, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of networks has received increased attention recently not only from
the social sciences and statistics but also from physicists, computer
scientists and mathematicians. One of the principal problem in networks is
community detection. Many algorithms have been proposed for community finding
but most of them do not have have theoretical guarantee for sparse networks and
networks close to the phase transition boundary proposed by physicists. There
are some exceptions but all have some incomplete theoretical basis. Here we
propose an algorithm based on the graph distance of vertices in the network. We
give theoretical guarantees that our method works in identifying communities
for block models and can be extended for degree-corrected block models and
block models with the number of communities growing with number of vertices.
Despite favorable simulation results, we are not yet able to conclude that our
method is satisfactory for worst possible case. We illustrate on a network of
political blogs, Facebook networks and some other networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3916</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3916</id><created>2014-01-16</created><updated>2014-09-15</updated><authors><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Huang</keyname><forenames>Yichen</forenames></author><author><keyname>Landau</keyname><forenames>Zeph</forenames></author><author><keyname>Shin</keyname><forenames>Seung Woo</forenames></author></authors><title>Quantum Hamiltonian Complexity</title><categories>quant-ph cond-mat.str-el cs.CC</categories><comments>v3 is identical to v2; simply added new author (S. W. Shin) to arXiv
  metadata. Comments otherwise identical to v2: 58 pages. Substantial changes:
  (1) Length almost doubled via addition of sections on physics motivations,
  indistinguishable particles (i.e. bosons, fermions), area laws, etc. (2)
  Added new author, S. W. Shin. Minor changes to improve readability throughout</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the growing field of Quantum Hamiltonian Complexity, which includes
the study of Quantum Constraint Satisfaction. In particular, our aim is to
provide a computer science-oriented introduction to the subject in order to
help bridge the language barrier between computer scientists and physicists in
the field. As such, we include the following in this paper: (1) The motivations
and history of the field, (2) a glossary of condensed matter physics terms
explained in computer-science friendly language, (3) overviews of central ideas
from condensed matter physics, such as indistinguishable particles, mean field
theory, tensor networks, and area laws, and (4) brief expositions of selected
computer science-based results in the area. For example, as part of the latter,
we provide a novel information theoretic presentation of Bravyi's polynomial
time algorithm for Quantum 2-SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3918</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3918</id><created>2014-01-16</created><authors><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>A universal law in human mobility</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intrinsic factor that drives the human movement remains unclear for
decades. While our observations from intra-urban and inter-urban trips both
demonstrate a universal law in human mobility. Be specific, the probability
from one location to another is inversely proportional to the number of
population living in locations which are closer than the destination. A simple
rank-based model is then presented, which is parameterless but predicts human
flows with a convincing fidelity. Besides, comparison with other models shows
that our model is more stable and fundamental at different spatial scales by
implying the strong correlation between human mobility and social relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3922</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3922</id><created>2014-01-16</created><updated>2015-08-25</updated><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Discrete Convexity and Stochastic Approximation for Cross-layer On-off
  Transmission Control</title><categories>cs.IT cs.SY math.IT</categories><comments>29 pages, 8 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the discrete convexity of a cross-layer on-off
transmission control problem in wireless communications. In this system, a
scheduler decides whether or not to transmit in order to optimize the long-term
quality of service (QoS) incurred by the queueing effects in the data link
layer and the transmission power consumption in the physical (PHY) layer
simultaneously. Using a Markov decision process (MDP) formulation, we show that
the optimal policy can be determined by solving a minimization problem over a
set of queue thresholds if the dynamic programming (DP) is submodular. We prove
that this minimization problem is discrete convex. In order to search the
minimizer, we consider two discrete stochastic approximation (DSA) algorithms:
discrete simultaneous perturbation stochastic approximation (DSPSA) and
L-natural-convex stochastic approximation (L-natural-convex SA). Through
numerical studies, we show that the two DSA algorithms converge significantly
faster than the existing continuous simultaneous perturbation stochastic
approximation (CSPSA) algorithm in multi-user systems. Finally, we compare the
convergence results and complexity of two DSA and CSPSA algorithms where we
show that DSPSA achieves the best trade-off between complexity and accuracy in
multi-user systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3928</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3928</id><created>2014-01-16</created><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Cherif</keyname><forenames>Zouha</forenames></author><author><keyname>Danger</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author><author><keyname>Zhang</keyname><forenames>Xiande</forenames></author></authors><title>Multiply Constant-Weight Codes and the Reliability of Loop Physically
  Unclonable Functions</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the class of multiply constant-weight codes to improve the
reliability of certain physically unclonable function (PUF) response. We extend
classical coding methods to construct multiply constant-weight codes from known
$q$-ary and constant-weight codes. Analogues of Johnson bounds are derived and
are shown to be asymptotically tight to a constant factor under certain
conditions. We also examine the rates of the multiply constant-weight codes and
interestingly, demonstrate that these rates are the same as those of
constant-weight codes of suitable parameters. Asymptotic analysis of our code
constructions is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3936</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3936</id><created>2014-01-16</created><updated>2014-01-22</updated><authors><author><keyname>Anwar</keyname><forenames>Adnan</forenames></author><author><keyname>Mahmood</keyname><forenames>Abdun Naser</forenames></author></authors><title>Cyber Security of Smart Grid Infrastructure</title><categories>cs.CR</categories><comments>The State of the Art in Intrusion Prevention and Detection, CRC
  Press, Taylor &amp; Francis Group, USA, January 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart grid security is crucial to maintain stable and reliable power system
operation during the contingency situation due to the failure of any critical
power system component. Ensuring a secured smart grid involves with a less
possibility of power grid collapse or equipment malfunction. Due to lack of the
proper security measures, a major blackout may occur which can even lead to a
cascading failure. Therefore, to protect this critical power system
infrastructure and to ensure a reliable and an uninterrupted power supply to
the end users, smart grid security issues must be addressed with high priority.
In a smart grid environment, electric power infrastructure is modernized by
incorporating the current and future requirements and advanced functionalities
to its consumers. To make the smart grid happen, cyber system is integrated
with the physical power system. Although adoption of cyber system has made the
grid more energy efficient and modernized, it has introduced cyber-attack
issues which are critical for national infrastructure security and customer
satisfaction. Due to the cyber-attack, power grid may face operational failures
and loss of synchronization. This operational failure may damage critical power
system components which may interrupt the power supply and make the system
unstable resulting high financial penalties. In this chapter, some recent cyber
attack related incidents into a smart grid environment are discussed. The
requirements and the state of the art of cyber security issues of a critical
power system infrastructure are illustrated elaborately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3938</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3938</id><created>2014-01-16</created><updated>2014-01-16</updated><authors><author><keyname>Pudasaini</keyname><forenames>Subodh</forenames></author><author><keyname>Shin</keyname><forenames>Seokjoo</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Robust Modulation Technique for Diffusion-based Molecular Communication
  in Nanonetworks</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 fugures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Diffusion-based molecular communication over nanonetworks is an emerging
communication paradigm that enables nanomachines to communicate by using
molecules as the information carrier. For such a communication paradigm,
Concentration Shift Keying (CSK) has been considered as one of the most
promising techniques for modulating information symbols, owing to its inherent
simplicity and practicality. CSK modulated subsequent information symbols,
however, may interfere with each other due to the random amount of time that
molecules of each modulated symbols take to reach the receiver nanomachine. To
alleviate Inter Symbol Interference (ISI) problem associated with CSK, we
propose a new modulation technique called Zebra-CSK. The proposed Zebra-CSK
adds inhibitor molecules in CSK-modulated molecular signal to selectively
suppress ISI causing molecules. Numerical results from our newly developed
probabilistic analytical model show that Zebra-CSK not only enhances capacity
of the molecular channel but also reduces symbol error probability observed at
the receiver nanomachine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3941</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3941</id><created>2014-01-16</created><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Feng</keyname><forenames>Rongquan</forenames></author></authors><title>Network Coding for $3$s$/n$t Sum-Networks</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures, conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A sum-network is a directed acyclic network where each source independently
generates one symbol from a given field $\mathbb F$ and each terminal wants to
receive the sum $($over $\mathbb F)$ of the source symbols. For sum-networks
with two sources or two terminals, the solvability is characterized by the
connection condition of each source-terminal pair [3]. A necessary and
sufficient condition for the solvability of the $3$-source $3$-terminal
$(3$s$/3$t$)$ sum-networks was given by Shenvi and Dey [6]. However, the
general case of arbitrary sources/sinks is still open. In this paper, we
investigate the sum-network with three sources and $n$ sinks using a region
decomposition method. A sufficient and necessary condition is established for a
class of $3$s$/n$t sum-networks. As a direct application of this result, a
necessary and sufficient condition of solvability is obtained for the special
case of $3$s$/3$t sum-networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3945</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3945</id><created>2014-01-16</created><authors><author><keyname>Zahedi</keyname><forenames>Adel</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author><author><keyname>Jensen</keyname><forenames>Soren Holdt</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick</forenames></author><author><keyname>Bech</keyname><forenames>Soren</forenames></author></authors><title>Distributed Remote Vector Gaussian Source Coding for Wireless Acoustic
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>10 pages, to be presented at the IEEE DCC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of remote vector Gaussian source
coding for a wireless acoustic sensor network. Each node receives messages from
multiple nodes in the network and decodes these messages using its own
measurement of the sound field as side information. The node's measurement and
the estimates of the source resulting from decoding the received messages are
then jointly encoded and transmitted to a neighboring node in the network. We
show that for this distributed source coding scenario, one can encode a
so-called conditional sufficient statistic of the sources instead of jointly
encoding multiple sources. We focus on the case where node measurements are in
form of noisy linearly mixed combinations of the sources and the acoustic
channel mixing matrices are invertible. For this problem, we derive the
rate-distortion function for vector Gaussian sources and under covariance
distortion constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3957</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3957</id><created>2014-01-16</created><updated>2014-02-12</updated><authors><author><keyname>Boker</keyname><forenames>Udi</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames><affiliation>IST AUstria</affiliation></author></authors><title>Exact and Approximate Determinization of Discounted-Sum Automata</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (February
  13, 2014) lmcs:1104</journal-ref><doi>10.2168/LMCS-10(1:10)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discounted-sum automaton (NDA) is a nondeterministic finite automaton with
edge weights, valuing a run by the discounted sum of visited edge weights. More
precisely, the weight in the i-th position of the run is divided by
$\lambda^i$, where the discount factor $\lambda$ is a fixed rational number
greater than 1. The value of a word is the minimal value of the automaton runs
on it. Discounted summation is a common and useful measuring scheme, especially
for infinite sequences, reflecting the assumption that earlier weights are more
important than later weights. Unfortunately, determinization of NDAs, which is
often essential in formal verification, is, in general, not possible. We
provide positive news, showing that every NDA with an integral discount factor
is determinizable. We complete the picture by proving that the integers
characterize exactly the discount factors that guarantee determinizability: for
every nonintegral rational discount factor $\lambda$, there is a
nondeterminizable $\lambda$-NDA. We also prove that the class of NDAs with
integral discount factors enjoys closure under the algebraic operations min,
max, addition, and subtraction, which is not the case for general NDAs nor for
deterministic NDAs. For general NDAs, we look into approximate determinization,
which is always possible as the influence of a word's suffix decays. We show
that the naive approach, of unfolding the automaton computations up to a
sufficient level, is doubly exponential in the discount factor. We provide an
alternative construction for approximate determinization, which is singly
exponential in the discount factor, in the precision, and in the number of
states. We also prove matching lower bounds, showing that the exponential
dependency on each of these three parameters cannot be avoided. All our results
hold equally for automata over finite words and for automata over infinite
words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3973</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3973</id><created>2014-01-16</created><authors><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Arcos</keyname><forenames>Josep Lluis</forenames></author></authors><title>An Empirical Evaluation of Similarity Measures for Time Series
  Classification</title><categories>cs.LG cs.CV stat.ML</categories><comments>28 pages, 5 figures, 3 tables</comments><journal-ref>Knowledge-Based Systems 67: 305-314, 2014</journal-ref><doi>10.1016/j.knosys.2014.04.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series are ubiquitous, and a measure to assess their similarity is a
core part of many computational systems. In particular, the similarity measure
is the most essential ingredient of time series clustering and classification
systems. Because of this importance, countless approaches to estimate time
series similarity have been proposed. However, there is a lack of comparative
studies using empirical, rigorous, quantitative, and large-scale assessment
strategies. In this article, we provide an extensive evaluation of similarity
measures for time series classification following the aforementioned
principles. We consider 7 different measures coming from alternative measure
`families', and 45 publicly-available time series data sets coming from a wide
variety of scientific domains. We focus on out-of-sample classification
accuracy, but in-sample accuracies and parameter choices are also discussed.
Our work is based on rigorous evaluation methodologies and includes the use of
powerful statistical significance tests to derive meaningful conclusions. The
obtained results show the equivalence, in terms of accuracy, of a number of
measures, but with one single candidate outperforming the rest. Such findings,
together with the followed methodology, invite researchers on the field to
adopt a more consistent evaluation criteria and a more informed decision
regarding the baseline measures to which new developments should be compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3985</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3985</id><created>2014-01-16</created><authors><author><keyname>Mamun</keyname><forenames>Md Abdullah Al</forenames></author><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Hansson</keyname><forenames>J&#xf6;rgen</forenames></author></authors><title>Engineering the Hardware/Software Interface for Robotic Platforms - A
  Comparison of Applied Model Checking with Prolog and Alloy</title><categories>cs.SE cs.RO</categories><comments>Presented at DSLRob 2013 (arXiv:cs/1312.5952)</comments><report-no>DSLRob/2013/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic platforms serve different use cases ranging from experiments for
prototyping assistive applications up to embedded systems for realizing
cyber-physical systems in various domains. We are using 1:10 scale miniature
vehicles as a robotic platform to conduct research in the domain of
self-driving cars and collaborative vehicle fleets. Thus, experiments with
different sensors like e.g.~ultra-sonic, infrared, and rotary encoders need to
be prepared and realized using our vehicle platform. For each setup, we need to
configure the hardware/software interface board to handle all sensors and
actors. Therefore, we need to find a specific configuration setting for each
pin of the interface board that can handle our current hardware setup but which
is also flexible enough to support further sensors or actors for future use
cases. In this paper, we show how to model the domain of the configuration
space for a hardware/software interface board to enable model checking for
solving the tasks of finding any, all, and the best possible pin configuration.
We present results from a formal experiment applying the declarative languages
Alloy and Prolog to guide the process of engineering the hardware/software
interface for robotic platforms on the example of a configuration complexity up
to ten pins resulting in a configuration space greater than 14.5 million
possibilities. Our results show that our domain model in Alloy performs better
compared to Prolog to find feasible solutions for larger configurations with an
average time of 0.58s. To find the best solution, our model for Prolog performs
better taking only 1.38s for the largest desired configuration; however, this
important use case is currently not covered by the existing tools for the
hardware used as an example in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3995</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3995</id><created>2014-01-16</created><authors><author><keyname>Maier</keyname><forenames>Henning</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>$Y$-$\Delta$ Product in 3-Way $\Delta$ and Y-Channels for Cyclic
  Interference and Signal Alignment</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a full-duplex 3-way $\Delta$ channel, three transceivers communicate to
each other, so that a number of six messages is exchanged. In a $Y$-channel,
however, these three transceivers are connected to an intermediate full-duplex
relay. Loop-back self-interference is suppressed perfectly. The relay forwards
network-coded messages to their dedicated users by means of interference
alignment (IA) and signal alignment. A conceptual channel model with cyclic
shifts described by a polynomial ring is considered for these two related
channels. The maximally achievable rates in terms of the degrees of freedom
measure are derived. We observe that the Y-channel and the 3-way $\Delta$
channel provide a $Y$-$\Delta$ product relationship. Moreover, we briefly
discuss how this analysis relates to spatial IA and MIMO IA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.3998</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.3998</id><created>2014-01-16</created><authors><author><keyname>Meric</keyname><forenames>Hugo</forenames></author><author><keyname>Piquer</keyname><forenames>Jose Miguel</forenames></author></authors><title>Performance Evaluation of Bit Division Multiplexing combined with
  Non-Uniform QAM</title><categories>cs.IT math.IT</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcasting systems have to deal with channel variability in order to offer
the best spectral efficiency to the receivers. However, the transmission
parameters that maximise the spectral efficiency generally leads to a large
link unavailability. In this paper, we study analytically the trade-off between
spectral efficiency and coverage for various channel resource allocation
strategies when broadcasting two services. More precisely, we consider the
following strategies: time sharing, hierarchical modulation and bit division
multiplexing. Our main contribution is the combination of bit division
multiplexing with non-uniform QAM to improve the performance of broadcasting
systems. The results show that this scheme outperforms all the previous channel
resource allocation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4002</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4002</id><created>2014-01-16</created><authors><author><keyname>Shamkanov</keyname><forenames>Daniyar</forenames></author></authors><title>Circular Proofs for G\&quot;odel-L\&quot;ob Logic</title><categories>math.LO cs.LO</categories><comments>13 pages</comments><msc-class>03F07, 03F45</msc-class><journal-ref>Mathematical Notes. 2014. Vol. 96. No. 4. P. 575-585</journal-ref><doi>10.1134/S0001434614090326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sequent-style proof system for provability logic GL that admits
so-called circular proofs. For these proofs, the graph underlying a proof is
not a finite tree but is allowed to contain cycles. As an application, we
establish Lindon interpolation for GL syntactically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4005</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4005</id><created>2014-01-16</created><updated>2015-03-11</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Studying the SINR process of the typical user in Poisson networks by
  using its factorial moment measures</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a stationary Poisson point process, a wireless network model with
random propagation effects (shadowing and/or fading) is considered in order to
examine the process formed by the signal-to-interference-plus-noise ratio
(SINR) values experienced by a typical user with respect to all base stations
in the down-link channel. This SINR process is completely characterized by
deriving its factorial moment measures, which involve numerically tractable,
explicit integral expressions. This novel framework naturally leads to
expressions for the k-coverage probability, including the case of random SINR
threshold values considered in multi-tier network models. While the k-coverage
probabilities correspond to the marginal distributions of the order statistics
of the SINR process, a more general relation is presented connecting the
factorial moment measures of the SINR process to the joint densities of these
order statistics. This gives a way for calculating exact values of the coverage
probabilities arising in a general scenario of signal combination and
interference cancellation between base stations. The presented framework
consisting of mathematical representations of SINR characteristics with respect
to the factorial moment measures holds for the whole domain of SINR and is
amenable to considerable model extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4012</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4012</id><created>2014-01-16</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Towards a Cellular Automata Based Network Intrusion Detection System
  with Power Level Metric in Wireless Adhoc Networks (IDFADNWCA)</title><categories>cs.NI</categories><comments>2008 International Conference on Advanced Computer Theory and
  Engineering,978-0-7695-3489-3/08, IEEE. arXiv admin note: text overlap with
  arXiv:1401.3046</comments><doi>10.1109/ICACTE.2008.160</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adhoc wireless network with their changing topology and distributed nature
are more prone to intruders. The efficiency of an Intrusion detection system in
the case of an adhoc network is not only determined by its dynamicity in
monitoring but also in its flexibility in utilizing the available power in each
of its nodes. In this paper we propose a hybrid intrusion detection system,
based on a power level metric for potential adhoc hosts, which is used to
determine the duration for which a particular node can support a
network-monitoring node. The detection of intrusions in the network is done
with the help of Cellular Automata (CA). IDFADNWCA (Intrusion Detection for
Adhoc Networks with Cellular Automata) focuses on the available power level in
each of the nodes and determines the network monitors. Power Level Metric in
the network results in maintaining power for network monitoring, with monitors
changing often, since it is an iterative power optimal solution to identify
nodes for distributed agent based intrusion detection. The advantage of this
approach entails is the inherent flexibility it provides, by means of
considering only fewer nodes for reestablishing network monitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4020</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4020</id><created>2014-01-16</created><authors><author><keyname>Zhou</keyname><forenames>Tong</forenames></author></authors><title>Robust Recursive State Estimation with Random Measurements Droppings</title><categories>cs.SY</categories><comments>36 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recursive state estimation procedure is derived for a linear time varying
system with both parametric uncertainties and stochastic measurement droppings.
This estimator has a similar form as that of the Kalman filter with
intermittent observations, but its parameters should be adjusted when a plant
output measurement arrives. A new recursive form is derived for the
pseudo-covariance matrix of estimation errors, which plays important roles in
analyzing its asymptotic properties. Based on a Riemannian metric for positive
definite matrices, some necessary and sufficient conditions have been obtained
for the strict contractiveness of an iteration of this recursion. It has also
been proved that under some controllability and observability conditions, as
well as some weak requirements on measurement arrival probability, the gain
matrix of this recursive robust state estimator converges in probability one to
a stationary distribution. Numerical simulation results show that estimation
accuracy of the suggested procedure is more robust against parametric modelling
errors than the Kalman filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4023</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4023</id><created>2014-01-16</created><authors><author><keyname>Zhou</keyname><forenames>Tong</forenames></author></authors><title>Asymptotic Behavior of the Pseudo-Covariance Matrix of a Robust State
  Estimator with Intermittent Measurements</title><categories>cs.SY cs.IT math.IT</categories><comments>33 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ergodic properties and asymptotic stationarity are investigated in this paper
for the pseudo-covariance matrix (PCM) of a recursive state estimator which is
robust against parametric uncertainties and is based on plant output
measurements that may be randomly dropped. When the measurement dropping
process is described by a Markov chain and the modified plant is both
controllable and observable, it is proved that if the dropping probability is
less than 1, this PCM converges to a stationary distribution that is
independent of its initial values. A convergence rate is also provided. In
addition, it has also been made clear that when the initial value of the PCM is
set to the stabilizing solution of the algebraic Riccati equation related to
the robust state estimator without measurement dropping, this PCM converges to
an ergodic process. Based on these results, two approximations are derived for
the probability distribution function of the stationary PCM, as well as a bound
of approximation errors. A numerical example is provided to illustrate the
obtained theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4025</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4025</id><created>2014-01-16</created><updated>2014-01-17</updated><authors><author><keyname>Michalewski</keyname><forenames>Henryk</forenames></author><author><keyname>Skrzypczak</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Unambiguous Buchi is weak</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-deterministic automaton running on infinite trees is unambiguous if it
has at most one accepting run on every tree. The class of languages
recognisable by unambiguous tree automata is still not well-understood. In
particular, decidability of the problem whether a given language is
recognisable by some unambiguous automaton is open. Moreover, there are no
known upper bounds on the descriptive complexity of unambiguous languages among
all regular tree languages.
  In this paper we show the following complexity collapse: if a
non-deterministic parity tree automaton $A$ is unambiguous and its priorities
are between $i$ and $2n$ then the language recognised by $A$ is in the class
$Comp(i+1,2n)$. A particular case of this theorem is for $i=n=1$: if $A$ is an
unambiguous Buchi tree automaton then $L(A)$ is recognisable by a weak
alternating automaton (or equivalently definable in weak MSO). The main
motivation for this result is a theorem by Finkel and Simonnet stating that
every unambiguous Buchi automaton recognises a Borel language.
  The assumptions of the presented theorem are syntactic (we require one
automaton to be both unambiguous and of particular parity index). However, to
the authors' best knowledge this is the first theorem showing a collapse of the
parity index that exploits the fact that a given automaton is unambiguous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4063</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4063</id><created>2014-01-16</created><authors><author><keyname>Katarzy&#x144;ski</keyname><forenames>Jakub</forenames></author><author><keyname>Cytowski</keyname><forenames>Maciej</forenames></author></authors><title>Towards Autotuning of OpenMP Applications on Multicore Architectures</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe an autotuning tool for optimization of OpenMP
applications on highly multicore and multithreaded architectures. Our work was
motivated by in-depth performance analysis of scientific applications and
synthetic benchmarks on IBM Power 775 architecture. The tool provides an
automatic code instrumentation of OpenMP parallel regions. Based on measurement
of chosen hardware performance counters the tool decides on the number of
parallel threads that should be used for execution of chosen code fragments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4068</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4068</id><created>2014-01-16</created><updated>2015-11-23</updated><authors><author><keyname>Wollstadt</keyname><forenames>Patricia</forenames></author><author><keyname>Mart&#xed;nez-Zarzuela</keyname><forenames>Mario</forenames></author><author><keyname>Vicente</keyname><forenames>Raul</forenames></author><author><keyname>D&#xed;az-Pernas</keyname><forenames>Francisco J.</forenames></author><author><keyname>Wibral</keyname><forenames>Michael</forenames></author></authors><title>Efficient transfer entropy analysis of non-stationary neural time series</title><categories>cs.IT math.IT q-bio.NC</categories><comments>27 pages, 7 figures, submitted to PLOS ONE</comments><journal-ref>PLoS ONE 10(10): e0140530 (2014)</journal-ref><doi>10.1371/journal.pone.0102833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory allows us to investigate information processing in neural
systems in terms of information transfer, storage and modification. Especially
the measure of information transfer, transfer entropy, has seen a dramatic
surge of interest in neuroscience. Estimating transfer entropy from two
processes requires the observation of multiple realizations of these processes
to estimate associated probability density functions. To obtain these
observations, available estimators assume stationarity of processes to allow
pooling of observations over time. This assumption however, is a major obstacle
to the application of these estimators in neuroscience as observed processes
are often non-stationary. As a solution, Gomez-Herrero and colleagues
theoretically showed that the stationarity assumption may be avoided by
estimating transfer entropy from an ensemble of realizations. Such an ensemble
is often readily available in neuroscience experiments in the form of
experimental trials. Thus, in this work we combine the ensemble method with a
recently proposed transfer entropy estimator to make transfer entropy
estimation applicable to non-stationary time series. We present an efficient
implementation of the approach that deals with the increased computational
demand of the ensemble method's practical application. In particular, we use a
massively parallel implementation for a graphics processing unit to handle the
computationally most heavy aspects of the ensemble method. We test the
performance and robustness of our implementation on data from simulated
stochastic processes and demonstrate the method's applicability to
magnetoencephalographic data. While we mainly evaluate the proposed method for
neuroscientific data, we expect it to be applicable in a variety of fields that
are concerned with the analysis of information transfer in complex biological,
social, and artificial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4069</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4069</id><created>2014-01-15</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Binary Scientific Star Coauthors Core Size</title><categories>physics.soc-ph cs.DL</categories><comments>30 pages, 8 Tables, 1 Appendix, 40 references, to be published in
  Scientometrics</comments><journal-ref>Scientometrics (2014) 99: 331-351</journal-ref><doi>10.1007/s11192-014-1230-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is examined whether the relationship $ J \propto A/r^{\alpha}$, and the
subsequent coauthor core notion (Ausloos 2013), between the number ($J$) of
joint publications (JP) by a &quot;main scientist&quot; (LI) with her/his coauthors (CAs)
can be extended to a team-like system. This is done by considering that each
coauthor can be so strongly tied to the LI that they are forming {\it binary
scientific star} (BSS) systems with respect to their other collaborators.
Moreover, publications in peer review journals and in &quot;proceedings&quot;, both often
thought to be of &quot;different quality&quot;, are separetely distinguished. The role of
a time interval for measuring $J$ and $\alpha$ is also examined. New indirect
measures are also introduced.
  For making the point, two LI cases with numerous CAs are studied. It is found
that only a few BSS need to be usefully examined. The exponent $\alpha$ turns
out to be &quot;second scientist&quot; weakly dependent, but still &quot;size&quot; and
&quot;publication type&quot; dependent, according to the number of CAs or JP. The CA core
value is found to be (CA or JP) size and publication type dependent, but
remains in an understandable range. Somewhat unexpectedly, no special
qualitative difference on the binary scientific star CA core value is found
between publications in peer review journals and in proceedings.
  In conclusion, some remark is made on partner cooperation in BSS teams. It is
suggested that such measures can serve as criteria for distinguishing the role
of scientists in a team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4082</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4082</id><created>2014-01-16</created><updated>2014-05-30</updated><authors><author><keyname>Rezende</keyname><forenames>Danilo Jimenez</forenames></author><author><keyname>Mohamed</keyname><forenames>Shakir</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author></authors><title>Stochastic Backpropagation and Approximate Inference in Deep Generative
  Models</title><categories>stat.ML cs.AI cs.LG stat.CO stat.ME</categories><comments>Appears In Proceedings of the 31st International Conference on
  Machine Learning (ICML), JMLR: W\&amp;CP volume 32, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We marry ideas from deep neural networks and approximate Bayesian inference
to derive a generalised class of deep, directed generative models, endowed with
a new algorithm for scalable inference and learning. Our algorithm introduces a
recognition model to represent approximate posterior distributions, and that
acts as a stochastic encoder of the data. We develop stochastic
back-propagation -- rules for back-propagation through stochastic variables --
and use this to develop an algorithm that allows for joint optimisation of the
parameters of both the generative and recognition model. We demonstrate on
several real-world data sets that the model generates realistic samples,
provides accurate imputations of missing data and is a useful tool for
high-dimensional data visualisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4092</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4092</id><created>2014-01-16</created><authors><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author><author><keyname>Xiao</keyname><forenames>David</forenames></author></authors><title>Redrawing the Boundaries on Purchasing Data from Privacy-Sensitive
  Individuals</title><categories>cs.GT</categories><journal-ref>Proc. 5th ITCS, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new positive and negative results concerning the existence of
truthful and individually rational mechanisms for purchasing private data from
individuals with unbounded and sensitive privacy preferences. We strengthen the
impossibility results of Ghosh and Roth (EC 2011) by extending it to a much
wider class of privacy valuations. In particular, these include privacy
valuations that are based on ({\epsilon}, {\delta})-differentially private
mechanisms for non-zero {\delta}, ones where the privacy costs are measured in
a per-database manner (rather than taking the worst case), and ones that do not
depend on the payments made to players (which might not be observable to an
adversary). To bypass this impossibility result, we study a natural special
setting where individuals have mono- tonic privacy valuations, which captures
common contexts where certain values for private data are expected to lead to
higher valuations for privacy (e.g. having a particular disease). We give new
mech- anisms that are individually rational for all players with monotonic
privacy valuations, truthful for all players whose privacy valuations are not
too large, and accurate if there are not too many players with too-large
privacy valuations. We also prove matching lower bounds showing that in some
respects our mechanism cannot be improved significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4105</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4105</id><created>2014-01-16</created><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author></authors><title>Learning $\ell_1$-based analysis and synthesis sparsity priors using
  bi-level optimization</title><categories>cs.CV</categories><comments>5 pages, 1 figure, appear at the Workshop on Analysis Operator
  Learning vs. Dictionary Learning, NIPS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the analysis operator and synthesis dictionary learning problems
based on the the $\ell_1$ regularized sparse representation model. We reveal
the internal relations between the $\ell_1$-based analysis model and synthesis
model. We then introduce an approach to learn both analysis operator and
synthesis dictionary simultaneously by using a unified framework of bi-level
optimization. Our aim is to learn a meaningful operator (dictionary) such that
the minimum energy solution of the analysis (synthesis)-prior based model is as
close as possible to the ground-truth. We solve the bi-level optimization
problem using the implicit differentiation technique. Moreover, we demonstrate
the effectiveness of our leaning approach by applying the learned analysis
operator (dictionary) to the image denoising task and comparing its performance
with state-of-the-art methods. Under this unified framework, we can compare the
performance of the two types of priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4107</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4107</id><created>2014-01-16</created><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author><author><keyname>Ranftl</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author></authors><title>Revisiting loss-specific training of filter-based MRFs for image
  restoration</title><categories>cs.CV</categories><comments>10 pages, 2 figures, appear at 35th German Conference, GCPR 2013,
  Saarbr\&quot;ucken, Germany, September 3-6, 2013. Proceedings</comments><doi>10.1007/978-3-642-40602-7_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now well known that Markov random fields (MRFs) are particularly
effective for modeling image priors in low-level vision. Recent years have seen
the emergence of two main approaches for learning the parameters in MRFs: (1)
probabilistic learning using sampling-based algorithms and (2) loss-specific
training based on MAP estimate. After investigating existing training
approaches, it turns out that the performance of the loss-specific training has
been significantly underestimated in existing work. In this paper, we revisit
this approach and use techniques from bi-level optimization to solve it. We
show that we can get a substantial gain in the final performance by solving the
lower-level problem in the bi-level framework with high accuracy using our
newly proposed algorithm. As a result, our trained model is on par with highly
specialized image denoising algorithms and clearly outperforms
probabilistically trained MRF models. Our findings suggest that for the
loss-specific training scheme, solving the lower-level problem with higher
accuracy is beneficial. Our trained model comes along with the additional
advantage, that inference is extremely efficient. Our GPU-based implementation
takes less than 1s to produce state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4112</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4112</id><created>2014-01-16</created><updated>2014-05-09</updated><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Ranftl</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>A bi-level view of inpainting - based image compression</title><categories>cs.CV</categories><comments>8 pages, 4 figures, best paper award of CVWW 2014, Computer Vision
  Winter Workshop, K\v{r}tiny, Czech Republic, 3-5th February 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inpainting based image compression approaches, especially linear and
non-linear diffusion models, are an active research topic for lossy image
compression. The major challenge in these compression models is to find a small
set of descriptive supporting points, which allow for an accurate
reconstruction of the original image. It turns out in practice that this is a
challenging problem even for the simplest Laplacian interpolation model. In
this paper, we revisit the Laplacian interpolation compression model and
introduce two fast algorithms, namely successive preconditioning primal dual
algorithm and the recently proposed iPiano algorithm, to solve this problem
efficiently. Furthermore, we extend the Laplacian interpolation based
compression model to a more general form, which is based on principles from
bi-level optimization. We investigate two different variants of the Laplacian
model, namely biharmonic interpolation and smoothed Total Variation
regularization. Our numerical results show that significant improvements can be
obtained from the biharmonic interpolation model, and it can recover an image
with very high quality from only 5% pixels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4126</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4126</id><created>2014-01-16</created><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author></authors><title>Lower bounds on the communication complexity of two-party (quantum)
  processes</title><categories>quant-ph cs.IT math.IT</categories><comments>Conference version. A more extensive version with more details will
  be available soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of state preparation, its transmission and subsequent measurement
can be classically simulated through the communication of some amount of
classical information. Recently, we proved that the minimal communication cost
is the minimum of a convex functional over a space of suitable probability
distributions. It is now proved that this optimization problem is the dual of a
geometric programming maximization problem, which displays some appealing
properties. First, the number of variables grows linearly with the input size.
Second, the objective function is linear in the input parameters and the
variables. Finally, the constraints do not depend on the input parameters.
These properties imply that, once a feasible point is found, the computation of
a lower bound on the communication cost in any two-party process is linearly
complex. The studied scenario goes beyond quantum processes and includes the
communication complexity scenario introduced by Yao. We illustrate the method
by analytically deriving some non-trivial lower bounds. Finally, we conjecture
the lower bound $n 2^n$ for a noiseless quantum channel with capacity $n$
qubits. This bound can have an interesting consequence in the context of the
recent quantum-foundational debate on the reality of the quantum state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4127</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4127</id><created>2014-01-08</created><updated>2014-07-19</updated><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Cognitive Robotics: for never was a story of more woe than this</title><categories>cs.RO cs.CY q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are now on the verge of the next technical revolution - robots are going
to invade our lives. However, to interact with humans or to be incorporated
into a human &quot;collective&quot; robots have to be provided with some human-like
cognitive abilities. What does it mean? - nobody knows. But robotics research
communities are trying hard to find out a way to cope with this problem.
Meanwhile, despite abundant funding these efforts did not lead to any
meaningful result (only in Europe, only in the past ten years, Cognitive
Robotics research funding has reached a ceiling of 1.39 billion euros). In the
next ten years, a similar budget is going to be spent to tackle the Cognitive
Robotics problems in the frame of the Human Brain Project. There is no reason
to expect that this time the result will be different. I would like to try to
explain why I'm so unhappy about this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4128</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4128</id><created>2014-01-16</created><authors><author><keyname>Cappelaere</keyname><forenames>Charles-Henri</forenames></author><author><keyname>Dubois</keyname><forenames>R.</forenames></author><author><keyname>Roussel</keyname><forenames>P.</forenames></author><author><keyname>Dreyfus</keyname><forenames>G.</forenames></author></authors><title>Towards the selection of patients requiring ICD implantation by
  automatic classification from Holter monitoring indices</title><categories>cs.LG stat.AP</categories><comments>Computing in Cardiology, Saragosse : Espagne (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is to optimize the selection of prophylactic
cardioverter defibrillator implantation candidates. Currently, the main
criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)
whose specificity is relatively poor. We designed two classifiers aimed to
predict, from long term ECG recordings (Holter), whether a low-LVEF patient is
likely or not to undergo ventricular arrhythmia in the next six months. One
classifier is a single hidden layer neural network whose variables are the most
relevant features extracted from Holter recordings, and the other classifier
has a structure that capitalizes on the physiological decomposition of the
arrhythmogenic factors into three disjoint groups: the myocardial substrate,
the triggers and the autonomic nervous system (ANS). In this ad hoc network,
the features were assigned to each group; one neural network classifier per
group was designed and its complexity was optimized. The outputs of the
classifiers were fed to a single neuron that provided the required probability
estimate. The latter was thresholded for final discrimination A dataset
composed of 186 pre-implantation 30-mn Holter recordings of patients equipped
with an implantable cardioverter defibrillator (ICD) in primary prevention was
used in order to design and test this classifier. 44 out of 186 patients
underwent at least one treated ventricular arrhythmia during the six-month
follow-up period. Performances of the designed classifier were evaluated using
a cross-test strategy that consists in splitting the database into several
combinations of a training set and a test set. The average arrhythmia
prediction performances of the ad-hoc classifier are NPV = 77% $\pm$ 13% and
PPV = 31% $\pm$ 19% (Negative Predictive Value $\pm$ std, Positive Predictive
Value $\pm$ std). According to our study, improving prophylactic
ICD-implantation candidate selection by automatic classification from ECG
features may be possible, but the availability of a sizable dataset appears to
be essential to decrease the number of False Negatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4130</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4130</id><created>2014-01-16</created><authors><author><keyname>Bonnet</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Lin</keyname><forenames>Anthony W.</forenames></author></authors><title>Analysis of Probabilistic Basic Parallel Processes</title><categories>cs.LO</categories><comments>This is the technical report for a FoSSaCS'14 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basic Parallel Processes (BPPs) are a well-known subclass of Petri Nets. They
are the simplest common model of concurrent programs that allows unbounded
spawning of processes. In the probabilistic version of BPPs, every process
generates other processes according to a probability distribution. We study the
decidability and complexity of fundamental qualitative problems over
probabilistic BPPs -- in particular reachability with probability 1 of
different classes of target sets (e.g. upward-closed sets). Our results concern
both the Markov-chain model, where processes are scheduled randomly, and the
MDP model, where processes are picked by a scheduler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4134</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4134</id><created>2014-01-16</created><authors><author><keyname>Pratas</keyname><forenames>Diogo</forenames></author><author><keyname>Pinho</keyname><forenames>Armando J.</forenames></author></authors><title>A conditional compression distance that unveils insights of the genomic
  evolution</title><categories>q-bio.GN cs.IT math.IT</categories><comments>Full version of DCC 2014 paper &quot;A conditional compression distance
  that unveils insights of the genomic evolution&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a compression-based distance for genomic sequences. Instead of
using the usual conjoint information content, as in the classical Normalized
Compression Distance (NCD), it uses the conditional information content. To
compute this Normalized Conditional Compression Distance (NCCD), we need a
normal conditional compressor, that we built using a mixture of static and
dynamic finite-context models. Using this approach, we measured chromosomal
distances between Hominidae primates and also between Muroidea (rat and mouse),
observing several insights of evolution that so far have not been reported in
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4140</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4140</id><created>2014-01-16</created><authors><author><keyname>Mathiesen</keyname><forenames>Joachim</forenames></author><author><keyname>Angheluta</keyname><forenames>Luiza</forenames></author><author><keyname>Jensen</keyname><forenames>Mogens H.</forenames></author></authors><title>Statistics of co-occurring keywords on Twitter</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social media such as the micro-blogging site Twitter has become a rich
source of real-time data on online human behaviors. Here we analyze the
occurrence and co-occurrence frequency of keywords in user posts on Twitter.
From the occurrence rate of major international brand names, we provide
examples on predictions of brand-user behaviors. From the co-occurrence rates,
we further analyze the user-perceived relationships between international brand
names and construct the corresponding relationship networks. In general the
user activity on Twitter is highly intermittent and we show that the occurrence
rate of brand names forms a highly correlated time signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4143</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4143</id><created>2014-01-16</created><authors><author><keyname>Park</keyname><forenames>Sunho</forenames></author><author><keyname>Hwang</keyname><forenames>TaeHyun</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Convex Optimization for Binary Classifier Aggregation in Multiclass
  Problems</title><categories>cs.LG</categories><comments>Appeared in Proceedings of the 2014 SIAM International Conference on
  Data Mining (SDM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiclass problems are often decomposed into multiple binary problems that
are solved by individual binary classifiers whose results are integrated into a
final answer. Various methods, including all-pairs (APs), one-versus-all (OVA),
and error correcting output code (ECOC), have been studied, to decompose
multiclass problems into binary problems. However, little study has been made
to optimally aggregate binary problems to determine a final answer to the
multiclass problem. In this paper we present a convex optimization method for
an optimal aggregation of binary classifiers to estimate class membership
probabilities in multiclass problems. We model the class membership probability
as a softmax function which takes a conic combination of discrepancies induced
by individual binary classifiers, as an input. With this model, we formulate
the regularized maximum likelihood estimation as a convex optimization problem,
which is solved by the primal-dual interior point method. Connections of our
method to large margin classifiers are presented, showing that the large margin
formulation can be considered as a limiting case of our convex formulation.
Numerical experiments on synthetic and real-world data sets demonstrate that
our method outperforms existing aggregation methods as well as direct methods,
in terms of the classification accuracy and the quality of class membership
probability estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4144</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4144</id><created>2014-01-16</created><authors><author><keyname>Besnard</keyname><forenames>Philippe</forenames><affiliation>INRIA - IRISA, IRIT</affiliation></author><author><keyname>Cordier</keyname><forenames>Marie-Odile</forenames><affiliation>INRIA - IRISA, UR1</affiliation></author><author><keyname>Moinard</keyname><forenames>Yves</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Arguments using ontological and causal knowledge</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>JIAF 2013 (Septi\`emes Journ\'ees de l'Intelligence Artificielle
  Fondamentale) (2013) 41-48</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate an approach to reasoning about causes through argumentation.
We consider a causal model for a physical system, and look for arguments about
facts. Some arguments are meant to provide explanations of facts whereas some
challenge these explanations and so on. At the root of argumentation here, are
causal links ({A_1, ... ,A_n} causes B) and ontological links (o_1 is_a o_2).
We present a system that provides a candidate explanation ({A_1, ... ,A_n}
explains {B_1, ... ,B_m}) by resorting to an underlying causal link
substantiated with appropriate ontological links. Argumentation is then at work
from these various explaining links. A case study is developed: a severe storm
Xynthia that devastated part of France in 2010, with an unaccountably high
number of casualties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4147</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4147</id><created>2014-01-16</created><authors><author><keyname>Ahmed</keyname><forenames>Mohammed F. A.</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Simple Semi-Distributed Lifetime Maximizing Strategy via Power
  Allocation in Collaborative Beamforming for Wireless Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>27 pages, 11 figures, Finished in March 2011, Unpublished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy-efficient communication is an important issue in wireless sensor
networks (WSNs) consisting of large number of energy-constrained sensor nodes.
Indeed, sensor nodes have different energy budgets assigned to data
transmission at individual nodes. Therefore, without energy-aware transmission
schemes, energy can deplete from sensor nodes with smaller energy budget faster
than from the rest of the sensor nodes in WSNs. This reduces the coverage area
as well as the lifetime of WSNs. Collaborative beamforming (CB) has been
proposed originally to achieve directional gain, however, it also inherently
distributes the corresponding energy consumption over the collaborative sensor
nodes. In fact, CB can be seen as a physical layer solution (versus the media
access control/network layer solution) to balance the lifetimes of individual
sensor nodes and extend the lifetime of the whole WSN. However, the
introduction of energy-aware CB schemes is critical for extending the WSNs
lifetime.
  In this paper, CB with power allocation (CB-PA) is developed to extend the
lifetime of a cluster of collaborative sensor nodes by balancing the individual
sensor node lifetimes. A novel strategy is proposed to utilize the residual
energy information available at each sensor node. It adjusts the energy
consumption rate at each sensor node while achieving the required average
signal-to-noise ratio (SNR) at the destination. It is a semi-distributed
strategy and it maintains average SNR. Different factors affecting the energy
consumption are studied as well. Simulation results show that CB-PA outperforms
CB with Equal Power Allocation (CB-EPA) in terms of extending the lifetime of a
cluster of collaborative nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4151</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4151</id><created>2014-01-16</created><authors><author><keyname>Culnane</keyname><forenames>Chris</forenames></author><author><keyname>Schneider</keyname><forenames>Steve</forenames></author></authors><title>A Peered Bulletin Board for Robust Use in Verifiable Voting Systems</title><categories>cs.CR</categories><comments>49 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web Bulletin Board (WBB) is a key component of verifiable election
systems. It is used in the context of election verification to publish evidence
of voting and tallying that voters and officials can check, and where
challenges can be launched in the event of malfeasance. In practice, the
election authority has responsibility for implementing the web bulletin board
correctly and reliably, and will wish to ensure that it behaves correctly even
in the presence of failures and attacks. To ensure robustness, an
implementation will typically use a number of peers to be able to provide a
correct service even when some peers go down or behave dishonestly. In this
paper we propose a new protocol to implement such a Web Bulletin Board,
motivated by the needs of the vVote verifiable voting system. Using a
distributed algorithm increases the complexity of the protocol and requires
careful reasoning in order to establish correctness. Here we use the Event-B
modelling and refinement approach to establish correctness of the peered design
against an idealised specification of the bulletin board behaviour. In
particular we show that for n peers, a threshold of t &gt; 2n/3 peers behaving
correctly is sufficient to ensure correct behaviour of the bulletin board
distributed design. The algorithm also behaves correctly even if honest or
dishonest peers temporarily drop out of the protocol and then return. The
verification approach also establishes that the protocols used within the
bulletin board do not interfere with each other. This is the first time a
peered web bulletin board suite of protocols has been formally verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4158</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4158</id><created>2014-01-16</created><authors><author><keyname>Froese</keyname><forenames>Tom</forenames></author><author><keyname>Iizuka</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Embodied social interaction constitutes social cognition in pairs of
  humans: A minimalist virtual reality experiment</title><categories>nlin.AO cs.HC cs.MA</categories><comments>10 pages, 5 figures, 2 tables</comments><msc-class>91B99</msc-class><acm-class>H.5.1; H.5.2; H.5.3</acm-class><journal-ref>Froese, T., Iizuka, H. &amp; Ikegami, T. Embodied social interaction
  constitutes social cognition in pairs of humans: A minimalist virtual reality
  experiment. Sci. Rep. 4, 3672; DOI:10.1038/srep03672 (2014)</journal-ref><doi>10.1038/srep03672</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Scientists have traditionally limited the mechanisms of social cognition to
one brain, but recent approaches claim that interaction also realizes cognitive
work. Experiments under constrained virtual settings revealed that interaction
dynamics implicitly guide social cognition. Here we show that embodied social
interaction can be constitutive of agency detection and of experiencing
another`s presence. Pairs of participants moved their &quot;avatars&quot; along an
invisible virtual line and could make haptic contact with three identical
objects, two of which embodied the other`s motions, but only one, the other`s
avatar, also embodied the other`s contact sensor and thereby enabled responsive
interaction. Co-regulated interactions were significantly correlated with
identifications of the other`s avatar and reports of the clearest awareness of
the other`s presence. These results challenge folk psychological notions about
the boundaries of mind, but make sense from evolutionary and developmental
perspectives: an extendible mind can offload cognitive work into its
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4161</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4161</id><created>2014-01-16</created><updated>2015-02-10</updated><authors><author><keyname>Bardhan</keyname><forenames>Bhaskar Roy</forenames></author><author><keyname>Garcia-Patron</keyname><forenames>Raul</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Strong converse for the classical capacity of optical quantum
  communication channels</title><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, final version accepted into IEEE Transactions on
  Information Theory. arXiv admin note: text overlap with arXiv:1312.3287</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 4, pages
  1842-1850, April 2015</journal-ref><doi>10.1109/TIT.2015.2403840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the classical capacity of optical quantum channels as a sharp
transition between two regimes---one which is an error-free regime for
communication rates below the capacity, and the other in which the probability
of correctly decoding a classical message converges exponentially fast to zero
if the communication rate exceeds the classical capacity. This result is
obtained by proving a strong converse theorem for the classical capacity of all
phase-insensitive bosonic Gaussian channels, a well-established model of
optical quantum communication channels, such as lossy optical fibers, amplifier
and free-space communication. The theorem holds under a particular
photon-number occupation constraint, which we describe in detail in the paper.
Our result bolsters the understanding of the classical capacity of these
channels and opens the path to applications, such as proving the security of
noisy quantum storage models of cryptography with optical links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4189</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4189</id><created>2014-01-16</created><updated>2015-09-30</updated><authors><author><keyname>Du</keyname><forenames>Jinfeng</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Scalable Capacity Bounding Models for Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of network equivalence theory developed by Koetter et al.
introduces a notion of channel emulation to construct noiseless networks as
upper (resp. lower) bounding models, which can be used to calculate the outer
(resp. inner) bounds for the capacity region of the original noisy network.
Based on the network equivalence framework, this paper presents scalable upper
and lower bounding models for wireless networks with potentially many nodes. A
channel decoupling method is proposed to decompose wireless networks into
decoupled multiple-access channels (MACs) and broadcast channels (BCs). The
upper bounding model, consisting of only point-to-point bit pipes, is
constructed by firstly extending the &quot;one-shot&quot; upper bounding models developed
by Calmon et al. and then integrating them with network equivalence tools. The
lower bounding model, consisting of both point-to-point and point-to-points bit
pipes, is constructed based on a two-step update of the lower bounding models
to incorporate the broadcast nature of wireless transmission. The main
advantages of the proposed methods are their simplicity and the fact that they
can be extended easily to large networks with a complexity that grows linearly
with the number of nodes. It is demonstrated that the resulting upper and lower
bounds can approach the capacity in some setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4205</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4205</id><created>2014-01-16</created><authors><author><keyname>Kalimeri</keyname><forenames>Maria</forenames></author><author><keyname>Constantoudis</keyname><forenames>Vassilios</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Constantinos</forenames></author><author><keyname>Karamanos</keyname><forenames>Kostantinos</forenames></author><author><keyname>Diakonos</keyname><forenames>Fotis K.</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Haris</forenames></author></authors><title>Entropy analysis of word-length series of natural language texts:
  Effects of text language and genre</title><categories>cs.CL physics.data-an</categories><comments>9 pages, 7 figures</comments><journal-ref>International Journal of Bifurcation and Chaos, 22, 1250223,
  (2012)</journal-ref><doi>10.1142/S0218127412502239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We estimate the $n$-gram entropies of natural language texts in word-length
representation and find that these are sensitive to text language and genre. We
attribute this sensitivity to changes in the probability distribution of the
lengths of single words and emphasize the crucial role of the uniformity of
probabilities of having words with length between five and ten. Furthermore,
comparison with the entropies of shuffled data reveals the impact of word
length correlations on the estimated $n$-gram entropies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4208</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4208</id><created>2014-01-16</created><authors><author><keyname>Cannarella</keyname><forenames>John</forenames></author><author><keyname>Spechler</keyname><forenames>Joshua A.</forenames></author></authors><title>Epidemiological modeling of online social network dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 4 figures, 2 tables</comments><acm-class>J.4; K.4; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last decade has seen the rise of immense online social networks (OSNs)
such as MySpace and Facebook. In this paper we use epidemiological models to
explain user adoption and abandonment of OSNs, where adoption is analogous to
infection and abandonment is analogous to recovery. We modify the traditional
SIR model of disease spread by incorporating infectious recovery dynamics such
that contact between a recovered and infected member of the population is
required for recovery. The proposed infectious recovery SIR model (irSIR model)
is validated using publicly available Google search query data for &quot;MySpace&quot; as
a case study of an OSN that has exhibited both adoption and abandonment phases.
The irSIR model is then applied to search query data for &quot;Facebook,&quot; which is
just beginning to show the onset of an abandonment phase. Extrapolating the
best fit model into the future predicts a rapid decline in Facebook activity in
the next few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4221</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4221</id><created>2014-01-16</created><authors><author><keyname>Xie</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Wensheng</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Hu</keyname><forenames>Wenrui</forenames></author><author><keyname>Qu</keyname><forenames>Yanyun</forenames></author><author><keyname>Wang</keyname><forenames>Hanzi</forenames></author></authors><title>Distortion-driven Turbulence Effect Removal using Variational Model</title><categories>cs.CV</categories><comments>28 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It remains a challenge to simultaneously remove geometric distortion and
space-time-varying blur in frames captured through a turbulent atmospheric
medium. To solve, or at least reduce these effects, we propose a new scheme to
recover a latent image from observed frames by integrating a new variational
model and distortion-driven spatial-temporal kernel regression. The proposed
scheme first constructs a high-quality reference image from the observed frames
using low-rank decomposition. Then, to generate an improved registered
sequence, the reference image is iteratively optimized using a variational
model containing a new spatial-temporal regularization. The proposed fast
algorithm efficiently solves this model without the use of partial differential
equations (PDEs). Next, to reduce blur variation, distortion-driven
spatial-temporal kernel regression is carried out to fuse the registered
sequence into one image by introducing the concept of the near-stationary
patch. Applying a blind deconvolution algorithm to the fused image produces the
final output. Extensive experimental testing shows, both qualitatively and
quantitatively, that the proposed method can effectively alleviate distortion
and blur and recover details of the original scene compared to state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4230</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4230</id><created>2014-01-16</created><authors><author><keyname>Rasouli</keyname><forenames>Mohammad</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Electricity Pooling Markets with Strategic Producers Possessing
  Asymmetric Information I: Elastic Demand</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the restructured electricity industry, electricity pooling markets are an
oligopoly with strategic producers possessing private information (private
production cost function). We focus on pooling markets where aggregate demand
is represented by a non-strategic agent. We consider demand to be elastic.
  We propose a market mechanism that has the following features. (F1) It is
individually rational. (F2) It is budget balanced. (F3) It is price efficient,
that is, at equilibrium the price of electricity is equal to the marginal cost
of production. (F4) The energy production profile corresponding to every
non-zero Nash equilibrium of the game induced by the mechanism is a solution of
the corresponding centralized problem where the objective is the maximization
of the sum of the producers' and consumers' utilities.
  We identify some open problems associated with our approach to electricity
pooling markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4234</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4234</id><created>2014-01-16</created><authors><author><keyname>Zuo</keyname><forenames>Xiang</forenames></author><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Skvoretz</keyname><forenames>John</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>The power of indirect social ties</title><categories>cs.SI physics.soc-ph</categories><comments>Technical Report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  While direct social ties have been intensely studied in the context of
computer-mediated social networks, indirect ties (e.g., friends of friends)
have seen little attention. Yet in real life, we often rely on friends of our
friends for recommendations (of good doctors, good schools, or good
babysitters), for introduction to a new job opportunity, and for many other
occasional needs. In this work we attempt to 1) quantify the strength of
indirect social ties, 2) validate it, and 3) empirically demonstrate its
usefulness for distributed applications on two examples. We quantify social
strength of indirect ties using a(ny) measure of the strength of the direct
ties that connect two people and the intuition provided by the sociology
literature. We validate the proposed metric experimentally by comparing
correlations with other direct social tie evaluators. We show via data-driven
experiments that the proposed metric for social strength can be used
successfully for social applications. Specifically, we show that it alleviates
known problems in friend-to-friend storage systems by addressing two previously
documented shortcomings: reduced set of storage candidates and data
availability correlations. We also show that it can be used for predicting the
effects of a social diffusion with an accuracy of up to 93.5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4236</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4236</id><created>2014-01-16</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>The Impact of Phase Fading on the Dirty Paper Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of phase fading on the classical Costa dirty paper coding channel
is studied. We consider a variation of this channel model in which the
amplitude of the interference sequence is known at the transmitter while its
phase is known at the receiver. Although the capacity of this channel has
already been established, it is expressed using an auxiliary random variable
and as the solution of a maximization problem. To circumvent the difficulty
evaluating capacity, we derive alternative inner and outer bounds and show that
the two expressions are to within a finite distance. This provide an
approximate characterization of the capacity which depends only on the channel
parameters. We consider, in particular, two distributions of the phase fading:
circular binomial and circular uniform. The first distribution models the
scenario in which the transmitter has a minimal uncertainty over the phase of
the interference while the second distribution models complete uncertainty. For
circular binomial fading, we show that binning with Gaussian signaling still
approaches capacity, as in the channel without phase fading. In the case of
circular uniform fading, instead, binning with Gaussian signaling is no longer
effective and novel interference avoidance strategies are developed to approach
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4237</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4237</id><created>2014-01-16</created><updated>2014-01-23</updated><authors><author><keyname>Bagheri</keyname><forenames>Saeed</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>The Cognitive Compressive Sensing Problem</title><categories>cs.IT math.IT math.OC</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Cognitive Compressive Sensing (CCS) problem, a Cognitive Receiver (CR)
seeks to optimize the reward obtained by sensing an underlying $N$ dimensional
random vector, by collecting at most $K$ arbitrary projections of it. The $N$
components of the latent vector represent sub-channels states, that change
dynamically from &quot;busy&quot; to &quot;idle&quot; and vice versa, as a Markov chain that is
biased towards producing sparse vectors. To identify the optimal strategy we
formulate the Multi-Armed Bandit Compressive Sensing (MAB-CS) problem,
generalizing the popular Cognitive Spectrum Sensing model, in which the CR can
sense $K$ out of the $N$ sub-channels, as well as the typical static setting of
Compressive Sensing, in which the CR observes $K$ linear combinations of the
$N$ dimensional sparse vector. The CR opportunistic choice of the sensing
matrix should balance the desire of revealing the state of as many dimensions
of the latent vector as possible, while not exceeding the limits beyond which
the vector support is no longer uniquely identifiable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4245</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4245</id><created>2014-01-17</created><authors><author><keyname>Gupta</keyname><forenames>Ekta</forenames></author><author><keyname>Kalyani</keyname></author><author><keyname>Nitin</keyname></author></authors><title>Preserving the Basic Property of Stable Matching by Deleting a pair</title><categories>cs.DS</categories><comments>5 pages, 6 tables, 2 figures, International Conference in Distributed
  Computing &amp; Internet Technology (ICDCIT-2014)
  http://www.ijcaonline.org/proceedings/icdcit2014/number1/14379-1303</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the transition of a male-pessimal matching set to
optimal when it is a man-oriented approach by deleting a pair from matching set
considering the score based approach. A descriptive explanation of the proposed
algorithm both in a sequential and parallel manner is given. The comparison
based theoretical analysis shows that the best case of the algorithm is lower
bound of n3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4248</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4248</id><created>2014-01-17</created><updated>2014-12-05</updated><authors><author><keyname>Jang</keyname><forenames>Dae-Sung</forenames></author><author><keyname>Choi</keyname><forenames>Han-Lim</forenames></author></authors><title>Complexity Analysis of Heuristic Pulse Interleaving Algorithms for
  Multi-Target Tracking with Multiple Simultaneous Receive Beams</title><categories>cs.SY</categories><comments>29 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents heuristic algorithms for interleaved pulse scheduling
problems on multi-target tracking in pulse Doppler phased array radars that can
process multiple simultaneous received beams. The interleaved pulse scheduling
problems for element and subarray level digital beamforming architectures are
formulated as the same integer program and the asymptotic time complexities of
the algorithms are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4251</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4251</id><created>2014-01-17</created><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author></authors><title>Bitwise MAP Algorithm for Group Testing based on Holographic
  Transformation</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an exact bitwise MAP (Maximum A Posteriori) estimation
algorithm for group testing problems is presented. We assume a simplest
non-adaptive group testing scenario including N-objects with binary status and
M-disjunctive tests. If a group contains a positive object, the test result for
the group is assumed to be one; otherwise, the test result becomes zero. Our
inference problem is to evaluate the posterior probabilities of the objects
from the observation of M-test results and from our knowledge on the prior
probabilities for objects. The heart of the algorithm is the dual expression of
the posterior values. The derivation of the dual expression can be naturally
described based on a holographic transformation to the normal factor graph
(NFG) representing the inference problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4254</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4254</id><created>2014-01-17</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Goal-oriented Composition of Software Process Patterns</title><categories>cs.SE</categories><comments>5 pages</comments><journal-ref>Proceedings of the 6th International Workshop on Software Process
  Simulation and Modeling (ProSim 2005), pages 164-168, St. Louis, Missouri,
  USA, May 14-15 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of high-quality software or software-intensive systems
requires custom-tailored process models that fit the organizational and project
goals as well as the development contexts. These models are a necessary
prerequisite for creating project plans that are expected to fulfill business
goals. Although project planners require individual process models
custom-tailored to their constraints, software or system developing
organizations also require generic processes (i.e., reference processes) that
capture project-independent knowledge for similar development contexts. The
latter is emphazised by assessment approaches (such as CMMI, SPICE) that
require explicit process descriptions in order to reach a certain capability or
maturity level. Among other concepts such as polymorphism, templates, or
generator-based descriptions, software process patterns are used to describe
generic process knowledge. Several approaches for describing the architecture
of process patterns have already been published (e.g., [7]). However, there is
a lack of descriptions on how to compose process patterns for a specific
decelopment context in order to gain a custom-tailored process model for a
project. This paper focuses on the composition of process patterns in a
goal-oriented way. First, the paper describes which information a process
pattern should contain so that it can be used for systematic composition.
Second, a composition method is sketched. Afterwards, the results of a
proof-of-concept evaluation of the method are described. Finally, the paper is
summarized and open research questions are sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4256</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4256</id><created>2014-01-17</created><authors><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ishigai</keyname><forenames>Yasushi</forenames></author><author><keyname>Yokoyama</keyname><forenames>Kenji</forenames></author><author><keyname>Kikuchi</keyname><forenames>Nahomi</forenames></author><author><keyname>Kawaguchi</keyname><forenames>T.</forenames></author></authors><title>Lessons Learned and Results from Applying Data-Driven Cost Estimation to
  Industrial Data Sets</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=4335245</comments><journal-ref>Proceedings of the 6th International Conference on the Quality of
  Information and Communications Technology (QUATIC 2007), pages 177-186,
  Lisbon New University, Lisbon, Portugal, September 12-14 2007</journal-ref><doi>10.1109/QUATIC.2007.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of cost-relevant data in industry allows
companies to apply data-intensive estimation methods. However, available data
are often inconsistent, invalid, or incomplete, so that most of the existing
data-intensive estimation methods cannot be applied. Only few estimation
methods can deal with imperfect data to a certain extent (e.g., Optimized Set
Reduction, OSR(c)). Results from evaluating these methods in practical
environments are rare. This article describes a case study on the application
of OSR(c) at Toshiba Information Systems (Japan) Corporation. An important
result of the case study is that estimation accuracy significantly varies with
the data sets used and the way of preprocessing these data. The study supports
current results in the area of quantitative cost estimation and clearly
illustrates typical problems. Experiences, lessons learned, and recommendations
with respect to data preprocessing and data-intensive cost estimation in
general are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4267</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4267</id><created>2014-01-17</created><authors><author><keyname>Oishi</keyname><forenames>Koji</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Abeliuk</keyname><forenames>Andres</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Iterated crowdsourcing dilemma game</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><comments>4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has enabled the emergence of collective problem solving, also
known as crowdsourcing, as a viable option for solving complex tasks. However,
the openness of crowdsourcing presents a challenge because solutions obtained
by it can be sabotaged, stolen, and manipulated at a low cost for the attacker.
We extend a previously proposed crowdsourcing dilemma game to an iterated game
to address this question. We enumerate pure evolutionarily stable strategies
within the class of so-called reactive strategies, i.e., those depending on the
last action of the opponent. Among the 4096 possible reactive strategies, we
find 16 strategies each of which is stable in some parameter regions. Repeated
encounters of the players can improve social welfare when the damage inflicted
by an attack and the cost of attack are both small. Under the current
framework, repeated interactions do not really ameliorate the crowdsourcing
dilemma in a majority of the parameter space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4269</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4269</id><created>2014-01-17</created><updated>2014-01-25</updated><authors><author><keyname>Cai</keyname><forenames>Sheng</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author></authors><title>SUPER: Sparse signals with Unknown Phases Efficiently Recovered</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose ${\bf x}$ is any exactly $k$-sparse vector in $\mathbb{C}^{n}$. We
present a class of phase measurement matrix $A$ in $\mathbb{C}^{m\times n}$,
and a corresponding algorithm, called SUPER, that can resolve ${\bf x}$ up to a
global phase from intensity measurements $|A{\bf x}|$ with high probability
over $A$. Here $|A{\bf x}|$ is a vector of component-wise magnitudes of $A{\bf
x}$. The SUPER algorithm is the first to simultaneously have the following
properties: (a) it requires only ${\cal O}(k)$ (order-optimal) measurements,
(b) the computational complexity of decoding is ${\cal O}(k\log k)$ (near
order-optimal) arithmetic operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4271</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4271</id><created>2014-01-17</created><authors><author><keyname>Toscani</keyname><forenames>Giuseppe</forenames></author></authors><title>R\'enyi entropies and nonlinear diffusion equations</title><categories>math-ph cs.IT math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since their introduction in the early sixties, the R\'enyi entropies have
been used in many contexts, ranging from information theory to astrophysics,
turbulence phenomena and others. In this note, we enlighten the main
connections between R\'enyi entropies and nonlinear diffusion equations. In
particular, it is shown that these relationships allow to prove various
functional inequalities in sharp form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4273</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4273</id><created>2014-01-17</created><authors><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author><author><keyname>Hansson</keyname><forenames>Anders</forenames></author></authors><title>Nuclear Norm Subspace Identification (N2SID) for short data batches</title><categories>cs.SY</categories><comments>Submitted to 2014 IFAC World Congress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace identification is revisited in the scope of nuclear norm
minimization methods. It is shown that essential structural knowledge about the
unknown data matrices in the data equation that relates Hankel matrices
constructed from input and output data can be used in the first step of the
numerical solution presented. The structural knowledge comprises the low rank
property of a matrix that is the product of the extended observability matrix
and the state sequence and the Toeplitz structure of the matrix of Markov
parameters (of the system in innovation form). The new subspace identification
method is referred to as the N2SID (twice the N of Nuclear Norm and SID for
Subspace IDentification) method. In addition to include key structural
knowledge in the solution it integrates the subspace calculation with
minimization of a classical prediction error cost function. The nuclear norm
relaxation enables us to perform such integration while preserving convexity.
The advantages of N2SID are demonstrated in a numerical open- and closed-loop
simulation study. Here a comparison is made with another widely used SID
method, i.e. N4SID. The comparison focusses on the identification with short
data batches, i.e. where the number of measurements is a small multiple of the
system order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4276</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4276</id><created>2014-01-17</created><authors><author><keyname>Wang</keyname><forenames>Xiaohui</forenames></author><author><keyname>Jia</keyname><forenames>Jia</forenames></author><author><keyname>Cai</keyname><forenames>Lianhong</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author></authors><title>Modeling Emotion Influence from Images in Social Networks</title><categories>cs.SI cs.HC cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images become an important and prevalent way to express users' activities,
opinions and emotions. In a social network, individual emotions may be
influenced by others, in particular by close friends. We focus on understanding
how users embed emotions into the images they uploaded to the social websites
and how social influence plays a role in changing users' emotions. We first
verify the existence of emotion influence in the image networks, and then
propose a probabilistic factor graph based emotion influence model to answer
the questions of &quot;who influences whom&quot;. Employing a real network from Flickr as
experimental data, we study the effectiveness of factors in the proposed model
with in-depth data analysis. Our experiments also show that our model, by
incorporating the emotion influence, can significantly improve the accuracy
(+5%) for predicting emotions from images. Finally, a case study is used as the
anecdotal evidence to further demonstrate the effectiveness of the proposed
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4282</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4282</id><created>2014-01-17</created><authors><author><keyname>Soto</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>The Secret Life of a Process Description: A Look into the Evolution of a
  Large Process Model</title><categories>cs.SE</categories><comments>12 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-79588-9_23</comments><journal-ref>Making Globally Distributed Software Development a Success Story,
  volume 5007 of Lecture Notes in Computer Science, pages 257-268, Springer
  Berlin Heidelberg, 2008</journal-ref><doi>10.1007/978-3-540-79588-9_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software process models must change continuously in order to remain
consistent over time with the reality they represent, as well as relevant to
the task they are intended for. Performing these changes in a sound and disci-
plined fashion requires software process model evolution to be understood and
controlled. The current situation can be characterized by a lack of
understanding of software process model evolution and, in consequence, by a
lack of systematic support for evolving software process models in
organizations. This paper presents an analysis of the evolution of a large
software process standard, namely, the process standard for the German Federal
Government (V-Modell(R) XT). The analysis was performed with the Evolyzer tool
suite, and is based on the complete history of over 600 versions that have been
created during the development and maintenance of the standard. The analysis
reveals similarities and differences between process evolution and empirical
findings in the area of software system evolution. These findings provide hints
on how to better manage process model evolution in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4307</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4307</id><created>2014-01-17</created><updated>2014-02-03</updated><authors><author><keyname>Belhajjame</keyname><forenames>Khalid</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Garijo</keyname><forenames>Daniel</forenames></author><author><keyname>Hettne</keyname><forenames>Kristina</forenames></author><author><keyname>Palma</keyname><forenames>Raul</forenames></author><author><keyname>Corcho</keyname><forenames>&#xd3;scar</forenames></author><author><keyname>G&#xf3;mez-P&#xe9;rez</keyname><forenames>Jos&#xe9;-Manuel</forenames></author><author><keyname>Bechhofer</keyname><forenames>Sean</forenames></author><author><keyname>Klyne</keyname><forenames>Graham</forenames></author><author><keyname>Goble</keyname><forenames>Carole</forenames></author></authors><title>The Research Object Suite of Ontologies: Sharing and Exchanging Research
  Data and Methods on the Open Web</title><categories>cs.DL</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in life sciences is increasingly being conducted in a digital and
online environment. In particular, life scientists have been pioneers in
embracing new computational tools to conduct their investigations. To support
the sharing of digital objects produced during such research investigations, we
have witnessed in the last few years the emergence of specialized repositories,
e.g., DataVerse and FigShare. Such repositories provide users with the means to
share and publish datasets that were used or generated in research
investigations. While these repositories have proven their usefulness,
interpreting and reusing evidence for most research results is a challenging
task. Additional contextual descriptions are needed to understand how those
results were generated and/or the circumstances under which they were
concluded. Because of this, scientists are calling for models that go beyond
the publication of datasets to systematically capture the life cycle of
scientific investigations and provide a single entry point to access the
information about the hypothesis investigated, the datasets used, the
experiments carried out, the results of the experiments, the people involved in
the research, etc. In this paper we present the Research Object (RO) suite of
ontologies, which provide a structured container to encapsulate research data
and methods along with essential metadata descriptions. Research Objects are
portable units that enable the sharing, preservation, interpretation and reuse
of research investigation results. The ontologies we present have been designed
in the light of requirements that we gathered from life scientists. They have
been built upon existing popular vocabularies to facilitate interoperability.
Furthermore, we have developed tools to support the creation and sharing of
Research Objects, thereby promoting and facilitating their adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4312</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4312</id><created>2014-01-17</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Li</keyname><forenames>Jing</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Shen</keyname><forenames>Yanning</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Li</keyname><forenames>Hongbin</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Super-Resolution Compressed Sensing: An Iterative Reweighted Algorithm
  for Joint Parameter Learning and Sparse Signal Recovery</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2014.2316004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical applications such as direction-of-arrival (DOA) estimation
and line spectral estimation, the sparsifying dictionary is usually
characterized by a set of unknown parameters in a continuous domain. To apply
the conventional compressed sensing to such applications, the continuous
parameter space has to be discretized to a finite set of grid points.
Discretization, however, incurs errors and leads to deteriorated recovery
performance. To address this issue, we propose an iterative reweighted method
which jointly estimates the unknown parameters and the sparse signals.
Specifically, the proposed algorithm is developed by iteratively decreasing a
surrogate function majorizing a given objective function, which results in a
gradual and interweaved iterative process to refine the unknown parameters and
the sparse signal. Numerical results show that the algorithm provides superior
performance in resolving closely-spaced frequency components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4313</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4313</id><created>2014-01-17</created><authors><author><keyname>Li</keyname><forenames>Wenjie</forenames></author><author><keyname>Bassi</keyname><forenames>Francesca</forenames></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames></author></authors><title>Robust Bayesian compressed sensing over finite fields: asymptotic
  performance analysis</title><categories>cs.IT math.IT</categories><comments>42 pages, 4 figures</comments><msc-class>68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the topic of robust Bayesian compressed sensing over
finite fields. For stationary and ergodic sources, it provides asymptotic (with
the size of the vector to estimate) necessary and sufficient conditions on the
number of required measurements to achieve vanishing reconstruction error, in
presence of sensing and communication noise. In all considered cases, the
necessary and sufficient conditions asymptotically coincide. Conditions on the
sparsity of the sensing matrix are established in presence of communication
noise. Several previously published results are generalized and extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4321</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4321</id><created>2014-01-17</created><authors><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author><author><keyname>Zahedi</keyname><forenames>Zohreh</forenames></author><author><keyname>Wouters</keyname><forenames>Paul</forenames></author></authors><title>Do altmetrics correlate with citations? Extensive comparison of
  altmetric indicators with citations from a multidisciplinary perspective</title><categories>cs.DL</categories><doi>10.1002/asi.23309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extensive analysis of the presence of different altmetric indicators
provided by Altmetric.com across scientific fields is presented, particularly
focusing on their relationship with citations. Our results confirm that the
presence and density of social media altmetric counts are still very low and
not very frequent among scientific publications, with 15%-24% of the
publications presenting some altmetric activity and concentrating in the most
recent publications, although their presence is increasing over time.
Publications from the social sciences, humanities and the medical and life
sciences show the highest presence of altmetrics, indicating their potential
value and interest for these fields. The analysis of the relationships between
altmetrics and citations confirms previous claims of positive correlations but
relatively weak, thus supporting the idea that altmetrics do not reflect the
same concept of impact as citations. Also, altmetric counts do not always
present a better filtering of highly cited publications than journal citation
scores. Altmetrics scores (particularly mentions in blogs) are able to identify
highly cited publications with higher levels of precision than journal citation
scores (JCS), but they have a lower level of recall. The value of altmetrics as
a complementary tool of citation analysis is highlighted, although more
research is suggested to disentangle the potential meaning and value of
altmetric indicators for research evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4330</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4330</id><created>2014-01-17</created><authors><author><keyname>Hetzl</keyname><forenames>Stefan</forenames></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames></author><author><keyname>Reis</keyname><forenames>Giselle</forenames></author><author><keyname>Weller</keyname><forenames>Daniel</forenames></author></authors><title>Algorithmic Introduction of Quantified Cuts</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for inverting Gentzen's cut-elimination in classical
first-order logic. Our algorithm is based on first computign a compressed
representation of the terms present in the cut-free proof and then cut-formulas
that realize such a compression. Finally, a proof using these cut-formulas is
constructed. This method allows an exponential compression of proof length. It
can be applied to the output of automated theorem provers, which typically
produce analytic proofs. An implementation is available on the web and
described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4335</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4335</id><created>2014-01-17</created><authors><author><keyname>Zhou</keyname><forenames>Tong</forenames></author></authors><title>On the Controllability and Observability of Networked Dynamic Systems</title><categories>cs.SY</categories><comments>30 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some necessary and sufficient conditions are obtained for the controllability
and observability of a networked system with linear time invariant (LTI)
dynamics. The topology of this system is fixed but arbitrary, and every
subsystem is permitted to have different dynamic input-output relations. These
conditions essentially depend only on transmission zeros of every subsystem and
the connection matrix among subsystems, which makes them attractive in the
analysis and synthesis of a large scale networked system. As an application,
these conditions are utilized to characterize systems whose steady state
estimation accuracy with the distributed predictor developed in (Zhou, 2013) is
equal to that of the lumped Kalman filter. Some necessary and sufficient
conditions on system matrices are derived for this equivalence. It has been
made clear that to guarantee this equivalence, the steady state update gain
matrix of the Kalman filter must be block diagonal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4337</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4337</id><created>2014-01-17</created><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Karimian</keyname><forenames>Yasser</forenames></author><author><keyname>Huber</keyname><forenames>Johannes</forenames></author><author><keyname>Ahmadian</keyname><forenames>Mahmoud</forenames></author></authors><title>On the Design of Fast Convergent LDPC Codes: An Optimization Approach</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity-performance trade-off is a fundamental aspect of the design of
low-density parity-check (LDPC) codes. In this paper, we consider LDPC codes
for the binary erasure channel (BEC), use code rate for performance metric, and
number of decoding iterations to achieve a certain residual erasure probability
for complexity metric. We first propose a quite accurate approximation of the
number of iterations for the BEC. Moreover, a simple but efficient utility
function corresponding to the number of iterations is developed. Using the
aforementioned approximation and the utility function, two optimization
problems w.r.t. complexity are formulated to find the code degree
distributions. We show that both optimization problems are convex. In
particular, the problem with the proposed approximation belongs to the class of
semi-infinite problems which are computationally challenging to be solved.
However, the problem with the proposed utility function falls into the class of
semi-definite programming (SDP) and thus, the global solution can be found
efficiently using available SDP solvers. Numerical results reveal the
superiority of the proposed code design compared to existing code designs from
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4339</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4339</id><created>2014-01-17</created><updated>2014-01-21</updated><authors><author><keyname>Bichhawat</keyname><forenames>Abhishek</forenames></author><author><keyname>Rajani</keyname><forenames>Vineet</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Hammer</keyname><forenames>Christian</forenames></author></authors><title>Information Flow Control in WebKit's JavaScript Bytecode</title><categories>cs.CR cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Websites today routinely combine JavaScript from multiple sources, both
trusted and untrusted. Hence, JavaScript security is of paramount importance. A
specific interesting problem is information flow control (IFC) for JavaScript.
In this paper, we develop, formalize and implement a dynamic IFC mechanism for
the JavaScript engine of a production Web browser (specifically, Safari's
WebKit engine). Our IFC mechanism works at the level of JavaScript bytecode and
hence leverages years of industrial effort on optimizing both the source to
bytecode compiler and the bytecode interpreter. We track both explicit and
implicit flows and observe only moderate overhead. Working with bytecode
results in new challenges including the extensive use of unstructured control
flow in bytecode (which complicates lowering of program context taints),
unstructured exceptions (which complicate the matter further) and the need to
make IFC analysis permissive. We explain how we address these challenges,
formally model the JavaScript bytecode semantics and our instrumentation, prove
the standard property of termination-insensitive non-interference, and present
experimental results on an optimized prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4374</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4374</id><created>2014-01-17</created><updated>2014-01-22</updated><authors><author><keyname>Carreiro</keyname><forenames>Facundo</forenames></author><author><keyname>Facchini</keyname><forenames>Alessandro</forenames></author><author><keyname>Venema</keyname><forenames>Yde</forenames></author><author><keyname>Zanasi</keyname><forenames>Fabio</forenames></author></authors><title>Weak MSO: Automata and Expressiveness Modulo Bisimilarity</title><categories>cs.LO</categories><comments>Technical Report, 57 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the bisimulation-invariant fragment of weak monadic
second-order logic (WMSO) is equivalent to the fragment of the modal
$\mu$-calculus where the application of the least fixpoint operator $\mu
p.\varphi$ is restricted to formulas $\varphi$ that are continuous in $p$. Our
proof is automata-theoretic in nature; in particular, we introduce a class of
automata characterizing the expressive power of WMSO over tree models of
arbitrary branching degree. The transition map of these automata is defined in
terms of a logic $\mathrm{FOE}_1^\infty$ that is the extension of first-order
logic with a generalized quantifier $\exists^\infty$, where $\exists^\infty x.
\phi$ means that there are infinitely many objects satisfying $\phi$. An
important part of our work consists of a model-theoretic analysis of
$\mathrm{FOE}_1^\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4381</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4381</id><created>2014-01-17</created><authors><author><keyname>Khalil</keyname><forenames>Khaled M.</forenames></author><author><keyname>Abdel-Aziz</keyname><forenames>M.</forenames></author><author><keyname>Nazmy</keyname><forenames>Taymour T.</forenames></author><author><keyname>Salem</keyname><forenames>Abdel-Badeeh M.</forenames></author></authors><title>Intelligent Techniques for Resolving Conflicts of Knowledge in
  Multi-Agent Decision Support Systems</title><categories>cs.MA</categories><comments>5 pages, 1 table, Sixth International Conference on Intelligence
  Computing and Information Systems, Cairo, Egypt, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on some of the key intelligent techniques for conflict
resolution in Multi-Agent Decision Support Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4383</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4383</id><created>2014-01-17</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Rambau</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>On the Hegselmann-Krause conjecture in opinion dynamics</title><categories>math.DS cs.SI</categories><comments>22 pages, 8 figures</comments><msc-class>39A11, 91D10, 37N99</msc-class><journal-ref>Journal of Difference Equations and Applications, Vol. 17, Nr. 6
  (2011), Pages 859-876</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an elementary proof of a conjecture by Hegselmann and Krause in
opinion dynamics, concerning a symmetric bounded confidence interval model: If
there is a truth and all individuals take each other seriously by a positive
amount bounded away from zero, then all truth seekers will converge to the
truth. Here truth seekers are the individuals which are attracted by the truth
by a positive amount. In the absence of truth seekers it was already shown by
Hegselmann and Krause that the opinions of the individuals converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4387</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4387</id><created>2014-01-17</created><updated>2014-05-06</updated><authors><author><keyname>Bonacina</keyname><forenames>Fausto</forenames></author><author><keyname>D'Errico</keyname><forenames>Marco</forenames></author><author><keyname>Moretto</keyname><forenames>Enrico</forenames></author><author><keyname>Stefani</keyname><forenames>Silvana</forenames></author><author><keyname>Torriero</keyname><forenames>Anna</forenames></author></authors><title>A Multiple Network Approach to Corporate Governance</title><categories>q-fin.GN cs.SI physics.soc-ph</categories><comments>version 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider Corporate Governance (CG) ties among companies from
a multiple network perspective. Such a structure naturally arises from the
close interrelation between the Shareholding Network (SH) and the Board of
Directors network (BD). In order to capture the simultaneous effects of both
networks on CG, we propose to model the CG multiple network structure via
tensor analysis. In particular, we consider the TOPHITS model, based on the
PARAFAC tensor decomposition, to show that tensor techniques can be
successfully applied in this context. By providing some empirical results from
the Italian financial market in the univariate case, we then show that a
tensor--based multiple network approach can reveal important information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4396</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4396</id><created>2014-01-17</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards plant wires</title><categories>cs.ET physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In experimental laboratory studies we evaluate a possibility of making
electrical wires from living plants. In scoping experiments we use lettuce
seedlings as a prototype model of a plant wire. We approximate an electrical
potential transfer function by applying direct current voltage to the lettuce
seedlings and recording output voltage. We analyse oscillation frequencies of
the output potential and assess noise immunity of the plant wires. Our findings
will be used in future designs of self-growing wetware circuits and devices,
and integration of plant-based electronic components into future and emergent
bio-hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4420</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4420</id><created>2014-01-17</created><updated>2014-01-22</updated><authors><author><keyname>Kikot</keyname><forenames>Stanislav</forenames></author><author><keyname>Kontchakov</keyname><forenames>Roman</forenames></author><author><keyname>Podolskii</keyname><forenames>Vladimir</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>Michael</forenames></author></authors><title>On the Succinctness of Query Rewriting over OWL 2 QL Ontologies with
  Shallow Chases</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the size of first-order rewritings of conjunctive queries over
OWL 2 QL ontologies of depth 1 and 2 by means of hypergraph programs computing
Boolean functions. Both positive and negative results are obtained. Conjunctive
queries over ontologies of depth 1 have polynomial-size nonrecursive datalog
rewritings; tree-shaped queries have polynomial positive existential
rewritings; however, in the worst case, positive existential rewritings can
only be of superpolynomial size. Positive existential and nonrecursive datalog
rewritings of queries over ontologies of depth 2 suffer an exponential blowup
in the worst case, while first-order rewritings are superpolynomial unless
$\text{NP} \subseteq \text{P}/\text{poly}$. We also analyse rewritings of
tree-shaped queries over arbitrary ontologies and observe that the query
entailment problem for such queries is fixed-parameter tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4428</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4428</id><created>2014-01-17</created><authors><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Hoskins</keyname><forenames>Jeremy G.</forenames></author><author><keyname>Schotland</keyname><forenames>John C.</forenames></author></authors><title>Diffuse Scattering on Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and analyze difference equations on graphs analogous to
time-independent diffusion equations arising in the study of diffuse scattering
in continuous media. Moreover, we show how to construct solutions in the
presence of weak scatterers from the solution to the homogeneous (background
problem) using Born series, providing necessary conditions for convergence and
demonstrating the process through numerous examples. In addition, we outline a
method for finding Green's functions for Cayley graphs for both abelian and
non-abelian groups. Finally, we conclude with a discussion of the effects of
sparsity on our method and results, outlining the simplifications that can be
made provided that the scatterers are weak and well-separated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4436</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4436</id><created>2014-01-15</created><authors><author><keyname>Abedin</keyname><forenames>Muhammad Arshad Ul</forenames></author><author><keyname>Ng</keyname><forenames>Vincent</forenames></author><author><keyname>Khan</keyname><forenames>Latifur</forenames></author></authors><title>Cause Identification from Aviation Safety Incident Reports via Weakly
  Supervised Semantic Lexicon Construction</title><categories>cs.CL cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  569-631, 2010</journal-ref><doi>10.1613/jair.2986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Aviation Safety Reporting System collects voluntarily submitted reports
on aviation safety incidents to facilitate research work aiming to reduce such
incidents. To effectively reduce these incidents, it is vital to accurately
identify why these incidents occurred. More precisely, given a set of possible
causes, or shaping factors, this task of cause identification involves
identifying all and only those shaping factors that are responsible for the
incidents described in a report. We investigate two approaches to cause
identification. Both approaches exploit information provided by a semantic
lexicon, which is automatically constructed via Thelen and Riloffs Basilisk
framework augmented with our linguistic and algorithmic modifications. The
first approach labels a report using a simple heuristic, which looks for the
words and phrases acquired during the semantic lexicon learning process in the
report. The second approach recasts cause identification as a text
classification problem, employing supervised and transductive text
classification algorithms to learn models from incident reports labeled with
shaping factors and using the models to label unseen reports. Our experiments
show that both the heuristic-based approach and the learning-based approach
(when given sufficient training data) outperform the baseline system
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4441</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4441</id><created>2014-01-17</created><authors><author><keyname>Niethammer</keyname><forenames>Christoph</forenames></author><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author><author><keyname>Gracia</keyname><forenames>Jose</forenames></author></authors><title>Avoiding Serialization Effects in Data-Dependency aware Task Parallel
  Algorithms for Spatial Decomposition</title><categories>cs.DC</categories><comments>6 pages, 9 figures, published at ISPA2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial decomposition is a popular basis for parallelising code. Cast in the
frame of task parallelism, calculations on a spatial domain can be treated as a
task. If neighbouring domains interact and share results, access to the
specific data needs to be synchronized to avoid race conditions. This is the
case for a variety of applications, like most molecular dynamics and many
computational fluid dynamics codes. Here we present an unexpected problem which
can occur in dependency-driven task parallelization models like StarSs: the
tasks accessing a specific spatial domain are treated as interdependent, as
dependencies are detected automatically via memory addresses. Thus, the order
in which tasks are generated will have a severe impact on the dependency tree.
In the worst case, a complete serialization is reached and no two tasks can be
calculated in parallel. We present the problem in detail based on an example
from molecular dynamics, and introduce a theoretical framework to calculate the
degree of serialization. Furthermore, we present strategies to avoid this
unnecessary problem. We recommend treating these strategies as best practice
when using dependency-driven task parallel programming models like StarSs on
such scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4443</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4443</id><created>2014-01-17</created><authors><author><keyname>Fatema</keyname><forenames>Nusrat</forenames></author><author><keyname>Brad</keyname><forenames>Remus</forenames></author></authors><title>Attacks And Counterattacks On Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>Nusrat Fatema and Remus Brad, Attacks and Counterattacks on
  Wireless Sensor Networks, International Journal of Ad hoc, Sensor and
  Ubiquitous Computing, vol. 4(6), pp. 1-15, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WSN is formed by autonomous nodes with partial memory, communication range,
power, and bandwidth. Their occupation depends on inspecting corporal and
environmental conditions and communing through a system and performing data
processing. The application field is vast, comprising military, ecology,
healthcare, home or commercial and require a highly secured communication. The
paper analyses different types of attacks and counterattacks and provides
solutions for the WSN threats
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4446</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4446</id><created>2014-01-17</created><authors><author><keyname>Basca</keyname><forenames>Cosmin</forenames></author><author><keyname>Talos</keyname><forenames>Mihai</forenames></author><author><keyname>Brad</keyname><forenames>Remus</forenames></author></authors><title>Humanoid Robot With Vision Recognition Control System</title><categories>cs.RO</categories><journal-ref>Proceedings of The International Symposium on System Theory,
  Automation, Robotics, Computers, Informatics, Electronics and
  Instrumentation, pp. 243-248, 20-22 October 2005, Craiova, Romania, ISBN
  973-742-148-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution to controlling humanoid robotic systems. The
robot can be programmed to execute certain complex actions based on basic
motion primitives. The humanoid robot is programmed using a PC. The software
running on the PC can obtain at any given moment information about the state of
the robot, or it can program the robot to execute a different action, providing
the possibility of implementing a complex behavior. We want to provide the
robotic system the ability to understand more on the external real world. In
this paper we describe a method for detecting ellipses in real world images
using the Randomized Hough Transform with Result Clustering. Real world images
are preprocessed, noise reduction, greyscale transform, edge detection and
finaly binarization in order to be processed by the actual ellipse detector.
After all the ellipses are detected a post processing phase clusters the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4447</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4447</id><created>2013-11-20</created><authors><author><keyname>Kadir</keyname><forenames>Abdul</forenames></author><author><keyname>Nugroho</keyname><forenames>Lukito Edi</forenames></author><author><keyname>Susanto</keyname><forenames>Adhi</forenames></author><author><keyname>Santosa</keyname><forenames>Paulus Insap</forenames></author></authors><title>Leaf Classification Using Shape, Color, and Texture Features</title><categories>cs.CV cs.CY</categories><comments>6 pages, International Journal of Computer Trends and Technology-
  July to Aug Issue 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several methods to identify plants have been proposed by several researchers.
Commonly, the methods did not capture color information, because color was not
recognized as an important aspect to the identification. In this research,
shape and vein, color, and texture features were incorporated to classify a
leaf. In this case, a neural network called Probabilistic Neural network (PNN)
was used as a classifier. The experimental result shows that the method for
classification gives average accuracy of 93.75% when it was tested on Flavia
dataset, that contains 32 kinds of plant leaves. It means that the method gives
better performance compared to the original work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4448</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4448</id><created>2013-10-21</created><authors><author><keyname>Bradai</keyname><forenames>Abbas</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Abbasi</keyname><forenames>Ubaid</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Landa</keyname><forenames>Raul</forenames><affiliation>UCL-CS</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author></authors><title>An efficient playout smoothing mechanism for layered streaming in P2P
  networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Peer-to-Peer Networking and Applications, Springer (2012) ISSN
  1936-6442</journal-ref><doi>10.1007/s12083-012-0170-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Layered video streaming in peer-to-peer (P2P) networks has drawn great
interest, since it can not only accommodate large numbers of users, but also
handle peer heterogeneity. However, there's still a lack of comprehensive
studies on chunk scheduling for the smooth playout of layered streams in P2P
networks. In these situations, a playout smoothing mechanism can be used to
ensure the uniform delivery of the layered stream. This can be achieved by
reducing the quality changes that the stream undergoes when adapting to
changing network conditions. This paper complements previous efforts in
throughput maximization and delay minimization for P2P streaming by considering
the consequences of playout smoothing on the scheduling mechanisms for stream
layer acquisition. The two main problems to be considered when designing a
playout smoothing mechanism for P2P streaming are the fluctuation in available
bandwidth between peers and the unreliability of user-contributed
resources--particularly peer churn. Since the consideration of these two
factors in the selection and scheduling of stream layers is crucial to maintain
smooth stream playout, the main objective of our smoothing mechanism becomes
the determination of how many layers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4451</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4451</id><created>2014-01-17</created><updated>2014-01-28</updated><authors><author><keyname>Kadhe</keyname><forenames>Swanand</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Reliable, Deniable, and Hidable Communication over Multipath Networks</title><categories>cs.IT math.IT</categories><comments>26 pages, 4 figures; Extended version of the paper submitted for ISIT
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the scenario wherein Alice wants to (potentially) communicate to
the intended receiver Bob over a network consisting of multiple parallel links
in the presence of a passive eavesdropper Willie, who observes an unknown
subset of links. A primary goal of our communication protocol is to make the
communication &quot;deniable&quot;, {\it i.e.}, Willie should not be able to {\it
reliably} estimate whether or not Alice is transmitting any {\it covert}
information to Bob. Moreover, if Alice is indeed actively communicating, her
covert messages should be information-theoretically &quot;hidable&quot; in the sense that
Willie's observations should not {\it leak any information} about Alice's
(potential) message to Bob -- our notion of hidability is slightly stronger
than the notion of information-theoretic strong secrecy well-studied in the
literature, and may be of independent interest. It can be shown that
deniability does not imply either hidability or (weak or strong)
information-theoretic secrecy; nor does any form of information-theoretic
secrecy imply deniability. We present matching inner and outer bounds on the
capacity for deniable and hidable communication over {\it multipath networks}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4472</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4472</id><created>2014-01-17</created><authors><author><keyname>Kanawati</keyname><forenames>Rushed</forenames></author></authors><title>YASCA: A collective intelligence approach for community detection in
  complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>4pags</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an original approach for community detection in
complex networks. The approach belongs to the family of seed-centric
algorithms. However, instead of expanding communities around selected seeds as
most of existing approaches do, we explore here applying an ensemble clustering
approach to different network partitions derived from ego-centered communities
computed for each selected seed. Ego-centered communities are themselves
computed applying a recently proposed ensemble ranking based approach that
allow to efficiently combine various local modularities used to guide a greedy
optimisation process. Results of first experiments on real world networks for
which a ground truth decomposition into communities are known, argue for the
validity of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4473</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4473</id><created>2013-11-29</created><authors><author><keyname>Ko&#xe7;</keyname><forenames>Yakup</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author><author><keyname>Kooij</keyname><forenames>Robert E.</forenames></author><author><keyname>Brazier</keyname><forenames>Frances M. T.</forenames></author></authors><title>The Impact of the Topology on Cascading Failures in Electric Power Grids</title><categories>physics.soc-ph cs.SY</categories><doi>10.1016/j.physa.2014.01.056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading failures are one of the main reasons for blackouts in power
transmission grids. The topology of a power grid, together with its operative
state determine, for the most part, the robustness of the power grid against
cascading failures. Secure electrical power supply requires, together with
careful operation, a robust design of the electrical power grid topology. This
paper investigates the impact of a power grid topology on its robustness
against cascading failures. Currently, the impact of the topology on a grid
robustness is mainly assessed by using purely topological approaches that fail
to capture the essence of electric power flow. This paper proposes a metric,
the effective graph resistance, that relates the topology of a power grid to
its robustness against cascading failures by deliberate attacks, while also
taking the fundamental characteristics of the electric power grid into account
such as power flow allocation according to Kirchoff Laws. Experimental
verification shows that the proposed metric anticipates the grid robustness
accurately. The proposed metric is used to optimize a grid topology for a
higher level of robustness. To demonstrate its applicability, the metric is
applied on the IEEE 118 bus power system to improve its robustness against
cascading failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4477</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4477</id><created>2014-01-17</created><authors><author><keyname>Crouseilles</keyname><forenames>Nicolas</forenames></author><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author><author><keyname>Faou</keyname><forenames>Erwan</forenames></author></authors><title>A Hamiltonian splitting for the Vlasov-Maxwell system</title><categories>math.NA cs.NA physics.comp-ph</categories><doi>10.1016/j.jcp.2014.11.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new splitting is proposed for solving the Vlasov-Maxwell system. This
splitting is based on a decomposition of the Hamiltonian of the Vlasov-Maxwell
system and allows for the construction of arbitrary high order methods by
composition (independent of the specific deterministic method used for the
discretization of the phase space). Moreover, we show that for a spectral
method in space this scheme satisfies Poisson's equation without explicitly
solving it. Finally, we present some examples in the context of the time
evolution of an electromagnetic plasma instability which emphasizes the
excellent behavior of the new splitting compared to methods from the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4484</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4484</id><created>2014-01-17</created><authors><author><keyname>Buzaglo</keyname><forenames>Sarit</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Constrained Codes for Rank Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the rank modulation scheme, a recent work by Sala and Dolecek
explored the study of constraint codes for permutations. The constraint studied
by them is inherited by the inter-cell interference phenomenon in flash
memories, where high-level cells can inadvertently increase the level of
low-level cells.
  In this paper, the model studied by Sala and Dolecek is extended into two
constraints. A permutation $\sigma \in S_n$ satisfies the \emph{two-neighbor
$k$-constraint} if for all $2 \leq i \leq n-1$ either
$|\sigma(i-1)-\sigma(i)|\leq k$ or $|\sigma(i)-\sigma(i+1)|\leq k$, and it
satisfies the \emph{asymmetric two-neighbor $k$-constraint} if for all $2 \leq
i \leq n-1$, either $\sigma(i-1)-\sigma(i) &lt; k$ or $\sigma(i+1)-\sigma(i) &lt; k$.
We show that the capacity of the first constraint is $(1+\epsilon)/2$ in case
that $k=\Theta(n^{\epsilon})$ and the capacity of the second constraint is 1
regardless to the value of $k$. We also extend our results and study the
capacity of these two constraints combined with error-correction codes in the
Kendall's $\tau$ metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4489</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4489</id><created>2014-01-17</created><updated>2014-11-13</updated><authors><author><keyname>Arpit</keyname><forenames>Devansh</forenames></author><author><keyname>Nwogu</keyname><forenames>Ifeoma</forenames></author><author><keyname>Srivastava</keyname><forenames>Gaurav</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>An Analysis of Random Projections in Cancelable Biometrics</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing concerns about security, the need for highly secure physical
biometrics-based authentication systems utilizing \emph{cancelable biometric}
technologies is on the rise. Because the problem of cancelable template
generation deals with the trade-off between template security and matching
performance, many state-of-the-art algorithms successful in generating high
quality cancelable biometrics all have random projection as one of their early
processing steps. This paper therefore presents a formal analysis of why random
projections is an essential step in cancelable biometrics. By formally defining
the notion of an \textit{Independent Subspace Structure} for datasets, it can
be shown that random projection preserves the subspace structure of data
vectors generated from a union of independent linear subspaces. The bound on
the minimum number of random vectors required for this to hold is also derived
and is shown to depend logarithmically on the number of data samples, not only
in independent subspaces but in disjoint subspace settings as well. The
theoretical analysis presented is supported in detail with empirical results on
real-world face recognition datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4492</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4492</id><created>2014-01-17</created><updated>2014-01-20</updated><authors><author><keyname>Clarkson</keyname><forenames>Michael R.</forenames></author><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames></author><author><keyname>Koleini</keyname><forenames>Masoud</forenames></author><author><keyname>Micinski</keyname><forenames>Kristopher K.</forenames></author><author><keyname>Rabe</keyname><forenames>Markus N.</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>C&#xe9;sar</forenames></author></authors><title>Temporal Logics for Hyperproperties</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two new logics for verification of hyperproperties are proposed.
Hyperproperties characterize security policies, such as noninterference, as a
property of sets of computation paths. Standard temporal logics such as LTL,
CTL, and CTL* can refer only to a single path at a time, hence cannot express
many hyperproperties of interest. The logics proposed here, HyperLTL and
HyperCTL*, add explicit and simultaneous quantification over multiple paths to
LTL and to CTL*. This kind of quantification enables expression of
hyperproperties. A model checking algorithm for the proposed logics is given.
For a fragment of HyperLTL, a prototype model checker has been implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4499</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4499</id><created>2014-01-17</created><authors><author><keyname>Echenique</keyname><forenames>Federico</forenames></author></authors><title>Testing for separability is hard</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that it is computationally hard to decide (or test) if a
consumption data set is consistent with separable preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4506</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4506</id><created>2014-01-17</created><updated>2014-01-23</updated><authors><author><keyname>Yu</keyname><forenames>Jiyang</forenames></author><author><keyname>Carosino</keyname><forenames>Michael</forenames></author><author><keyname>Sivakumar</keyname><forenames>Krishnamoorthy</forenames></author><author><keyname>Belzer</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Chen</keyname><forenames>Yiming</forenames></author></authors><title>Detection and Decoding for 2D Magnetic Recording Channels with 2D
  Intersymbol Interference</title><categories>cs.IT math.IT</categories><comments>8 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers iterative detection and decoding on the concatenated
communication channel consisting of a two-dimensional magnetic recording (TDMR)
channel modeled by the four-grain rectangular discrete grain model (DGM)
proposed by Kavcic et. al., followed by a two-dimensional intersymbol
interference (2D-ISI) channel modeled by linear convolution of the DGM model's
output with a finite-extent 2D blurring mask followed by addition of white
Gaussian noise. An iterative detection and decoding scheme combines TDMR
detection, 2D-ISI detection, and soft-in/soft-out (SISO) channel decoding in a
structure with two iteration loops. In the first loop, the 2D-ISI channel
detector exchanges log-likelihood ratios (LLRs) with the TDMR detector. In the
second loop, the TDMR detector exchanges LLRs with a serially concatenated
convolutional code (SCCC) decoder. Simulation results for the concatenated TDMR
and 2 x 2 averaging mask ISI channel with 10 dB SNR show that densities of 0.48
user bits per grain and above can be achieved, corresponding to an areal
density of about 9.6 Terabits per square inch, over the entire range of grain
probabilities in the TDMR model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4509</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4509</id><created>2014-01-17</created><updated>2014-09-05</updated><authors><author><keyname>Ahmad</keyname><forenames>Imad</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames></author></authors><title>When and By How Much Can Helper Node Selection Improve Regenerating
  Codes?</title><categories>cs.IT math.IT</categories><comments>35 pages, 10 figures, submitted to IEEE Transactions on Information
  Theory on September 04, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes (RCs) can significantly reduce the repair-bandwidth of
distributed storage networks. Initially, the analysis of RCs was based on the
assumption that during the repair process, the newcomer does not distinguish
(among all surviving nodes) which nodes to access, i.e., the newcomer is
oblivious to the set of helpers being used. Such a scheme is termed the blind
repair (BR) scheme. Nonetheless, it is intuitive in practice that the newcomer
should choose to access only those &quot;good&quot; helpers. In this paper, a new
characterization of the effect of choosing the helper nodes in terms of the
storage-bandwidth tradeoff is given. Specifically, answers to the following
fundamental questions are given: Under what conditions does proactively
choosing the helper nodes improve the storage-bandwidth tradeoff? Can this
improvement be analytically quantified?
  This paper answers the former question by providing a necessary and
sufficient condition under which optimally choosing good helpers strictly
improves the storage-bandwidth tradeoff. To answer the latter question, a
low-complexity helper selection solution, termed the family repair (FR) scheme,
is proposed and the corresponding storage/repair-bandwidth curve is
characterized. For example, consider a distributed storage network with 60
total number of nodes and the network is resilient against 50 node failures. If
the number of helper nodes is 10, then the FR scheme and its variant
demonstrate 27% reduction in the repair-bandwidth when compared to the BR
solution. This paper also proves that under some design parameters, the FR
scheme is indeed optimal among all helper selection schemes. An explicit
construction of an exact-repair code is also proposed that can achieve the
minimum-bandwidth-regenerating point of the FR scheme. The new exact-repair
code can be viewed as a generalization of the existing fractional repetition
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4512</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4512</id><created>2014-01-17</created><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>A quadratically tight partition bound for classical communication
  complexity and query complexity</title><categories>cs.CC</categories><comments>8 pages, version 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce, both for classical communication complexity and
query complexity, a modification of the 'partition bound' introduced by Jain
and Klauck [2010]. We call it the 'public-coin partition bound'. We show that
(the logarithm to the base two of) its communication complexity and query
complexity versions form, for all relations, a quadratically tight lower bound
on the public-coin randomized communication complexity and randomized query
complexity respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4516</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4516</id><created>2014-01-17</created><authors><author><keyname>Wang</keyname><forenames>Yanqing</forenames></author><author><keyname>Jiang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolei</forenames></author><author><keyname>Zhang</keyname><forenames>Siyu</forenames></author><author><keyname>Liang</keyname><forenames>Yaowen</forenames></author></authors><title>Solving reviewer assignment problem in software peer review: An approach
  based on preference matrix and asymmetric TSP model</title><categories>cs.DL math.OC</categories><comments>11 pages, 1 figure, 2 tables</comments><msc-class>90B80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimized reviewer assignment can effectively utilize limited intellectual
resources and significantly assure review quality in various scenarios such as
paper selection in conference or journal, proposal selection in funding
agencies and so on. However, little research on reviewer assignment of software
peer review has been found. In this study, an optimization approach is proposed
based on students' preference matrix and the model of asymmetric traveling
salesman problem (ATSP). Due to the most critical role of rule matrix in this
approach, we conduct a questionnaire to obtain students' preference matrices
and convert them to rule matrices. With the help of software ILOG CPLEX, the
approach is accomplished by controlling the exit criterion of ATSP model. The
comparative study shows that the assignment strategies with both reviewers'
preference matrix and authors' preference matrix get better performance than
the random assignment. Especially, it is found that the performance is just a
little better than that of random assignment when the reviewers' and authors'
preference matrices are merged. In other words, the majority of students have a
strong wish of harmonious development even though high-level students are not
willing to do that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4528</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4528</id><created>2014-01-18</created><authors><author><keyname>Li</keyname><forenames>Di</forenames></author><author><keyname>Mitseva</keyname><forenames>Asya</forenames></author></authors><title>Mobile Adhoc Offloading</title><categories>cs.NI</categories><comments>Published in: E. Baccelli, F. Juraschek, O. Hahm, T. C. Schmidt, H.
  Will, M. W\&quot;ahlisch (Eds.), Proc. of 3rd MANIAC Challenge, Berlin, Germany,
  July 27 - 28, 2013, arXiv:1401.1163, Jan. 2014</comments><report-no>MANIAC/2013/02</report-no><acm-class>C.2.1; C.2.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This problem is a series of biddings and auctions. Each round of bidding and
auction are different from previous ones because of the change of network
topology, variance of budget set by the sender, and possible evolution of
strategies of other nodes. The huge strategy space of relay nodes makes the
formulation to a game very difficult. We present a brief qualitative analysis
in this paper, and propose a bidding strategy based on learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4529</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4529</id><created>2014-01-18</created><updated>2015-05-19</updated><authors><author><keyname>Hidasi</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Tikk</keyname><forenames>Domonkos</forenames></author></authors><title>General factorization framework for context-aware recommendations</title><categories>cs.IR cs.LG</categories><comments>The final publication is available at Springer via
  http://dx.doi.org/10.1007/s10618-015-0417-y. Data Mining and Knowledge
  Discovery, 2015</comments><doi>10.1007/s10618-015-0417-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-aware recommendation algorithms focus on refining recommendations by
considering additional information, available to the system. This topic has
gained a lot of attention recently. Among others, several factorization methods
were proposed to solve the problem, although most of them assume explicit
feedback which strongly limits their real-world applicability. While these
algorithms apply various loss functions and optimization strategies, the
preference modeling under context is less explored due to the lack of tools
allowing for easy experimentation with various models. As context dimensions
are introduced beyond users and items, the space of possible preference models
and the importance of proper modeling largely increases.
  In this paper we propose a General Factorization Framework (GFF), a single
flexible algorithm that takes the preference model as an input and computes
latent feature matrices for the input dimensions. GFF allows us to easily
experiment with various linear models on any context-aware recommendation task,
be it explicit or implicit feedback based. The scaling properties makes it
usable under real life circumstances as well.
  We demonstrate the framework's potential by exploring various preference
models on a 4-dimensional context-aware problem with contexts that are
available for almost any real life datasets. We show in our experiments --
performed on five real life, implicit feedback datasets -- that proper
preference modelling significantly increases recommendation accuracy, and
previously unused models outperform the traditional ones. Novel models in GFF
also outperform state-of-the-art factorization algorithms.
  We also extend the method to be fully compliant to the Multidimensional
Dataspace Model, one of the most extensive data models of context-enriched
data. Extended GFF allows the seamless incorporation of information into the
fac[truncated]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4532</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4532</id><created>2014-01-18</created><updated>2014-01-24</updated><authors><author><keyname>Yan</keyname><forenames>Yanfei</forenames></author><author><keyname>Liu</keyname><forenames>Ling</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Polar Lattices for Strong Secrecy Over the Mod-$\Lambda$ Gaussian
  Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, extended version of a paper submitted to ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar lattices, which are constructed from polar codes, are provably good for
the additive white Gaussian noise (AWGN) channel. In this work, we propose a
new polar lattice construction that achieves the secrecy capacity under the
strong secrecy criterion over the mod-$\Lambda$ Gaussian wiretap channel. This
construction leads to an AWGN-good lattice and a secrecy-good lattice
simultaneously. The design methodology is mainly based on the equivalence in
terms of polarization between the $\Lambda/\Lambda'$ channel in lattice coding
and the equivalent channel derived from the chain rule of mutual information in
multilevel coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4533</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4533</id><created>2014-01-18</created><updated>2014-02-28</updated><authors><author><keyname>Mainka</keyname><forenames>Agnes</forenames></author><author><keyname>Hartmann</keyname><forenames>Sarah</forenames></author><author><keyname>Stock</keyname><forenames>Wolfgang G.</forenames></author><author><keyname>Peters</keyname><forenames>Isabella</forenames></author></authors><title>Government and Social Media: A Case Study of 31 Informational World
  Cities</title><categories>cs.CY cs.DL cs.SI physics.soc-ph</categories><comments>In Proceedings of the 47th Hawaii International Conference on System
  Sciences (pp. 1715-1724). IEEE Computer Society, 2014</comments><acm-class>J.1</acm-class><doi>10.1109/HICSS.2014.219</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Social media platforms are increasingly being used by governments to foster
user interaction. Particularly in cities with enhanced ICT infrastructures
(i.e., Informational World Cities) and high internet penetration rates, social
media platforms are valuable tools for reaching high numbers of citizens. This
empirical investigation of 31 Informational World Cities will provide an
overview of social media services used for governmental purposes, of their
popularity among governments, and of their usage intensity in broadcasting
information online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4538</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4538</id><created>2014-01-18</created><updated>2014-09-25</updated><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author></authors><title>Dialectica models of additive-free linear logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a construction which transforms categorical models of
additive-free propositional linear logic, closely based on de Paiva's
dialectica categories and Oliva's functional interpretations of classical
linear logic. The construction is defined using dependent type theory, which
proves to be a useful tool for reasoning about dialectica categories.
Abstractly, we have a closure operator on the class of models: it preserves
soundness and completeness and has a monad-like structure. When applied to
categories of games we obtain `games with bidding', which are hybrids of
dialectica and game models, and we prove completeness theorems for two specific
such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4539</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4539</id><created>2014-01-18</created><updated>2014-05-21</updated><authors><author><keyname>Ferdous</keyname><forenames>S. M.</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>Solving the Minimum Common String Partition Problem with the Help of
  Ants</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of finding a minimum common partition
of two strings. The problem has its application in genome comparison. As it is
an NP-hard, discrete combinatorial optimization problem, we employ a
metaheuristic technique, namely, MAX-MIN ant system to solve this problem. To
achieve better efficiency we first map the problem instance into a special kind
of graph. Subsequently, we employ a MAX-MIN ant system to achieve high quality
solutions for the problem. Experimental results show the superiority of our
algorithm in comparison with the state of art algorithm in the literature. The
improvement achieved is also justified by standard statistical test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4543</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4543</id><created>2014-01-18</created><authors><author><keyname>Akin</keyname><forenames>Meriem Ben-Salah</forenames></author></authors><title>On the Potential of Twitter for Understanding the Tunisia of the
  Post-Arab Spring</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>7 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Micro-blogging through Twitter has made information short and to the point,
and more importantly systematically searchable. This work is the first of a
series in which quotidian observations about Tunisia are obtained using the
micro-blogging site Twitter. Data was extracted using the open source Twitter
API v1.1. Specific tweets were obtained using functional search operators in
particular thematic hash tags, geo-location, date, time and language. The
presence of Tunisia in the international tweet stream, the language of
communication of Tunisian residents through Twitter as well as Twitter usage
across Tunisia are the center of attention of this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4566</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4566</id><created>2014-01-18</created><updated>2014-02-08</updated><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Excess Risk Bounds for Exponentially Concave Losses</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overarching goal of this paper is to derive excess risk bounds for
learning from exp-concave loss functions in passive and sequential learning
settings. Exp-concave loss functions encompass several fundamental problems in
machine learning such as squared loss in linear regression, logistic loss in
classification, and negative logarithm loss in portfolio management. In batch
setting, we obtain sharp bounds on the performance of empirical risk
minimization performed in a linear hypothesis space and with respect to the
exp-concave loss functions. We also extend the results to the online setting
where the learner receives the training examples in a sequential manner. We
propose an online learning algorithm that is a properly modified version of
online Newton method to obtain sharp risk bounds. Under an additional mild
assumption on the loss function, we show that in both settings we are able to
achieve an excess risk bound of $O(d\log n/n)$ that holds with a high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4567</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4567</id><created>2014-01-18</created><updated>2014-04-26</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Kaltofen</keyname><forenames>Erich</forenames></author></authors><title>Essentially optimal interactive certificates in linear algebra</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a---possibly randomized---verification
algorithm that proves the correctness of each output. The certificates are
essentially optimal if the time (and space) complexity of verification is
essentially linear in the input size $N$, meaning $N$ times a factor
$N^{o(1)}$, i.e., a factor $N^{\eta(N)}$ with $\lim_{N\to \infty} \eta(N)$ $=$
$0$. We give algorithms that compute essentially optimal certificates for the
positive semidefiniteness, Frobenius form, characteristic and minimal
polynomial of an $n\times n$ dense integer matrix $A$. Our certificates can be
verified in Monte-Carlo bit complexity $(n^2 \lognormA)^{1+o(1)}$, where
$\lognormA$ is the bit size of the integer entries, solving an open problem in
[Kaltofen, Nehring, Saunders, Proc.\ ISSAC 2011] subject to computational
hardness assumptions. Second, we give algorithms that compute certificates for
the rank of sparse or structured $n\times n$ matrices over an abstract field,
whose Monte Carlo verification complexity is $2$ matrix-times-vector products
$+$ $n^{1+o(1)}$ arithmetic operations in the field. For example, if the
$n\times n$ input matrix is sparse with $n^{1+o(1)}$ non-zero entries, our rank
certificate can be verified in $n^{1+o(1)}$ field operations. This extends also
to integer matrices with only an extra $||A||^{1+o(1)}$ factor. All our
certificates are based on interactive verification protocols with the
interaction removed by a Fiat-Shamir identification heuristic. The validity of
our verification procedure is subject to standard computational hardness
assumptions from cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4568</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4568</id><created>2014-01-18</created><updated>2014-07-21</updated><authors><author><keyname>Bensmail</keyname><forenames>Julien</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Harutyunyan</keyname><forenames>Ararat</forenames><affiliation>MI</affiliation></author><author><keyname>Hocquard</keyname><forenames>Herv&#xe9;</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Valicov</keyname><forenames>Petru</forenames><affiliation>LIP</affiliation></author></authors><title>Strong edge-colouring of sparse planar graphs</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong edge-colouring of a graph is a proper edge-colouring where each
colour class induces a matching. It is known that every planar graph with
maximum degree $\Delta$ has a strong edge-colouring with at most $4\Delta+4$
colours. We show that $3\Delta+1$ colours suffice if the graph has girth 6, and
$4\Delta$ colours suffice if $\Delta\geq 7$ or the girth is at least 5. In the
last part of the paper, we raise some questions related to a long-standing
conjecture of Vizing on proper edge-colouring of planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4575</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4575</id><created>2014-01-18</created><updated>2014-02-06</updated><authors><author><keyname>Lutz</keyname><forenames>Tobias</forenames></author></authors><title>Various Views on the Trapdoor Channel and an Upper Bound on its Capacity</title><categories>cs.IT math.IT</categories><comments>A mistake in the notation section was corrected. The correct
  definition of matrix $\tilde{I}_n$ is: $\tilde{I}_n$ is a $2^n \times 2^n$
  matrix whose secondary diagonal entries are all equal to 1 while the
  remaining entries are all equal to 0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two novel views are presented on the trapdoor channel. First, by deriving the
underlying iterated function system (IFS), it is shown that the trapdoor
channel with input blocks of length $n$ can be regarded as the $n$th element of
a sequence of shapes approximating a fractal. Second, an algorithm is presented
that fully characterizes the trapdoor channel and resembles the recursion of
generating all permutations of a given string. Subsequently, the problem of
maximizing a $n$-letter mutual information is considered. It is shown that
$\frac{1}{2}\log_2\left(\frac{5}{2}\right)\approx 0.6610$ bits per use is an
upper bound on the capacity of the trapdoor channel. This upper bound, which is
the tightest upper bound known proves that feedback increases capacity of the
trapdoor channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4578</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4578</id><created>2014-01-18</created><authors><author><keyname>Caminiti</keyname><forenames>Saverio</forenames></author><author><keyname>Cicali</keyname><forenames>Claudio</forenames></author><author><keyname>Gravino</keyname><forenames>Pietro</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Servedio</keyname><forenames>Vito D. P.</forenames></author><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Tria</keyname><forenames>Francesca</forenames></author></authors><title>XTribe: a web-based social computation platform</title><categories>cs.CY cs.HC</categories><comments>11 pages, 2 figures, 1 table, 2013 Third International Conference on
  Cloud and Green Computing (CGC), Sept. 30 2013-Oct. 2 2013, Karlsruhe,
  Germany</comments><journal-ref>IEEE Xplore, Cloud and Green Computing (CGC), 2013 Third
  International Conference on, 397-403 (2013)</journal-ref><doi>10.1109/CGC.2013.69</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years the Web has progressively acquired the status of an
infrastructure for social computation that allows researchers to coordinate the
cognitive abilities of human agents in on-line communities so to steer the
collective user activity towards predefined goals. This general trend is also
triggering the adoption of web-games as a very interesting laboratory to run
experiments in the social sciences and whenever the contribution of human
beings is crucially required for research purposes. Nowadays, while the number
of on-line users has been steadily growing, there is still a need of
systematization in the approach to the web as a laboratory. In this paper we
present Experimental Tribe (XTribe in short), a novel general purpose web-based
platform for web-gaming and social computation. Ready to use and already
operational, XTribe aims at drastically reducing the effort required to develop
and run web experiments. XTribe has been designed to speed up the
implementation of those general aspects of web experiments that are independent
of the specific experiment content. For example, XTribe takes care of user
management by handling their registration and profiles and in case of
multi-player games, it provides the necessary user grouping functionalities.
XTribe also provides communication facilities to easily achieve both
bidirectional and asynchronous communication. From a practical point of view,
researchers are left with the only task of designing and implementing the game
interface and logic of their experiment, on which they maintain full control.
Moreover, XTribe acts as a repository of different scientific experiments, thus
realizing a sort of showcase that stimulates users' curiosity, enhances their
participation, and helps researchers in recruiting volunteers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4580</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4580</id><created>2014-01-18</created><updated>2015-08-08</updated><authors><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Graph eigenvectors, fundamental weights and centrality metrics for nodes
  in networks</title><categories>math.SP cond-mat.stat-mech cs.DM cs.SI physics.soc-ph</categories><comments>New results are included. The appendices contain supplementary
  material. All comments are welcome!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several expressions for the $j$-th component $\left( x_{k}\right) _{j}$ of
the $k$-th eigenvector $x_{k}$ of a symmetric matrix $A$ belonging to
eigenvalue $\lambda_{k}$ and normalized as $x_{k}^{T}x_{k}=1$ are presented. In
particular, the expression \[ \left( x_{k}\right)
_{j}^{2}=-\frac{1}{c_{A}^{\prime}\left( \lambda _{k}\right) }\det\left(
A_{\backslash\left\{ j\right\} }-\lambda _{k}I\right) \] where $c_{A}\left(
\lambda\right) =\det\left( A-\lambda I\right) $ is the characteristic
polynomial of $A$, $c_{A}^{\prime}\left( \lambda\right) =\frac{dc_{A}\left(
\lambda\right) }{d\lambda}$ and $A_{\backslash\left\{ j\right\} }$ is obtained
from $A$ by removal of row $j$ and column $j$, suggests us to consider the
square eigenvector component as a graph centrality metric for node $j$ that
reflects the impact of the removal of node $j$ from the graph at an
eigenfrequency/eigenvalue $\lambda_{k}$ of a graph related matrix (such as the
adjacency or Laplacian matrix). Removal of nodes in a graph relates to the
robustness of a graph. The set of such nodal centrality metrics is
\textquotedblleft ideal\textquotedblright\ in the sense of being complete,
uncorrelated and mathematically precisely defined and computable. Fundamental
weights (column sum of $X$) and dual fundamental weights (row sum of $X$) are
introduced as spectral metrics that condense information embedded in the
orthogonal eigenvector matrix $X$, with elements $X_{ij}=\left(
x_{j}\right)_{i}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4582</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4582</id><created>2014-01-18</created><authors><author><keyname>Birch</keyname><forenames>David</forenames></author><author><keyname>Liang</keyname><forenames>Helen</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H J</forenames></author><author><keyname>Mullineux</keyname><forenames>Glen</forenames></author><author><keyname>Field</keyname><forenames>Tony</forenames></author><author><keyname>Ko</keyname><forenames>Joan</forenames></author><author><keyname>Simondetti</keyname><forenames>Alvise</forenames></author></authors><title>Multidisciplinary Engineering Models: Methodology and Case Study in
  Spreadsheet Analytics</title><categories>cs.SE</categories><comments>12 Pages, 8 Colour figures</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2013, ISBN:
  978-1-9054045-1-3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates a methodology to help practitioners maximise the
utility of complex multidisciplinary engineering models implemented as
spreadsheets, an area presenting unique challenges. As motivation we
investigate the expanding use of Integrated Resource Management(IRM) models
which assess the sustainability of urban masterplan designs. IRM models reflect
the inherent complexity of multidisciplinary sustainability analysis by
integrating models from many disciplines. This complexity makes their use
time-consuming and reduces their adoption.
  We present a methodology and toolkit for analysing multidisciplinary
engineering models implemented as spreadsheets to alleviate such problems and
increase their adoption. For a given output a relevant slice of the model is
extracted, visualised and analysed by computing model and interdisciplinary
metrics. A sensitivity analysis of the extracted model supports engineers in
their optimisation efforts. These methods expose, manage and reduce model
complexity and risk whilst giving practitioners insight into multidisciplinary
model composition. We report application of the methodology to several
generations of an industrial IRM model and detail the insight generated,
particularly considering model evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4589</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4589</id><created>2014-01-18</created><authors><author><keyname>Ibrahim</keyname><forenames>Rania</forenames></author><author><keyname>Yousri</keyname><forenames>Noha A.</forenames></author><author><keyname>Ismail</keyname><forenames>Mohamed A.</forenames></author><author><keyname>El-Makky</keyname><forenames>Nagwa M.</forenames></author></authors><title>miRNA and Gene Expression based Cancer Classification using Self-
  Learning and Co-Training Approaches</title><categories>cs.CE cs.LG</categories><comments>8 pages, 4 figures, 10 tables, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  miRNA and gene expression profiles have been proved useful for classifying
cancer samples. Efficient classifiers have been recently sought and developed.
A number of attempts to classify cancer samples using miRNA/gene expression
profiles are known in literature. However, the use of semi-supervised learning
models have been used recently in bioinformatics, to exploit the huge corpuses
of publicly available sets. Using both labeled and unlabeled sets to train
sample classifiers, have not been previously considered when gene and miRNA
expression sets are used. Moreover, there is a motivation to integrate both
miRNA and gene expression for a semi-supervised cancer classification as that
provides more information on the characteristics of cancer samples. In this
paper, two semi-supervised machine learning approaches, namely self-learning
and co-training, are adapted to enhance the quality of cancer sample
classification. These approaches exploit the huge public corpuses to enrich the
training data. In self-learning, miRNA and gene based classifiers are enhanced
independently. While in co-training, both miRNA and gene expression profiles
are used simultaneously to provide different views of cancer samples. To our
knowledge, it is the first attempt to apply these learning approaches to cancer
classification. The approaches were evaluated using breast cancer,
hepatocellular carcinoma (HCC) and lung cancer expression sets. Results show up
to 20% improvement in F1-measure over Random Forests and SVM classifiers.
Co-Training also outperforms Low Density Separation (LDS) approach by around
25% improvement in F1-measure in breast cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4590</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4590</id><created>2014-01-18</created><authors><author><keyname>Amig&#xf3;</keyname><forenames>Enrique</forenames></author><author><keyname>Gonzalo</keyname><forenames>Julio</forenames></author><author><keyname>Artiles</keyname><forenames>Javier</forenames></author><author><keyname>Verdejo</keyname><forenames>Felisa</forenames></author></authors><title>Combining Evaluation Metrics via the Unanimous Improvement Ratio and its
  Application to Clustering Tasks</title><categories>cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  689-718, 2011</journal-ref><doi>10.1613/jair.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Artificial Intelligence tasks cannot be evaluated with a single quality
criterion and some sort of weighted combination is needed to provide system
rankings. A problem of weighted combination measures is that slight changes in
the relative weights may produce substantial changes in the system rankings.
This paper introduces the Unanimous Improvement Ratio (UIR), a measure that
complements standard metric combination criteria (such as van Rijsbergen's
F-measure) and indicates how robust the measured differences are to changes in
the relative weights of the individual metrics. UIR is meant to elucidate
whether a perceived difference between two systems is an artifact of how
individual metrics are weighted.
  Besides discussing the theoretical foundations of UIR, this paper presents
empirical results that confirm the validity and usefulness of the metric for
the Text Clustering problem, where there is a tradeoff between precision and
recall based metrics and results are particularly sensitive to the weighting
scheme used to combine them. Remarkably, our experiments show that UIR can be
used as a predictor of how well differences between systems measured on a given
test bed will also hold in a different test bed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4591</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4591</id><created>2014-01-18</created><authors><author><keyname>Ponsen</keyname><forenames>Marc</forenames></author><author><keyname>de Jong</keyname><forenames>Steven</forenames></author><author><keyname>Lanctot</keyname><forenames>Marc</forenames></author></authors><title>Computing Approximate Nash Equilibria and Robust Best-Responses Using
  Sampling</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  575-605, 2011</journal-ref><doi>10.1613/jair.3402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses two contributions to decision-making in complex
partially observable stochastic games. First, we apply two state-of-the-art
search techniques that use Monte-Carlo sampling to the task of approximating a
Nash-Equilibrium (NE) in such games, namely Monte-Carlo Tree Search (MCTS) and
Monte-Carlo Counterfactual Regret Minimization (MCCFR). MCTS has been proven to
approximate a NE in perfect-information games. We show that the algorithm
quickly finds a reasonably strong strategy (but not a NE) in a complex
imperfect information game, i.e. Poker. MCCFR on the other hand has theoretical
NE convergence guarantees in such a game. We apply MCCFR for the first time in
Poker. Based on our experiments, we may conclude that MCTS is a valid approach
if one wants to learn reasonably strong strategies fast, whereas MCCFR is the
better choice if the quality of the strategy is most important. Our second
contribution relates to the observation that a NE is not a best response
against players that are not playing a NE. We present Monte-Carlo Restricted
Nash Response (MCRNR), a sample-based algorithm for the computation of
restricted Nash strategies. These are robust best-response strategies that (1)
exploit non-NE opponents more than playing a NE and (2) are not (overly)
exploitable by other strategies. We combine the advantages of two
state-of-the-art algorithms, i.e. MCCFR and Restricted Nash Response (RNR).
MCRNR samples only relevant parts of the game tree. We show that MCRNR learns
quicker than standard RNR in smaller games. Also we show in Poker that MCRNR
learns robust best-response strategies fast, and that these strategies exploit
opponents more than playing a NE does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4592</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4592</id><created>2014-01-18</created><authors><author><keyname>Baum</keyname><forenames>Jiri</forenames></author><author><keyname>Nicholson</keyname><forenames>Ann E.</forenames></author><author><keyname>Dix</keyname><forenames>Trevor I.</forenames></author></authors><title>Proximity-Based Non-uniform Abstractions for Approximate Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  477-522, 2012</journal-ref><doi>10.1613/jair.3414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a deterministic world, a planning agent can be certain of the consequences
of its planned sequence of actions. Not so, however, in dynamic, stochastic
domains where Markov decision processes are commonly used. Unfortunately these
suffer from the curse of dimensionality: if the state space is a Cartesian
product of many small sets (dimensions), planning is exponential in the number
of those dimensions.
  Our new technique exploits the intuitive strategy of selectively ignoring
various dimensions in different parts of the state space. The resulting
non-uniformity has strong implications, since the approximation is no longer
Markovian, requiring the use of a modified planner. We also use a spatial and
temporal proximity measure, which responds to continued planning as well as
movement of the agent through the state space, to dynamically adapt the
abstraction as planning progresses.
  We present qualitative and quantitative results across a range of
experimental domains showing that an agent exploiting this novel approximation
method successfully finds solutions to the planning problem using much less
than the full state space. We assess and analyse the features of domains which
our method can exploit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4593</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4593</id><created>2014-01-18</created><authors><author><keyname>Sadilek</keyname><forenames>Adam</forenames></author><author><keyname>Kautz</keyname><forenames>Henry</forenames></author></authors><title>Location-Based Reasoning about Complex Multi-Agent Behavior</title><categories>cs.MA cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  87-133, 2012</journal-ref><doi>10.1613/jair.3421</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that surprisingly rich models of human activity can
be learned from GPS (positional) data. However, most effort to date has
concentrated on modeling single individuals or statistical properties of groups
of people. Moreover, prior work focused solely on modeling actual successful
executions (and not failed or attempted executions) of the activities of
interest. We, in contrast, take on the task of understanding human
interactions, attempted interactions, and intentions from noisy sensor data in
a fully relational multi-agent setting. We use a real-world game of capture the
flag to illustrate our approach in a well-defined domain that involves many
distinct cooperative and competitive joint activities. We model the domain
using Markov logic, a statistical-relational language, and learn a theory that
jointly denoises the data and infers occurrences of high-level activities, such
as a player capturing an enemy. Our unified model combines constraints imposed
by the geometry of the game area, the motion model of the players, and by the
rules and dynamics of the game in a probabilistically and logically sound
fashion. We show that while it may be impossible to directly detect a
multi-agent activity due to sensor noise or malfunction, the occurrence of the
activity can still be inferred by considering both its impact on the future
behaviors of the people involved as well as the events that could have preceded
it. Further, we show that given a model of successfully performed multi-agent
activities, along with a set of examples of failed attempts at the same
activities, our system automatically learns an augmented model that is capable
of recognizing success and failure, as well as goals of peoples actions with
high accuracy. We compare our approach with other alternatives and show that
our unified model, which takes into account not only relationships among
individual players, but also relationships among activities over the entire
length of a game, although more computationally costly, is significantly more
accurate. Finally, we demonstrate that explicitly modeling unsuccessful
attempts boosts performance on other important recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4595</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4595</id><created>2014-01-18</created><authors><author><keyname>Fu</keyname><forenames>Na</forenames></author><author><keyname>Lau</keyname><forenames>Hoong Chuin</forenames></author><author><keyname>Varakantham</keyname><forenames>Pradeep R.</forenames></author><author><keyname>Xiao</keyname><forenames>Fei</forenames></author></authors><title>Robust Local Search for Solving RCPSP/max with Durational Uncertainty</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  43-86, 2012</journal-ref><doi>10.1613/jair.3424</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling problems in manufacturing, logistics and project management have
frequently been modeled using the framework of Resource Constrained Project
Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the
importance of these problems, providing scalable solution schedules for
RCPSP/max problems is a topic of extensive research. However, all existing
methods for solving RCPSP/max assume that durations of activities are known
with certainty, an assumption that does not hold in real world scheduling
problems where unexpected external events such as manpower availability,
weather changes, etc. lead to delays or advances in completion of activities.
Thus, in this paper, our focus is on providing a scalable method for solving
RCPSP/max problems with durational uncertainty. To that end, we introduce the
robust local search method consisting of three key ideas: (a) Introducing and
studying the properties of two decision rule approximations used to compute
start times of activities with respect to dynamic realizations of the
durational uncertainty; (b) Deriving the expression for robust makespan of an
execution strategy based on decision rule approximations; and (c) A robust
local search mechanism to efficiently compute activity execution strategies
that are robust against durational uncertainty. Furthermore, we also provide
enhancements to local search that exploit temporal dependencies between
activities. Our experimental results illustrate that robust local search is
able to provide robust execution strategies efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4596</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4596</id><created>2014-01-18</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Calimeri</keyname><forenames>Francesco</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Perri</keyname><forenames>Simona</forenames></author></authors><title>Unfounded Sets and Well-Founded Semantics of Answer Set Programs with
  Aggregates</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  487-527, 2011</journal-ref><doi>10.1613/jair.3432</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic programs with aggregates (LPA) are one of the major linguistic
extensions to Logic Programming (LP). In this work, we propose a generalization
of the notions of unfounded set and well-founded semantics for programs with
monotone and antimonotone aggregates (LPAma programs). In particular, we
present a new notion of unfounded set for LPAma programs, which is a sound
generalization of the original definition for standard (aggregate-free) LP. On
this basis, we define a well-founded operator for LPAma programs, the fixpoint
of which is called well-founded model (or well-founded semantics) for LPAma
programs. The most important properties of unfounded sets and the well-founded
semantics for standard LP are retained by this generalization, notably
existence and uniqueness of the well-founded model, together with a strong
relationship to the answer set semantics for LPAma programs. We show that one
of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for
a broader class of aggregates using approximating operators, coincides with the
well-founded model as defined in this work on LPAma programs. We also discuss
some complexity issues, most importantly we give a formal proof of tractable
computation of the well-founded model for LPA programs. Moreover, we prove that
for general LPA programs, which may contain aggregates that are neither
monotone nor antimonotone, deciding satisfaction of aggregate expressions with
respect to partial interpretations is coNP-complete. As a consequence, a
well-founded semantics for general LPA programs that allows for tractable
computation is unlikely to exist, which justifies the restriction on LPAma
programs. Finally, we present a prototype system extending DLV, which supports
the well-founded semantics for LPAma programs, at the time of writing the only
implemented system that does so. Experiments with this prototype show
significant computational advantages of aggregate constructs over equivalent
aggregate-free encodings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4597</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4597</id><created>2014-01-18</created><authors><author><keyname>Ginsberg</keyname><forenames>Matthew L.</forenames></author></authors><title>Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  851-886, 2011</journal-ref><doi>10.1613/jair.3437</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Dr.Fill, a program that solves American-style crossword puzzles.
From a technical perspective, Dr.Fill works by converting crosswords to
weighted CSPs, and then using a variety of novel techniques to find a solution.
These techniques include generally applicable heuristics for variable and value
selection, a variant of limited discrepancy search, and postprocessing and
partitioning ideas. Branch and bound is not used, as it was incompatible with
postprocessing and was determined experimentally to be of little practical
value. Dr.Fillls performance on crosswords from the American Crossword Puzzle
Tournament suggests that it ranks among the top fifty or so crossword solvers
in the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4598</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4598</id><created>2014-01-18</created><authors><author><keyname>Huang</keyname><forenames>Ruoyun</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author><author><keyname>Zhang</keyname><forenames>Weixiong</forenames></author></authors><title>SAS+ Planning as Satisfiability</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  293-328, 2012</journal-ref><doi>10.1613/jair.3442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning as satisfiability is a principal approach to planning with many
eminent advantages. The existing planning as satisfiability techniques usually
use encodings compiled from STRIPS. We introduce a novel SAT encoding scheme
(SASE) based on the SAS+ formalism. The new scheme exploits the structural
information in SAS+, resulting in an encoding that is both more compact and
efficient for planning. We prove the correctness of the new encoding by
establishing an isomorphism between the solution plans of SASE and that of
STRIPS based encodings. We further analyze the transition variables newly
introduced in SASE to explain why it accommodates modern SAT solving algorithms
and improves performance. We give empirical statistical results to support our
analysis. We also develop a number of techniques to further reduce the encoding
size of SASE, and conduct experimental studies to show the strength of each
individual technique. Finally, we report extensive experimental results to
demonstrate significant improvements of SASE over the state-of-the-art STRIPS
based encoding schemes in terms of both time and memory efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4599</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4599</id><created>2014-01-18</created><authors><author><keyname>Stulp</keyname><forenames>Freek</forenames></author><author><keyname>Fedrizzi</keyname><forenames>Andreas</forenames></author><author><keyname>M&#xf6;senlechner</keyname><forenames>Lorenz</forenames></author><author><keyname>Beetz</keyname><forenames>Michael</forenames></author></authors><title>Learning and Reasoning with Action-Related Places for Robust Mobile
  Manipulation</title><categories>cs.RO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  1-42, 2012</journal-ref><doi>10.1613/jair.3451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the concept of Action-Related Place (ARPlace) as a powerful and
flexible representation of task-related place in the context of mobile
manipulation. ARPlace represents robot base locations not as a single position,
but rather as a collection of positions, each with an associated probability
that the manipulation action will succeed when located there. ARPlaces are
generated using a predictive model that is acquired through experience-based
learning, and take into account the uncertainty the robot has about its own
location and the location of the object to be manipulated.
  When executing the task, rather than choosing one specific goal position
based only on the initial knowledge about the task context, the robot
instantiates an ARPlace, and bases its decisions on this ARPlace, which is
updated as new information about the task becomes available. To show the
advantages of this least-commitment approach, we present a transformational
planner that reasons about ARPlaces in order to optimize symbolic plans. Our
empirical evaluation demonstrates that using ARPlaces leads to more robust and
efficient mobile manipulation in the face of state estimation uncertainty on
our simulated robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4600</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4600</id><created>2014-01-18</created><authors><author><keyname>Zeng</keyname><forenames>Yifeng</forenames></author><author><keyname>Doshi</keyname><forenames>Prashant</forenames></author></authors><title>Exploiting Model Equivalences for Solving Interactive Dynamic Influence
  Diagrams</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  211-255, 2012</journal-ref><doi>10.1613/jair.3461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the problem of sequential decision making in partially observable
environments shared with other agents of uncertain types having similar or
conflicting objectives. This problem has been previously formalized by multiple
frameworks one of which is the interactive dynamic influence diagram (I-DID),
which generalizes the well-known influence diagram to the multiagent setting.
I-DIDs are graphical models and may be used to compute the policy of an agent
given its belief over the physical state and others models, which changes as
the agent acts and observes in the multiagent setting.
  As we may expect, solving I-DIDs is computationally hard. This is
predominantly due to the large space of candidate models ascribed to the other
agents and its exponential growth over time. We present two methods for
reducing the size of the model space and stemming its exponential growth. Both
these methods involve aggregating individual models into equivalence classes.
Our first method groups together behaviorally equivalent models and selects
only those models for updating which will result in predictive behaviors that
are distinct from others in the updated model space. The second method further
compacts the model space by focusing on portions of the behavioral predictions.
Specifically, we cluster actionally equivalent models that prescribe identical
actions at a single time step. Exactly identifying the equivalences would
require us to solve all models in the initial set. We avoid this by selectively
solving some of the models, thereby introducing an approximation. We discuss
the error introduced by the approximation, and empirically demonstrate the
improved efficiency in solving I-DIDs due to the equivalences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4601</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4601</id><created>2014-01-18</created><authors><author><keyname>Pesant</keyname><forenames>Gilles</forenames></author><author><keyname>Quimper</keyname><forenames>Claude-Guy</forenames></author><author><keyname>Zanarini</keyname><forenames>Alessandro</forenames></author></authors><title>Counting-Based Search: Branching Heuristics for Constraint Satisfaction
  Problems</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  173-210, 2012</journal-ref><doi>10.1613/jair.3463</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a search heuristic for constraint programming that is reliable
across problem domains has been an important research topic in recent years.
This paper concentrates on one family of candidates: counting-based search.
Such heuristics seek to make branching decisions that preserve most of the
solutions by determining what proportion of solutions to each individual
constraint agree with that decision. Whereas most generic search heuristics in
constraint programming rely on local information at the level of the individual
variable, our search heuristics are based on more global information at the
constraint level. We design several algorithms that are used to count the
number of solutions to specific families of constraints and propose some search
heuristics exploiting such information. The experimental part of the paper
considers eight problem domains ranging from well-established benchmark puzzles
to rostering and sport scheduling. An initial empirical analysis identifies
heuristic maxSD as a robust candidate among our proposals.eWe then evaluate the
latter against the state of the art, including the latest generic search
heuristics, restarts, and discrepancy-based tree traversals. Experimental
results show that counting-based search generally outperforms other generic
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4602</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4602</id><created>2014-01-18</created><authors><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Cloning in Elections: Finding the Possible Winners</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  529-573, 2011</journal-ref><doi>10.1613/jair.3468</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of manipulating elections by cloning candidates. In
our model, a manipulator can replace each candidate c by several clones, i.e.,
new candidates that are so similar to c that each voter simply replaces c in
his vote with a block of these new candidates, ranked consecutively. The
outcome of the resulting election may then depend on the number of clones as
well as on how each voter orders the clones within the block. We formalize what
it means for a cloning manipulation to be successful (which turns out to be a
surprisingly delicate issue), and, for a number of common voting rules,
characterize the preference profiles for which a successful cloning
manipulation exists. We also consider the model where there is a cost
associated with producing each clone, and study the complexity of finding a
minimum-cost cloning manipulation. Finally, we compare cloning with two related
problems: the problem of control by adding candidates and the problem of
possible (co)winners when new alternatives can join.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4603</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4603</id><created>2014-01-18</created><authors><author><keyname>Albacete</keyname><forenames>Esperanza</forenames></author><author><keyname>Calle</keyname><forenames>Javier</forenames></author><author><keyname>Castro</keyname><forenames>Elena</forenames></author><author><keyname>Cuadra</keyname><forenames>Dolores</forenames></author></authors><title>Semantic Similarity Measures Applied to an Ontology for Human-Like
  Interaction</title><categories>cs.AI cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  397-421, 2012</journal-ref><doi>10.1613/jair.3612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is the calculation of similarity between two concepts
from an ontology for a Human-Like Interaction system. In order to facilitate
this calculation, a similarity function is proposed based on five dimensions
(sort, compositional, essential, restrictive and descriptive) constituting the
structure of ontological knowledge. The paper includes a proposal for computing
a similarity function for each dimension of knowledge. Later on, the similarity
values obtained are weighted and aggregated to obtain a global similarity
measure. In order to calculate those weights associated to each dimension, four
training methods have been proposed. The training methods differ in the element
to fit: the user, concepts or pairs of concepts, and a hybrid approach. For
evaluating the proposal, the knowledge base was fed from WordNet and extended
by using a knowledge editing toolkit (Cognos). The evaluation of the proposal
is carried out through the comparison of system responses with those given by
human test subjects, both providing a measure of the soundness of the procedure
and revealing ways in which the proposal may be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4604</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4604</id><created>2014-01-18</created><authors><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Stoilos</keyname><forenames>Giorgos</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Completeness Guarantees for Incomplete Ontology Reasoners: Theory and
  Practice</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  419-476, 2012</journal-ref><doi>10.1613/jair.3470</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve scalability of query answering, the developers of Semantic Web
applications are often forced to use incomplete OWL 2 reasoners, which fail to
derive all answers for at least one query, ontology, and data set. The lack of
completeness guarantees, however, may be unacceptable for applications in areas
such as health care and defence, where missing answers can adversely affect the
applications functionality. Furthermore, even if an application can tolerate
some level of incompleteness, it is often advantageous to estimate how many and
what kind of answers are being lost.
  In this paper, we present a novel logic-based framework that allows one to
check whether a reasoner is complete for a given query Q and ontology T---that
is, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and
an arbitrary data set A. Since ontologies and typical queries are often fixed
at application design time, our approach allows application developers to check
whether a reasoner known to be incomplete in general is actually complete for
the kinds of input relevant for the application.
  We also present a technique that, given a query Q, an ontology T, and
reasoners R_1 and R_2 that satisfy certain assumptions, can be used to
determine whether, for each data set A, reasoner R_1 computes more answers to Q
w.r.t. T and A than reasoner R_2. This allows application developers to select
the reasoner that provides the highest degree of completeness for Q and T that
is compatible with the applications scalability requirements.
  Our results thus provide a theoretical and practical foundation for the
design of future ontology-based information systems that maximise scalability
while minimising or even eliminating incompleteness of query answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4605</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4605</id><created>2014-01-18</created><authors><author><keyname>Lee</keyname><forenames>J. H. M.</forenames></author><author><keyname>Leung</keyname><forenames>Ka Lun</forenames></author></authors><title>Consistency Techniques for Flow-Based Projection-Safe Global Cost
  Functions in Weighted Constraint Satisfaction</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  257-292, 2012</journal-ref><doi>10.1613/jair.3476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many combinatorial problems deal with preferences and violations, the goal of
which is to find solutions with the minimum cost. Weighted constraint
satisfaction is a framework for modeling such problems, which consists of a set
of cost functions to measure the degree of violation or preferences of
different combinations of variable assignments. Typical solution methods for
weighted constraint satisfaction problems (WCSPs) are based on branch-and-bound
search, which are made practical through the use of powerful consistency
techniques such as AC*, FDAC*, EDAC* to deduce hidden cost information and
value pruning during search. These techniques, however, are designed to be
efficient only on binary and ternary cost functions which are represented in
table form. In tackling many real-life problems, high arity (or global) cost
functions are required. We investigate efficient representation scheme and
algorithms to bring the benefits of the consistency techniques to also high
arity cost functions, which are often derived from hard global constraints from
classical constraint satisfaction. The literature suggests some global cost
functions can be represented as flow networks, and the minimum cost flow
algorithm can be used to compute the minimum costs of such networks in
polynomial time. We show that naive adoption of this flow-based algorithmic
method for global cost functions can result in a stronger form of null-inverse
consistency. We further show how the method can be modified to handle cost
projections and extensions to maintain generalized versions of AC* and FDAC*
for cost functions with more than two variables. Similar generalization for the
stronger EDAC* is less straightforward. We reveal the oscillation problem when
enforcing EDAC* on cost functions sharing more than one variable. To avoid
oscillation, we propose a weak version of EDAC* and generalize it to weak
EDGAC* for non-binary cost functions. Using various benchmarks involving the
soft variants of hard global constraints ALLDIFFERENT, GCC, SAME, and REGULAR,
empirical results demonstrate that our proposal gives improvements of up to an
order of magnitude when compared with the traditional constraint optimization
approach, both in terms of time and pruning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4606</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4606</id><created>2014-01-18</created><authors><author><keyname>Conrad</keyname><forenames>Patrick Raymond</forenames></author><author><keyname>Williams</keyname><forenames>Brian</forenames></author></authors><title>Drake: An Efficient Executive for Temporal Plans with Choice</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 42, pages
  607-659, 2011</journal-ref><doi>10.1613/jair.3478</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents Drake, a dynamic executive for temporal plans with choice.
Dynamic plan execution strategies allow an autonomous agent to react quickly to
unfolding events, improving the robustness of the agent. Prior work developed
methods for dynamically dispatching Simple Temporal Networks, and further
research enriched the expressiveness of the plans executives could handle,
including discrete choices, which are the focus of this work. However, in some
approaches to date, these additional choices induce significant storage or
latency requirements to make flexible execution possible.
  Drake is designed to leverage the low latency made possible by a
preprocessing step called compilation, while avoiding high memory costs through
a compact representation. We leverage the concepts of labels and environments,
taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to
concisely record the implications of the discrete choices, exploiting the
structure of the plan to avoid redundant reasoning or storage. Our labeling and
maintenance scheme, called the Labeled Value Set Maintenance System, is
distinguished by its focus on properties fundamental to temporal problems, and,
more generally, weighted graph algorithms. In particular, the maintenance
system focuses on maintaining a minimal representation of non-dominated
constraints. We benchmark Drakes performance on random structured problems, and
find that Drake reduces the size of the compiled representation by a factor of
over 500 for large problems, while incurring only a modest increase in run-time
latency, compared to prior work in compiled executives for temporal plans with
discrete choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4607</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4607</id><created>2014-01-18</created><authors><author><keyname>Lee</keyname><forenames>Joohyung</forenames></author><author><keyname>Palla</keyname><forenames>Ravi</forenames></author></authors><title>Reformulating the Situation Calculus and the Event Calculus in the
  General Theory of Stable Models and in Answer Set Programming</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  571-620, 2012</journal-ref><doi>10.1613/jair.3489</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circumscription and logic programs under the stable model semantics are two
well-known nonmonotonic formalisms. The former has served as a basis of
classical logic based action formalisms, such as the situation calculus, the
event calculus and temporal action logics; the latter has served as a basis of
a family of action languages, such as language A and several of its
descendants. Based on the discovery that circumscription and the stable model
semantics coincide on a class of canonical formulas, we reformulate the
situation calculus and the event calculus in the general theory of stable
models. We also present a translation that turns the reformulations further
into answer set programs, so that efficient answer set solvers can be applied
to compute the situation calculus and the event calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4609</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4609</id><created>2014-01-18</created><authors><author><keyname>Planken</keyname><forenames>L&#xe9;on R.</forenames></author><author><keyname>de Weerdt</keyname><forenames>Mathijs M.</forenames></author><author><keyname>van der Krogt</keyname><forenames>Roman P. J.</forenames></author></authors><title>Computing All-Pairs Shortest Paths by Leveraging Low Treewidth</title><categories>cs.DS cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  353-388, 2012</journal-ref><doi>10.1613/jair.3509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two new and efficient algorithms for computing all-pairs shortest
paths. The algorithms operate on directed graphs with real (possibly negative)
weights. They make use of directed path consistency along a vertex ordering d.
Both algorithms run in O(n^2 w_d) time, where w_d is the graph width induced by
this vertex ordering. For graphs of constant treewidth, this yields O(n^2)
time, which is optimal. On chordal graphs, the algorithms run in O(nm) time. In
addition, we present a variant that exploits graph separators to arrive at a
run time of O(n w_d^2 + n^2 s_d) on general graphs, where s_d andlt= w_d is the
size of the largest minimal separator induced by the vertex ordering d. We show
empirically that on both constructed and realistic benchmarks, in many cases
the algorithms outperform Floyd-Warshalls as well as Johnsons algorithm, which
represent the current state of the art with a run time of O(n^3) and O(nm + n^2
log n), respectively. Our algorithms can be used for spatial and temporal
reasoning, such as for the Simple Temporal Problem, which underlines their
relevance to the planning and scheduling community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4612</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4612</id><created>2014-01-18</created><authors><author><keyname>Velez</keyname><forenames>Javier</forenames></author><author><keyname>Hemann</keyname><forenames>Garrett</forenames></author><author><keyname>Huang</keyname><forenames>Albert S.</forenames></author><author><keyname>Posner</keyname><forenames>Ingmar</forenames></author><author><keyname>Roy</keyname><forenames>Nicholas</forenames></author></authors><title>Modelling Observation Correlations for Active Exploration and Robust
  Object Detection</title><categories>cs.RO cs.CV</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  423-453, 2012</journal-ref><doi>10.1613/jair.3516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, mobile robots are expected to carry out increasingly complex tasks in
multifarious, real-world environments. Often, the tasks require a certain
semantic understanding of the workspace. Consider, for example, spoken
instructions from a human collaborator referring to objects of interest; the
robot must be able to accurately detect these objects to correctly understand
the instructions. However, existing object detection, while competent, is not
perfect. In particular, the performance of detection algorithms is commonly
sensitive to the position of the sensor relative to the objects in the scene.
This paper presents an online planning algorithm which learns an explicit model
of the spatial dependence of object detection and generates plans which
maximize the expected performance of the detection, and by extension the
overall plan performance. Crucially, the learned sensor model incorporates
spatial correlations between measurements, capturing the fact that successive
measurements taken at the same or nearby locations are not independent. We show
how this sensor model can be incorporated into an efficient forward search
algorithm in the information space of detected objects, allowing the robot to
generate motion plans efficiently. We investigate the performance of our
approach by addressing the tasks of door and text detection in indoor
environments and demonstrate significant improvement in detection performance
during task execution over alternative methods in simulated and real robot
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4613</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4613</id><created>2014-01-18</created><authors><author><keyname>Jeavons</keyname><forenames>Peter</forenames></author><author><keyname>Petke</keyname><forenames>Justyna</forenames></author></authors><title>Local Consistency and SAT-Solvers</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  329-351, 2012</journal-ref><doi>10.1613/jair.3531</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local consistency techniques such as k-consistency are a key component of
specialised solvers for constraint satisfaction problems. In this paper we show
that the power of using k-consistency techniques on a constraint satisfaction
problem is precisely captured by using a particular inference rule, which we
call negative-hyper-resolution, on the standard direct encoding of the problem
into Boolean clauses. We also show that current clause-learning SAT-solvers
will discover in expected polynomial time any inconsistency that can be deduced
from a given set of clauses using negative-hyper-resolvents of a fixed size. We
combine these two results to show that, without being explicitly designed to do
so, current clause-learning SAT-solvers efficiently simulate k-consistency
techniques, for all fixed values of k. We then give some experimental results
to show that this feature allows clause-learning SAT-solvers to efficiently
solve certain families of constraint problems which are challenging for
conventional constraint-programming solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4629</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4629</id><created>2014-01-18</created><authors><author><keyname>Mohamed</keyname><forenames>Moustafa</forenames></author><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Mickelson</keyname><forenames>Alan</forenames></author></authors><title>HERMES: A Hierarchical Broadcast-Based Silicon Photonic Interconnect for
  Scalable Many-Core Systems</title><categories>cs.AR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical interconnection networks, as enabled by recent advances in silicon
photonic device and fabrication technology, have the potential to address
on-chip and off-chip communication bottlenecks in many-core systems. Although
several designs have shown superior power efficiency and performance compared
to electrical alternatives, these networks will not scale to the thousands of
cores required in the future.
  In this paper, we introduce Hermes, a hybrid network composed of an optimized
broadcast for power-efficient low-latency global-scale coordination and
circuit-switch sub-networks for high-throughput data delivery. This network
will scale for use in thousand core chip systems. At the physical level,
SoI-based adiabatic coupler has been designed to provide low-loss and compact
optical power splitting. Based on the adiabatic coupler, a topology based on
2-ary folded butterfly is designed to provide linear power division in a
thousand core layout with minimal cross-overs. To address the network agility
and provide for efficient use of optical bandwidth, a flow control and routing
mechanism is introduced to dynamically allocate bandwidth and provide fairness
usage of network resources. At the system level, bloom filter-based filtering
for localization of communication are designed for reducing global traffic. In
addition, a novel greedy-based data and workload migration are leveraged to
increase the locality of communication in a NUCA (non-uniform cache access)
architecture. First order analytic evaluation results have indicated that
Hermes is scalable to at least 1024 cores and offers significant performance
improvement and power savings over prior silicon photonic designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4633</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4633</id><created>2014-01-18</created><authors><author><keyname>Wang</keyname><forenames>Pengwei</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>Efficient Codes for Adversarial Wiretap Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [13] we proposed a ({\rho}_r , {\rho}_w )-adversarial wiretap channel
model (AWTP) in which the adversary can adaptively choose to see a fraction
{\rho}_r of the codeword sent over the channel, and modify a fraction {\rho}_w
of the codeword by adding arbitrary noise values to them. In this paper we give
the first efficient construction of a capacity achieving code family that
provides perfect secrecy for this channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4634</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4634</id><created>2014-01-18</created><authors><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>The Capacity of String-Replication Systems</title><categories>cs.IT cs.CL math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the majority of the human genome consists of repeated
sequences. Furthermore, it is believed that a significant part of the rest of
the genome also originated from repeated sequences and has mutated to its
current form. In this paper, we investigate the possibility of constructing an
exponentially large number of sequences from a short initial sequence and
simple replication rules, including those resembling genomic replication
processes. In other words, our goal is to find out the capacity, or the
expressive power, of these string-replication systems. Our results include
exact capacities, and bounds on the capacities, of four fundamental
string-replication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4642</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4642</id><created>2014-01-19</created><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>On the Capacity of Memoryless Adversary</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a model of communication under adversarial noise. In
this model, the adversary makes online decisions on whether to corrupt a
transmitted bit based on only the value of that bit. Like the usual binary
symmetric channel of information theory or the fully adversarial channel of
combinatorial coding theory, the adversary can, with high probability,
introduce at most a given fraction of error.
  It is shown that, the capacity (maximum rate of reliable information
transfer) of such memoryless adversary is strictly below that of the binary
symmetric channel. We give new upper bound on the capacity of such channel --
the tightness of this upper bound remains an open question. The main component
of our proof is the careful examination of error-correcting properties of a
code with skewed distance distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4644</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4644</id><created>2014-01-19</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Haurant</keyname><forenames>Pierrick</forenames><affiliation>CETHIL</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author></authors><title>Time series modeling and large scale global solar radiation forecasting
  from geostationary satellites data</title><categories>cs.CE physics.comp-ph stat.AP</categories><comments>Solar Energy (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a territory is poorly instrumented, geostationary satellites data can be
useful to predict global solar radiation. In this paper, we use geostationary
satellites data to generate 2-D time series of solar radiation for the next
hour. The results presented in this paper relate to a particular territory, the
Corsica Island, but as data used are available for the entire surface of the
globe, our method can be easily exploited to another place. Indeed 2-D hourly
time series are extracted from the HelioClim-3 surface solar irradiation
database treated by the Heliosat-2 model. Each point of the map have been used
as training data and inputs of artificial neural networks (ANN) and as inputs
for two persistence models (scaled or not). Comparisons between these models
and clear sky estimations were proceeded to evaluate the performances. We found
a normalized root mean square error (nRMSE) close to 16.5% for the two best
predictors (scaled persistence and ANN) equivalent to 35-45% related to ground
measurements. Finally in order to validate our 2-D predictions maps, we
introduce a new error metric called the gamma index which is a criterion for
comparing data from two matrixes in medical physics. As first results, we found
that in winter and spring, scaled persistence gives the best results (gamma
index test passing rate is respectively 67.7% and 86%), in autumn simple
persistence is the best predictor (95.3%) and ANN is the best in summer
(99.8%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4648</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4648</id><created>2014-01-19</created><authors><author><keyname>Siddiqui</keyname><forenames>Rafid</forenames></author><author><keyname>Khatibi</keyname><forenames>Siamak</forenames></author></authors><title>Visual Tracking using Particle Swarm Optimization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of robust extraction of visual odometry from a sequence of images
obtained by an eye in hand camera configuration is addressed. A novel approach
toward solving planar template based tracking is proposed which performs a
non-linear image alignment for successful retrieval of camera transformations.
In order to obtain global optimum a bio-metaheuristic is used for optimization
of similarity among the planar regions. The proposed method is validated on
image sequences with real as well as synthetic transformations and found to be
resilient to intensity variations. A comparative analysis of the various
similarity measures as well as various state-of-art methods reveal that the
algorithm succeeds in tracking the planar regions robustly and has good
potential to be used in real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4650</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4650</id><created>2014-01-19</created><authors><author><keyname>Bernini</keyname><forenames>Antonio</forenames></author><author><keyname>Bilotta</keyname><forenames>Stefano</forenames></author><author><keyname>Pinzani</keyname><forenames>Renzo</forenames></author><author><keyname>Vajnovszki</keyname><forenames>Vincent</forenames></author></authors><title>A Gray Code for cross-bifix-free sets</title><categories>cs.IT cs.DM math.CO math.IT</categories><doi>10.1017/S0960129515000067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cross-bifix-free set of words is a set in which no prefix of any length of
any word is the suffix of any other word in the set. A construction of
cross-bifix-free sets has recently been proposed by Chee {\it et al.} in 2013
within a constant factor of optimality. We propose a \emph{trace partitioned}
Gray code for these cross-bifix-free sets and a CAT algorithm generating it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4655</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4655</id><created>2014-01-19</created><authors><author><keyname>De Vogeleer</keyname><forenames>Karel</forenames></author><author><keyname>Memmi</keyname><forenames>Gerard</forenames></author><author><keyname>Jouvelot</keyname><forenames>Pierre</forenames></author><author><keyname>Coelho</keyname><forenames>Fabien</forenames></author></authors><title>The Energy/Frequency Convexity Rule: Modeling and Experimental
  Validation on Mobile Devices</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides both theoretical and experimental evidence for the
existence of an Energy/Frequency Convexity Rule, which relates energy
consumption and CPU frequency on mobile devices. We monitored a typical
smartphone running a specific computing-intensive kernel of multiple nested
loops written in C using a high-resolution power gauge. Data gathered during a
week-long acquisition campaign suggest that energy consumed per input element
is strongly correlated with CPU frequency, and, more interestingly, the curve
exhibits a clear minimum over a 0.2 GHz to 1.6 GHz window. We provide and
motivate an analytical model for this behavior, which fits well with the data.
Our work should be of clear interest to researchers focusing on energy usage
and minimization for mobile devices, and provide new insights for optimization
opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4657</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4657</id><created>2014-01-19</created><updated>2014-02-03</updated><authors><author><keyname>Kumar</keyname><forenames>Suman</forenames></author><author><keyname>Giridhar</keyname><forenames>K.</forenames></author></authors><title>Power Control Factor Selection in Uplink OFDMA Cellular Networks</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1203.1304 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uplink power control plays a key role on the performance of uplink cellular
network. In this work, the power control factor ($\in[0,1]$) is evaluated based
on three parameters namely: average transmit power, coverage probability and
average rate. In other words, we evaluate power control factor such that
average transmit power should be low, coverage probability of cell-edge users
should be high and also average rate over all the uplink users should be high.
We show through numerical studies that the power control factor should be close
to $0.5$ in order to achieve an acceptable trade-off between these three
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4658</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4658</id><created>2014-01-19</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author><author><keyname>Li</keyname><forenames>Yali</forenames></author><author><keyname>Ma</keyname><forenames>Zhanyou</forenames></author></authors><title>Computation Tree Logic Model Checking Based on Possibility Measures</title><categories>cs.LO</categories><comments>30 pages, 5 figures</comments><msc-class>68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to deal with the systematic verification with uncertain infromation
in possibility theory, Li and Li \cite{li12} introduced model checking of
linear-time properties in which the uncertainty is modeled by possibility
measures. Xue, Lei and Li \cite{Xue09} defined computation tree logic (CTL)
based on possibility measures, which is called possibilistic CTL (PoCTL). This
paper is a continuation of the above work. First, we study the expressiveness
of PoCTL. Unlike probabilistic CTL, it is shown that PoCTL (in particular,
qualitative PoCTL) is more powerful than CTL with respect to their
expressiveness. The equivalent expressions of basic CTL formulae using
qualitative PoCTL formulae are presented in detail. Some PoCTL formulae that
can not be expressed by any CTL formulae are presented. In particular, some
qualitative properties of repeated reachability and persistence are expressed
using PoCTL formulae. Next, adapting CTL model-checking algorithm, a method to
solve the PoCTL model-checking problem and its time complexity are discussed in
detail. Finally, an example is given to illustrate the PoCTL model-checking
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4660</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4660</id><created>2014-01-19</created><updated>2014-02-16</updated><authors><author><keyname>Crisan</keyname><forenames>Gloria Cerasela</forenames></author><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Pop</keyname><forenames>Petrica C.</forenames></author></authors><title>On the Resilience of an Ant-based System in Fuzzy Environments. An
  Empirical Study</title><categories>cs.NE</categories><comments>This paper has been withdrawn by the author due to copyrights</comments><msc-class>03B52, 78M50, 68T27, 68T05, 68T20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current work describes an empirical study conducted in order to
investigate the behavior of an optimization method in a fuzzy environment.
MAX-MIN Ant System, an efficient implementation of a heuristic method is used
for solving an optimization problem derived from the Traveling Salesman Problem
(TSP). Several publicly-available symmetric TSP instances and their fuzzy
variants are tested in order to extract some general features. The entry data
was adapted by introducing a two-dimensional systematic degree of fuzziness,
proportional with the number of nodes, the dimension of the instance and also
with the distances between nodes, the scale of the instance. The results show
that our proposed method can handle the data uncertainty, showing good
resilience and adaptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4662</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4662</id><created>2014-01-19</created><authors><author><keyname>Kumar</keyname><forenames>Suman</forenames></author><author><keyname>Kalyani</keyname><forenames>Sheetal</forenames></author><author><keyname>Giridhar</keyname><forenames>K.</forenames></author></authors><title>Optimal Thresholds for Coverage and Rate in FFR Schemes for Planned
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to: IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional frequency reuse (FFR) is an inter-cell interference coordination
scheme that is being actively researched for emerging wireless cellular
networks. In this work, we consider hexagonal tessellation based planned FFR
deployments, and derive expressions for the coverage probability and normalized
average rate for the downlink. In particular, given reuse $\frac{1}{3}$ (FR$3$
) and reuse $1$ (FR$1$) regions, and a Signal-to-Interference-plus-noise-Ratio
(SINR) threshold $S_{th}$ which decides the user assignment to either the FR$1$
or FR$3$ regions, we theoretically show that: $(i)$ The optimal choice of
$S_{th}$ which maximizes the coverage probability is $S_{th} = T$, where $T$ is
the required target SINR (for ensuring coverage), and $(ii)$ The optimal choice
of $S_{th}$ which maximizes the normalized average rate is given by the
expression $S_{th}=\max(T, T')$, where $T'$ is a function of the path loss
exponent and the fade parameters. For the optimal choice of $S_{th}$, we show
that FFR gives a higher rate than FR$1$ and a better coverage probability than
FR$3$. The impact of frequency correlation over the sub-bands allocated to the
FR$1$ and FR$3$ regions is analysed, and it is shown that correlation decreases
the average rate of the FFR network. Numerical results are provided, and these
match with the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4663</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4663</id><created>2014-01-19</created><updated>2014-07-25</updated><authors><author><keyname>Kumar</keyname><forenames>Suman</forenames></author><author><keyname>Kalyani</keyname><forenames>Sheetal</forenames></author></authors><title>Impact of Correlation between Nakagami-m Interferers on Coverage
  Probability and Rate in Cellular Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage probability and rate expressions are theoretically compared for the
following cases: $(i).$ Both the user channel and the $N$ interferers are
independent and non identical Nakagami-m distributed random variables (RVs).
$(ii).$ The $N$ interferers are correlated Nakagami-m RVs. It is analytically
shown that the coverage probability in the presence of correlated interferers
is greater than or equal to the coverage probability in the presence of
non-identical independent interferers when the shape parameter of the channel
between the user and its base station is not greater than one. It is further
analytically shown that the average rate in the presence of correlated
interferers is greater than or equal to the average rate in the presence of
non-identical independent interferers. Simulation results are provided and
these match with the obtained theoretical results. The utility of our results
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4666</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4666</id><created>2014-01-19</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Feng</keyname><forenames>Ruyong</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author><author><keyname>Singer</keyname><forenames>Michael F.</forenames></author></authors><title>Parallel Telescoping and Parameterized Picard--Vessiot Theory</title><categories>cs.SC math.CA</categories><comments>19 pages</comments><msc-class>12H05, 33F10</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Parallel telescoping is a natural generalization of differential
creative-telescoping for single integrals to line integrals. It computes a
linear ordinary differential operator $L$, called a parallel telescoper, for
several multivariate functions, such that the applications of $L$ to the
functions yield antiderivatives of a single function. We present a necessary
and sufficient condition guaranteeing the existence of parallel telescopers for
differentially finite functions, and develop an algorithm to compute minimal
ones for compatible hyperexponential functions. Besides computing annihilators
of parametric line integrals, we use the parallel telescoping for determining
Galois groups of parameterized partial differential systems of first order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4672</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4672</id><created>2014-01-19</created><authors><author><keyname>Righi</keyname><forenames>Simone</forenames></author><author><keyname>Tak&#xe1;cs</keyname><forenames>K&#xe1;roly</forenames></author></authors><title>Parallel versus Sequential Update and the Evolution of Cooperation with
  the Assistance of Emotional Strategies</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our study contributes to the debate on the evolution of cooperation in the
single-shot Prisoner's Dilemma (PD) played on networks. We construct a model in
which individuals are connected with positive and negative ties. Some agents
play sign-dependent strategies that use the sign of the relation as a shorthand
for determining appropriate action toward the opponent. In the context of our
model in which network topology, agent strategic types and relational signs
coevolve, the presence of sign-dependent strategies catalyzes the evolution of
cooperation. We highlight how the success of cooperation depends on a crucial
aspect of implementation: whether we apply parallel or sequential strategy
update. Parallel updating, with averaging of payoffs across interactions in the
social neighborhood, supports cooperation in a much wider set of parameter
values than sequential updating. Our results cast doubts about the realism and
generalizability of models that claim to explain the evolution of cooperation
but implicitly assume parallel updating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4674</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4674</id><created>2014-01-19</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Evolving Accuracy: A Genetic Algorithm to Improve Election Night
  Forecasts</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply genetic algorithms to the field of electoral studies.
Forecasting election results is one of the most exciting and demanding tasks in
the area of market research, especially due to the fact that decisions have to
be made within seconds on live television. We show that the proposed method
outperforms currently applied approaches and thereby provide an argument to
tighten the intersection between computer science and social science,
especially political science, further. We scrutinize the performance of our
algorithm's runtime behavior to evaluate its applicability in the field.
Numerical results with real data from a local election in the Austrian province
of Styria from 2010 substantiate the applicability of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4676</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4676</id><created>2014-01-19</created><authors><author><keyname>Mones</keyname><forenames>Enys</forenames></author><author><keyname>Pollner</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Vicsek</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Universal hierarchical behavior of citation networks</title><categories>physics.soc-ph cs.DL cs.SI physics.data-an</categories><doi>10.1088/1742-5468/2014/05/P05023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the essential features of the evolution of scientific research are
imprinted in the structure of citation networks. Connections in these networks
imply information about the transfer of knowledge among papers, or in other
words, edges describe the impact of papers on other publications. This inherent
meaning of the edges infers that citation networks can exhibit hierarchical
features, that is typical of networks based on decision-making. In this paper,
we investigate the hierarchical structure of citation networks consisting of
papers in the same field. We find that the majority of the networks follow a
universal trend towards a highly hierarchical state, and i) the various fields
display differences only concerning their phase in life (distance from the
&quot;birth&quot; of a field) or ii) the characteristic time according to which they are
approaching the stationary state. We also show by a simple argument that the
alterations in the behavior are related to and can be understood by the degree
of specialization corresponding to the fields. Our results suggest that during
the accumulation of knowledge in a given field, some papers are gradually
becoming relatively more influential than most of the other papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4680</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4680</id><created>2014-01-19</created><updated>2014-07-01</updated><authors><author><keyname>Wu</keyname><forenames>Chong</forenames></author><author><keyname>Ji</keyname><forenames>Shenggong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Liujun</forenames></author><author><keyname>Chen</keyname><forenames>Jiawei</forenames></author><author><keyname>Li</keyname><forenames>Xiaobin</forenames></author><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author></authors><title>Multiple Hybrid Phase Transition: Bootstrap Percolation on Complex
  Networks with Communities</title><categories>physics.soc-ph cs.SI</categories><doi>10.1209/0295-5075/107/48001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bootstrap percolation is a well-known model to study the spreading of rumors,
new products or innovations on social networks. The empirical studies show that
community structure is ubiquitous among various social networks. Thus, studying
the bootstrap percolation on the complex networks with communities can bring us
new and important insights of the spreading dynamics on social networks. It
attracts a lot of scientists' attentions recently. In this letter, we study the
bootstrap percolation on Erd\H{o}s-R\'{e}nyi networks with communities and
observed second order, hybrid (both second and first order) and multiple hybrid
phase transitions, which is rare in natural system. Moreover, we have
analytically solved this system and obtained the phase diagram, which is
further justified well by the corresponding simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4691</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4691</id><created>2014-01-19</created><authors><author><keyname>Hochrainer</keyname><forenames>Stefan</forenames></author><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Pflug</keyname><forenames>Georg</forenames></author></authors><title>An algorithm for calculating steady state probabilities of $M|E_r|c|K$
  queueing systems</title><categories>cs.SY cs.PF</categories><journal-ref>Central European Journal of Operations Research 13(1): 1-13. 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for calculating steady state probabilities of
$M|E_r|c|K$ queueing systems. The infinitesimal generator matrix is used to
define all possible states in the system and their transition probabilities.
While this matrix can be written down immediately for many other $M|PH|c|K$
queueing systems with phase-type service times (e.g. Coxian, Hypoexponential,
\ldots), it requires a more careful analysis for systems with Erlangian service
times. The constructed matrix may then be used to calculate steady state
probabilities using an iterative algorithm. The resulting steady state
probabilities can be used to calculate various performance measures, e.g. the
average queue length. Additionally, computational issues of the implementation
are discussed and an example from the field of telecommunication call-center
queue length will be outlined to substantiate the applicability of these
efforts. In the appendix, tables of the average queueing length given a
specific number of service channels, traffic density, and system size are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4696</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4696</id><created>2014-01-19</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author></authors><title>Evolutionary Optimization for Decision Making under Uncertainty</title><categories>cs.NE</categories><comments>Keynote talk at the MENDEL 2011</comments><journal-ref>Proceedings of MENDEL 2011: 107-113. 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing decision problems under uncertainty can be done using a variety of
solution methods. Soft computing and heuristic approaches tend to be powerful
for solving such problems. In this overview article, we survey Evolutionary
Optimization techniques to solve Stochastic Programming problems - both for the
single-stage and multi-stage case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4697</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4697</id><created>2014-01-19</created><authors><author><keyname>Fiore</keyname><forenames>Marcelo</forenames></author><author><keyname>Mahmoud</keyname><forenames>Ola</forenames></author></authors><title>Functorial Semantics of Second-Order Algebraic Theories</title><categories>math.CT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this work is to complete the algebraic foundations of
second-order languages from the viewpoint of categorical algebra as developed
by Lawvere. To this end, this paper introduces the notion of second-order
algebraic theory and develops its basic theory. A crucial role in the
definition is played by the second-order theory of equality $\M$, representing
the most elementary operators and equations present in every second-order
language. The category $\M$ can be described abstractly via the universal
property of being the free cartesian category on an exponentiable object.
Thereby, in the tradition of categorical algebra, a second-order algebraic
theory consists of a cartesian category $\Mlaw$ and a strict cartesian
identity-on-objects functor $\M \to \Mlaw$ that preserves the universal
exponentiable object of $\Mlaw$. Lawvere's functorial semantics for algebraic
theories can then be generalised to the second-order setting. To verify the
correctness of our theory, two categorical equivalences are established: at the
syntactic level, that of second-order equational presentations and second-order
algebraic theories; at the semantic level, that of second-order algebras and
second-order functorial models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4709</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4709</id><created>2014-01-19</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Adaptive Power Allocation Strategies using DSTC in Cooperative MIMO
  Networks</title><categories>cs.IT math.IT</categories><comments>5 figures, 9 pages. IET Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive Power Allocation (PA) algorithms with different criteria for a
cooperative Multiple-Input Multiple-Output (MIMO) network equipped with
Distributed Space-Time Coding (DSTC) are proposed and evaluated. Joint
constrained optimization algorithms to determine the power allocation
parameters, the channel parameters and the receive filter are proposed for each
transmitted stream in each link. Linear receive filter and maximum-likelihood
(ML) detection are considered with Amplify-and-Forward (AF) and
Decode-and-Forward (DF) cooperation strategies. In the proposed algorithms, the
elements in the PA matrices are optimized at the destination node and then
transmitted back to the relay nodes via a feedback channel. The effects of the
feedback errors are considered. Linear MMSE expressions and the PA matrices
depend on each other and are updated iteratively. Stochastic gradient (SG)
algorithms are developed with reduced computational complexity. Simulation
results show that the proposed algorithms obtain significant performance gains
as compared to existing power allocation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4712</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4712</id><created>2014-01-19</created><authors><author><keyname>Bodini</keyname><forenames>Olivier</forenames><affiliation>LIPN</affiliation></author><author><keyname>Julien</keyname><forenames>David</forenames><affiliation>LIPN</affiliation></author><author><keyname>Philippe</keyname><forenames>Marchal</forenames><affiliation>LAGA</affiliation></author></authors><title>Random-bit optimal uniform sampling for rooted planar trees with given
  sequence of degrees and Applications</title><categories>cs.DM math.CO</categories><comments>19 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we redesign and simplify an algorithm due to Remy et al. for
the generation of rooted planar trees that satisfies a given partition of
degrees. This new version is now optimal in terms of random bit complexity, up
to a multiplicative constant. We then apply a natural process
&quot;simulate-guess-and-proof&quot; to analyze the height of a random Motzkin in
function of its frequency of unary nodes. When the number of unary nodes
dominates, we prove some unconventional height phenomenon (i.e. outside the
universal square root behaviour.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4714</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4714</id><created>2014-01-19</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Revolutionary Algorithms</title><categories>cs.NE</categories><journal-ref>Proceedings of BIOMA 2012: 37-48. 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimization of dynamic problems is both widespread and difficult. When
conducting dynamic optimization, a balance between reinitialization and
computational expense has to be found. There are multiple approaches to this.
In parallel genetic algorithms, multiple sub-populations concurrently try to
optimize a potentially dynamic problem. But as the number of sub-population
increases, their efficiency decreases. Cultural algorithms provide a framework
that has the potential to make optimizations more efficient. But they adapt
slowly to changing environments. We thus suggest a confluence of these
approaches: revolutionary algorithms. These algorithms seek to extend the
evolutionary and cultural aspects of the former to approaches with a notion of
the political. By modeling how belief systems are changed by means of
revolution, these algorithms provide a framework to model and optimize dynamic
problems in an efficient fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4715</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4715</id><created>2014-01-19</created><authors><author><keyname>Blaum</keyname><forenames>Mario</forenames></author><author><keyname>Plank</keyname><forenames>James S.</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Construction of Partial MDS (PMDS) and Sector-Disk (SD) Codes with Two
  Global Parity Symbols</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial MDS (PMDS) codes are erasure codes combining local (row) correction
with global additional correction of entries, while Sector-Disk (SD) codes are
erasure codes that address the mixed failure mode of current RAID systems. It
has been an open problem to construct general codes that have the PMDS and the
SD properties, and previous work has relied on Monte-Carlo searches. In this
paper, we present a general construction that addresses the case of any number
of failed disks and in addition, two erased sectors. The construction requires
a modest field size. This result generalizes previous constructions extending
RAID~5 and RAID~6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4716</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4716</id><created>2014-01-19</created><updated>2014-01-25</updated><authors><author><keyname>He</keyname><forenames>Yunlong</forenames></author><author><keyname>Huang</keyname><forenames>Jun</forenames></author><author><keyname>Duan</keyname><forenames>Qiang</forenames></author><author><keyname>Xiong</keyname><forenames>Zi</forenames></author><author><keyname>Lv</keyname><forenames>Juan</forenames></author><author><keyname>Liu</keyname><forenames>Yanbing</forenames></author></authors><title>A Novel Admission Control Model in Cloud Computing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of Cloud computing technologies and wide adopt of
Cloud services and applications, QoS provisioning in Clouds becomes an
important research topic. In this paper, we propose an admission control
mechanism for Cloud computing. In particular we consider the high volume of
simultaneous requests for Cloud services and develop admission control for
aggregated traffic flows to address this challenge. By employ network calculus,
we determine effective bandwidth for aggregate flow, which is used for making
admission control decision. In order to improve network resource allocation
while achieving Cloud service QoS, we investigate the relationship between
effective bandwidth and equivalent capacity. We have also conducted extensive
experiments to evaluate performance of the proposed admission control
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4720</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4720</id><created>2014-01-19</created><updated>2014-06-24</updated><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author></authors><title>Computing low-degree factors of lacunary polynomials: a Newton-Puiseux
  approach</title><categories>cs.SC cs.CC cs.DS</categories><comments>22 pages</comments><acm-class>I.1.2; F.2.2</acm-class><journal-ref>Proceedings of the 39th International Symposium on Symbolic and
  Algebraic Computation (ISSAC'14), pp 224-231, ACM, 2014</journal-ref><doi>10.1145/2608628.2608649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for the computation of the irreducible factors of
degree at most $d$, with multiplicity, of multivariate lacunary polynomials
over fields of characteristic zero. The algorithm reduces this computation to
the computation of irreducible factors of degree at most $d$ of univariate
lacunary polynomials and to the factorization of low-degree multivariate
polynomials. The reduction runs in time polynomial in the size of the input
polynomial and in $d$. As a result, we obtain a new polynomial-time algorithm
for the computation of low-degree factors, with multiplicity, of multivariate
lacunary polynomials over number fields, but our method also gives partial
results for other fields, such as the fields of $p$-adic numbers or for
absolute or approximate factorization for instance.
  The core of our reduction uses the Newton polygon of the input polynomial,
and its validity is based on the Newton-Puiseux expansion of roots of bivariate
polynomials. In particular, we bound the valuation of $f(X,\phi)$ where $f$ is
a lacunary polynomial and $\phi$ a Puiseux series whose vanishing polynomial
has low degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4721</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4721</id><created>2014-01-19</created><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Energy Harvesting in M2M and WSN Space</title><categories>cs.NI cs.ET</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting or power harvesting or energy scavenging is a process where
energy is derived from external sources (e.g. solar power, thermal energy, wind
energy, salinity gradients, kinetic energy etc.), captured, and stored for
small, wireless autonomous devices, like those used in wearable electronics and
wireless sensor networks. This paper is focused to applications of Energy
Harvesting in Wireless Sensor Networks. This is going to help the ever growing
M2M (Machine to Machine) field where there is an exponential growth of
intelligent devices and automatic control of these is of paramount importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4725</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4725</id><created>2014-01-19</created><authors><author><keyname>Pinho</keyname><forenames>Armando J.</forenames></author><author><keyname>Pratas</keyname><forenames>Diogo</forenames></author><author><keyname>Ferreira</keyname><forenames>Paulo J. S. G.</forenames></author></authors><title>Information profiles for DNA pattern discovery</title><categories>q-bio.GN cs.IT math.IT</categories><comments>Full version of DCC 2014 paper &quot;Information profiles for DNA pattern
  discovery&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite-context modeling is a powerful tool for compressing and hence for
representing DNA sequences. We describe an algorithm to detect genomic
regularities, within a blind discovery strategy. The algorithm uses information
profiles built using suitable combinations of finite-context models. We used
the genome of the fission yeast Schizosaccharomyces pombe strain 972 h- for
illustration, unveilling locations of low information content, which are
usually associated with DNA regions of potential biological interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4726</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4726</id><created>2014-01-19</created><authors><author><keyname>Hoerbe</keyname><forenames>Rainer</forenames></author></authors><title>A Model for Privacy-enhanced Federated Identity Management</title><categories>cs.CR</categories><comments>36 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Identity federations operating in a business or consumer context need to
prevent the collection of user data across trust service providers for legal
and business case reasons. Legal reasons are given by data protection
legislation. Other reasons include business owners becoming increasingly aware
of confidentiality risks that go beyond traditional information security, e.g.,
the numbers of authentications to an EDI service might provide insights into
the volume of invoices, from which one could derive insider information. This
paper proposes extended technical controls supporting three privacy
requirements: a) Limit d Linkability: Two service providers cannot link data
related to a user without the help of a third party, using neither an
identifier nor other identifying attributes like email addresses or payment
data; b) Limited Observability: An identity provider cannot trace which
services a user is using without the help of a third party; c) Non-Disclosure:
Attributes provided to the service provider by an attribute provider are not
disclosed to the identity provider or an intermediate service broker. Using a
hub-and-spoke federation style following the privacy-by-design principle, this
reference architecture addresses the privacy controls mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4730</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4730</id><created>2014-01-19</created><authors><author><keyname>Koleini</keyname><forenames>Masoud</forenames></author><author><keyname>Ritter</keyname><forenames>Eike</forenames></author><author><keyname>Ryan</keyname><forenames>Mark</forenames></author></authors><title>Verification of agent knowledge in dynamic access control policies</title><categories>cs.LO cs.CR</categories><comments>The original version of this paper, &quot;Model checking agent knowledge
  in dynamic access control policies&quot;, appeared in Lecture Notes in Computer
  Science (LNCS), Volume 7795, 2013, pp 448-462</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a modeling technique based on interpreted systems in order to
verify temporal-epistemic properties over access control policies. This
approach enables us to detect information flow vulnerabilities in dynamic
policies by verifying the knowledge of the agents gained by both reading and
reasoning about system information. To overcome the practical limitations of
state explosion in model-checking temporal-epistemic properties, we introduce a
novel abstraction and refinement technique for temporal-epistemic safety
properties in ACTLK (ACTL with knowledge modality K) and a class of interesting
properties that does fall in this category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4734</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4734</id><created>2014-01-19</created><updated>2015-01-21</updated><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Optimal Fractional Repetition Codes based on Graphs and Designs</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional repetition (FR) codes is a family of codes for distributed storage
systems that allow for uncoded exact repairs having the minimum repair
bandwidth. However, in contrast to minimum bandwidth regenerating (MBR) codes,
where a random set of a certain size of available nodes is used for a node
repair, the repairs with FR codes are table based. This usually allows to store
more data compared to MBR codes. In this work, we consider bounds on the
fractional repetition capacity, which is the maximum amount of data that can be
stored using an FR code. Optimal FR codes which attain these bounds are
presented. The constructions of these FR codes are based on combinatorial
designs and on families of regular and biregular graphs. These constructions of
FR codes for given parameters raise some interesting questions in graph theory.
These questions and some of their solutions are discussed in this paper. In
addition, based on a connection between FR codes and batch codes, we propose a
new family of codes for DSS, namely fractional repetition batch codes, which
have the properties of batch codes and FR codes simultaneously. These are the
first codes for DSS which allow for uncoded efficient exact repairs and load
balancing which can be performed by several users in parallel. Other concepts
related to FR codes are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4735</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4735</id><created>2014-01-19</created><updated>2014-01-21</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Axioms for Definability and Full Completeness</title><categories>cs.LO</categories><comments>18 pages</comments><journal-ref>In Proof, language, and Interaction, MIT Press, pp. 55-76. 2000</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Axioms are presented which encapsulate the properties satisfied by categories
of games which form the basis of results on full abstraction for PCF and other
programming languages, and on full completeness for various logics and type
theories.
  Axioms are presented on models of PCF from which full abstraction can be
proved. These axioms have been distilled from recent results on definability
and full abstraction of game semantics for a number of programming languages.
Full completeness for pure simply-typed $\lambda$-calculus is also axiomatized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4739</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4739</id><created>2014-01-19</created><authors><author><keyname>Manuch</keyname><forenames>Jan</forenames></author><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author></authors><title>Finding minimum Tucker submatrices</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary matrix has the Consecutive Ones Property (C1P) if its columns can be
ordered in such a way that all 1s on each row are consecutive. These matrices
are used for DNA physical mapping and ancestral genome reconstruction in
computational biology on the other hand they represents a class of convex
bipartite graphs and are of interest of algorithm graph theory researchers.
Tucker gave a forbidden submartices characterization of matrices that have C1P
property in 1972. Booth and Lucker (1976) gave a first linear time recognition
algorithm for matrices with C1P property and then in 2002, Habib, et al. gave a
simpler linear time recognition algorithm. There has been substantial amount of
works on efficiently finding minimum size forbidden submatrix. Our algorithm is
at least $n$ times faster than the existing algorithm where $n$ is the number
of columns of the input matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4740</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4740</id><created>2014-01-19</created><authors><author><keyname>Friedkin</keyname><forenames>Noah E.</forenames></author></authors><title>Generalization of the PageRank Model</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a generalization of the PageRank model of page
centralities in the global webgraph of hyperlinks. The webgraph of adjacencies
is generalized to a valued directed graph, and the scalar dampening coefficient
for walks through the graph is relaxed to allow for heterogeneous values. A
visitation count approach may be employed to apply the more general model,
based on the number of visits to a page and the page's proportionate
allocations of these visits to other nodes of the webgraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4744</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4744</id><created>2014-01-19</created><updated>2014-05-02</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Sparse interpolation over finite fields via low-order roots of unity</title><categories>cs.SC</categories><comments>18 pages, 1 table, 4 procedures, accepted to ISSAC 2014</comments><msc-class>68W30, 12Y05, 13P05</msc-class><acm-class>G.4; I.1.1</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present a new Monte Carlo algorithm for the interpolation of a
straight-line program as a sparse polynomial $f$ over an arbitrary finite field
of size $q$. We assume a priori bounds $D$ and $T$ are given on the degree and
number of terms of $f$. The approach presented in this paper is a hybrid of the
diversified and recursive interpolation algorithms, the two previous fastest
known probabilistic methods for this problem. By making effective use of the
information contained in the coefficients themselves, this new algorithm
improves on the bit complexity of previous methods by a &quot;soft-Oh&quot; factor of
$T$, $\log D$, or $\log q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4750</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4750</id><created>2014-01-19</created><updated>2014-04-11</updated><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>How Much Frequency Can Be Reused in 5G Cellular Networks---A Matrix
  Graph Model</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5th Generation cellular network may have the key feature of smaller cell
size and denser resource employment, resulted from diminishing resource and
increasing communication demands. However, small cell may result in high
interference between cells. Moreover, the random geographic patterns of small
cell networks make them hard to analyze, at least excluding schemes in the
well-accepted hexagonal grid model. In this paper, a new model---the matrix
graph is proposed which takes advantage of the small cell size and high
inter-cell interference to reduce computation complexity. This model can
simulate real world networks accurately and offers convenience in frequency
allocation problems which are usually NP-complete. An algorithm dealing with
this model is also given, which asymptotically achieves the theoretical limit
of frequency allocation, and has a complexity which decreases with cell size
and grows linearly with the network size. This new model is specifically
proposed to characterize the next-generation cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4753</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4753</id><created>2014-01-19</created><authors><author><keyname>Zu</keyname><forenames>K.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Haardt</keyname><forenames>M.</forenames></author></authors><title>Multi-Branch Tomlinson-Harashima Precoding for MU-MIMO Systems: Theory
  and Algorithms</title><categories>cs.IT math.IT</categories><comments>13 figures, 12 pages. IEEE Transactions on Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tomlinson-Harashima precoding (THP) is a nonlinear processing technique
employed at the transmit side and is a dual to the successive interference
cancelation (SIC) detection at the receive side. Like SIC detection, the
performance of THP strongly depends on the ordering of the precoded symbols.
The optimal ordering algorithm, however, is impractical for multiuser MIMO
(MU-MIMO) systems with multiple receive antennas due to the fact that the users
are geographically distributed. In this paper, we propose a multi-branch THP
(MB-THP) scheme and algorithms that employ multiple transmit processing and
ordering strategies along with a selection scheme to mitigate interference in
MU-MIMO systems. Two types of multi-branch THP (MB-THP) structures are
proposed. The first one employs a decentralized strategy with diagonal weighted
filters at the receivers of the users and the second uses a diagonal weighted
filter at the transmitter. The MB-MMSE-THP algorithms are also derived based on
an extended system model with the aid of an LQ decomposition, which is much
simpler compared to the conventional MMSE-THP algorithms. Simulation results
show that a better bit error rate (BER) performance can be achieved by the
proposed MB-MMSE-THP precoder with a small computational complexity increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4770</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4770</id><created>2014-01-19</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Opinion Exchange Dynamics</title><categories>math.PR cs.GT</categories><comments>74 pages. arXiv admin note: substantial text overlap with
  arXiv:1207.5893</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey a range of models of opinion exchange. From the introduction: &quot;The
exchange of opinions between individuals is a fundamental social interaction...
Moreover, many models in this field are an excellent playground for
mathematicians, especially those working in probability, algorithms and
combinatorics. One of the goals of this survey is to introduce such models to
mathematicians, and especially to those working in discrete mathematics,
information theory, optimization and probability and statistics.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4780</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4780</id><created>2014-01-19</created><updated>2014-06-07</updated><authors><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Wright</keyname><forenames>Stephen J.</forenames></author><author><keyname>Sridhar</keyname><forenames>Srikrishna</forenames></author></authors><title>An Asynchronous Parallel Randomized Kaczmarz Algorithm</title><categories>math.NA cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an asynchronous parallel variant of the randomized Kaczmarz (RK)
algorithm for solving the linear system $Ax=b$. The analysis shows linear
convergence and indicates that nearly linear speedup can be expected if the
number of processors is bounded by a multiple of the number of rows in $A$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4786</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4786</id><created>2014-01-19</created><authors><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Common Information based Markov Perfect Equilibria for Linear-Gaussian
  Games with Asymmetric Information</title><categories>cs.SY cs.GT math.OC</categories><comments>Submitted to SIAM Journal of Control and Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of two-player dynamic stochastic nonzero-sum games where
the state transition and observation equations are linear, and the primitive
random variables are Gaussian. Each controller acquires possibly different
dynamic information about the state process and the other controller's past
actions and observations. This leads to a dynamic game of asymmetric
information among the controllers. Building on our earlier work on finite games
with asymmetric information, we devise an algorithm to compute a Nash
equilibrium by using the common information among the controllers. We call such
equilibria common information based Markov perfect equilibria of the game,
which can be viewed as a refinement of Nash equilibrium in games with
asymmetric information. If the players' cost functions are quadratic, then we
show that under certain conditions a unique common information based Markov
perfect equilibrium exists. Furthermore, this equilibrium can be computed by
solving a sequence of linear equations. We also show through an example that
there could be other Nash equilibria in a game of asymmetric information, not
corresponding to common information based Markov perfect equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4788</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4788</id><created>2014-01-19</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using
  quasi-arithmetic means</title><categories>cs.CV cs.IT math.IT</categories><comments>22 pages, include R code. To appear in Pattern Recognition Letters</comments><doi>10.1016/j.patrec.2014.01.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian classification labels observations based on given prior information,
namely class-a priori and class-conditional probabilities. Bayes' risk is the
minimum expected classification cost that is achieved by the Bayes' test, the
optimal decision rule. When no cost incurs for correct classification and unit
cost is charged for misclassification, Bayes' test reduces to the maximum a
posteriori decision rule, and Bayes risk simplifies to Bayes' error, the
probability of error. Since calculating this probability of error is often
intractable, several techniques have been devised to bound it with closed-form
formula, introducing thereby measures of similarity and divergence between
distributions like the Bhattacharyya coefficient and its associated
Bhattacharyya distance. The Bhattacharyya upper bound can further be tightened
using the Chernoff information that relies on the notion of best error
exponent. In this paper, we first express Bayes' risk using the total variation
distance on scaled distributions. We then elucidate and extend the
Bhattacharyya and the Chernoff upper bound mechanisms using generalized
weighted means. We provide as a byproduct novel notions of statistical
divergences and affinity coefficients. We illustrate our technique by deriving
new upper bounds for the univariate Cauchy and the multivariate
$t$-distributions, and show experimentally that those bounds are not too
distant to the computationally intractable Bayes' error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4790</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4790</id><created>2014-01-19</created><authors><author><keyname>Mishra</keyname><forenames>Dheerendra</forenames></author></authors><title>Cryptanalysis of Multi-Server Authenticated Key Agreement Scheme Based
  on Trust Computing Using Smart Cards and Biometrics</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancement in communication technology provides a scalable platform for
various services where a remote user can access the server from anywhere
without moving from its place. It has provided a unique opportunity for online
services, such that the user need not physically present at the service center.
These services adopt authentication and key agreement protocols to ensure
authorized and secure access to resources. Most of the authentication schemes
support single server environment where the user has to register with each
server. If a user wishes to access multiple application servers, he requires to
register with each of the servers. Although multi-server authentication schemes
introduced a scalable platform such that a user can interact with any server
using single registration. Recently, Chuang and Chen proposed an efficient
multi-server authenticated key agreement scheme based on smart cards along with
password and biometrics. This is a lightweight authentication scheme which
requires the computation of only hash function. In this article, we present a
brief review of Chuang and Chen's scheme. We analyze Chuang and Chen's scheme
and identify that their scheme does not resist stolen smart card attack which
causes the user's impersonation attack, server spoofing attack and man-in-the
middle attack. Additionally, we show that their scheme has a weak key agreement
protocol, which does not ensure forward secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4795</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4795</id><created>2014-01-20</created><authors><author><keyname>Cheung</keyname><forenames>Wai-Shun</forenames></author><author><keyname>Ng</keyname><forenames>Tuen-Wai</forenames></author></authors><title>A Three-Dimensional Voting System in Hong Kong</title><categories>cs.GT</categories><msc-class>91A40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The voting system in the Legislative Council of Hong Kong (Legco) is
sometimes unicameral and sometimes bicameral, depending on whether the bill is
proposed by the Hong Kong government. Therefore, although without any
representative within Legco, the Hong Kong government has certain degree of
legislative power --- as if there is a virtual representative of the Hong Kong
government within the Legco. By introducing such a virtual representative of
the Hong Kong government, we show that Legco is a three-dimensional voting
system. We also calculate two power indices of the Hong Kong government through
this virtual representative and consider the $C$-dimension and the
$W$-dimension of Legco. Finally, some implications of this Legco model to the
current constitutional reform in Hong Kong will be given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4799</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4799</id><created>2014-01-20</created><updated>2014-02-12</updated><authors><author><keyname>Cohen</keyname><forenames>Rami</forenames></author><author><keyname>Cassuto</keyname><forenames>Yuval</forenames></author></authors><title>LDPC Codes for Partial-Erasure Channels in Multi-Level Memories</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures. Partial version was submitted to ISIT 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we develop a new channel model, which we name the $q$-ary
partial erasure channel (QPEC). QPEC has a $q$-ary input, and its output is
either one symbol or a set of $M$ possible values. This channel mimics
situations when current/voltage levels in measurement channels are only
partially known, due to high read rates or imperfect current/voltage sensing.
Our investigation is concentrated on the performance of low-density
parity-pheck (LDPC) codes when used over this channel, due to their low
decoding complexity with iterative-decoding algorithms. We give the density
evolution equations of this channel, and develop its decoding-threshold
analysis. Part of the analysis shows that finding the exact decoding threshold
efficiently lies upon a solution to an open problem in additive combinatorics.
For this part we give bounds and approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4802</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4802</id><created>2014-01-20</created><authors><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Process Evolution Supported by Rationale: An Empirical Investigation of
  Process Changes</title><categories>cs.SE</categories><comments>8 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F11754305_36</comments><journal-ref>Software Process Change, volume 3966 of Lecture Notes in Computer
  Science, pages 334-341, Springer Berlin Heidelberg, 2006</journal-ref><doi>10.1007/11754305_36</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolving a software process model without a retrospective and, in
consequence, without an understanding of the process evolution, can lead to
severe problems for the software development organization, e.g., inefficient
performance as a consequence of the arbitrary introduction of changes or
difficulty in demonstrating compliance to a given standard. Capturing
information on the rationale behind changes can provide a means for better
understanding process evolution. This article presents the results of an
exploratory study with the goal of understanding the nature of process changes
in a given context. It presents the most important issues that motivated
process engineers changing important aerospace software process standards
during an industrial project. The study is part of research work intended to
incrementally define a systematic mechanism for process evolution supported by
rationale information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4808</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4808</id><created>2014-01-20</created><authors><author><keyname>Soto</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Maintaining a Large Process Model Aligned with a Process Standard: An
  Industrial Example</title><categories>cs.SE</categories><comments>12 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-75381-0_3</comments><journal-ref>Software Process Improvement, volume 4764 of Lecture Notes in
  Computer Science, pages 19-30, Springer Berlin Heidelberg, 2007</journal-ref><doi>10.1007/978-3-540-75381-0_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An essential characteristic of mature software and system development
organizations is the definition and use of explicit process models. For a
number of reasons, it can be valuable to produce new process models by
tailoring existing process standards (such as the V-Modell XT). Both process
models and standards evolve over time in order to integrate improvements or
adapt the process models to context changes. An important challenge for a
process engineering team is to keep tailored process models aligned over time
with the standards originally used to produce them. This article presents an
approach that supports the alignment of process standards evolving in parallel
to derived process models, using an actual industrial example to illustrate the
problems and potential solutions. We present and discuss the results of a
quantitative analysis done to determine whether a strongly tailored model can
still be aligned with its parent standard and to assess the potential cost of
such an alignment. We close the paper with conclusions and outlook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4821</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4821</id><created>2014-01-20</created><authors><author><keyname>Soto</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Process Model Difference Analysis for Supporting Process Evolution</title><categories>cs.SE</categories><comments>12 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F11908562_12</comments><journal-ref>Software Process Improvement, volume 4257 of Lecture Notes in
  Computer Science, pages 123-134, Springer Berlin Heidelberg, 2006</journal-ref><doi>10.1007/11908562_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development processes are subject to variations in time and space,
variations that can originate from learning effects, differences in application
domains, or a number of other causes. Identifying and analyzing such
differences is crucial for a variety of process activities, like defining and
evolving process standards, or analyzing the compliance of process models to
existing standards, among others. In this paper, we show why appropriately
identifying, describing, and visualizing differences between process models in
order to support such activities is a highly challenging task. We present
scenarios that motivate the need for process model difference analysis, and
describe the conceptual and technical challenges arising from them. In
addition, we sketch an initial tool-based approach implementing difference
analysis, and contrast it with similar existing approaches. The results from
this paper constitute the requirements for our ongoing development effort,
whose objectives we also describe briefly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4831</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4831</id><created>2014-01-20</created><updated>2014-02-25</updated><authors><author><keyname>Wang</keyname><forenames>Yi-Kai</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author><author><keyname>Zhong</keyname><forenames>Sheng</forenames></author></authors><title>Approximate Capacities of Two-Dimensional Codes by Spatial Mixing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply several state-of-the-art techniques developed in recent advances of
counting algorithms and statistical physics to study the spatial mixing
property of the two-dimensional codes arising from local hard (independent set)
constraints, including: hard-square, hard-hexagon, read/write isolated memory
(RWIM), and non-attacking kings (NAK). For these constraints, the strong
spatial mixing would imply the existence of polynomial-time approximation
scheme (PTAS) for computing the capacity. It was previously known for the
hard-square constraint the existence of strong spatial mixing and PTAS. We show
the existence of strong spatial mixing for hard-hexagon and RWIM constraints by
establishing the strong spatial mixing along self-avoiding walks, and
consequently we give PTAS for computing the capacities of these codes. We also
show that for the NAK constraint, the strong spatial mixing does not hold along
self-avoiding walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4834</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4834</id><created>2014-01-20</created><updated>2014-06-20</updated><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On Low-Complexity Full-diversity Detection In Multi-User MIMO
  Multiple-Access Channels</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, submitted to the IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) techniques are becoming commonplace in
recent wireless communication standards. This added dimension (i.e., space) can
be efficiently used to mitigate the interference in the multi-user MIMO
context. In this paper, we focus on the uplink of a MIMO multiple access
channel (MAC) where perfect channel state information (CSI) is only available
at the destination. We provide a new set of sufficient conditions for a wide
range of space-time block codes (STBC)s to achieve full-diversity under
\emph{partial interference cancellation group decoding} (PICGD) with or without
successive interference cancellation (SIC) for completely blind users. Explicit
interference cancellation (IC) schemes for two and three users are then
provided and shown to satisfy the derived full-diversity criteria. Besides the
complexity reduction due to the fact that the proposed IC schemes enable
separate decoding of distinct users without sacrificing the diversity gain,
further reduction of the decoding complexity may be obtained. In fact, thanks
to the structure of the proposed schemes, the real and imaginary parts of each
user's symbols may be decoupled without any loss of performance. Finally, our
theoretical claims are corroborated by simulation results and the new IC scheme
for two-user MIMO MAC is shown to outperform the recently proposed two-user IC
scheme especially for high spectral efficiency while requiring significantly
less decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4840</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4840</id><created>2014-01-20</created><updated>2014-02-14</updated><authors><author><keyname>Gogacz</keyname><forenames>Tomasz</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jerzy</forenames></author></authors><title>Termination of oblivious chase is undecidable</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We show that all--instances termination of chase is undecidable. More
precisely, there is no algorithm deciding, for a given set $\cal T$ consisting
of Tuple Generating Dependencies (a.k.a. Datalog$^\exists$ program), whether
the $\cal T$-chase on $D$ will terminate for every finite database instance
$D$. Our method applies to Oblivious Chase, Semi-Oblivious Chase and -- after a
slight modification -- also for Standard Chase. This means that we give a
(negative) solution to the all--instances termination problem for all version
of chase that are usually considered.
  The arity we need for our undecidability proof is three. We also show that
the problem is EXPSPACE-hard for binary signatures, but decidability for this
case is left open.
  Both the proofs -- for ternary and binary signatures -- are easy. Once you
know them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4844</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4844</id><created>2014-01-20</created><authors><author><keyname>Mourya</keyname><forenames>Ashish Kumar</forenames></author><author><keyname>Singhal</keyname><forenames>Niraj</forenames></author></authors><title>Managing Congestion Control in Mobile AD-HOC Network Using Mobile Agents</title><categories>cs.NI</categories><comments>9 Pages. IJCEA, 2014. arXiv admin note: substantial text overlap with
  arXiv:0907.5441 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mobile adhoc networks, congestion occurs with limited resources. The
standard TCP congestion control mechanism is not able to handle the special
properties of a shared wireless channel. TCP congestion control works very well
on the Internet. But mobile adhoc networks exhibit some unique properties that
greatly affect the design of appropriate protocols and protocol stacks in
general, and of congestion control mechanism in particular. As it turned out,
the vastly differing environment in a mobile adhoc network is highly
problematic for standard TCP. Many approaches have been proposed to overcome
these difficulties. Mobile agent based congestion control Technique is proposed
to avoid congestion in adhoc network. When mobile agent travels through the
network, it can select a less-loaded neighbor node as its next hop and update
the routing table according to the node congestion status. With the aid of
mobile agents, the nodes can get the dynamic network topology in time. In this
paper, a mobile agent based congestion control mechanism is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4848</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4848</id><created>2014-01-20</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>An Evolutionary Approach towards Clustering Airborne Laser Scanning Data</title><categories>cs.NE</categories><journal-ref>Proceedings of MENDEL 2012: 7-12. 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In land surveying, the generation of maps was greatly simplified with the
introduction of orthophotos and at a later stage with airborne LiDAR laser
scanning systems. While the original purpose of LiDAR systems was to determine
the altitude of ground elevations, newer full wave systems provide additional
information that can be used on classifying the type of ground cover and the
generation of maps. The LiDAR resulting point clouds are huge, multidimensional
data sets that need to be grouped in classes of ground cover. We propose a
genetic algorithm that aids in classifying these data sets and thus make them
usable for map generation. A key feature are tailor-made genetic operators and
fitness functions for the subject. The algorithm is compared to a traditional
k-means clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4849</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4849</id><created>2014-01-20</created><updated>2014-03-28</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>R&#xe1;cz</keyname><forenames>Mikl&#xf3;s Z.</forenames></author></authors><title>On the influence of the seed graph in the preferential attachment model</title><categories>math.PR cs.DM cs.SI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the influence of the seed graph in the preferential attachment
model, focusing on the case of trees. We first show that the seed has no effect
from a weak local limit point of view. On the other hand, we conjecture that
different seeds lead to different distributions of limiting trees from a total
variation point of view. We take a first step in proving this conjecture by
showing that seeds with different degree profiles lead to different limiting
distributions for the (appropriately normalized) maximum degree, implying that
such seeds lead to different (in total variation) limiting trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4857</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4857</id><created>2014-01-20</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>A Genetic Algorithm to Optimize a Tweet for Retweetability</title><categories>cs.NE cs.CY cs.SI physics.soc-ph</categories><journal-ref>Proceedings of MENDEL 2013: 13-18. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is a popular microblogging platform. When users send out messages,
other users have the ability to forward these messages to their own subgraph.
Most research focuses on increasing retweetability from a node's perspective.
Here, we center on improving message style to increase the chance of a message
being forwarded. To this end, we simulate an artificial Twitter-like network
with nodes deciding deterministically on retweeting a message or not. A genetic
algorithm is used to optimize message composition, so that the reach of a
message is increased. When analyzing the algorithm's runtime behavior across a
set of different node types, we find that the algorithm consistently succeeds
in significantly improving the retweetability of a message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4862</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4862</id><created>2014-01-20</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Antifragility = Elasticity + Resilience + Machine Learning: Models and
  Algorithms for Open System Fidelity</title><categories>cs.OH</categories><comments>Preliminary version submitted to the 1st International Workshop &quot;From
  Dependable to Resilient, from Resilient to Antifragile Ambients and Systems&quot;
  (ANTIFRAGILE 2014), https://sites.google.com/site/resilience2antifragile/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model of the fidelity of open systems - fidelity being
interpreted here as the compliance between corresponding figures of interest in
two separate but communicating domains. A special case of fidelity is given by
real-timeliness and synchrony, in which the figure of interest is the physical
and the system's notion of time. Our model covers two orthogonal aspects of
fidelity, the first one focusing on a system's steady state and the second one
capturing that system's dynamic and behavioural characteristics. We discuss how
the two aspects correspond respectively to elasticity and resilience and we
highlight each aspect's qualities and limitations. Finally we sketch the
elements of a new model coupling both of the first model's aspects and
complementing them with machine learning. Finally, a conjecture is put forward
that the new model may represent a first step towards compositional criteria
for antifragile systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4869</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4869</id><created>2014-01-20</created><authors><author><keyname>Rama</keyname><forenames>Taraka</forenames></author><author><keyname>Gali</keyname><forenames>Karthik</forenames></author><author><keyname>PVS</keyname><forenames>Avinesh</forenames></author></authors><title>Does Syntactic Knowledge help English-Hindi SMT?</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we explore various parameter settings of the state-of-art
Statistical Machine Translation system to improve the quality of the
translation for a `distant' language pair like English-Hindi. We proposed new
techniques for efficient reordering. A slight improvement over the baseline is
reported using these techniques. We also show that a simple pre-processing step
can improve the quality of the translation significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4872</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4872</id><created>2014-01-20</created><authors><author><keyname>Gabra</keyname><forenames>Hany Nashat</forenames></author><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman Mohammad</forenames></author><author><keyname>Korashy</keyname><forenames>Huda</forenames></author></authors><title>Classification of IDS Alerts with Data Mining Techniques</title><categories>cs.CR cs.DB cs.LG</categories><comments>2012 International Conference on Internet Study (NETs2012), Bangkok,
  Thailand</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A data mining technique to reduce the amount of false alerts within an IDS
system is proposed. The new technique achieves an accuracy of 99% compared to
97% by the current systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4876</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4876</id><created>2014-01-20</created><authors><author><keyname>Khanh</keyname><forenames>Ngo Tan Vu</forenames></author></authors><title>The critical factors affecting E-Government adoption: A Conceptual
  Framework in Vietnam</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic government (e-government) has established as an effective
mechanism for increasing government productivity and efficiency and a key
enabler of citizen- centric services. However, e-government implementation is
surrounded by technological, governing and social issues, which have to be
considered and treated carefully in order to facilitate this change. This
research attempts to explore and investigate the key challenges that influence
e-government implementation and the factors influencing citizen adoption in
Vietnam. It develops a conceptual framework on the basis of existing
experiences drawn from administrative reforms. Survey data from public employee
will be used to test the proposed hypothesis and the model. Therefore, this
research has identified factors that determine if the citizen will adopt
E-government services and thereby aiding governments in accessing what is
required to increase adoption. We will also highlight several research,
practitioner and policy implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4879</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4879</id><created>2014-01-20</created><updated>2014-04-15</updated><authors><author><keyname>Mamino</keyname><forenames>Marcello</forenames></author></authors><title>On the Computing Power of $+$, $-$, and $\times$</title><categories>cs.CC</categories><comments>11 pages, final version accepted by CSL-LICS 2014</comments><msc-class>68Q05, 68Q15</msc-class><acm-class>F.1.1; F.1.3; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modify the Blum-Shub-Smale model of computation replacing the permitted
computational primitives (the real field operations) with any finite set $B$ of
real functions semialgebraic over the rationals. Consider the class of boolean
decision problems that can be solved in polynomial time in the new model by
machines with no machine constants. How does this class depend on $B$? We prove
that it is always contained in the class obtained for $B = \{+, -, \times\}$.
Moreover, if $B$ is a set of continuous semialgebraic functions containing $+$
and $-$, and such that arbitrarily small numbers can be computed using $B$,
then we have the following dichotomy: either our class is $\mathsf P$ or it
coincides with the class obtained for $B = \{+, -, \times\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4891</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4891</id><created>2014-01-20</created><authors><author><keyname>Achballah</keyname><forenames>Ahmed Ben</forenames></author><author><keyname>Saoud</keyname><forenames>Slim Ben</forenames></author></authors><title>The Design of a Network-On-Chip Architecture Based On An Avionic
  Protocol</title><categories>cs.AR</categories><comments>5 pages World Symposium on Computer Applications &amp; Research WSCAR'
  2014, 18-20 January, Sousse, Tunisia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the Network-On-Chip (NoC) paradigm was introduced, many researchers have
proposed many novelistic NoC architectures, tools and design strategies. In
this paper we introduce a new approach in the field of designing
Network-On-Chip (NoC). Our inspiration came from an avionic protocol which is
the AFDX protocol. The proposed NoC architecture is a switch centric
architecture, with exclusive shortcuts between hosts and utilizes the
flexibility, the reliability and the performances offered by AFDX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4907</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4907</id><created>2014-01-20</created><updated>2014-11-10</updated><authors><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>Impact of Transceiver Power Consumption on the Energy Efficiency of
  Zero-Forcing Detector in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications (accepted, 13
  October 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the impact of transceiver power consumption on the energy
efficiency (EE) of the Zero Forcing (ZF) detector in the uplink of massive MIMO
systems, where a base station (BS) with $M$ antennas communicates coherently
with $K$ single antenna user terminals (UTs). We consider the problem of
maximizing the EE with respect to (M,K) for a fixed sum spectral efficiency.
Through analysis we study the impact of system parameters on the optimal EE.
System parameters consists of the average channel gain to the users and the
power consumption parameters (PCPs) (e.g., power consumed by each RF
antenna/receiver at BS). When the average user channel gain is high or else the
BS/UT design is power inefficient, our analysis reveals that it is optimal to
have a few BS antennas and a single user, i.e., non-massive MIMO regime.
Similarly, when the channel gain is small or else the BS/UT design is power
efficient, it is optimal of have a larger (M,K), i.e., massive MIMO regime.
Tight analytical bounds on the optimal EE are proposed for both these regimes.
The impact of the system parameters on the optimal EE is studied and several
interesting insights are drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4912</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4912</id><created>2014-01-20</created><updated>2015-02-05</updated><authors><author><keyname>Molkaraie</keyname><forenames>Mehdi</forenames></author></authors><title>An Importance Sampling Scheme on Dual Factor Graphs. I. Models in a
  Strong External Field</title><categories>stat.CO cond-mat.stat-mech cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an importance sampling scheme to estimate the partition function
of the two-dimensional ferromagnetic Ising model and the two-dimensional
ferromagnetic $q$-state Potts model, both in the presence of an external
magnetic field. The proposed scheme operates in the dual Forney factor graph
and is capable of efficiently computing an estimate of the partition function
under a wide range of model parameters. In particular, we consider models that
are in a strong external magnetic field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4917</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4917</id><created>2014-01-20</created><authors><author><keyname>Winter</keyname><forenames>Philipp</forenames></author><author><keyname>Lindskog</keyname><forenames>Stefan</forenames></author></authors><title>Spoiled Onions: Exposing Malicious Tor Exit Relays</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several hundred Tor exit relays together push more than 1 GiB/s of network
traffic. However, it is easy for exit relays to snoop and tamper with
anonymised network traffic and as all relays are run by independent volunteers,
not all of them are innocuous.
  In this paper, we seek to expose malicious exit relays and document their
actions. First, we monitored the Tor network after developing a fast and
modular exit relay scanner. We implemented several scanning modules for
detecting common attacks and used them to probe all exit relays over a period
of four months. We discovered numerous malicious exit relays engaging in
different attacks. To reduce the attack surface users are exposed to, we
further discuss the design and implementation of a browser extension patch
which fetches and compares suspicious X.509 certificates over independent Tor
circuits.
  Our work makes it possible to continuously monitor Tor exit relays. We are
able to detect and thwart many man-in-the-middle attacks which makes the
network safer for its users. All our code is available under a free license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4927</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4927</id><created>2014-01-20</created><authors><author><keyname>Sevenster</keyname><forenames>Merlijn</forenames></author></authors><title>Linear Programming Tools for Analyzing Strategic Games of
  Independence-Friendly Logic and Applications</title><categories>math.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, semantic games of independence-friendly logic were studied in
strategic form in terms of (mixed strategy) Nash equilibria. The class of
strategic games of independence-friendly logic is contained in the class of
win-loss, zero-sum two-player games. In this note we draw on the theory of
linear programming to develop tools to analyze the value of such games. We give
two applications of these tools to independence-friendly logic under the
so-called equilibrium semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4931</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4931</id><created>2014-01-20</created><updated>2015-05-26</updated><authors><author><keyname>K&#xfc;hn</keyname><forenames>Daniela</forenames></author><author><keyname>Osthus</keyname><forenames>Deryk</forenames></author><author><keyname>Patel</keyname><forenames>Viresh</forenames></author></authors><title>A domination algorithm for $\{0,1\}$-instances of the travelling
  salesman problem</title><categories>cs.DS math.CO</categories><comments>29 pages (final version to appear in Random Structures and
  Algorithms)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approximation algorithm for $\{0,1\}$-instances of the
travelling salesman problem which performs well with respect to combinatorial
dominance. More precisely, we give a polynomial-time algorithm which has
domination ratio $1-n^{-1/29}$. In other words, given a
$\{0,1\}$-edge-weighting of the complete graph $K_n$ on $n$ vertices, our
algorithm outputs a Hamilton cycle $H^*$ of $K_n$ with the following property:
the proportion of Hamilton cycles of $K_n$ whose weight is smaller than that of
$H^*$ is at most $n^{-1/29}$. Our analysis is based on a martingale approach.
Previously, the best result in this direction was a polynomial-time algorithm
with domination ratio $1/2-o(1)$ for arbitrary edge-weights. We also prove a
hardness result showing that, if the Exponential Time Hypothesis holds, there
exists a constant $C$ such that $n^{-1/29}$ cannot be replaced by $\exp(-(\log
n)^C)$ in the result above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4932</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4932</id><created>2014-01-20</created><authors><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author></authors><title>The existential fragment of S1S over element and successor is the
  co-Buchi languages</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buchi's theorem, in establishing the equivalence between languages definable
in S1S over element and &lt; and the omega-regular languages also demonstrated
that S1S over element and &lt; is no more expressive than its existential
fragment. It is also easy to see that S1S over element and &lt; is equi-expressive
with S1S over element and successor. However, it is not immediately obvious
whether it is possible to adapt Buchi's argument to establish equivalence
between expressivity in S1S over element and successor and its existential
fragment. In this paper we show that it is not: the existential fragment of S1S
over element and successor is strictly less expressive, and is in fact
equivalent to the co-Buchi languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4935</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4935</id><created>2014-01-20</created><authors><author><keyname>Ramesh</keyname><forenames>Chithrupa</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Performance Analysis of a Network of Event-based Systems</title><categories>cs.SY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a scenario where multiple event-based systems use a wireless
network to communicate with their respective controllers. These systems use a
contention resolution mechanism (CRM) to arbitrate access to the network. We
present a Markov model for the network interactions between the event-based
systems. Using this model, we obtain an analytical expression for the
reliability, or the probability of successfully transmitting a packet, in this
network. There are two important aspects to our model. Firstly, our model
captures the joint interactions of the event-triggering policy and the CRM.
This is required because event-triggering policies typically adapt to the CRM
outcome. Secondly, the model is obtained by decoupling interactions between the
different systems in the network, drawing inspiration from Bianchi's analysis
of IEEE 802.11. This is required because the network interactions introduce a
correlation between the system variables. We present Monte-Carlo simulations
that validate our model under various network configurations, and verify our
performance analysis as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4936</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4936</id><created>2014-01-20</created><authors><author><keyname>Li</keyname><forenames>P.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Low-Complexity Robust Data-Adaptive Dimensionality Reduction Based on
  Joint Iterative Optimization of Parameters</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. CAMSAP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a low-complexity robust data-dependent dimensionality
reduction based on a modified joint iterative optimization (MJIO) algorithm for
reduced-rank beamforming and steering vector estimation. The proposed robust
optimization procedure jointly adjusts the parameters of a rank-reduction
matrix and an adaptive beamformer. The optimized rank-reduction matrix projects
the received signal vector onto a subspace with lower dimension. The
beamformer/steering vector optimization is then performed in a
reduced-dimension subspace. We devise efficient stochastic gradient and
recursive least-squares algorithms for implementing the proposed robust MJIO
design. The proposed robust MJIO beamforming algorithms result in a faster
convergence speed and an improved performance. Simulation results show that the
proposed MJIO algorithms outperform some existing full-rank and reduced-rank
algorithms with a comparable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4942</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4942</id><created>2013-11-02</created><authors><author><keyname>Dodig-Crnkovic</keyname><forenames>Gordana</forenames></author></authors><title>Info-computational constructivism in modelling of life as cognition</title><categories>cs.AI</categories><comments>5 pages, SMLC conference University of Bergamo 12-14.09.2013,
  http://www.pt-ai.org/smlc/2013/schedule</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper addresses the open question formulated as: Which levels of
abstraction are appropriate in the synthetic modelling of life and cognition?
within the framework of info-computational constructivism, treating natural
phenomena as computational processes on informational structures. At present we
lack the common understanding of the processes of life and cognition in living
organisms with the details of co-construction of informational structures and
computational processes in embodied, embedded cognizing agents, both living and
artifactual ones. Starting with the definition of an agent as an entity capable
of acting on its own behalf, as an actor in Hewitt Actor model of computation,
even so simple systems as molecules can be modelled as actors exchanging
messages (information). We adopt Kauffmans view of a living agent as something
that can reproduce and undergoes at least one thermodynamic work cycle. This
definition of living agents leads to the Maturana and Varelas identification of
life with cognition. Within the info-computational constructive approach to
living beings as cognizing agents, from the simplest to the most complex living
systems, mechanisms of cognition can be studied in order to construct synthetic
model classes of artifactual cognizing agents on different levels of
organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4944</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4944</id><created>2014-01-20</created><updated>2014-05-13</updated><authors><author><keyname>Deleu</keyname><forenames>Thibault</forenames></author><author><keyname>Dervin</keyname><forenames>Mathieu</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Horlin</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Iterative pre-distortion of the non-linear satellite channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Video Broadcasting - Satellite - Second Generation (DVB-S2) is the
current European standard for satellite broadcast and broadband communications.
It relies on high order modulations up to 32-amplitude/phase-shift-keying
(APSK) in order to increase the system spectral efficiency. Unfortunately, as
the modulation order increases, the receiver becomes more sensitive to physical
layer impairments, and notably to the distortions induced by the power
amplifier and the channelizing filters aboard the satellite. Pre-distortion of
the non-linear satellite channel has been studied for many years. However, the
performance of existing pre-distortion algorithms generally becomes poor when
high-order modulations are used on a non-linear channel with a long memory. In
this paper, we investigate a new iterative method that pre-distorts blocks of
transmitted symbols so as to minimize the Euclidian distance between the
transmitted and received symbols. We also propose approximations to relax the
pre-distorter complexity while keeping its performance acceptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4950</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4950</id><created>2014-01-20</created><authors><author><keyname>Petschow</keyname><forenames>Matthias</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>MRRR-based Eigensolvers for Multi-core Processors and Supercomputers</title><categories>cs.MS cs.NA cs.PF math.NA</categories><comments>PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real symmetric tridiagonal eigenproblem is of outstanding importance in
numerical computations; it arises frequently as part of eigensolvers for
standard and generalized dense Hermitian eigenproblems that are based on a
reduction to tridiagonal form. For its solution, the algorithm of Multiple
Relatively Robust Representations (MRRR or MR3 in short) - introduced in the
late 1990s - is among the fastest methods. To compute k eigenpairs of a real
n-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in
contrast, all the other practical methods require O(k^2 n) or O(n^3) operations
in the worst case. This thesis centers around the performance and accuracy of
MRRR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4952</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4952</id><created>2013-11-08</created><authors><author><keyname>Oliveira</keyname><forenames>Washington A.</forenames></author><author><keyname>Neto</keyname><forenames>Luiz L. Salles</forenames></author><author><keyname>Moretti</keyname><forenames>Ant&#xf4;nio C.</forenames></author><author><keyname>Reis</keyname><forenames>Ednei F.</forenames></author></authors><title>Nonidentical circle packing problem: multiple disks installed in a
  rotating circular container</title><categories>cs.CG cs.CE</categories><comments>16 pages, 5 figures, 3 tables</comments><msc-class>90C59</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a heuristic algorithm for the layout optimization for
disks installed in a rotating circular container. This is a unequal circle
packing problem with additional balance constraints. It proved to be an NP-hard
problem, which justifies heuristic methods for its resolution. The main feature
of our heuristic is based on the selection of the next circle to be placed
inside the container according to the position of the system's center of mass.
Our approach has been tested on a series of instances up to 55 circles and
compared with the literature. Computational results show good performance in
terms of solution quality and computational time for the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4953</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4953</id><created>2014-01-20</created><updated>2014-05-17</updated><authors><author><keyname>Han</keyname><forenames>Jingjun</forenames></author><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Constructing Fewer Open Cells by GCD Computation in CAD Projection</title><categories>cs.SC</categories><comments>Accepted by ISSAC 2014 (July 23--25, 2014, Kobe, Japan)</comments><doi>10.1145/2608628.2608676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new projection operator based on cylindrical algebraic decomposition (CAD)
is proposed. The new operator computes the intersection of projection factor
sets produced by different CAD projection orders. In other words, it computes
the gcd of projection polynomials in the same variables produced by different
CAD projection orders. We prove that the new operator still guarantees
obtaining at least one sample point from every connected component of the
highest dimension, and therefore, can be used for testing semi-definiteness of
polynomials. Although the complexity of the new method is still doubly
exponential, in many cases, the new operator does produce smaller projection
factor sets and fewer open cells. Some examples of testing semi-definiteness of
polynomials, which are difficult to be solved by existing tools, have been
worked out efficiently by our program based on the new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4965</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4965</id><created>2014-01-20</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Marc</keyname><forenames>Tilen</forenames></author></authors><title>On the Cartesian Skeleton and the Factorization of the Strong Product of
  Digraphs</title><categories>cs.DM math.CO</categories><doi>10.1016/j.tcs.2014.10.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three standard products (the Cartesian, the direct and the strong
product) of undirected graphs have been wellinvestigated, unique prime factor
decomposition (PFD) are known and polynomial time algorithms have been
established for determining the prime factors.
  For directed graphs, unique PFD results with respect to the standard products
are known. However, there is still a lack of algorithms, that computes the PFD
of directed graphs with respect to the direct and the strong product in
general. In this contribution, we focus on the algorithmic aspects for
determining the PFD of directed graphs with respect to the strong product.
Essential for computing the prime factors is the construction of a so-called
Cartesian skeleton. This article introduces the notion of the Cartesian
skeleton of directed graphs as a generalization of the Cartesian skeleton of
undirected graphs. We provide new, fast and transparent algorithms for its
construction. Moreover, we present a first polynomial time algorithm for
determining the PFD with respect to the strong product of arbitrary connected
digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4972</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4972</id><created>2014-01-20</created><authors><author><keyname>Blin</keyname><forenames>L&#xe9;lia</forenames></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Compact Deterministic Self-Stabilizing Leader Election: The Exponential
  Advantage of Being Talkative</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on compact deterministic self-stabilizing solutions for
the leader election problem. When the protocol is required to be \emph{silent}
(i.e., when communication content remains fixed from some point in time during
any execution), there exists a lower bound of Omega(\log n) bits of memory per
node participating to the leader election (where n denotes the number of nodes
in the system). This lower bound holds even in rings. We present a new
deterministic (non-silent) self-stabilizing protocol for n-node rings that uses
only O(\log\log n) memory bits per node, and stabilizes in O(n\log^2 n) rounds.
Our protocol has several attractive features that make it suitable for
practical purposes. First, the communication model fits with the model used by
existing compilers for real networks. Second, the size of the ring (or any
upper bound on this size) needs not to be known by any node. Third, the node
identifiers can be of various sizes. Finally, no synchrony assumption, besides
a weakly fair scheduler, is assumed. Therefore, our result shows that, perhaps
surprisingly, trading silence for exponential improvement in term of memory
space does not come at a high cost regarding stabilization time or minimal
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4973</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4973</id><created>2014-01-20</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>What are the fundamental structures of concurrency? We still don't know!</title><categories>cs.LO</categories><comments>5 pages</comments><journal-ref>Electronic Notes in Theoretical Computer Science 162 (2006): 37-41</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process algebra has been successful in many ways; but we don't yet see the
lineaments of a fundamental theory. Some fleeting glimpses are sought from
Petri Nets, physics and geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.4994</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.4994</id><created>2014-01-20</created><authors><author><keyname>Mavridis</keyname><forenames>Nikolaos</forenames></author></authors><title>A Review of Verbal and Non-Verbal Human-Robot Interactive Communication</title><categories>cs.RO cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an overview of human-robot interactive communication is
presented, covering verbal as well as non-verbal aspects of human-robot
interaction. Following a historical introduction, and motivation towards fluid
human-robot communication, ten desiderata are proposed, which provide an
organizational axis both of recent as well as of future research on human-robot
communication. Then, the ten desiderata are examined in detail, culminating to
a unifying discussion, and a forward-looking conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5004</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5004</id><created>2014-01-20</created><authors><author><keyname>Ramesh</keyname><forenames>Chithrupa</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Stability Analysis and Design of a Network of Event-based Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network of event-based systems that use a shared wireless
medium to communicate with their respective controllers. These systems use a
contention resolution mechanism to arbitrate access to the shared network. We
identify sufficient conditions for Lyapunov mean square stability of each
control system in the network, and design event-based policies that guarantee
it. Our stability analysis is based on a Markov model that removes the
network-induced correlation between the states of the control systems in the
network. Analyzing the stability of this Markov model remains a challenge, as
the event-triggering policy renders the estimation error non-Gaussian. Hence,
we identify an auxiliary system that furnishes an upper bound for the variance
of the system states. Using the stability analysis, we design policies, such as
the constant-probability policy, for adapting the event-triggering thresholds
to the delay in accessing the network. Realistic wireless networked control
examples illustrate the applicability of the presented approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5014</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5014</id><created>2014-01-20</created><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Solomon</keyname><forenames>Shay</forenames></author></authors><title>Light spanners for snowflake metrics</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classic result in the study of spanners is the existence of light
low-stretch spanners for Euclidean spaces. These spanners ahve arbitrary low
stretch, and weight only a constant factor greater than that of the minimum
spanning tree of the points (with dependence on the stretch and Euclidean
dimention). A central open problem in this field asks whether other spaces
admit low weight spanners as well - for example metric space with low intrinsic
dimension - yet only a handful of results of this type are known.
  In this paper, we consider snowflake metric spaces of low intrinsic
dimension. The {\alpha}-snowflake of a metric (X,{\delta}) is the metric
(X,${\delta}^{\alpha}$) for 0&lt;{\alpha}&lt;1. By utilizing an approach completely
different than those used for Euclidean spaces, we demonstrate that snowflake
metrics admit light spanners. Further, we show that the spanner is of diameter
O($\log$n), a result not possible for Euclidean spaces. As an immediate
corollary to our spanner, we obtain dramatic improvments in algorithms for the
traveling salesman problem in this setting, achieving a polynomial-time
approximation scheme with near-linear runtime. Along the way, we show that all
${\ell}_p$ spaces admit light spanners, a result of interest in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5024</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5024</id><created>2013-12-21</created><authors><author><keyname>Fauzia</keyname><forenames>Naznin</forenames><affiliation>OSU</affiliation></author><author><keyname>Elango</keyname><forenames>Venmugil</forenames><affiliation>OSU</affiliation></author><author><keyname>Ravishankar</keyname><forenames>Mahesh</forenames><affiliation>OSU</affiliation></author><author><keyname>Ramanujam</keyname><forenames>J.</forenames><affiliation>ECE</affiliation></author><author><keyname>Rastello</keyname><forenames>Fabrice</forenames><affiliation>LIP</affiliation></author><author><keyname>Rountev</keyname><forenames>Atanas</forenames><affiliation>OSU</affiliation></author><author><keyname>Pouchet</keyname><forenames>Louis-No&#xeb;l</forenames><affiliation>UCLA-CS</affiliation></author><author><keyname>Sadayappan</keyname><forenames>P.</forenames><affiliation>CSE</affiliation></author></authors><title>Beyond Reuse Distance Analysis: Dynamic Analysis for Characterization of
  Data Locality Potential</title><categories>cs.OH</categories><comments>Transaction on Architecture and Code Optimization (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging computer architectures will feature drastically decreased flops/byte
(ratio of peak processing rate to memory bandwidth) as highlighted by recent
studies on Exascale architectural trends. Further, flops are getting cheaper
while the energy cost of data movement is increasingly dominant. The
understanding and characterization of data locality properties of computations
is critical in order to guide efforts to enhance data locality. Reuse distance
analysis of memory address traces is a valuable tool to perform data locality
characterization of programs. A single reuse distance analysis can be used to
estimate the number of cache misses in a fully associative LRU cache of any
size, thereby providing estimates on the minimum bandwidth requirements at
different levels of the memory hierarchy to avoid being bandwidth bound.
However, such an analysis only holds for the particular execution order that
produced the trace. It cannot estimate potential improvement in data locality
through dependence preserving transformations that change the execution
schedule of the operations in the computation. In this article, we develop a
novel dynamic analysis approach to characterize the inherent locality
properties of a computation and thereby assess the potential for data locality
enhancement via dependence preserving transformations. The execution trace of a
code is analyzed to extract a computational directed acyclic graph (CDAG) of
the data dependences. The CDAG is then partitioned into convex subsets, and the
convex partitioning is used to reorder the operations in the execution trace to
enhance data locality. The approach enables us to go beyond reuse distance
analysis of a single specific order of execution of the operations of a
computation in characterization of its data locality properties. It can serve a
valuable role in identifying promising code regions for manual transformation,
as well as assessing the effectiveness of compiler transformations for data
locality enhancement. We demonstrate the effectiveness of the approach using a
number of benchmarks, including case studies where the potential shown by the
analysis is exploited to achieve lower data movement costs and better
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5027</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5027</id><created>2014-01-20</created><updated>2014-07-06</updated><authors><author><keyname>Ke</keyname><forenames>Qing</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Tie Strength Distribution in Scientific Collaboration Networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 90, 032804 (2014)</journal-ref><doi>10.1103/PhysRevE.90.032804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Science is increasingly dominated by teams. Understanding patterns of
scientific collaboration and their impacts on the productivity and evolution of
disciplines is crucial to understand scientific processes. Electronic
bibliography offers a unique opportunity to map and investigate the nature of
scientific collaboration. Recent work have demonstrated a counter-intuitive
organizational pattern of scientific collaboration networks: densely
interconnected local clusters consist of weak ties, whereas strong ties play
the role of connecting different clusters. This pattern contrasts itself from
many other types of networks where strong ties form communities while weak ties
connect different communities. Although there are many models for collaboration
networks, no model reproduces this pattern. In this paper, we present an
evolution model of collaboration networks, which reproduces many properties of
real-world collaboration networks, including the organization of tie strengths,
skewed degree and weight distribution, high clustering and assortative mixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5031</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5031</id><created>2014-01-20</created><updated>2014-01-29</updated><authors><author><keyname>Ramsey</keyname><forenames>Joseph D.</forenames></author></authors><title>A Scalable Conditional Independence Test for Nonlinear, Non-Gaussian
  Data</title><categories>cs.AI stat.ME</categories><comments>4 Figures, 2 Boxes, 1 Table, 15 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many relations of scientific interest are nonlinear, and even in linear
systems distributions are often non-Gaussian, for example in fMRI BOLD data. A
class of search procedures for causal relations in high dimensional data relies
on sample derived conditional independence decisions. The most common
applications rely on Gaussian tests that can be systematically erroneous in
nonlinear non-Gaussian cases. Recent work (Gretton et al. (2009), Tillman et
al. (2009), Zhang et al. (2011)) has proposed conditional independence tests
using Reproducing Kernel Hilbert Spaces (RKHS). Among these, perhaps the most
efficient has been KCI (Kernel Conditional Independence, Zhang et al. (2011)),
with computational requirements that grow effectively at least as O(N3),
placing it out of range of large sample size analysis, and restricting its
applicability to high dimensional data sets. We propose a class of O(N2) tests
using conditional correlation independence (CCI) that require a few seconds on
a standard workstation for tests that require tens of minutes to hours for the
KCI method, depending on degree of parallelization, with similar accuracy. For
accuracy on difficult nonlinear, non-Gaussian data sets, we also compare a
recent test due to Harris &amp; Drton (2012), applicable to nonlinear, non-Gaussian
distributions in the Gaussian copula, as well as to partial correlation, a
linear Gaussian test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5037</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5037</id><created>2014-01-20</created><authors><author><keyname>Mukherjee</keyname><forenames>Manuj</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Sankarasubramaniam</keyname><forenames>Yogesh</forenames></author></authors><title>Achieving SK Capacity in the Source Model: When Must All Terminals Talk?</title><categories>cs.IT math.IT</categories><comments>A 5-page version of this paper was submitted to the 2014 IEEE
  International Symposium on Information Theory (ISIT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of characterizing the instances of the
multiterminal source model of Csisz\'ar and Narayan in which communication from
all terminals is needed for establishing a secret key of maximum rate. We give
an information-theoretic sufficient condition for identifying such instances.
We believe that our sufficient condition is in fact an exact characterization,
but we are only able to prove this in the case of the three-terminal source
model. We also give a relatively simple criterion for determining whether or
not our condition holds for a given multiterminal source model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5039</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5039</id><created>2014-01-20</created><authors><author><keyname>Driggs-Campbell</keyname><forenames>Katherine</forenames></author><author><keyname>Bellegarda</keyname><forenames>Guillaume</forenames></author><author><keyname>Shia</keyname><forenames>Victor</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Experimental Design for Human-in-the-Loop Driving Simulations</title><categories>cs.SY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes a new experimental setup for human-in-the-loop
simulations. A force feedback simulator with four axis motion has been setup
for real-time driving experiments. The simulator will move to simulate the
forces a driver feels while driving, which allows for a realistic experience
for the driver. This setup allows for flexibility and control for the
researcher in a realistic simulation environment. Experiments concerning driver
distraction can also be carried out safely in this test bed, in addition to
multi-agent experiments. All necessary code to run the simulator, the
additional sensors, and the basic processing is available for use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5051</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5051</id><created>2014-01-20</created><authors><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author><author><keyname>Blin</keyname><forenames>Guillaume</forenames></author><author><keyname>Revuz</keyname><forenames>Dominique</forenames></author><author><keyname>Faye</keyname><forenames>David</forenames></author></authors><title>WaterFowl, a Compact, Self-indexed RDF Store with Inference-enabled
  Dictionaries</title><categories>cs.DB</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach -- called WaterFowl -- for the
storage of RDF triples that addresses some key issues in the contexts of big
data and the Semantic Web. The architecture of our prototype, largely based on
the use of succinct data structures, enables the representation of triples in a
self-indexed, compact manner without requiring decompression at query answering
time. Moreover, it is adapted to efficiently support RDF and RDFS entailment
regimes thanks to an optimized encoding of ontology concepts and properties
that does not require a complete inference materialization or extensive query
rewriting algorithms. This approach implies to make a distinction between the
terminological and the assertional components of the knowledge base early in
the process of data preparation, i.e., preprocessing the data before storing it
in our structures. The paper describes the complete architecture of this system
and presents some preliminary results obtained from evaluations conducted on
our first prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5054</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5054</id><created>2014-01-20</created><updated>2014-06-21</updated><authors><author><keyname>Guti&#xe9;rrez</keyname><forenames>Jos&#xe9; Alberto Garc&#xed;a</forenames></author><author><keyname>D&#xed;az</keyname><forenames>Alejandro Mateo Hern&#xe1;ndez</forenames></author></authors><title>An\'alisis e implementaci\'on de algoritmos evolutivos para la
  optimizaci\'on de simulaciones en ingenier\'ia civil. (draft)</title><categories>cs.NE cs.AI</categories><comments>In Spanish. The authors acknowledge corrections, comments and
  suggestions to this paper, thanks.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the applicability of evolutionary algorithms,
particularly, the evolution strategies family in order to estimate a
degradation parameter in the shear design of reinforced concrete members. This
problem represents a great computational task and is highly relevant in the
framework of the structural engineering that for the first time is solved using
genetic algorithms.
  You are viewing a draft, the authors appreciate corrections, comments and
suggestions to this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5086</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5086</id><created>2014-01-20</created><authors><author><keyname>Ruatta</keyname><forenames>Olivier</forenames></author><author><keyname>Sciabica</keyname><forenames>Mark</forenames></author><author><keyname>Szanto</keyname><forenames>Agnes</forenames></author></authors><title>Over-constrained Weierstrass iteration and the nearest consistent system</title><categories>cs.SC math.OC</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We propose a generalization of the Weierstrass iteration for over-constrained
systems of equations and we prove that the proposed method is the Gauss-Newton
iteration to find the nearest system which has at least $k$ common roots and
which is obtained via a perturbation of prescribed structure. In the univariate
case we show the connection of our method to the optimization problem
formulated by Karmarkar and Lakshman for the nearest GCD. In the multivariate
case we generalize the expressions of Karmarkar and Lakshman, and give
explicitly several iteration functions to compute the optimum.
  The arithmetic complexity of the iterations is detailed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5092</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5092</id><created>2014-01-20</created><authors><author><keyname>Geng</keyname><forenames>Quan</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author></authors><title>Symmetric Two-User Gaussian Interference Channel with Common Messages</title><categories>cs.IT math.IT</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider symmetric two-user Gaussian interference channel with common
messages. We derive an upper bound on the sum capacity, and show that the upper
bound is tight in the low interference regime, where the optimal transmission
scheme is to send no common messages and each receiver treats interference as
noise. Our result shows that although the availability of common messages
provides a cooperation opportunity for transmitters, in the low interference
regime the presence of common messages does not help increase the sum capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5093</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5093</id><created>2014-01-20</created><updated>2015-01-03</updated><authors><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Localization and centrality in networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>5 pages, 1 figure</comments><journal-ref>Phys. Rev. E 90, 052808 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eigenvector centrality is a common measure of the importance of nodes in a
network. Here we show that under common conditions the eigenvector centrality
displays a localization transition that causes most of the weight of the
centrality to concentrate on a small number of nodes in the network. In this
regime the measure is no longer useful for distinguishing among the remaining
nodes and its efficacy as a network metric is impaired. As a remedy, we propose
an alternative centrality measure based on the nonbacktracking matrix, which
gives results closely similar to the standard eigenvector centrality in dense
networks where the latter is well behaved, but avoids localization and gives
useful results in regimes where the standard centrality fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5097</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5097</id><created>2014-01-20</created><authors><author><keyname>Fredriksson</keyname><forenames>Olle</forenames></author></authors><title>Distributed call-by-value machines</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new abstract machine, called DCESH, which describes the
execution of higher-order programs running in distributed architectures. DCESH
implements a generalised form of Remote Procedure Call that supports calling
higher-order functions across node boundaries, without sending actual code. Our
starting point is a variant of the SECD machine that we call the CES machine,
which implements reduction for untyped call-by-value PCF. We successively add
the features that we need for distributed execution and show the correctness of
each addition. First we add heaps, forming the CESH machine, which provides
features necessary for more efficient execution, and show that there is a
bisimulation between the CES and the CESH machine. Then we construct a
two-level operational semantics, where the high level is a network of
communicating machines, and the low level is given by local machine
transitions. Using these networks, we arrive at our final system, the
distributed CESH machine (DCESH). We show that there is a bisimulation relation
also between the CESH machine and the DCESH machine. All the technical results
have been formalised and proved correct in Agda, and a prototype compiler has
been developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5098</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5098</id><created>2014-01-20</created><authors><author><keyname>El-Sayed</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Abdel-Khalek</keyname><forenames>S.</forenames></author><author><keyname>Abdel-Aziz</keyname><forenames>Eman</forenames></author></authors><title>Study of Efficient Technique Based On 2D Tsallis Entropy For Image
  Thresholding</title><categories>cs.CV</categories><msc-class>68U10</msc-class><journal-ref>International Journal on Computer Science and Engineering (IJCSE),
  Vol. 3 No. 9, pp. 3125-3138, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thresholding is an important task in image processing. It is a main tool in
pattern recognition, image segmentation, edge detection and scene analysis. In
this paper, we present a new thresholding technique based on two-dimensional
Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the
twodimensional histogram which was determined by using the gray value of the
pixels and the local average gray value of the pixels, the work it was applied
a generalized entropy formalism that represents a recent development in
statistical mechanics. The effectiveness of the proposed method is demonstrated
by using examples from the real-world and synthetic images. The performance
evaluation of the proposed technique in terms of the quality of the thresholded
images are presented. Experimental results demonstrate that the proposed method
achieve better result than the Shannon method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5099</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5099</id><created>2014-01-20</created><authors><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author></authors><title>A Stable Fountain Code Mechanism for Peer-to-Peer Content Distribution</title><categories>cs.NI</categories><comments>accepted to IEEE INFOCOM 2014, 9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most peer-to-peer content distribution systems require the peers to privilege
the welfare of the overall system over greedily maximizing their own utility.
When downloading a file broken up into multiple pieces, peers are often asked
to pass on some possible download opportunities of common pieces in order to
favor rare pieces. This is to avoid the missing piece syndrome, which throttles
the download rate of the peer-to-peer system to that of downloading the file
straight from the server. In other situations, peers are asked to stay in the
system even though they have collected all the file's pieces and have an
incentive to leave right away.
  We propose a mechanism which allows peers to act greedily and yet stabilizes
the peer-to-peer content sharing system. Our mechanism combines a fountain code
at the server to generate innovative new pieces, and a prioritization for the
server to deliver pieces only to new peers. While by itself, neither the
fountain code nor the prioritization of new peers alone stabilizes the system,
we demonstrate that their combination does, through both analytical and
numerical evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5102</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5102</id><created>2014-01-20</created><authors><author><keyname>G&#xf3;mez-Cuba</keyname><forenames>Felipe</forenames></author><author><keyname>Gonz&#xe1;lez-Casta&#xf1;o</keyname><forenames>Francisco J.</forenames></author></authors><title>Improving Third-Party Relaying for LTE-A: A Realistic Simulation
  Approach</title><categories>cs.NI</categories><comments>17 one-column pages, 9 figures, accepted for publication in IEEE ICC
  2014 MWN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we propose solutions to diverse conflicts that result from
the deployment of the (still immature) relay node (RN) technology in LTE-A
networks. These conflicts and their possible solutions have been observed by
implementing standard-compliant relay functionalities on the Vienna simulator.
  As an original experimental approach, we model realistic RN operation, taking
into account that transmitters are not active all the time due to half-duplex
RN operation. We have rearranged existing elements in the simulator in a manner
that emulates RN behavior, rather than implementing a standalone brand-new
component for the simulator. We also study analytically some of the issues
observed in the interaction between the network and the RNs, to draw
conclusions beyond simulation observation.
  The main observations of this paper are that: $i$) Additional time-varying
interference management steps are needed, because the LTE-A standard employs a
fixed time division between eNB-RN and RN-UE transmissions (typical relay
capacity or throughput research models balance them optimally, which is
unrealistic nowadays); $ii$) There is a trade-off between the time-division
constraints of relaying and multi-user diversity; the stricter the constraints
on relay scheduling are, the less flexibility schedulers have to exploit
channel variation; and $iii$) Thee standard contains a variety of parameters
for relaying configuration, but not all cases of interest are covered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5107</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5107</id><created>2014-01-20</created><authors><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>B\&quot;uchi Types for Infinite Traces and Liveness</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new type and effect system based on B\&quot;uchi automata to capture
finite and infinite traces produced by programs in a small language which
allows non-deterministic choices and infinite recursions. There are two key
technical contributions: (a) an abstraction based on equivalence relations
defined by the policy B\&quot;uchi automaton, the B\&quot;uchi abstraction; (b) a novel
type and effect system to correctly capture infinite traces. We show how the
B\&quot;uchi abstraction fits into the abstract interpretation framework and show
soundness and completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5108</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5108</id><created>2014-01-20</created><authors><author><keyname>El-Sayed</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Khafagy</keyname><forenames>Mohamed A.</forenames></author></authors><title>An Identification System Using Eye Detection Based On Wavelets And
  Neural Networks</title><categories>cs.CV</categories><comments>Mohamed A. El-Sayed and Mohamed A. Khafagy, An Identification System
  Using Eye Detection Based On Wavelets And Neural Networks. International
  Journal Of Computer And Information Technology. Vol. 1, No. 2, pp. 43-48,
  2012. arXiv admin note: text overlap with arXiv:1205.5097 by other authors
  without attribution</comments><journal-ref>International Journal Of Computer And Information Technology, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The randomness and uniqueness of human eye patterns is a major breakthrough
in the search for quicker, easier and highly reliable forms of automatic human
identification. It is being used extensively in security solutions. This
includes access control to physical facilities, security systems and
information databases, Suspect tracking, surveillance and intrusion detection
and by various Intelligence agencies through out the world. We use the
advantage of human eye uniqueness to identify people and approve its validity
as a biometric. . Eye detection involves first extracting the eye from a
digital face image, and then encoding the unique patterns of the eye in such a
way that they can be compared with pre-registered eye patterns. The eye
detection system consists of an automatic segmentation system that is based on
the wavelet transform, and then the Wavelet analysis is used as a pre-processor
for a back propagation neural network with conjugate gradient learning. The
inputs to the neural network are the wavelet maxima neighborhood coefficients
of face images at a particular scale. The output of the neural network is the
classification of the input into an eye or non-eye region. An accuracy of 90%
is observed for identifying test images under different conditions included in
training stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5111</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5111</id><created>2014-01-20</created><authors><author><keyname>Stikkolorum</keyname><forenames>Dave R.</forenames></author><author><keyname>Chaudron</keyname><forenames>Michel R. V.</forenames></author><author><keyname>de Bruin</keyname><forenames>Oswald</forenames></author></authors><title>The Art of Software Design, a Video Game for Learning Software Design
  Principles</title><categories>cs.HC cs.SE</categories><comments>Winning paper gamification contest at ACM/IEEE 15th International
  Conference on Model Driven Engineering Languages and Systems (Models 2012,
  Innsbruck)</comments><acm-class>D.2.2; D.2.10; K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces our gamification of a part of our software design
curriculum. Based on typical design principles a motivating learning game is
developed to train students in software design. We use Bloom's taxonomy to
determine learning objectives. We keep the player engaged with direct feedback
in a challenging level based game with increasing complexity. Players can
evaluate their design actions with the help of the visualisation of control and
data flows. The main learning objective: applying design principles, fits the
game's main activity. This supports the learning by doing approach of
lecturers. A user test indicates possible learning effects and a playable game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5113</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5113</id><created>2014-01-20</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Retracing some paths in Process Algebra</title><categories>cs.LO</categories><comments>17 pages</comments><journal-ref>In CONCUR'96: Concurrency Theory, pp. 1-17. Springer Berlin
  Heidelberg, 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use traced monoidal categories to give a precise general version of
&quot;geometry of interaction&quot;. We give a number of examples of both
&quot;particle-style&quot; and &quot;wave-style&quot; instances of this construction. We relate
these ideas to semantics of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5124</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5124</id><created>2014-01-20</created><updated>2015-10-07</updated><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Channels with cost constraints: strong converse and dispersion</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 5, pp.
  2415-2429, May 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows the strong converse and the dispersion of memoryless
channels with cost constraints and performs refined analysis of the third order
term in the asymptotic expansion of the maximum achievable channel coding rate,
showing that it is equal to $\frac 1 2 \frac {\log n}{n}$ in most cases of
interest. The analysis is based on a non-asymptotic converse bound expressed in
terms of the distribution of a random variable termed the $\mathsf b$-tilted
information density, which plays a role similar to that of the $\mathsf
d$-tilted information in lossy source coding. We also analyze the fundamental
limits of lossy joint-source-channel coding over channels with cost
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5125</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5125</id><created>2014-01-20</created><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Nonasymptotic noisy lossy source coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows new general nonasymptotic achievability and converse bounds
and performs their dispersion analysis for the lossy compression problem in
which the compressor observes the source through a noisy channel. While this
problem is asymptotically equivalent to a noiseless lossy source coding problem
with a modified distortion function, nonasymptotically there is a noticeable
gap in how fast their minimum achievable coding rates approach the
rate-distortion function, providing yet another example in which at finite
blocklengths one must put aside traditional asymptotic thinking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5136</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5136</id><created>2014-01-20</created><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>A Unifying Framework for Typical Multi-Task Multiple Kernel Learning
  Problems</title><categories>cs.LG</categories><comments>17 pages, 1 figure. Accepted by IEEE Transactions on Neural Networks
  and Learning Systems; currently published as Early Access Article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, Multi-Kernel Learning (MKL) has received significant
attention among data-driven feature selection techniques in the context of
kernel-based learning. MKL formulations have been devised and solved for a
broad spectrum of machine learning problems, including Multi-Task Learning
(MTL). Solving different MKL formulations usually involves designing algorithms
that are tailored to the problem at hand, which is, typically, a non-trivial
accomplishment.
  In this paper we present a general Multi-Task Multi-Kernel Learning
(Multi-Task MKL) framework that subsumes well-known Multi-Task MKL
formulations, as well as several important MKL approaches on single-task
problems. We then derive a simple algorithm that can solve the unifying
framework. To demonstrate the flexibility of the proposed framework, we
formulate a new learning problem, namely Partially-Shared Common Space (PSCS)
Multi-Task MKL, and demonstrate its merits through experimentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5143</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5143</id><created>2014-01-20</created><updated>2014-01-29</updated><authors><author><keyname>Maruyama</keyname><forenames>Shirou</forenames></author><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author></authors><title>Fully Online Grammar Compression in Constant Space</title><categories>cs.DS</categories><comments>This is an extended version of a proceeding accepted to Data
  Compression Conference (DCC), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel variants of fully online LCA (FOLCA), a fully online grammar
compression that builds a straight line program (SLP) and directly encodes it
into a succinct representation in an online manner. FOLCA enables a direct
encoding of an SLP into a succinct representation that is asymptotically
equivalent to an information theoretic lower bound for representing an SLP
(Maruyama et al., SPIRE'13). The compression of FOLCA takes linear time
proportional to the length of an input text and its working space depends only
on the size of the SLP, which enables us to apply FOLCA to large-scale
repetitive texts. Recent repetitive texts, however, include some noise. For
example, current sequencing technology has significant error rates, which
embeds noise into genome sequences. For such noisy repetitive texts, FOLCA
working in the SLP size consumes a large amount of memory. We present two
variants of FOLCA working in constant space by leveraging the idea behind
stream mining techniques. Experiments using 100 human genomes corresponding to
about 300GB from the 1000 human genomes project revealed the applicability of
our method to large-scale, noisy repetitive texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5148</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5148</id><created>2014-01-20</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>Solving Cubic Equations By the Quadratic Formula</title><categories>cs.NA math.NA</categories><comments>6 pages, 2 figures</comments><msc-class>65Y20, 65D18, 65D99, 65D99, 97A30</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p(z)$ be a monic cubic complex polynomial with distinct roots and
distinct critical points. We say a critical point has the {\it Voronoi
property} if it lies in the Voronoi cell of a root $\theta$, $V(\theta)$, i.e.
the set of points that are closer to $\theta$ than to the other roots. We prove
at least one critical point has the Voronoi property and characterize the cases
when both satisfy this property. It is known that for any $\xi \in V(\theta)$,
the sequence $B_m(\xi) =\xi - p(\xi) d_{m-2}/d_{m-1}$ converges to $\theta$,
where $d_m$ satisfies the recurrence $d_m =p'(\xi)d_{m-1}-0.5
p(\xi)p''(\xi)d_{m-2} +p^2(\xi)d_{m-3}$, $d_0 =1, d_{-1}=d_{-2}=0$. Thus by the
Voronoi property, there is a solution $c$ of $p'(z)=0$ where $B_m(c)$ converges
to a root of $p(z)$. The speed of convergence is dependent on the ratio of the
distances between $c$ and the closest and the second closest roots of $p(z)$.
This results in a different algorithm for solving a cubic equation than the
classical methods. We give polynomiography for an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5151</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5151</id><created>2014-01-20</created><updated>2014-07-06</updated><authors><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Vehkapera</keyname><forenames>Mikko</forenames></author></authors><title>Signal recovery using expectation consistent approximation for linear
  observations</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>5 pages, 1 figure</comments><journal-ref>ISIT 2014 proceedings pp. 226-230 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A signal recovery scheme is developed for linear observation systems based on
expectation consistent (EC) mean field approximation. Approximate message
passing (AMP) is known to be consistent with the results obtained using the
replica theory, which is supposed to be exact in the large system limit, when
each entry of the observation matrix is independently generated from an
identical distribution. However, this is not necessarily the case for general
matrices. We show that EC recovery exhibits consistency with the replica theory
for a wider class of random observation matrices. This is numerically confirmed
by experiments for the Bayesian optimal signal recovery of compressed sensing
using random row-orthogonal matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5155</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5155</id><created>2014-01-20</created><authors><author><keyname>Shayan</keyname><forenames>Jafar</forenames></author><author><keyname>Azarnik</keyname><forenames>Ahmad</forenames></author><author><keyname>Chuprat</keyname><forenames>Suriayati</forenames></author><author><keyname>Karamizadeh</keyname><forenames>Sasan</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mojtaba</forenames></author></authors><title>Identifying Benefits and risks associated with utilizing cloud computing</title><categories>cs.DC cs.CY</categories><doi>10.7321/jscse.v3.n3.63</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is an emerging computing model where IT and computing
operations are delivered as services in highly scalable and cost effective
manner. Recently, embarking this new model in business has become popular.
Companies in diverse sectors intend to leverage cloud computing architecture,
platforms and applications in order to gain higher competitive advantages.
Likewise other models, cloud computing brought advantages to attract business
but meanwhile fostering cloud has led to some risks, which can cause major
impacts if business does not plan for mitigation. This paper surveys the
advantages of cloud computing and in contrast the risks associated using them.
Finally we conclude that a well-defined risk management program that focused on
cloud computing is an essential part of gaining value from benefits of cloud
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5156</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5156</id><created>2014-01-20</created><authors><author><keyname>Wahid</keyname><forenames>Juliana</forenames></author><author><keyname>Hussin</keyname><forenames>Naimah Mohd</forenames></author></authors><title>Harmony Search Algorithm for Curriculum-Based Course Timetabling Problem</title><categories>cs.AI</categories><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 365-371, 2013</journal-ref><doi>10.7321/jscse.v3.n3.55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, harmony search algorithm is applied to curriculum-based course
timetabling. The implementation, specifically the process of improvisation
consists of memory consideration, random consideration and pitch adjustment. In
memory consideration, the value of the course number for new solution was
selected from all other course number located in the same column of the Harmony
Memory. This research used the highest occurrence of the course number to be
scheduled in a new harmony. The remaining courses that have not been scheduled
by memory consideration will go through random consideration, i.e. will select
any feasible location available to be scheduled in the new harmony solution.
Each course scheduled out of memory consideration is examined as to whether it
should be pitch adjusted with probability of eight procedures. However, the
algorithm produced results that were not comparatively better than those
previously known as best solution. With proper modification in terms of the
approach in this algorithm would make the algorithm perform better on
curriculum-based course timetabling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5157</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5157</id><created>2014-01-20</created><authors><author><keyname>Maeda</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Fujii</keyname><forenames>Masanori</forenames></author><author><keyname>Hayashi</keyname><forenames>Isao</forenames></author></authors><title>Skill Analysis with Time Series Image Data</title><categories>cs.AI</categories><comments>5 pages, 6 figures</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 576-580, 2013</journal-ref><doi>10.7321/jscse.v3.n3.87</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a skill analysis with time series image data using data mining
methods, focused on table tennis. We do not use body model, but use only
hi-speed movies, from which time series data are obtained and analyzed using
data mining methods such as C4.5 and so on. We identify internal models for
technical skills as evaluation skillfulness for the forehand stroke of table
tennis, and discuss mono and meta-functional skills for improving skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5162</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5162</id><created>2014-01-20</created><authors><author><keyname>Ulapane</keyname><forenames>Nalika</forenames></author><author><keyname>Abeyratne</keyname><forenames>Sunil</forenames></author><author><keyname>Binduhewa</keyname><forenames>Prabath</forenames></author><author><keyname>Dhanapala</keyname><forenames>Chamari</forenames></author><author><keyname>Wickramasinghe</keyname><forenames>Shyama</forenames></author><author><keyname>Rathnayake</keyname><forenames>Nimal</forenames></author></authors><title>A Simple Software Application for Simulating Commercially Available
  Solar Panels</title><categories>cs.CE</categories><comments>21 pages, 8 figures, 2 tables</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE) e-ISSN: 2251-7545 Vol.2, No.5, 2012</journal-ref><doi>10.7321/jscse.v2.n5.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the formulation and validation of a simple PC based
software application developed for simulating commercially available solar
panels. The important feature of this application is its capability to produce
speedy results in the form of solar panel output characteristics at given
environmental conditions by using minimal input data. Besides, it is able to
deliver critical information about the maximum power point of the panel at a
given environmental condition in quick succession. The application is based on
a standard equation which governs solar panels and works by means of estimating
unknown parameters in the equation to fit a given solar panel. The process of
parameter estimation is described in detail with the aid of equations and data
of a commercial solar panel. A validation of obtained results for commercial
solar panels is also presented by comparing the panel manufacturers' results
with the results generated by the application. In addition, implications of the
obtained results are discussed along with possible improvements to the
developed software application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5163</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5163</id><created>2014-01-20</created><authors><author><keyname>Silva</keyname><forenames>Alexandre M Melo</forenames></author><author><keyname>Maciel</keyname><forenames>Christiano C</forenames></author><author><keyname>Correa</keyname><forenames>Suelene do Carmo</forenames></author></authors><title>Multi-hop Energy-efficient Control for Heterogeneous Wireless Sensor
  Networks Using Fuzzy Logic</title><categories>cs.NI</categories><comments>2013 JSCSE</comments><report-no>SCSE'13 2013</report-no><journal-ref>JSCSE Vol. 3, No. 3, 2013</journal-ref><doi>10.7321/jscse.v3.n3.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSN) have severe energy constraints imposed by
limited capacity of the internal battery of sensor nodes. These restrictions
stimulate the development of energy-efficient strategies aimed at increasing
the period of stability and lifetime of these networks. In this paper, we
propose a centralized control to elect more appropriate Cluster Heads, assuming
three levels of heterogeneity and multi-hop communication between Cluster
Heads. The centralized control uses the k-means algorithm, responsible for the
division of clusters and Fuzzy Logic to elect the Cluster Head and selecting
the best route of communication. The study results indicate that the proposed
approach can increase the period of stability and lifetime in WSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5165</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5165</id><created>2014-01-20</created><authors><author><keyname>Suresh</keyname><forenames>Yeresime</forenames></author><author><keyname>Rath</keyname><forenames>Santanu Ku.</forenames></author></authors><title>A Genetic Algorithm based Approach for Test Data Generation in Basis
  Path Testing</title><categories>cs.SE</categories><comments>7 pages, 5 figures</comments><doi>10.7321/jscse.v3.n3.49</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Software Testing is a process to identify the quality and reliability of
software, which can be achieved through the help of proper test data. However,
doing this manually is a difficult task due to the presence of number of
predicate nodes in the module. So, this leads towards a problem of NP-complete.
Therefore some intelligence-based search algorithms have to be used to generate
test data. In this paper, we use a soft computing based approach, genetic
algorithm to generate test data based on the set of basis paths. This paper
combines the characteristics of genetic algorithm with test data, making use of
the merits of respective global and local optimization capability to improve
the generation capacity of test data. This automated process of generating test
data optimally helps in reducing the test effort and time of a tester. Finally,
the proposed approach is applied for ATM withdrawal task. Experimental results
show that genetic algorithm was able to generate suitable test data based on a
fitness value and avoid redundant data by optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5168</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5168</id><created>2014-01-20</created><authors><author><keyname>Lu</keyname><forenames>Jiyong</forenames></author><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Distributed Storage Schemes over Unidirectional Ring Networks</title><categories>cs.IT math.IT</categories><comments>two columns, 5 pages, 8 figures, and submitted to the ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study distributed storage problems over unidirectional ring
networks. A lower bound on the reconstructing bandwidth to recover total
original data for each user is proposed, and it is achievable for arbitrary
parameters. If a distributed storage scheme can achieve this lower bound with
equality for each user, we say it an optimal reconstructing distributed storage
scheme (ORDSS). Furthermore, the repair problem for a failed storage node in
ORDSSes is under consideration and a tight lower bound on the repair bandwidth
for each storage node is obtained. Particularly, we indicate the fact that for
any ORDSS, every storage node can be repaired with repair bandwidth achieving
the lower bound with equality. In addition, we present an efficient approach to
construct ORDSSes for arbitrary parameters by using the concept of Euclidean
division. Finally, we take an example to characterize the above approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5174</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5174</id><created>2014-01-20</created><authors><author><keyname>Li</keyname><forenames>Zhi</forenames></author><author><keyname>Begen</keyname><forenames>Ali C.</forenames></author><author><keyname>Gahm</keyname><forenames>Joshua</forenames></author><author><keyname>Shan</keyname><forenames>Yufeng</forenames></author><author><keyname>Osler</keyname><forenames>Bruce</forenames></author><author><keyname>Oran</keyname><forenames>David</forenames></author></authors><title>Streaming Video over HTTP with Consistent Quality</title><categories>cs.NI cs.MM</categories><comments>Refined version submitted to ACM Multimedia Systems Conference
  (MMSys), 2014</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conventional HTTP-based adaptive streaming (HAS), a video source is
encoded at multiple levels of constant bitrate representations, and a client
makes its representation selections according to the measured network
bandwidth. While greatly simplifying adaptation to the varying network
conditions, this strategy is not the best for optimizing the video quality
experienced by end users. Quality fluctuation can be reduced if the natural
variability of video content is taken into consideration. In this work, we
study the design of a client rate adaptation algorithm to yield consistent
video quality. We assume that clients have visibility into incoming video
within a finite horizon. We also take advantage of the client-side video
buffer, by using it as a breathing room for not only network bandwidth
variability, but also video bitrate variability. The challenge, however, lies
in how to balance these two variabilities to yield consistent video quality
without risking a buffer underrun. We propose an optimization solution that
uses an online algorithm to adapt the video bitrate step-by-step, while
applying dynamic programming at each step. We incorporate our solution into
PANDA -- a practical rate adaptation algorithm designed for HAS deployment at
scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5175</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5175</id><created>2014-01-20</created><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author></authors><title>Supporting MOOC Instruction with Social Network Analysis</title><categories>cs.SI</categories><comments>10 pages, 4 figures, 3 tables</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an expansive and ubiquitously available gold mine of educational data,
Massive Open Online courses (MOOCs) have become the an important foci of
learning analytics research. In this paper, we investigate potential reasons as
to why are these digitalized learning repositories being plagued with huge
attrition rates. We analyze an ongoing online course offered in Coursera using
a social network perspective, with an objective to identify students who are
actively participating in course discussions and those who are potentially at a
risk of dropping off. We additionally perform extensive forum analysis to
visualize student's posting patterns longitudinally. Our results provide
insights that can assist educational designers in establishing a pedagogical
basis for decision-making while designing MOOCs. We infer prominent
characteristics about the participation patterns of distinct groups of students
in the networked learning community, and effectively discover important
discussion threads. These methods can, despite the otherwise prohibitive number
of students involved, allow an instructor to leverage forum behavior to
identify opportunities for support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5178</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5178</id><created>2014-01-21</created><authors><author><keyname>Asgher</keyname><forenames>Umer</forenames></author><author><keyname>Dar</keyname><forenames>Fahad Moazzam</forenames></author><author><keyname>Hamza</keyname><forenames>Ali</forenames></author><author><keyname>Paracha</keyname><forenames>Abdul Moeed</forenames></author></authors><title>Analysis of Increasing Malwares and Cyber Crimes Using Economic Approach</title><categories>cs.CY</categories><comments>5 pages, Special Issue The Proceeding of International Conference on
  Soft Computing and Software Engineering 2013 [SCSE 13], San Francisco State
  University, CA, U.S.A., March 2013</comments><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE],2013, Vol. 3, No. 3, e-ISSN: 2251-7545</journal-ref><doi>10.7321/jscse.v3.n3.74</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The economics of an internet crime has newly developed into a field of
controlling black money. This economic approach not only provides estimated
technique of analyzing internet crimes but also gives details to analyzers of
system dependability and divergence. This paper will highlight on the subject
of online crime, which has formed its industry since. It all started from
amateur hackers who cracked websites and wrote malicious software in pursuit of
fun or achieving limited objectives to professional hacking. In the past days,
electronic fraud was main objective but now it has been changed into electronic
hacking. This study focuses the issue through an economic analysis of available
web forum to deals in malware and private information. The findings of this
survey research provide considerable in-depth sight into the functions of
malware economy spinning around computer impositions and compromise. In this
regard, the survey research paper may benefit particularly computer security
officials, the law enforcement agencies, and in general prospective anyone
involved in better understanding cybercrime from the offender standpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5181</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5181</id><created>2014-01-21</created><authors><author><keyname>Dar</keyname><forenames>Fahad Moazzam</forenames></author><author><keyname>Asgher</keyname><forenames>Umer</forenames></author><author><keyname>Malik</keyname><forenames>Daniyal</forenames></author><author><keyname>Adil</keyname><forenames>Emmad</forenames></author><author><keyname>Shahzad</keyname><forenames>Hassan</forenames></author><author><keyname>Ali</keyname><forenames>Anees</forenames></author></authors><title>Automation of Prosthetic Upper Limbs for Transhumeral Amputees Using
  Switch-controlled Motors</title><categories>cs.HC cs.RO</categories><comments>7 Pages, The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, Special Issue The Proceeding of
  International Conference on Soft Computing and Software Engineering 2013
  [SCSE 13], San Francisco State University, CA, U.S.A., March 2013</comments><report-no>e-ISSN: 2251-7545</report-no><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3, 2013. e-ISSN: 2251-7545</journal-ref><doi>10.7321/jscse.v3.n3.90</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issues of research required in the field of bio medical engineering and
externally-powered prostheses are attracting attention of regulatory bodies and
the common people in various parts of the globe. Today, 90 percent of
prostheses used are conventional body powered cable-controlled ones which are
very uncomfortable to the amputees as fairly large amount of forces and
excursions have to be generated by the amputee. Additionally, its amount of
rotation is limited. Alternatively, prosthetic limbs driven using electrical
motors might deliver added functionality and improved control, accompanied by
better cosmesis, however,it could be bulky and costly. Presently existing
proposals usually require fewer bodily response and need additional upkeep than
the cable operated prosthetic limbs. Due to the motives mentioned, proposal for
mechanization of body-powered prostheses, with ease of maintenance and cost in
mind, is presented in this paper. The prosthetic upper limb which is being
automated is for Transhumeral type of amputees that is amputated from above
elbow. The study consists of two main portions: one is lifting mechanism of the
limb and the other is gripping mechanism for the hand using switch controls,
which is the most cost effective and optimized solution, rather than using
complex and expensive myoelectric control signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5183</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5183</id><created>2014-01-21</created><authors><author><keyname>Al-Jamimi</keyname><forenames>Hamdi A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Moataz</forenames></author></authors><title>Transition from Analysis to Software Design: A Review and New
  Perspective</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Analysis and design phases are the most crucial part of the software
development life-cycle. Reusing the artifacts of these early phases is very
beneficial to improve the productivity and software quality. In this paper we
analyze the literature on the automatic transformation of artifacts from the
problem space (i.e., requirement analysis models) into artifacts in the
solution space (i.e., architecture, design and implementation code). The goal
is to assess the current state of the art with regard to the ability of
automatically reusing previously developed software designs in synthesizing a
new design for a given requirement. We surveyed various related areas such as
model-driven development and model transformation techniques. Our analysis
revealed that this topic has not been satisfactorily covered yet. Accordingly,
we propose a framework consists of three stages to address uncovered
limitations in current approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5187</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5187</id><created>2014-01-21</created><authors><author><keyname>Weinstein</keyname><forenames>Asaf</forenames></author><author><keyname>Weinstein</keyname><forenames>Ehud</forenames></author></authors><title>Inequalities for the Bayes Risk</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several inequalities are presented which, in part, generalize inequalities by
Weinstein and Weiss, giving rise to new lower bounds for the Bayes risk under
squared error loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5191</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5191</id><created>2014-01-21</created><authors><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Goal-Oriented Setup and Usage of Custom-Tailored Software Cockpits</title><categories>cs.SE</categories><comments>15 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-69566-0_4</comments><journal-ref>Product-Focused Software Process Improvement, volume 5089 of
  Lecture Notes in Computer Science, pages 4-18. Springer Berlin Heidelberg,
  2008</journal-ref><doi>10.1007/978-3-540-69566-0_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Cockpits, also known as Software Project Control Centers, support
the management and controlling of software and system development projects and
provide means for quantitative measurement-based project control. Currently,
many companies are developing simple control dashboards that are mainly based
on Spreadsheet applications. Alternatively, they use solutions providing a
fixed set of project control functionality that cannot be sufficiently
customized to their specific needs and goals. Specula is a systematic approach
for defining reusable, customizable control components and instantiate them
according to different organizational goals and characteristics based on the
Quality Improvement Paradigm (QIP) and GQM. This article gives an overview of
the Specula approach, including the basic conceptual model, goal-oriented
measurement, and the composition of control components based on explicitly
stated measurement goals. Related approaches are discussed and the use of
Specula as part of industrial case studies is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5194</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5194</id><created>2014-01-21</created><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author><author><keyname>Elkouss</keyname><forenames>David</forenames></author></authors><title>Fundamental Finite Key Limits for Information Reconciliation in Quantum
  Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Proc. IEEE ISIT 2014, p. 1469 - 1473 (Honolulu, USA, 2014)</journal-ref><doi>10.1109/ISIT.2014.6875077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of quantum key distribution protocols is guaranteed by the laws
of quantum mechanics. However, a precise analysis of the security properties
requires tools from both classical cryptography and information theory. Here,
we employ recent results in non-asymptotic classical information theory to show
that information reconciliation imposes fundamental limitations on the amount
of secret key that can be extracted in the finite key regime. In particular, we
find that an often used approximation for the information leakage during
information reconciliation is flawed and we propose an improved estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5197</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5197</id><created>2014-01-21</created><updated>2014-04-15</updated><authors><author><keyname>Wang</keyname><forenames>Shenghao</forenames></author><author><keyname>Zhang</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Zhili</forenames></author><author><keyname>Gao</keyname><forenames>Kun</forenames></author><author><keyname>Wu</keyname><forenames>Zhao</forenames></author><author><keyname>Zhu</keyname><forenames>Peiping</forenames></author><author><keyname>Wu</keyname><forenames>Ziyu</forenames></author></authors><title>A user-friendly nano-CT image alignment and 3D reconstruction platform
  based on LabVIEW</title><categories>cs.CE physics.med-ph</categories><comments>9 pages, 5 figures, 1 chart</comments><acm-class>I.4.0; I.4.6; D.2.6</acm-class><journal-ref>2015 Chinese Physics C, 39 (1): 018001</journal-ref><doi>10.1088/1674-1137/39/1/018001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  X-ray computed tomography at the nanometer scale (nano-CT) offers a wide
range of applications in scientific and industrial areas. Here we describe a
reliable, user-friendly and fast software package based on LabVIEW that may
allow to perform all procedures after the acquisition of raw projection images
in order to obtain the inner structure of the investigated sample. A suitable
image alignment process to address misalignment problems among image series due
to mechanical manufacturing errors, thermal expansion and other external
factors has been considered together with a novel fast parallel beam 3D
reconstruction procedure, developed ad hoc to perform the tomographic
reconstruction. Remarkably improved reconstruction results obtained at the
Beijing Synchrotron Radiation Facility after the image calibration confirmed
the fundamental role of this image alignment procedure that minimizes unwanted
blurs and additional streaking artifacts always present in reconstructed
slices. Moreover, this nano-CT image alignment and its associated 3D
reconstruction procedure fully based on LabVIEW routines, significantly reduce
the data post-processing cycle, thus making faster and easier the activity of
the users during experimental runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5198</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5198</id><created>2014-01-21</created><authors><author><keyname>Ahmed</keyname><forenames>Kushal</forenames></author><author><keyname>Myers</keyname><forenames>Toby</forenames></author><author><keyname>Wen</keyname><forenames>Lian</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>Detecting Requirements Defects Utilizing A Mathematical Framework for
  Behavior Engineering</title><categories>cs.SE</categories><comments>12 pages</comments><journal-ref>The International Journal of Soft Computing and Software
  Engineering [JSCSE], Vol. 3, No. 3. 2013
  (http://www.jscse.com/papers/vol3.no3/vol3.no3.29.pdf)</journal-ref><doi>10.7321/jscse.v3.n3.29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavior Engineering (BE) provides a rigorous way to derive a formal
specification of a software system from the requirements written in natural
language. Its graphical specification language, Behavior Tree (BT), has been
used with success in industry to systematically translate large, complex, and
often erroneous requirements into an integrated model of the software system.
BE's process, the Behavior Modeling Process (BMP), allows requirements to be
translated into individual requirement BTs one at a time, which are then
integrated to form a holistic view of the system. The integrated BT then goes
through a series of modifications to construct a specification BT, which is
used for validation and verification. The BMP also addresses different types of
defects in the requirements throughout its process. However, BT itself is a
graphical modeling notation, and the types of integration relations, how they
correspond to particular issues, how they should be integrated and how to get
formal specification have not been clearly defined. As a result, the BMP is
informal, and provides guidelines to perform all these tasks on an ad-hoc
basis. In this paper, we first introduce a mathematical framework which defines
the graphical form of BTs which we use to define the integration relationships
of BTs and to formalize the integration strategy of the BMP. We then formulate
semi-automated requirements defects detection techniques by utilizing this
underlying mathematical framework, which may be extended to formalize the BMP,
develop change management framework for it, build techniques for round-trip
engineering and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5200</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5200</id><created>2014-01-21</created><updated>2014-05-31</updated><authors><author><keyname>Abbas</keyname><forenames>Houssam</forenames></author><author><keyname>Hoxha</keyname><forenames>Bardh</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios</forenames></author><author><keyname>Deshmukh</keyname><forenames>Jyotirmoy V.</forenames></author><author><keyname>Kapinski</keyname><forenames>James</forenames></author><author><keyname>Ueda</keyname><forenames>Koichi</forenames></author></authors><title>Conformance Testing as Falsification for Cyber-Physical Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Model-Based Design of Cyber-Physical Systems (CPS), it is often desirable
to develop several models of varying fidelity. Models of different fidelity
levels can enable mathematical analysis of the model, control synthesis, faster
simulation etc. Furthermore, when (automatically or manually) transitioning
from a model to its implementation on an actual computational platform, then
again two different versions of the same system are being developed. In all
previous cases, it is necessary to define a rigorous notion of conformance
between different models and between models and their implementations. This
paper argues that conformance should be a measure of distance between systems.
Albeit a range of theoretical distance notions exists, a way to compute such
distances for industrial size systems and models has not been proposed yet.
This paper addresses exactly this problem. A universal notion of conformance as
closeness between systems is rigorously defined, and evidence is presented that
this implies a number of other application-dependent conformance notions. An
algorithm for detecting that two systems are not conformant is then proposed,
which uses existing proven tools. A method is also proposed to measure the
degree of conformance between two systems. The results are demonstrated on a
range of models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5208</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5208</id><created>2014-01-21</created><authors><author><keyname>Guo</keyname><forenames>Chen</forenames></author><author><keyname>Zhu</keyname><forenames>Cenzhe</forenames></author><author><keyname>Tay</keyname><forenames>Teng Tiow</forenames></author></authors><title>ShAppliT: A Novel Broker-mediated Solution to Generic Application
  Sharing in a Cluster of Closed Operating Systems</title><categories>cs.DC</categories><comments>17 pages</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  Vol. 2, No. 6, 2012</journal-ref><doi>10.7321/jscse.v2.n6.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With advances in hardware and networking technologies and mass manufacturing,
the cost of high end hardware had fall dramatically in recent years. However,
software cost still remains high and is the dominant fraction of the overall
computing budget. Application sharing is a promising solution to reduce the
overall IT cost. Currently software licenses are still based on the number of
copies installed. An organization can thus reduce the IT cost if the users are
able to remotely access the software that is installed on certain computer
servers instead of running the software on every local computer. In this paper,
we propose a generic application sharing architecture for users' application
sharing in a cluster of closed operating systems such as Microsoft Windows. We
also propose a broker-mediated solution where we allow multiple users to access
a single user software license on a time multiplex basis through a single
logged in user. An application sharing tool called ShAppliT has been introduced
and implemented in Microsoft Windows operating system. We evaluated their
performance on CPU usage and memory consumption when a computer is hosting
multiple concurrent shared application sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5209</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5209</id><created>2014-01-21</created><authors><author><keyname>Tissera</keyname><forenames>Pablo Cristian</forenames><affiliation>LIDIC-Departamento de Inform&#xe1;tica-Universidad Nacional de San Luis Argentina</affiliation></author><author><keyname>Castro</keyname><forenames>Alicia</forenames><affiliation>LIDIC-Departamento de Inform&#xe1;tica-Universidad Nacional de San Luis Argentina</affiliation></author><author><keyname>Printista</keyname><forenames>A. Marcela</forenames><affiliation>LIDIC-Departamento de Inform&#xe1;tica-Universidad Nacional de San Luis Argentina</affiliation></author><author><keyname>Luque</keyname><forenames>Emilio</forenames><affiliation>Departamento de Arquitectura y Sistemas Operativos - Universidad Aut&#xf3;noma de Barcelona</affiliation></author></authors><title>Simulating Behaviours to face up an Emergency Evacuation</title><categories>cs.OH</categories><report-no>431</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer based models describing pedestrian behavior in an emergency
evacuation play a vital role in the development of active strategies that
minimize the evacuation time when a closed area must be evacuated. The
reference model has a hybrid structure where the dynamics of fire and smoke
propagation are modeled by means of Cellular Automata and for simulating
people's behavior we are using Intelligent Agents. The model consists of two
sub-models, called environmental and pedestrian ones. As part of the pedestrian
model, this paper concentrates in a methodology that is able to model some of
the frequently observed human's behaviors in evacuation exercises. Each agent
will perceive what is happening around, select the options that exist in that
context and then it makes a decision that will reflect its ability to cope with
an emergency evacuation, called in this work, behavior. We also developed
simple exercises where the model is applied to the simulation of an evacuation
due to a potential hazard, such as fire, smoke or some kind of collapse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5216</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5216</id><created>2014-01-21</created><authors><author><keyname>Karpi&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Pacut</keyname><forenames>Maciej</forenames></author></authors><title>Multi-GPU parallel memetic algorithm for capacitated vehicle routing
  problem</title><categories>cs.DC cs.NE</categories><comments>12 pages, 2 figures, Extended version of paper accepted for
  publication in proceedings of PPAM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to propose and test a new memetic algorithm for the
capacitated vehicle routing problem in parallel computing environment. In this
paper we consider simple variation of vehicle routing problem in which the only
parameter is the capacity of the vehicle and each client only needs one
package. We present simple reduction to prove the existence of polynomial-time
algorithm for capacity 2. We analyze the efficiency of the algorithm using
hierarchical Parallel Random Access Machine (PRAM) model and run experiments
with code written in CUDA (for capacities larger than 2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5221</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5221</id><created>2014-01-21</created><authors><author><keyname>kasiri</keyname><forenames>Hadi</forenames></author><author><keyname>momeni</keyname><forenames>hamid reza</forenames></author><author><keyname>Kasiri</keyname><forenames>Atiyeh</forenames></author></authors><title>Optimal Intelligent Control for Wind Turbulence Rejection in WECS Using
  ANNs and Genetic Fuzzy Approach</title><categories>cs.SY cs.NE</categories><comments>International journal of soft computing &amp; soft engineering 2012</comments><doi>10.7321/jscse.v2.n.9.2</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the disadvantages in Connection of wind energy conversion systems
(WECSs) to transmission networks is plentiful turbulence of wind speed.
Therefore effects of this problem must be controlled. Nowadays,
pitch-controlled WECSs are increasingly used for variable speed and pitch wind
turbines. Megawatt class wind turbines generally turn at variable speed in wind
farm. Thus turbine operation must be controlled in order to maximize the
conversion efficiency below rated power and reduce loading on the drive-train.
Due to random and non-linear nature of the wind turbulence and the ability of
Multi-Layer Perceptron (MLP) and Radial Basis Function (RBF) Artificial Neural
Networks (ANNs) in the modeling and control of this turbulence, in this study,
widespread changes of wind have been perused using MLP and RBF artificial NNs.
In addition in this study, a new genetic fuzzy system has been successfully
applied to identify disturbance wind in turbine input. Thus output power has
been regulated in optimal and nominal range by pitch angle regulation.
Consequently, our proposed approaches have regulated output aerodynamic power
and torque in the nominal rang.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5224</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5224</id><created>2014-01-21</created><authors><author><keyname>Giannoccaro</keyname><forenames>Nicola Ivan</forenames></author><author><keyname>Indiveri</keyname><forenames>Giovanni</forenames></author><author><keyname>Spedicato</keyname><forenames>Luigi</forenames></author></authors><title>Least Entropy-Like Approach for Reconstructing L-Shaped Surfaces Using a
  Rotating Array of Ultrasonic Sensors</title><categories>cs.RO</categories><comments>15 pages, 15 figures</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE) e-ISSN: 2251-7545 Vol.2,No.6, 2012 Published online: Jun 6, 2012</journal-ref><doi>10.7321/jscse.v2.n6.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new algorithm for accurately reconstructing two
smooth orthogonal surfaces by processing ultrasonic data. The proposed
technique is based on a preliminary analysis of a waveform energy indicator in
order to classify the data as belonging to one of the two flat surfaces. The
following minimization of a nonlinear cost function, inspired by the
mathematical definition of Gibbs entropy, allows to estimate the plane
parameters robustly with respect to the presence of outlying data. These
outliers are mainly due to the effect of multiple reflections arising in the
surfaces intersection region. The scanning system consists of four inexpensive
ultrasonic sensors rotated by means of a precision servo digital motor in order
to obtain distance measurements for each orientation. Experimental results are
presented and compared with the classic Least Squares Method demonstrating the
potentiality of the proposed approach in terms of precision and reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5226</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5226</id><created>2014-01-21</created><updated>2014-03-07</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author></authors><title>The Why and How of Nonnegative Matrix Factorization</title><categories>stat.ML cs.IR cs.LG math.OC</categories><comments>25 pages, 5 figures. Some typos and errors corrected, Section 3.2
  reorganized</comments><report-no>In: &quot;Regularization, Optimization, Kernels, and Support Vector
  Machines&quot;, J.A.K. Suykens, M. Signoretto and A. Argyriou (eds), Chapman &amp;
  Hall/CRC, Machine Learning and Pattern Recognition Series, pp. 257-291, 2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) has become a widely used tool for the
analysis of high-dimensional data as it automatically extracts sparse and
meaningful features from a set of nonnegative data vectors. We first illustrate
this property of NMF on three applications, in image processing, text mining
and hyperspectral imaging --this is the why. Then we address the problem of
solving NMF, which is NP-hard in general. We review some standard NMF
algorithms, and also present a recent subclass of NMF problems, referred to as
near-separable NMF, that can be solved efficiently (that is, in polynomial
time), even in the presence of noise --this is the how. Finally, we briefly
describe some problems in mathematics and computer science closely related to
NMF via the nonnegative rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5232</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5232</id><created>2014-01-21</created><authors><author><keyname>Dermitzakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Carbajal</keyname><forenames>Juan Pablo</forenames></author></authors><title>Bio-inspired friction switches: adaptive pulley systems</title><categories>cs.RO</categories><comments>Conference. First submission, before reviews</comments><journal-ref>Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International
  Conference on , vol., no., pp.947,952, 3-7 Nov. 2013</journal-ref><doi>10.1109/IROS.2013.6696464</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frictional influences in tendon-driven robotic systems are generally
unwanted, with efforts towards minimizing them where possible. In the human
hand however, the tendon-pulley system is found to be frictional with a
difference between high-loaded static post-eccentric and post-concentric force
production of 9-12% of the total output force. This difference can be directly
attributed to tendon-pulley friction. Exploiting this phenomenon for robotic
and prosthetic applications we can achieve a reduction of actuator size, weight
and consequently energy consumption. In this study, we present the design of a
bio-inspired friction switch. The adaptive pulley is designed to minimize the
influence of frictional forces under low and medium-loading conditions and
maximize it under high-loading conditions. This is achieved with a
dual-material system that consists of a high-friction silicone substrate and
low-friction polished steel pins. The system, designed to switch its frictional
properties between the low-loaded and high-loaded conditions, is described and
its behavior experimentally validated with respect to the number and spacing of
pins. The results validate its intended behavior, making it a viable choice for
robotic tendon-driven systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5234</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5234</id><created>2014-01-21</created><authors><author><keyname>Leducq</keyname><forenames>Elodie</forenames></author></authors><title>On the third weight of generalized Reed-Muller codes</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the third weight of generalized Reed-Muller codes. We
prove under some restrictive condition that the third weight of generalized
Reed-Muller codes depends on the third weight of generalized Reed-Muller codes
of small order with two variables. In some cases, we are able to determine the
third weight and the third weight codewords of generalized Reed-Muller codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5236</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5236</id><created>2014-01-21</created><authors><author><keyname>Gunduz</keyname><forenames>Semseddin</forenames></author><author><keyname>Namlu</keyname><forenames>Aysen</forenames></author></authors><title>The Effect Of Online Cooperative Homework On Students' Academic Success</title><categories>cs.CY</categories><comments>The International Conference on Soft Computing and Software
  Engineering (SCSE'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the effect of online cooperative learning homework practices
on academic success of students is searched. The experience group of the
research consists of 58 students from Anadolu University Education Faculty
Education of Computer and Instruction Technology Section. Students in A section
are taken to traditional method by neutral appointment; those in B section are
taken to online homework practice method. In each class consisting of 29
people, it's decided that 14 students prepare their homework individually; the
rest 15 students prepare their homework with cooperative as triple groups. It's
students' own choice to prepare their homework individually or cooperatively.
There has been a success scale at the end of the teaching period. According to
research results, there isn't statistically considerable difference between
students who attend traditional homework practices and online homework
practices. According to research results, there isn't statistically
considerable difference between students who attend individual homework
practices and cooperative homework practices. The academic success of the
students who attend online-based individual homework practices is higher than
traditional individual and online based cooperative learning homework
practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5245</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5245</id><created>2014-01-21</created><authors><author><keyname>Bahaa-Eldeen</keyname><forenames>Ayman M</forenames></author><author><keyname>Wahdan</keyname><forenames>Abdel-Moneim A.</forenames></author><author><keyname>Mahdi</keyname><forenames>Hani M. K.</forenames></author></authors><title>Edge detection of binary images using the method of masks</title><categories>cs.CV</categories><journal-ref>Ain Shams University, Faculty of Engineering Scientific Bulletin,
  Volume 35, Issue 3, pp 349-355, (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the method of masks, creating and using of inverted image masks,
together with binary operation of image data are used in edge detection of
binary images, monochrome images, which yields about 300 times faster than
ordinary methods. The method is divided into three stages: Mask construction,
Fundamental edge detection, and Edge Construction Comparison with an ordinary
method and a fuzzy based method is carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5246</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5246</id><created>2014-01-21</created><authors><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Wahdan</keyname><forenames>A. M. A.</forenames></author><author><keyname>Mahdi</keyname><forenames>H. M. K.</forenames></author></authors><title>Genetic Algorithms and its use with back-propagation network</title><categories>cs.NE</categories><journal-ref>AIN Shams University, Faculty of Engineering Scientific Bulletin,
  Volume 35, Issue 3, pp 337-348 (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic algorithms are considered as one of the most efficient search
techniques. Although they do not offer an optimal solution, their ability to
reach a suitable solution in considerably short time gives them their
respectable role in many AI techniques. This work introduces genetic algorithms
and describes their characteristics. Then a novel method using genetic
algorithm in best training set generation and selection for a back-propagation
network is proposed. This work also offers a new extension to the original
genetic algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5247</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5247</id><created>2014-01-21</created><updated>2014-05-17</updated><authors><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Sousa</keyname><forenames>Pedro A.</forenames></author><author><keyname>Menasalvas</keyname><forenames>Ernestina</forenames></author></authors><title>Information content: assessing meso-scale structures in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>Published as: M. Zanin, P. A. Sousa and E. Menasalvas, Information
  content: assessing meso-scale structures in complex networks EPL 106 (3),
  (2014) 30001</comments><journal-ref>EPL 106 (3), (2014) 30001</journal-ref><doi>10.1209/0295-5075/106/30001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel measure to assess the presence of meso-scale structures in
complex networks. This measure is based on the identification of regular
patterns in the adjacency matrix of the network, and on the calculation of the
quantity of information lost when pairs of nodes are iteratively merged. We
show how this measure is able to quantify several meso-scale structures, like
the presence of modularity, bipartite and core-periphery configurations, or
motifs. Results corresponding to a large set of real networks are used to
validate its ability to detect non-trivial topological patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5254</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5254</id><created>2014-01-21</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Marra</keyname><forenames>Vincenzo</forenames></author></authors><title>Valuations in G\&quot;{o}del Logic, and the Euler Characteristic</title><categories>cs.LO cs.DM math.LO</categories><comments>15 pages, 1 figure</comments><journal-ref>Journal of Multiple-Valued Logic and Soft Computing 19 (2012)
  71-84</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the lattice-theoretic version of the Euler characteristic introduced by
V. Klee and G.-C. Rota in the Sixties, we define the Euler characteristic of a
formula in G\&quot;{o}del logic (over finitely or infinitely many truth-values). We
then prove that the information encoded by the Euler characteristic is
classical, i.e. coincides with the analogous notion defined over Boolean logic.
Building on this, we define many-valued versions of the Euler characteristic of
a formula $\varphi$, and prove that they indeed provide information about the
logical status of $\varphi$ in G\&quot;{o}del logic. Specifically, our first main
result shows that the many-valued Euler characteristics are invariants that
separate many-valued tautologies from non-tautologies. Further, we offer an
initial investigation of the linear structure of these generalised
characteristics. Our second main result is that the collection of many-valued
characteristics forms a linearly independent set in the real vector space of
all valuations of G\&quot;{o}del logic over finitely many propositional variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5258</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5258</id><created>2014-01-21</created><authors><author><keyname>Arslan</keyname><forenames>Farrukh</forenames></author></authors><title>Service Oriented Paradigm for Massive Multiplayer Online Games</title><categories>cs.NI cs.DC cs.MM cs.SE</categories><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 2, No. 5, pp. 35-47, 2012</journal-ref><doi>10.7321/jscse.v2.n5.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times Massive Multiplayer Online Game has appeared as a computer
game that enables hundreds of players from all parts of the world to interact
in a game world (common platform) at the same time instance. Current
architecture used for MMOGs based on the classic tightly coupled distributed
system. While, MMOGs are getting more interactive same time number of
interacting users is increasing, classic implementation architecture may raise
scalability and interdependence issues. This requires a loosely coupled service
oriented architecture to support evolution in MMOG application. Data flow
architecture, Event driven architecture and client server architecture are
basic date orchestration approaches used by any service oriented architecture.
Real time service is hottest issue for service oriented architecture. The basic
requirement of any real time service oriented architecture is to ensure the
quality of service. In this paper we have proposed a service oriented
architecture for massive multiplayer online game and a specific middleware
(based on open source DDS) in MMOGs for fulfilling real time constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5261</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5261</id><created>2014-01-21</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Marra</keyname><forenames>Vincenzo</forenames></author></authors><title>An Analysis of Ruspini Partitions in G\&quot;odel Logic</title><categories>cs.LO math.LO</categories><comments>22 pages</comments><journal-ref>International Journal of Approximate Reasoning 50/6 (2009) 825-836</journal-ref><doi>10.1016/j.ijar.2009.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By a Ruspini partition we mean a finite family of fuzzy sets $\{f_1, \ldots,
f_n\}$, $f_i : [0,1] \to [0,1]$, such that $\sum_{i=1}^n f_i(x)=1$ for all $x
\in [0,1]$, where $[0,1]$ denotes the real unit interval. We analyze such
partitions in the language of G\&quot;odel logic. Our first main result identifies
the precise degree to which the Ruspini condition is expressible in this
language, and yields inter alia a constructive procedure to axiomatize a given
Ruspini partition by a theory in G\&quot;odel logic. Our second main result extends
this analysis to Ruspini partitions fulfilling the natural additional condition
that each $f_i$ has at most one left and one right neighbour, meaning that
$\min_{x \in [0,1]}{\{f_{i_1}(x),f_{i_2}(x),f_{i_3}(x)\}}=0$ holds for $i_1\neq
i_2\neq i_3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5265</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5265</id><created>2014-01-21</created><authors><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author><author><keyname>Ochs</keyname><forenames>Michael</forenames></author><author><keyname>Wickenkamp</keyname><forenames>Axel</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ishigai</keyname><forenames>Yasushi</forenames></author><author><keyname>Kawaguchi</keyname><forenames>Takashi</forenames></author></authors><title>An Integrated Approach for Identifying Relevant Factors Influencing
  Software Development Productivity</title><categories>cs.SE</categories><comments>15 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-540-85279-7_18</comments><journal-ref>Balancing Agility and Formalism in Software Engineering, volume
  5082 of Lecture Notes in Computer Science, pages 223-237, Springer Berlin
  Heidelberg, 2008</journal-ref><doi>10.1007/978-3-540-85279-7_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managing software development productivity and effort are key issues in
software organizations. Identifying the most relevant factors influencing
project performance is essential for implementing business strategies by
selecting and adjusting proper improvement activities. There is, however, a
large number of potential influencing factors. This paper proposes a novel
approach for identifying the most relevant factors influencing software
development productivity. The method elicits relevant factors by integrating
data analysis and expert judgment approaches by means of a multi-criteria
decision support technique. Empirical evaluation of the method in an industrial
context has indicated that it delivers a different set of factors compared to
individual data- and expert-based factor selection methods. Moreover,
application of the integrated method significantly improves the performance of
effort estimation in terms of accuracy and precision. Finally, the study did
not replicate the observation of similar investigations regarding improved
estimation performance on the factor sets reduced by a data-based selection
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5266</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5266</id><created>2014-01-21</created><updated>2014-05-12</updated><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author></authors><title>Subclasses of Presburger Arithmetic and the Weak EXP Hierarchy</title><categories>cs.LO</categories><comments>10 pages, 2 figures</comments><msc-class>03B70</msc-class><acm-class>F.4.1</acm-class><doi>10.1145/2603088.2603092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that for any fixed $i&gt;0$, the $\Sigma_{i+1}$-fragment of
Presburger arithmetic, i.e., its restriction to $i+1$ quantifier alternations
beginning with an existential quantifier, is complete for
$\mathsf{\Sigma}^{\mathsf{EXP}}_{i}$, the $i$-th level of the weak EXP
hierarchy, an analogue to the polynomial-time hierarchy residing between
$\mathsf{NEXP}$ and $\mathsf{EXPSPACE}$. This result completes the
computational complexity landscape for Presburger arithmetic, a line of
research which dates back to the seminal work by Fischer &amp; Rabin in 1974.
Moreover, we apply some of the techniques developed in the proof of the lower
bound in order to establish bounds on sets of naturals definable in the
$\Sigma_1$-fragment of Presburger arithmetic: given a $\Sigma_1$-formula
$\Phi(x)$, it is shown that the set of non-negative solutions is an ultimately
periodic set whose period is at most doubly-exponential and that this bound is
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5269</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5269</id><created>2014-01-21</created><updated>2015-01-21</updated><authors><author><keyname>Mertens</keyname><forenames>Stephan</forenames></author></authors><title>Stable Roommates Problem with Random Preferences</title><categories>cs.DS cs.CC math.CO</categories><comments>14 pages, 6 figures, 4 algorithms, 1 table; Journal of Statistical
  Mechanics: Theory and Experiment (2015) P01020</comments><acm-class>F.2.2; G.2.1; G.3</acm-class><doi>10.1088/1742-5468/2015/01/P01020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable roommates problem with $n$ agents has worst case complexity
$O(n^2)$ in time and space. Random instances can be solved faster and with less
memory, however. We introduce an algorithm that has average time and space
complexity $O(n^\frac{3}{2})$ for random instances. We use this algorithm to
simulate large instances of the stable roommates problem and to measure the
probabilty $p_n$ that a random instance of size $n$ admits a stable matching.
Our data supports the conjecture that $p_n = \Theta(n^{-1/4})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5272</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5272</id><created>2014-01-21</created><updated>2015-12-18</updated><authors><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author></authors><title>The Rate-Distortion Function and Error Exponent of Sparse Regression
  Codes with Optimal Encoding</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>28 pages. A shorter version appeared in the proceedings of ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of Sparse Regression Codes for lossy
compression with the squared-error distortion criterion. In a sparse regression
code, codewords are linear combinations of subsets of columns of a design
matrix. It is shown that with minimum-distance encoding, sparse regression
codes achieve the Shannon rate-distortion function for i.i.d. Gaussian sources
$R^*(D)$ as well as the optimal error exponent. This completes a previous
result which showed that $R^*(D)$ and the optimal exponent were achievable for
distortions below a certain threshold. The proof of the rate-distortion result
is based on the second moment method, a popular technique to show that a
non-negative random variable $X$ is strictly positive with high probability. We
first identify the reason behind the failure of the standard second moment
method for certain distortions, and illustrate the different failure modes via
a stylized example. We then use a refinement of the second moment method to
show that $R^*(D)$ is achievable for all distortion values. Finally, the
refinement technique is applied to Suen's correlation inequality to prove the
achievability of the optimal Gaussian error exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5277</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5277</id><created>2014-01-21</created><updated>2014-04-28</updated><authors><author><keyname>Goncharov</keyname><forenames>Sergey</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author></authors><title>Towards a Coalgebraic Chomsky Hierarchy</title><categories>cs.LO cs.FL</categories><comments>corrected; converted to lncs style</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Chomsky hierarchy plays a prominent role in the foundations of the
theoretical computer science relating classes of formal languages of primary
importance. In this paper we use recent developments on coalgebraic and
monad-based semantics to obtain a generic notion of a $\mathbb{T}$-automaton,
where $\mathbb{T}$ is a monad, which allows the uniform study of various
notions of machines (e.g. finite state machines, multi-stack machines, Turing
machines, valence automata, weighted automata). We use the generalized powerset
construction to define a generic (trace) semantics for $\mathbb{T}$-automata,
and we show by numerous examples that it correctly instantiates for the known
classes of machines/languages captured by the Chomsky hierarchy. Moreover, our
approach provides new generic techniques for proving expressivity bounds of
various machine-based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5287</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5287</id><created>2014-01-21</created><authors><author><keyname>Kalampakas</keyname><forenames>Antonios</forenames></author></authors><title>k-Colorability is Graph Automaton Recognizable</title><categories>cs.FL cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata operating on general graphs have been introduced by virtue of
graphoids. In this paper we construct a graph automaton that recognizes
$k$-colorable graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5289</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5289</id><created>2014-01-21</created><authors><author><keyname>Simeonov</keyname><forenames>Stanislav</forenames></author><author><keyname>Simeonova</keyname><forenames>Neli</forenames></author></authors><title>Graphical Interface for Visually Impaired People Based on Bi-stable
  Solenoids</title><categories>cs.HC</categories><comments>4 pages, 5 figures, The International Journal of Soft Computing and
  Software Engineering, ISSN:2251-7545</comments><doi>10.7321/jscse</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper a concept for hardware realization of graphic tactile display
for visually impaired peoples is presented. For realization of tactile
actuators bi-stable, solenoids and PIC based control board are used. The
selected algorithm for series activation of each row of display allows using
minimal number of active components to set and reset the solenoids. Finally, a
program algorithm of control board is discussed. The project is funded by
Bulgarian National Science Fund NSF Grant D ID 02 14, 2009 2013
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5292</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5292</id><created>2014-01-21</created><authors><author><keyname>Payet</keyname><forenames>&#xc9;tienne</forenames></author><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author><author><keyname>Spoto</keyname><forenames>Fausto</forenames></author></authors><title>Non-Termination Analysis of Java Bytecode</title><categories>cs.PL</categories><comments>17 pages, technical report written in May 2012</comments><acm-class>F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a fully automated static analysis that takes a sequential Java
bytecode program P as input and attempts to prove that there exists an infinite
execution of P. The technique consists in compiling P into a constraint logic
program P_CLP and in proving non-termination of P_CLP; when P consists of
instructions that are exactly compiled into constraints, the non-termination of
P_CLP entails that of P. Our approach can handle method calls; to the best of
our knowledge, it is the first static approach for Java bytecode able to prove
the existence of infinite recursions. We have implemented our technique inside
the Julia analyser. We have compared the results of Julia on a set of 113
programs with those provided by AProVE and Invel, the only freely usable
non-termination analysers comparable to ours that we are aware of. Only Julia
could detect non-termination due to infinite recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5297</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5297</id><created>2014-01-21</created><authors><author><keyname>Biczok</keyname><forenames>Gergely</forenames></author><author><keyname>Martinez</keyname><forenames>Santiago Diez</forenames></author><author><keyname>Jelle</keyname><forenames>Thomas</forenames></author><author><keyname>Krogstie</keyname><forenames>John</forenames></author></authors><title>Navigating MazeMap: indoor human mobility, spatio-logical ties and
  future potential</title><categories>cs.SY cs.NI cs.SI</categories><comments>6 pages, accepted at PerMoby Workshop at IEEE PerCom 2014</comments><acm-class>C.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global navigation systems and location-based services have found their way
into our daily lives. Recently, indoor positioning techniques have also been
proposed, and there are several live or trial systems already operating. In
this paper, we present insights from MazeMap, the first live indoor/outdoor
positioning and navigation system deployed at a large university campus in
Norway. Our main contribution is a measurement case study; we show the spatial
and temporal distribution of MazeMap geo-location and wayfinding requests,
construct the aggregated human mobility map of the campus and find strong
logical ties between different locations. On one hand, our findings are
specific to the venue; on the other hand, the nature of available data and
insights coupled with our discussion on potential usage scenarios for indoor
positioning and location-based services predict a successful future for these
systems and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5300</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5300</id><created>2014-01-21</created><updated>2014-05-31</updated><authors><author><keyname>Wang</keyname><forenames>Yanqing</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Li</keyname><forenames>Xiaojie</forenames></author><author><keyname>Yun</keyname><forenames>Sijing</forenames></author><author><keyname>Song</keyname><forenames>Minjing</forenames></author></authors><title>How are identifiers named in open source software? About popularity and
  consistency</title><categories>cs.SE cs.PL</categories><comments>10 pages, 3 figures, 5 tables</comments><journal-ref>International Journal of Computer and Information Technology, vol
  3, no 3, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid increasing of software project size and maintenance cost,
adherence to coding standards especially by managing identifier naming, is
attracting a pressing concern from both computer science educators and software
managers. Software developers mainly use identifier names to represent the
knowledge recorded in source code. However, the popularity and adoption
consistency of identifier naming conventions have not been revealed yet in this
field. Taking forty-eight popular open source projects written in three
top-ranking programming languages Java, C and C++ as examples, an identifier
extraction tool based on regular expression matching is developed. In the
subsequent investigation, some interesting findings are obtained. For the
identifier naming popularity, it is found that Camel and Pascal naming
conventions are leading the road while Hungarian notation is vanishing. For the
identifier naming consistency, we have found that the projects written in Java
have a much better performance than those written in C and C++. Finally,
academia and software industry are urged to adopt the most popular naming
conventions consistently in their practices so as to lead the identifier naming
to a standard, unified and high-quality road.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5305</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5305</id><created>2014-01-21</created><authors><author><keyname>Zhuang</keyname><forenames>Qiutao</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Kavcic</keyname><forenames>Aleksander</forenames></author></authors><title>Bounds on the ML Decoding Error Probability of RS-Coded Modulation over
  AWGN Channels</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1309.1555 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with bounds on the maximum-likelihood (ML) decoding
error probability of Reed-Solomon (RS) codes over additive white Gaussian noise
(AWGN) channels. To resolve the difficulty caused by the dependence of the
Euclidean distance spectrum on the way of signal mapping, we propose to use
random mapping, resulting in an ensemble of RS-coded modulation (RS-CM)
systems. For this ensemble of RS-CM systems, analytic bounds are derived, which
can be evaluated from the known (symbol-level) Hamming distance spectrum. Also
presented in this paper are simulation-based bounds, which are applicable to
any specific RS-CM system and can be evaluated by the aid of a list decoding
(in the Euclidean space) algorithm. The simulation-based bounds do not need
distance spectrum and are numerically tight for short RS codes in the regime
where the word error rate (WER) is not too low. Numerical comparison results
are relevant in at least three aspects. First, in the short code length regime,
RS-CM using BPSK modulation with random mapping has a better performance than
binary random linear codes. Second, RS-CM with random mapping (time varying)
can have a better performance than with specific mapping. Third, numerical
results show that the recently proposed Chase-type decoding algorithm is
essentially the ML decoding algorithm for short RS codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5306</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5306</id><created>2014-01-21</created><authors><author><keyname>Marcus</keyname><forenames>Anthony</forenames></author><author><keyname>Cardei</keyname><forenames>Ionut</forenames></author><author><keyname>Furht</keyname><forenames>Borko</forenames></author><author><keyname>Salem</keyname><forenames>Osman</forenames></author><author><keyname>Mehaoua</keyname><forenames>Ahmed</forenames></author></authors><title>A Mobile Device Prototype Application for the Detection and Prediction
  of Node Faults in Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various implementations of wireless sensor networks (i.e. personal area-,
wireless body area- networks) are prone to node and network failures by such
characteristics as limited node energy resources and hardware damage incurred
from their surrounding environment (i.e. flooding, forest fires, a patient
falling). This may jeopardize their reliability to act as early warning
systems, monitoring systems for patients and athletes, and industrial and
environmental observation networks. Following the current trend and widespread
use of hand held, mobile communication devices, we outline an application
architecture designed to detect and predict faulty nodes in wireless sensor
networks. Furthermore, we implement our design as a proof of concept prototype
for Android-based smartphones, which may be extended to develop other
applications used for monitoring networked wireless personal area and body
sensors used in other capacities. We have conducted several preliminary
experiments to demonstrate the use of our design, which is capable of
monitoring networks of wireless sensor devices and predicting node faults based
on several localized metrics. As attributes of such networks may change over
time, any models generated when the application is initialized must be updated
periodically such that the applied machine learning algorithm maintains high
levels of both accuracy and precision. The application is designed to discover
node faults and, once identified, alert the user so that appropriate action may
be taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5311</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5311</id><created>2014-01-21</created><updated>2015-08-05</updated><authors><author><keyname>Ding</keyname><forenames>Changxing</forenames></author><author><keyname>Choi</keyname><forenames>Jonghyun</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face
  Recognition</title><categories>cs.CV</categories><comments>accepted version to IEEE TPAMI</comments><doi>10.1109/TPAMI.2015.2462338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To perform unconstrained face recognition robust to variations in
illumination, pose and expression, this paper presents a new scheme to extract
&quot;Multi-Directional Multi-Level Dual-Cross Patterns&quot; (MDML-DCPs) from face
images. Specifically, the MDMLDCPs scheme exploits the first derivative of
Gaussian operator to reduce the impact of differences in illumination and then
computes the DCP feature at both the holistic and component levels. DCP is a
novel face image descriptor inspired by the unique textural structure of human
faces. It is computationally efficient and only doubles the cost of computing
local binary patterns, yet is extremely robust to pose and expression
variations. MDML-DCPs comprehensively yet efficiently encodes the invariant
characteristics of a face image from multiple levels into patterns that are
highly discriminative of inter-personal differences but robust to
intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC
2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local
descriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face
identification and face verification tasks. More impressively, the best
performance is achieved on the challenging LFW and FRGC 2.0 databases by
deploying MDML-DCPs in a simple recognition scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5316</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5316</id><created>2014-01-21</created><authors><author><keyname>Su</keyname><forenames>Hsin-Hao</forenames></author></authors><title>A Distributed Minimum Cut Approximation Scheme</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of approximating the minimum cut in a
distributed message-passing model, the CONGEST model. The minimum cut problem
has been well-studied in the context of centralized algorithms. However, there
were no known non-trivial algorithms in the distributed model until the recent
work of Ghaffari and Kuhn. They gave algorithms for finding cuts of size
$O(\epsilon^{-1}\lambda)$ and $(2+\epsilon)\lambda$ in
$O(D)+\tilde{O}(n^{1/2+\epsilon})$ rounds and $\tilde{O}(D+\sqrt{n})$ rounds
respectively, where $\lambda$ is the size of the minimum cut. This matches the
lower bound they provided up to a polylogarithmic factor. Yet, no scheme that
achieves $(1+\epsilon)$-approximation ratio is known. We give a distributed
algorithm that finds a cut of size $(1+\epsilon)\lambda$ in
$\tilde{O}(D+\sqrt{n})$ time, which is optimal up to polylogarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5321</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5321</id><created>2014-01-21</created><authors><author><keyname>Jung</keyname><forenames>Wook</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Chun</forenames></author></authors><title>An Integer Programming Approach to UEP Coding for Multiuser Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an integer programming approach is introduced to construct
Unequal Error Protection (UEP) codes for multiuser broadcast channels. We show
that the optimal codes can be constructed that satisfy the integer programming
bound. Based on the bound, we compute asymptotic code rate and perform
throughput analysis for the degraded broadcast channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5325</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5325</id><created>2014-01-21</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>jagadeesan</keyname><forenames>Radha</forenames></author></authors><title>Game Semantics for Access Control</title><categories>cs.LO</categories><comments>21 pages</comments><journal-ref>In Proceedings of MFPS 25, Electronic Notes in Theoretical
  Computer Science, Vol. 249, pages 135-156, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a semantic approach to the study of logics for access control
and dependency analysis, based on Game Semantics. We use a variant of AJM games
with explicit justification (but without pointers). Based on this, we give a
simple and intuitive model of the information flow constraints underlying
access control. This is used to give strikingly simple proofs of
\emph{non-interference theorems} in robust, semantic versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5327</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5327</id><created>2014-01-21</created><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author></authors><title>Compositional Operators in Distributional Semantics</title><categories>cs.CL cs.AI math.CT</categories><doi>10.1007/s40362-014-0017-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey presents in some detail the main advances that have been recently
taking place in Computational Linguistics towards the unification of the two
prominent semantic paradigms: the compositional formal semantics view and the
distributional models of meaning based on vector spaces. After an introduction
to these two approaches, I review the most important models that aim to provide
compositionality in distributional semantics. Then I proceed and present in
more detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based
on the abstract mathematical setting of category theory, as a more complete
example capable to demonstrate the diversity of techniques and scientific
disciplines that this kind of research can draw from. This paper concludes with
a discussion about important open issues that need to be addressed by the
researchers in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5330</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5330</id><created>2014-01-21</created><authors><author><keyname>El-Sayed</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Abdel-Khalek</keyname><forenames>S.</forenames></author><author><keyname>Amin</keyname><forenames>Hanan H.</forenames></author></authors><title>Study of Neural Network Algorithm for Straight-Line Drawings of Planar
  Graphs</title><categories>cs.CG cs.NE</categories><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS) ISSN 1947-5500, Vol. 9, No. 9, pp. 13-19, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph drawing addresses the problem of finding a layout of a graph that
satisfies given aesthetic and understandability objectives. The most important
objective in graph drawing is minimization of the number of crossings in the
drawing, as the aesthetics and readability of graph drawings depend on the
number of edge crossings. VLSI layouts with fewer crossings are more easily
realizable and consequently cheaper. A straight-line drawing of a planar graph
G of n vertices is a drawing of G such that each edge is drawn as a
straight-line segment without edge crossings. However, a problem with current
graph layout methods which are capable of producing satisfactory results for a
wide range of graphs is that they often put an extremely high demand on
computational resources. This paper introduces a new layout method, which
nicely draws internally convex of planar graph that consumes only little
computational resources and does not need any heavy duty preprocessing. Here,
we use two methods: The first is self organizing map known from unsupervised
neural networks which is known as (SOM) and the second method is Inverse Self
Organized Map (ISOM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5334</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5334</id><created>2014-01-21</created><authors><author><keyname>Michel</keyname><forenames>Laurent</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>A Microkernel Architecture for Constraint Programming</title><categories>cs.AI cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a microkernel architecture for constraint programming
organized around a number of small number of core functionalities and minimal
interfaces. The architecture contrasts with the monolithic nature of many
implementations. Experimental results indicate that the software engineering
benefits are not incompatible with runtime efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5338</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5338</id><created>2014-01-21</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Heizmann</keyname><forenames>Matthias</forenames></author></authors><title>Ranking Templates for Linear Loops</title><categories>cs.LO</categories><comments>TACAS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for the constraint-based synthesis of termination
arguments for linear loop programs based on linear ranking templates. Linear
ranking templates are parametrized, well-founded relations such that an
assignment to the parameters gives rise to a ranking function. This approach
generalizes existing methods and enables us to use templates for many different
ranking functions with affine-linear components. We discuss templates for
multiphase, piecewise, and lexicographic ranking functions. Because these
ranking templates require both strict and non-strict inequalities, we use
Motzkin's Transposition Theorem instead of Farkas Lemma to transform the
generated $\exists\forall$-constraint into an $\exists$-constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5339</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5339</id><created>2014-01-21</created><authors><author><keyname>Friedkin</keyname><forenames>Noah E.</forenames></author></authors><title>Complex Objects in the Polytopes of the Linear State-Space Process</title><categories>math.OC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple object (one point in $m$-dimensional space) is the resultant of the
evolving matrix polynomial of walks in the irreducible aperiodic network
structure of the first order DeGroot (weighted averaging) state-space process.
This paper draws on a second order generalization the DeGroot model that allows
complex object resultants, i.e, multiple points with distinct coordinates, in
the convex hull of the initial state-space. It is shown that, holding network
structure constant, a unique solution exists for the particular initial space
that is a sufficient condition for the convergence of the process to a
specified complex object. In addition, it is shown that, holding network
structure constant, a solution exists for dampening values sufficient for the
convergence of the process to a specified complex object. These dampening
values, which modify the values of the walks in the network, control the
system's outcomes, and any strongly connected typology is a sufficient
condition of such control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5341</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5341</id><created>2014-01-21</created><authors><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Michel</keyname><forenames>Laurent</forenames></author></authors><title>Domain Views for Constraint Programming</title><categories>cs.AI cs.PL</categories><comments>Workshop: TRICS13: Techniques foR Implementing Constraint
  programming, September 2013, CP, Uppsala</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Views are a standard abstraction in constraint programming: They make it
possible to implement a single version of each constraint, while avoiding to
create new variables and constraints that would slow down propagation.
Traditional constraint-programming systems provide the concept of {\em variable
views} which implement a view of the type $y = f(x)$ by delegating all (domain
and constraint) operations on variable $y$ to variable $x$. This paper proposes
the alternative concept of {\em domain views} which only delegate domain
operations. Domain views preserve the benefits of variable views but simplify
the implementation of value-based propagation. Domain views also support
non-injective views compositionally, expanding the scope of views
significantly. Experimental results demonstrate the practical benefits of
domain views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5346</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5346</id><created>2014-01-21</created><authors><author><keyname>McRoberts</keyname><forenames>Malcolm</forenames></author></authors><title>Software Licensing in the Cloud Age</title><categories>cs.CY cs.DC</categories><msc-class>91B32</msc-class><acm-class>K.6.2; K.5.1; C.5.m</acm-class><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 395-402, 2013</journal-ref><doi>10.7321/jscse.v3.n3.60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing represents a major shift in information systems architecture,
combining both new deployment models and new business models. Rapid
provisioning, elastic scaling, and metered usage are essential characteristics
of cloud services, and they require cloud resources with these same
characteristics. When cloud services depend on commercial software, the
licenses for that software become another resource to be managed by the cloud.
This paper examines common licensing models, including open source, and how
well they function in a cloud services model. It discusses creative, new,
cloud-centric licensing models and how they allow providers to preserve and
expand their revenue streams as their partners and customers transition to the
cloud. The paper concludes by identifying the next steps to achieve
standardized, cloud-friendly licensing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5347</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5347</id><created>2014-01-21</created><authors><author><keyname>Heizmann</keyname><forenames>Matthias</forenames></author><author><keyname>Hoenicke</keyname><forenames>Jochen</forenames></author><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Podelski</keyname><forenames>Andreas</forenames></author></authors><title>Linear Ranking for Linear Lasso Programs</title><categories>cs.LO</categories><comments>ATVA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general setting of this work is the constraint-based synthesis of
termination arguments. We consider a restricted class of programs called lasso
programs. The termination argument for a lasso program is a pair of a ranking
function and an invariant. We present the---to the best of our
knowledge---first method to synthesize termination arguments for lasso programs
that uses linear arithmetic. We prove a completeness theorem. The completeness
theorem establishes that, even though we use only linear (as opposed to
non-linear) constraint solving, we are able to compute termination arguments in
several interesting cases. The key to our method lies in a constraint
transformation that replaces a disjunction by a sum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5351</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5351</id><created>2014-01-21</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author></authors><title>Ranking Function Synthesis for Linear Lasso Programs</title><categories>cs.LO</categories><comments>Master's Thesis</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The scope of this work is the constraint-based synthesis of termination
arguments for the restricted class of programs called linear lasso programs. A
termination argument consists of a ranking function as well as a set of
supporting invariants.
  We extend existing methods in several ways. First, we use Motzkin's
Transposition Theorem instead of Farkas' Lemma. This allows us to consider
linear lasso programs that can additionally contain strict inequalities.
Existing methods are restricted to non-strict inequalities and equalities.
  Second, we consider several kinds of ranking functions: affine-linear,
piecewise and lexicographic ranking functions. Moreover, we present a novel
kind of ranking function called multiphase ranking function which proceeds
through a fixed number of phases such that for each phase, there is an
affine-linear ranking function. As an abstraction to the synthesis of specific
ranking functions, we introduce the notion ranking function template. This
enables us to handle all ranking functions in a unified way.
  Our method relies on non-linear algebraic constraint solving as a subroutine
which is known to scale poorly to large problems. As a mitigation we formalize
an assessment of the difficulty of our constraints and present an argument why
they are of an easier kind than general non-linear constraints.
  We prove our method to be complete: if there is a termination argument of the
form specified by the given ranking function template with a fixed number of
affine-linear supporting invariants, then our method will find a termination
argument.
  To our knowledge, the approach we propose is the most powerful technique of
synthesis-based discovery of termination arguments for linear lasso programs
and encompasses and enhances several methods having been proposed thus far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5353</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5353</id><created>2014-01-21</created><authors><author><keyname>Barker</keyname><forenames>Blake</forenames></author></authors><title>STABLAB Documentation for KdV : Numerical proof of stability of roll
  waves in the small-amplitude limit for inclined thin film flow</title><categories>cs.MS cs.NA math.NA</categories><comments>Documentation, 255 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We document the MATLAB code used in the following study: Numerical proof of
stability of roll waves in the small-amplitude limit for inclined thin film
flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5354</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5354</id><created>2014-01-21</created><updated>2015-10-07</updated><authors><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Counting Houses of Pareto Optimal Matchings in the House Allocation
  Problem</title><categories>math.CO cs.CC cs.DM cs.GT</categories><comments>21 pages 2 Figures only small cosmetic changes to the previous
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A,B$ with $|A| = m$ and $|B| = n\ge m$ be two sets. We assume that every
element $a\in A$ has a reference list over all elements from $B$. We call an
injective mapping $\tau$ from $A$ to $B$ a matching. A blocking coalition of
$\tau$ is a subset $A'$ of $A$ such that there exists a matching $\tau'$ that
differs from $\tau$ only on elements of $A'$, and every element of $A'$
improves in $\tau'$, compared to $\tau$ according to its preference list. If
there exists no blocking coalition, we call the matching $\tau$ an exchange
stable matching (ESM). An element $b\in B$ is reachable if there exists an
exchange stable matching using $b$. The set of all reachable elements is
denoted by $E^*$. We show \[|E^*| \leq \sum_{i = 1,\ldots,
m}{\left\lfloor\frac{m}{i}\right\rfloor} = \Theta(m\log m).\] This is
asymptotically tight. A set $E\subseteq B$ is reachable (respectively exactly
reachable) if there exists an exchange stable matching $\tau$ whose image
contains $E$ as a subset (respectively equals $E$). We give bounds for the
number of exactly reachable sets. We find that our results hold in the more
general setting of multi-matchings, when each element $a$ of $A$ is matched
with $\ell_a$ elements of $B$ instead of just one. Further, we give complexity
results and algorithms for corresponding algorithmic questions. Finally, we
characterize unavoidable elements, i.e., elements of $B$ that are used by all
ESM's. This yields efficient algorithms to determine all unavoidable elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5360</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5360</id><created>2014-01-21</created><updated>2014-05-07</updated><authors><author><keyname>Boche</keyname><forenames>H.</forenames></author><author><keyname>Noetzel</keyname><forenames>J.</forenames></author></authors><title>Positivity, Discontinuity, Finite Resources and Nonzero Error for
  Arbitrarily Varying Quantum Channels</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>19 pages, no figures. Corrected typos. Large parts of the
  introduction are rewritten, especially the historical part from the earlier
  verions is completely deleted. This version contains an additional theorem
  (Theorem 5) which summarizes our findings concerning the points of
  discontinuity of the deterministic message transmission capacity</comments><doi>10.1063/1.4902930</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is motivated by a quite general question: Under which circumstances
are the capacities of information transmission systems continuous? The research
is explicitly carried out on arbitrarily varying quantum channels (AVQCs). We
give an explicit example that answers the recent question whether the
transmission of messages over AVQCs can benefit from distribution of randomness
between the legitimate sender and receiver in the affirmative. The specific
class of channels introduced in that example is then extended to show that the
deterministic capacity does have discontinuity points, while that behaviour is,
at the same time, not generic: We show that it is continuous around its
positivity points. This is in stark contrast to the randomness-assisted
capacity, which is always continuous in the channel. Our results imply that the
deterministic message transmission capacity of an AVQC can be discontinuous
only in points where it is zero, while the randomness assisted capacity is
nonzero. Apart from the zero-error capacities, this is the first result that
shows a discontinuity of a capacity for a large class of quantum channels. The
continuity of the respective capacity for memoryless quantum channels had,
among others, been listed as an open problem on the problem page of the ITP
Hannover for about six years before it was proven to be continuous. We also
quantify the interplay between the distribution of finite amounts of randomness
between the legitimate sender and receiver, the (nonzero) decoding error with
respect to the average error criterion that can be achieved over a finite
number of channel uses and the number of messages that can be sent. This part
of our results also applies to entanglement- and strong subspace transmission.
In addition, we give a new sufficient criterion for the entanglement
transmission capacity with randomness assistance to vanish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5364</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5364</id><created>2014-01-21</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>HMACA: Towards Proposing a Cellular Automata Based Tool for Protein
  Coding, Promoter Region Identification and Protein Structure Prediction</title><categories>cs.CE cs.LG</categories><journal-ref>International Journal of Research in Computer Applications &amp;
  Information Technology, Volume 1, Issue 1, July-September, 2013, pp. 26-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human body consists of lot of cells, each cell consist of DeOxaRibo Nucleic
Acid (DNA). Identifying the genes from the DNA sequences is a very difficult
task. But identifying the coding regions is more complex task compared to the
former. Identifying the protein which occupy little place in genes is a really
challenging issue. For understating the genes coding region analysis plays an
important role. Proteins are molecules with macro structure that are
responsible for a wide range of vital biochemical functions, which includes
acting as oxygen, cell signaling, antibody production, nutrient transport and
building up muscle fibers. Promoter region identification and protein structure
prediction has gained a remarkable attention in recent years. Even though there
are some identification techniques addressing this problem, the approximate
accuracy in identifying the promoter region is closely 68% to 72%. We have
developed a Cellular Automata based tool build with hybrid multiple attractor
cellular automata (HMACA) classifier for protein coding region, promoter region
identification and protein structure prediction which predicts the protein and
promoter regions with an accuracy of 76%. This tool also predicts the structure
of protein with an accuracy of 80%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5367</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5367</id><created>2014-01-21</created><authors><author><keyname>Lopez-Herrejon</keyname><forenames>Roberto E.</forenames></author><author><keyname>Ferrer</keyname><forenames>Javier</forenames></author><author><keyname>Chicano</keyname><forenames>Francisco</forenames></author><author><keyname>Haslinger</keyname><forenames>Evelyn Nicole</forenames></author><author><keyname>Egyed</keyname><forenames>Alexander</forenames></author><author><keyname>Alba</keyname><forenames>Enrique</forenames></author></authors><title>Towards a Benchmark and a Comparison Framework for Combinatorial
  Interaction Testing of Software Product Lines</title><categories>cs.SE</categories><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Software Product Lines (SPLs) are becoming a more pervasive development
practice, their effective testing is becoming a more important concern. In the
past few years many SPL testing approaches have been proposed, among them, are
those that support Combinatorial Interaction Testing (CIT) whose premise is to
select a group of products where faults, due to feature interactions, are more
likely to occur. Many CIT techniques for SPL testing have been put forward;
however, no systematic and comprehensive comparison among them has been
performed. To achieve such goal two items are important: a common benchmark of
feature models, and an adequate comparison framework. In this
research-in-progress paper, we propose 19 feature models as the base of a
benchmark, which we apply to three different techniques in order to analyze the
comparison framework proposed by Perrouin et al. We identify the shortcomings
of this framework and elaborate alternatives for further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5382</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5382</id><created>2014-01-21</created><authors><author><keyname>Prasath</keyname><forenames>M. Tharun</forenames></author></authors><title>Continuous Speech Recognition Based on Deterministic Finite Automata
  Machine using Utterance and Pitch Verification</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a set of acoustic modeling techniques for utterance
verification (UV) based continuous speech recognition (CSR). Utterance
verification in this work implies the ability to determine when portions of a
hypothesized word string correspond to incorrectly decoded vocabulary words or
out-of-vocabulary words that may appear in an utterance. This capability is
implemented here as a likelihood ratio (LR). There are two UV techniques that
are presented here. The first is voice verification along with the vocabulary
testing, at the same time the parameters for UV models are generated based on
the optimization criterion which is directly related to the LR measure. The
second technique is a pitch recognition based on weighted finite-state
transducers. These transducers provide a common and natural representation for
major components of speech recognition systems, including hidden Markov models
(HMMs), context-dependency models, pronunciation dictionaries, statistical
grammars, and word or phone lattices. The finite state machine processes the
acoustic parameters of UV models. The results of an experimental study
presented in the paper shows that LR based parameter estimation results in a
significant improvement in UV performance for this task. The study also found
that the use of the LR based weighted finite-state transducers along with the
UV, can provide as much as an 11% improvement in UV performance when compared
to existing UV procedures. Finally, it was also found that the performance of
the finite state machine was highly dependent on the use of the LR criterion in
training acoustic models. Several observations are made in the paper concerning
the formation of confidence measures for UV and the interaction of these
techniques with statistical language models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5383</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5383</id><created>2014-01-21</created><updated>2014-10-06</updated><authors><author><keyname>Chikhi</keyname><forenames>Rayan</forenames></author><author><keyname>Limasset</keyname><forenames>Antoine</forenames></author><author><keyname>Jackman</keyname><forenames>Shaun</forenames></author><author><keyname>Simpson</keyname><forenames>Jared</forenames></author><author><keyname>Medvedev</keyname><forenames>Paul</forenames></author></authors><title>On the representation of de Bruijn graphs</title><categories>q-bio.QM cs.DS q-bio.GN</categories><comments>Journal version (JCB). A preliminary version of this article was
  published in the proceedings of RECOMB 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The de Bruijn graph plays an important role in bioinformatics, especially in
the context of de novo assembly. However, the representation of the de Bruijn
graph in memory is a computational bottleneck for many assemblers. Recent
papers proposed a navigational data structure approach in order to improve
memory usage. We prove several theoretical space lower bounds to show the
limitation of these types of approaches. We further design and implement a
general data structure (DBGFM) and demonstrate its use on a human whole-genome
dataset, achieving space usage of 1.5 GB and a 46% improvement over previous
approaches. As part of DBGFM, we develop the notion of frequency-based
minimizers and show how it can be used to enumerate all maximal simple paths of
the de Bruijn graph using only 43 MB of memory. Finally, we demonstrate that
our approach can be integrated into an existing assembler by modifying the
ABySS software to use DBGFM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5389</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5389</id><created>2014-01-15</created><authors><author><keyname>Dasgupta</keyname><forenames>Sajib</forenames></author><author><keyname>Ng</keyname><forenames>Vincent</forenames></author></authors><title>Which Clustering Do You Want? Inducing Your Ideal Clustering with
  Minimal Feedback</title><categories>cs.IR cs.CL cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  581-632, 2010</journal-ref><doi>10.1613/jair.3003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While traditional research on text clustering has largely focused on grouping
documents by topic, it is conceivable that a user may want to cluster documents
along other dimensions, such as the authors mood, gender, age, or sentiment.
Without knowing the users intention, a clustering algorithm will only group
documents along the most prominent dimension, which may not be the one the user
desires. To address the problem of clustering documents along the user-desired
dimension, previous work has focused on learning a similarity metric from data
manually annotated with the users intention or having a human construct a
feature space in an interactive manner during the clustering process. With the
goal of reducing reliance on human knowledge for fine-tuning the similarity
function or selecting the relevant features required by these approaches, we
propose a novel active clustering algorithm, which allows a user to easily
select the dimension along which she wants to cluster the documents by
inspecting only a small number of words. We demonstrate the viability of our
algorithm on a variety of commonly-used sentiment datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5390</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5390</id><created>2014-01-18</created><authors><author><keyname>Branavan</keyname><forenames>S. R. K.</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author></authors><title>Learning to Win by Reading Manuals in a Monte-Carlo Framework</title><categories>cs.CL cs.AI cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  661-704, 2012</journal-ref><doi>10.1613/jair.3484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5391</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5391</id><created>2014-01-21</created><authors><author><keyname>Orchard</keyname><forenames>Dominic</forenames></author><author><keyname>Petricek</keyname><forenames>Tomas</forenames></author><author><keyname>Mycroft</keyname><forenames>Alan</forenames></author></authors><title>The semantic marriage of monads and effects</title><categories>cs.PL</categories><comments>extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wadler and Thiemann unified type-and-effect systems with monadic semantics
via a syntactic correspondence and soundness results with respect to an
operational semantics. They conjecture that a general, &quot;coherent&quot; denotational
semantics can be given to unify effect systems with a monadic-style semantics.
We provide such a semantics based on the novel structure of an indexed monad,
which we introduce. We redefine the semantics of Moggi's computational
lambda-calculus in terms of (strong) indexed monads which gives a one-to-one
correspondence between indices of the denotations and the effect annotations of
traditional effect systems. Dually, this approach yields indexed comonads which
gives a unified semantics and effect system to contextual notions of effect
(called coeffects), which we have previously described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5394</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5394</id><created>2014-01-21</created><authors><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Varghese</keyname><forenames>Thomas</forenames></author></authors><title>Determinising Parity Automata</title><categories>cs.FL cs.GT</categories><journal-ref>Proceedings of MFCS 2014, Springer-Verlag Lecture Notes in
  Computer Science Vol. 8634(1): pp 486-498</journal-ref><doi>10.1007/978-3-662-44522-8_41</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parity word automata and their determinisation play an important role in
automata and game theory. We discuss a determinisation procedure for
nondeterministic parity automata through deterministic Rabin to deterministic
parity automata. We prove that the intermediate determinisation to Rabin
automata is optimal. We show that the resulting determinisation to parity
automata is optimal up to a small constant. Moreover, the lower bound refers to
the more liberal Streett acceptance. We thus show that determinisation to
Streett would not lead to better bounds than determinisation to parity. As a
side-result, this optimality extends to the determinisation of B\&quot;uchi
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5401</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5401</id><created>2014-01-21</created><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Linear MIMO Precoding in Jointly-Correlated Fading Multiple Access
  Channels with Finite Alphabet Signaling</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, accepted for ICC14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design of linear precoders for
multiple-input multiple-output (MIMO) multiple access channels (MAC). We assume
that statistical channel state information (CSI) is available at the
transmitters and consider the problem under the practical finite alphabet input
assumption. First, we derive an asymptotic (in the large-system limit) weighted
sum rate (WSR) expression for the MIMO MAC with finite alphabet inputs and
general jointly-correlated fading. Subsequently, we obtain necessary conditions
for linear precoders maximizing the asymptotic WSR and propose an iterative
algorithm for determining the precoders of all users. In the proposed
algorithm, the search space of each user for designing the precoding matrices
is its own modulation set. This significantly reduces the dimension of the
search space for finding the precoding matrices of all users compared to the
conventional precoding design for the MIMO MAC with finite alphabet inputs,
where the search space is the combination of the modulation sets of all users.
As a result, the proposed algorithm decreases the computational complexity for
MIMO MAC precoding design with finite alphabet inputs by several orders of
magnitude. Simulation results for finite alphabet signalling indicate that the
proposed iterative algorithm achieves significant performance gains over
existing precoder designs, including the precoder design based on the Gaussian
input assumption, in terms of both the sum rate and the coded bit error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5407</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5407</id><created>2014-01-21</created><authors><author><keyname>Xu</keyname><forenames>J</forenames></author><author><keyname>Wickramarathne</keyname><forenames>TL</forenames></author><author><keyname>Grey</keyname><forenames>EK</forenames></author><author><keyname>Steinhaeuser</keyname><forenames>K</forenames></author><author><keyname>Keller</keyname><forenames>R</forenames></author><author><keyname>Drake</keyname><forenames>J</forenames></author><author><keyname>Chawla</keyname><forenames>N</forenames></author><author><keyname>Lodge</keyname><forenames>DM</forenames></author></authors><title>Patterns of Ship-borne Species Spread: A Clustering Approach for Risk
  Assessment and Management of Non-indigenous Species Spread</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of non-indigenous species (NIS) through the global shipping
network (GSN) has enormous ecological and economic cost throughout the world.
Previous attempts at quantifying NIS invasions have mostly taken &quot;bottom-up&quot;
approaches that eventually require the use of multiple simplifying assumptions
due to insufficiency and/or uncertainty of available data. By modeling implicit
species exchanges via a graph abstraction that we refer to as the Species Flow
Network (SFN), a different approach that exploits the power of network science
methods in extracting knowledge from largely incomplete data is presented.
Here, coarse-grained species flow dynamics are studied via a graph clustering
approach that decomposes the SFN to clusters of ports and inter-cluster
connections. With this decomposition of ports in place, NIS flow among clusters
can be very efficiently reduced by enforcing NIS management on a few chosen
inter-cluster connections. Furthermore, efficient NIS management strategy for
species exchanges within a cluster (often difficult due higher rate of travel
and pathways) are then derived in conjunction with ecological and environmental
aspects that govern the species establishment. The benefits of the presented
approach include robustness to data uncertainties, implicit incorporation of
&quot;stepping-stone&quot; spread of invasive species, and decoupling of species spread
and establishment risk estimation. Our analysis of a multi-year (1997--2006)
GSN dataset using the presented approach shows the existence of a few large
clusters of ports with higher intra-cluster species flow that are fairly stable
over time. Furthermore, detailed investigations were carried out on vessel
types, ports, and inter-cluster connections. Finally, our observations are
discussed in the context of known NIS invasions and future research directions
are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5424</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5424</id><created>2014-01-21</created><authors><author><keyname>Hayes</keyname><forenames>Roy</forenames></author><author><keyname>Beling</keyname><forenames>Peter</forenames></author><author><keyname>Scherer</keyname><forenames>William</forenames></author></authors><title>Real Time Strategy Language</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real Time Strategy (RTS) games provide complex domain to test the latest
artificial intelligence (AI) research. In much of the literature, AI systems
have been limited to playing one game. Although, this specialization has
resulted in stronger AI gaming systems it does not address the key concerns of
AI researcher. AI researchers seek the development of AI agents that can
autonomously interpret learn, and apply new knowledge. To achieve human level
performance, current AI systems rely on game specific knowledge of an expert.
The paper presents the full RTS language in hopes of shifting the current
research focus to the development of general RTS agents. General RTS agents are
AI gaming systems that can play any RTS games, defined in the RTS language.
This prevents game specific knowledge from being hard coded into the system,
thereby facilitating research that addresses the fundamental concerns of
artificial intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5433</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5433</id><created>2014-01-21</created><authors><author><keyname>Boumahdi</keyname><forenames>Fatima</forenames></author><author><keyname>Chalal</keyname><forenames>Rachid</forenames></author></authors><title>SoaDssPm: A new Service-Oriented Architecture of the decision support
  system for the Project Management</title><categories>cs.SE</categories><comments>International Conference on Control, Engineering Information
  Technology (CEIT'13)</comments><journal-ref>Economics &amp; Strategic Management of Business Process (ESMB) vol 1;
  pp. 6-10, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents an architecture for the Project Management, which is
defined using the concepts behind ServiceOriented and Decision Support System.
The framework described, denominated as SoaDssPm, represents the following: a
coherent solution to the problem of control Project Management the existing gap
between the real execution of Project Management by describing the business
process and relationships required by a SOA solution, and its objectives
representation, in which the decisional aspects determine the final shape of
the system, providing decision support to the identified business processes and
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5444</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5444</id><created>2014-01-21</created><authors><author><keyname>Maleh</keyname><forenames>Ray</forenames></author><author><keyname>Boyle</keyname><forenames>Frank A.</forenames></author></authors><title>Exploiting Spectral Leakage for Spectrogram Frequency Super-resolution</title><categories>cs.IT math.IT</categories><comments>Presented at the 2013 Asilomar Conference on Signals, Systems &amp;
  Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectrogram is a classical DSP tool used to view signals in both time and
frequency. Unfortunately, the Heisenberg Uncertainty Principal limits our
ability to use them for detecting and measuring narrowband signal modulation in
wideband environments. On a spectrogram, instantaneous frequency can only be
measured to the nearest bin without additional interpolation. This work
presents a novel technique for extracting higher accuracy frequency estimates.
Whereas most practitioners seek to suppress spectral leakage, we use mismatched
windows to exploit such artifacts in order to produce super-resolved spectral
displays. We present a derivation of our methodology and exhibit several
interesting examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5465</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5465</id><created>2014-01-21</created><updated>2014-02-26</updated><authors><author><keyname>Ming</keyname><forenames>Zijian</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author></authors><title>BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data generation is a key issue in big data benchmarking that aims to generate
application-specific data sets to meet the 4V requirements of big data.
Specifically, big data generators need to generate scalable data (Volume) of
different types (Variety) under controllable generation rates (Velocity) while
keeping the important characteristics of raw data (Veracity). This gives rise
to various new challenges about how we design generators efficiently and
successfully. To date, most existing techniques can only generate limited types
of data and support specific big data systems such as Hadoop. Hence we develop
a tool, called Big Data Generator Suite (BDGS), to efficiently generate
scalable big data while employing data models derived from real data to
preserve data veracity. The effectiveness of BDGS is demonstrated by developing
six data generators covering three representative data types (structured,
semi-structured and unstructured) and three data sources (text, graph, and
table data).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5509</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5509</id><created>2014-01-21</created><authors><author><keyname>Boulaalam</keyname><forenames>Abdelhak</forenames></author><author><keyname>Nfaoui</keyname><forenames>El Habib</forenames></author><author><keyname>Beqqali</keyname><forenames>Omar El</forenames></author></authors><title>Intelligent Product: Mobile Agent Architecture Integrating the End of
  Life Cycle (EOL) For minimizing the lunch phase PLM</title><categories>cs.OH</categories><comments>8 pages, 11 figures, The Proceeding of International Conference on
  Soft Computing and Software Engineering 2013</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 107-114, 2013, Doi: 10.7321/jscse.v3.n3.18</journal-ref><doi>10.7321/jscse.v3.n3.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the increasingly demands products that are customized, all
business activities performed along the product life cycle must be coordinated
and efficiently managed along the extended enterprise. For this, enterprise had
wanted to retain control over the whole product lifecycle especially when the
product is in use/recycling (End Of Life phase). Although there have been many
previous research works about product lifecycle management in the beginning of
life (BOL) and middle of life (MOL) phases, few addressed the end of life (EOL)
phase, in particular, when the product is at the customers. In this paper,
based on product embedded device identification (PEID) and mobile agent
technologies, and with the advent of the development of the &quot;intelligent
products&quot;, we will try to improve innovation: (a) by minimize the lunch phase,
(b) and the involvement of the customer in product lifecycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5512</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5512</id><created>2014-01-21</created><updated>2014-02-18</updated><authors><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Paturi</keyname><forenames>Ramamohan</forenames></author><author><keyname>Schneider</keyname><forenames>Stefan</forenames></author></authors><title>0-1 Integer Linear Programming with a Linear Number of Constraints</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an exact algorithm for the 0-1 Integer Linear Programming problem
with a linear number of constraints that improves over exhaustive search by an
exponential factor. Specifically, our algorithm runs in time
$2^{(1-\text{poly}(1/c))n}$ where n is the number of variables and cn is the
number of constraints. The key idea for the algorithm is a reduction to the
Vector Domination problem and a new algorithm for that subproblem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5522</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5522</id><created>2014-01-21</created><authors><author><keyname>Faverge</keyname><forenames>Mathieu</forenames></author><author><keyname>Herrmann</keyname><forenames>Julien</forenames></author><author><keyname>Langou</keyname><forenames>Julien</forenames></author><author><keyname>Lowery</keyname><forenames>Bradley</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author><author><keyname>Dongarra</keyname><forenames>Jack</forenames></author></authors><title>Designing LU-QR hybrid solvers for performance and stability</title><categories>math.NA cs.NA</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces hybrid LU-QR al- gorithms for solving dense linear
systems of the form Ax = b. Throughout a matrix factorization, these al-
gorithms dynamically alternate LU with local pivoting and QR elimination steps,
based upon some robustness criterion. LU elimination steps can be very
efficiently parallelized, and are twice as cheap in terms of floating- point
operations, as QR steps. However, LU steps are not necessarily stable, while QR
steps are always stable. The hybrid algorithms execute a QR step when a
robustness criterion detects some risk for instability, and they execute an LU
step otherwise. Ideally, the choice between LU and QR steps must have a small
computational overhead and must provide a satisfactory level of stability with
as few QR steps as possible. In this paper, we introduce several robustness
criteria and we establish upper bounds on the growth factor of the norm of the
updated matrix incurred by each of these criteria. In addition, we describe the
implementation of the hybrid algorithms through an exten- sion of the PaRSEC
software to allow for dynamic choices during execution. Finally, we analyze
both stability and performance results compared to state-of-the-art linear
solvers on parallel distributed multicore platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5528</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5528</id><created>2014-01-21</created><authors><author><keyname>Shrestha</keyname><forenames>Bharat</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Choi</keyname><forenames>Kae Won</forenames></author></authors><title>Distributed and Centralized Hybrid CSMA/CA-TDMA Schemes for Single-Hop
  Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications (TWC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The strength of carrier-sense multiple access with collision avoidance
(CSMA/CA) can be combined with that of time-division multiple access (TDMA) to
enhance the channel access performance in wireless networks such as the IEEE
802.15.4-based wireless personal area networks (WPANs). In particular, the
performance of legacy CSMA/CA-based medium access control (MAC) scheme in
congested networks can be enhanced through a hybrid CSMA/CA-TDMA scheme while
preserving the scalability property. In this paper, we present distributed and
centralized channel access models which follow the transmission strategies
based on Markov decision process (MDP) to access both contention period and
contention-free period in an intelligent way. The models consider the buffer
status as an indication of congestion provided that the offered traffic does
not exceed the channel capacity. We extend the models to consider the hidden
node collision problem encountered due to the signal attenuation caused by
channel fading. The simulation results show that the MDP-based distributed
channel access scheme outperforms the legacy slotted CSMA/CA scheme. This
scheme also works efficiently in a network consisting of heterogeneous nodes.
The centralized model outperforms the distributed model but requires the global
information of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5530</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5530</id><created>2014-01-21</created><updated>2014-02-16</updated><authors><author><keyname>Hossain</keyname><forenames>E.</forenames></author><author><keyname>Rasti</keyname><forenames>M.</forenames></author><author><keyname>Tabassum</keyname><forenames>H.</forenames></author><author><keyname>Abdelnasser</keyname><forenames>A.</forenames></author></authors><title>Evolution Towards 5G Multi-tier Cellular Wireless Networks: An
  Interference Management Perspective</title><categories>cs.NI</categories><comments>IEEE Wireless Communications Magazine, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolving fifth generation (5G) cellular wireless networks are envisioned
to overcome the fundamental challenges of existing cellular networks, e.g.,
higher data rates, excellent end-to-end performance and user-coverage in
hot-spots and crowded areas with lower latency, energy consumption and cost per
information transfer. To address these challenges, 5G systems need to adopt a
multi-tier architecture consisting of macrocells, different types of licensed
small cells, relays, and device-to-device (D2D) networks to serve users with
different quality-of-service (QoS) requirements in a spectrum and
energy-efficient manner. Starting with the visions and requirements of 5G
multi-tier networks, this article outlines the challenges of interference
management (e.g., power control, cell association) in these networks with
shared spectrum access (i.e., when the different network tiers shares the same
licensed spectrum). It is argued that the existing interference management
schemes will not be able to address the interference management problem in
prioritized 5G multi-tier networks where users in different tiers have
different priorities for channel access. In this context, a survey and
qualitative comparison of the potential existing cell association and power
control schemes is provided to demonstrate their limitations for interference
management in 5G networks. Open challenges are highlighted and guidelines are
provided to modify the existing schemes in order to overcome these limitations
and make them suitable for the emerging 5G systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5535</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5535</id><created>2014-01-21</created><updated>2014-01-23</updated><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Jiang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>Learning Mid-Level Features and Modeling Neuron Selectivity for Image
  Classification</title><categories>cs.CV cs.LG cs.NE cs.RO</categories><comments>19 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We now know that mid-level features can greatly enhance the performance of
image learning, but how to automatically learn the image features efficiently
and in an unsupervised manner is still an open question. In this paper, we
present a very efficient mid-level feature learning approach (MidFea), which
only involves simple operations such as $k$-means clustering, convolution,
pooling, vector quantization and random projection. We explain why this simple
method generates the desired features, and argue that there is no need to spend
much time in learning low-level feature extractors. Furthermore, to boost the
performance, we propose to model the neuron selectivity (NS) principle by
building an additional layer over the mid-level features before feeding the
features into the classifier. We show that the NS-layer learns
category-specific neurons with both bottom-up inference and top-down analysis,
and thus supports fast inference for a query image. We run extensive
experiments on several public databases to demonstrate that our approach can
achieve state-of-the-art performances for face recognition, gender
classification, age estimation and object categorization. In particular, we
demonstrate that our approach is more than an order of magnitude faster than
some recently proposed sparse coding based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5536</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5536</id><created>2014-01-21</created><authors><author><keyname>Dytso</keyname><forenames>Alex</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>On Discrete Alphabets for the Two-user Gaussian Interference Channel
  with One Receiver Lacking Knowledge of the Interfering Codebook</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-user information theory it is often assumed that every node in the
network possesses all codebooks used in the network. This assumption is however
impractical in distributed ad-hoc and cognitive networks. This work considers
the two- user Gaussian Interference Channel with one Oblivious Receiver
(G-IC-OR), i.e., one receiver lacks knowledge of the interfering cookbook while
the other receiver knows both codebooks. We ask whether, and if so how much,
the channel capacity of the G-IC- OR is reduced compared to that of the
classical G-IC where both receivers know all codebooks. Intuitively, the
oblivious receiver should not be able to jointly decode its intended message
along with the unintended interfering message whose codebook is unavailable. We
demonstrate that in strong and very strong interference, where joint decoding
is capacity achieving for the classical G-IC, lack of codebook knowledge does
not reduce performance in terms of generalized degrees of freedom (gDoF).
Moreover, we show that the sum-capacity of the symmetric G-IC- OR is to within
O(log(log(SNR))) of that of the classical G-IC. The key novelty of the proposed
achievable scheme is the use of a discrete input alphabet for the non-oblivious
transmitter, whose cardinality is appropriately chosen as a function of SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5543</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5543</id><created>2014-01-21</created><updated>2014-06-18</updated><authors><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Takahara</keyname><forenames>Glen</forenames></author></authors><title>Lower Bounds on the Probability of a Finite Union of Events</title><categories>math.PR cs.IT math.IT</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, lower bounds on the probability of a finite union of events
are considered, i.e. $P\left(\bigcup_{i=1}^N A_i\right)$, in terms of the
individual event probabilities $\{P(A_i), i=1,\ldots,N\}$ and the sums of the
pairwise event probabilities, i.e., $\{\sum_{j:j\neq i} P(A_i\cap A_j),
i=1,\ldots,N\}$. The contribution of this paper includes the following: (i) in
the class of all lower bounds that are established in terms of only the
$P(A_i)$'s and $\sum_{j:j\neq i} P(A_i\cap A_j)$'s, the optimal lower bound is
given numerically by solving a linear programming (LP) problem with $N^2-N+1$
variables; (ii) a new analytical lower bound is proposed based on a relaxed LP
problem, which is at least as good as the bound due to Kuai, et al.; (iii)
numerical examples are provided to illustrate the performance of the bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5546</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5546</id><created>2014-01-21</created><authors><author><keyname>Li</keyname><forenames>Chen</forenames></author></authors><title>GreenMail: Reducing Email Service's Carbon Emission with Minimum Cost</title><categories>cs.OH</categories><comments>Master's Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet services contribute a large fraction of worldwide carbon emission
nowadays, in a context of increasing number of companies tending to provide and
more and more developers use Internet services. Noticeably, a trend is those
service providers are trying to reduce their carbon emissions by utilizing
on-site or off-site renewable energy in their datacenters in order to attract
more customers. With such efforts have been paid, there are still some users
who are aggressively calling for even cleaner Internet services. For example,
over 500,000 Facebook users petitioned the social networking site to use
renewable energy to power its datacenter. However, it seems impossible for such
demand to be satisfied merely from the inside of those production datacenters,
considering the transition cost and stability. Outside the existing Internet
services, on the other hand, may easily set up a proxy service to attract those
renewable-energy-sensitive users, by 1) using carbon neutral or even
over-offsetting cloud instances to bridge the end user and traditional Internet
services; and 2) estimating and offsetting the carbon emissions from the
traditional Internet services. In our paper, we proposed GreenMail, which is a
general IMAP proxy caching system that connects email users and traditional
email services. GreenMail runs on green web hosts to cache users' emails on
green cloud instances. Besides, it offsets the carbon emitted by traditional
backend email services. With GreenMail, users could set a carbon emission
constraint and use traditional email service without breaking any code
modification of user side and email server side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5551</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5551</id><created>2014-01-21</created><updated>2014-12-22</updated><authors><author><keyname>Roozbehani</keyname><forenames>Hajir</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Algebraic Methods of Classifying Directed Graphical Models</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directed acyclic graphical models (DAGs) are often used to describe common
structural properties in a family of probability distributions. This paper
addresses the question of classifying DAGs up to an isomorphism. By considering
Gaussian densities, the question reduces to verifying equality of certain
algebraic varieties. A question of computing equations for these varieties has
been previously raised in the literature. Here it is shown that the most
natural method adds spurious components with singular principal minors, proving
a conjecture of Sullivant. This characterization is used to establish an
algebraic criterion for isomorphism, and to provide a randomized algorithm for
checking that criterion. Results are applied to produce a list of the
isomorphism classes of tree models on 4,5, and 6 nodes. Finally, some evidence
is provided to show that projectivized DAG varieties contain useful information
in the sense that their relative embedding is closely related to efficient
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5553</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5553</id><created>2014-01-21</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author></authors><title>Peer Ratings in Massive Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 1 table, 6 figures. Presented at INSNA SunBelt, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instant quality feedback in the form of online peer ratings is a prominent
feature of modern massive online social networks (MOSNs). It allows network
members to indicate their appreciation of a post, comment, photograph, etc.
Some MOSNs support both positive and negative (signed) ratings. In this study,
we rated 11 thousand MOSN member profiles and collected user responses to the
ratings. MOSN users are very sensitive to peer ratings: 33% of the subjects
visited the researcher's profile in response to rating, 21% also rated the
researcher's profile picture, and 5% left a text comment. The grades left by
the subjects are highly polarized: out of the six available grades, the most
negative and the most positive are also the most popular. The grades fall into
three almost equally sized categories: reciprocal, generous, and stingy. We
proposed quantitative measures for generosity, reciprocity, and benevolence,
and analyzed them with respect to the subjects' demographics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5555</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5555</id><created>2014-01-22</created><updated>2014-01-30</updated><authors><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Interference Statistics and Capacity Analysis for Uplink Transmission in
  Two-Tier Small Cell Networks: A Geometric Probability Approach</title><categories>cs.IT cs.NI math.IT math.ST stat.TH</categories><comments>We have withdrawn the paper due to some limitations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell networks are evolving as an economically viable solution to
ameliorate the capacity and coverage of state-of-the-art wireless cellular
systems. Nonetheless, the dense and unplanned deployment of the small cells
(e.g., femtocells, picocells) with restricted user access significantly
increases the impact of interference on the overall network performance. To
this end, this paper presents a novel framework to derive the statistics of the
interference considering dedicated and shared spectrum access for uplink
transmissions in two-tier small cell networks such as the macrocell-femtocell
networks. The derived expressions are validated by the Monte-Carlo simulations.
Numerical results are generated to assess the feasibility of shared and
dedicated spectrum access in femtocells under varying traffic load and spectral
reuse scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5559</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5559</id><created>2014-01-22</created><authors><author><keyname>Ibrahim</keyname><forenames>Nuzulha Khilwani</forenames></author><author><keyname>Kasmuri</keyname><forenames>Emaliana</forenames></author><author><keyname>Jalil</keyname><forenames>Norazira A</forenames></author><author><keyname>Norasikin</keyname><forenames>Mohd Adili</forenames></author><author><keyname>Salam</keyname><forenames>Sazilah</forenames></author><author><keyname>Nawawi</keyname><forenames>Mohamad Riduwan Md</forenames></author></authors><title>License Plate Recognition (LPR): A Review with Experiments for Malaysia
  Case Study</title><categories>cs.CV</categories><doi>10.7321/jscse.v3.n3.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most vehicle license plate recognition use neural network techniques to
enhance its computing capability. The image of the vehicle license plate is
captured and processed to produce a textual output for further processing. This
paper reviews image processing and neural network techniques applied at
different stages which are preprocessing, filtering, feature extraction,
segmentation and recognition in such way to remove the noise of the image, to
enhance the image quality and to expedite the computing process by converting
the characters in the image into respective text. An exemplar experiment has
been done in MATLAB to show the basic process of the image processing
especially for license plate in Malaysia case study. An algorithm is adapted
into the solution for parking management system. The solution then is
implemented as proof of concept to the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5561</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5561</id><created>2014-01-22</created><authors><author><keyname>Sumathi</keyname><forenames>C. P.</forenames><affiliation>Department of Computer Science, SDNB Vaishnav College For Women, Chennai, India</affiliation></author><author><keyname>Santanam</keyname><forenames>T.</forenames><affiliation>Department of Computer Science, DG Vaishnav College For Men, Chennai, India</affiliation></author><author><keyname>Umamaheswari</keyname><forenames>G.</forenames><affiliation>Department of Computer Science, SDNB Vaishnav College For Women, Chennai, India</affiliation></author></authors><title>A Study of Various Steganographic Techniques Used for Information Hiding</title><categories>cs.MM</categories><comments>17 Pages, International Journal of Computer Science &amp; Engineering
  Survey (IJCSES) Vol.4, No.6, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography derives from the Greek word steganos, meaning covered or
secret, and graphy (writing or drawing). Steganography is a technology where
modern data compression, information theory, spread spectrum, and cryptography
technologies are brought together to satisfy the need for privacy on the
Internet. This paper is an attempt to analyse the various techniques used in
steganography and to identify areas in which this technique can be applied, so
that the human race can be benefited at large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5567</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5567</id><created>2014-01-22</created><authors><author><keyname>Tie</keyname><forenames>Lin</forenames></author></authors><title>On Controllability and Near-controllability of Multi-input Discrete-time
  Bilinear Systems in Dimension Two</title><categories>cs.SY</categories><comments>14 pages</comments><msc-class>93B05, 93C10, 93C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper completely solves the controllability problems of two-dimensional
multi-input discrete-time bilinear systems with and without drift. Necessary
and sufficient conditions for controllability, which cover the existing
results, are obtained by using an algebraic method. Furthermore, for the
uncontrollable systems, near-controllability is studied and necessary and
sufficient conditions for the systems to be nearly controllable are also
presented. Examples are provided to demonstrate the conceptions and results of
the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5577</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5577</id><created>2014-01-22</created><authors><author><keyname>Barasz</keyname><forenames>Mihaly</forenames></author><author><keyname>Christiano</keyname><forenames>Paul</forenames></author><author><keyname>Fallenstein</keyname><forenames>Benja</forenames></author><author><keyname>Herreshoff</keyname><forenames>Marcello</forenames></author><author><keyname>LaVictoire</keyname><forenames>Patrick</forenames></author><author><keyname>Yudkowsky</keyname><forenames>Eliezer</forenames></author></authors><title>Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via
  Provability Logic</title><categories>cs.GT cs.LO</categories><comments>18 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the one-shot Prisoner's Dilemma between algorithms with
read-access to one anothers' source codes, and we use the modal logic of
provability to build agents that can achieve mutual cooperation in a manner
that is robust, in that cooperation does not require exact equality of the
agents' source code, and unexploitable, meaning that such an agent never
cooperates when its opponent defects. We construct a general framework for such
&quot;modal agents&quot;, and study their properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5580</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5580</id><created>2014-01-22</created><authors><author><keyname>Banoth</keyname><forenames>Jugalkishore K.</forenames></author><author><keyname>Sircar</keyname><forenames>Pradip</forenames></author></authors><title>Polynomial Transformation Method for Non-Gaussian Noise Environment</title><categories>math.ST cs.CE stat.TH</categories><comments>4 pages</comments><journal-ref>WORLDCOMP 2011, Proc. CSC, pp. 329, Jul 18-21, 2011, Las Vegas,
  Nevada, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal processing in non-Gaussian noise environment is addressed in this
paper. For many real-life situations, the additive noise process present in the
system is found to be dominantly non-Gaussian. The problem of detection and
estimation of signals corrupted with non-Gaussian noise is difficult to track
mathematically. In this paper, we present a novel approach for optimal
detection and estimation of signals in non-Gaussian noise. It is demonstrated
that preprocessing of data by the orthogonal polynomial approximation together
with the minimum error-variance criterion converts an additive non-Gaussian
noise process into an approximation-error process which is close to Gaussian.
The Monte Carlo simulations are presented to test the Gaussian hypothesis based
on the bicoherence of a sequence. The histogram test and the kurtosis test are
carried out to verify the Gaussian hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5582</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5582</id><created>2014-01-22</created><authors><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author></authors><title>Beyond One-Way Communication: Degrees of Freedom of Multi-Way Relay MIMO
  Interference Networks</title><categories>cs.IT math.IT</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the degrees of freedom (DoF) of multi-way relay MIMO
interference networks. In particular, we consider a wireless network consisting
of 4 user nodes, each with M antennas, and one N-antenna relay node. In this
network, each user node sends one independent message to each of the other user
nodes, and there are no direct links between any two user nodes, i.e., all
communication must pass through the relay node. For this network, we show that
the symmetric DoF value per message is given by max(min(M/3,N/7),min(2M/7,N/6))
normalized by space dimensions, i.e., piecewise linear depending on M and N
alternatively. While the information theoretic DoF upper bound is established
for every M and N, the achievability relying on linear signal subspace
alignment is established in the spatially-normalized sense in general. In
addition, by deactivating 4 messages to form a two-way relay MIMO X channel, we
also present the DoF result in the similar piecewise linear type. The central
new insight to emerge from this work is the notion of inter-user signal
subspace alignment incorporating the idea of network coding, which is the key
to achieve the optimal DoF for multi-way relay interference networks. Moreover,
this work also settles the feasibility of linear interference alignment that
extends the feasibility framework from one-way to multi-way relay interference
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5583</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5583</id><created>2014-01-22</created><authors><author><keyname>Brubach</keyname><forenames>Brian</forenames></author></authors><title>Improved Online Square-into-Square Packing</title><categories>cs.CG</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show an improved bound and new algorithm for the online
square-into-square packing problem. This two-dimensional packing problem
involves packing an online sequence of squares into a unit square container
without any two squares overlapping. The goal is to find the largest area
$\alpha$ such that any set of squares with total area $\alpha$ can be packed.
We show an algorithm that can pack any set of squares with total area $\alpha
\leq 3/8$ into a unit square in an online setting, improving the previous bound
of $11/32$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5589</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5589</id><created>2014-01-22</created><authors><author><keyname>Odaibo</keyname><forenames>Stephen G.</forenames></author></authors><title>The Gabor-Einstein Wavelet: A Model for the Receptive Fields of V1 to MT
  Neurons</title><categories>q-bio.NC cs.CV physics.bio-ph</categories><comments>40 pages, 13 Figures. We presented a portion of this work in various
  parts at the National Medical Association's 111th Annual Convention and
  Scientific Assembly in Toronto Ontario, Canada (Jul. 2013); at the 23rd
  Annual Washington Retina Symposium in Washington D.C., U.S.A. (Oct. 2013);
  and at the Society for Neuroscience's 43rd Annual Meeting in San Diego
  California, U.S.A. (Nov. 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our visual system is astonishingly efficient at detecting moving objects.
This process is mediated by the neurons which connect the primary visual cortex
(V1) to the middle temporal (MT) area. Interestingly, since Kuffler's
pioneering experiments on retinal ganglion cells, mathematical models have been
vital for advancing our understanding of the receptive fields of visual
neurons. However, existing models were not designed to describe the most
salient attributes of the highly specialized neurons in the V1 to MT motion
processing stream; and they have not been able to do so. Here, we introduce the
Gabor-Einstein wavelet, a new family of functions for representing the
receptive fields of V1 to MT neurons. We show that the way space and time are
mixed in the visual cortex is analogous to the way they are mixed in the
special theory of relativity (STR). Hence we constrained the Gabor-Einstein
model by requiring: (i) relativistic-invariance of the wave carrier, and (ii)
the minimum possible number of parameters. From these two constraints, the sinc
function emerged as a natural descriptor of the wave carrier. The particular
distribution of lowpass to bandpass temporal frequency filtering properties of
V1 to MT neurons (Foster et al 1985; DeAngelis et al 1993b; Hawken et al 1996)
is clearly explained by the Gabor-Einstein basis. Furthermore, it does so in a
manner innately representative of the motion-processing stream's neuronal
hierarchy. Our analysis and computer simulations show that the distribution of
temporal frequency filtering properties along the motion processing stream is a
direct effect of the way the brain jointly encodes space and time. We uncovered
this fundamental link by demonstrating that analogous mathematical structures
underlie STR and joint cortical spacetime encoding. This link will provide new
physiological insights into how the brain represents visual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5602</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5602</id><created>2014-01-22</created><authors><author><keyname>Dias</keyname><forenames>Fabio</forenames><affiliation>LIGM</affiliation></author><author><keyname>Cousty</keyname><forenames>Jean</forenames><affiliation>LIGM</affiliation></author><author><keyname>Najman</keyname><forenames>Laurent</forenames><affiliation>LIGM</affiliation></author></authors><title>Dimensional operators for mathematical morphology on simplicial
  complexes</title><categories>cs.DM math.AT</categories><comments>Pattern Recognition Letters (2014) To appear</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the framework of mathematical morphology on simplicial
complex spaces. Simplicial complexes are widely used to represent
multidimensional data, such as meshes, that are two dimensional complexes, or
graphs, that can be interpreted as one dimensional complexes. Mathematical
morphology is one of the most powerful frameworks for image processing,
including the processing of digital structures, and is heavily used for many
applications. However, mathematical morphology operators on simplicial complex
spaces is not a concept fully developed in the literature. Specifically, we
explore properties of the dimensional operators, small, versatile operators
that can be used to define new operators on simplicial complexes, while
maintaining properties from mathematical morphology. These operators can also
be used to recover many morphological operators from the literature. Matlab
code and additional material, including the proofs of the original properties,
are freely available at
\url{https://code.google.com/p/math-morpho-simplicial-complexes.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5607</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5607</id><created>2014-01-22</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Quality Indicators for Collective Systems Resilience</title><categories>cs.CY</categories><comments>In submission(revised version) to Technological Forecasting &amp; Social
  Change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resilience is widely recognized as an important design goal though it is one
that seems to escape a general and consensual understanding. Often mixed up
with other system attributes; traditionally used with different meanings in as
many different disciplines; sought or applied through diverse approaches in
various application domains, resilience in fact is a multi-attribute property
that implies a number of constitutive abilities. To further complicate the
matter, resilience is not an absolute property but rather it is the result of
the match between a system, its current condition, and the environment it is
set to operate in. In this paper we discuss this problem and provide a
definition of resilience as a property measurable as a system-environment fit.
This brings to the foreground the dynamic nature of resilience as well as its
hard dependence on the context. A major problem becomes then that, being a
dynamic figure, resilience cannot be assessed in absolute terms. As a way to
partially overcome this obstacle, in this paper we provide a number of
indicators of the quality of resilience. Our focus here is that of collective
systems, namely those systems resulting from the union of multiple individual
parts, sub-systems, or organs. Through several examples of such systems we
observe how our indicators provide insight, at least in the cases at hand, on
design flaws potentially affecting the efficiency of the resilience strategies.
A number of conjectures are finally put forward to associate our indicators
with factors affecting the quality of resilience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5612</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5612</id><created>2014-01-22</created><authors><author><keyname>Louati</keyname><forenames>Aymen</forenames></author><author><keyname>Jerad</keyname><forenames>Chadlia</forenames></author><author><keyname>Barkaoui</keyname><forenames>Kamel</forenames></author></authors><title>Formalization and Verification of Hierarchical Use of Interaction
  Overview Diagrams Using Timing Diagrams</title><categories>cs.SE cs.LO</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to its graphical notation and simplicity, Unified Modeling Language
(UML) is a de facto standard and a widespread language used in both industry
and academia, despite the fact that its semantics is still informal. The
Interaction Overview Diagram (IOD) is introduced in UML2; it allows the
specification of the behavior in the hierarchical way. This paper is a
contribution towards a formal dynamic semantics of UML2. We start by
formalizing the Hierarchical use of IOD. Afterward, we complete the mapping of
IOD, Sequence Diagrams and Timing Diagrams into Hierarchical Colored Petri Nets
(HCPNs) using the Timed colored Petri Nets (timed CP-net). Our approach helps
designers to get benefits from abstraction as well as refinement at more than
two levels of hierarchy which reduces verification complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5632</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5632</id><created>2014-01-22</created><authors><author><keyname>Krishnaswamy</keyname><forenames>Manoj</forenames></author><author><keyname>Kumar</keyname><forenames>G. Hemantha</forenames></author></authors><title>Enhancing Template Security of Face Biometrics by Using Edge Detection
  and Hashing</title><categories>cs.CV</categories><comments>11 pages, 13 figures, Journal. arXiv admin note: text overlap with
  arXiv:1307.7474 by other authors</comments><journal-ref>International Journal of Information Processing, 7(4), 11-20,
  2013, ISSN : 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the issues of using edge detection techniques on
facial images to produce cancellable biometric templates and a novel method for
template verification against tampering. With increasing use of biometrics,
there is a real threat for the conventional systems using face databases, which
store images of users in raw and unaltered form. If compromised not only it is
irrevocable, but can be misused for cross-matching across different databases.
So it is desirable to generate and store revocable templates for the same user
in different applications to prevent cross-matching and to enhance security,
while maintaining privacy and ethics. By comparing different edge detection
methods it has been observed that the edge detection based on the Roberts Cross
operator performs consistently well across multiple face datasets, in which the
face images have been taken under a variety of conditions. We have proposed a
novel scheme using hashing, for extra verification, in order to harden the
security of the stored biometric templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5636</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5636</id><created>2014-01-22</created><authors><author><keyname>Inazumi</keyname><forenames>Takanori</forenames></author><author><keyname>Washio</keyname><forenames>Takashi</forenames></author><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author><author><keyname>Suzuki</keyname><forenames>Joe</forenames></author><author><keyname>Yamamoto</keyname><forenames>Akihiro</forenames></author><author><keyname>Kawahara</keyname><forenames>Yoshinobu</forenames></author></authors><title>Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM</title><categories>stat.ML cs.LG</categories><comments>10 pages. A longer version of our UAI2011 paper (Inazumi et al.,
  2011). arXiv admin note: text overlap with arXiv:1202.3736</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering causal relations among observed variables in a given data set is
a major objective in studies of statistics and artificial intelligence.
Recently, some techniques to discover a unique causal model have been explored
based on non-Gaussianity of the observed data distribution. However, most of
these are limited to continuous data. In this paper, we present a novel causal
model for binary data and propose an efficient new approach to deriving the
unique causal model governing a given binary data set under skew distributions
of external binary noises. Experimental evaluation shows excellent performance
for both artificial and real world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5638</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5638</id><created>2014-01-22</created><authors><author><keyname>Youcef</keyname><forenames>Dahmani</forenames></author><author><keyname>Maamar</keyname><forenames>Hamri</forenames></author></authors><title>Specification of the State Lifetime in the DEVS Formalism by Fuzzy
  Controller</title><categories>cs.OH</categories><comments>IJAIT International Journal of Advanced Information Technology
  (IJAIT) Vol. 3, No.2, April 2013</comments><doi>10.5121/ijait.2013.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to develop a new approach to assess the duration of state in
the DEVS formalism by fuzzy controller. The idea is to define a set of fuzzy
rules obtained from observers or expert knowledge and to specify a fuzzy model
which computes this duration, this latter is fed into the simulator to specify
the new value in the model. In conventional model, each state is defined by a
mean lifetime value whereas our method, calculates for each state the new
lifetime according to inputs values. A wildfire case study is presented at the
end of the paper. It is a challenging task due to its complex behavior,
dynamical weather condition, and various variables involved. A global
specification of the fuzzy controller and the forest fire model are presented
in the DEVS formalism and comparison between conventional and fuzzy method is
illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5644</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5644</id><created>2014-01-22</created><authors><author><keyname>Sahmoudi</keyname><forenames>Issam</forenames></author><author><keyname>Froud</keyname><forenames>Hanane</forenames></author><author><keyname>Lachkar</keyname><forenames>Abdelmonaime</forenames></author></authors><title>A new keyphrases extraction method based on suffix tree data structure
  for arabic documents clustering</title><categories>cs.CL cs.IR</categories><comments>17 pages, 3 figures</comments><acm-class>H.2.3</acm-class><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.5, No.6, December 2013</journal-ref><doi>10.5121/ijdms.2013.5602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document Clustering is a branch of a larger area of scientific study known as
data mining .which is an unsupervised classification using to find a structure
in a collection of unlabeled data. The useful information in the documents can
be accompanied by a large amount of noise words when using Full Text
Representation, and therefore will affect negatively the result of the
clustering process. So it is with great need to eliminate the noise words and
keeping just the useful information in order to enhance the quality of the
clustering results. This problem occurs with different degree for any language
such as English, European, Hindi, Chinese, and Arabic Language. To overcome
this problem, in this paper, we propose a new and efficient Keyphrases
extraction method based on the Suffix Tree data structure (KpST), the extracted
Keyphrases are then used in the clustering process instead of Full Text
Representation. The proposed method for Keyphrases extraction is language
independent and therefore it may be applied to any language. In this
investigation, we are interested to deal with the Arabic language which is one
of the most complex languages. To evaluate our method, we conduct an
experimental study on Arabic Documents using the most popular Clustering
approach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with
seven linkage techniques and a variety of distance functions and similarity
measures to perform Arabic Document Clustering task. The obtained results show
that our method for extracting Keyphrases increases the quality of the
clustering results. We propose also to study the effect of using the stemming
for the testing dataset to cluster it with the same documents clustering
techniques and similarity/distance measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5648</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5648</id><created>2014-01-22</created><authors><author><keyname>Rocha</keyname><forenames>Luis Enrique Correa</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Random walk centrality for temporal networks</title><categories>physics.soc-ph cs.SI</categories><comments>main text + supplementary material</comments><journal-ref>New Journal of Physics 16 063023 (2014)</journal-ref><doi>10.1088/1367-2630/16/6/063023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes can be ranked according to their relative importance within the
network. Ranking algorithms based on random walks are particularly useful
because they connect topological and diffusive properties of the network.
Previous methods based on random walks, as for example the PageRank, have
focused on static structures. However, several realistic networks are indeed
dynamic, meaning that their structure changes in time. In this paper, we
propose a centrality measure for temporal networks based on random walks which
we call TempoRank. While in a static network, the stationary density of the
random walk is proportional to the degree or the strength of a node, we find
that in temporal networks, the stationary density is proportional to the
in-strength of the so-called effective network. The stationary density also
depends on the sojourn probability q which regulates the tendency of the walker
to stay in the node. We apply our method to human interaction networks and show
that although it is important for a node to be connected to another node with
many random walkers at the right moment (one of the principles of the
PageRank), this effect is negligible in practice when the time order of link
activation is included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5657</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5657</id><created>2014-01-22</created><authors><author><keyname>Kurdej</keyname><forenames>Marek</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Moras</keyname><forenames>Julien</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Cherfaoui</keyname><forenames>V&#xe9;ronique</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Bonnifait</keyname><forenames>Philippe</forenames><affiliation>HEUDIASYC</affiliation></author></authors><title>Enhancing Mobile Object Classification Using Geo-referenced Maps and
  Evidential Grids</title><categories>cs.RO</categories><comments>6 pp. arXiv admin note: substantial text overlap with arXiv:1207.1016</comments><proxy>ccsd</proxy><journal-ref>IEEE/RSJ International Conference on Intelligent Robots and
  Systems. 5th Workshop on Planning, Perception and Navigation for Intelligent
  Vehicles, Tokyo : Japan (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidential grids have recently shown interesting properties for mobile object
perception. Evidential grids are a generalisation of Bayesian occupancy grids
using Dempster- Shafer theory. In particular, these grids can handle
efficiently partial information. The novelty of this article is to propose a
perception scheme enhanced by geo-referenced maps used as an additional source
of information, which is fused with a sensor grid. The paper presents the key
stages of such a data fusion process. An adaptation of conjunctive combination
rule is presented to refine the analysis of the conflicting information. The
method uses temporal accumulation to make the distinction between stationary
and mobile objects, and applies contextual discounting for modelling
information obsolescence. As a result, the method is able to better
characterise the occupied cells by differentiating, for instance, moving
objects, parked cars, urban infrastructure and buildings. Experiments carried
out on real- world data illustrate the benefits of such an approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5674</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5674</id><created>2014-01-18</created><authors><author><keyname>S&#xe1;nchez-Mart&#xed;nez</keyname><forenames>Felipe</forenames></author><author><keyname>Carrasco</keyname><forenames>Rafael C.</forenames></author><author><keyname>Mart&#xed;nez-Prieto</keyname><forenames>Miguel A.</forenames></author><author><keyname>Adiego</keyname><forenames>Joaquin</forenames></author></authors><title>Generalized Biwords for Bitext Compression and Translation Spotting</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  389-418, 2012</journal-ref><doi>10.1613/jair.3500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large bilingual parallel texts (also known as bitexts) are usually stored in
a compressed form, and previous work has shown that they can be more
efficiently compressed if the fact that the two texts are mutual translations
is exploited. For example, a bitext can be seen as a sequence of biwords
---pairs of parallel words with a high probability of co-occurrence--- that can
be used as an intermediate representation in the compression process. However,
the simple biword approach described in the literature can only exploit
one-to-one word alignments and cannot tackle the reordering of words. We
therefore introduce a generalization of biwords which can describe multi-word
expressions and reorderings. We also describe some methods for the binary
compression of generalized biword sequences, and compare their performance when
different schemes are applied to the extraction of the biword sequence. In
addition, we show that this generalization of biwords allows for the
implementation of an efficient algorithm to look on the compressed bitext for
words or text segments in one of the texts and retrieve their counterpart
translations in the other text ---an application usually referred to as
translation spotting--- with only some minor modifications in the compression
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5675</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5675</id><created>2014-01-22</created><authors><author><keyname>So&#xf3;s</keyname><forenames>S&#xe1;ndor</forenames><affiliation>Dept. Science Policy and Scientometrics, Library and Information Centre of the Hungarian Academy of Sciences, MTA</affiliation></author></authors><title>How science maps reveal knowledge transfer: new measurement for a
  historical case</title><categories>cs.DL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling actors of science via science (overlay) maps has recently become a
popular practice in Interdisciplinarity Research (IDR). The benefits of this
toolkit have also been recognized for other areas of scientometrics, such as
the study of science dynamics. In this paper we propose novel methods of
measuring knowledge diffusion/integration based on previous applications of the
overlay methodology. New indices called Mean Overlay Distance and Overlay
Diversity Ratio, respectively, are being drawn from previous uses of the
Stirling index as the main proxy for knowledge diversification. We demonstrate
the added value of this proposal via a case study addressing the development of
a rather complex discourse in biology, usually referred to as the Species
Problem. The selected topic is known for a history connecting various research
fields and traditions, being, therefore, both an ideal and challenging case for
the study of knowledge diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5676</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5676</id><created>2014-01-22</created><updated>2014-01-24</updated><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>A Novel Proof for the DoF Region of the MIMO Broadcast Channel with No
  CSIT</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to the IEEE International Symposium on Information
  Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new proof for the degrees of freedom (DoF) region of the
K-user multiple-input multiple-output (MIMO) broadcast channel (BC) with no
channel state information at the transmitter (CSIT) and perfect channel state
information at the receivers (CSIR) is provided. Based on this proof, the
capacity region of a certain class of MIMO BC with channel distribution
information at the transmitter (CDIT) and perfect CSIR is derived. Finally, an
outer bound for the DoF region of the MIMO interference channel (IC) with no
CSIT is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5677</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5677</id><created>2014-01-22</created><updated>2014-04-29</updated><authors><author><keyname>Li</keyname><forenames>Jianwen</forenames></author><author><keyname>Pu</keyname><forenames>Geguang</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author><author><keyname>He</keyname><forenames>Jifeng</forenames></author></authors><title>Fast LTL Satisfiability Checking by SAT Solvers</title><categories>cs.LO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satisfiability checking for Linear Temporal Logic (LTL) is a fundamental step
in checking for possible errors in LTL assertions. Extant LTL satisfiability
checkers use a variety of different search procedures. With the sole exception
of LTL satisfiability checking based on bounded model checking, which does not
provide a complete decision procedure, LTL satisfiability checkers have not
taken advantage of the remarkable progress over the past 20 years in Boolean
satisfiability solving. In this paper, we propose a new LTL
satisfiability-checking framework that is accelerated using a Boolean SAT
solver. Our approach is based on the variant of the \emph{obligation-set
method}, which we proposed in earlier work. We describe here heuristics that
allow the use of a Boolean SAT solver to analyze the obligations for a given
LTL formula. The experimental evaluation indicates that the new approach
provides a a significant performance advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5686</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5686</id><created>2014-01-21</created><authors><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Mohamead</keyname><forenames>Hoda K.</forenames></author><author><keyname>Deraz</keyname><forenames>Sally S.</forenames></author></authors><title>Increasing Server Availability for Overall System Security: A Preventive
  Maintenance Approach Based on Failure Prediction</title><categories>cs.DC cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1206.1534 by other authors</comments><journal-ref>Ain Shams Journal of Electrical Engineering (ASJEE), Volume 1,
  Issue 2, pp 135-143 (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Server Availability (SA) is an important measure of overall systems security.
Important security systems rely on the availability of their hosting servers to
deliver critical security services. Many of these servers offer management
interface through web mainly using an Apache server. This paper investigates
the increase of Server Availability by the use of Artificial Neural Networks
(ANN) to predict software aging phenomenon. Several resource usage data is
collected and analyzed on a typical long-running software system (a web
server). A Multi-Layer Perceptron feed forward Artificial Neural Network was
trained on an Apache web server data-set to predict future server resource
exhaustion through uni-variate time series forecasting. The results were
benchmarked against those obtained from non-parametric statistical techniques,
parametric time series models and empirical modeling techniques reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5688</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5688</id><created>2014-01-22</created><updated>2014-04-02</updated><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Capacities and Capacity-Achieving Decoders for Various Fingerprinting
  Games</title><categories>cs.IT cs.CR math.IT</categories><comments>13 pages, 2 figures</comments><journal-ref>ACM Workshop on Information Hiding and Multimedia Security
  (IH&amp;MMSec), pp. 123-134, 2014</journal-ref><doi>10.1145/2600918.2600925</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining an information-theoretic approach to fingerprinting with a more
constructive, statistical approach, we derive new results on the fingerprinting
capacities for various informed settings, as well as new log-likelihood
decoders with provable code lengths that asymptotically match these capacities.
The simple decoder built against the interleaving attack is further shown to
achieve the simple capacity for unknown attacks, and is argued to be an
improved version of the recently proposed decoder of Oosterwijk et al. With
this new universal decoder, cut-offs on the bias distribution function can
finally be dismissed.
  Besides the application of these results to fingerprinting, a direct
consequence of our results to group testing is that (i) a simple decoder
asymptotically requires a factor 1.44 more tests to find defectives than a
joint decoder, and (ii) the simple decoder presented in this paper provably
achieves this bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5690</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5690</id><created>2014-01-22</created><updated>2014-07-31</updated><authors><author><keyname>Kobel</keyname><forenames>Alexander</forenames></author><author><keyname>Sagraloff</keyname><forenames>Michael</forenames></author></authors><title>On the Complexity of Computing with Planar Algebraic Curves</title><categories>cs.SC cs.NA math.AG math.GT math.NA</categories><comments>41 pages, 1 figure</comments><msc-class>13P15, 68W30, 14Q05 (Primary) 12Y05 (Secondary)</msc-class><acm-class>G.1.5; I.1.2; F.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give improved bounds for the computational complexity of
computing with planar algebraic curves. More specifically, for arbitrary
coprime polynomials $f$, $g \in \mathbb{Z}[x,y]$ and an arbitrary polynomial $h
\in \mathbb{Z}[x,y]$, each of total degree less than $n$ and with integer
coefficients of absolute value less than $2^\tau$, we show that each of the
following problems can be solved in a deterministic way with a number of bit
operations bounded by $\tilde{O}(n^6+n^5\tau)$, where we ignore polylogarithmic
factors in $n$ and $\tau$:
  (1) The computation of isolating regions in $\mathbb{C}^2$ for all complex
solutions of the system $f = g = 0$,
  (2) the computation of a separating form for the solutions of $f = g = 0$,
  (3) the computation of the sign of $h$ at all real valued solutions of $f = g
= 0$, and
  (4) the computation of the topology of the planar algebraic curve
$\mathcal{C}$ defined as the real valued vanishing set of the polynomial $f$.
  Our bound improves upon the best currently known bounds for the first three
problems by a factor of $n^2$ or more and closes the gap to the
state-of-the-art randomized complexity for the last problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5693</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5693</id><created>2014-01-15</created><authors><author><keyname>Cohn</keyname><forenames>Trevor Anthony</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Sentence Compression as Tree Transduction</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  637-674, 2009</journal-ref><doi>10.1613/jair.2655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tree-to-tree transduction method for sentence
compression. Our model is based on synchronous tree substitution grammar, a
formalism that allows local distortion of the tree topology and can thus
naturally capture structural mismatches. We describe an algorithm for decoding
in this framework and show how the model can be trained discriminatively within
a large margin framework. Experimental results on sentence compression bring
significant improvements over a state-of-the-art model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5694</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5694</id><created>2014-01-15</created><authors><author><keyname>Pado</keyname><forenames>Sebastian</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Cross-lingual Annotation Projection for Semantic Roles</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  307-340, 2009</journal-ref><doi>10.1613/jair.2863</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the task of automatically inducing role-semantic
annotations in the FrameNet paradigm for new languages. We propose a general
framework that is based on annotation projection, phrased as a graph
optimization problem. It is relatively inexpensive and has the potential to
reduce the human effort involved in creating role-semantic resources. Within
this framework, we present projection models that exploit lexical and syntactic
information. We provide an experimental evaluation on an English-German
parallel corpus which demonstrates the feasibility of inducing high-precision
German semantic role annotation both for manually and automatically annotated
English data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5695</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5695</id><created>2014-01-15</created><authors><author><keyname>Naseem</keyname><forenames>Tahira</forenames></author><author><keyname>Snyder</keyname><forenames>Benjamin</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author></authors><title>Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 36, pages
  341-385, 2009</journal-ref><doi>10.1613/jair.2843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5696</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5696</id><created>2014-01-15</created><authors><author><keyname>Yates</keyname><forenames>Alexander Pieter</forenames></author><author><keyname>Etzioni</keyname><forenames>Oren</forenames></author></authors><title>Unsupervised Methods for Determining Object and Relation Synonyms on the
  Web</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  255-296, 2009</journal-ref><doi>10.1613/jair.2772</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of identifying synonymous relations and objects, or synonym
resolution, is critical for high-quality information extraction. This paper
investigates synonym resolution in the context of unsupervised information
extraction, where neither hand-tagged training examples nor domain knowledge is
available. The paper presents a scalable, fully-implemented system that runs in
O(KN log N) time in the number of extractions, N, and the maximum number of
synonyms per word, K. The system, called Resolver, introduces a probabilistic
relational model for predicting whether two strings are co-referential based on
the similarity of the assertions containing them. On a set of two million
assertions extracted from the Web, Resolver resolves objects with 78% precision
and 68% recall, and resolves relations with 90% precision and 35% recall.
Several variations of resolvers probabilistic model are explored, and
experiments demonstrate that under appropriate conditions these variations can
improve F1 by 5%. An extension to the basic Resolver system allows it to handle
polysemous names with 97% precision and 95% recall on a data set from the TREC
corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5697</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5697</id><created>2014-01-15</created><authors><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author><author><keyname>Markovitch</keyname><forenames>Shaul</forenames></author></authors><title>Wikipedia-based Semantic Interpretation for Natural Language Processing</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  443-498, 2009</journal-ref><doi>10.1613/jair.2669</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adequate representation of natural language semantics requires access to vast
amounts of common sense and domain-specific world knowledge. Prior work in the
field was based on purely statistical techniques that did not make use of
background knowledge, on limited lexicographic knowledge bases such as WordNet,
or on huge manual efforts such as the CYC project. Here we propose a novel
method, called Explicit Semantic Analysis (ESA), for fine-grained semantic
interpretation of unrestricted natural language texts. Our method represents
meaning in a high-dimensional space of concepts derived from Wikipedia, the
largest encyclopedia in existence. We explicitly represent the meaning of any
text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our
method on text categorization and on computing the degree of semantic
relatedness between fragments of natural language text. Using ESA results in
significant improvements over the previous state of the art in both tasks.
Importantly, due to the use of natural concepts, the ESA model is easy to
explain to human users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5698</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5698</id><created>2014-01-15</created><authors><author><keyname>Li</keyname><forenames>Yifan</forenames></author><author><keyname>Musilek</keyname><forenames>Petr</forenames></author><author><keyname>Reformat</keyname><forenames>Marek</forenames></author><author><keyname>Wyard-Scott</keyname><forenames>Loren</forenames></author></authors><title>Identification of Pleonastic It Using the Web</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  339-389, 2009</journal-ref><doi>10.1613/jair.2622</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a significant minority of cases, certain pronouns, especially the pronoun
it, can be used without referring to any specific entity. This phenomenon of
pleonastic pronoun usage poses serious problems for systems aiming at even a
shallow understanding of natural language texts. In this paper, a novel
approach is proposed to identify such uses of it: the extrapositional cases are
identified using a series of queries against the web, and the cleft cases are
identified using a simple set of syntactic rules. The system is evaluated with
four sets of news articles containing 679 extrapositional cases as well as 78
cleft constructs. The identification results are comparable to those obtained
by human efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5699</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5699</id><created>2014-01-15</created><authors><author><keyname>Tsatsaronis</keyname><forenames>George</forenames></author><author><keyname>Varlamis</keyname><forenames>Iraklis</forenames></author><author><keyname>Vazirgiannis</keyname><forenames>Michalis</forenames></author></authors><title>Text Relatedness Based on a Word Thesaurus</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  1-39, 2010</journal-ref><doi>10.1613/jair.2880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of relatedness between two fragments of text in an automated
manner requires taking into account a wide range of factors pertaining to the
meaning the two fragments convey, and the pairwise relations between their
words. Without doubt, a measure of relatedness between text segments must take
into account both the lexical and the semantic relatedness between words. Such
a measure that captures well both aspects of text relatedness may help in many
tasks, such as text retrieval, classification and clustering. In this paper we
present a new approach for measuring the semantic relatedness between words
based on their implicit semantic links. The approach exploits only a word
thesaurus in order to devise implicit semantic links between words. Based on
this approach, we introduce Omiotis, a new measure of semantic relatedness
between texts which capitalizes on the word-to-word semantic relatedness
measure (SR) and extends it to measure the relatedness between texts. We
gradually validate our method: we first evaluate the performance of the
semantic relatedness measure between individual words, covering word-to-word
similarity and relatedness, synonym identification and word analogy; then, we
proceed with evaluating the performance of our method in measuring text-to-text
semantic relatedness in two tasks, namely sentence-to-sentence similarity and
paraphrase recognition. Experimental evaluation shows that the proposed method
outperforms every lexicon-based method of semantic relatedness in the selected
tasks and the used data sets, and competes well against corpus-based and hybrid
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5700</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5700</id><created>2014-01-15</created><authors><author><keyname>S&#xe1;nchez-Mart&#xed;nez</keyname><forenames>Felipe</forenames></author><author><keyname>Forcada</keyname><forenames>Mikel L.</forenames></author></authors><title>Inferring Shallow-Transfer Machine Translation Rules from Small Parallel
  Corpora</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 34, pages
  605-635, 2009</journal-ref><doi>10.1613/jair.2735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for the automatic inference of structural
transfer rules to be used in a shallow-transfer machine translation (MT) system
from small parallel corpora. The structural transfer rules are based on
alignment templates, like those used in statistical MT. Alignment templates are
extracted from sentence-aligned parallel corpora and extended with a set of
restrictions which are derived from the bilingual dictionary of the MT system
and control their application as transfer rules. The experiments conducted
using three different language pairs in the free/open-source MT platform
Apertium show that translation quality is improved as compared to word-for-word
translation (when no transfer rules are used), and that the resulting
translation quality is close to that obtained using hand-coded transfer rules.
The method we present is entirely unsupervised and benefits from information in
the rest of modules of the MT system in which the inferred rules are applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5703</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5703</id><created>2014-01-22</created><updated>2014-12-05</updated><authors><author><keyname>Shariati</keyname><forenames>Nafiseh</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Low-Complexity Polynomial Channel Estimation in Large-Scale MIMO with
  Arbitrary Statistics</title><categories>cs.IT math.IT</categories><comments>Published at IEEE Journal of Selected Topics in Signal Processing -
  Special Issue on Signal Processing for Large-Scale MIMO Communications, 16
  pages, 14 figures</comments><doi>10.1109/JSTSP.2014.2316063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers pilot-based channel estimation in large-scale
multiple-input multiple-output (MIMO) communication systems, also known as
massive MIMO, where there are hundreds of antennas at one side of the link.
Motivated by the fact that computational complexity is one of the main
challenges in such systems, a set of low-complexity Bayesian channel
estimators, coined Polynomial ExpAnsion CHannel (PEACH) estimators, are
introduced for arbitrary channel and interference statistics. While the
conventional minimum mean square error (MMSE) estimator has cubic complexity in
the dimension of the covariance matrices, due to an inversion operation, our
proposed estimators significantly reduce this to square complexity by
approximating the inverse by a L-degree matrix polynomial. The coefficients of
the polynomial are optimized to minimize the mean square error (MSE) of the
estimate.
  We show numerically that near-optimal MSEs are achieved with low polynomial
degrees. We also derive the exact computational complexity of the proposed
estimators, in terms of the floating-point operations (FLOPs), by which we
prove that the proposed estimators outperform the conventional estimators in
large-scale MIMO systems of practical dimensions while providing a reasonable
MSEs. Moreover, we show that L needs not scale with the system dimensions to
maintain a certain normalized MSE. By analyzing different interference
scenarios, we observe that the relative MSE loss of using the low-complexity
PEACH estimators is smaller in realistic scenarios with pilot contamination. On
the other hand, PEACH estimators are not well suited for noise-limited
scenarios with high pilot power; therefore, we also introduce the
low-complexity diagonalized estimator that performs well in this regime.
Finally, we ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5707</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5707</id><created>2014-01-22</created><updated>2014-01-24</updated><authors><author><keyname>Ben-Basat</keyname><forenames>Ran</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author></authors><title>Relations between automata and the simple k-path problem</title><categories>cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a directed graph on $n$ vertices. Given an integer $k&lt;=n$, the
SIMPLE $k$-PATH problem asks whether there exists a simple $k$-path in $G$. In
case $G$ is weighted, the MIN-WT SIMPLE $k$-PATH problem asks for a simple
$k$-path in $G$ of minimal weight. The fastest currently known deterministic
algorithm for MIN-WT SIMPLE $k$-PATH by Fomin, Lokshtanov and Saurabh runs in
time $O(2.851^k\cdot n^{O(1)}\cdot \log W)$ for graphs with integer weights in
the range $[-W,W]$. This is also the best currently known deterministic
algorithm for SIMPLE k-PATH- where the running time is the same without the
$\log W$ factor. We define $L_k(n)\subseteq [n]^k$ to be the set of words of
length $k$ whose symbols are all distinct. We show that an explicit
construction of a non-deterministic automaton (NFA) of size $f(k)\cdot
n^{O(1)}$ for $L_k(n)$ implies an algorithm of running time $O(f(k)\cdot
n^{O(1)}\cdot \log W)$ for MIN-WT SIMPLE $k$-PATH when the weights are
non-negative or the constructed NFA is acyclic as a directed graph. We show
that the algorithm of Kneis et al. and its derandomization by Chen et al. for
SIMPLE $k$-PATH can be used to construct an acylic NFA for $L_k(n)$ of size
$O^*(4^{k+o(k)})$.
  We show, on the other hand, that any NFA for $L_k(n)$ must be size at least
$2^k$. We thus propose closing this gap and determining the smallest NFA for
$L_k(n)$ as an interesting open problem that might lead to faster algorithms
for MIN-WT SIMPLE $k$-PATH.
  We use a relation between SIMPLE $k$-PATH and non-deterministic xor automata
(NXA) to give another direction for a deterministic algorithm with running time
$O^*(2^k)$ for SIMPLE $k$-PATH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5709</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5709</id><created>2014-01-22</created><authors><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>Three Generalizations of Davenport-Schinzel Sequences</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new, and mostly sharp, bounds on the maximum length of certain
generalizations of Davenport-Schinzel sequences. Among the results are sharp
bounds on order-$s$ {\em double DS} sequences, for all $s$, sharp bounds on
sequences avoiding {\em catenated permutations} (aka formation free sequences),
and new lower bounds on sequences avoiding {\em zig-zagging} patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5710</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5710</id><created>2014-01-22</created><authors><author><keyname>Xia</keyname><forenames>Peng</forenames></author><author><keyname>Tu</keyname><forenames>Kun</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Jiang</keyname><forenames>Hua</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Chen</keyname><forenames>Cindy</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Who is Dating Whom: Characterizing User Behaviors of a Large Online
  Dating Site</title><categories>cs.SI cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online dating sites have become popular platforms for people to look for
potential romantic partners. It is important to understand users' dating
preferences in order to make better recommendations on potential dates. The
message sending and replying actions of a user are strong indicators for what
he/she is looking for in a potential date and reflect the user's actual dating
preferences. We study how users' online dating behaviors correlate with various
user attributes using a large real-world dateset from a major online dating
site in China. Many of our results on user messaging behavior align with
notions in social and evolutionary psychology: males tend to look for younger
females while females put more emphasis on the socioeconomic status (e.g.,
income, education level) of a potential date. In addition, we observe that the
geographic distance between two users and the photo count of users play an
important role in their dating behaviors. Our results show that it is important
to differentiate between users' true preferences and random selection. Some
user behaviors in choosing attributes in a potential date may largely be a
result of random selection. We also find that both males and females are more
likely to reply to users whose attributes come closest to the stated
preferences of the receivers, and there is significant discrepancy between a
user's stated dating preference and his/her actual online dating behavior.
These results can provide valuable guidelines to the design of a recommendation
engine for potential dates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5714</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5714</id><created>2014-01-22</created><updated>2014-01-30</updated><authors><author><keyname>Suzuki</keyname><forenames>Akira</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author></authors><title>Reconfiguration of Dominating Sets</title><categories>cs.DM math.CO</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a reconfiguration version of the dominating set problem, where a
dominating set in a graph $G$ is a set $S$ of vertices such that each vertex is
either in $S$ or has a neighbour in $S$. In a reconfiguration problem, the goal
is to determine whether there exists a sequence of feasible solutions
connecting given feasible solutions $s$ and $t$ such that each pair of
consecutive solutions is adjacent according to a specified adjacency relation.
Two dominating sets are adjacent if one can be formed from the other by the
addition or deletion of a single vertex.
  For various values of $k$, we consider properties of $D_k(G)$, the graph
consisting of a vertex for each dominating set of size at most $k$ and edges
specified by the adjacency relation. Addressing an open question posed by Haas
and Seyffarth, we demonstrate that $D_{\Gamma(G)+1}(G)$ is not necessarily
connected, for $\Gamma(G)$ the maximum cardinality of a minimal dominating set
in $G$. The result holds even when graphs are constrained to be planar, of
bounded tree-width, or $b$-partite for $b \ge 3$. Moreover, we construct an
infinite family of graphs such that $D_{\gamma(G)+1}(G)$ has exponential
diameter, for $\gamma(G)$ the minimum size of a dominating set. On the positive
side, we show that $D_{n-m}(G)$ is connected and of linear diameter for any
graph $G$ on $n$ vertices having at least $m+1$ independent edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5726</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5726</id><created>2014-01-22</created><updated>2014-04-16</updated><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Data Mining Cultural Aspects of Social Media Marketing</title><categories>cs.SI cs.SY physics.soc-ph</categories><journal-ref>Lecture Notes in Computer Science 8557: 130-143. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For marketing to function in a globalized world it must respect a diverse set
of local cultures. With marketing efforts extending to social media platforms,
the crossing of cultural boundaries can happen in an instant. In this paper we
examine how culture influences the popularity of marketing messages in social
media platforms. Text mining, automated translation and sentiment analysis
contribute largely to our research. From our analysis of 400 posts on the
localized Google+ pages of German car brands in Germany and the US, we conclude
that posting time and emotions are important predictors for reshare counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5731</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5731</id><created>2014-01-22</created><authors><author><keyname>Parra-Arnau</keyname><forenames>Javier</forenames></author><author><keyname>M&#xe1;rmol</keyname><forenames>F&#xe9;lix G&#xf3;mez</forenames></author><author><keyname>Rebollo-Monedero</keyname><forenames>David</forenames></author><author><keyname>Forn&#xe9;</keyname><forenames>Jordi</forenames></author></authors><title>Smart Deferral of Messages for Privacy Protection in Online Social
  Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the several advantages commonly attributed to social networks such as
easiness and immediacy to communicate with acquaintances and friends,
significant privacy threats provoked by unexperienced or even irresponsible
users recklessly publishing sensitive material are also noticeable. Yet, a
different, but equally hazardous privacy risk might arise from social networks
profiling the online activity of their users based on the timestamp of the
interactions between the former and the latter. In order to thwart this last
type of commonly neglected attacks, this paper presents a novel, smart deferral
mechanism for messages in online social networks. Such solution suggests
intelligently delaying certain messages posted by end users in social networks
in a way that the observed online-activity profile generated by the attacker
does not reveal any time-based sensitive information. Conducted experiments as
well as a proposed architecture implementing this approach demonstrate the
suitability and feasibility of our mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5741</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5741</id><created>2014-01-22</created><authors><author><keyname>Tib&#xe9;ly</keyname><forenames>Gergely</forenames></author><author><keyname>Pollner</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Vicsek</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Palla</keyname><forenames>Gergely</forenames></author></authors><title>Extracting tag hierarchies</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>25 pages with 21 pages of supporting information, 25 figures</comments><journal-ref>PLoS ONE 8, e84133 (2013)</journal-ref><doi>10.1371/journal.pone.0084133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tagging items with descriptive annotations or keywords is a very natural way
to compress and highlight information about the properties of the given entity.
Over the years several methods have been proposed for extracting a hierarchy
between the tags for systems with a &quot;flat&quot;, egalitarian organization of the
tags, which is very common when the tags correspond to free words given by
numerous independent people. Here we present a complete framework for automated
tag hierarchy extraction based on tag occurrence statistics. Along with
proposing new algorithms, we are also introducing different quality measures
enabling the detailed comparison of competing approaches from different
aspects. Furthermore, we set up a synthetic, computer generated benchmark
providing a versatile tool for testing, with a couple of tunable parameters
capable of generating a wide range of test beds. Beside the computer generated
input we also use real data in our studies, including a biological example with
a pre-defined hierarchy between the tags. The encouraging similarity between
the pre-defined and reconstructed hierarchy, as well as the seemingly
meaningful hierarchies obtained for other real systems indicate that tag
hierarchy extraction is a very promising direction for further research with a
great potential for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5742</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5742</id><created>2014-01-22</created><updated>2014-01-23</updated><authors><author><keyname>Braca</keyname><forenames>Paolo</forenames></author><author><keyname>Marano</keyname><forenames>Stefano</forenames></author><author><keyname>Matta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Asymptotic Performance of Adaptive Distributed Detection over Networks</title><categories>cs.IT math.IT</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines the close interplay between cooperation and adaptation for
distributed detection schemes over fully decentralized networks. The combined
attributes of cooperation and adaptation are necessary to enable networks of
detectors to continually learn from streaming data and to continually track
drifts in the state of nature when deciding in favor of one hypothesis or
another. The results in the paper establish a fundamental scaling law for the
probabilities of miss-detection and false-alarm, when the agents interact with
each other according to distributed strategies that employ constant step-sizes.
The latter are critical to enable continuous adaptation and learning. The work
establishes three key results. First, it is shown that the output of the
collaborative process at each agent has a steady-state distribution. Second, it
is shown that this distribution is asymptotically Gaussian in the slow
adaptation regime of small step-sizes. And third, by carrying out a detailed
large-deviations analysis, closed-form expressions are derived for the decaying
rates of the false-alarm and miss-detection probabilities. Interesting insights
are gained from these expressions. In particular, it is verified that as the
step-size $\mu$ decreases, the error probabilities are driven to zero
exponentially fast as functions of $1/\mu$, and that the exponents governing
the decay increase linearly in the number of agents. It is also verified that
the scaling laws governing errors of detection and errors of estimation over
networks behave very differently, with the former having an exponential decay
proportional to $1/\mu$, while the latter scales linearly with decay
proportional to $\mu$. It is shown that the cooperative strategy allows each
agent to reach the same detection performance, in terms of detection error
exponents, of a centralized stochastic-gradient solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5743</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5743</id><created>2014-01-22</created><authors><author><keyname>Amini</keyname><forenames>Alexander</forenames></author><author><keyname>Kung</keyname><forenames>Kevin</forenames></author><author><keyname>Kang</keyname><forenames>Chaogui</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>The Impact of Social Segregation on Human Mobility in Developing and
  Urbanized Regions</title><categories>cs.SI physics.soc-ph</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study leverages mobile phone data to analyze human mobility patterns in
developing countries, especially in comparison to more industrialized
countries. Developing regions, such as the Ivory Coast, are marked by a number
of factors that may influence mobility, such as less infrastructural coverage
and maturity, less economic resources and stability, and in some cases, more
cultural and language-based diversity. By comparing mobile phone data collected
from the Ivory Coast to similar data collected in Portugal, we are able to
highlight both qualitative and quantitative differences in mobility patterns -
such as differences in likelihood to travel, as well as in the time required to
travel - that are relevant to consideration on policy, infrastructure, and
economic development. Our study illustrates how cultural and linguistic
diversity in developing regions (such as Ivory Coast) can present challenges to
mobility models that perform well and were conceptualized in less culturally
diverse regions. Finally, we address these challenges by proposing novel
techniques to assess the strength of borders in a regional partitioning scheme
and to quantify the impact of border strength on mobility model accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5752</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5752</id><created>2014-01-22</created><authors><author><keyname>Ostadzadeh</keyname><forenames>Shervin</forenames></author><author><keyname>Shams</keyname><forenames>Fereidoon</forenames></author></authors><title>Towards a Software Architecture Maturity Model for Improving
  Ultra-Large-Scale Systems Interoperability</title><categories>cs.SE</categories><acm-class>D.2.9; D.2.11; D.2.12</acm-class><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 69-74, 2013</journal-ref><doi>10.7321/jscse.v3.n3.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the last two decades, software architecture has been adopted as one of
the main viable solutions to address the ever-increasing demands in the design
and development of software systems. Nevertheless, the rapidly growing
utilization of communication networks and interconnections among software
systems have introduced some critical challenges, which need to be handled in
order to fully unleash the potential of these systems. In this respect,
Ultra-Large-Scale (ULS) systems, generally considered as a system of systems,
have gained considerable attention, since their scale is incomparable to the
traditional systems. The scale of ULS systems makes drastic changes in various
aspects of system development. As a result, it requires that we broaden our
understanding of software architectures and the ways we structure them. In this
paper, we investigate the lack of an architectural maturity model framework for
ULS system interoperability, and propose an architectural maturity model
framework to improve ULS system interoperability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5753</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5753</id><created>2014-01-22</created><authors><author><keyname>Zargham</keyname><forenames>Michael</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Worst-Case Scenarios for Greedy, Centrality-Based Network Protection
  Strategies</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of allocating preventative resources to a computer network in order
to protect against the spread of viruses is addressed. Virus spreading dynamics
are described by a linearized SIS model and protection is framed by an
optimization problem which maximizes the rate at which a virus in the network
is contained given finite resources. One approach to problems of this type
involve greedy heuristics which allocate all resources to the nodes with large
centrality measures. We address the worst case performance of such greedy
algorithms be constructing networks for which these greedy allocations are
arbitrarily inefficient. An example application is presented in which such a
worst case network might arise naturally and our results are verified
numerically by leveraging recent results which allow the exact optimal solution
to be computed via geometric programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5759</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5759</id><created>2014-01-22</created><authors><author><keyname>Tavafoghi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Optimal Energy Procurement from a Strategic Seller with Private
  Renewable and Conventional Generation</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a mechanism design problem for energy procurement, when there is
one buyer and one seller, and the buyer is the mechanism designer. The seller
can generate energy from conventional (deterministic) and renewable (random)
plants, and has multi-dimensional private information which determines her
production cost. The objective is to maximize the buyer's utility under the
constraint that the seller voluntarily participates in the energy procurement
process. We show that the optimal mechanism is a menu of contracts (nonlinear
pricing) that the buyer offers to the seller, and the seller chooses one based
on her private information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5767</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5767</id><created>2014-01-22</created><updated>2014-04-23</updated><authors><author><keyname>Wang</keyname><forenames>Ligong</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author></authors><title>A refined analysis of the Poisson channel in the high-photon-efficiency
  regime</title><categories>cs.IT math.IT</categories><comments>Revised version to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the discrete-time Poisson channel under the constraint that its
average input power (in photons per channel use) must not exceed some constant
E. We consider the wideband, high-photon-efficiency extreme where E approaches
zero, and where the channel's &quot;dark current&quot; approaches zero proportionally
with E. Improving over a previously obtained first-order capacity
approximation, we derive a refined approximation, which includes the exact
characterization of the second-order term, as well as an asymptotic
characterization of the third-order term with respect to the dark current. We
also show that pulse-position modulation is nearly optimal in this regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5775</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5775</id><created>2014-01-16</created><updated>2014-05-28</updated><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author></authors><title>Trusty URIs: Verifiable, Immutable, and Permanent Digital Artifacts for
  Linked Data</title><categories>cs.CR cs.DL cs.NI</categories><comments>Small error corrected in the text (table data was correct) on page
  13: &quot;All average values are below 0.8s (0.03s for batch mode). Using Java in
  batch mode even requires only 1ms per file.&quot;</comments><acm-class>H.3.4; H.3.5</acm-class><journal-ref>Proceedings of The Semantic Web: Trends and Challenges, 11th
  International Conference, ESWC 2014, Springer</journal-ref><doi>10.1007/978-3-319-07443-6_27</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To make digital resources on the web verifiable, immutable, and permanent, we
propose a technique to include cryptographic hash values in URIs. We call them
trusty URIs and we show how they can be used for approaches like
nanopublications to make not only specific resources but their entire reference
trees verifiable. Digital artifacts can be identified not only on the byte
level but on more abstract levels such as RDF graphs, which means that
resources keep their hash values even when presented in a different format. Our
approach sticks to the core principles of the web, namely openness and
decentralized architecture, is fully compatible with existing standards and
protocols, and can therefore be used right away. Evaluation of our reference
implementations shows that these desired properties are indeed accomplished by
our approach, and that it remains practical even for very large files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5781</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5781</id><created>2014-01-22</created><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Pudl&#xe1;k</keyname><forenames>Pavel</forenames></author></authors><title>Partition Expanders</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new concept, which we call partition expanders. The basic idea
is to study quantitative properties of graphs in a slightly different way than
it is in the standard definition of expanders. While in the definition of
expanders it is required that the number of edges between any pair of
sufficiently large sets is close to the expected number, we consider partitions
and require this condition only for most of the pairs of blocks. As a result,
the blocks can be substantially smaller.
  We show that for some range of parameters, to be a partition expander a
random graph needs exponentially smaller degree than any expander would require
in order to achieve similar expanding properties.
  We apply the concept of partition expanders in communication complexity.
First, we give a PRG for the SMP model of the optimal seed length, n+O(log k).
Second, we compare the model of SMP to that of Simultaneous Two-Way
Communication, and give a new separation that is stronger both qualitatively
and quantitatively than the previously known ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5787</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5787</id><created>2014-01-22</created><updated>2014-01-26</updated><authors><author><keyname>Santana</keyname><forenames>Yeray Cachon</forenames></author></authors><title>Orthogonal Matrix in Cryptography</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work is proposed a method using orthogonal matrix transform
properties to encrypt and decrypt a message. It will be showed how to use
matrix functions to create complex encryptions. Because orthogonal matrix are
always diagonalizable on R, and the exponential of a diagonal matrix is easy to
compute, the exponential of orthogonal matrix will be used to encrypt text
messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5789</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5789</id><created>2014-01-21</created><authors><author><keyname>Tchorzewski</keyname><forenames>Jerzy</forenames></author><author><keyname>Chyzy</keyname><forenames>Emil</forenames></author></authors><title>Reaserchnig the Development of the Electrical Power System Using
  Systemically Evolutionary Algorithm</title><categories>cs.NE cs.SY</categories><comments>9 pages, 4 tables, 17 equation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper contains the concept and the results of research concerning the
evolutionary algorithm, identified based on the systems control theory, which
was called the Systemically of Evolutionary Algorithm (SAE). Special attention
was paid to two elements of evolutionary algorithms, which have not been fully
solved yet, i.e. to the methods used to create the initial population and the
method of creating the robustness (fitness) function. Other elements of the SEA
algorithm, i.a. cross-over, mutation, selection, etc. were also defined from a
systemic point of view. Computational experiments were conducted using a
selected subsystem of the Polish Electrical Power System and three programming
languages: Java, C++ and Matlab. Selected comparative results for the SAE
algorithm in different implementations were also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5791</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5791</id><created>2014-01-22</created><authors><author><keyname>Dash</keyname><forenames>Debadatta</forenames></author></authors><title>Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EEG monitoring has an important milestone provide valuable information of
those candidates who suffer from epilepsy.In this paper human normal and
epileptic Electroencephalogram signals are analyzed with popular and efficient
signal processing techniques like Fourier and Wavelet transform. The delta,
theta, alpha, beta and gamma sub bands of EEG are obtained and studied for
detection of seizure and epilepsy. The extracted feature is then applied to ANN
for classification of the EEG signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5808</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5808</id><created>2014-01-02</created><authors><author><keyname>Pourbahman</keyname><forenames>Zahra</forenames></author><author><keyname>Hamzeh</keyname><forenames>Ali</forenames></author></authors><title>Reducing the Computational Cost in Multi-objective Evolutionary
  Algorithms by Filtering Worthless Individuals</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large number of exact fitness function evaluations makes evolutionary
algorithms to have computational cost. In some real-world problems, reducing
number of these evaluations is much more valuable even by increasing
computational complexity and spending more time. To fulfill this target, we
introduce an effective factor, in spite of applied factor in Adaptive Fuzzy
Fitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filter
out worthless individuals more precisely. Our proposed approach is compared
with respect to Adaptive Fuzzy Fitness Granulation with Non-dominated Sorting
Genetic Algorithm-II, using the Hyper volume and the Inverted Generational
Distance performance measures. The proposed method is applied to 1 traditional
and 1 state-of-the-art benchmarks with considering 3 different dimensions. From
an average performance view, the results indicate that although decreasing the
number of fitness evaluations leads to have performance reduction but it is not
tangible compared to what we gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5813</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5813</id><created>2014-01-22</created><authors><author><keyname>&#x141;a&#x144;cucki</keyname><forenames>Adrian</forenames></author></authors><title>GGP with Advanced Reasoning and Board Knowledge Discovery</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of General Game Playing (GGP) matches suffers from slow
state-switching and weak knowledge modules. Instantiation and Propositional
Networks offer great performance gains over Prolog-based reasoning, but do not
scale well. In this publication mGDL, a variant of GDL stripped of function
constants, has been defined as a basis for simple reasoning machines. mGDL
allows to easily map rules to C++ functions. 253 out of 270 tested GDL rule
sheets conformed to mGDL without any modifications; the rest required minor
changes. A revised (m)GDL to C++ translation scheme has been reevaluated; it
brought gains ranging from 28% to 7300% over YAP Prolog, managing to compile
even demanding rule sheets under few seconds. For strengthening game knowledge,
spatial features inspired by similar successful techniques from computer Go
have been proposed. For they required an Euclidean metric, a small board
extension to GDL has been defined through a set of ground atomic sentences. An
SGA-based genetic algorithm has been designed for tweaking game parameters and
conducting self-plays, so the features could be mined from meaningful game
records. The approach has been tested on a small cluster, giving performance
gains up to 20% more wins against the baseline UCT player. Implementations of
proposed ideas constitutes the core of GGP Spatium - a small C++/Python GGP
framework, created for developing compact GGP Players and problem solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5814</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5814</id><created>2014-01-22</created><authors><author><keyname>Schneider</keyname><forenames>Johannes</forenames></author><author><keyname>Vlachos</keyname><forenames>Michail</forenames></author></authors><title>On Randomly Projected Hierarchical Clustering with Guarantees</title><categories>cs.IR cs.DS</categories><comments>This version contains the conference paper &quot;On Randomly Projected
  Hierarchical Clustering with Guarantees'', SIAM International Conference on
  Data Mining (SDM), 2014 and, additionally, proofs omitted in the conference
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical clustering (HC) algorithms are generally limited to small data
instances due to their runtime costs. Here we mitigate this shortcoming and
explore fast HC algorithms based on random projections for single (SLC) and
average (ALC) linkage clustering as well as for the minimum spanning tree
problem (MST). We present a thorough adaptive analysis of our algorithms that
improve prior work from $O(N^2)$ by up to a factor of $N/(\log N)^2$ for a
dataset of $N$ points in Euclidean space. The algorithms maintain, with
arbitrary high probability, the outcome of hierarchical clustering as well as
the worst-case running-time guarantees. We also present parameter-free
instances of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5820</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5820</id><created>2014-01-22</created><updated>2015-07-08</updated><authors><author><keyname>Morrison</keyname><forenames>David R.</forenames></author><author><keyname>Sewell</keyname><forenames>Edward C.</forenames></author><author><keyname>Jacobson</keyname><forenames>Sheldon H.</forenames></author></authors><title>Solving the Pricing Problem in a Branch-and-Price Algorithm for Graph
  Coloring using Zero-Suppressed Binary Decision Diagrams</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Branch-and-price algorithms combine a branch-and-bound search with an
exponentially-sized LP formulation that must be solved via column generation.
Unfortunately, the standard branching rules used in branch-and-bound for
integer programming interfere with the structure of the column generation
routine; therefore, most such algorithms employ alternate branching rules to
circumvent this difficulty. This paper shows how a zero-suppressed binary
decision diagram (ZDD) can be used to solve the pricing problem in a
branch-and-price algorithm for the graph coloring problem, even in the presence
of constraints imposed by branching decisions. This approach facilitates a much
more direct solution method, and can improve convergence of the column
generation subroutine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5826</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5826</id><created>2014-01-22</created><authors><author><keyname>Ta</keyname><forenames>Tuan</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author><author><keyname>Zhu</keyname><forenames>Chenxi</forenames></author></authors><title>Improving Smartphone Battery Life Utilizing Device-to-device Cooperative
  Relays Underlaying LTE Networks</title><categories>cs.NI</categories><comments>Accepted at IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The utility of smartphones has been limited to a great extent by their short
battery life. In this work, we propose a new approach to prolonging smartphone
battery life. We introduce the notions of &quot;valueless&quot; and &quot;valued battery&quot;, as
being the available battery when the user does or does not have access to a
power source, respectively. We propose a cooperative system where users with
high battery level help carry the traffic of users with low battery level. Our
scheme helps increase the amount of valued battery in the network, thus it
reduces the chance of users running out of battery early. Our system can be
realized in the form of a proximity service (ProSe) which utilizes a
device-to-device (D2D) communication architecture underlaying LTE. We show
through simulations that our system reduces the probability of cellular users
running out of battery before their target usage time (probability of outage).
Our simulator source code is made available to the public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5828</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5828</id><created>2014-01-22</created><updated>2014-04-29</updated><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>Applications of Information Nonanticipative Rate Distortion Function</title><categories>cs.IT math.IT math.OC math.PR</categories><comments>5 pages, 3 figures, accepted for publication in IEEE International
  Symposium on Information Theory (ISIT) proceedings, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to further investigate various applications of
information Nonanticipative Rate Distortion Function (NRDF) by discussing two
working examples, the Binary Symmetric Markov Source with parameter $p$
(BSMS($p$)) with Hamming distance distortion, and the multidimensional
partially observed Gaussian-Markov source. For the BSMS($p$), we give the
solution to the NRDF, and we use it to compute the Rate Loss (RL) of causal
codes with respect to noncausal codes. For the multidimensional Gaussian-Markov
source, we give the solution to the NRDF, we show its operational meaning via
joint source-channel matching over a vector of parallel Gaussian channels, and
we compute the RL of causal and zero-delay codes with respect to noncausal
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5830</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5830</id><created>2014-01-22</created><authors><author><keyname>Suffian</keyname><forenames>Muhammad Dhiauddin Mohamed</forenames></author><author><keyname>Ibrahim</keyname><forenames>Suhaimi</forenames></author></authors><title>A Prediction Model for System Testing Defects using Regression Analysis</title><categories>cs.SE</categories><comments>14 pages, 12 figures and 3 tables. e-ISSN: 2251-7545</comments><msc-class>68N30</msc-class><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE), Vol.2,o.7, 2012, Published online: July 25, 2012</journal-ref><doi>10.7321/jscse.v2.n7.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research describes the initial effort of building a prediction model for
defects in system testing carried out by an independent testing team. The
motivation to have such defect prediction model is to serve as early quality
indicator of the software entering system testing and assist the testing team
to manage and control test execution activities. Metrics collected from prior
phases to system testing are identified and analyzed to determine the potential
predictors for building the model. The selected metrics are then put into
regression analysis to generate several mathematical equations. Mathematical
equation that has p-value of less than 0.05 with R-squared and R-squared
(adjusted) more than 90% is selected as the desired prediction model for system
testing defects. This model is verified using new projects to confirm that it
is fit for actual implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5836</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5836</id><created>2014-01-22</created><updated>2014-05-27</updated><authors><author><keyname>Sekara</keyname><forenames>Vedran</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>The Strength of Friendship Ties in Proximity Sensor Data</title><categories>physics.soc-ph cs.SI</categories><comments>Updated Introduction, added references. 12 pages, 7 figures</comments><report-no>PLoS One 9.7 (2014): e100915</report-no><doi>10.1371/journal.pone.0100915</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how people interact and socialize is important in many contexts
from disease control to urban planning. Datasets that capture this specific
aspect of human life have increased in size and availability over the last few
years. We have yet to understand, however, to what extent such electronic
datasets may serve as a valid proxy for real life social interactions. For an
observational dataset, gathered using mobile phones, we analyze the problem of
identifying transient and non-important links, as well as how to highlight
important social interactions. Applying the Bluetooth signal strength parameter
to distinguish between observations, we demonstrate that weak links, compared
to strong links, have a lower probability of being observed at later times,
while such links--on average--also have lower link-weights and probability of
sharing an online friendship. Further, the role of link-strength is
investigated in relation to social network properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5842</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5842</id><created>2014-01-22</created><updated>2014-06-02</updated><authors><author><keyname>Sinn</keyname><forenames>Moritz</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author></authors><title>A Simple and Scalable Static Analysis for Bound Analysis and Amortized
  Complexity Analysis</title><categories>cs.PL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first scalable bound analysis that achieves amortized
complexity analysis. In contrast to earlier work, our bound analysis is not
based on general purpose reasoners such as abstract interpreters, software
model checkers or computer algebra tools. Rather, we derive bounds directly
from abstract program models, which we obtain from programs by comparatively
simple invariant generation and symbolic execution techniques. As a result, we
obtain an analysis that is more predictable and more scalable than earlier
approaches. Our experiments demonstrate that our analysis is fast and at the
same time able to compute bounds for challenging loops in a large real-world
benchmark. Technically, our approach is based on lossy vector addition systems
(VASS). Our bound analysis first computes a lexicographic ranking function that
proves the termination of a VASS, and then derives a bound from this ranking
function. Our methodology achieves amortized analysis based on a new insight
how lexicographic ranking functions can be used for bound analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5846</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5846</id><created>2014-01-22</created><updated>2015-11-14</updated><authors><author><keyname>Achilleos</keyname><forenames>Antonis</forenames></author></authors><title>Modal Logics with Hard Diamond-free Fragments</title><categories>cs.LO</categories><comments>New version: improvements and corrections according to reviewers'
  comments. Accepted at LFCS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of modal satisfiability for certain
combinations of modal logics. In particular we examine four examples of
multimodal logics with dependencies and demonstrate that even if we restrict
our inputs to diamond-free formulas (in negation normal form), these logics
still have a high complexity. This result illustrates that having D as one or
more of the combined logics, as well as the interdependencies among logics can
be important sources of complexity even in the absence of diamonds and even
when at the same time in our formulas we allow only one propositional variable.
We then further investigate and characterize the complexity of the
diamond-free, 1-variable fragments of multimodal logics in a general setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5848</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5848</id><created>2014-01-22</created><authors><author><keyname>B&#xe4;ckstr&#xf6;m</keyname><forenames>Christer</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author></authors><title>Algorithms and Limits for Compact Plan Representations</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  141-177, 2012</journal-ref><doi>10.1613/jair.3534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compact representations of objects is a common concept in computer science.
Automated planning can be viewed as a case of this concept: a planning instance
is a compact implicit representation of a graph and the problem is to find a
path (a plan) in this graph. While the graphs themselves are represented
compactly as planning instances, the paths are usually represented explicitly
as sequences of actions. Some cases are known where the plans always have
compact representations, for example, using macros. We show that these results
do not extend to the general case, by proving a number of bounds for compact
representations of plans under various criteria, like efficient sequential or
random access of actions. In addition to this, we show that our results have
consequences for what can be gained from reformulating planning into some other
problem. As a contrast to this we also prove a number of positive results,
demonstrating restricted cases where plans do have useful compact
representations, as well as proving that macro plans have favourable access
properties. Our results are finally discussed in relation to other relevant
contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5849</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5849</id><created>2014-01-22</created><authors><author><keyname>Belardinelli</keyname><forenames>Francesco</forenames></author><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author></authors><title>Interactions between Knowledge and Time in a First-Order Logic for
  Multi-Agent Systems: Completeness Results</title><categories>cs.MA cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  1-45, 2012</journal-ref><doi>10.1613/jair.3547</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a class of first-order temporal-epistemic logics for reasoning
about multi-agent systems. We encode typical properties of systems including
perfect recall, synchronicity, no learning, and having a unique initial state
in terms of variants of quantified interpreted systems, a first-order extension
of interpreted systems. We identify several monodic fragments of first-order
temporal-epistemic logic and show their completeness with respect to their
corresponding classes of quantified interpreted systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5850</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5850</id><created>2014-01-22</created><authors><author><keyname>Konev</keyname><forenames>Boris</forenames></author><author><keyname>Ludwig</keyname><forenames>Michel</forenames></author><author><keyname>Walther</keyname><forenames>Dirk</forenames></author><author><keyname>Wolter</keyname><forenames>Frank</forenames></author></authors><title>The Logical Difference for the Lightweight Description Logic EL</title><categories>cs.LO cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  633-708, 2012</journal-ref><doi>10.1613/jair.3552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a logic-based approach to versioning of ontologies. Under this view,
ontologies provide answers to queries about some vocabulary of interest. The
difference between two versions of an ontology is given by the set of queries
that receive different answers. We investigate this approach for terminologies
given in the description logic EL extended with role inclusions and domain and
range restrictions for three distinct types of queries: subsumption, instance,
and conjunctive queries. In all three cases, we present polynomial-time
algorithms that decide whether two terminologies give the same answers to
queries over a given vocabulary and compute a succinct representation of the
difference if it is non- empty. We present an implementation, CEX2, of the
developed algorithms for subsumption and instance queries and apply it to
distinct versions of Snomed CT and the NCI ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5851</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5851</id><created>2014-01-22</created><authors><author><keyname>Vasirani</keyname><forenames>Matteo</forenames></author><author><keyname>Ossowski</keyname><forenames>Sascha</forenames></author></authors><title>A Market-Inspired Approach for Intersection Management in Urban Road
  Traffic Networks</title><categories>cs.GT cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  621-659, 2012</journal-ref><doi>10.1613/jair.3560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic congestion in urban road networks is a costly problem that affects
all major cities in developed countries. To tackle this problem, it is possible
(i) to act on the supply side, increasing the number of roads or lanes in a
network, (ii) to reduce the demand, restricting the access to urban areas at
specific hours or to specific vehicles, or (iii) to improve the efficiency of
the existing network, by means of a widespread use of so-called Intelligent
Transportation Systems (ITS). In line with the recent advances in smart
transportation management infrastructures, ITS has turned out to be a promising
field of application for artificial intelligence techniques. In particular,
multiagent systems seem to be the ideal candidates for the design and
implementation of ITS. In fact, drivers can be naturally modelled as autonomous
agents that interact with the transportation management infrastructure, thereby
generating a large-scale, open, agent-based system. To regulate such a system
and maintain a smooth and efficient flow of traffic, decentralised mechanisms
for the management of the transportation infrastructure are needed.
  In this article we propose a distributed, market-inspired, mechanism for the
management of a future urban road network, where intelligent autonomous
vehicles, operated by software agents on behalf of their human owners, interact
with the infrastructure in order to travel safely and efficiently through the
road network. Building on the reservation-based intersection control model
proposed by Dresner and Stone, we consider two different scenarios: one with a
single intersection and one with a network of intersections. In the former, we
analyse the performance of a novel policy based on combinatorial auctions for
the allocation of reservations. In the latter, we analyse the impact that a
traffic assignment strategy inspired by competitive markets has on the drivers
route choices. Finally we propose an adaptive management mechanism that
integrates the auction-based traffic control policy with the competitive
traffic assignment strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5852</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5852</id><created>2014-01-22</created><authors><author><keyname>Ghosh</keyname><forenames>Priyankar</forenames></author><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Chakrabarti</keyname><forenames>P. P.</forenames></author><author><keyname>Dasgupta</keyname><forenames>Pallab</forenames></author></authors><title>Algorithms for Generating Ordered Solutions for Explicit AND/OR
  Structures</title><categories>cs.AI cs.DS</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  275-333, 2012</journal-ref><doi>10.1613/jair.3576</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms for generating alternative solutions for explicit
acyclic AND/OR structures in non-decreasing order of cost. The proposed
algorithms use a best first search technique and report the solutions using an
implicit representation ordered by cost. In this paper, we present two versions
of the search algorithm -- (a) an initial version of the best first search
algorithm, ASG, which may present one solution more than once while generating
the ordered solutions, and (b) another version, LASG, which avoids the
construction of the duplicate solutions. The actual solutions can be
reconstructed quickly from the implicit compact representation used. We have
applied the methods on a few test domains, some of them are synthetic while the
others are based on well known problems including the search space of the 5-peg
Tower of Hanoi problem, the matrix-chain multiplication problem and the problem
of finding secondary structure of RNA. Experimental results show the efficacy
of the proposed algorithms over the existing approach. Our proposed algorithms
have potential use in various domains ranging from knowledge based frameworks
to service composition, where the AND/OR structure is widely used for
representing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5853</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5853</id><created>2014-01-22</created><authors><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author></authors><title>Reasoning over Ontologies with Hidden Content: The Import-by-Query
  Approach</title><categories>cs.AI cs.LO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  197-255, 2012</journal-ref><doi>10.1613/jair.3579</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is currently a growing interest in techniques for hiding parts of the
signature of an ontology Kh that is being reused by another ontology Kv.
Towards this goal, in this paper we propose the import-by-query framework,
which makes the content of Kh accessible through a limited query interface. If
Kv reuses the symbols from Kh in a certain restricted way, one can reason over
Kv U Kh by accessing only Kv and the query interface. We map out the landscape
of the import-by-query problem. In particular, we outline the limitations of
our framework and prove that certain restrictions on the expressivity of Kh and
the way in which Kv reuses symbols from Kh are strictly necessary to enable
reasoning in our setting. We also identify cases in which reasoning is possible
and we present suitable import-by-query reasoning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5854</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5854</id><created>2014-01-22</created><authors><author><keyname>Hern&#xe1;ndez</keyname><forenames>Carlos</forenames></author><author><keyname>Baier</keyname><forenames>Jorge A</forenames></author></authors><title>Avoiding and Escaping Depressions in Real-Time Heuristic Search</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 43, pages
  523-570, 2012</journal-ref><doi>10.1613/jair.3590</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heuristics used for solving hard real-time search problems have regions with
depressions. Such regions are bounded areas of the search space in which the
heuristic function is inaccurate compared to the actual cost to reach a
solution. Early real-time search algorithms, like LRTA*, easily become trapped
in those regions since the heuristic values of their states may need to be
updated multiple times, which results in costly solutions. State-of-the-art
real-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*s
mechanism to update the heuristic, resulting in improved performance. Those
algorithms, however, do not guide search towards avoiding depressed regions.
This paper presents depression avoidance, a simple real-time search principle
to guide search towards avoiding states that have been marked as part of a
heuristic depression. We propose two ways in which depression avoidance can be
implemented: mark-and-avoid and move-to-border. We implement these strategies
on top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search
algorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is
to find a single solution by running the real-time search algorithm once, we
show that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by
one order of magnitude. Of the four new algorithms, daRTAA* produces the best
solutions given a fixed deadline on the average time allowed per planning
episode. We prove all our algorithms have good theoretical properties: in
finite search spaces, they find a solution if one exists, and converge to an
optimal after a number of trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5855</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5855</id><created>2014-01-22</created><authors><author><keyname>Cooper</keyname><forenames>Martin C.</forenames></author><author><keyname>&#x17d;ivn&#xfd;</keyname><forenames>Stanislav</forenames></author></authors><title>Tractable Triangles and Cross-Free Convexity in Discrete Optimisation</title><categories>cs.CC cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1008.4035 by other authors</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  455-490, 2012</journal-ref><doi>10.1613/jair.3598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimisation problem of a sum of unary and pairwise functions of discrete
variables is a general NP-hard problem with wide applications such as computing
MAP configurations in Markov Random Fields (MRF), minimising Gibbs energy, or
solving binary Valued Constraint Satisfaction Problems (VCSPs).
  We study the computational complexity of classes of discrete optimisation
problems given by allowing only certain types of costs in every triangle of
variable-value assignments to three distinct variables. We show that for
several computational problems, the only non- trivial tractable classes are the
well known maximum matching problem and the recently discovered joint-winner
property. Our results, apart from giving complete classifications in the
studied cases, provide guidance in the search for hybrid tractable classes;
that is, classes of problems that are not captured by restrictions on the
functions (such as submodularity) or the structure of the problem graph (such
as bounded treewidth).
  Furthermore, we introduce a class of problems with convex cardinality
functions on cross-free sets of assignments. We prove that while imposing only
one of the two conditions renders the problem NP-hard, the conjunction of the
two gives rise to a novel tractable class satisfying the cross-free convexity
property, which generalises the joint-winner property to problems of unbounded
arity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5856</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5856</id><created>2014-01-22</created><authors><author><keyname>Haslum</keyname><forenames>Patrik</forenames></author></authors><title>Narrative Planning: Compilations to Classical Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  383-395, 2012</journal-ref><doi>10.1613/jair.3602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of story generation recently proposed by Riedl and Young casts it as
planning, with the additional condition that story characters behave
intentionally. This means that characters have perceivable motivation for the
actions they take. I show that this condition can be compiled away (in more
ways than one) to produce a classical planning problem that can be solved by an
off-the-shelf classical planner, more efficiently than by Riedl and Youngs
specialised planner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5857</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5857</id><created>2014-01-22</created><authors><author><keyname>Coles</keyname><forenames>Amanda J.</forenames></author><author><keyname>Coles</keyname><forenames>Andrew I.</forenames></author><author><keyname>Fox</keyname><forenames>Maria</forenames></author><author><keyname>Long</keyname><forenames>Derek</forenames></author></authors><title>COLIN: Planning with Continuous Linear Numeric Change</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  1-96, 2012</journal-ref><doi>10.1613/jair.3608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe COLIN, a forward-chaining heuristic search planner,
capable of reasoning with COntinuous LINear numeric change, in addition to the
full temporal semantics of PDDL. Through this work we make two advances to the
state-of-the-art in terms of expressive reasoning capabilities of planners: the
handling of continuous linear change, and the handling of duration-dependent
effects in combination with duration inequalities, both of which require
tightly coupled temporal and numeric reasoning during planning. COLIN combines
FF-style forward chaining search, with the use of a Linear Program (LP) to
check the consistency of the interacting temporal and numeric constraints at
each state. The LP is used to compute bounds on the values of variables in each
state, reducing the range of actions that need to be considered for
application. In addition, we develop an extension of the Temporal Relaxed
Planning Graph heuristic of CRIKEY3, to support reasoning directly with
continuous change. We extend the range of task variables considered to be
suitable candidates for specifying the gradient of the continuous numeric
change effected by an action. Finally, we explore the potential for employing
mixed integer programming as a tool for optimising the timestamps of the
actions in the plan, once a solution has been found. To support this, we
further contribute a selection of extended benchmark domains that include
continuous numeric effects. We present results for COLIN that demonstrate its
scalability on a range of benchmarks, and compare to existing state-of-the-art
planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5858</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5858</id><created>2014-01-22</created><authors><author><keyname>Hoffman</keyname><forenames>Joerg</forenames></author><author><keyname>Weber</keyname><forenames>Ingo</forenames></author><author><keyname>Kraft</keyname><forenames>Frank Michael</forenames></author></authors><title>SAP Speaks PDDL: Exploiting a Software-Engineering Model for Planning in
  Business Process Management</title><categories>cs.AI cs.SE</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  587-632, 2012</journal-ref><doi>10.1613/jair.3636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning is concerned with the automated solution of action sequencing
problems described in declarative languages giving the action preconditions and
effects. One important application area for such technology is the creation of
new processes in Business Process Management (BPM), which is essential in an
ever more dynamic business environment. A major obstacle for the application of
Planning in this area lies in the modeling. Obtaining a suitable model to plan
with -- ideally a description in PDDL, the most commonly used planning language
-- is often prohibitively complicated and/or costly. Our core observation in
this work is that this problem can be ameliorated by leveraging synergies with
model-based software development. Our application at SAP, one of the leading
vendors of enterprise software, demonstrates that even one-to-one model re-use
is possible.
  The model in question is called Status and Action Management (SAM). It
describes the behavior of Business Objects (BO), i.e., large-scale data
structures, at a level of abstraction corresponding to the language of business
experts. SAM covers more than 400 kinds of BOs, each of which is described in
terms of a set of status variables and how their values are required for, and
affected by, processing steps (actions) that are atomic from a business
perspective. SAM was developed by SAP as part of a major model-based software
engineering effort. We show herein that one can use this same model for
planning, thus obtaining a BPM planning application that incurs no modeling
overhead at all.
  We compile SAM into a variant of PDDL, and adapt an off-the-shelf planner to
solve this kind of problem. Thanks to the resulting technology, business
experts may create new processes simply by specifying the desired behavior in
terms of status variable value changes: effectively, by describing the process
in their own language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5859</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5859</id><created>2014-01-22</created><authors><author><keyname>Fox</keyname><forenames>Maria</forenames></author><author><keyname>Long</keyname><forenames>Derek</forenames></author><author><keyname>Magazzeni</keyname><forenames>Daniele</forenames></author></authors><title>Plan-based Policies for Efficient Multiple Battery Load Management</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  335-382, 2012</journal-ref><doi>10.1613/jair.3643</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient use of multiple batteries is a practical problem with wide and
growing application. The problem can be cast as a planning problem under
uncertainty. We describe the approach we have adopted to modelling and solving
this problem, seen as a Markov Decision Problem, building effective policies
for battery switching in the face of stochastic load profiles.
  Our solution exploits and adapts several existing techniques: planning for
deterministic mixed discrete-continuous problems and Monte Carlo sampling for
policy learning. The paper describes the development of planning techniques to
allow solution of the non-linear continuous dynamic models capturing the
battery behaviours. This approach depends on carefully handled discretisation
of the temporal dimension. The construction of policies is performed using a
classification approach and this idea offers opportunities for wider
exploitation in other problems. The approach and its generality are described
in the paper.
  Application of the approach leads to construction of policies that, in
simulation, significantly outperform those that are currently in use and the
best published solutions to the battery management problem. We achieve
solutions that achieve more than 99% efficiency in simulation compared with the
theoretical limit and do so with far fewer battery switches than existing
policies. Behaviour of physical batteries does not exactly match the simulated
models for many reasons, so to confirm that our theoretical results can lead to
real measured improvements in performance we also conduct and report
experiments using a physical test system. These results demonstrate that we can
obtain 5%-15% improvement in lifetimes in the case of a two battery system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5860</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5860</id><created>2014-01-22</created><authors><author><keyname>Ab&#xed;o</keyname><forenames>Ignasi</forenames></author><author><keyname>Nieuwenhuis</keyname><forenames>Robert</forenames></author><author><keyname>Oliveras</keyname><forenames>Albert</forenames></author><author><keyname>Rodriguez-Carbonell</keyname><forenames>Enric</forenames></author><author><keyname>Mayer-Eichberger</keyname><forenames>Valentin</forenames></author></authors><title>A New Look at BDDs for Pseudo-Boolean Constraints</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  443-480, 2012</journal-ref><doi>10.1613/jair.3653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudo-Boolean constraints are omnipresent in practical applications, and
thus a significant effort has been devoted to the development of good SAT
encoding techniques for them. Some of these encodings first construct a Binary
Decision Diagram (BDD) for the constraint, and then encode the BDD into a
propositional formula. These BDD-based approaches have some important
advantages, such as not being dependent on the size of the coefficients, or
being able to share the same BDD for representing many constraints.
  We first focus on the size of the resulting BDDs, which was considered to be
an open problem in our research community. We report on previous work where it
was proved that there are Pseudo-Boolean constraints for which no polynomial
BDD exists. We also give an alternative and simpler proof assuming that NP is
different from Co-NP. More interestingly, here we also show how to overcome the
possible exponential blowup of BDDs by phcoefficient decomposition. This allows
us to give the first polynomial generalized arc-consistent ROBDD-based encoding
for Pseudo-Boolean constraints.
  Finally, we focus on practical issues: we show how to efficiently construct
such ROBDDs, how to encode them into SAT with only 2 clauses per node, and
present experimental results that confirm that our approach is competitive with
other encodings and state-of-the-art Pseudo-Boolean solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5861</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5861</id><created>2014-01-22</created><authors><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author><author><keyname>Karpas</keyname><forenames>Erez</forenames></author><author><keyname>Markovitch</keyname><forenames>Shaul</forenames></author></authors><title>Online Speedup Learning for Optimal Planning</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  709-755, 2012</journal-ref><doi>10.1613/jair.3676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain-independent planning is one of the foundational areas in the field of
Artificial Intelligence. A description of a planning task consists of an
initial world state, a goal, and a set of actions for modifying the world
state. The objective is to find a sequence of actions, that is, a plan, that
transforms the initial world state into a goal state. In optimal planning, we
are interested in finding not just a plan, but one of the cheapest plans. A
prominent approach to optimal planning these days is heuristic state-space
search, guided by admissible heuristic functions. Numerous admissible
heuristics have been developed, each with its own strengths and weaknesses, and
it is well known that there is no single &quot;best heuristic for optimal planning
in general. Thus, which heuristic to choose for a given planning task is a
difficult question. This difficulty can be avoided by combining several
heuristics, but that requires computing numerous heuristic estimates at each
state, and the tradeoff between the time spent doing so and the time saved by
the combined advantages of the different heuristics might be high. We present a
novel method that reduces the cost of combining admissible heuristics for
optimal planning, while maintaining its benefits. Using an idealized search
space model, we formulate a decision rule for choosing the best heuristic to
compute at each state. We then present an active online learning approach for
learning a classifier with that decision rule as the target concept, and employ
the learned classifier to decide which heuristic to compute at each state. We
evaluate this technique empirically, and show that it substantially outperforms
the standard method for combining several heuristics via their pointwise
maximum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5863</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5863</id><created>2014-01-22</created><authors><author><keyname>Endriss</keyname><forenames>Ulle</forenames></author><author><keyname>Grandi</keyname><forenames>Umberto</forenames></author><author><keyname>Porello</keyname><forenames>Daniele</forenames></author></authors><title>Complexity of Judgment Aggregation</title><categories>cs.MA</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  481-514, 2012</journal-ref><doi>10.1613/jair.3708</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the computational complexity of three problems in judgment
aggregation: (1) computing a collective judgment from a profile of individual
judgments (the winner determination problem); (2) deciding whether a given
agent can influence the outcome of a judgment aggregation procedure in her
favour by reporting insincere judgments (the strategic manipulation problem);
and (3) deciding whether a given judgment aggregation scenario is guaranteed to
result in a logically consistent outcome, independently from what the judgments
supplied by the individuals are (the problem of the safety of the agenda). We
provide results both for specific aggregation procedures (the quota rules, the
premise-based procedure, and a distance-based procedure) and for classes of
aggregation procedures characterised in terms of fundamental axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5869</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5869</id><created>2014-01-22</created><authors><author><keyname>Zhang</keyname><forenames>Zizhen</forenames></author><author><keyname>Qin</keyname><forenames>Hu</forenames></author><author><keyname>Liang</keyname><forenames>Xiaocong</forenames></author><author><keyname>Lim</keyname><forenames>Andrew</forenames></author></authors><title>An Enhanced Branch-and-bound Algorithm for the Talent Scheduling Problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The talent scheduling problem is a simplified version of the real-world film
shooting problem, which aims to determine a shooting sequence so as to minimize
the total cost of the actors involved. In this article, we first formulate the
problem as an integer linear programming model. Next, we devise a
branch-and-bound algorithm to solve the problem. The branch-and-bound algorithm
is enhanced by several accelerating techniques, including preprocessing,
dominance rules and caching search states. Extensive experiments over two sets
of benchmark instances suggest that our algorithm is superior to the current
best exact algorithm. Finally, the impacts of different parameter settings are
disclosed by some additional experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5871</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5871</id><created>2014-01-22</created><authors><author><keyname>Verma</keyname><forenames>Pramod</forenames></author></authors><title>Serefind: A Social Networking Website for Classifieds</title><categories>cs.SI cs.CY</categories><comments>A prototype is available at http://www.serefind.com. Pramod Verma.
  2013. Serefind: a social networking website for classifieds. In Proceedings
  of the 22nd international conference on World Wide Web companion (WWW '13
  Companion), 289-292</comments><acm-class>H.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents the design and implementation of a social networking
website for classifieds, called Serefind. We designed search interfaces with
focus on security, privacy, usability, design, ranking, and communications. We
deployed this site at the Johns Hopkins University, and the results show it can
be used as a self-sustaining classifieds site for public or private
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5874</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5874</id><created>2014-01-23</created><authors><author><keyname>Jiang</keyname><forenames>Yupeng</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>Distribution properties of compressing sequences derived from primitive
  sequences modulo odd prime powers</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><msc-class>11B50, 94A55, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\underline{a}$ and $\underline{b}$ be primitive sequences over
$\mathbb{Z}/(p^e)$ with odd prime $p$ and $e\ge 2$. For certain compressing
maps, we consider the distribution properties of compressing sequences of
$\underline{a}$ and $\underline{b}$, and prove that
$\underline{a}=\underline{b}$ if the compressing sequences are equal at the
times $t$ such that $\alpha(t)=k$, where $\underline{\alpha}$ is a sequence
related to $\underline{a}$. We also discuss the $s$-uniform distribution
property of compressing sequences. For some compressing maps, we have that
there exist different primitive sequences such that the compressing sequences
are $s$-uniform. We also discuss that compressing sequences can be $s$-uniform
for how many elements $s$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5878</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5878</id><created>2014-01-23</created><authors><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Jeffery</keyname><forenames>Ross</forenames></author></authors><title>State of the Practice in Software Effort Estimation: A Survey and
  Literature Review</title><categories>cs.SE</categories><comments>14 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007/978-3-642-22386-0_18</comments><journal-ref>Software Engineering Techniques, Proceedings of the 3rd IFIP TC2
  Central and Eastern European Conference on Software Engineering Techniques
  (CEE-SET 2008), pages 232-245, 2008</journal-ref><doi>10.1007/978-3-642-22386-0_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effort estimation is a key factor for software project success, defined as
delivering software of agreed quality and functionality within schedule and
budget. Traditionally, effort estimation has been used for planning and
tracking project resources. Effort estimation methods founded on those goals
typically focus on providing exact estimates and usually do not support
objectives that have recently become important within the software industry,
such as systematic and reliable analysis of causal effort dependencies. This
article presents the results of a study of software effort estimation from an
industrial perspective. The study surveys industrial objectives, the abilities
of software organizations to apply certain estimation methods, and actually
applied practices of software effort estimation. Finally, requirements for
effort estimation methods identified in the survey are compared against
existing estimation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5888</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5888</id><created>2014-01-23</created><updated>2014-09-17</updated><authors><author><keyname>Shang</keyname><forenames>Changxing</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongying</forenames></author><author><keyname>Fan</keyname><forenames>Jianping</forenames></author></authors><title>Efficiently Detecting Overlapping Communities through Seeding and
  Semi-Supervised Learning</title><categories>cs.SI cs.LG physics.soc-ph</categories><doi>10.1007/s13042-015-0338-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seeding then expanding is a commonly used scheme to discover overlapping
communities in a network. Most seeding methods are either too complex to scale
to large networks or too simple to select high-quality seeds, and the
non-principled functions used by most expanding methods lead to poor
performance when applied to diverse networks. This paper proposes a new method
that transforms a network into a corpus where each edge is treated as a
document, and all nodes of the network are treated as terms of the corpus. An
effective seeding method is also proposed that selects seeds as a training set,
then a principled expanding method based on semi-supervised learning is applied
to classify edges. We compare our new algorithm with four other community
detection algorithms on a wide range of synthetic and empirical networks.
Experimental results show that the new algorithm can significantly improve
clustering performance in most cases. Furthermore, the time complexity of the
new algorithm is linear to the number of edges, and this low complexity makes
the new algorithm scalable to large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5891</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5891</id><created>2014-01-23</created><authors><author><keyname>Kharinov</keyname><forenames>M.</forenames></author></authors><title>Hierarchical pixel clustering for image segmentation</title><categories>cs.CV</categories><comments>5 pages, 3 figures, 4 formulas, submitted to the 12 International
  Conference on Pattern Recognition and Information Processing May 28-30, 2014,
  Minsk, Belarus</comments><journal-ref>Proc. of the 12th International Conference on Pattern Recognition
  and Information Processing (PRIP'2014), May 28-30, 2014, Minsk, Belarus,
  pp.103-107</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper a piecewise constant image approximations of sequential number
of pixel clusters or segments are treated. A majorizing of optimal
approximation sequence by hierarchical sequence of image approximations is
studied. Transition from pixel clustering to image segmentation by reducing of
segment numbers in clusters is provided. Algorithms are proved by elementary
formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5895</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5895</id><created>2014-01-23</created><updated>2014-05-11</updated><authors><author><keyname>Watanabe</keyname><forenames>Yohei</forenames></author><author><keyname>Shikata</keyname><forenames>Junji</forenames></author></authors><title>Timed-Release Secret Sharing Scheme with Information Theoretic Security</title><categories>cs.CR</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern cryptography, the secret sharing scheme is an important
cryptographic primitive and it is used in various situations. In this paper, a
timed-release secret sharing scheme (TR-SS) with information-theoretic security
is first studied. TR-SS is a secret sharing scheme with the property that
participants more than a threshold number can reconstruct a secret by using
their shares only when the time specified by a dealer has come. Specifically,
in this paper we first introduce a model and formalization of security for
TR-SS based on the traditional secret sharing scheme and information-theoretic
timed-release security. We also derive tight lower bounds on the sizes of
shares, time-signals, and entities' secret-keys required for TR-SS. In
addition, we propose a direct construction for TR-SS. Our direct construction
is optimal in the sense that the construction meets equality in each of our
bounds. As a result, it is shown that the timed-release security can be
realized without any additional redundancy on the share-size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5896</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5896</id><created>2014-01-23</created><updated>2014-04-28</updated><authors><author><keyname>Iwamoto</keyname><forenames>Mitsugu</forenames></author><author><keyname>Shikata</keyname><forenames>Junji</forenames></author></authors><title>Secret Sharing Schemes Based on Min-Entropies</title><categories>cs.CR cs.IT math.IT</categories><comments>7 pages, This is the full version of the paper to appear at IEEE
  ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental results on secret sharing schemes (SSSs) are discussed in the
setting where security and share size are measured by (conditional)
min-entropies.
  We first formalize a unified framework of SSSs based on (conditional) R\'enyi
entropies, which includes SSSs based on Shannon and min entropies etc. as
special cases. By deriving the lower bound of share sizes in terms of R\'enyi
entropies based on the technique introduced by Iwamoto-Shikata, we obtain the
lower bounds of share sizes measured by min entropies as well as by Shannon
entropies in a unified manner.
  As the main contributions of this paper, we show two existential results of
non-perfect SSSs based on min-entropies under several important settings. We
first show that there exists a non-perfect SSS for arbitrary binary secret
information and arbitrary monotone access structure. In addition, for every
integers $k$ and $n$ ($k \le n$), we prove that the ideal non-perfect
$(k,n)$-threshold scheme exists even if the distribution of the secret is not
uniformly distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5897</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5897</id><created>2014-01-23</created><updated>2014-04-18</updated><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>A Generalization of Threshold Saturation: Application to Spatially
  Coupled BICM-ID</title><categories>cs.IT math.IT</categories><comments>accepted for publication in Proc. ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial coupling was proved to improve the belief-propagation (BP)
performance up to the maximum-a-posteriori (MAP) performance. This paper
addresses an extended class of spatially coupled (SC) systems. A potential
function is derived for characterizing a lower bound on the BP performance of
the extended SC systems, and shown to be different from the potential for the
conventional SC systems. This may imply that the BP performance for the
extended SC systems does not coincide with the MAP performance for the
corresponding uncoupled system. SC bit-interleaved coded modulation with
iterative decoding (BICM-ID) is also investigated as an application of the
extended SC systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5899</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5899</id><created>2014-01-23</created><updated>2014-02-11</updated><authors><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Liang</keyname><forenames>Junli</forenames></author><author><keyname>Zheng</keyname><forenames>Nanning</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>Kernel Least Mean Square with Adaptive Kernel Size</title><categories>stat.ML cs.LG</categories><comments>25 pages, 9 figures, and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel adaptive filters (KAF) are a class of powerful nonlinear filters
developed in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is
usually the default kernel in KAF algorithms, but selecting the proper kernel
size (bandwidth) is still an open important issue especially for learning with
small sample sizes. In previous research, the kernel size was set manually or
estimated in advance by Silvermans rule based on the sample distribution. This
study aims to develop an online technique for optimizing the kernel size of the
kernel least mean square (KLMS) algorithm. A sequential optimization strategy
is proposed, and a new algorithm is developed, in which the filter weights and
the kernel size are both sequentially updated by stochastic gradient algorithms
that minimize the mean square error (MSE). Theoretical results on convergence
are also presented. The excellent performance of the new algorithm is confirmed
by simulations on static function estimation and short term chaotic time series
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5900</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5900</id><created>2014-01-23</created><authors><author><keyname>Wang</keyname><forenames>Nan</forenames></author><author><keyname>Melchior</keyname><forenames>Jan</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image
  Statistics</title><categories>cs.NE cs.LG stat.ML</categories><comments>Current version is only an early manuscript and is subject to further
  change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theoretical analysis of Gaussian-binary restricted Boltzmann
machines (GRBMs) from the perspective of density models. The key aspect of this
analysis is to show that GRBMs can be formulated as a constrained mixture of
Gaussians, which gives a much better insight into the model's capabilities and
limitations. We show that GRBMs are capable of learning meaningful features
both in a two-dimensional blind source separation task and in modeling natural
images. Further, we show that reported difficulties in training GRBMs are due
to the failure of the training algorithm rather than the model itself. Based on
our analysis we are able to propose several training recipes, which allowed
successful and fast training in our experiments. Finally, we discuss the
relationship of GRBMs to several modifications that have been proposed to
improve the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5910</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5910</id><created>2014-01-23</created><updated>2014-01-24</updated><authors><author><keyname>Aransay</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Divas&#xf3;n</keyname><forenames>Jose</forenames></author></authors><title>Applications of the Gauss-Jordan algorithm, done right</title><categories>cs.LO cs.SC</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Algebra systems are widely spread because of some of their
remarkable features such as their ease of use and performance. Nonetheless,
this focus on performance sometimes leads to unwanted consequences: algorithms
and computations are implemented and carried out in a way which is sometimes
not transparent to the users, and that can lead to unexpected failures. In this
paper we present a formalisation in a proof assistant system of a \emph{naive}
version of the Gauss-Jordan algorithm, with explicit proofs of some of its
applications, and additionally a process to obtain versions of this algorithm
in two different functional languages (SML and Haskell) by means of code
generation techniques from the verified algorithm. The obtained programs are
then applied to test cases, which, despite the simplicity of the original
algorithm, have shown remarkable features in comparison to some Computer
Algebra systems, such as Mathematica\textsuperscript{\textregistered} (where
some of these computations are even incorrect), or Sage (in comparison to which
the generated programs show a compelling performance). The aim of the paper is
to show that, with the current technology in Theorem Proving, formalising
Linear Algebra procedures is a challenging but rewarding task, which provides
programs that can be compared in some aspects to \emph{state of the art}
procedures in Computer Algebra systems, and whose correctness is formally
proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5919</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5919</id><created>2014-01-23</created><authors><author><keyname>Gadiyar</keyname><forenames>H. Gopalakrishna</forenames></author><author><keyname>Padma</keyname><forenames>R.</forenames></author></authors><title>Hamming's Original Paper Rewritten in Symbolic Form: A Preamble to
  Coding Theory</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><msc-class>11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we try to bring out the ideas of Hamming's classic paper on
coding theory in a form understandable by undergraduate students of
mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5921</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5921</id><created>2014-01-23</created><updated>2014-09-04</updated><authors><author><keyname>McCreesh</keyname><forenames>Ciaran</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>The Shape of the Search Tree for the Maximum Clique Problem, and the
  Implications for Parallel Branch and Bound</title><categories>cs.DC</categories><comments>Substantial revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a maximum clique in a given graph is one of the fundamental NP-hard
problems. We compare two multi-core thread-parallel adaptations of a
state-of-the-art branch and bound algorithm for the maximum clique problem, and
provide a novel explanation as to why they are successful. We show that load
balance is sometimes a problem, but that the interaction of parallel search
order and the most likely location of solutions within the search space is
often the dominating consideration. We use this explanation to propose a new
low-overhead, scalable work splitting mechanism. Our approach uses explicit
early diversity to avoid strong commitment to the weakest heuristic advice, and
late resplitting for balance. More generally, we argue that for branch and
bound, parallel algorithm design should not be performed independently of the
underlying sequential algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5934</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5934</id><created>2014-01-23</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Adnan</forenames></author><author><keyname>Umair</keyname><forenames>Muhammad</forenames></author><author><keyname>Choudry</keyname><forenames>Muhammad Aamer Saleem</forenames></author></authors><title>Accelerated Assistant to SubOptimum Receiver for Multi Carrier Code
  Division Multiple Access System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multiple Input Multiple output system are considered to be the strongest
candidate for the maximum utilization of available bandwidth. In this paper,
the MIMO system with the combination of Multi-Carrier Code Division Multiple
Access and Space Time Coding using the Alamoutis scheme is considered. A
Genetic Algorithm based receiver with an exceptional relationship between
filter weights while detecting symbols is proposed. This scheme has better
Convergence Rate and Bit Error Rate than the Fast-LMS Adaptive receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5951</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5951</id><created>2014-01-23</created><authors><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Sebti</keyname><forenames>Nadia Ouali</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>An Efficient Algorithm for the Equation Tree Automaton via the
  $k$-C-Continuations</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Champarnaud and Ziadi, and Khorsi et al. show how to compute the equation
automaton of word regular expression $E$ via the $k$-C-Continuations.
  Kuske and Meinecke extend the computation of the equation automaton to a
regular tree expression $E$ over a ranked alphabet $\Sigma$ and produce a
$O(R\cdot|E|^2)$ time and space complexity algorithm, where $R$ is the maximal
rank of a symbol occurring in $\Sigma$ and $|E|$ is the size of $E$. In this
paper, we give a full description of the algorithm based on the acyclic
minimization of Revuz. Our algorithm, which is performed in an $O(|Q|\cdot|E|)$
time and space complexity, where $|Q|$ is the number of states of the produced
automaton, is more efficient than the one obtained by Kuske and Meinecke.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5953</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5953</id><created>2014-01-23</created><authors><author><keyname>Sankaran</keyname><forenames>Abhisekh</forenames></author><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author></authors><title>A Generalization of the {\L}o\'s-Tarski Preservation Theorem over
  Classes of Finite Structures</title><categories>cs.LO math.LO</categories><comments>28 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a generalization of the {\L}o\'s-Tarski preservation theorem
via the semantic notion of \emph{preservation under substructures modulo
$k$-sized cores}. It was shown earlier that over arbitrary structures, this
semantic notion for first-order logic corresponds to definability by
$\exists^k\forall^*$ sentences. In this paper, we identify two properties of
classes of finite structures that ensure the above correspondence. The first is
based on well-quasi-ordering under the embedding relation. The second is a
logic-based combinatorial property that strictly generalizes the first. We show
that starting with classes satisfying any of these properties, the classes
obtained by applying operations like disjoint union, cartesian and tensor
products, or by forming words and trees over the classes, inherit the same
property. As a fallout, we obtain interesting classes of structures over which
an effective version of the {\L}o\'s-Tarski theorem holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5959</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5959</id><created>2014-01-23</created><authors><author><keyname>Lange-Hegermann</keyname><forenames>Markus</forenames></author></authors><title>The Differential Dimension Polynomial for Characterizable Differential
  Ideals</title><categories>math.AC cs.SC</categories><msc-class>13N99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the differential dimension polynomial from prime differential
ideals to characterizable differential ideals. Its computation is algorithmic,
its degree and leading coefficient remain differential birational invariants,
and it decides equality of characterizable differential ideals contained in
each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5966</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5966</id><created>2014-01-23</created><authors><author><keyname>Hosseini</keyname><forenames>Hossein</forenames></author><author><keyname>Goli</keyname><forenames>Ali</forenames></author><author><keyname>Marvasti</keyname><forenames>Neda Barzegar</forenames></author><author><keyname>Azghani</keyname><forenames>Masoume</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>On Image Block Loss Restoration Using the Sparsity Pattern as Side
  Information</title><categories>cs.MM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an image block loss restoration method based on the
notion of sparse representation. The sparsity pattern is exploited as side
information to efficiently restore block losses, by iteratively imposing the
constraints of spatial and transform domains on the corrupted image. Two novel
features, including a pre-interpolation and a criterion for stopping the
iterations, are added for performance improvement. Besides, a scheme is
presented for no-reference quality estimation of the restored image with
respect to the original and sparse images. To the best of our knowledge, this
is the first attempt to estimate the image quality in the restoration methods.
Also, to deal with practical applications, we develop a technique to transmit
the side information along with the image. In this technique, we first compress
the side information and then embed its LDPC coded version in the least
significant bits of the image pixels. This technique ensures the error-free
transmission of the side information, while causing only a small perturbation
on the transmitted image. Mathematical analysis and extensive simulations are
performed to evaluate the method and investigate the efficiency of the proposed
techniques. The results verify that the suggested method outperforms its
counterparts for image block loss restoration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5980</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5980</id><created>2014-01-23</created><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Pulman</keyname><forenames>Stephen</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>Reasoning about Meaning in Natural Language with Compact Closed
  Categories and Frobenius Algebras</title><categories>cs.CL cs.AI math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compact closed categories have found applications in modeling quantum
information protocols by Abramsky-Coecke. They also provide semantics for
Lambek's pregroup algebras, applied to formalizing the grammatical structure of
natural language, and are implicit in a distributional model of word meaning
based on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh
used the product category of pregroups with vector spaces and provided a
distributional model of meaning for sentences. We recast this theory in terms
of strongly monoidal functors and advance it via Frobenius algebras over vector
spaces. The former are used to formalize topological quantum field theories by
Atiyah and Baez-Dolan, and the latter are used to model classical data in
quantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us
to work in a single space in which meanings of words, phrases, and sentences of
any structure live. Hence we can compare meanings of different language
constructs and enhance the applicability of the theory. We report on
experimental results on a number of language tasks and verify the theoretical
predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5986</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5986</id><created>2014-01-22</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>How are excellent (highly cited) papers defined in bibliometrics? A
  quantitative analysis of the literature</title><categories>cs.DL</categories><comments>Accepted for publication in Research Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the subject of research excellence has received increasing attention (in
science policy) over the last few decades, increasing numbers of bibliometric
studies have been published dealing with excellent papers. However, many
different methods have been used in these studies to identify excellent papers.
The present quantitative analysis of the literature has been carried out in
order to acquire an overview of these methods and an indication of an &quot;average&quot;
or &quot;most frequent&quot; bibliometric practice. The search in the Web of Science
yielded 321 papers dealing with &quot;highly cited&quot;, &quot;most cited&quot;, &quot;top cited&quot; and
&quot;most frequently cited&quot;. Of the 321 papers, 16 could not be used in this study.
In around 80% of the papers analyzed in this study, a quantitative definition
has been provided with which to identify excellent papers. With definitions
which relate to an absolute number, either a certain number of top cited papers
(58%) or papers with a minimum number of citations are selected (17%). Around
23% worked with percentile rank classes. Over these papers, there is an
arithmetic average of the top 7.6% (arithmetic average) or of the top 3%
(median). The top 1% is used most frequently in the papers, followed by the top
10%. With the thresholds presented in this study, in future, it will be
possible to identify excellent papers based on an &quot;average&quot; or &quot;most frequent&quot;
practice among bibliometricians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.5996</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.5996</id><created>2014-01-23</created><authors><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author><author><keyname>Lin</keyname><forenames>Tingting</forenames></author></authors><title>Collaboration in the open-source arena: The WebKit case</title><categories>cs.CY cs.SI</categories><comments>As submitted to ACM SIG MIS Computers and People Research (CPR) May
  29 - 31, 2014 Singapore</comments><acm-class>D.2.9; K.6.3; K.6.4</acm-class><doi>10.1145/2599990.2600009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an era of software crisis, the move of firms towards distributed software
development teams is being challenged by emerging collaboration issues. On this
matter, the open-source phenomenon may shed some light, as successful cases on
distributed collaboration in the open-source community have been recurrently
reported. In this paper, we explore the collaboration networks in the WebKit
open-source project, by mining WebKit's source-code version-control-system data
with Social Network Analysis (SNA). Our approach allows us to observe how key
events in the mobile-device industry have affected the WebKit collaboration
network over time. With our findings, we show the explanation power from
network visualizations capturing the collaborative dynamics of a high-networked
software project over time; and highlight the power of the open-source fork
concept as a nexus enabling both features of competition and collaboration. We
also reveal the WebKit project as a valuable research site manifesting the
novel notion of open-coopetition, where rival firms collaborate with
competitors in the open-source community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6000</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6000</id><created>2014-01-23</created><authors><author><keyname>Jaberi</keyname><forenames>Raed</forenames></author></authors><title>On computing the $2$-vertex-connected components of directed graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of computing the $2$-vertex-connected
components ($2$-vccs) of directed graphs. We present two new algorithms for
solving this problem. The first algorithm runs in $O(mn^{2})$ time, the second
in $O(nm)$ time. Furthermore, we show that the old algorithm of Erusalimskii
and Svetlov runs in $O(nm^{2})$ time. In this paper, we investigate the
relationship between $2$-vccs and dominator trees. We also present an algorithm
for computing the $3$-vertex-connected components ($3$-vccs) of a directed
graph in $O(n^{3}m)$ time, and we show that the $k$-vertex-connected components
($k$-vccs) of a directed graph can be computed in $O(mn^{2k-3})$ time. Finally,
we consider three applications of our new algorithms, which are approximation
algorithms for problems that are generalization of the problem of approximating
the smallest $2$-vertex-connected spanning subgraph of $2$-vertex-connected
directed graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6002</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6002</id><created>2014-01-22</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Notton</keyname><forenames>Gilles</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Dahmani</keyname><forenames>Kahina</forenames><affiliation>LRIA</affiliation></author></authors><title>Numerical weather prediction or stochastic modeling: an objective
  criterion of choice for the global radiation forecasting</title><categories>stat.AP cs.LG</categories><comments>International Journal of Energy Technology and Policy (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous methods exist and were developed for global radiation forecasting.
The two most popular types are the numerical weather predictions (NWP) and the
predictions using stochastic approaches. We propose to compute a parameter
noted constructed in part from the mutual information which is a quantity that
measures the mutual dependence of two variables. Both of these are calculated
with the objective to establish the more relevant method between NWP and
stochastic models concerning the current problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6011</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6011</id><created>2014-01-23</created><authors><author><keyname>Sagraloff</keyname><forenames>Michael</forenames></author></authors><title>A Near-Optimal Algorithm for Computing Real Roots of Sparse Polynomials</title><categories>cs.NA cs.CC cs.SC math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p\in\mathbb{Z}[x]$ be an arbitrary polynomial of degree $n$ with $k$
non-zero integer coefficients of absolute value less than $2^\tau$. In this
paper, we answer the open question whether the real roots of $p$ can be
computed with a number of arithmetic operations over the rational numbers that
is polynomial in the input size of the sparse representation of $p$. More
precisely, we give a deterministic, complete, and certified algorithm that
determines isolating intervals for all real roots of $p$ with
$O(k^3\cdot\log(n\tau)\cdot \log n)$ many exact arithmetic operations over the
rational numbers.
  When using approximate but certified arithmetic, the bit complexity of our
algorithm is bounded by $\tilde{O}(k^4\cdot n\tau)$, where $\tilde{O}(\cdot)$
means that we ignore logarithmic. Hence, for sufficiently sparse polynomials
(i.e. $k=O(\log^c (n\tau))$ for a positive constant $c$), the bit complexity is
$\tilde{O}(n\tau)$. We also prove that the latter bound is optimal up to
logarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6013</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6013</id><created>2014-01-23</created><updated>2016-01-05</updated><authors><author><keyname>Li</keyname><forenames>Linhao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Hu</keyname><forenames>Qinghua</forenames></author><author><keyname>Cai</keyname><forenames>Sijia</forenames></author></authors><title>Efficient Background Modeling Based on Sparse Representation and Outlier
  Iterative Removal</title><categories>cs.CV</categories><comments>12pages, 10figures</comments><acm-class>G.1.2; H.5.1; I.2.10; I.4.7; I.4.9</acm-class><doi>10.1109/TCSVT.2014.2380195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background modeling is a critical component for various vision-based
applications. Most traditional methods tend to be inefficient when solving
large-scale problems. In this paper, we introduce sparse representation into
the task of large scale stable background modeling, and reduce the video size
by exploring its 'discriminative' frames. A cyclic iteration process is then
proposed to extract the background from the discriminative frame set. The two
parts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm.
The algorithm operates in tensor space to obey the natural data structure of
videos. Experimental results show that a few discriminative frames determine
the performance of the background extraction. Further, SOIR can achieve high
accuracy and high speed simultaneously when dealing with real video sequences.
Thus, SOIR has an advantage in solving large-scale tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6015</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6015</id><created>2014-01-23</created><authors><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Primi</keyname><forenames>Marco</forenames></author><author><keyname>Schiper</keyname><forenames>Nicolas</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author></authors><title>Ring Paxos: High-Throughput Atomic Broadcast</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Atomic broadcast is an important communication primitive often used to
implement state-machine replication. Despite the large number of atomic
broadcast algorithms proposed in the literature, few papers have discussed how
to turn these algorithms into efficient executable protocols. This paper
focuses on a class of atomic broadcast algorithms based on Paxos, with its
corresponding desirable properties: safety under asynchrony assumptions,
liveness under weak synchrony assumptions, and resiliency-optimality. The paper
presents two protocols, M-Ring Paxos and U-Ring Paxos, derived from Paxos. The
protocols inherit the properties of Paxos and can be implemented very
efficiently. We report a detailed performance analysis of M-Ring Paxos and
U-Ring Paxos and compare them to other atomic broadcast protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6020</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6020</id><created>2014-01-23</created><authors><author><keyname>Tofighi</keyname><forenames>Ghassem</forenames></author><author><keyname>Raahemifar</keyname><forenames>Kaamran</forenames></author><author><keyname>Venetsanopoulos</keyname><forenames>Anastasios N.</forenames></author></authors><title>A Brief Review on Models for Performance Evaluation in DSS Architecture</title><categories>cs.PF cs.SE</categories><comments>Find it online here: http://www.jscse.com/papers/?vol=3&amp;no=3&amp;n=68</comments><doi>10.7321/jscse.v3.n3.68</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Distributed Software Systems are used these days by many people in the real
time operations and modern enterprise applications. One of the most important
and essential attributes of measurements for the quality of service of
distributed software is performance. Performance models can be employed at
early stages of the software development cycle to characterize the quantitative
behavior of software systems. In this research, performance models based on
fuzzy logic approach, queuing network approach and Petri net approach have been
reviewed briefly. One of the most common ways in performance analysis of
distributed software systems is translating the UML diagrams to mathematical
modeling languages for the description of distributed systems such as queuing
networks or Petri nets. In this paper, some of these approaches are reviewed
briefly. Attributes which are used for performance modeling in the literature
are mostly machine based. On the other hand, end users and client parameters
for performance evaluation are not covered extensively. In this way, future
research could be based on developing hybrid models to capture user decision
variables which make system performance evaluation more user driven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6022</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6022</id><created>2014-01-23</created><authors><author><keyname>Cai</keyname><forenames>Xiang</forenames></author><author><keyname>Nithyanand</keyname><forenames>Rishab</forenames></author><author><keyname>Johnson</keyname><forenames>Rob</forenames></author></authors><title>New Approaches to Website Fingerprinting Defenses</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Website fingerprinting attacks enable an adversary to infer which website a
victim is visiting, even if the victim uses an encrypting proxy, such as Tor.
Previous work has shown that all proposed defenses against website
fingerprinting attacks are ineffective.
  This paper advances the study of website fingerprinting attacks and defenses
in two ways. First, we develop bounds on the trade-off between security and
bandwidth overhead that any fingerprinting defense scheme can achieve. This
enables us to compare schemes with different security/overhead trade-offs by
comparing how close they are to the lower bound. We then refine, implement, and
evaluate the Congestion Sensitive BuFLO scheme outlined by Cai, et al.
CS-BuFLO, which is based on the provably-secure BuFLO defense proposed by Dyer,
et al., was not fully-specified by Cai, et al, but has nonetheless attracted
the attention of the Tor developers. Our experiments find that CS-BuFLO has
high overhead (around 2.3-2.8x) but can get 6x closer to the bandwidth/security
trade-off lower bound than Tor or plain SSH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6023</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6023</id><created>2014-01-23</created><updated>2015-05-21</updated><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>A Unified Approach for Network Information Theory</title><categories>cs.IT math.IT</categories><comments>52 pages, 7 figures, submitted to IEEE Transactions on Information
  theory, a shorter version will appear in Proc. IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we take a unified approach for network information theory and
prove a coding theorem, which can recover most of the achievability results in
network information theory that are based on random coding. The final
single-letter expression has a very simple form, which was made possible by
many novel elements such as a unified framework that represents various network
problems in a simple and unified way, a unified coding strategy that consists
of a few basic ingredients but can emulate many known coding techniques if
needed, and new proof techniques beyond the use of standard covering and
packing lemmas. For example, in our framework, sources, channels, states and
side information are treated in a unified way and various constraints such as
cost and distortion constraints are unified as a single joint-typicality
constraint.
  Our theorem can be useful in proving many new achievability results easily
and in some cases gives simpler rate expressions than those obtained using
conventional approaches. Furthermore, our unified coding can strictly
outperform existing schemes. For example, we obtain a generalized
decode-compress-amplify-and-forward bound as a simple corollary of our main
theorem and show it strictly outperforms previously known coding schemes. Using
our unified framework, we formally define and characterize three types of
network duality based on channel input-output reversal and network flow
reversal combined with packing-covering duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6024</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6024</id><created>2014-01-23</created><authors><author><keyname>Slawski</keyname><forenames>Martin</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author><author><keyname>Lutsik</keyname><forenames>Pavlo</forenames></author></authors><title>Matrix factorization with Binary Components</title><categories>stat.ML cs.LG</categories><comments>appeared in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an application in computational biology, we consider low-rank
matrix factorization with $\{0,1\}$-constraints on one of the factors and
optionally convex constraints on the second one. In addition to the
non-convexity shared with other matrix factorization schemes, our problem is
further complicated by a combinatorial constraint set of size $2^{m \cdot r}$,
where $m$ is the dimension of the data points and $r$ the rank of the
factorization. Despite apparent intractability, we provide - in the line of
recent work on non-negative matrix factorization by Arora et al. (2012) - an
algorithm that provably recovers the underlying factorization in the exact case
with $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain this
result, we use theory around the Littlewood-Offord lemma from combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6025</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6025</id><created>2014-01-23</created><updated>2016-02-29</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>Cryptanalysis of McEliece Cryptosystem Based on Algebraic Geometry Codes
  and their subcodes</title><categories>cs.IT math.AG math.IT</categories><comments>A part of the material of this article has been published at the
  conferences ISIT 2014 with title &quot;A polynomial time attack against AG code
  based PKC&quot; and 4ICMCTA with title &quot;Crypt. of PKC that use subcodes of AG
  codes&quot;. This long version includes detailed proofs and new results: the
  proceedings articles only considered the reconstruction of ECP while we
  discuss here the reconstruction of ECA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give polynomial time attacks on the McEliece public key cryptosystem based
either on algebraic geometry (AG) codes or on small codimensional subcodes of
AG codes. These attacks consist in the blind reconstruction either of an
\emph{Error Correcting Pair (ECP)}, or an \emph{Error Correcting Array (ECA)}
from the single data of an arbitrary generator matrix of a code. Take notice
that the choice of computing an ECP or an ECA depends on the number of errors
that we need to correct: an ECP provides a decoding algorithm that corrects up
to $\frac{d^*-1-g}{2}$ errors, where $d^*$ denotes the designed distance and
$g$ denotes the genus of the corresponding curve; while with an ECA the
decoding algorithm arrives up to $\frac{d^*-1}{2}$ errors. Roughly speaking,
for a public code of length $n$ over $\mathbb F_q$, these attacks run in
$O(n^4\log (n))$ operations in $\mathbb F_q$ for the reconstruction of an ECP
and $O(n^5)$ operations for the reconstruction of an ECA. A probabilistic
shortcut allows to reduce the complexities respectively to $O(n^{3+\varepsilon}
\log (n))$ and $O(n^{4+\varepsilon})$. Compared to the previous known attack
due to Faure and Minder, our attack is efficient on codes from curves of
arbitrary genus. Furthermore we investigate how far these methods apply to
subcodes of AG codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6030</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6030</id><created>2014-01-23</created><updated>2014-12-09</updated><authors><author><keyname>Sysoev</keyname><forenames>Sergey</forenames></author></authors><title>The Effective Solving of the Tasks from NP by a Quantum Computer</title><categories>cs.CC</categories><comments>The paper is submitted to the IEEE Transactions on Information Theory
  journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new model of quantum computation is proposed, for which an effective
algorithm of solving any task in NP is described. The work is based and
inspired be the Grover's algorithm for solving NP-tasks with quadratic speedup
compared to the classical computation model. The provided model and algorithm
exhibit the exponential speedup over that described by Grover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6036</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6036</id><created>2014-01-23</created><updated>2014-11-24</updated><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author></authors><title>On involutions in extremal self-dual codes and the dual distance of semi
  self-dual codes</title><categories>math.CO cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical result of Conway and Pless is that a natural projection of the
fixed code of an automorphism of odd prime order of a self-dual binary linear
code is self-dual. In this paper we prove that the same holds for involutions
under some (quite strong) conditions on the codes. In order to prove it, we
introduce a new family of binary codes: the semi self-dual codes. A binary
self-orthogonal code is called semi self-dual if it contains the all-ones
vector and is of codimension 2 in its dual code. We prove upper bounds on the
dual distance of semi self-dual codes. As an application we get the following:
let C be an extremal self-dual binary linear code of length 24m and s in Aut(C)
be a fixed point free automorphism of order 2. If m is odd or if m=2k with
binom{5k-1}{k-1} odd then C is a free F_2&lt;s&gt;-module. This result has quite
strong consequences on the structure of the automorphism group of such codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6039</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6039</id><created>2014-01-23</created><updated>2014-04-28</updated><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Constant Compositions in the Sphere Packing Bound for Classical-Quantum
  Channels</title><categories>cs.IT math.IT quant-ph</categories><comments>ISIT 2014. Two issues that were left open in Section IV of the first
  version are now solved</comments><journal-ref>Proc. ISIT 2014, pp. 151-155</journal-ref><doi>10.1109/ISIT.2014.6874813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sphere packing bound, in the form given by Shannon, Gallager and
Berlekamp, was recently extended to classical-quantum channels, and it was
shown that this creates a natural setting for combining probabilistic
approaches with some combinatorial ones such as the Lov\'asz theta function. In
this paper, we extend the study to the case of constant composition codes. We
first extend the sphere packing bound for classical-quantum channels to this
case, and we then show that the obtained result is related to a variation of
the Lov\'asz theta function studied by Marton. We then propose a further
extension to the case of varying channels and codewords with a constant
conditional composition given a particular sequence. This extension is then
applied to auxiliary channels to deduce a bound which can be interpreted as an
extension of the Elias bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6048</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6048</id><created>2014-01-23</created><authors><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Shani</keyname><forenames>Guy</forenames></author></authors><title>Replanning in Domains with Partial Information and Sensing Actions</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  565-600, 2012</journal-ref><doi>10.1613/jair.3711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replanning via determinization is a recent, popular approach for online
planning in MDPs. In this paper we adapt this idea to classical, non-stochastic
domains with partial information and sensing actions, presenting a new planner:
SDR (Sample, Determinize, Replan). At each step we generate a solution plan to
a classical planning problem induced by the original problem. We execute this
plan as long as it is safe to do so. When this is no longer the case, we
replan. The classical planning problem we generate is based on the
translation-based approach for conformant planning introduced by Palacios and
Geffner. The state of the classical planning problem generated in this approach
captures the belief state of the agent in the original problem. Unfortunately,
when this method is applied to planning problems with sensing, it yields a
non-deterministic planning problem that is typically very large. Our main
contribution is the introduction of state sampling techniques for overcoming
these two problems. In addition, we introduce a novel, lazy, regression-based
method for querying the agents belief state during run-time. We provide a
comprehensive experimental evaluation of the planner, showing that it scales
better than the state-of-the-art CLG planner on existing benchmark problems,
but also highlighting its weaknesses with new domains. We also discuss its
theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6049</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6049</id><created>2014-01-23</created><authors><author><keyname>Hoshino</keyname><forenames>Richard</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Generating Approximate Solutions to the TTP using a Linear Distance
  Relaxation</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  257-286, 2012</journal-ref><doi>10.1613/jair.3713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some domestic professional sports leagues, the home stadiums are located
in cities connected by a common train line running in one direction. For these
instances, we can incorporate this geographical information to determine
optimal or nearly-optimal solutions to the n-team Traveling Tournament Problem
(TTP), an NP-hard sports scheduling problem whose solution is a double
round-robin tournament schedule that minimizes the sum total of distances
traveled by all n teams. We introduce the Linear Distance Traveling Tournament
Problem (LD-TTP), and solve it for n=4 and n=6, generating the complete set of
possible solutions through elementary combinatorial techniques. For larger n,
we propose a novel &quot;expander construction&quot; that generates an approximate
solution to the LD-TTP. For n congruent to 4 modulo 6, we show that our
expander construction produces a feasible double round-robin tournament
schedule whose total distance is guaranteed to be no worse than 4/3 times the
optimal solution, regardless of where the n teams are located. This
4/3-approximation for the LD-TTP is stronger than the currently best-known
ratio of 5/3 + epsilon for the general TTP. We conclude the paper by applying
this linear distance relaxation to general (non-linear) n-team TTP instances,
where we develop fast approximate solutions by simply &quot;assuming&quot; the n teams
lie on a straight line and solving the modified problem. We show that this
technique surprisingly generates the distance-optimal tournament on all
benchmark sets on 6 teams, as well as close-to-optimal schedules for larger n,
even when the teams are located around a circle or positioned in
three-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6050</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6050</id><created>2014-01-23</created><authors><author><keyname>Zhao</keyname><forenames>Hai</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaotian</forenames></author><author><keyname>Kit</keyname><forenames>Chunyu</forenames></author></authors><title>Integrative Semantic Dependency Parsing via Efficient Large-scale
  Feature Selection</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  203-233, 2013</journal-ref><doi>10.1613/jair.3717</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic parsing, i.e., the automatic derivation of meaning representation
such as an instantiated predicate-argument structure for a sentence, plays a
critical role in deep processing of natural language. Unlike all other top
systems of semantic dependency parsing that have to rely on a pipeline
framework to chain up a series of submodels each specialized for a specific
subtask, the one presented in this article integrates everything into one
model, in hopes of achieving desirable integrity and practicality for real
applications while maintaining a competitive performance. This integrative
approach tackles semantic parsing as a word pair classification problem using a
maximum entropy classifier. We leverage adaptive pruning of argument candidates
and large-scale feature selection engineering to allow the largest feature
space ever in use so far in this field, it achieves a state-of-the-art
performance on the evaluation data set for CoNLL-2008 shared task, on top of
all but one top pipeline system, confirming its feasibility and effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6053</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6053</id><created>2014-01-23</created><authors><author><keyname>Nasrabadi</keyname><forenames>Ebrahim</forenames></author><author><keyname>Koch</keyname><forenames>Ronald</forenames></author></authors><title>Flows over time in time-varying networks</title><categories>cs.SY</categories><msc-class>90B10, 49N05, 05C38, 90C46</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much research on network flows over time due to their
important role in real world applications. This has led to many results, but
the more challenging continuous time model still lacks some of the key concepts
and techniques that are the cornerstones of static network flows. The aim of
this paper is to advance the state of the art for dynamic network flows by
developing the continuous time analogues of the theory for static network
flows. Specifically, we make use of ideas from the static case to establish a
reduced cost optimality condition, a negative cycle optimality condition, and a
strong duality result for a very general class of network flows over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6054</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6054</id><created>2014-01-23</created><updated>2014-04-18</updated><authors><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author></authors><title>Computing the (number or sum of) inverses of Euler's totient and other
  multiplicative functions</title><categories>cs.DM cs.DS math.NT</categories><msc-class>11Y16, 11Y50, 11Y55, 11A05, 11N25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generic algorithm for computing the inverses of a multiplicative
function. We illustrate our algorithm with Euler's totient function and the sum
of k-th powers of divisors. Our approach can be further adapted for computing
certain functions of the inverses, such as their quantity, the smallest/largest
inverse, which may be computed without and possibly faster than the inverses
themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6058</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6058</id><created>2014-01-23</created><authors><author><keyname>Davenport</keyname><forenames>James R. A.</forenames></author><author><keyname>DeLine</keyname><forenames>Robert</forenames></author></authors><title>The Readability of Tweets and their Geographic Correlation with
  Education</title><categories>cs.SI physics.soc-ph</categories><comments>4 page note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter has rapidly emerged as one of the largest worldwide venues for
written communication. Thanks to the ease with which vast quantities of tweets
can be mined, Twitter has also become a source for studying modern linguistic
style. The readability of text has long provided a simple method to
characterize the complexity of language and ease that documents may be
understood by readers. In this note we use a modified version of the Flesch
Reading Ease formula, applied to a corpus of 17.4 million tweets. We find
tweets have characteristically more difficult readability scores compared to
other short format communication, such as SMS or chat. This linguistic
difference is insensitive to the presence of &quot;hashtags&quot; within tweets. By
utilizing geographic data provided by 2% of users, joined with &quot;ZIP Code
Tabulation Area&quot; (ZCTA) level education data from the U.S. Census, we find an
intriguing correlation between the average readability and the college
graduation rate within a ZCTA. This points towards a difference in either the
underlying language, or a change in the type of content being tweeted in these
areas
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6060</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6060</id><created>2014-01-23</created><updated>2014-10-10</updated><authors><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Achieving Marton's Region for Broadcast Channels Using Polar Codes</title><categories>cs.IT math.IT</categories><comments>26 pages, 11 figures, accepted to IEEE Trans. Inform. Theory and
  presented in part at ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents polar coding schemes for the 2-user discrete memoryless
broadcast channel (DM-BC) which achieve Marton's region with both common and
private messages. This is the best achievable rate region known to date, and it
is tight for all classes of 2-user DM-BCs whose capacity regions are known. To
accomplish this task, we first construct polar codes for both the superposition
as well as the binning strategy. By combining these two schemes, we obtain
Marton's region with private messages only. Finally, we show how to handle the
case of common information. The proposed coding schemes possess the usual
advantages of polar codes, i.e., they have low encoding and decoding complexity
and a super-polynomial decay rate of the error probability.
  We follow the lead of Goela, Abbe, and Gastpar, who recently introduced polar
codes emulating the superposition and binning schemes. In order to align the
polar indices, for both schemes, their solution involves some degradedness
constraints that are assumed to hold between the auxiliary random variables and
the channel outputs. To remove these constraints, we consider the transmission
of $k$ blocks and employ a chaining construction that guarantees the proper
alignment of the polarized indices. The techniques described in this work are
quite general, and they can be adopted to many other multi-terminal scenarios
whenever there polar indices need to be aligned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6063</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6063</id><created>2014-01-23</created><updated>2014-08-26</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Jan&#xdf;en</keyname><forenames>Gisbert</forenames></author></authors><title>Resource cost results for one-way entanglement distillation and state
  merging of compound and arbitrarily varying quantum sources</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>Improved presentation. Close to the published version. Results
  unchanged. 25 pages, 0 figures</comments><journal-ref>J. Math. Phys. 55, 082208 (2014)</journal-ref><doi>10.1063/1.4893635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider one-way quantum state merging and entanglement distillation under
compound and arbitrarily varying source models. Regarding quantum compound
sources, where the source is memoryless, but the source state an unknown member
of a certain set of density matrices, we continue investigations begun in the
work of Bjelakovi\'c et. al. [Universal quantum state merging, J. Math. Phys.
54, 032204 (2013)] and determine the classical as well as entanglement cost of
state merging. We further investigate quantum state merging and entanglement
distillation protocols for arbitrarily varying quantum sources (AVQS). In the
AVQS model, the source state is assumed to vary in an arbitrary manner for each
source output due to environmental fluctuations or adversarial manipulation. We
determine the one-way entanglement distillation capacity for AVQS, where we
invoke the famous robustification and elimination techniques introduced by R.
Ahlswede. Regarding quantum state merging for AVQS we show by example, that the
robustification and elimination based approach generally leads to suboptimal
entanglement as well as classical communication rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6069</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6069</id><created>2014-01-23</created><updated>2014-04-28</updated><authors><author><keyname>Barletta</keyname><forenames>Luca</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>On Continuous-Time White Phase Noise Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, Accepted for presentation. ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A continuous-time model for the additive white Gaussian noise (AWGN) channel
in the presence of white (memoryless) phase noise is proposed and discussed. It
is shown that for linear modulation the output of the baud-sampled filter
matched to the shaping waveform represents a sufficient statistic. The analysis
shows that the phase noise channel has the same information rate as an AWGN
channel but with a penalty on the average signal-to-noise ratio, the amount of
penalty depending on the phase noise statistic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6070</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6070</id><created>2014-01-23</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Ghosh</keyname><forenames>Anirban</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>On Fence Patrolling by Mobile Agents</title><categories>cs.DS cs.DM math.CO</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a fence needs to be protected (perpetually) by $k$ mobile agents
with maximum speeds $v_1,\ldots,v_k$ so that no point on the fence is left
unattended for more than a given amount of time. The problem is to determine if
this requirement can be met, and if so, to design a suitable patrolling
schedule for the agents. Alternatively, one would like to find a schedule that
minimizes the \emph{idle time}, that is, the longest time interval during which
some point is not visited by any agent. We revisit this problem, introduced by
Czyzowicz et al.(2011), and discuss several strategies for the cases where the
fence is an open and a closed curve, respectively.
  In particular: (i) we disprove a conjecture by Czyzowicz et al. regarding the
optimality of their Algorithm ${\mathcal A_2}$ for unidirectional patrolling of
a closed fence; (ii) we present an algorithm with a lower idle time for
patrolling an open fence, improving an earlier result of Kawamura and
Kobayashi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6082</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6082</id><created>2013-12-21</created><authors><author><keyname>Nadia</keyname><forenames>Afsana</forenames></author><author><keyname>Chowdhury</keyname><forenames>Arifur Rahim</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Shoayeb</forenames></author><author><keyname>Islam</keyname><forenames>Md. Imdadul</forenames></author><author><keyname>Amin</keyname><forenames>M. R.</forenames></author></authors><title>Performance Evaluation of Two-Hop Wireless Link under Nakagami-m Fading</title><categories>cs.IT math.IT</categories><journal-ref>IJACSA,Vol. 4,No. 7,July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days, intense research is going on two-hop wireless link under
different fading conditions with its remedial measures. In this paper work, a
two-hop link under three different conditions is considered: (i) MIMO on both
hops, (ii) MISO in first hop and SIMO in second hop and finally (iii) SIMO in
first hop and MISO in second hop. The three models used here give the
flexibility of using STBC (Space Time Block Coding) and combining scheme on any
of the source to relay (S- R) and relay to destination (R-D) link. Even
incorporation of Transmitting Antenna Selection (TAS) is possible on any link.
Here, the variation of SER (Symbol Error Rate) is determined against mean SNR
(Signal-to-Noise Ratio) of R-D link for three different modulation schemes:
BPSK, 8-PSK and 16-PSK, taking the number of antennas and SNR of S-R link as
parameters under Nakagami -m fading condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6083</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6083</id><created>2013-11-25</created><authors><author><keyname>Cheung</keyname><forenames>Kent Tsz Kan</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Maximizing Energy-Efficiency in Multi-Relay OFDMA Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures, 1 table, to appear in Proc. IEEE 2013 56th Global
  Communications Conference (GLOBECOM 2013), Atlanta, USA, December, 2013</comments><journal-ref>Proc. IEEE Global Communications Conference (GLOBECOM 2013),
  Atlanta, GA, December 2013, pp. 2767 - 2772</journal-ref><doi>10.1109/GLOCOM.2013.6831493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution presents a method of obtaining the optimal power and
subcarrier allocations that maximize the energy-efficiency (EE) of a
multi-user, multi-relay, orthogonal frequency division multiple access (OFDMA)
cellular network. Initially, the objective function (OF) is formulated as the
ratio of the spectral-efficiency (SE) over the power consumption of the
network. This OF is shown to be quasi-concave, thus Dinkelbach's method can be
employed for solving it as a series of parameterized concave problems. We
characterize the performance of the aforementioned method by comparing the
optimal solutions obtained to those found using an exhaustive search.
Additionally, we explore the relationship between the achievable SE and EE in
the cellular network upon increasing the number of active users. In general,
increasing the number of users supported by the system benefits both the SE and
EE, and higher SE values may be obtained at the cost of EE, when an increased
power may be allocated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6087</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6087</id><created>2013-12-14</created><authors><author><keyname>Sharma</keyname><forenames>Prerana</forenames></author></authors><title>Efficient Image Encryption and Decryption Using Discrete Wavelet
  Transform and Fractional Fourier Transform</title><categories>cs.CR cs.IT cs.MM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional Fourier transform and chaos functions play a key role in many of
encryption-decryption algorithms. In this work performance of image
encryption-decryption algorithms is quantified and compared using the
computation time i.e. the time consumption of encryption-decryption process and
resemblance of input image to the restored image, quantified by MSE. This work
proposes an improvement in computation-time of image encryptiondecryption
algorithms by utilizing image compression properties of the 2-dimensional
Discrete Wavelet Transform (DWT2). Initially, computation complexity of the
algorithms is evaluated and compared with that of existing algorithms. This
analysis claims the proposed algorithms to be nearly 8 times faster than the
existing algorithms. Further, simulations are performed using MATLAB7.7 to
quantify performance of existing algorithms and the proposed algorithms using
MSE and computation time. The results obtained in these simulations prove that
for the proposed algorithms MSE between restored and original images is lesser
than that of existing algorithms thereby maintaining the robustness of the
existing algorithms. These algorithms are found sensitive to a variation of
1x10-1 in the fractional orders used in encryption-decryption process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6092</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6092</id><created>2013-12-20</created><authors><author><keyname>Engstr&#xf6;m</keyname><forenames>Christopher</forenames></author><author><keyname>Silvestrov</keyname><forenames>Sergei</forenames></author></authors><title>PageRank for evolving link structures</title><categories>cs.IR math.PR</categories><comments>56 pages, 19 figures</comments><msc-class>05C50, 15A18, 15A51, 65C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we will look at the PageRank algorithm used as part of the
ranking process of different Internet pages in search engines by for example
Google. This article has its main focus in the understanding of the behavior of
PageRank as the system dynamically changes either by contracting or expanding
such as when adding or subtracting nodes or links or groups of nodes or links.
In particular we will take a look at link structures consisting of a line of
nodes or a complete graph where every node links to all others.
  We will look at PageRank as the solution of a linear system of equations and
do our examination in both the ordinary normalized version of PageRank as well
as the non-normalized version found by solving the linear system. We will see
that it is possible to find explicit formulas for the PageRank in some simple
link structures and using these formulas take a more in-depth look at the
behavior of the ranking as the system changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6097</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6097</id><created>2014-01-23</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>Polarization as a novel architecture to boost the classical mismatched
  capacity of B-DMCs</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the mismatched capacity of binary discrete memoryless channels
can be improved by channel combining and splitting via Ar{\i}kan's polar
transformations. We also show that the improvement is possible even if the
transformed channels are decoded with a mismatched polar decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6098</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6098</id><created>2014-01-14</created><authors><author><keyname>Wu</keyname><forenames>Guohua</forenames></author><author><keyname>Wang</keyname><forenames>Huilin</forenames></author><author><keyname>Li</keyname><forenames>Haifeng</forenames></author><author><keyname>Pedrycz</keyname><forenames>Witold</forenames></author><author><keyname>Qiu</keyname><forenames>Dishan</forenames></author><author><keyname>Ma</keyname><forenames>Manhao</forenames></author><author><keyname>Liu</keyname><forenames>Jin</forenames></author></authors><title>An adaptive Simulated Annealing-based satellite observation scheduling
  method combined with a dynamic task clustering strategy</title><categories>cs.AI cs.CE</categories><comments>23 pages, 5 figures, 4 tables</comments><msc-class>90B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient scheduling is of great significance to rationally make use of
scarce satellite resources. Task clustering has been demonstrated to realize an
effective strategy to improve the efficiency of satellite scheduling. However,
the previous task clustering strategy is static. That is, it is integrated into
the scheduling in a two-phase manner rather than in a dynamic fashion, without
expressing its full potential in improving the satellite scheduling
performance. In this study, we present an adaptive Simulated Annealing based
scheduling algorithm aggregated with a dynamic task clustering strategy (or
ASA-DTC for short) for satellite observation scheduling problems (SOSPs).
First, we develop a formal model for the scheduling of Earth observing
satellites. Second, we analyze the related constraints involved in the
observation task clustering process. Thirdly, we detail an implementation of
the dynamic task clustering strategy and the adaptive Simulated Annealing
algorithm. The adaptive Simulated Annealing algorithm is efficient, with the
endowment of some sophisticated mechanisms, i.e. adaptive temperature control,
tabu-list based revisiting avoidance mechanism, and intelligent combination of
neighborhood structures. Finally, we report on experimental simulation studies
to demonstrate the competitive performance of ASA-DTC. Moreover, we show that
ASA-DTC is especially effective when SOSPs contain a large number of targets or
these targets are densely distributed in a certain area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6100</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6100</id><created>2014-01-08</created><authors><author><keyname>Harper</keyname><forenames>K. Eric</forenames></author><author><keyname>de Gooijer</keyname><forenames>Thijmen</forenames></author></authors><title>Performance Impact of Lock-Free Algorithms on Multicore Communication
  APIs</title><categories>cs.DC cs.OS cs.PF cs.SE</categories><comments>17 pages, 8 figures, 36 references, Embedded World Conference 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data race conditions in multi-tasking software applications are prevented by
serializing access to shared memory resources, ensuring data consistency and
deterministic behavior. Traditionally tasks acquire and release locks to
synchronize operations on shared memory. Unfortunately, lock management can add
significant processing overhead especially for multicore deployments where
tasks on different cores convoy in queues waiting to acquire a lock.
Implementing more than one lock introduces the risk of deadlock and using
spinlocks constrains which cores a task can run on. The better alternative is
to eliminate locks and validate that real-time properties are met, which is not
directly considered in many embedded applications. Removing the locks is
non-trivial and packaging lock-free algorithms for developers reduces the
possibility of concurrency defects. This paper details how a multicore
communication API implementation is enhanced to support lock-free messaging and
the impact this has on data exchange latency between tasks. Throughput and
latency are compared on Windows and Linux between lock-based and lock-free
implementations for data exchange of messages, packets, and scalars. A model of
the lock-free exchange predicts performance at the system architecture level
and provides a stop criterion for the refactoring. The results show that
migration from single to multicore hardware architectures degrades lock-based
performance, and increases lock-free performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6102</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6102</id><created>2014-01-09</created><authors><author><keyname>Almeida</keyname><forenames>Fernando</forenames></author><author><keyname>Santos</keyname><forenames>Jos&#xe9; D.</forenames></author><author><keyname>Monteiro</keyname><forenames>Jos&#xe9; A.</forenames></author></authors><title>e-commerce business models in the context of web3.0 paradigm</title><categories>cs.CY</categories><comments>12 pages, International Journal of Advanced Information Technology
  (IJAIT) Vol. 3, No. 6, December 2013</comments><msc-class>68Uxx</msc-class><acm-class>H.4.0</acm-class><doi>10.5121/ijait.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web 3.0 promises to have a significant effect in users and businesses. It
will change how people work and play, how companies use information to market
and sell their products, as well as operate their businesses. The basic shift
occurring in Web 3.0 is from information-centric to knowledge-centric patterns
of computing. Web 3.0 will enable people and machines to connect, evolve, share
and use knowledge on an unprecedented scale and in new ways that make our
experience of the Internet better. Additionally, semantic technologies have the
potential to drive significant improvements in capabilities and life cycle
economics through cost reductions, improved efficiencies, enhanced
effectiveness, and new functionalities that were not possible or economically
feasible before. In this paper we look to the semantic web and Web 3.0
technologies as enablers for the creation of value and appearance of new
business models. For that, we analyze the role and impact of Web 3.0 in
business and we identify nine potential business models, based in direct and
undirected revenue sources, which have emerged with the appearance of semantic
web technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6106</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6106</id><created>2013-11-09</created><authors><author><keyname>Wang</keyname><forenames>Cheng-Jun</forenames></author></authors><title>Bringing Reference Groups Back: Agent-based Modeling of the Spiral of
  Silence</title><categories>cs.CY</categories><comments>31 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is threefold: first, to bring reference groups back
into the framework of spiral of silence (SOS) by proposing an extended
framework of dual opinion climate; second, to investigate the boundary
conditions of SOS; third, to identify the characteristics of SOS in terms of
spatial variation and temporal evolution. Modeling SOS with agent-based models,
the findings suggest (1) there is no guarantee of SOS with reference groups
being brought back; (2) Stable existence of SOS is contingent upon the
comparative strength of mass media over reference groups; (3) SOS is
size-dependent upon reference groups and the population; (4) the growth rate of
SOS decreases over time. Thus, this research presents an extension of the SOS
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6108</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6108</id><created>2013-11-19</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Manjupriya</keyname></author><author><keyname>Chithra</keyname><forenames>C. K.</forenames></author><author><keyname>Divya</keyname><forenames>M.</forenames></author></authors><title>Face Verification Using Kernel Principle Component Analysis</title><categories>cs.CV</categories><comments>7 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the beginning stage, face verification is done using easy method of
geometric algorithm models, but the verification route has now developed into a
scientific progress of complicated geometric representation and matching
process. In modern time the skill have enhanced face detection system into the
vigorous focal point. Researchers currently undergoing strong research on
finding face recognition system for wider area information taken under
hysterical elucidation dissimilarity. The proposed face recognition system
consists of a narrative exposition indiscreet preprocessing method, a hybrid
Fourier-based facial feature extraction and a score fusion scheme. We take in
conventional the face detection in unlike cheer up circumstances and at unusual
setting. Image processing, Image detection, Feature removal and Face detection
are the methods used for Face Verification System . This paper focuses mainly
on the issue of toughness to lighting variations. The proposed system has
obtained an average of verification rate on Two-Dimensional images under
different lightening conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6112</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6112</id><created>2013-11-19</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Divya</keyname><forenames>M.</forenames></author><author><keyname>Chithra</keyname><forenames>C. K.</forenames></author><author><keyname>Priya</keyname><forenames>K. Manju</forenames></author></authors><title>Face Verification System based on Integral Normalized Gradient
  Image(INGI)</title><categories>cs.CV</categories><doi>10.5120/11116-6077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Character identification plays a vital role in the contemporary world of
Image processing. It can solve many composite problems and makes humans work
easier. An instance is Handwritten Character detection. Handwritten recognition
is not a novel expertise, but it has not gained community notice until Now. The
eventual aim of designing Handwritten Character recognition structure with an
accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character
recognition system uses the Neural Networks to distinguish them. Neural Network
and structural characteristics are used to instruct and recognize written
characters. After training and testing the exactness rate reached 99%. This
correctness rate is extremely high. In this paper we are exploring image
processing through the Hilditch algorithm foundation and structural
characteristics of a character in the image. And we recognized some character
of the Tamil language, and we are trying to identify all the character of Tamil
In our future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6113</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6113</id><created>2014-01-18</created><authors><author><keyname>Wang</keyname><forenames>Yanqing</forenames></author><author><keyname>Liang</keyname><forenames>Yaowen</forenames></author><author><keyname>Liu</keyname><forenames>Luning</forenames></author><author><keyname>Liu</keyname><forenames>Ying</forenames></author></authors><title>A Motivation Model of Peer Assessment in Programming Language Learning</title><categories>cs.CY</categories><comments>12 pages, 2 figures, 3 tables</comments><acm-class>K.3.1</acm-class><doi>10.1177/0735633115571303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer assessment is an efficient and effective learning assessment method that
has been used widely in diverse fields in higher education. Despite its many
benefits, a fundamental problem in peer assessment is that participants lack
the motivation to assess others' work faithfully and fairly. Non-consensus is a
common challenge that makes the reliability of peer assessment a primary
concern in practices. This research proposes a motivation model that uses
review deviation and radicalization to identify non-consensus in peer
assessment. The proposed model is implemented as a software module in a peer
code review system called EduPCR4. EduPCR4 is able to monitor this measure and
trigger teacher's arbitration when it detects possible non-consensus. An
empirical study conducted in a university-level C programming course showed
that the proposed model and its implementation helped to improve the peer
assessment practices in many aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6118</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6118</id><created>2013-12-24</created><authors><author><keyname>Nirkhi</keyname><forenames>Smita</forenames></author><author><keyname>Dharaskar</keyname><forenames>R. V.</forenames></author></authors><title>Comparative study of Authorship Identification Techniques for Cyber
  Forensics Analysis</title><categories>cs.CY cs.CR cs.IR cs.LG</categories><journal-ref>published 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorship Identification techniques are used to identify the most
appropriate author from group of potential suspects of online messages and find
evidences to support the conclusion. Cybercriminals make misuse of online
communication for sending blackmail or a spam email and then attempt to hide
their true identities to void detection.Authorship Identification of online
messages is the contemporary research issue for identity tracing in cyber
forensics. This is highly interdisciplinary area as it takes advantage of
machine learning, information retrieval, and natural language processing. In
this paper, a study of recent techniques and automated approaches to
attributing authorship of online messages is presented. The focus of this
review study is to summarize all existing authorship identification techniques
used in literature to identify authors of online messages. Also it discusses
evaluation criteria and parameters for authorship attribution studies and list
open questions that will attract future work in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6120</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6120</id><created>2014-01-08</created><authors><author><keyname>Bonfante</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Marion</keyname><forenames>Jean-Yves</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Sabatier</keyname><forenames>Fabrice</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Thierry</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Analysis and Diversion of Duqu's Driver</title><categories>cs.CR</categories><comments>Malware 2013 - 8th International Conference on Malicious and Unwanted
  Software (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The propagation techniques and the payload of Duqu have been closely studied
over the past year and it has been said that Duqu shared functionalities with
Stuxnet. We focused on the driver used by Duqu during the infection, our
contribution consists in reverse-engineering the driver: we rebuilt its source
code and analyzed the mechanisms it uses to execute the payload while avoiding
detection. Then we diverted the driver into a defensive version capable of
detecting injections in Windows binaries, thus preventing further attacks. We
specifically show how Duqu's modified driver would have detected Duqu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6121</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6121</id><created>2013-12-06</created><authors><author><keyname>Sekhar</keyname><forenames>Vorugunti Chandra</forenames></author><author><keyname>Sarvabhatla</keyname><forenames>Mrudula</forenames></author></authors><title>A Robust Password-Based Multi-Server Authentication Scheme</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2013, Tsai et al. cryptanalyzed Yeh et al. scheme and shown that Yeh et
al., scheme is vulnerable to various cryptographic attacks and proposed an
improved scheme. In this poster we will show that Tsai et al., scheme is also
vulnerable to undetectable online password guessing attack, on success of the
attack, the adversary can perform all major cryptographic attacks. As apart of
our contribution, we have proposed an improved scheme which overcomes the
defects in Tsai et al. and Yeh et al. schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6122</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6122</id><created>2014-01-23</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Das</keyname><forenames>Dipankar</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Sivaji</forenames></author></authors><title>Identifying Bengali Multiword Expressions using Semantic Clustering</title><categories>cs.CL</categories><comments>25 pages, 3 figures, 5 tables, International Journal of Linguistics
  and Language Resources (Lingvistic{\ae} Investigationes), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key issues in both natural language understanding and generation
is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge
problem to the precise language processing due to their idiosyncratic nature
and diversity in lexical, syntactical and semantic properties. The semantics of
a MWE cannot be expressed after combining the semantics of its constituents.
Therefore, the formalism of semantic clustering is often viewed as an
instrument for extracting MWEs especially for resource constraint languages
like Bengali. The present semantic clustering approach contributes to locate
clusters of the synonymous noun tokens present in the document. These clusters
in turn help measure the similarity between the constituent words of a
potentially candidate phrase using a vector space model and judge the
suitability of this phrase to be a MWE. In this experiment, we apply the
semantic clustering approach for noun-noun bigram MWEs, though it can be
extended to any types of MWEs. In parallel, the well known statistical models,
namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),
Significance function are also employed to extract MWEs from the Bengali
corpus. The comparative evaluation shows that the semantic clustering approach
outperforms all other competing statistical models. As a by-product of this
experiment, we have started developing a standard lexicon in Bengali that
serves as a productive Bengali linguistic thesaurus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6123</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6123</id><created>2013-12-20</created><authors><author><keyname>Zhu</keyname><forenames>Jinxiao</forenames></author><author><keyname>Chen</keyname><forenames>Yin</forenames></author><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Takahashi</keyname><forenames>Osamu</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Shiratori</keyname><forenames>Norio</forenames></author></authors><title>Secrecy Transmission Capacity in Noisy Wireless Ad Hoc Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>26 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the transmission of confidential messages over noisy
wireless ad hoc networks, where both background noise and interference from
concurrent transmitters affect the received signals. For the random networks
where the legitimate nodes and the eavesdroppers are distributed as Poisson
point processes, we study the secrecy transmission capacity (STC), as well as
the connection outage probability and secrecy outage probability, based on the
physical layer security. We first consider the basic fixed transmission
distance model, and establish a theoretical model of the STC. We then extend
the above results to a more realistic random distance transmission model,
namely nearest receiver transmission. Finally, extensive simulation and
numerical results are provided to validate the efficiency of our theoretical
results and illustrate how the STC is affected by noise, connection and secrecy
outage probabilities, transmitter and eavesdropper densities, and other system
parameters. Remarkably, our results reveal that a proper amount of noise is
helpful to the secrecy transmission capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6124</identifier>
 <datestamp>2014-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6124</id><created>2014-01-23</created><authors><author><keyname>de Franca</keyname><forenames>Fabricio Olivetti</forenames></author></authors><title>Iterative Universal Hash Function Generator for Minhashing</title><categories>cs.LG cs.IR</categories><comments>6 pages, 4 tables, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minhashing is a technique used to estimate the Jaccard Index between two sets
by exploiting the probability of collision in a random permutation. In order to
speed up the computation, a random permutation can be approximated by using an
universal hash function such as the $h_{a,b}$ function proposed by Carter and
Wegman. A better estimate of the Jaccard Index can be achieved by using many of
these hash functions, created at random. In this paper a new iterative
procedure to generate a set of $h_{a,b}$ functions is devised that eliminates
the need for a list of random values and avoid the multiplication operation
during the calculation. The properties of the generated hash functions remains
that of an universal hash function family. This is possible due to the random
nature of features occurrence on sparse datasets. Results show that the
uniformity of hashing the features is maintaned while obtaining a speed up of
up to $1.38$ compared to the traditional approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6125</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6125</id><created>2014-01-21</created><authors><author><keyname>Kim</keyname><forenames>HyungTae</forenames></author><author><keyname>Yang</keyname><forenames>HaeJeong</forenames></author></authors><title>Software Architecture and Subclassing Technique for Semiconductor
  Manufacturing Machines</title><categories>cs.OH</categories><comments>12 pages, 10 figures, International Journal of Soft Computing And
  Software Engineering (JSCSE). 2012 ISSN: 2251-7545</comments><doi>10.7321/jscse.v2.n12.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed software architecture for operating an automatic
semiconductor manufacturing machine. Recent machines for semiconductor process
are required for high level of automation which are composed of motion control,
machine vision, data acquisition and networking. These functions are executed
through industrial equipments that are generally installed in a computer. The
equipments occupy a great part of system resource and generate a lot of
computation, so the software structure should be designed for efficiency. The
proposed architecture is consisted of four layers and virtual equipments(VEs).
The VEs are made by subclassing the physical equipments(PEs) and the layers are
coded into thread which updates the status of VEs. Subroutines in a program
refer to the pointer of VEs, and direct access to physical equipment are
prohibited. The number of access (NOA) to PEs in typical industrial application
was simulated for the unlimited access structure and the presented structure.
The result showed that the proposed structure was more efficient than typical
ones and irrespective of subroutines. This architecture was also applied to
design a machine operating software and performed automatic wafer dicing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6126</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6126</id><created>2013-12-19</created><authors><author><keyname>Gleibman</keyname><forenames>Andrew</forenames></author></authors><title>Delegating Custom Object Detection Tasks to a Universal Classification
  System</title><categories>cs.CV</categories><comments>3 pages, 2 figures, 6 refs. arXiv admin note: substantial text
  overlap with arXiv:1310.7170</comments><msc-class>68T10</msc-class><acm-class>I.2.10; I.4.7; I.4.8; I.4.9; I.5; I.5.2; I.5.4; I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a concept of multipurpose object detection system, recently
introduced in our previous work, is clarified. The business aspect of this
method is transformation of a classifier into an object detector/locator via an
image grid. This is a universal framework for locating objects of interest
through classification. The framework standardizes and simplifies
implementation of custom systems by doing only a custom analysis of the
classification results on the image grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6127</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6127</id><created>2013-11-23</created><authors><author><keyname>G</keyname><forenames>Narkhede Sachin</forenames></author><author><keyname>Khairnar</keyname><forenames>Vaishali</forenames></author></authors><title>Brain Tumor Detection Based On Symmetry Information</title><categories>cs.CV</categories><comments>3 Pages, 3 figures, 2 tables</comments><journal-ref>Int. Journal of Engineering Research and Application Vol. 3, Issue
  6, Nov - Dec 2013, pp. 430 -432 (ISSN : 2248 - 9622)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in computing technology have allowed researchers across many fields
of endeavor to collect and maintain vast amounts of observational statistical
data such as clinical data, biological patient data, data regarding access of
web sites, financial data, and the like. This paper addresses some of the
challenging issues on brain magnetic resonance (MR) image tumor segmentation
caused by the weak correlation between magnetic resonance imaging (MRI)
intensity and anatomical meaning. With the objective of utilizing more
meaningful information to improve brain tumor segmentation, an approach which
employs bilateral symmetry information as an additional feature for
segmentation is proposed. This is motivated by potential performance
improvement in the general automatic brain tumor segmentation systems which are
important for many medical and scientific applications
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6129</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6129</id><created>2013-11-19</created><authors><author><keyname>Mukane</keyname><forenames>S. M.</forenames></author><author><keyname>Ghodake</keyname><forenames>Y. S.</forenames></author><author><keyname>Khandagle</keyname><forenames>P. S.</forenames></author></authors><title>Image enhancement using fusion by wavelet transform and laplacian
  pyramid</title><categories>cs.CV</categories><journal-ref>Published in IJCSI Journal, Volume 10, Issue 4, No 2, July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of combining multiple image modalities to provide a single, enhanced
image is well established different fusion methods have been proposed in
literature. This paper is based on image fusion using laplacian pyramid and
wavelet transform method. Images of same size are used for experimentation.
Images used for the experimentation are standard images and averaging filter is
used of equal weights in original images to burl. Performance of image fusion
technique is measured by mean square error, normalized absolute error and peak
signal to noise ratio. From the performance analysis it has been observed that
MSE is decreased in case of both the methods where as PSNR increased, NAE
decreased in case of laplacian pyramid where as constant for wavelet transform
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6130</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6130</id><created>2013-11-13</created><authors><author><keyname>K</keyname><forenames>MuthuKalyani.</forenames></author><author><keyname>A</keyname><forenames>VeeraMuthu.</forenames></author></authors><title>smart application for AMS using Face Recognition</title><categories>cs.CY cs.CV</categories><comments>20 pages,4 figures</comments><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 3, No. 5, October 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attendance Management System (AMS) can be made into smarter way by using face
recognition technique, where we use a CCTV camera to be fixed at the entry
point of a classroom, which automatically captures the image of the person and
checks the observed image with the face database using android enhanced smart
phone. It is typically used for two purposes. Firstly, marking attendance for
student by comparing the face images produced recently and secondly,
recognition of human who are strange to the environment i.e. an unauthorized
person For verification of image, a newly emerging trend 3D Face Recognition is
used which claims to provide more accuracy in matching the image databases and
has an ability to recognize a subject at different view angles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6131</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6131</id><created>2014-01-16</created><authors><author><keyname>Gra&#xe7;a</keyname><forenames>Jo&#xe3;o V.</forenames></author><author><keyname>Ganchev</keyname><forenames>Kuzman</forenames></author><author><keyname>Coheur</keyname><forenames>Luisa</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Controlling Complexity in Part-of-Speech Induction</title><categories>cs.CL cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  527-551, 2011</journal-ref><doi>10.1613/jair.3348</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fully unsupervised learning of grammatical
(part-of-speech) categories from unlabeled text. The standard
maximum-likelihood hidden Markov model for this task performs poorly, because
of its weak inductive bias and large model capacity. We address this problem by
refining the model and modifying the learning objective to control its capacity
via para- metric and non-parametric constraints. Our approach enforces
word-category association sparsity, adds morphological and orthographic
features, and eliminates hard-to-estimate parameters for rare words. We develop
an efficient learning algorithm that is not much more computationally intensive
than standard training. We also provide an open-source implementation of the
algorithm. Our experiments on five diverse languages (Bulgarian, Danish,
English, Portuguese, Spanish) achieve significant improvements compared with
previous methods for the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6132</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6132</id><created>2013-10-21</created><authors><author><keyname>Bradai</keyname><forenames>Abbas</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author></authors><title>Differenciated Bandwidth Allocation in P2P Layered Streaming</title><categories>cs.NI cs.MM</categories><comments>International Workshop on Computer Aided Modeling and Design of
  Communication Links and Networks (CAMAD), Barcelone : Spain (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing demand for P2P streaming in particular for layered
video. In this category of applications, the stream is composed of
hierarchically encoded sub-streams layers namely the base layer and
enhancements layers. We consider a scenario where the receiver peer uses the
pull-based approach to adjust the video quality level to their capability by
subscribing to different number of layers. We note that higher layers received
without their corresponding lower layers are considered as useless and cannot
be played, consequently the throughput of the system will drastically degrade.
To avoid this situation, we propose an economical model based on auction
mechanisms to optimize the allocation of sender peers' upload bandwidth. The
upstream peers organize auctions to &quot;sell&quot; theirs items (links' bandwidth)
according to bids submitted by the downstream peers taking into consideration
the peers priorities and the requested layers importance. The ultimate goal is
to satisfy the quality level requirement for each peer, while reducing the
overall streaming cost. Through theoretical study and performance evaluation we
show the effectiveness of our model in terms of users and network's utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6134</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6134</id><created>2014-01-22</created><authors><author><keyname>Yilmaz</keyname><forenames>Yasin</forenames></author><author><keyname>Guo</keyname><forenames>Ziyu</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Sequential Joint Spectrum Sensing and Channel Estimation for Dynamic
  Spectrum Access</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum access under channel uncertainties is considered. With the
goal of maximizing the secondary user (SU) throughput subject to constraints on
the primary user (PU) outage probability we formulate a joint problem of
spectrum sensing and channel state estimation. The problem is cast into a
sequential framework since sensing time minimization is crucial for throughput
maximization. In the optimum solution, the sensing decision rule is coupled
with the channel estimator, making the separate treatment of the sensing and
channel estimation strictly suboptimal. Using such a joint structure for
spectrum sensing and channel estimation we propose a distributed (cooperative)
dynamic spectrum access scheme under statistical channel state information
(CSI). In the proposed scheme, the SUs report their sufficient statistics to a
fusion center (FC) via level-triggered sampling, a nonuniform sampling
technique that is known to be bandwidth-and-energy efficient. Then, the FC
makes a sequential spectrum sensing decision using local statistics and channel
estimates, and selects the SU with the best transmission opportunity. The
selected SU, using the sensing decision and its channel estimates, computes the
transmit power and starts data transmission. Simulation results demonstrate
that the proposed scheme significantly outperforms its conventional
counterparts, under the same PU outage constraints, in terms of the achievable
SU throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6135</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6135</id><created>2014-01-23</created><authors><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Capacity Bounds for a Class of Diamond Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of diamond networks are studied where the broadcast component is
modelled by two independent bit-pipes. New upper and low bounds are derived on
the capacity which improve previous bounds. The upper bound is in the form of a
max-min problem, where the maximization is over a coding distribution and the
minimization is over an auxiliary channel. The proof technique generalizes
bounding techniques of Ozarow for the Gaussian multiple description problem
(1981), and Kang and Liu for the Gaussian diamond network (2011). The bounds
are evaluated for a Gaussian multiple access channel (MAC) and the binary adder
MAC, and the capacity is found for interesting ranges of the bit-pipe
capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6136</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6136</id><created>2014-01-23</created><updated>2014-06-04</updated><authors><author><keyname>Zahedi</keyname><forenames>Adel</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author><author><keyname>Jensen</keyname><forenames>Soren Holdt</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick</forenames></author><author><keyname>Bech</keyname><forenames>Soren</forenames></author></authors><title>Distributed Remote Vector Gaussian Source Coding with Covariance
  Distortion Constraints</title><categories>cs.IT math.IT</categories><comments>This is the final version accepted at ISIT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a distributed remote source coding problem, where
a sequence of observations of source vectors is available at the encoder. The
problem is to specify the optimal rate for encoding the observations subject to
a covariance matrix distortion constraint and in the presence of side
information at the decoder. For this problem, we derive lower and upper bounds
on the rate-distortion function (RDF) for the Gaussian case, which in general
do not coincide. We then provide some cases, where the RDF can be derived
exactly. We also show that previous results on specific instances of this
problem can be generalized using our results. We finally show that if the
distortion measure is the mean squared error, or if it is replaced by a certain
mutual information constraint, the optimal rate can be derived from our main
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6145</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6145</id><created>2014-01-23</created><authors><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>On Stochastic Geometry Modeling of Cellular Uplink Transmission with
  Truncated Channel Inversion Power Control</title><categories>cs.IT cs.NI math.IT math.ST stat.TH</categories><comments>Submitted to IEEE Transactions on Wireless Communications (TWC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using stochastic geometry, we develop a tractable uplink modeling paradigm
for outage probability and spectral efficiency in both single and multi-tier
cellular wireless networks. The analysis accounts for per user equipment (UE)
power control as well as the maximum power limitations for UEs. More
specifically, for interference mitigation and robust uplink communication, each
UE is required to control its transmit power such that the average received
signal power at its serving base station (BS) is equal to a certain threshold
$\rho_o$. Due to the limited transmit power, the UEs employ a truncated channel
inversion power control policy with a cutoff threshold of $\rho_o$. We show
that there exists a transfer point in the uplink system performance that
depends on the tuple: BS intensity ($\lambda$), maximum transmit power of UEs
($P_u$), and $\rho_o$. That is, when $P_u$ is a tight operational constraint
with respect to [w.r.t.] $\lambda$ and $\rho_o$, the uplink outage probability
and spectral efficiency highly depend on the values of $\lambda$ and $\rho_o$.
In this case, there exists an optimal cutoff threshold $\rho^*_o$, which
depends on the system parameters, that minimizes the outage probability. On the
other hand, when $P_u$ is not a binding operational constraint w.r.t. $\lambda$
and $\rho_o$, the uplink outage probability and spectral efficiency become
independent of $\lambda$ and $\rho_o$. We obtain approximate yet accurate
simple expressions for outage probability and spectral efficiency which reduce
to closed-forms in some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6157</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6157</id><created>2014-01-23</created><updated>2014-12-09</updated><authors><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>Mazloumian</keyname><forenames>Amin</forenames></author><author><keyname>Petersen</keyname><forenames>Alexander M</forenames></author><author><keyname>Penner</keyname><forenames>Orion</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Exploiting citation networks for large-scale author name disambiguation</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>14 pages, 5 figures</comments><journal-ref>EPJ Data Science 2014, 3:11</journal-ref><doi>10.1140/epjds/s13688-014-0011-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm and validation method for disambiguating author
names in very large bibliographic data sets and apply it to the full Web of
Science (WoS) citation index. Our algorithm relies only upon the author and
citation graphs available for the whole period covered by the WoS. A pair-wise
publication similarity metric, which is based on common co-authors,
self-citations, shared references and citations, is established to perform a
two-step agglomerative clustering that first connects individual papers and
then merges similar clusters. This parameterized model is optimized using an
h-index based recall measure, favoring the correct assignment of well-cited
publications, and a name-initials-based precision using WoS metadata and
cross-referenced Google Scholar profiles. Despite the use of limited metadata,
we reach a recall of 87% and a precision of 88% with a preference for
researchers with high h-index values. 47 million articles of WoS can be
disambiguated on a single machine in less than a day. We develop an h-index
distribution model, confirming that the prediction is in excellent agreement
with the empirical data, and yielding insight into the utility of the h-index
in real academic ranking scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6169</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6169</id><created>2014-01-22</created><updated>2014-09-11</updated><authors><author><keyname>Soleimani</keyname><forenames>Hossein</forenames></author><author><keyname>Miller</keyname><forenames>David J.</forenames></author></authors><title>Parsimonious Topic Models with Salient Word Discovery</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><acm-class>I.7.0; I.5.3; G.3; I.5.2</acm-class><doi>10.1109/TKDE.2014.2345378</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a parsimonious topic model for text corpora. In related models
such as Latent Dirichlet Allocation (LDA), all words are modeled
topic-specifically, even though many words occur with similar frequencies
across different topics. Our modeling determines salient words for each topic,
which have topic-specific probabilities, with the rest explained by a universal
shared model. Further, in LDA all topics are in principle present in every
document. By contrast our model gives sparse topic representation, determining
the (small) subset of relevant topics for each document. We derive a Bayesian
Information Criterion (BIC), balancing model complexity and goodness of fit.
Here, interestingly, we identify an effective sample size and corresponding
penalty specific to each parameter type in our model. We minimize BIC to
jointly determine our entire model -- the topic-specific words,
document-specific topics, all model parameter values, {\it and} the total
number of topics -- in a wholly unsupervised fashion. Results on three text
corpora and an image dataset show that our model achieves higher test set
likelihood and better agreement with ground-truth class labels, compared to LDA
and to a model designed to incorporate sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6170</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6170</id><created>2014-01-23</created><authors><author><keyname>Salminen</keyname><forenames>Joni</forenames></author><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author></authors><title>Fools gold? Developer dilemmas in a closed mobile application market
  platform</title><categories>cs.CY</categories><comments>Presented at the 15th International Conference on Electronic
  Commerce, ICEC 2013, Turku, Finland, August 13-15, 2013</comments><acm-class>K.6.0; K.7.4</acm-class><doi>10.1007/978-3-642-39808-7_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we outline some potential conflicts that platform owners and
software developers face in mobile application markets. Our arguments are based
on comments captured in specialized online discussion forums, in which
developers gather to share knowledge and experiences. The key findings indicate
conflicts of interests, including 1) intra-platform competition, 2)
discriminative promotion, 3) entry prevention, 4) restricted monetization, 5)
restricted knowledge sharing, 6) substitution, and 7) strategic technology
selection. Opportunistic platform owners may use their power to discriminate
between third-part software developers. However, there are also potential
strategic solutions that developers can apply; for example diversification
(multi-homing), syndication and brand building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6189</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6189</id><created>2014-01-23</created><authors><author><keyname>Bourgain</keyname><forenames>Jean</forenames></author><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Leeman</keyname><forenames>Ethan</forenames></author></authors><title>Affine extractors over large fields with exponential error</title><categories>cs.CC math.NT</categories><comments>To appear in Comput. Complex</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a construction of explicit affine extractors over large finite
fields with exponentially small error and linear output length. Our
construction relies on a deep theorem of Deligne giving tight estimates for
exponential sums over smooth varieties in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6190</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6190</id><created>2014-01-23</created><updated>2014-04-19</updated><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Probabilistic Signal Shaping for Bit-Metric Decoding</title><categories>cs.IT math.IT</categories><comments>7 pages; compared to v2, the title has slightly changed. More
  references to related work have been added. A shortened version will appear
  in the proceedings of the ISIT 14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scheme is proposed that combines probabilistic signal shaping with
bit-metric decoding. The transmitter generates symbols according to a
distribution on the channel input alphabet. The symbols are labeled by bit
strings. At the receiver, the channel output is decoded with respect to a
bit-metric. An achievable rate is derived using random coding arguments. For
the 8-ASK AWGN channel, numerical results show that at a spectral efficiency of
2 bits/s/Hz, the new scheme outperforms bit-interleaved coded modulation (BICM)
without shaping and BICM with bit shaping (i Fabregas and Martinez, 2010) by
0.87 dB and 0.15 dB, respectively, and is within 0.0094 dB of the coded
modulation capacity. The new scheme is implemented by combining a distribution
matcher with a systematic binary low-density parity-check code. The measured
finite-length gains are very close to the gains predicted by the asymptotic
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6196</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6196</id><created>2014-01-23</created><authors><author><keyname>Zhou</keyname><forenames>Q.</forenames></author><author><keyname>Michailovich</keyname><forenames>O.</forenames></author><author><keyname>Rathi</keyname><forenames>Y.</forenames></author></authors><title>Spatially regularized reconstruction of fibre orientation distributions
  in the presence of isotropic diffusion</title><categories>cs.CV</categories><comments>33 pages, 14 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connectivity and structural integrity of the white matter of the brain is
nowadays known to be implicated into a wide range of brain-related disorders.
However, it was not before the advent of diffusion Magnetic Resonance Imaging
(dMRI) that researches have been able to examine the properties of white matter
in vivo. Presently, among a range of various methods of dMRI, high angular
resolution diffusion imaging (HARDI) is known to excel in its ability to
provide reliable information about the local orientations of neural fasciculi
(aka fibre tracts). Moreover, as opposed to the more traditional diffusion
tensor imaging (DTI), HARDI is capable of distinguishing the orientations of
multiple fibres passing through a given spatial voxel. Unfortunately, the
ability of HARDI to discriminate between neural fibres that cross each other at
acute angles is always limited, which is the main reason behind the development
of numerous post-processing tools, aiming at the improvement of the directional
resolution of HARDI. Among such tools is spherical deconvolution (SD). Due to
its ill-posed nature, however, SD standardly relies on a number of a priori
assumptions which are to render its results unique and stable. In this paper,
we propose a different approach to the problem of SD in HARDI, which accounts
for the spatial continuity of neural fibres as well as the presence of
isotropic diffusion. Subsequently, we demonstrate how the proposed solution can
be used to successfully overcome the effect of partial voluming, while
preserving the spatial coherency of cerebral diffusion at moderate-to-severe
noise levels. In a series of both in silico and in vivo experiments, the
performance of the proposed method is compared with that of several available
alternatives, with the comparative results clearly supporting the viability and
usefulness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6210</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6210</id><created>2014-01-23</created><authors><author><keyname>Alfonsetti</keyname><forenames>Elisabetta</forenames></author><author><keyname>Weeraddana</keyname><forenames>Pradeep Chathuranga</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>A Semi Distributed Approach for Min-Max Fair Car-Parking Slot Assignment
  Problem</title><categories>math.OC cs.DS</categories><comments>14 pages, 12 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing efficient car parking mechanisms that can be potentially integrated
into future intelligent transportation systems is of crucial importance.
Usually, the related design problems are combinatorial and the worst-case
complexity of optimal solution approaches grows exponentially with the problem
sizes. Therefore, such optimal approaches are not scalable and practically
undesirable. As a result, almost all existing methods for parking slot
assignment are simple and greedy approaches, where each car is assigned a free
parking slot, which is closer to its destination. Moreover, no emphasis is
placed to optimize the social benefit of the users during the parking slot
assignment. In this paper, the fairness as a metric for modeling the aggregate
social benefit of the users is considered and a distributed algorithm based on
Lagrange duality theory is developed. The proposed algorithm is gracefully
scalable compared to the optimal methods. In addition, it is shown that the
proposed car parking mechanism preserves privacy in the sense that any car
involved in the algorithm will not be able to discover the destination of any
other car during the algorithm iterations. Numerical results illustrate the
performance of the proposed algorithm compared to the optimal assignment and a
greedy method. They show that our algorithm yields a good tradeoff between the
implementation-level simplicity and the performance. Even though the main
emphasis in this paper resides in the car parking slot assignment problem, our
formulation and the algorithms, in general, can also be applied or adopted in
fair agent-target assignment problems in other application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6219</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6219</id><created>2014-01-23</created><updated>2016-02-18</updated><authors><author><keyname>Wu</keyname><forenames>Youlong</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Coding Schemes with Rate-Limited Feedback that Improve over the
  Nofeedback Capacity for a Large Class of Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>58 pages, 4 figures, accepted by IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two coding schemes for the two-receiver discrete memoryless
broadcast channel (BC) with rate-limited feedback from one or both receivers.
They improve over the nofeedback capacity region for a large class of channels,
including the class of \emph{strictly essentially less-noisy BCs} that we
introduce in this article. Examples of strictly essentially less-noisy BCs are
the binary symmetric BC (BSBC) or the binary erasure BC (BEBC) with unequal
cross-over or erasure probabilities at the two receivers. When the feedback
rates are sufficiently large, our schemes recover all previously known capacity
results for discrete memoryless BCs with feedback.
  In both our schemes, we let the receivers feed back quantization messages
about their receive signals. In the first scheme, the transmitter simply
\emph{relays} the quantization information obtained from Receiver 1 to Receiver
2, and vice versa. This provides each receiver with a second observation of the
input signal and can thus improve its decoding performance unless the BC is
physically degraded. Moreover, each receiver uses its knowledge of the
quantization message describing its own outputs so as to attain the same
performance as if this message had not been transmitted at all.
  In our second scheme the transmitter first \emph{reconstructs and processes}
the quantized output signals, and then sends the outcome as a common update
information to both receivers. A special case of our second scheme applies also
to memoryless BCs without feedback but with strictly-causal state-information
at the transmitter and causal state-information at the receivers. It recovers
all previous achievable regions also for this setup with state-information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6220</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6220</id><created>2014-01-23</created><authors><author><keyname>Memon</keyname><forenames>Abdul Basit</forenames></author><author><keyname>Verriest</keyname><forenames>Erik I.</forenames></author></authors><title>Maximally persistent connections for the periodic type</title><categories>cs.SY math.OC</categories><comments>Submitted to the 21st International Symposium on Mathematical Theory
  of Networks and Systems, Groningen, The Netherlands</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimal control problem of connecting two periodic
trajectories with maximal persistence. A maximally persistent trajectory is
close to the periodic type in the sense that the norm of the image of this
trajectory under the operator defining the periodic type is minimal among all
trajectories. A solution is obtained in this paper for the case when the two
trajectories have the same period but it turns out to be only piecewise
continuous and so an alternate norm is employed to obtain a continuous
connection. The case when the two trajectories have different but rational
periods is also solved. The problem of connecting periodic trajectories is of
interest because of the observation that the operating points of many
biological and artificial systems are limit cycles and so there is a need for a
unified optimal framework of connections between different operating points.
This paper is a first step towards that goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6224</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6224</id><created>2014-01-23</created><authors><author><keyname>Kalimeri</keyname><forenames>Maria</forenames></author><author><keyname>Constantoudis</keyname><forenames>Vassilios</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Constantinos</forenames></author><author><keyname>Karamanos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Diakonos</keyname><forenames>Fotis K.</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Harris</forenames></author></authors><title>Word-length entropies and correlations of natural language written texts</title><categories>cs.CL physics.data-an</categories><comments>13 pages + 1 page of supporting information, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the frequency distributions and correlations of the word lengths of
ten European languages. Our findings indicate that a) the word-length
distribution of short words quantified by the mean value and the entropy
distinguishes the Uralic (Finnish) corpus from the others, b) the tails at long
words, manifested in the high-order moments of the distributions, differentiate
the Germanic languages (except for English) from the Romanic languages and
Greek and c) the correlations between nearby word lengths measured by the
comparison of the real entropies with those of the shuffled texts are found to
be smaller in the case of Germanic and Finnish languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6226</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6226</id><created>2014-01-23</created><authors><author><keyname>Adebiyi</keyname><forenames>Adetunji</forenames></author><author><keyname>Imafidon</keyname><forenames>Chris</forenames></author></authors><title>Using Neural Network to Propose Solutions to Threats in Attack Patterns</title><categories>cs.CR cs.AI</categories><comments>11 Pages</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 1, 2013</journal-ref><doi>10.7321/jscse.v3.n1.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, a lot of effort has been put into securing software
application during development in the software industry. Software security is a
research field in this area which looks at how security can be weaved into
software at each phase of software development lifecycle (SDLC). The use of
attack patterns is one of the approaches that have been proposed for
integrating security during the design phase of SDLC. While this approach help
developers in identify security flaws in their software designs, the need to
apply the proper security capability that will mitigate the threat identified
is very important. To assist in this area, the uses of security patterns have
been proposed to help developers to identify solutions to recurring security
problems. However due to different types of security patterns and their
taxonomy, software developers are faced with the challenge of finding and
selecting appropriate security patterns that addresses the security risks in
their design. In this paper, we propose a tool based on Neural Network for
proposing solutions in form of security patterns to threats in attack patterns
matching attacking patterns. From the result of performance of the neural
network, we found out that the neural network was able to match attack patterns
to security patterns that can mitigate the threat in the attack pattern. With
this information developers are better informed in making decision on the
solution for securing their application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6227</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6227</id><created>2014-01-23</created><authors><author><keyname>Vazou</keyname><forenames>Niki</forenames></author><author><keyname>Seidel</keyname><forenames>Eric L.</forenames></author><author><keyname>Jhala</keyname><forenames>Ranjit</forenames></author></authors><title>From Safety To Termination And Back: SMT-Based Verification For Lazy
  Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SMT-based verifiers have long been an effective means of ensuring safety
properties of programs. While these techniques are well understood, we show
that they implicitly require eager semantics; directly applying them to a lazy
language is unsound due to the presence of divergent sub-computations. We
recover soundness by composing the safety analysis with a termination analysis.
Of course, termination is itself a challenging problem, but we show how the
safety analysis can be used to ensure termination, thereby bootstrapping
soundness for the entire system. Thus, while safety invariants have long been
required to prove termination, we show how termination proofs can be to soundly
establish safety. We have implemented our approach in liquidHaskell, a
Refinement Type-based verifier for Haskell. We demonstrate its effectiveness
via an experimental evaluation using liquidHaskell to verify safety, functional
correctness and termination properties of real-world Haskell libraries,
totaling over 10,000 lines of code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6235</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6235</id><created>2014-01-23</created><updated>2014-07-10</updated><authors><author><keyname>Latkin</keyname><forenames>Evgeny</forenames></author></authors><title>Twofold fast arithmetic</title><categories>cs.NA</categories><comments>C++ experimental code and test results available via Web:
  https://sites.google.com/site/yevgenylatkin/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we assure math computations by automatic verifying floating-point
accuracy? We define fast arithmetic (based on Dekker [1971]) over twofold
approximations $z\approx z_0+z_1$, such that $z_0$ is standard result and $z_1$
assesses inaccuracy $\Delta z_0=z-z_0$. We propose on-fly tracking $z_1$,
detecting if $\Delta z_0$ appears too high. We believe permanent tracking is
worth its cost. C++ test code for Intel AVX available via web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6236</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6236</id><created>2014-01-23</created><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Kyng</keyname><forenames>Rasmus</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub W.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Rao</keyname><forenames>Anup</forenames></author></authors><title>Preconditioning in Expectation</title><categories>cs.DS cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that preconditioners constructed by random sampling can perform well
without meeting the standard requirements of iterative methods. When applied to
graph Laplacians, this leads to ultra-sparsifiers that in expectation behave as
the nearly-optimal ones given by [Kolla-Makarychev-Saberi-Teng STOC`10].
Combining this with the recursive preconditioning framework by [Spielman-Teng
STOC`04] and improved embedding algorithms, this leads to algorithms that solve
symmetric diagonally dominant linear systems and electrical flow problems in
expected time close to $m\log^{1/2}n$ .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6240</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6240</id><created>2014-01-23</created><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Liu</keyname><forenames>Xia</forenames></author><author><keyname>Fang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II)</title><categories>cs.LG</categories><comments>13 pages</comments><msc-class>68T05</msc-class><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An extreme learning machine (ELM) can be regarded as a two stage feed-forward
neural network (FNN) learning system which randomly assigns the connections
with and within hidden neurons in the first stage and tunes the connections
with output neurons in the second stage. Therefore, ELM training is essentially
a linear learning problem, which significantly reduces the computational
burden. Numerous applications show that such a computation burden reduction
does not degrade the generalization capability. It has, however, been open that
whether this is true in theory. The aim of our work is to study the theoretical
feasibility of ELM by analyzing the pros and cons of ELM. In the previous part
on this topic, we pointed out that via appropriate selection of the activation
function, ELM does not degrade the generalization capability in the expectation
sense. In this paper, we launch the study in a different direction and show
that the randomness of ELM also leads to certain negative consequences. On one
hand, we find that the randomness causes an additional uncertainty problem of
ELM, both in approximation and learning. On the other hand, we theoretically
justify that there also exists an activation function such that the
corresponding ELM degrades the generalization capability. In particular, we
prove that the generalization capability of ELM with Gaussian kernel is
essentially worse than that of FNN with Gaussian kernel. To facilitate the use
of ELM, we also provide a remedy to such a degradation. We find that the
well-developed coefficient regularization technique can essentially improve the
generalization capability. The obtained results reveal the essential
characteristic of ELM and give theoretical guidance concerning how to use ELM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6244</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6244</id><created>2014-01-23</created><authors><author><keyname>Wang</keyname><forenames>Yanqing</forenames></author><author><keyname>Ge</keyname><forenames>Hong</forenames></author><author><keyname>Feng</keyname><forenames>Xiaojing</forenames></author><author><keyname>Yu</keyname><forenames>Jie</forenames></author></authors><title>On measuring team stability in cooperative learning: An example of
  consecutive course projects on software engineering</title><categories>cs.CY cs.SE</categories><comments>11 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative learning theory has shown that stable membership is a hallmark of
effective work teams. According to relation strength and social network
centrality, this paper proposes an approach to measure team stability reasons
in consecutive cooperative learning. Taking consecutive course projects of
software engineering in a university as examples, we examine the relation
between team stability and learning performance in consecutive cooperative
learning from two parts: learning score and learning satisfaction. Through
empirical analysis, it arrives at the conclusion that learning score is in weak
positive correlation with team stability. Through questionnaire and interviews,
it finds out 78% of the students did not value the importance of team
stability, and 67% of the teachers never recommend the students to keep stable
teams. Finally, we put forward an expected correlation model of learning
performance as future work and discuss instability as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6249</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6249</id><created>2014-01-23</created><authors><author><keyname>Li</keyname><forenames>Yangjia</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>(Un)decidable Problems about Reachability of Quantum Systems</title><categories>cs.LO quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the reachability problem of a quantum system modelled by a quantum
automaton. The reachable sets are chosen to be boolean combinations of (closed)
subspaces of the state space of the quantum system. Four different reachability
properties are considered: eventually reachable, globally reachable, ultimately
forever reachable, and infinitely often reachable. The main result of this
paper is that all of the four reachability properties are undecidable in
general; however, the last three become decidable if the reachable sets are
boolean combinations without negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6252</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6252</id><created>2014-01-23</created><updated>2015-03-16</updated><authors><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>Note on the residue codes of self-dual $\mathbb{Z}_4$-codes having large
  minimum Lee weights</title><categories>math.CO cs.IT math.IT</categories><comments>18 pages</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the residue code of a self-dual $\mathbb{Z}_4$-code of
length $24k$ (resp.\ $24k+8$) and minimum Lee weight $8k+4 \text{ or }8k+2$
(resp.\ $8k+8 \text{ or }8k+6$) is a binary extremal doubly even self-dual code
for every positive integer $k$. A number of new self-dual $\mathbb{Z}_4$-codes
of length $24$ and minimum Lee weight $10$ are constructed using the above
characterization. These codes are Type I $\mathbb{Z}_4$-codes having the
largest minimum Lee weight and the largest Euclidean weight among all Type I
$\mathbb{Z}_4$-codes of that length. In addition, new extremal Type II
$\mathbb{Z}_4$-codes of length $56$ are found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6254</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6254</id><created>2014-01-23</created><updated>2014-03-19</updated><authors><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>On a 5-design related to a putative extremal doubly even self-dual code
  of length a multiple of 24</title><categories>math.CO cs.IT math.IT</categories><comments>16 pages, Designs, Codes and Cryptogr. (to appear)</comments><msc-class>94B05, 05B30</msc-class><journal-ref>Designs, Codes and Cryptogr. 76 (2015), 373-384</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By the Assmus and Mattson theorem, the codewords of each nontrivial weight in
an extremal doubly even self-dual code of length 24m form a self-orthogonal
5-design. In this paper, we study the codes constructed from self-orthogonal
5-designs with the same parameters as the above 5-designs. We give some
parameters of a self-orthogonal 5-design whose existence is equivalent to that
of an extremal doubly even self-dual code of length 24m for m=3,...,6. If $m
\in \{1,\ldots,6\}$, $k \in \{m+1,\ldots,5m-1\}$ and $(m,k) \ne (6,18)$, then
it is shown that an extremal doubly even self-dual code of length 24m is
generated by codewords of weight 4k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6258</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6258</id><created>2014-01-23</created><updated>2014-02-20</updated><authors><author><keyname>Xu</keyname><forenames>Yinfei</forenames></author><author><keyname>Wang</keyname><forenames>Qiao</forenames></author></authors><title>Rate Region of the Vector Gaussian CEO Problem with the Trace Distortion
  Constraint</title><categories>cs.IT math.IT</categories><comments>12 pages, 1 figures, a shorter version is submitted for publication
  in IEEE International Symposium on Information Theory (ISIT), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a new extremal inequality, which is further leveraged to give a
complete characterization of the rate region of the vector Gaussian CEO problem
with the trace distortion constraint. The proof of this extremal inequality
hinges on a careful analysis of the Karush-Kuhn-Tucker necessary conditions for
the non-convex optimization problem associated with the Berger-Tung scheme,
which enables us to integrate the perturbation argument by Wang and Chen with
the distortion projection method by Rahman and Wagner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6260</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6260</id><created>2014-01-24</created><updated>2014-09-26</updated><authors><author><keyname>Zhang</keyname><forenames>Yijin</forenames></author><author><keyname>Lo</keyname><forenames>Yuan-Hsun</forenames></author><author><keyname>Shu</keyname><forenames>Feng</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Protocol Sequences for Multiple-Packet Reception</title><categories>cs.IT math.IT</categories><comments>25 pages, 4 figures. A revision submitted to IEEE Trans. Inform.
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a time slotted communication channel shared by $K$ active users and
a single receiver. It is assumed that the receiver has the ability of the
multiple-packet reception (MPR) to correctly receive at most $\gamma$ ($1 \leq
\gamma &lt; K$) simultaneously transmitted packets. Each user accesses the channel
following a specific periodical binary sequence, called the protocol sequence,
and transmits a packet within a channel slot if and only if the sequence value
is equal to one. The fluctuation in throughput is incurred by inevitable random
relative shifts among the users due to the lack of feedback. A set of protocol
sequences is said to be throughput-invariant (TI) if it can be employed to
produce invariant throughput for any relative shifts, i.e., maximize the
worst-case throughput. It was shown in the literature that the TI property
without considering MPR (i.e., $\gamma=1$) can be achieved by using
shift-invariant (SI) sequences, whose generalized Hamming cross-correlation is
independent of relative shifts. This paper investigates TI sequences for MPR;
results obtained include achievable throughput value, a lower bound on the
sequence period, an optimal construction of TI sequences that achieves the
lower bound on the sequence period, and intrinsic structure of TI sequences. In
addition, we present a practical packet decoding mechanism for TI sequences
that incorporates packet header, forward error-correcting code, and advanced
physical layer blind signal separation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6264</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6264</id><created>2014-01-24</created><updated>2014-10-02</updated><authors><author><keyname>Balmahoon</keyname><forenames>Reevana</forenames></author><author><keyname>Cheng</keyname><forenames>Ling</forenames></author></authors><title>Information Leakage of Correlated Source Coded Sequences over Channel
  with an Eavesdropper</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A new generalised approach for multiple correlated sources over a wiretap
network is investigated. A basic model consisting of two correlated sources
where each produce a component of the common information is initially
investigated. There are several cases that consider wiretapped syndromes on the
transmission links and based on these cases a new quantity, the information
leakage at the source/s is determined. An interesting feature of the models
described in this paper is the information leakage quantification. Shannon's
cipher system with eavesdroppers is incorporated into the two correlated
sources model to minimize key lengths. These aspects of quantifying information
leakage and reducing key lengths using Shannon's cipher system are also
considered for a multiple correlated source network approach. A new scheme that
incorporates masking using common information combinations to reduce the key
lengths is presented and applied to the generalised model for multiple sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6267</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6267</id><created>2014-01-24</created><authors><author><keyname>Er</keyname><forenames>Harun Rasit</forenames></author><author><keyname>Erdogan</keyname><forenames>Nadia</forenames></author></authors><title>Parallel Genetic Algorithm to Solve Traveling Salesman Problem on
  MapReduce Framework using Hadoop Cluster</title><categories>cs.DC cs.NE</categories><comments>The International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, Special Issue. The Proceeding of International
  Conference on Soft Computing and Software Engineering 2013</comments><report-no>e-ISSN: 2251-7545</report-no><doi>10.7321/jscse.v3.n3.57</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traveling Salesman Problem (TSP) is one of the most common studied problems
in combinatorial optimization. Given the list of cities and distances between
them, the problem is to find the shortest tour possible which visits all the
cities in list exactly once and ends in the city where it starts. Despite the
Traveling Salesman Problem is NP-Hard, a lot of methods and solutions are
proposed to the problem. One of them is Genetic Algorithm (GA). GA is a simple
but an efficient heuristic method that can be used to solve Traveling Salesman
Problem. In this paper, we will show a parallel genetic algorithm
implementation on MapReduce framework in order to solve Traveling Salesman
Problem. MapReduce is a framework used to support distributed computation on
clusters of computers. We used free licensed Hadoop implementation as MapReduce
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6275</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6275</id><created>2014-01-24</created><authors><author><keyname>Zeng</keyname><forenames>Hongyi</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Delay-Energy lower bound on Two-Way Relay Wireless Network Coding</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding is a novel solution that significantly improve the throughput
and energy consumed of wireless networks by mixing traffic flows through
algebraic operations. In conventional network coding scheme, a packet has to
wait for packets from other sources to be coded before transmitting. The
wait-and-code scheme will naturally result in packet loss rate in a finite
buffer. We will propose Enhanced Network Coding (ENC), an extension to ONC in
continuous time domain.
  In ENC, the relay transmits both coded and uncoded packets to reduce delay.
In exchange, more energy is consumed in transmitting uncoded packets. ENC is a
practical algorithm to achieve minimal average delay and zero packet-loss rate
under given energy constraint. The system model for ENC on a general renewal
process queuing is presented. In particular, we will show that there exists a
fundamental trade-off between average delay and energy. We will also present
the analytic result of lower bound for this trade-off curve, which can be
achieved by ENC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6290</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6290</id><created>2014-01-24</created><authors><author><keyname>Riahi</keyname><forenames>Ghazal</forenames></author></authors><title>A Security Plan for Smart Grid Systems Based On AGC4ISR</title><categories>cs.CR cs.NI</categories><comments>The International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, Special Issue</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, 2013 pp.760-768</journal-ref><doi>10.7321/jscse.v3.n3.115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is proposed a security plan for Smart Grid Systems based on
AGC4ISR which is an architecture for Autonomic Grid Computing Systems. Smart
Grid incorporates has many benefits of distributed computing and communications
to deliver a real-time information and enable the near-instantaneous balance of
supply and demand at the device level. AGC4ISR architecture is Organized by
Autonomic Grid Computing and C4ISR (Command, Control, Communications, Computers
and Intelligence, Surveillance, &amp; Reconnaissance) Architecture. In this paper
we will present a solution for as security plan which will be consider
encryption, intrusion detection, key management and detail of cyber security in
Smart Grids. In this paper we use the cryptography for the packet in the C4ISR
and we use a key management for send and receive a packet in the smart grid
because it is necessary for intelligent networks to keeping away from packet
missing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6294</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6294</id><created>2014-01-24</created><authors><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Wang</keyname><forenames>Guangmin</forenames></author><author><keyname>Zheng</keyname><forenames>Nanning</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>An Extended Result on the Optimal Estimation under Minimum Error Entropy
  Criterion</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>15 pages, no figures, submitted to Entropy</comments><journal-ref>Entropy 2014, 16(4), 2223-2233</journal-ref><doi>10.3390/e16042223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum error entropy (MEE) criterion has been successfully used in
fields such as parameter estimation, system identification and the supervised
machine learning. There is in general no explicit expression for the optimal
MEE estimate unless some constraints on the conditional distribution are
imposed. A recent paper has proved that if the conditional density is
conditionally symmetric and unimodal (CSUM), then the optimal MEE estimate
(with Shannon entropy) equals the conditional median. In this study, we extend
this result to the generalized MEE estimation where the optimality criterion is
the Renyi entropy or equivalently, the \alpha-order information potential (IP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6304</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6304</id><created>2014-01-24</created><updated>2014-05-07</updated><authors><author><keyname>D&#xfc;ck</keyname><forenames>Natalia</forenames></author><author><keyname>Zimmermann</keyname><forenames>Karl-Heinz</forenames></author></authors><title>Graver Bases and Universal Gr\&quot;obner Bases for Linear Codes</title><categories>math.AC cs.IT math.IT</categories><comments>18 pages</comments><msc-class>13P10, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two correspondences have been provided that associate any linear code over a
finite field with a binomial ideal. In this paper, algorithms for computing
their Graver bases and universal Gr\&quot;obner bases are given. To this end, a
connection between these binomial ideals and toric ideals will be established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6307</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6307</id><created>2014-01-24</created><authors><author><keyname>Capelli</keyname><forenames>Florent</forenames></author><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Hypergraph Acyclicity and Propositional Model Counting</title><categories>cs.CC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the propositional model counting problem #SAT for CNF- formulas
with hypergraphs that allow a disjoint branches decomposition can be solved in
polynomial time. We show that this class of hypergraphs is incomparable to
hypergraphs of bounded incidence cliquewidth which were the biggest class of
hypergraphs for which #SAT was known to be solvable in polynomial time so far.
Furthermore, we present a polynomial time algorithm that computes a disjoint
branches decomposition of a given hypergraph if it exists and rejects
otherwise. Finally, we show that some slight extensions of the class of
hypergraphs with disjoint branches decompositions lead to intractable #SAT,
leaving open how to generalize the counting result of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6309</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6309</id><created>2014-01-24</created><updated>2014-01-27</updated><authors><author><keyname>Mayzel</keyname><forenames>Maxim</forenames></author><author><keyname>Kazimierczuk</keyname><forenames>Krzysztof</forenames></author><author><keyname>Orekhov</keyname><forenames>Vladislav Yu.</forenames></author></authors><title>Causality principle in reconstruction of sparse NMR spectra</title><categories>physics.chem-ph cs.IT math.IT</categories><comments>changes in v2: fixed typos and minor text corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid development of sparse sampling methodology offers dramatic increase in
power and efficiency of magnetic resonance techniques in medicine, chemistry,
molecular structural biology, and other fields. We suggest to use available yet
usually unexploited prior knowledge about the phase and the causality of the
sparsely detected NMR signal as a general approach for a major improvement of
the spectra quality. The work gives a theoretical framework of the method and
demonstrates notable improvement of the protein spectra reconstructed with two
commonly used state-of-the-art signal processing algorithms, compressed sensing
and SIFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6310</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6310</id><created>2014-01-24</created><updated>2014-06-10</updated><authors><author><keyname>Bradford</keyname><forenames>R.</forenames></author><author><keyname>Chen</keyname><forenames>C.</forenames></author><author><keyname>Davenport</keyname><forenames>J. H.</forenames></author><author><keyname>England</keyname><forenames>M.</forenames></author><author><keyname>Maza</keyname><forenames>M. Moreno</forenames></author><author><keyname>Wilson</keyname><forenames>D.</forenames></author></authors><title>Truth Table Invariant Cylindrical Algebraic Decomposition by Regular
  Chains</title><categories>cs.SC math.AG</categories><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>V.P. Gerdt W. Koepf, W.M. Seiler and E.V. Vorozhtsov, eds.
  Computer Algebra in Scientific Computing, pp. 44-58. (Lecture Notes in
  Computer Science, 8660). Springer International, 2014</journal-ref><doi>10.1007/978-3-319-10515-4_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new algorithm to compute cylindrical algebraic decompositions (CADs) is
presented, building on two recent advances. Firstly, the output is truth table
invariant (a TTICAD) meaning given formulae have constant truth value on each
cell of the decomposition. Secondly, the computation uses regular chains theory
to first build a cylindrical decomposition of complex space (CCD) incrementally
by polynomial. Significant modification of the regular chains technology was
used to achieve the more sophisticated invariance criteria. Experimental
results on an implementation in the RegularChains Library for Maple verify that
combining these advances gives an algorithm superior to its individual
components and competitive with the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6312</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6312</id><created>2014-01-24</created><authors><author><keyname>De Cat</keyname><forenames>Broes</forenames></author><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Predicate Logic as a Modelling Language: The IDP System</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the technology of the time, Kowalski's seminal 1974 paper
  Predicate Logic as a Programming Language was a breakthrough for the use of
logic in computer science. The more recent tremendous progress in automated
reasoning technologies, particularly in SAT solving and Constraint Programming,
has paved the way for the use of logic as a modelling language. % This paper
describes the realisation of such a modelling language as the IDP KBS. In
contrast to declarative programming, the user only specifies her knowledge
about a problem and has not to pay attention to control issues. In the IDP
system, declarative modelling is done in the language FO(.) which combines
inductive definitions (similar to sets of Prolog rules) with first-order logic,
types and aggregates, allowing for concise specifications.
  The paper presents the language, motivates the design choices and gives an
overview of the system architecture and the implementation techniques. It also
gives an overview of different inference tasks supported by the system such as
query evaluation, model expansion and theorem proving, and explains in detail
how combining various functionalities results in a state-of-the-art model
expansion engine. Finally, it explains how a tight integration with a
procedural language (Lua) allows users to treat logical components as
first-class citizens and to solve complex problems in a workflow of
(multi-inference) interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6317</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6317</id><created>2014-01-24</created><updated>2014-03-21</updated><authors><author><keyname>Aamir</keyname><forenames>Muhammad</forenames></author><author><keyname>Zaidi</keyname><forenames>Mustafa Ali</forenames></author></authors><title>DDoS Attack and Defense: Review of Some Traditional and Current
  Techniques</title><categories>cs.CR</categories><comments>This paper is being withdrawn by the author due to a critical signing
  mistake in equation 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Denial of Service (DDoS) attacks exhaust victim's bandwidth or
services. Traditional architecture of Internet is vulnerable to DDoS attacks
and an ongoing cycle of attack &amp; defense is observed. In this paper, different
types and techniques of DDoS attacks and their countermeasures are reviewed.
The significance of this paper is the coverage of many aspects of countering
DDoS attacks including new research on the topic. We survey different papers
describing methods of defense against DDoS attacks based on entropy variations,
traffic anomaly parameters, neural networks, device level defense, botnet flux
identifications and application layer DDoS defense. We also discuss some
traditional methods of defense such as traceback and packet filtering
techniques so that readers can identify major differences between traditional
and current techniques of defense against DDoS attacks. Before the discussion
on countermeasures, we mention different attack types under DDoS with
traditional and advanced schemes while some information on DDoS trends in the
year 2012 Quarter-1 is also provided. We identify that application layer DDoS
attacks possess the ability to produce greater impact on the victim as they are
driven by legitimate-like traffic making it quite difficult to identify and
distinguish from legitimate requests. The need of improved defense against such
attacks is therefore more demanding in research. The study conducted in this
paper can be helpful for readers and researchers to recognize better techniques
of defense in current times against DDoS attacks and contribute with more
research on the topic in the light of future challenges identified in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6325</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6325</id><created>2014-01-24</created><authors><author><keyname>Kochems</keyname><forenames>Jonathan</forenames></author><author><keyname>Ong</keyname><forenames>C-H Luke</forenames></author></authors><title>Safety verification of asynchronous pushdown systems with shaped stacks</title><categories>cs.LO cs.FL cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the program-point reachability problem of concurrent
pushdown systems that communicate via unbounded and unordered message buffers.
Our goal is to relax the common restriction that messages can only be retrieved
by a pushdown process when its stack is empty. We use the notion of partially
commutative context-free grammars to describe a new class of asynchronously
communicating pushdown systems with a mild shape constraint on the stacks for
which the program-point coverability problem remains decidable. Stacks that fit
the shape constraint may reach arbitrary heights; further a process may execute
any communication action (be it process creation, message send or retrieval)
whether or not its stack is empty. This class extends previous computational
models studied in the context of asynchronous programs, and enables the safety
verification of a large class of message passing programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6330</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6330</id><created>2014-01-24</created><updated>2015-03-05</updated><authors><author><keyname>Dong</keyname><forenames>Li</forenames></author><author><keyname>Wei</keyname><forenames>Furu</forenames></author><author><keyname>Liu</keyname><forenames>Shujie</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>A Statistical Parsing Framework for Sentiment Classification</title><categories>cs.CL</categories><comments>Accepted by Computational Linguistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a statistical parsing framework for sentence-level sentiment
classification in this article. Unlike previous works that employ syntactic
parsing results for sentiment analysis, we develop a statistical parser to
directly analyze the sentiment structure of a sentence. We show that
complicated phenomena in sentiment analysis (e.g., negation, intensification,
and contrast) can be handled the same as simple and straightforward sentiment
expressions in a unified and probabilistic way. We formulate the sentiment
grammar upon Context-Free Grammars (CFGs), and provide a formal description of
the sentiment parsing framework. We develop the parsing model to obtain
possible sentiment parse trees for a sentence, from which the polarity model is
proposed to derive the sentiment strength and polarity, and the ranking model
is dedicated to selecting the best sentiment tree. We train the parser directly
from examples of sentences annotated only with sentiment polarity labels but
without any syntactic annotations or polarity annotations of constituents
within sentences. Therefore we can obtain training data easily. In particular,
we train a sentiment parser, s.parser, from a large amount of review sentences
with users' ratings as rough sentiment polarity labels. Extensive experiments
on existing benchmark datasets show significant improvements over baseline
sentiment classification approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6333</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6333</id><created>2014-01-24</created><updated>2014-04-11</updated><authors><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Qian</keyname><forenames>Hong</forenames></author></authors><title>The Sampling-and-Learning Framework: A Statistical View of Evolutionary
  Algorithms</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms (EAs), a large class of general purpose optimization
algorithms inspired from the natural phenomena, are widely used in various
industrial optimizations and often show excellent performance. This paper
presents an attempt towards revealing their general power from a statistical
view of EAs. By summarizing a large range of EAs into the sampling-and-learning
framework, we show that the framework directly admits a general analysis on the
probable-absolute-approximate (PAA) query complexity. We particularly focus on
the framework with the learning subroutine being restricted as a binary
classification, which results in the sampling-and-classification (SAC)
algorithms. With the help of the learning theory, we obtain a general upper
bound on the PAA query complexity of SAC algorithms. We further compare SAC
algorithms with the uniform search in different situations. Under the
error-target independence condition, we show that SAC algorithms can achieve
polynomial speedup to the uniform search, but not super-polynomial speedup.
Under the one-side-error condition, we show that super-polynomial speedup can
be achieved. This work only touches the surface of the framework. Its power
under other conditions is still open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6335</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6335</id><created>2014-01-24</created><updated>2015-11-19</updated><authors><author><keyname>Markstr&#xf6;m</keyname><forenames>Klas</forenames></author></authors><title>From the Ising and Potts models to the general graph homomorphism
  polynomial</title><categories>math.CO cond-mat.stat-mech cs.DM</categories><comments>V2. Extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we study some of the properties of the generating polynomial for
homomorphisms from a graph to at complete weighted graph on $q$ vertices. We
discuss how this polynomial relates to a long list of other well known graph
polynomials and the partition functions for different spin models, many of
which are specialisations of the homomorphism polynomial.
  We also identify the smallest graphs which are not determined by their
homomorphism polynomials for $q=2$ and $q=3$ and compare this with the
corresponding minimal examples for the $U$-polynomial, which generalizes the
well known Tutte-polynomal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6336</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6336</id><created>2014-01-24</created><authors><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Senecal</keyname><forenames>Stephane</forenames></author><author><keyname>Bridon</keyname><forenames>Constant</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author></authors><title>A Fluid Approach for Poisson Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the different models of networks usually considered, the hexagonal
network model is the most popular. However, it requires extensive numerical
computations. The Poisson network model, for which the base stations (BS)
locations form a spatial Poisson process, allows to consider a non constant
distance between base stations. Therefore, it may characterize more
realistically operational networks. The Fluid network model, for which the
interfering BS are replaced by a continuum of infinitesimal interferers, allows
to establish closed-form formula for the SINR (Signal on Interference plus
Noise Ratio). This model was validated by comparison with an hexagonal network.
The two models establish very close results. As a consequence, the Fluid
network model can be used to analyze hexagonal networks. In this paper, we show
that the Fluid network model can also be used to analyze Poisson networks.
Therefore, the analysis of performance and quality of service becomes very
easy, whatever the type of network model, by using the analytical expression of
the SINR established by considering the Fluid network model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6338</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6338</id><created>2014-01-24</created><updated>2014-06-02</updated><authors><author><keyname>Bunte</keyname><forenames>Christoph</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>Encoding Tasks and R\'enyi Entropy</title><categories>cs.IT math.IT</categories><comments>12 pages; accepted for publication in the IEEE Transactions on
  Information Theory; minor changes in the presentation; added a section on
  tasks with costs; presented in part at ITW 2013; to be presented in part at
  ISIT 2014</comments><doi>10.1109/TIT.2014.2329490</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A task is randomly drawn from a finite set of tasks and is described using a
fixed number of bits. All the tasks that share its description must be
performed. Upper and lower bounds on the minimum $\rho$-th moment of the number
of performed tasks are derived. The case where a sequence of tasks is produced
by a source and $n$ tasks are jointly described using $nR$ bits is considered.
If $R$ is larger than the R\'enyi entropy rate of the source of order
$1/(1+\rho)$ (provided it exists), then the $\rho$-th moment of the ratio of
performed tasks to $n$ can be driven to one as $n$ tends to infinity. If $R$ is
smaller than the R\'enyi entropy rate, this moment tends to infinity. The
results are generalized to account for the presence of side-information. In
this more general setting, the key quantity is a conditional version of R\'enyi
entropy that was introduced by Arimoto. For IID sources two additional
extensions are solved, one of a rate-distortion flavor and the other where
different tasks may have different nonnegative costs. Finally, a divergence
that was identified by Sundaresan as a mismatch penalty in the Massey-Arikan
guessing problem is shown to play a similar role here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6346</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6346</id><created>2014-01-24</created><authors><author><keyname>Burcsi</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Lipt&#xe1;k</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Ruskey</keyname><forenames>Frank</forenames></author><author><keyname>Sawada</keyname><forenames>Joe</forenames></author></authors><title>On Combinatorial Generation of Prefix Normal Words</title><categories>cs.DS cs.DM</categories><comments>12 pages, 5 figures</comments><journal-ref>Combinatorial Pattern Matching 2014, LNCS 8464, 60-69</journal-ref><doi>10.1007/978-3-319-07566-2_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prefix normal word is a binary word with the property that no substring has
more 1s than the prefix of the same length. This class of words is important in
the context of binary jumbled pattern matching. In this paper we present an
efficient algorithm for exhaustively listing the prefix normal words with a
fixed length. The algorithm is based on the fact that the language of prefix
normal words is a bubble language, a class of binary languages with the
property that, for any word w in the language, exchanging the first occurrence
of 01 by 10 in w results in another word in the language. We prove that each
prefix normal word is produced in O(n) amortized time, and conjecture, based on
experimental evidence, that the true amortized running time is O(polylog(n)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6348</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6348</id><created>2014-01-24</created><authors><author><keyname>Khan</keyname><forenames>Atif Ali</forenames></author><author><keyname>Naseer</keyname><forenames>Oumair</forenames></author></authors><title>Fuzzy Logic Based Multi User Adaptive Test System</title><categories>cs.CY</categories><comments>Published online: Aug 20, 2012</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE), e-ISSN: 2251-7545, Vol. 2, no. 8, 2012</journal-ref><doi>10.7321/jscse.v2.n8.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present proliferation of e-learning has been actively underway for the
last 10 years. Current research in Adaptive Testing System focuses on the
development of psychometric models with items selection strategies applicable
to adaptive testing processes. The key aspect of proposed Adaptive Testing
System is to develop an increasingly sophisticated latent trait model which can
assist users in developing and enhancing their skills. Computerized Adaptive
Test (CAT) System requires a lot of investment in time and effort to develop
analyze and administrate an adaptive test. In this paper a fuzzy logic based
Multi User Adaptive Test System (MUATS) is developed. Which is a Short
Messaging Service (SMS) based System, currently integrated with GSM network
based on the new psychometric model in education assessment. MUATS is not only
a platform independent Adaptive Test System but also it eases the evaluation
effort for adaptive test process. It further uses fuzzy logic to pick the most
appropriate question from the pool of database for a specific user to be asked
which makes the overall system an intelligent one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6354</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6354</id><created>2014-01-24</created><updated>2015-04-02</updated><authors><author><keyname>Schnass</keyname><forenames>Karin</forenames></author></authors><title>Local Identification of Overcomplete Dictionaries</title><categories>cs.IT math.IT stat.ML</categories><comments>32 pages, 2 figures, final version accepted to JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first theoretical results showing that stable
identification of overcomplete $\mu$-coherent dictionaries $\Phi \in
\mathbb{R}^{d\times K}$ is locally possible from training signals with sparsity
levels $S$ up to the order $O(\mu^{-2})$ and signal to noise ratios up to
$O(\sqrt{d})$. In particular the dictionary is recoverable as the local maximum
of a new maximisation criterion that generalises the K-means criterion. For
this maximisation criterion results for asymptotic exact recovery for sparsity
levels up to $O(\mu^{-1})$ and stable recovery for sparsity levels up to
$O(\mu^{-2})$ as well as signal to noise ratios up to $O(\sqrt{d})$ are
provided. These asymptotic results translate to finite sample size recovery
results with high probability as long as the sample size $N$ scales as $O(K^3dS
\tilde \varepsilon^{-2})$, where the recovery precision $\tilde \varepsilon$
can go down to the asymptotically achievable precision. Further, to actually
find the local maxima of the new criterion, a very simple Iterative
Thresholding and K (signed) Means algorithm (ITKM), which has complexity
$O(dKN)$ in each iteration, is presented and its local efficiency is
demonstrated in several experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6359</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6359</id><created>2014-01-24</created><updated>2014-06-06</updated><authors><author><keyname>Gor&#xed;n</keyname><forenames>Daniel</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author></authors><title>Subsumption Checking in Conjunctive Coalgebraic Fixpoint Logics</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While reasoning in a logic extending a complete Boolean basis is coNP-hard,
restricting to conjunctive fragments of modal languages sometimes allows for
tractable reasoning even in the presence of greatest fixpoints. One such
example is the EL family of description logics; here, efficient reasoning is
based on satisfaction checking in suitable small models that characterize
formulas in terms of simulations. It is well-known, though, that not every
conjunctive modal language has a tractable reasoning problem. Natural questions
are then how common such tractable fragments are and how to identify them. In
this work we provide sufficient conditions for tractability in a general way by
considering unlabeled tableau rules for a given modal logic. We work in the
framework of coalgebraic logic as a unifying semantic setting. Apart from
recovering known results for description logics such as EL and FL0, we obtain
new ones for conjunctive fragments of relational and non-relational modal
logics with greatest fixpoints. Most notably we find tractable fragments of
game logic and the alternating-time mu-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6360</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6360</id><created>2014-01-24</created><authors><author><keyname>Dayan</keyname><forenames>Niv</forenames><affiliation>IT</affiliation></author><author><keyname>Svendsen</keyname><forenames>Martin Kjaer</forenames><affiliation>IT</affiliation></author><author><keyname>Bjorling</keyname><forenames>Matias</forenames><affiliation>IT</affiliation></author><author><keyname>Bonnet</keyname><forenames>Philippe</forenames><affiliation>IT</affiliation></author><author><keyname>Bouganim</keyname><forenames>Luc</forenames><affiliation>PRISM, INRIA Rocquencourt</affiliation></author></authors><title>EagleTree: Exploring the Design Space of SSD-Based Algorithms</title><categories>cs.DB</categories><comments>39th International Conference on Very Large Data Bases (VLDB) (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solid State Drives (SSDs) are a moving target for system designers: they are
black boxes, their internals are undocumented, and their performance
characteristics vary across models. There is no appropriate analytical model
and experimenting with commercial SSDs is cumbersome, as it requires a careful
experimental methodology to ensure repeatability. Worse, performance results
obtained on a given SSD cannot be generalized. Overall, it is impossible to
explore how a given algorithm, say a hash join or LSM-tree insertions,
leverages the intrinsic parallelism of a modern SSD, or how a slight change in
the internals of an SSD would impact its overall performance. In this paper, we
propose a new SSD simulation framework, named EagleTree, which addresses these
problems, and enables a principled study of SSD-Based algorithms. The
demonstration scenario illustrates the design space for algorithms based on an
SSD-based IO stack, and shows how researchers and practitioners can use
EagleTree to perform tractable explorations of this complex design space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6361</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6361</id><created>2014-01-24</created><authors><author><keyname>Changuel</keyname><forenames>Nesrine</forenames><affiliation>LTCI, IUF</affiliation></author><author><keyname>Sayadi</keyname><forenames>Bessem</forenames><affiliation>LTCI, IUF</affiliation></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames><affiliation>LTCI, IUF</affiliation></author></authors><title>Control of Multiple Remote Servers for Quality-Fair Delivery of
  Multimedia Contents</title><categories>cs.MM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a control scheme for the quality-fair delivery of several
encoded video streams to mobile users sharing a common wireless resource. Video
quality fairness, as well as similar delivery delays are targeted among
streams. The proposed controller is implemented within some aggregator located
near the bottleneck of the network. The transmission rate among streams is
adapted based on the quality of the already encoded and buffered packets in the
aggregator. Encoding rate targets are evaluated by the aggregator and fed back
to each remote video server (fully centralized solution), or directly evaluated
by each server in a distributed way (partially distributed solution). Each
encoding rate target is adjusted for each stream independently based on the
corresponding buffer level or buffering delay in the aggregator. Communication
delays between the servers and the aggregator are taken into account. The
transmission and encoding rate control problems are studied with a
control-theoretic perspective. The system is described with a multi-input
multi-output model. Proportional Integral (PI) controllers are used to adjust
the video quality and control the aggregator buffer levels. The system
equilibrium and stability properties are studied. This provides guidelines for
choosing the parameters of the PI controllers. Experimental results show the
convergence of the proposed control system and demonstrate the improvement in
video quality fairness compared to a classical transmission rate fair streaming
solution and to a utility max-min fair approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6362</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6362</id><created>2014-01-24</created><updated>2014-09-11</updated><authors><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author></authors><title>The Capacity of Known Interference Channel (updated)</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the capacity of known interference channel,
where the receiver knows the interference data but not the channel gain of the
interference data. We first derive a tight upper bound for the capacity of this
known-interference channel. After that, we obtain an achievable rate of the
channel with a blind known interference cancellation (BKIC) scheme in closed
form. We prove that the aforementioned upper bound in the high SNR regime can
be approached by our achievable rate. Moreover, the achievable rate of our BKIC
scheme is much larger than that of the traditional interference cancellation
scheme. In particular, the achievable rate of BKIC continues to increase with
SNR in the high SNR regime (non-zero degree of freedom), while that of the
traditional scheme approaches a fixed bound that does not improve with SNR
(zero degree of freedom).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6365</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6365</id><created>2014-01-24</created><authors><author><keyname>Faghih</keyname><forenames>Behnam</forenames></author><author><keyname>Azadehfar</keyname><forenames>Dr. Mohammad Reza</forenames></author><author><keyname>Katebi</keyname><forenames>Prof. S. D.</forenames></author></authors><title>User Interface Design for E-Learning Software</title><categories>cs.CY cs.HC</categories><comments>9 pages</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 786-794, 2013</journal-ref><doi>10.7321/jscse.v3.n3.119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User interface (UI) is point of interaction between user and computer
software. The success and failure of a software application depends on User
Interface Design (UID). Possibility of using a software, easily using and
learning are issues influenced by UID. The UI is significant in designing of
educational software (e-Learning). Principles and concepts of learning should
be considered in addition to UID principles in UID for e-learning. In this
regard, to specify the logical relationship between education, learning, UID
and multimedia at first we readdress the issues raised in previous studies. It
is followed by examining the principle concepts of e-learning and UID. Then, we
will see how UID contributes to e-learning through the educational software
built by authors. Also we show the way of using UI to improve learning and
motivating the learners and to improve the time efficiency of using e-learning
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6370</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6370</id><created>2014-01-24</created><authors><author><keyname>Zhang</keyname><forenames>Haipeng</forenames></author><author><keyname>Kong</keyname><forenames>Lingjun</forenames></author><author><keyname>Huang</keyname><forenames>Xiuju</forenames></author><author><keyname>Cao</keyname><forenames>Mengmeng</forenames></author></authors><title>Design of a High Speed XAUI Based on Dynamic Reconfigurable Transceiver
  IP Core</title><categories>cs.AR</categories><comments>10 pages, 5 figures, 5 tables, 14 references</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE) Vol.2,No.9, 2012 Published online: Sep 25, 2012</journal-ref><doi>10.7321/jscse.v2.n9.4</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  By using the dynamic reconfigurable transceiver in high speed interface
design, designer can solve critical technology problems such as ensuring signal
integrity conveniently, with lower error binary rate. In this paper, we
designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to
transparently extend the physical reach of the XGMII. The following points are
focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two
transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design
which is featured of dynamic reconfiguration performance, that is,
ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by
changing settings of the devices without power off. These two blocks can
accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium
Attachment), however, with higher efficiency and reliability. (2) 1+1
protection. In our design, two ALTGX IP cores are used to work in parallel,
which named XAUI0 and XAUI1. The former works as the main channel while the
latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1
will start to work to keep the business. (3) RTL (Register Transfer Level)
coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and
ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then
connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the
design was simulated on VCS simulator. The validated result indicates that the
packets are transferred efficiently. FPGA makes high-speed optical
communication system design simplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6375</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6375</id><created>2014-01-24</created><authors><author><keyname>Liu</keyname><forenames>Yixin</forenames></author><author><keyname>Zhang</keyname><forenames>Haipeng</forenames></author><author><keyname>Feng</keyname><forenames>Tao</forenames></author></authors><title>Design of an Encryption-Decryption Module Oriented for Internet
  Information Security SOC Design</title><categories>cs.AR cs.CR</categories><comments>11 pages, 5 figures, 4 tables, 14 references</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE),Vol.2,No.7, 2012</journal-ref><doi>10.7321/jscse.v2.n7.3</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In order to protect the security of network data, a high speed chip module
for encrypting and decrypting of network data packet is designed. The chip
module is oriented for internet information security SOC (System on Chip)
design. During the design process, AES (Advanced Encryption Standard) and 3DES
(Data Encryption Standard) encryption algorithm are adopted to protect the
security of network data. The following points are focused: (1) The SOC (System
on Chip) design methodology based on IP (Intellectual Property) core is used.
AES (Advanced Encryption Standard) and 3DES (Data Encryption Standard) IP
(Intellectual Property) cores are embedded in the chip module, peripheral
control sub-modules are designed to control the encryption-decryption module,
which is capable of shortening the design period of the chip module. (2) The
implementation of encryption-decryption with hardware was presented, which
improves the safety of data through the encryption-decryption chip and reduce
the load of CPU. (3) In our hardware solution, two AES (Advanced Encryption
Standard) cores are used to work in parallel, which improves the speed of the
encryption module. Moreover, the key length of AES (Advanced Encryption
Standard) encryption algorithm is designed with three optional configurations
at 128 bits, 256 bits and 192 bits respectively and six optional encryption
algorithm modes: CBC (Cipher Block Chaining) mode, ECB (Electronic Code Book)
mode, GCM (Galois/Counter Mode) mode, XTS(cipherteXT Stealing) mode, CTR
(CounTeR) mode and 3DES respectively, which adds the flexibility to its
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6376</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6376</id><created>2014-01-24</created><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Bermudez</keyname><forenames>Jos&#xe9; Carlos M.</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Steady-state performance of non-negative least-mean-square algorithm and
  its variants</title><categories>cs.LG</categories><comments>10 pages, 4 figures</comments><doi>10.1109/LSP.2014.2320944</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative least-mean-square (NNLMS) algorithm and its variants have been
proposed for online estimation under non-negativity constraints. The transient
behavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMS
algorithms have been studied in our previous work. In this technical report, we
derive closed-form expressions for the steady-state excess mean-square error
(EMSE) for the four algorithms. Simulations results illustrate the accuracy of
the theoretical results. This is a complementary material to our previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6379</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6379</id><created>2013-11-20</created><updated>2014-01-27</updated><authors><author><keyname>Nyagudi</keyname><forenames>Nyagudi Musandu</forenames></author></authors><title>Post-Westgate SWAT : C4ISTAR Architectural Framework for Autonomous
  Network Integrated Multifaceted Warfighting Solutions Version 1.0 : A
  Peer-Reviewed Monograph</title><categories>cs.CR</categories><comments>52 pages, 6 Figures, over 40 references, reviewed by a reader</comments><msc-class>68-68</msc-class><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Police SWAT teams and Military Special Forces face mounting pressure and
challenges from adversaries that can only be resolved by way of ever more
sophisticated inputs into tactical operations. Lethal Autonomy provides
constrained military/security forces with a viable option, but only if
implementation has got proper empirically supported foundations. Autonomous
weapon systems can be designed and developed to conduct ground, air and naval
operations. This monograph offers some insights into the challenges of
developing legal, reliable and ethical forms of autonomous weapons, that
address the gap between Police or Law Enforcement and Military operations that
is growing exponentially small. National adversaries are today in many
instances hybrid threats, that manifest criminal and military traits, these
often require deployment of hybrid-capability autonomous weapons imbued with
the capability to taken on both Military and/or Security objectives. The
Westgate Terrorist Attack of 21st September 2013 in the Westlands suburb of
Nairobi, Kenya is a very clear manifestation of the hybrid combat scenario that
required military response and police investigations against a fighting cell of
the Somalia based globally networked Al Shabaab terrorist group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6380</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6380</id><created>2014-01-24</created><authors><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Properties of spatial coupling in compressed sensing</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address a series of open questions about the construction of
spatially coupled measurement matrices in compressed sensing. For hardware
implementations one is forced to depart from the limiting regime of parameters
in which the proofs of the so-called threshold saturation work. We investigate
quantitatively the behavior under finite coupling range, the dependence on the
shape of the coupling interaction, and optimization of the so-called seed to
minimize distance from optimality. Our analysis explains some of the properties
observed empirically in previous works and provides new insight on spatially
coupled compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6384</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6384</id><created>2014-01-24</created><authors><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>On Convergence of Approximate Message Passing</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>5 pages, 3 figures</comments><journal-ref>Information Theory Proceedings (ISIT), 2014 IEEE International
  Symposium on, page(s) 1812-1816</journal-ref><doi>10.1109/ISIT.2014.6875146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message passing is an iterative algorithm for compressed sensing
and related applications. A solid theory about the performance and convergence
of the algorithm exists for measurement matrices having iid entries of zero
mean. However, it was observed by several authors that for more general
matrices the algorithm often encounters convergence problems. In this paper we
identify the reason of the non-convergence for measurement matrices with iid
entries and non-zero mean in the context of Bayes optimal inference. Finally we
demonstrate numerically that when the iterative update is changed from parallel
to sequential the convergence is restored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6385</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6385</id><created>2014-01-24</created><authors><author><keyname>Lu</keyname><forenames>Songjian</forenames></author><author><keyname>Lu</keyname><forenames>Xinghua</forenames></author></authors><title>An exact algorithm for the weighed mutually exclusive maximum set cover
  problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an exact algorithm with a time complexity of
$O^*(1.325^m)$ for the {\sc weighted mutually exclusive maximum set cover}
problem, where $m$ is the number of subsets in the problem. This is an NP-hard
motivated and abstracted from a bioinformatics problem of identifying signaling
pathways based gene mutations. Currently, this problem is addressed using
heuristic algorithms, which cannot guarantee the performance of the solution.
By providing a relatively efficient exact algorithm, our approach will like
increase the capability of finding better solutions in the application of
cancer research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6393</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6393</id><created>2014-01-24</created><authors><author><keyname>Hansard</keyname><forenames>Miles</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author><author><keyname>Amat</keyname><forenames>Michel</forenames></author><author><keyname>Evangelidis</keyname><forenames>Georgios</forenames></author></authors><title>Automatic Detection of Calibration Grids in Time-of-Flight Images</title><categories>cs.CV</categories><comments>11 pages, 11 figures, 1 table</comments><journal-ref>Computer Vision and Image Understanding, 121 (2014) 108-118</journal-ref><doi>10.1016/j.cviu.2014.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is convenient to calibrate time-of-flight cameras by established methods,
using images of a chequerboard pattern. The low resolution of the amplitude
image, however, makes it difficult to detect the board reliably. Heuristic
detection methods, based on connected image-components, perform very poorly on
this data. An alternative, geometrically-principled method is introduced here,
based on the Hough transform. The projection of a chequerboard is represented
by two pencils of lines, which are identified as oriented clusters in the
gradient-data of the image. A projective Hough transform is applied to each of
the two clusters, in axis-aligned coordinates. The range of each transform is
properly bounded, because the corresponding gradient vectors are approximately
parallel. Each of the two transforms contains a series of collinear peaks; one
for every line in the given pencil. This pattern is easily detected, by
sweeping a dual line through the transform. The proposed Hough-based method is
compared to the standard OpenCV detection routine, by application to several
hundred time-of-flight images. It is shown that the new method detects
significantly more calibration boards, over a greater variety of poses, without
any overall loss of accuracy. This conclusion is based on an analysis of both
geometric and photometric error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6396</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6396</id><created>2014-01-24</created><updated>2014-07-09</updated><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Mazo</keyname><forenames>Manuel</forenames><suffix>Jr</suffix></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Symbolic Models for Networked Control Systems</title><categories>math.OC cs.FL cs.SY</categories><comments>17 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1006.2853 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last decade has witnessed significant attention on networked control
systems (NCS) due to their high architecture flexibility and low installation
and maintenance costs. In a NCS, communication between sensors, controllers,
and actuators is supported by a shared communication channel that is subject to
variable communication delays, quantization errors, packet losses, limited
bandwidth, and other practical non idealities leading to numerous technical
challenges. Although stability properties of NCS have been investigated
extensively in the past few years, there exist no remarkable results for NCS
dealing with more complex and general objectives, such as verification or
(controller) synthesis for logical specifications. This work investigates those
complex objectives by constructively deriving symbolic models of NCS, while
encompassing the mentioned network non-idealities: the obtained abstracted
models can be employed to synthesize hybrid controllers enforcing rich logical
specifications over NCS. Examples of such specifications include properties
expressed as formulae in linear temporal logic (LTL) or as automata on infinite
strings, as evidenced in a final example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6399</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6399</id><created>2014-01-24</created><updated>2015-05-06</updated><authors><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Boytsov</keyname><forenames>Leonid</forenames></author><author><keyname>Kurz</keyname><forenames>Nathan</forenames></author></authors><title>SIMD Compression and the Intersection of Sorted Integers</title><categories>cs.IR cs.DB</categories><doi>10.1002/spe.2326</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sorted lists of integers are commonly used in inverted indexes and database
systems. They are often compressed in memory. We can use the SIMD instructions
available in common processors to boost the speed of integer compression
schemes. Our S4-BP128-D4 scheme uses as little as 0.7 CPU cycles per decoded
integer while still providing state-of-the-art compression.
  However, if the subsequent processing of the integers is slow, the effort
spent on optimizing decoding speed can be wasted. To show that it does not have
to be so, we (1) vectorize and optimize the intersection of posting lists; (2)
introduce the SIMD Galloping algorithm. We exploit the fact that one SIMD
instruction can compare 4 pairs of integers at once.
  We experiment with two TREC text collections, GOV2 and ClueWeb09 (Category
B), using logs from the TREC million-query track. We show that using only the
SIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive
queries can double the speed of a state-of-the-art approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6404</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6404</id><created>2014-01-24</created><authors><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author><author><keyname>Chandra</keyname><forenames>Abhishek</forenames></author></authors><title>Predicting Multi-actor collaborations using Hypergraphs</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are now ubiquitous and most of them contain interactions
involving multiple actors (groups) like author collaborations, teams or emails
in an organizations, etc. Hypergraphs are natural structures to effectively
capture multi-actor interactions which conventional dyadic graphs fail to
capture. In this work the problem of predicting collaborations is addressed
while modeling the collaboration network as a hypergraph network. The problem
of predicting future multi-actor collaboration is mapped to hyperedge
prediction problem. Given that the higher order edge prediction is an
inherently hard problem, in this work we restrict to the task of predicting
edges (collaborations) that have already been observed in past. In this work,
we propose a novel use of hyperincidence temporal tensors to capture time
varying hypergraphs and provides a tensor decomposition based prediction
algorithm. We quantitatively compare the performance of the hypergraphs based
approach with the conventional dyadic graph based approach. Our hypothesis that
hypergraphs preserve the information that simple graphs destroy is corroborated
by experiments using author collaboration network from the DBLP dataset. Our
results demonstrate the strength of hypergraph based approach to predict higher
order collaborations (size&gt;4) which is very difficult using dyadic graph based
approach. Moreover, while predicting collaborations of size&gt;2 hypergraphs in
most cases provide better results with an average increase of approx. 45% in
F-Score for different sizes = {3,4,5,6,7}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6410</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6410</id><created>2014-01-24</created><authors><author><keyname>Steinruecken</keyname><forenames>Christian</forenames></author></authors><title>Compressing Sets and Multisets of Sequences</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes lossless compression algorithms for multisets of
sequences, taking advantage of the multiset's unordered structure. Multisets
are a generalisation of sets where members are allowed to occur multiple times.
A multiset can be encoded na\&quot;ively by simply storing its elements in some
sequential order, but then information is wasted on the ordering. We propose a
technique that transforms the multiset into an order-invariant tree
representation, and derive an arithmetic code that optimally compresses the
tree. Our method achieves compression even if the sequences in the multiset are
individually incompressible (such as cryptographic hash sums). The algorithm is
demonstrated practically by compressing collections of SHA-1 hash sums, and
multisets of arbitrary, individually encodable objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6413</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6413</id><created>2014-01-23</created><updated>2014-10-06</updated><authors><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Sayin</keyname><forenames>Muhammed O.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Predicting Nearly As Well As the Optimal Twice Differentiable Regressor</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study nonlinear regression of real valued data in an individual sequence
manner, where we provide results that are guaranteed to hold without any
statistical assumptions. We address the convergence and undertraining issues of
conventional nonlinear regression methods and introduce an algorithm that
elegantly mitigates these issues via an incremental hierarchical structure,
(i.e., via an incremental decision tree). Particularly, we present a piecewise
linear (or nonlinear) regression algorithm that partitions the regressor space
in a data driven manner and learns a linear model at each region. Unlike the
conventional approaches, our algorithm gradually increases the number of
disjoint partitions on the regressor space in a sequential manner according to
the observed data. Through this data driven approach, our algorithm
sequentially and asymptotically achieves the performance of the optimal twice
differentiable regression function for any data sequence with an unknown and
arbitrary length. The computational complexity of the introduced algorithm is
only logarithmic in the data length under certain regularity conditions. We
provide the explicit description of the algorithm and demonstrate the
significant gains for the well-known benchmark real data sets and chaotic
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6420</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6420</id><created>2014-01-22</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Zombie Politics: Evolutionary Algorithms to Counteract the Spread of
  Negative Opinions</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about simulating the spread of opinions in a society and about
finding ways to counteract that spread. To abstract away from potentially
emotionally laden opinions, we instead simulate the spread of a zombie outbreak
in a society. The virus causing this outbreak is different from traditional
approaches: it not only causes a binary outcome (healthy vs infected) but
rather a continuous outcome. To counteract the outbreak, a discrete number of
infection-level specific treatments is available. This corresponds to acts of
mild persuasion or the threats of legal action in the opinion spreading use
case. This paper offers a genetic and a cultural algorithm that find the
optimal mixture of treatments during the run of the simulation. They are
assessed in a number of different scenarios. It is shown, that albeit far from
being perfect, the cultural algorithm delivers superior performance at lower
computational expense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6421</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6421</id><created>2014-01-22</created><authors><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>Riffled Independence for Efficient Inference with Partial Rankings</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1202.3734</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 44, pages
  491-532, 2012</journal-ref><doi>10.1613/jair.3543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributions over rankings are used to model data in a multitude of real
world settings such as preference analysis and political elections. Modeling
such distributions presents several computational challenges, however, due to
the factorial size of the set of rankings over an item set. Some of these
challenges are quite familiar to the artificial intelligence community, such as
how to compactly represent a distribution over a combinatorially large space,
and how to efficiently perform probabilistic inference with these
representations. With respect to ranking, however, there is the additional
challenge of what we refer to as human task complexity users are rarely willing
to provide a full ranking over a long list of candidates, instead often
preferring to provide partial ranking information. Simultaneously addressing
all of these challenges i.e., designing a compactly representable model which
is amenable to efficient inference and can be learned using partial ranking
data is a difficult task, but is necessary if we would like to scale to
problems with nontrivial size. In this paper, we show that the recently
proposed riffled independence assumptions cleanly and efficiently address each
of the above challenges. In particular, we establish a tight mathematical
connection between the concepts of riffled independence and of partial
rankings. This correspondence not only allows us to then develop efficient and
exact algorithms for performing inference tasks using riffled independence
based represen- tations with partial rankings, but somewhat surprisingly, also
shows that efficient inference is not possible for riffle independent models
(in a certain sense) with observations which do not take the form of partial
rankings. Finally, using our inference algorithm, we introduce the first method
for learning riffled independence based models from partially ranked data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6422</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6422</id><created>2014-01-22</created><authors><author><keyname>Sauper</keyname><forenames>Christina</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author></authors><title>Automatic Aggregation by Joint Modeling of Aspects and Values</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  89-127, 2013</journal-ref><doi>10.1613/jair.3647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model for aggregation of product review snippets by joint aspect
identification and sentiment analysis. Our model simultaneously identifies an
underlying set of ratable aspects presented in the reviews of a product (e.g.,
sushi and miso for a Japanese restaurant) and determines the corresponding
sentiment of each aspect. This approach directly enables discovery of
highly-rated or inconsistent aspects of a product. Our generative model admits
an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model
structure and inference. We test our model on two tasks, joint aspect
identification and sentiment analysis on a set of Yelp reviews and aspect
identification alone on a set of medical summaries. We evaluate the performance
of the model on aspect identification, sentiment analysis, and per-word
labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction
on aspect identification and up to 20% relative error reduction on sentiment
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6423</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6423</id><created>2014-01-23</created><updated>2014-04-16</updated><authors><author><keyname>Liu</keyname><forenames>Hanlin</forenames></author></authors><title>A Algorithm for the Hamilton Circuit Problem</title><categories>cs.DS</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we introduce two new algorithm to find a Hamilton Circuit in a
graph G=(V,E). One algorithm is use a multistage graph as a special NFAs to
find all Hamilton Circuit in exponential time; while another is use O(|V|)
variant multistage graph as a special state set NFAs to find a fuzzy data of
all Hamilton Circuit in polynomial time. The fuzzy data of the data contain
those data, and the fuzzy data is not empty, if and only if the data is also
not empty. And, the data is also not empty if and only if there are Hamilton
Circuit in the graph. And we can find a Hamilton Circuit by the fuzzy data. Our
result implies NP=P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6424</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6424</id><created>2014-01-22</created><authors><author><keyname>Goernitz</keyname><forenames>Nico</forenames></author><author><keyname>Kloft</keyname><forenames>Marius Micha</forenames></author><author><keyname>Rieck</keyname><forenames>Konrad</forenames></author><author><keyname>Brefeld</keyname><forenames>Ulf</forenames></author></authors><title>Toward Supervised Anomaly Detection</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 46, pages
  235-262, 2013</journal-ref><doi>10.1613/jair.3623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection is being regarded as an unsupervised learning task as
anomalies stem from adversarial or unlikely events with unknown distributions.
However, the predictive performance of purely unsupervised anomaly detection
often fails to match the required detection rates in many tasks and there
exists a need for labeled data to guide the model generation. Our first
contribution shows that classical semi-supervised approaches, originating from
a supervised classifier, are inappropriate and hardly detect new and unknown
anomalies. We argue that semi-supervised anomaly detection needs to ground on
the unsupervised learning paradigm and devise a novel algorithm that meets this
requirement. Although being intrinsically non-convex, we further show that the
optimization problem has a convex equivalent under relatively mild assumptions.
Additionally, we propose an active learning strategy to automatically filter
candidates for labeling. In an empirical study on network intrusion detection
data, we observe that the proposed learning methodology requires much less
labeled data than the state-of-the-art, while achieving higher detection
accuracies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6427</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6427</id><created>2014-01-22</created><authors><author><keyname>Mirroshandel</keyname><forenames>Seyed Abolghasem</forenames></author><author><keyname>Ghassem-Sani</keyname><forenames>Gholamreza</forenames></author></authors><title>Towards Unsupervised Learning of Temporal Relations between Events</title><categories>cs.LG cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  125-163, 2012</journal-ref><doi>10.1613/jair.3693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic extraction of temporal relations between event pairs is an
important task for several natural language processing applications such as
Question Answering, Information Extraction, and Summarization. Since most
existing methods are supervised and require large corpora, which for many
languages do not exist, we have concentrated our efforts to reduce the need for
annotated data as much as possible. This paper presents two different
algorithms towards this goal. The first algorithm is a weakly supervised
machine learning approach for classification of temporal relations between
events. In the first stage, the algorithm learns a general classifier from an
annotated corpus. Then, inspired by the hypothesis of &quot;one type of temporal
relation per discourse, it extracts useful information from a cluster of
topically related documents. We show that by combining the global information
of such a cluster with local decisions of a general classifier, a bootstrapping
cross-document classifier can be built to extract temporal relations between
events. Our experiments show that without any additional annotated data, the
accuracy of the proposed algorithm is higher than that of several previous
successful systems. The second proposed method for temporal relation extraction
is based on the expectation maximization (EM) algorithm. Within EM, we used
different techniques such as a greedy best-first search and integer linear
programming for temporal inconsistency removal. We think that the experimental
results of our EM based algorithm, as a first step toward a fully unsupervised
temporal relation extraction method, is encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6428</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6428</id><created>2014-01-23</created><authors><author><keyname>Voice</keyname><forenames>Thomas</forenames></author><author><keyname>Polukarov</keyname><forenames>Maria</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Coalition Structure Generation over Graphs</title><categories>cs.DS cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1102.1747</comments><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  165-196, 2012</journal-ref><doi>10.1613/jair.3715</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the analysis of the computational complexity of coalition structure
generation over graphs. Given an undirected graph G = (N,E) and a valuation
function v : P(N) \to R over the subsets of nodes, the problem is to find a
partition of N into connected subsets, that maximises the sum of the components
values. This problem is generally NP-complete; in particular, it is hard for a
defined class of valuation functions which are independent of disconnected
members - that is, two nodes have no effect on each other's marginal
contribution to their vertex separator. Nonetheless, for all such functions we
provide bounds on the complexity of coalition structure generation over general
and minor-free graphs. Our proof is constructive and yields algorithms for
solving corresponding instances of the problem. Furthermore, we derive linear
time bounds for graphs of bounded treewidth. However, as we show, the problem
remains NP-complete for planar graphs, and hence, for any K_k minor free graphs
where k \geq 5. Moreover, a 3-SAT problem with m clauses can be represented by
a coalition structure generation problem over a planar graph with O(m^2) nodes.
Importantly, our hardness result holds for a particular subclass of valuation
functions, termed edge sum, where the value of each subset of nodes is simply
determined by the sum of given weights of the edges in the induced subgraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6432</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6432</id><created>2014-01-24</created><updated>2014-04-27</updated><authors><author><keyname>Elkayam</keyname><forenames>Nir</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>A Universal Decoder Relative to a Given Family of Metrics</title><categories>cs.IT math.IT</categories><comments>Accepted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following framework of universal decoding suggested in
[MerhavUniversal]. Given a family of decoding metrics and random coding
distribution (prior), a single, universal, decoder is optimal if for any
possible channel the average error probability when using this decoder is
better than the error probability attained by the best decoder in the family up
to a subexponential multiplicative factor. We describe a general universal
decoder in this framework. The penalty for using this universal decoder is
computed. The universal metric is constructed as follows. For each metric, a
canonical metric is defined and conditions for the given prior to be normal are
given. A sub-exponential set of canonical metrics of normal prior can be merged
to a single universal optimal metric. We provide an example where this decoder
is optimal while the decoder of [MerhavUniversal] is not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6437</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6437</id><created>2014-01-24</created><updated>2014-11-06</updated><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author></authors><title>On Phase Noise Suppression in Full-Duplex Systems</title><categories>cs.IT math.IT</categories><comments>Published in IEEE transactions on wireless communications on
  October-2014. Please refer to the IEEE version for the most updated document</comments><doi>10.1109/TWC.2014.2365536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oscillator phase noise has been shown to be one of the main performance
limiting factors in full-duplex systems. In this paper, we consider the problem
of self-interference cancellation with phase noise suppression in full-duplex
systems. The feasibility of performing phase noise suppression in full-duplex
systems in terms of both complexity and achieved gain is analytically and
experimentally investigated. First, the effect of phase noise on full-duplex
systems and the possibility of performing phase noise suppression are studied.
Two different phase noise suppression techniques with a detailed complexity
analysis are then proposed. For each suppression technique, both free-running
and phase locked loop based oscillators are considered. Due to the fact that
full-duplex system performance highly depends on hardware impairments,
experimental analysis is essential for reliable results. In this paper, the
performance of the proposed techniques is experimentally investigated in a
typical indoor environment. The experimental results are shown to confirm the
results obtained from numerical simulations on two different experimental
research platforms. At the end, the tradeoff between the required complexity
and the gain achieved using phase noise suppression is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6444</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6444</id><created>2014-01-24</created><authors><author><keyname>Karlsson</keyname><forenames>Karl-Johan</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author></authors><title>Android Anti-forensics: Modifying CyanogenMod</title><categories>cs.CR</categories><comments>Karlsson, K.-J. and W.B. Glisson, Android Anti-forensics: Modifying
  CyanogenMod in Hawaii International Conference on System Sciences (HICSS-47).
  2014, IEEE Computer Society Press: Hawaii</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices implementing Android operating systems inherently create
opportunities to present environments that are conducive to anti-forensic
activities. Previous mobile forensics research focused on applications and data
hiding anti-forensics solutions. In this work, a set of modifications were
developed and implemented on a CyanogenMod community distribution of the
Android operating system. The execution of these solutions successfully
prevented data extractions, blocked the installation of forensic tools, created
extraction delays and presented false data to industry accepted forensic
analysis tools without impacting normal use of the device. The research
contribution is an initial empirical analysis of the viability of operating
system modifications in an anti-forensics context along with providing the
foundation for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6449</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6449</id><created>2014-01-24</created><updated>2015-05-22</updated><authors><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames><affiliation>MATCOM</affiliation></author><author><keyname>De Arazoza</keyname><forenames>Hector</forenames><affiliation>MATCOM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames></author><author><keyname>Tran</keyname><forenames>Viet Chi</forenames></author></authors><title>A statistical network analysis of the HIV/AIDS epidemics in Cuba</title><categories>stat.AP cs.SI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cuban contact-tracing detection system set up in 1986 allowed the
reconstruction and analysis of the sexual network underlying the epidemic
(5,389 vertices and 4,073 edges, giant component of 2,386 nodes and 3,168
edges), shedding light onto the spread of HIV and the role of contact-tracing.
Clustering based on modularity optimization provides a better visualization and
understanding of the network, in combination with the study of covariates. The
graph has a globally low but heterogeneous density, with clusters of high
intraconnectivity but low interconnectivity. Though descriptive, our results
pave the way for incorporating structure when studying stochastic SIR epidemics
spreading on social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6455</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6455</id><created>2014-01-26</created><authors><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Kubo</keyname><forenames>Takeshi</forenames></author><author><keyname>Sugiyama</keyname><forenames>Kohei</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Hasegawa</keyname><forenames>Teruyuki</forenames></author><author><keyname>Walrand</keyname><forenames>Jean</forenames></author></authors><title>Motivating Smartphone Collaboration in Data Acquisition and Distributed
  Computing</title><categories>cs.GT</categories><comments>This paper will appear at IEEE Transactions on Mobil Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes and compares different incentive mechanisms for a master
to motivate the collaboration of smartphone users on both data acquisition and
distributed computing applications. To collect massive sensitive data from
users, we propose a reward-based collaboration mechanism, where the master
announces a total reward to be shared among collaborators, and the
collaboration is successful if there are enough users wanting to collaborate.
We show that if the master knows the users' collaboration costs, then he can
choose to involve only users with the lowest costs. However, without knowing
users' private information, then he needs to offer a larger total reward to
attract enough collaborators. Users will benefit from knowing their costs
before the data acquisition. Perhaps surprisingly, the master may benefit as
the variance of users' cost distribution increases.
  To utilize smartphones' computation resources to solve complex computing
problems, we study how the master can design an optimal contract by specifying
different task-reward combinations for different user types. Under complete
information, we show that the master involves a user type as long as the
master's preference characteristic outweighs that type's unit cost. All
collaborators achieve a zero payoff in this case. If the master does not know
users' private cost information, however, he will conservatively target at a
smaller group of users with small costs, and has to give most benefits to the
collaborators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6476</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6476</id><created>2014-01-24</created><authors><author><keyname>Bethanabhotla</keyname><forenames>Dilip</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Adaptive Video Streaming in MU-MIMO Networks</title><categories>cs.IT cs.MM cs.NI math.IT math.OC</categories><comments>submitted to IEEE Intl. Symposium on Information Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider extensions and improvements on our previous work on dynamic
adaptive video streaming in a multi-cell multiuser ``small cell'' wireless
network. Previously, we treated the case of single-antenna base stations and,
starting from a network utility maximization (NUM) formulation, we devised a
``push'' scheduling policy, where users place requests to sequential video
chunks to possibly different base stations with adaptive video quality, and
base stations schedule their downlink transmissions in order to stabilize their
transmission queues. In this paper we consider a ``pull'' strategy, where every
user maintains a request queue, such that users keep track of the video chunks
that are effectively delivered. The pull scheme allows to download the chunks
in the playback order without skipping or missing them. In addition, motivated
by the recent/forthcoming progress in small cell networks (e.g., in wave-2 of
the recent IEEE 802.11ac standard), we extend our dynamic streaming approach to
the case of base stations capable of multiuser MIMO downlink, i.e., serving
multiple users on the same time-frequency slot by spatial multiplexing. By
exploiting the ``channel hardening'' effect of high dimensional MIMO channels,
we devise a low complexity user selection scheme to solve the underlying
max-weighted rate scheduling, which can be easily implemented and runs
independently at each base station. Through simulations, we show MIMO gains in
terms of video streaming QoE metrics like the pre-buffering and re-buffering
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6481</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6481</id><created>2014-01-24</created><authors><author><keyname>Robertson</keyname><forenames>Neil</forenames></author><author><keyname>Sanders</keyname><forenames>Daniel P.</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Reducibility in the Four-Color Theorem</title><categories>math.CO cs.DM</categories><comments>8 pages, 2 ancillary files. These files were originally posted on an
  anonymous ftp server and later (after the server's shutdown) were relocated
  to the last author's website</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [J. Combin. Theory Ser. B 70 (1997), 2-44] we gave a simplified proof of
the Four-Color Theorem. The proof is computer-assisted in the sense that for
two lemmas in the article we did not give proofs, and instead asserted that we
have verified those statements using a computer. Here we give additional
details for one of those lemmas, and we include the original computer programs
and data as &quot;ancillary files&quot; accompanying this submission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6482</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6482</id><created>2014-01-24</created><authors><author><keyname>Sahebi</keyname><forenames>Aria G.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Nested Polar Codes Achieve the Shannon Rate-Distortion Function and the
  Shannon Capacity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that nested polar codes achieve the Shannon rate-distortion
function for arbitrary (binary or non-binary) discrete memoryless sources and
the Shannon capacity of arbitrary discrete memoryless channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6483</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6483</id><created>2014-01-24</created><authors><author><keyname>Luo</keyname><forenames>Zhixin</forenames></author><author><keyname>Qin</keyname><forenames>Hu</forenames></author><author><keyname>Zhu</keyname><forenames>Wenbin</forenames></author><author><keyname>Lim</keyname><forenames>Andrew</forenames></author></authors><title>Branch-and-price-and-cut for the Split-collection Vehicle Routing
  Problem with Time Windows and Linear Weight-related Cost</title><categories>cs.DS</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a new vehicle routing problem that simultaneously
involves time windows, split collection and linear weight-related cost, which
is a generalization of the split delivery vehicle routing problem with time
windows (SDVRPTW). This problem consists of determining least-cost vehicle
routes to serve a set of customers while respecting the restrictions of vehicle
capacity and time windows. The travel cost per unit distance is a linear
function of the vehicle weight and the customer demand can be fulfilled by
multiple vehicles. To solve this problem, we propose a exact
branch-and-price-and-cut algorithm, where the pricing subproblem is a
resource-constrained elementary least-cost path problem. We first prove that at
least an optimal solution to the pricing subproblem is associated with an
extreme collection pattern, and then design a tailored and novel label-setting
algorithm to solve it. Computational results show that our proposed algorithm
can handle both the SDVRPTW and our problem effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6484</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6484</id><created>2014-01-24</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Identification of Protein Coding Regions in Genomic DNA Using
  Unsupervised FMACA Based Pattern Classifier</title><categories>cs.CE cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1312.2642</comments><journal-ref>IJCSNS International Journal of Computer Science and Network
  Security, VOL.8 No.1, January 2008,305-310</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genes carry the instructions for making proteins that are found in a cell as
a specific sequence of nucleotides that are found in DNA molecules. But, the
regions of these genes that code for proteins may occupy only a small region of
the sequence. Identifying the coding regions play a vital role in understanding
these genes. In this paper we propose a unsupervised Fuzzy Multiple Attractor
Cellular Automata (FMCA) based pattern classifier to identify the coding region
of a DNA sequence. We propose a distinct K-Means algorithm for designing FMACA
classifier which is simple, efficient and produces more accurate classifier
than that has previously been obtained for a range of different sequence
lengths. Experimental results confirm the scalability of the proposed
Unsupervised FCA based classifier to handle large volume of datasets
irrespective of the number of classes, tuples and attributes. Good
classification accuracy has been established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6485</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6485</id><created>2014-01-24</created><authors><author><keyname>Robertson</keyname><forenames>Neil</forenames></author><author><keyname>Sanders</keyname><forenames>Daniel P.</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Discharging cartwheels</title><categories>math.CO cs.DM</categories><comments>22 pages, 9 ancillary files. These files were originally posted on an
  anonymous ftp server and later (after the server's shutdown) were relocated
  to the last author's website</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [J. Combin. Theory Ser. B 70 (1997), 2-44] we gave a simplified proof of
the Four-Color Theorem. The proof is computer-assisted in the sense that for
two lemmas in the article we did not give proofs, and instead asserted that we
have verified those statements using a computer. Here we give additional
details for one of those lemmas, and we include the original computer programs
and data as &quot;ancillary files&quot; accompanying this submission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6488</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6488</id><created>2014-01-24</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Chasing diagrams in cryptography</title><categories>cs.CR math.CT</categories><comments>17 pages, 4 figures; to appear in: 'Categories in Logic, Language and
  Physics. Festschrift on the occasion of Jim Lambek's 90th birthday', Claudia
  Casadio, Bob Coecke, Michael Moortgat, and Philip Scott (editors)</comments><msc-class>18B20, 68Q10</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography is a theory of secret functions. Category theory is a general
theory of functions. Cryptography has reached a stage where its structures
often take several pages to define, and its formulas sometimes run from page to
page. Category theory has some complicated definitions as well, but one of its
specialties is taming the flood of structure. Cryptography seems to be in need
of high level methods, whereas category theory always needs concrete
applications. So why is there no categorical cryptography? One reason may be
that the foundations of modern cryptography are built from probabilistic
polynomial-time Turing machines, and category theory does not have a good
handle on such things. On the other hand, such foundational problems might be
the very reason why cryptographic constructions often resemble low level
machine programming. I present some preliminary explorations towards
categorical cryptography. It turns out that some of the main security concepts
are easily characterized through the categorical technique of *diagram
chasing*, which was first used Lambek's seminal `Lecture Notes on Rings and
Modules'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6495</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6495</id><created>2014-01-24</created><updated>2015-06-09</updated><authors><author><keyname>Jeng</keyname><forenames>Wei</forenames></author><author><keyname>He</keyname><forenames>Daqing</forenames></author><author><keyname>Jiang</keyname><forenames>Jiepu</forenames></author></authors><title>User Participation in an Academic Social Networking Service: A Survey of
  Open Group Users on Mendeley</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Journal of the Association for Information Science and Technology.
  66(2015), 890-904</journal-ref><doi>10.1002/asi.23225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although there are a number of social networking services that specifically
target scholars, little has been published about the actual practices and the
usage of these so-called academic social networking services (ASNSs). To fill
this gap, we explore the populations of academics who engage in social
activities using an ASNS; as an indicator of further engagement, we also
determine their various motivations for joining a group in ASNSs. Using groups
and their members in Mendeley as the platform for our case study, we obtained
146 participant responses from our online survey about users' common
activities, usage habits, and motivations for joining groups. Our results show
that 1) participants did not engage with social-based features as frequently
and actively as they engaged with research-based features, and 2) users who
joined more groups seemed to have a stronger motivation to increase their
professional visibility and to contribute the research articles they had read
to the group reading list. Our results generate interesting insights into
Mendeley's user populations, their activities, and their motivations relative
to the social features of Mendeley. We also argue that further design of ASNSs
is needed to take greater account of disciplinary differences in scholarly
communication and to establish incentive mechanisms for encouraging user
participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6496</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6496</id><created>2014-01-24</created><authors><author><keyname>Fazeli</keyname><forenames>Arman</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Generalized Sphere Packing Bound</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kulkarni and Kiyavash recently introduced a new method to establish upper
bounds on the size of deletion-correcting codes. This method is based upon
tools from hypergraph theory. The deletion channel is represented by a
hypergraph whose edges are the deletion balls (or spheres), so that a
deletion-correcting code becomes a matching in this hypergraph. Consequently, a
bound on the size of such a code can be obtained from bounds on the matching
number of a hypergraph. Classical results in hypergraph theory are then invoked
to compute an upper bound on the matching number as a solution to a
linear-programming problem.
  The method by Kulkarni and Kiyavash can be applied not only for the deletion
channel but also for other error channels. This paper studies this method in
its most general setup. First, it is shown that if the error channel is regular
and symmetric then this upper bound coincides with the sphere packing bound and
thus is called the generalized sphere packing bound. Even though this bound is
explicitly given by a linear programming problem, finding its exact value may
still be a challenging task. In order to simplify the complexity of the
problem, we present a technique based upon graph automorphisms that in many
cases reduces the number of variables and constraints in the problem. We then
apply this method on specific examples of error channels. We start with the $Z$
channel and show how to exactly find the generalized sphere packing bound for
this setup. Next studied is the non-binary limited magnitude channel both for
symmetric and asymmetric errors, where we focus on the single-error case. We
follow up on the deletion and grain-error channels and show how to improve upon
the existing upper bounds for single deletion/error. Finally, we apply this
method for projective spaces and find its generalized sphere packing bound for
the single-error case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6497</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6497</id><created>2014-01-25</created><updated>2014-10-09</updated><authors><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Bayesian CP Factorization of Incomplete Tensors with Automatic Rank
  Determination</title><categories>cs.LG cs.CV stat.ML</categories><doi>10.1109/TPAMI.2015.2392756</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful
technique for tensor completion through explicitly capturing the multilinear
latent factors. The existing CP algorithms require the tensor rank to be
manually specified, however, the determination of tensor rank remains a
challenging problem especially for CP rank. In addition, existing approaches do
not take into account uncertainty information of latent factors, as well as
missing entries. To address these issues, we formulate CP factorization using a
hierarchical probabilistic model and employ a fully Bayesian treatment by
incorporating a sparsity-inducing prior over multiple latent factors and the
appropriate hyperpriors over all hyperparameters, resulting in automatic rank
determination. To learn the model, we develop an efficient deterministic
Bayesian inference algorithm, which scales linearly with data size. Our method
is characterized as a tuning parameter-free approach, which can effectively
infer underlying multilinear factors with a low-rank constraint, while also
providing predictive distributions over missing entries. Extensive simulations
on synthetic data illustrate the intrinsic capability of our method to recover
the ground-truth of CP rank and prevent the overfitting problem, even when a
large amount of entries are missing. Moreover, the results from real-world
applications, including image inpainting and facial image synthesis,
demonstrate that our method outperforms state-of-the-art approaches for both
tensor factorization and tensor completion in terms of predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6498</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6498</id><created>2014-01-25</created><updated>2014-04-27</updated><authors><author><keyname>Noorzad</keyname><forenames>Parham</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author></authors><title>On the Power of Cooperation: Can a Little Help a Lot? (Extended Version)</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper submitted to ISIT 2014 (10 pages, 3
  figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new cooperation model for discrete memoryless
multiple access channels. Unlike in prior cooperation models (e.g.,
conferencing encoders), where the transmitters cooperate directly, in this
model the transmitters cooperate through a larger network. We show that under
this indirect cooperation model, there exist channels for which the increase in
sum-capacity resulting from cooperation is significantly larger than the rate
shared by the transmitters to establish the cooperation. This result contrasts
both with results on the benefit of cooperation under prior models and results
in the network coding literature, where attempts to find examples in which
similar small network modifications yield large capacity benefits have to date
been unsuccessful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6499</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6499</id><created>2014-01-25</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Transmitter Optimization in MISO Broadcast Channel with Common and
  Secret Messages</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider transmitter optimization in multiple-input
single-output (MISO) broadcast channel with common and secret messages. The
secret message is intended for $K$ users and it is transmitted with perfect
secrecy with respect to $J$ eavesdroppers which are also assumed to be
legitimate users in the network. The common message is transmitted at a fixed
rate $R_{0}$ and it is intended for all $K$ users and $J$ eavesdroppers. The
source operates under a total power constraint. It also injects artificial
noise to improve the secrecy rate. We obtain the optimum covariance matrices
associated with the common message, secret message, and artificial noise, which
maximize the achievable secrecy rate and simultaneously meet the fixed rate
$R_{0}$ for the common message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6500</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6500</id><created>2014-01-25</created><updated>2014-02-06</updated><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author></authors><title>Holographic Transformation for Quantum Factor Graphs</title><categories>cs.IT cond-mat.stat-mech math.IT quant-ph</categories><comments>3 pages, revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a general tool called a holographic transformation, which
transforms an expression of the partition function to another form, has been
used for polynomial-time algorithms and for improvement and understanding of
the belief propagation. In this work, the holographic transformation is
generalized to quantum factor graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6508</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6508</id><created>2014-01-25</created><authors><author><keyname>Elkhatib</keyname><forenames>Yehia</forenames></author><author><keyname>Tyson</keyname><forenames>Gareth</forenames></author><author><keyname>Welzl</keyname><forenames>Michael</forenames></author></authors><title>The Effect of Network and Infrastructural Variables on SPDY's
  Performance</title><categories>cs.NI</categories><report-no>SCC-2013-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HTTP is a successful Internet technology on top of which a lot of the web
resides. However, limitations with its current specification, i.e. HTTP/1.1,
have encouraged some to look for the next generation of HTTP. In SPDY, Google
has come up with such a proposal that has growing community acceptance,
especially after being adopted by the IETF HTTPbis-WG as the basis for
HTTP/2.0. SPDY has the potential to greatly improve web experience with little
deployment overhead. However, we still lack an understanding of its true
potential in different environments. This paper seeks to resolve these issues,
offering a comprehensive evaluation of SPDY's performance using extensive
experiments. We identify the impact of network characteristics and website
infrastructure on SPDY's potential page loading benefits, finding that these
factors are decisive for SPDY and its optimal deployment strategy. Through
this, we feed into the wider debate regarding HTTP/2.0, exploring the key
aspects that impact the performance of this future protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6512</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6512</id><created>2014-01-25</created><authors><author><keyname>Karzand</keyname><forenames>Mina</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Achievable Degrees of Freedom in MIMO Correlatively Changing Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>submitted for ISIT 2014. arXiv admin note: substantial text overlap
  with arXiv:1401.2169</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between the transmitted signal and the noiseless received
signals in correlatively changing fading channels is modeled as a nonlinear
mapping over manifolds of different dimensions. Dimension counting argument
claims that the dimensionality of the neighborhood in which this mapping is
bijective with probability one is achievable as the degrees of freedom of the
system.We call the degrees of freedom achieved by the nonlinear decoding
methods the nonlinear degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6517</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6517</id><created>2014-01-25</created><authors><author><keyname>Chen</keyname><forenames>Jiangcheng</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Zhu</keyname><forenames>Lei</forenames></author></authors><title>Kinematics analysis and three-dimensional simulation of the
  rehabilitation lower extremity exoskeleton robot</title><categories>cs.RO</categories><comments>The international conference on Soft Computing and Software
  Engineering</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The kinematics recursive equation was built by using the modified D-H method
after the structure of rehabilitation lower extremity exoskeleton was analyzed.
The numerical algorithm of inverse kinematics was given too. Then the
three-dimensional simulation model of the exoskeleton robot was built using
MATLAB software, based on the model, 3D reappearance of a complete gait was
achieved. Finally, the reliability of numerical algorithm of inverse kinematics
was verified by the simulation result. All jobs above lay a foundation for
developing a three-dimensional simulation platform of exoskeleton robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6520</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6520</id><created>2014-01-25</created><updated>2015-11-09</updated><authors><author><keyname>Cui</keyname><forenames>Peng</forenames></author></authors><title>Approximation Resistance by Disguising Biased Distributions</title><categories>cs.CC</categories><comments>6 pages, short note</comments><acm-class>F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, the author shows that the gap problem of some 3-XOR is
NP-hard and can be solved by running Charikar\&amp;Wirth's SDP algorithm for two
rounds. To conclude, the author proves that $P=NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6523</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6523</id><created>2014-01-25</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Mattei</keyname><forenames>Nick</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Strategic aspects of the probabilistic serial rule for the allocation of
  goods</title><categories>cs.GT cs.DS</categories><comments>28 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic serial (PS) rule is one of the most prominent randomized
rules for the assignment problem. It is well-known for its superior fairness
and welfare properties. However, PS is not immune to manipulative behaviour by
the agents. We examine computational and non-computational aspects of
strategising under the PS rule. Firstly, we study the computational complexity
of an agent manipulating the PS rule. We present polynomial-time algorithms for
optimal manipulation. Secondly, we show that expected utility best responses
can cycle. Thirdly, we examine the existence and computation of Nash
equilibrium profiles under the PS rule. We show that a pure Nash equilibrium is
guaranteed to exist under the PS rule. For two agents, we identify two
different types of preference profiles that are not only in Nash equilibrium
but can also be computed in linear time. Finally, we conduct experiments to
check the frequency of manipulability of the PS rule under different
combinations of the number of agents, objects, and utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6528</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6528</id><created>2014-01-25</created><updated>2015-06-27</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Sandon</keyname><forenames>Colin</forenames></author></authors><title>Linear Boolean classification, coding and &quot;the critical problem&quot;</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of constructing a minimal rank matrix over GF(2) whose kernel
does not intersect a given set S is considered. In the case where S is a
Hamming ball centered at 0, this is equivalent to finding linear codes of
largest dimension. For a general set, this is an instance of &quot;the critical
problem&quot; posed by Crapo and Rota in 1970. This work focuses on the case where S
is an annulus. As opposed to balls, it is shown that an optimal kernel is
composed not only of dense but also of sparse vectors, and the optimal mixture
is identified in various cases. These findings corroborate a proposed
conjecture that for annulus of inner and outer radius nq and np respectively,
the optimal relative rank is given by (1-q)H(p/(1-q)), an extension of the
Gilbert-Varshamov bound H(p) conjectured for Hamming balls of radius np.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6533</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6533</id><created>2014-01-25</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Cong</keyname><forenames>Shuang</forenames></author></authors><title>A Robust Compressive Quantum State Tomography Algorithm Using ADMM</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possible state space dimension increases exponentially with respect to
the number of qubits. This feature makes the quantum state tomography expensive
and impractical for identifying the state of merely several qubits. The recent
developed approach, compressed sensing, gives us an alternative to estimate the
quantum state with fewer measurements. It is proved that the estimation then
can be converted to a convex optimization problem with quantum mechanics
constraints. In this paper we present an alternating augmented Lagrangian
method for quantum convex optimization problem aiming for recovering pure or
near pure quantum states corrupted by sparse noise given observables and the
expectation values of the measurements. The proposed algorithm is much faster,
robust to outlier noises (even very large for some entries) and can solve the
reconstruction problem distributively. The simulations verify the superiority
of the proposed algorithm and compare it to the conventional least square and
compressive quantum tomography using Dantzig method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6536</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6536</id><created>2014-01-25</created><updated>2014-11-08</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Mike</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author><author><keyname>Rao</keyname><forenames>Micha&#xeb;l</forenames></author></authors><title>Infinite square-free self-shuffling words</title><categories>cs.DM cs.FL math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we answer two recent questions from Charlier et al. and Harju
about self-shuffling words. An infinite word $w$ is called self-shuffling, if
$w=\prod_{i=0}^\infty U_iV_i=\prod_{i=0}^\infty U_i=\prod_{i=0}^\infty V_i$ for
some finite words $U_i$, $V_i$. Harju recently asked whether square-free
self-shuffling words exist. We answer this question affirmatively. Besides
that, we build an infinite word such that no word in its shift orbit closure is
self-shuffling, answering positively a question from Charlier et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6541</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6541</id><created>2014-01-25</created><updated>2015-08-23</updated><authors><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Network Synchronization with Nonlinear Dynamics and Switching
  Interactions</title><categories>cs.SY</categories><comments>27 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the synchronization problem for networks of coupled
nonlinear dynamical systems under switching communication topologies. Two types
of nonlinear agent dynamics are considered. The first one is non-expansive
dynamics (stable dynamics with a convex Lyapunov function $\varphi(\cdot)$) and
the second one is dynamics that satisfies a global Lipschitz condition. For the
non-expansive case, we show that various forms of joint connectivity for
communication graphs are sufficient for networks to achieve global asymptotic
$\varphi$-synchronization. We also show that $\varphi$-synchronization leads to
state synchronization provided that certain additional conditions are
satisfied. For the globally Lipschitz case, unlike the non-expansive case,
joint connectivity alone is not sufficient for achieving synchronization. A
sufficient condition for reaching global exponential synchronization is
established in terms of the relationship between the global Lipschitz constant
and the network parameters. We also extend the results to leader-follower
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6543</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6543</id><created>2014-01-25</created><authors><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Naresh</keyname><forenames>Yalagala</forenames></author><author><keyname>Datta</keyname><forenames>Tanumay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Pseudo-random Phase Precoded Spatial Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial modulation (SM) is a transmission scheme that uses multiple transmit
antennas but only one transmit RF chain. At each time instant, only one among
the transmit antennas will be active and the others remain silent. The index of
the active transmit antenna will also convey information bits in addition to
the information bits conveyed through modulation symbols (e.g.,QAM).
Pseudo-random phase precoding (PRPP) is a technique that can achieve high
diversity orders even in single antenna systems without the need for channel
state information at the transmitter (CSIT) and transmit power control (TPC).
In this paper, we exploit the advantages of both SM and PRPP simultaneously. We
propose a pseudo-random phase precoded SM (PRPP-SM) scheme, where both the
modulation bits and the antenna index bits are precoded by pseudo-random
phases. The proposed PRPP-SM system gives significant performance gains over SM
system without PRPP and PRPP system without SM. Since maximum likelihood (ML)
detection becomes exponentially complex in large dimensions, we propose low
complexity local search based detection (LSD) algorithm suited for PRPP-SM
systems with large precoder sizes. Our simulation results show that with 4
transmit antennas, 1 receive antenna, $5\times 20$ pseudo-random phase precoder
matrix and BPSK modulation, the performance of PRPP-SM using ML detection is
better than SM without PRPP with ML detection by about 9 dB at $10^{-2}$ BER.
This performance advantage gets even better for large precoding sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6567</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6567</id><created>2014-01-25</created><authors><author><keyname>Gayen</keyname><forenames>Vivekananda</forenames></author><author><keyname>Sarkar</keyname><forenames>Kamal</forenames></author></authors><title>A Machine Learning Approach for the Identification of Bengali Noun-Noun
  Compound Multiword Expressions</title><categories>cs.CL cs.LG</categories><journal-ref>In Proceedings of ICON-2013: 10th International Conference on
  Natural Language Processing, pp 290-296</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a machine learning approach for identification of Bengali
multiword expressions (MWE) which are bigram nominal compounds. Our proposed
approach has two steps: (1) candidate extraction using chunk information and
various heuristic rules and (2) training the machine learning algorithm called
Random Forest to classify the candidates into two groups: bigram nominal
compound MWE or not bigram nominal compound MWE. A variety of association
measures, syntactic and linguistic clues and a set of WordNet-based similarity
features have been used for our MWE identification task. The approach presented
in this paper can be used to identify bigram nominal compound MWE in Bengali
running text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6571</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6571</id><created>2014-01-25</created><authors><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author><author><keyname>Choudhury</keyname><forenames>Sagnik Ray</forenames></author><author><keyname>Caragea</keyname><forenames>Cornelia</forenames></author></authors><title>Keyword and Keyphrase Extraction Using Centrality Measures on
  Collocation Networks</title><categories>cs.CL cs.IR</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword and keyphrase extraction is an important problem in natural language
processing, with applications ranging from summarization to semantic search to
document clustering. Graph-based approaches to keyword and keyphrase extraction
avoid the problem of acquiring a large in-domain training corpus by applying
variants of PageRank algorithm on a network of words. Although graph-based
approaches are knowledge-lean and easily adoptable in online systems, it
remains largely open whether they can benefit from centrality measures other
than PageRank. In this paper, we experiment with an array of centrality
measures on word and noun phrase collocation networks, and analyze their
performance on four benchmark datasets. Not only are there centrality measures
that perform as well as or better than PageRank, but they are much simpler
(e.g., degree, strength, and neighborhood size). Furthermore, centrality-based
methods give results that are competitive with and, in some cases, better than
two strong unsupervised baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6573</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6573</id><created>2014-01-25</created><authors><author><keyname>Real-Coelho</keyname><forenames>Livy-Maria</forenames><affiliation>LaBRI, UFPR</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, IRIT</affiliation></author></authors><title>Deverbal semantics and the Montagovian generative lexicon</title><categories>cs.CL cs.LO</categories><comments>A revised version will appear in the Journal of Logic, Language and
  Information</comments><proxy>ccsd</proxy><journal-ref>Journal of Logic, Language and Information (2014) 21</journal-ref><doi>10.1007/s10849-014-9187-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a lexical account of action nominals, in particular of deverbal
nominalisations, whose meaning is related to the event expressed by their base
verb. The literature about nominalisations often assumes that the semantics of
the base verb completely defines the structure of action nominals. We argue
that the information in the base verb is not sufficient to completely determine
the semantics of action nominals. We exhibit some data from different
languages, especially from Romance language, which show that nominalisations
focus on some aspects of the verb semantics. The selected aspects, however,
seem to be idiosyncratic and do not automatically result from the internal
structure of the verb nor from its interaction with the morphological suffix.
We therefore propose a partially lexicalist approach view of deverbal nouns. It
is made precise and computable by using the Montagovian Generative Lexicon, a
type theoretical framework introduced by Bassac, Mery and Retor\'e in this
journal in 2010. This extension of Montague semantics with a richer type system
easily incorporates lexical phenomena like the semantics of action nominals in
particular deverbals, including their polysemy and (in)felicitous
copredications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6574</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6574</id><created>2014-01-25</created><authors><author><keyname>Gillibert</keyname><forenames>Jean</forenames><affiliation>IMB</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI</affiliation></author></authors><title>Category theory, logic and formal linguistics: some connections, old and
  new</title><categories>math.CT cs.CL cs.LO math.LO</categories><comments>Survey on the occasion of a special issue of the journal of applied
  logic</comments><proxy>ccsd</proxy><journal-ref>Journal of Applied Logic 12, 1 (2014) 1--13</journal-ref><doi>10.1016/j.jal.2014.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seize the opportunity of the publication of selected papers from the
\emph{Logic, categories, semantics} workshop in the \emph{Journal of Applied
Logic} to survey some current trends in logic, namely intuitionistic and linear
type theories, that interweave categorical, geometrical and computational
considerations. We thereafter present how these rich logical frameworks can
model the way language conveys meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6575</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6575</id><created>2014-01-25</created><updated>2015-10-08</updated><authors><author><keyname>Gimbert</keyname><forenames>Hugo</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Kelmendi</keyname><forenames>Edon</forenames><affiliation>LaBRI</affiliation></author></authors><title>Two-Player Perfect-Information Shift-Invariant Submixing Stochastic
  Games Are Half-Positional</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider zero-sum stochastic games with perfect information and finitely
many states and actions. The payoff is computed by a payoff function which
associates to each infinite sequence of states and actions a real number. We
prove that if the the payoff function is both shift-invariant and submixing,
then the game is half-positional, i.e. the first player has an optimal strategy
which is both deterministic and stationary. This result relies on the existence
of $\epsilon$-subgame-perfect equilibria in shift-invariant games, a second
contribution of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6576</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6576</id><created>2014-01-25</created><updated>2015-11-13</updated><authors><author><keyname>Dartois</keyname><forenames>Luc</forenames><affiliation>ULB</affiliation></author><author><keyname>Paperman</keyname><forenames>Charles</forenames></author></authors><title>Adding modular predicates to first-order fragments</title><categories>cs.LO cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the decidability of the definability problem for fragments of
first order logic over finite words enriched with modular predicates. Our
approach aims toward the most generic statements that we could achieve, which
successfully covers the quantifier alternation hierarchy of first order logic
and some of its fragments. We obtain that deciding this problem for each level
of the alternation hierarchy of both first order logic and its two-variable
fragment when equipped with all regular numerical predicates is not harder than
deciding it for the corresponding level equipped with only the linear order and
the successor. For two-variable fragments we also treat the case of the
signature containing only the order and modular predicates.Relying on some
recent results, this proves the decidability for each level of the alternation
hierarchy of the two-variable first order fragmentwhile in the case of the
first order logic the question remains open for levels greater than two.The
main ingredients of the proofs are syntactic transformations of first order
formulas as well as the algebraic framework of finite categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6578</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6578</id><created>2014-01-25</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Simple Error Bounds for Regularized Noisy Linear Inverse Problems</title><categories>math.OC cs.IT math.IT math.ST stat.TH</categories><comments>6pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider estimating a structured signal $\mathbf{x}_0$ from linear,
underdetermined and noisy measurements
$\mathbf{y}=\mathbf{A}\mathbf{x}_0+\mathbf{z}$, via solving a variant of the
lasso algorithm: $\hat{\mathbf{x}}=\arg\min_\mathbf{x}\{
\|\mathbf{y}-\mathbf{A}\mathbf{x}\|_2+\lambda f(\mathbf{x})\}$. Here, $f$ is a
convex function aiming to promote the structure of $\mathbf{x}_0$, say
$\ell_1$-norm to promote sparsity or nuclear norm to promote low-rankness. We
assume that the entries of $\mathbf{A}$ are independent and normally
distributed and make no assumptions on the noise vector $\mathbf{z}$, other
than it being independent of $\mathbf{A}$. Under this generic setup, we derive
a general, non-asymptotic and rather tight upper bound on the $\ell_2$-norm of
the estimation error $\|\hat{\mathbf{x}}-\mathbf{x}_0\|_2$. Our bound is
geometric in nature and obeys a simple formula; the roles of $\lambda$, $f$ and
$\mathbf{x}_0$ are all captured by a single summary parameter
$\delta(\lambda\partial((f(\mathbf{x}_0)))$, termed the Gaussian squared
distance to the scaled subdifferential. We connect our result to the literature
and verify its validity through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6580</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6580</id><created>2014-01-25</created><updated>2015-05-12</updated><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>A Multicast Approach for Constructive Interference Precoding in MISO
  Downlink Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. Accepted for a publication in the proceedings of
  ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the concept of jointly utilizing the data
information(DI)and channel state information (CSI) in order to design
symbol-level precoders for a multiple input and single output (MISO) downlink
channel. In this direction, the interference among the simultaneous data
streams is transformed to useful signal that can improve the signal to
interference noise ratio (SINR) of the downlink transmissions. We propose a
maximum ratio transmissions (MRT) based algorithm that jointly exploits DI and
CSI to gain the benefits from these useful signals. In this context, a novel
framework to minimize the power consumption is proposed by formalizing the
duality between the constructive interference downlink channel and the
multicast channels. The numerical results have shown that the proposed schemes
outperform other state of the art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6594</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6594</id><created>2014-01-25</created><authors><author><keyname>Balajewicz</keyname><forenames>Maciej</forenames></author><author><keyname>Farhat</keyname><forenames>Charbel</forenames></author></authors><title>Reduction of Nonlinear Embedded Boundary Models for Problems with
  Evolving Interfaces</title><categories>physics.comp-ph cs.NA math.NA</categories><journal-ref>J. Comput. Phys. (2014), vol. 274, pp. 489-504</journal-ref><doi>10.1016/j.jcp.2014.06.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded boundary methods alleviate many computational challenges, including
those associated with meshing complex geometries and solving problems with
evolving domains and interfaces. Developing model reduction methods for
computational frameworks based on such methods seems however to be challenging.
Indeed, most popular model reduction techniques are projection-based, and rely
on basis functions obtained from the compression of simulation snapshots. In a
traditional interface-fitted computational framework, the computation of such
basis functions is straightforward, primarily because the computational domain
does not contain in this case a fictitious region. This is not the case however
for an embedded computational framework because the computational domain
typically contains in this case both real and ghost regions whose definitions
complicate the collection and compression of simulation snapshots. The problem
is exacerbated when the interface separating both regions evolves in time. This
paper addresses this issue by formulating the snapshot compression problem as a
weighted low-rank approximation problem where the binary weighting identifies
the evolving component of the individual simulation snapshots. The proposed
approach is application independent and therefore comprehensive. It is
successfully demonstrated for the model reduction of several two-dimensional,
vortex-dominated, fluid-structure interaction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6596</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6596</id><created>2014-01-25</created><authors><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author><author><keyname>Altun</keyname><forenames>Oguz</forenames></author><author><keyname>Ayan</keyname><forenames>U&#x11f;ur</forenames></author><author><keyname>Mert</keyname><forenames>Cihan</forenames></author></authors><title>A Novel String Distance Function based on Most Frequent K Characters</title><categories>cs.DS cs.IR</categories><journal-ref>International Journal of Machine Learning and Computation (IJMLC),
  Issn : 2010-3700, vol.4, is.2, pp.177-183, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study aims to publish a novel similarity metric to increase the speed of
comparison operations. Also the new metric is suitable for distance-based
operations among strings. Most of the simple calculation methods, such as
string length are fast to calculate but does not represent the string
correctly. On the other hand the methods like keeping the histogram over all
characters in the string are slower but good to represent the string
characteristics in some areas, like natural language. We propose a new metric,
easy to calculate and satisfactory for string comparison. Method is built on a
hash function, which gets a string at any size and outputs the most frequent K
characters with their frequencies. The outputs are open for comparison and our
studies showed that the success rate is quite satisfactory for the text mining
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6597</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6597</id><created>2014-01-25</created><authors><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author><author><keyname>Unal</keyname><forenames>Y.</forenames></author><author><keyname>Erdem</keyname><forenames>Z.</forenames></author><author><keyname>Kocer</keyname><forenames>H. Erdinc</forenames></author></authors><title>Ensembled Correlation Between Liver Analysis Outputs</title><categories>stat.ML cs.CE cs.LG</categories><journal-ref>International Journal of Biology and Biomedical Engineering, ISSN:
  1998-4510, Volume 8, pp. 1-5, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining techniques on the biological analysis are spreading for most of
the areas including the health care and medical information. We have applied
the data mining techniques, such as KNN, SVM, MLP or decision trees over a
unique dataset, which is collected from 16,380 analysis results for a year.
Furthermore we have also used meta-classifiers to question the increased
correlation rate between the liver disorder and the liver analysis outputs. The
results show that there is a correlation among ALT, AST, Billirubin Direct and
Billirubin Total down to 15% of error rate. Also the correlation coefficient is
up to 94%. This makes possible to predict the analysis results from each other
or disease patterns can be applied over the linear correlation of the
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6598</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6598</id><created>2014-01-25</created><authors><author><keyname>Ochoa-Zezzatti</keyname><forenames>Alberto</forenames></author><author><keyname>Hatsukimi</keyname><forenames>Luana</forenames></author><author><keyname>Karuda</keyname><forenames>Hitomi</forenames></author><author><keyname>Arreola</keyname><forenames>Julio</forenames></author><author><keyname>Bustillos</keyname><forenames>Sandra</forenames></author></authors><title>Never forget, whom was my ancestors: A cross-cultural analysis from
  Yonsei (fourth-generation Nikkei) in four societies using Data Mining</title><categories>cs.SI</categories><comments>7 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explains the importance of transculturality in social
networking in a wide variety of activities of our daily life. We focus our
analysis to online activities that use social richness, analyzing societies in
Yakutia (A Russian Republic), Macau in China, Uberl\^andia in Brazil and Juarez
City in Mexico, all with people descending from Japanese people. To this end,
we performed surveys to gathering information about salient aspects of upgrade
and combined them using social data mining techniques to profile a number of
behavioural patterns and choices that describe social networking behaviours in
these societies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6604</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6604</id><created>2014-01-25</created><authors><author><keyname>Wu</keyname><forenames>Baofeng</forenames></author><author><keyname>Jin</keyname><forenames>Qingfang</forenames></author><author><keyname>Liu</keyname><forenames>Zhuojun</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>Constructing Boolean Functions With Potential Optimal Algebraic Immunity
  Based on Additive Decompositions of Finite Fields</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general approach to construct cryptographic significant Boolean
functions of $(r+1)m$ variables based on the additive decomposition
$\mathbb{F}_{2^{rm}}\times\mathbb{F}_{2^m}$ of the finite field
$\mathbb{F}_{2^{(r+1)m}}$, where $r$ is odd and $m\geq3$. A class of unbalanced
functions are constructed first via this approach, which coincides with a
variant of the unbalanced class of generalized Tu-Deng functions in the case
$r=1$. This class of functions have high algebraic degree, but their algebraic
immunity does not exceeds $m$, which is impossible to be optimal when $r&gt;1$. By
modifying these unbalanced functions, we obtain a class of balanced functions
which have optimal algebraic degree and high nonlinearity (shown by a lower
bound we prove). These functions have optimal algebraic immunity provided a
combinatorial conjecture on binary strings which generalizes the Tu-Deng
conjecture is true. Computer investigations show that, at least for small
values of number of variables, functions from this class also behave well
against fast algebraic attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6606</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6606</id><created>2014-01-25</created><updated>2015-03-23</updated><authors><author><keyname>Lisanti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Masi</keyname><forenames>Iacopo</forenames></author><author><keyname>Pernici</keyname><forenames>Federico</forenames></author><author><keyname>Del Bimbo</keyname><forenames>Alberto</forenames></author></authors><title>Continuous Localization and Mapping of a Pan Tilt Zoom Camera for Wide
  Area Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pan-tilt-zoom (PTZ) cameras are powerful to support object identification and
recognition in far-field scenes. However, the effective use of PTZ cameras in
real contexts is complicated by the fact that a continuous on-line camera
calibration is needed and the absolute pan, tilt and zoom positional values
provided by the camera actuators cannot be used because are not synchronized
with the video stream. So, accurate calibration must be directly extracted from
the visual content of the frames. Moreover, the large and abrupt scale changes,
the scene background changes due to the camera operation and the need of camera
motion compensation make target tracking with these cameras extremely
challenging. In this paper, we present a solution that provides continuous
on-line calibration of PTZ cameras which is robust to rapid camera motion,
changes of the environment due to illumination or moving objects and scales
beyond thousands of landmarks. The method directly derives the relationship
between the position of a target in the 3D world plane and the corresponding
scale and position in the 2D image, and allows real-time tracking of multiple
targets with high and stable degree of accuracy even at far distances and any
zooming level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6615</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6615</id><created>2014-01-26</created><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Iterative Approximate Consensus in the presence of Byzantine Link
  Failures</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1202.6094</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the problem of reaching approximate consensus in
synchronous point-to-point networks, where each directed link of the underlying
communication graph represents a communication channel between a pair of nodes.
We adopt the transient Byzantine link failure model [15, 16], where an
omniscient adversary controls a subset of the directed communication links, but
the nodes are assumed to be fault-free.
  Recent work has addressed the problem of reaching approximate consen- sus in
incomplete graphs with Byzantine nodes using a restricted class of iterative
algorithms that maintain only a small amount of memory across iterations [22,
21, 23, 12]. However, to the best of our knowledge, we are the fi?rst to
consider approximate consensus in the presence of Byzan- tine links. We extend
our past work that provided exact characterization of graphs in which the
iterative approximate consensus problem in the presence of Byzantine node
failures is solvable [22, 21]. In particular, we prove a tight necessary and
sufficient condition on the underlying com- munication graph for the existence
of iterative approximate consensus algorithms under transient Byzantine link
model. The condition answers (part of) the open problem stated in [16].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6621</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6621</id><created>2014-01-26</created><authors><author><keyname>Altman</keyname><forenames>Zwi</forenames><affiliation>CEA</affiliation></author><author><keyname>Sallem</keyname><forenames>Soumaya</forenames><affiliation>CEA</affiliation></author><author><keyname>Nasri</keyname><forenames>Ridha</forenames></author><author><keyname>Sayrac</keyname><forenames>Berna</forenames></author><author><keyname>Clerc</keyname><forenames>Maurice</forenames></author></authors><title>Particle Swarm Optimization for Mobility Load Balancing SON in LTE
  Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a self-optimizing solution for Mobility Load Balancing
(MLB). The MLB-SON is performed in two phases. In the first, a MLB controller
is designed using Multi-Objective Particle Swarm Optimization (MO-PSO) which
incorporates a priori expert knowledge to considerably reduce the search space
and optimization time. The dynamicity of the optimization phase is addressed.
In the second phase, the controller is pushed into the base stations to
implement the MLB SON. The method is applied to dynamically adapt Handover
Margin parameters of a large scale LTE network in order to balance traffic of
the network eNodeBs. Numerical results illustrate the benefits of the proposed
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6626</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6626</id><created>2014-01-26</created><updated>2014-04-03</updated><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffour</keyname><forenames>Tareq Y.</forenames></author></authors><title>Completion Time Reduction in Instantly Decodable Network Coding Through
  Decoding Delay Control</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several years, the completion time and decoding delay problems in
Instantly Decodable Network Coding (IDNC) were considered separately and were
thought to completely act against each other. Recently, some works aimed to
balance the effects of these two important IDNC metrics but none of them
studied a further optimization of one by controlling the other. In this paper,
we study the effect of controlling the decoding delay to reduce the completion
time below its currently best known solution. We first derive the
decoding-delay-dependent expressions of the users' and overall completion
times. Although using such expressions to find the optimal overall completion
time is NP-hard, we design a novel heuristic that minimizes the probability of
increasing the maximum of these decoding-delay-dependent completion time
expressions after each transmission through a layered control of their decoding
delays. Simulation results show that this new algorithm achieves both a lower
mean completion time and mean decoding delay compared to the best known
heuristic for completion time reduction. The gap in performance becomes
significant for harsh erasure scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6628</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6628</id><created>2014-01-26</created><authors><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Weng</keyname><forenames>Chuliang</forenames></author><author><keyname>Nambiar</keyname><forenames>Raghunath</forenames></author><author><keyname>Zhang</keyname><forenames>Jinchao</forenames></author><author><keyname>Chen</keyname><forenames>Xingzhen</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author></authors><title>BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking
  Framework</title><categories>cs.DC cs.DB cs.PF</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data is considered proprietary asset of companies, organizations, and
even nations. Turning big data into real treasure requires the support of big
data systems. A variety of commercial and open source products have been
unleashed for big data storage and processing. While big data users are facing
the choice of which system best suits their needs, big data system developers
are facing the question of how to evaluate their systems with regard to general
big data processing needs. System benchmarking is the classic way of meeting
the above demands. However, existent big data benchmarks either fail to
represent the variety of big data processing requirements, or target only one
specific platform, e.g. Hadoop.
  In this paper, with our industrial partners, we present BigOP, an end-to-end
system benchmarking framework, featuring the abstraction of representative
Operation sets, workload Patterns, and prescribed tests. BigOP is part of an
open-source big data benchmarking project, BigDataBench (available at
http://prof.ict.ac.cn/BigDataBench). BigOP's abstraction model not only guides
the development of BigDataBench, but also enables automatic generation of tests
with comprehensive workloads.
  We illustrate the feasibility of BigOP by implementing an automatic test
generation tool and benchmarking against three widely used big data processing
systems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three
different application scenarios are prescribed. The tests involve relational
data, text data and graph data, as well as all operations and workload
patterns. We report results following test specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6633</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6633</id><created>2014-01-26</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>Payoff Allocation of Service Coalition in Wireless Mesh Network: A
  Cooperative Game Perspective</title><categories>cs.GT cs.NI</categories><comments>IEEE GlobeCom. 6 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless mesh network (WMN), multiple service providers (SPs) can
cooperate to share resources (e.g., relay nodes and spectrum), to serve their
collective subscribed customers for better service. As a reward, SPs are able
to achieve more individual benefits, i.e., increased revenue or decreased cost,
through efficient utilization of shared network resources. However, this
cooperation can be realized only if fair allocation of aggregated payoff, which
is the sum of the payoff of all the cooperative SPs, can be achieved. We first
formulate such cooperation as a coalitional game with transferable utility,
specifically, a linear programming game, in which, each SP should obtain the
fair share of the aggregated payoff. Then we study the problem of allocating
aggregated payoff which leads to stable service coalition of SPs in WMN based
on the concepts of dual payoff and Shapley value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6634</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6634</id><created>2014-01-26</created><authors><author><keyname>Jitman</keyname><forenames>Somphong</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Sangwisut</keyname><forenames>Ekkasit</forenames></author></authors><title>Hermitian Self-Dual Cyclic Codes of Length $p^a$ over $GR(p^2,s)$</title><categories>math.RA cs.IT math.IT</categories><comments>18 pages. Submitted to Advances in Mathematics of Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study cyclic codes over the Galois ring ${\rm
GR}({p^2},s)$. The main result is the characterization and enumeration of
Hermitian self-dual cyclic codes of length $p^a$ over ${\rm GR}({p^2},s)$.
Combining with some known results and the standard Discrete Fourier Transform
decomposition, we arrive at the characterization and enumeration of Euclidean
self-dual cyclic codes of any length over ${\rm GR}({p^2},s)$. Some corrections
to results on Euclidean self-dual cyclic codes of even length over
$\mathbb{Z}_4$ in Discrete Appl. Math. 128, (2003), 27 and Des. Codes Cryptogr.
39, (2006), 127 are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6637</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6637</id><created>2014-01-26</created><updated>2015-04-20</updated><authors><author><keyname>Avigdor-Elgrabli</keyname><forenames>Noa</forenames></author><author><keyname>Rabani</keyname><forenames>Yuval</forenames></author><author><keyname>Yadgar</keyname><forenames>Gala</forenames></author></authors><title>Convergence of T\^atonnement in Fisher Markets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing simple and natural price-adjustment processes that converge to a
market equilibrium is a fundamental question in economics. Such an analysis may
have implications in economic theory, computational economics, and distributed
systems. T\^atonnement, proposed by Walras in 1874, is a process by which
prices go up in response to excess demand, and down in response to excess
supply. This paper analyzes the convergence of a time-discrete t\^atonnement
process, a problem that recently attracted considerable attention of computer
scientists. We prove that the simple t\^atonnement process that we consider
converges (efficiently) to equilibrium prices and allocation in markets with
nested CES-Leontief utilities, generalizing some of the previous convergence
proofs for more restricted types of utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6638</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6638</id><created>2014-01-26</created><authors><author><keyname>Wu</keyname><forenames>Tong</forenames></author><author><keyname>Polatkan</keyname><forenames>Gungor</forenames></author><author><keyname>Steel</keyname><forenames>David</forenames></author><author><keyname>Brown</keyname><forenames>William</forenames></author><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Painting Analysis Using Wavelets and Probabilistic Topic Models</title><categories>cs.CV cs.LG stat.ML</categories><comments>5 pages, 4 figures, ICIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, computer-based techniques for stylistic analysis of paintings
are applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto
di Bondone. Features are extracted by combining a dual-tree complex wavelet
transform with a hidden Markov tree (HMT) model. Hierarchical clustering is
used to identify stylistic keywords in image patches, and keyword frequencies
are calculated for sub-images that each contains many patches. A generative
hierarchical Bayesian model learns stylistic patterns of keywords; these
patterns are then used to characterize the styles of the sub-images; this in
turn, permits to discriminate between paintings. Results suggest that such
unsupervised probabilistic topic models can be useful to distill characteristic
elements of style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6642</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6642</id><created>2014-01-26</created><updated>2015-03-16</updated><authors><author><keyname>Ghavami</keyname><forenames>Siavash</forenames></author><author><keyname>Rahmati</keyname><forenames>Vahid</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author><author><keyname>Schwabe</keyname><forenames>Lars</forenames></author></authors><title>Synchrony in Neuronal Communications: An Energy Efficient Scheme</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, Accepted for publication to IWCIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in understanding the neural correlates of attentional
processes using first principles. Here we apply a recently developed first
principles approach that uses transmitted information in bits per joule to
quantify the energy efficiency of information transmission for an
inter-spike-interval (ISI) code that can be modulated by means of the synchrony
in the presynaptic population. We simulate a single compartment
conductance-based model neuron driven by excitatory and inhibitory spikes from
a presynaptic population, where the rate and synchrony in the presynaptic
excitatory population may vary independently from the average rate. We find
that for a fixed input rate, the ISI distribution of the post synaptic neuron
depends on the level of synchrony and is well-described by a Gamma distribution
for synchrony levels less than 50%. For levels of synchrony between 15% and 50%
(restricted for technical reasons), we compute the optimum input distribution
that maximizes the mutual information per unit energy. This optimum
distribution shows that an increased level of synchrony, as it has been
reported experimentally in attention-demanding conditions, reduces the mode of
the input distribution and the excitability threshold of post synaptic neuron.
This facilitates a more energy efficient neuronal communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6651</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6651</id><created>2014-01-26</created><updated>2014-02-06</updated><authors><author><keyname>Tie</keyname><forenames>Lin</forenames></author></authors><title>On Near-controllability, Nearly-controllable Subspaces, and
  Near-controllability Index of a Class of Discrete-time Bilinear Systems: A
  Root Locus Approach</title><categories>cs.SY</categories><comments>22 pages, submitted to SIAM Journal on Control and Optimization Sep.
  26, 2012; revised Jul.10, 2013; accepted Dec. 19, 2013</comments><msc-class>93B05, 93C10, 93C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies near-controllability of a class of discrete-time bilinear
systems via a root locus approach. A necessary and sufficient criterion for the
systems to be nearly controllable is given. In particular, by using the root
locus approach, the control inputs which achieve the state transition for the
nearly controllable systems can be computed. Furthermore, for the non-nearly
controllable systems, nearly-controllable subspaces are derived and
near-controllability index is defined. Accordingly, the controllability
properties of such class of discrete-time bilinear systems are fully
characterized. Finally, examples are provided to demonstrate the results of the
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6667</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6667</id><created>2014-01-26</created><updated>2014-01-31</updated><authors><author><keyname>Elsheikh</keyname><forenames>Mustafa</forenames></author><author><keyname>Novocin</keyname><forenames>Andy</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author></authors><title>Ranks of Quotients, Remainders and $p$-Adic Digits of Matrices</title><categories>math.NT cs.SC</categories><comments>8 pages</comments><msc-class>15A03, 15B33, 15B36, 11C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a prime $p$ and a matrix $A \in \mathbb{Z}^{n \times n}$, write $A$ as $A
= p (A \,\mathrm{quo}\, p) + (A \,\mathrm{rem}\, p)$ where the remainder and
quotient operations are applied element-wise. Write the $p$-adic expansion of
$A$ as $A = A^{[0]} + p A^{[1]} + p^2 A^{[2]} + \cdots$ where each $A^{[i]} \in
\mathbb{Z}^{n \times n}$ has entries between $[0, p-1]$. Upper bounds are
proven for the $\mathbb{Z}$-ranks of $A \,\mathrm{rem}\, p$, and $A
\,\mathrm{quo}\, p$. Also, upper bounds are proven for the
$\mathbb{Z}/p\mathbb{Z}$-rank of $A^{[i]}$ for all $i \ge 0$ when $p = 2$, and
a conjecture is presented for odd primes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6670</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6670</id><created>2014-01-26</created><authors><author><keyname>Babarczi</keyname><forenames>Peter</forenames></author><author><keyname>Tapolcai</keyname><forenames>Janos</forenames></author><author><keyname>Ronyai</keyname><forenames>Lajos</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Resilient Flow Decomposition of Unicast Connections with Network Coding</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE International Symposium on Information Theory
  (ISIT) 2014</comments><doi>10.1109/ISIT.2014.6874806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we close the gap between end-to-end diversity coding and
intra-session network coding for unicast connections resilient against single
link failures. In particular, we show that coding operations are sufficient to
perform at the source and receiver if the user data can be split into at most
two parts over the filed GF(2). Our proof is purely combinatorial and based on
standard graph and network flow techniques. It is a linear time construction
that defines the route of subflows A, B and A+B between the source and
destination nodes. The proposed resilient flow decomposition method generalizes
the 1+1 protection and the end-to-end diversity coding approaches while keeping
both of their benefits. It provides a simple yet resource efficient protection
method feasible in 2-connected backbone topologies. Since the core switches do
not need to be modified, this result can bring benefits to current transport
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6675</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6675</id><created>2014-01-26</created><updated>2014-05-15</updated><authors><author><keyname>Bonchi</keyname><forenames>Filippo</forenames><affiliation>LIP</affiliation></author><author><keyname>Petrisan</keyname><forenames>Daniela</forenames><affiliation>LIP</affiliation></author><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIP</affiliation></author><author><keyname>Rot</keyname><forenames>Jurriaan</forenames><affiliation>LIACS</affiliation></author></authors><title>Coinduction up to in a fibrational setting</title><categories>cs.LO cs.DM</categories><proxy>ccsd</proxy><journal-ref>CSL-LICS, Vienne : France (2014)</journal-ref><doi>10.1145/2603088.2603149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bisimulation up-to enhances the coinductive proof method for bisimilarity,
providing efficient proof techniques for checking properties of different kinds
of systems. We prove the soundness of such techniques in a fibrational setting,
building on the seminal work of Hermida and Jacobs. This allows us to
systematically obtain up-to techniques not only for bisimilarity but for a
large class of coinductive predicates modelled as coalgebras. By tuning the
parameters of our framework, we obtain novel techniques for unary predicates
and nominal automata, a variant of the GSOS rule format for similarity, and a
new categorical treatment of weak bisimilarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6679</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6679</id><created>2014-01-26</created><authors><author><keyname>Jeansoulin</keyname><forenames>Robert</forenames></author><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>Quality of Geographic Information: Ontological approach and Artificial
  Intelligence Tools</title><categories>cs.AI cs.HC</categories><comments>12 pages, 8th EC-GIS Workshop (European Commission), Dublin, Ireland,
  July, 3-5, 2002</comments><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective is to present one important aspect of the European IST-FET
project &quot;REV!GIS&quot;1: the methodology which has been developed for the
translation (interpretation) of the quality of the data into a &quot;fitness for
use&quot; information, that we can confront to the user needs in its application.
This methodology is based upon the notion of &quot;ontologies&quot; as a conceptual
framework able to capture the explicit and implicit knowledge involved in the
application. We do not address the general problem of formalizing such
ontologies, instead, we rather try to illustrate this with three applications
which are particular cases of the more general &quot;data fusion&quot; problem. In each
application, we show how to deploy our methodology, by comparing several
possible solutions, and we try to enlighten where are the quality issues, and
what kind of solution to privilege, even at the expense of a highly complex
computational approach. The expectation of the REV!GIS project is that
computationally tractable solutions will be available among the next generation
AI tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6681</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6681</id><created>2014-01-26</created><updated>2014-02-06</updated><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author><author><keyname>Hermon</keyname><forenames>Jonathan</forenames></author><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author></authors><title>On giant components and treewidth in the layers model</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected $n$-vertex graph $G(V,E)$ and an integer $k$, let
$T_k(G)$ denote the random vertex induced subgraph of $G$ generated by ordering
$V$ according to a random permutation $\pi$ and including in $T_k(G)$ those
vertices with at most $k-1$ of their neighbors preceding them in this order.
The distribution of subgraphs sampled in this manner is called the \emph{layers
model with parameter} $k$. The layers model has found applications in studying
$\ell$-degenerate subgraphs, the design of algorithms for the maximum
independent set problem, and in bootstrap percolation.
  In the current work we expand the study of structural properties of the
layers model.
  We prove that there are $3$-regular graphs $G$ for which with high
probability $T_3(G)$ has a connected component of size $\Omega(n)$. Moreover,
this connected component has treewidth $\Omega(n)$. This lower bound on the
treewidth extends to many other random graph models. In contrast, $T_2(G)$ is
known to be a forest (hence of treewidth~1), and we establish that if $G$ is of
bounded degree then with high probability the largest connected component in
$T_2(G)$ is of size $O(\log n)$. We also consider the infinite two-dimensional
grid, for which we prove that the first four layers contain a unique infinite
connected component with probability $1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6683</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6683</id><created>2014-01-26</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Resource Allocation Under Channel Uncertainties for Relay-Aided
  Device-to-Device Communication Underlaying LTE-A Cellular Networks</title><categories>cs.NI cs.IT math.IT math.OC</categories><comments>IEEE Transactions on Wireless Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication in cellular networks allows direct
transmission between two cellular devices with local communication needs. Due
to the increasing number of autonomous heterogeneous devices in future mobile
networks, an efficient resource allocation scheme is required to maximize
network throughput and achieve higher spectral efficiency. In this paper,
performance of network-integrated D2D communication under channel uncertainties
is investigated where D2D traffic is carried through relay nodes. Considering a
multi-user and multi-relay network, we propose a robust distributed solution
for resource allocation with a view to maximizing network sum-rate when the
interference from other relay nodes and the link gains are uncertain. An
optimization problem is formulated for allocating radio resources at the relays
to maximize end-to-end rate as well as satisfy the quality-of-service (QoS)
requirements for cellular and D2D user equipments under total power constraint.
Each of the uncertain parameters is modeled by a bounded distance between its
estimated and bounded values. We show that the robust problem is convex and a
gradient-aided dual decomposition algorithm is applied to allocate radio
resources in a distributed manner. Finally, to reduce the cost of robustness
defined as the reduction of achievable sum-rate, we utilize the \textit{chance
constraint approach} to achieve a trade-off between robustness and optimality.
The numerical results show that there is a distance threshold beyond which
relay-aided D2D communication significantly improves network performance when
compared to direct communication between D2D peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6686</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6686</id><created>2014-01-26</created><updated>2015-02-02</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Perturbed Message Passing for Constraint Satisfaction Problems</title><categories>cs.AI cs.CC stat.ML</categories><journal-ref>JMLR 16(Jul):1249-1274, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an efficient message passing scheme for solving Constraint
Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief
Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and
directly produce a single satisfying assignment. Our first CSP solver, called
Perturbed Blief Propagation, smoothly interpolates two well-known inference
procedures; it starts as BP and ends as a Gibbs sampler, which produces a
single sample from the set of solutions. Moreover we apply a similar
perturbation scheme to SP to produce another CSP solver, Perturbed Survey
Propagation. Experimental results on random and real-world CSPs show that
Perturbed BP is often more successful and at the same time tens to hundreds of
times more efficient than standard BP guided decimation. Perturbed BP also
compares favorably with state-of-the-art SP-guided decimation, which has a
computational complexity that generally scales exponentially worse than our
method (wrt the cardinality of variable domains and constraints). Furthermore,
our experiments with random satisfiability and coloring problems demonstrate
that Perturbed SP can outperform SP-guided decimation, making it the best
incomplete random CSP-solver in difficult regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6690</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6690</id><created>2014-01-26</created><updated>2015-05-08</updated><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Spatial DCT-Based Channel Estimation in Multi-Antenna Multi-Cell
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>Submitted for possible publication. arXiv admin note: text overlap
  with arXiv:1203.5924 by other authors</comments><journal-ref>IEEE Transactions on Signal Processing, (Volume:63 , Issue: 6 ),
  pp.1404 - 1418, March 2015</journal-ref><doi>10.1109/TSP.2015.2393844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses channel estimation in multiple antenna multicell
interference-limited networks. Channel state information (CSI) acquisition is
vital for interference mitigation. Wireless networks often suffer from
multicell interference, which can be mitigated by deploying beamforming to
spatially direct the transmissions. The accuracy of the estimated CSI plays an
important role in designing accurate beamformers that can control the amount of
interference created from simultaneous spatial transmissions to mobile users.
Therefore, a new technique based on the structure of the spatial covariance
matrix and the discrete cosine transform (DCT) is proposed to enhance channel
estimation in the presence of interference. Bayesian estimation and Least
Squares estimation frameworks are introduced by utilizing the DCT to separate
the overlapping spatial paths that create the interference. The spatial domain
is thus exploited to mitigate the contamination which is able to discriminate
across interfering users. Gains over conventional channel estimation techniques
are presented in our simulations which are also valid for a small number of
antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6694</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6694</id><created>2014-01-26</created><updated>2014-05-02</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Multivariate sparse interpolation using randomized Kronecker
  substitutions</title><categories>cs.SC cs.DS cs.MS</categories><comments>21 pages, 2 tables, 1 procedure. Accepted to ISSAC 2014</comments><msc-class>68W30</msc-class><acm-class>F.2.1; G.4; I.1.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present new techniques for reducing a multivariate sparse polynomial to a
univariate polynomial. The reduction works similarly to the classical and
widely-used Kronecker substitution, except that we choose the degrees randomly
based on the number of nonzero terms in the multivariate polynomial, that is,
its sparsity. The resulting univariate polynomial often has a significantly
lower degree than the Kronecker substitution polynomial, at the expense of a
small number of term collisions. As an application, we give a new algorithm for
multivariate interpolation which uses these new techniques along with any
existing univariate interpolation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6697</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6697</id><created>2014-01-26</created><updated>2014-11-16</updated><authors><author><keyname>Borodin</keyname><forenames>Allan</forenames></author><author><keyname>Le</keyname><forenames>Dai Tri Man</forenames></author><author><keyname>Ye</keyname><forenames>Yuli</forenames></author></authors><title>Weakly Submodular Functions</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular functions are well-studied in combinatorial optimization, game
theory and economics. The natural diminishing returns property makes them
suitable for many applications. We study an extension of monotone submodular
functions, which we call {\em weakly submodular functions}. Our extension
includes some (mildly) supermodular functions. We show that several natural
functions belong to this class and relate our class to some other recent
submodular function extensions.
  We consider the optimization problem of maximizing a weakly submodular
function subject to uniform and general matroid constraints. For a uniform
matroid constraint, the &quot;standard greedy algorithm&quot; achieves a constant
approximation ratio where the constant (experimentally) converges to 5.95 as
the cardinality constraint increases. For a general matroid constraint, a
simple local search algorithm achieves a constant approximation ratio where the
constant (analytically) converges to 10.22 as the rank of the matroid
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6702</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6702</id><created>2014-01-26</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>How to Run a Campaign: Optimal Control of SIS and SIR Information
  Epidemics</title><categories>cs.SY cs.SI math.OC</categories><comments>Proofs for Theorems 4.2 and 5.2 which do not appear in the published
  journal version are included in this version. Published version can be
  accessed here: http://dx.doi.org/10.1016/j.amc.2013.12.164</comments><journal-ref>Applied Mathematics and Computation, Vol 231, March 2014, pages
  79-92</journal-ref><doi>10.1016/j.amc.2013.12.164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information spreading in a population can be modeled as an epidemic.
Campaigners (e.g. election campaign managers, companies marketing products or
movies) are interested in spreading a message by a given deadline, using
limited resources. In this paper, we formulate the above situation as an
optimal control problem and the solution (using Pontryagin's Maximum Principle)
prescribes an optimal resource allocation over the time of the campaign. We
consider two different scenarios --- in the first, the campaigner can adjust a
direct control (over time) which allows her to recruit individuals from the
population (at some cost) to act as spreaders for the
Susceptible-Infected-Susceptible (SIS) epidemic model. In the second case, we
allow the campaigner to adjust the effective spreading rate by incentivizing
the infected in the Susceptible-Infected-Recovered (SIR) model, in addition to
the direct recruitment. We consider time varying information spreading rate in
our formulation to model the changing interest level of individuals in the
campaign, as the deadline is reached. In both the cases, we show the existence
of a solution and its uniqueness for sufficiently small campaign deadlines. For
the fixed spreading rate, we show the effectiveness of the optimal control
strategy against the constant control strategy, a heuristic control strategy
and no control. We show the sensitivity of the optimal control to the spreading
rate profile when it is time varying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6706</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6706</id><created>2014-01-26</created><updated>2014-03-26</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Information Processing Structure of Quantum Gravity</title><categories>quant-ph cs.IT gr-qc hep-th math.IT</categories><comments>30 pages, 6 figures, v2: notations harmonized</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of quantum gravity is aimed to fuse general relativity with
quantum theory into a more fundamental framework. The space of quantum gravity
provides both the non-fixed causality of general relativity and the quantum
uncertainty of quantum mechanics. In a quantum gravity scenario, the causal
structure is indefinite and the processes are causally non-separable. In this
work, we provide a model for the information processing structure of quantum
gravity. We show that the quantum gravity environment is an information
resource-pool from which valuable information can be extracted. We analyze the
structure of the quantum gravity space and the entanglement of the space-time
geometry. We study the information transfer capabilities of quantum gravity
space and define the quantum gravity channel. We reveal that the quantum
gravity space acts as a background noise on the local environment states. We
characterize the properties of the noise of the quantum gravity space and show
that it allows the separate local parties to simulate remote outputs from the
local environment state, through the process of remote simulation. We
characterize the information transfer of the gravity space and the correlation
measure functions of the gravity channel. We investigate the process of
stimulated storage for quantum gravity memories, a phenomenon that exploits the
information resource-pool property of quantum gravity. The results confirm the
perception that the benefits of the quantum gravity space can be exploited in
quantum computations, particularly in the development of quantum computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6716</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6716</id><created>2014-01-26</created><authors><author><keyname>Beatty</keyname><forenames>Ian D.</forenames></author></authors><title>Gaming the System: Video Games as a Theoretical Framework for
  Instructional Design</title><categories>physics.ed-ph cs.CY</categories><comments>Proposed for a conference paper presentation at the International
  Conference of the Learning Sciences 2014 (Boulder, CO)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to facilitate analyzing video games as learning systems and
instructional designs as games, we present a theoretical framework that
integrates ideas from a broad range of literature. The framework describes
games in terms of four layers, all sharing similar structural elements and
dynamics: a micro-level game focused on immediate problem-solving and skill
development, a macro-level game focused on the experience of the game world and
story and identity development, and two meta-level games focused on building or
modifying the game and on social interactions around it. Each layer casts
gameplay as a co-construction of the game and the player, and contains three
dynamical feedback loops: an exploratory learning loop, an intrinsic motivation
loop, and an identity loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6720</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6720</id><created>2014-01-26</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author></authors><title>Improve the Sustainability of Internet of Things Through Trading-based
  Value Creation</title><categories>cs.CY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.8198</comments><journal-ref>Proceedings of the IEEE World Forum on Internet of Things
  (WF-IoT), Seoul, Korea, March, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) has been widely discussed over the past few years in
technology point of view. However, the social aspects of IoT are seldom studied
to date. In this paper, we discuss the IoT in social point of view.
Specifically, we examine the strategies to increase the adoption of IoT in a
sustainable manner. Such discussion is essential in today's context where
adoption of IoT solutions by non-technical community is slow. Specially, large
number of IoT solutions making their way into the market every day. We propose
an trading-based value creation model based on sensing as a service paradigm in
order to fuel the adoption of IoT. We discuss the value creation and its impact
towards the society especially to households and their occupants. We also
present results of two different surveys we conducted in order to examine the
potential acceptance of the proposed model among the general public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6726</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6726</id><created>2014-01-26</created><authors><author><keyname>Fernandes</keyname><forenames>Earlence</forenames></author><author><keyname>Crowell</keyname><forenames>Alexander</forenames></author><author><keyname>Aluri</keyname><forenames>Ajit</forenames></author><author><keyname>Prakash</keyname><forenames>Atul</forenames></author></authors><title>Anception: Application Virtualization For Android</title><categories>cs.CR cs.OS</categories><comments>University of Michigan, Technical Report CSE-TR-583-13</comments><report-no>CSE-TR-583-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of malware has become significant on Android devices. Library
operating systems and application virtualization are both possible solutions
for confining malware. Unfortunately, such solutions do not exist for Android.
Designing mechanisms for application virtualization is a significant chal-
lenge for several reasons: (1) graphics performance is important due to
popularity of games and (2) applications with the same UID can share state.
This paper presents Anception, the first flexible application virtualization
framework for Android. It is imple- mented as a modification to the Android
kernel and supports application virtualization that addresses the above
requirements. Anception is able to confine many types of malware while
supporting unmodified Android applications. Our Anception- based system
exhibits up to 3.9% overhead on various 2D/3D benchmarks, and 1.8% overhead on
the SunSpider benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6728</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6728</id><created>2014-01-26</created><updated>2014-12-12</updated><authors><author><keyname>Jeon</keyname><forenames>Junekey</forenames></author></authors><title>A Generalized Typicality for Abstract Alphabets</title><categories>cs.IT math.IT</categories><comments>44 pages; submitted to IEEE Transactions on Information Theory</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new notion of typicality for arbitrary probability measures on standard
Borel spaces is proposed, which encompasses the classical notions of weak and
strong typicality as special cases. Useful lemmas about strong typical sets,
including conditional typicality lemma, joint typicality lemma, and packing and
covering lemmas, which are fundamental tools for deriving many inner bounds of
various multi-terminal coding problems, are obtained in terms of the proposed
notion. This enables us to directly generalize lots of results on finite
alphabet problems to general problems involving abstract alphabets, without any
complicated additional arguments. For instance, quantization procedure is no
longer necessary to achieve such generalizations. Another fundamental lemma,
Markov lemma, is also obtained but its scope of application is quite limited
compared to others. Yet, an alternative theory of typical sets for Gaussian
measures, free from this limitation, is also developed. Some remarks on a
possibility to generalize the proposed notion for sources with memory are also
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6733</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6733</id><created>2014-01-26</created><authors><author><keyname>Mehrle</keyname><forenames>David</forenames></author><author><keyname>Strosser</keyname><forenames>Amy</forenames></author><author><keyname>Harkin</keyname><forenames>Anthony</forenames></author></authors><title>Walk modularity and community structure in networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>13 pages, 6 figures, 1 table</comments><doi>10.1017/nws.2015.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity maximization has been one of the most widely used approaches in
the last decade for discovering community structure in networks of practical
interest in biology, computing, social science, statistical mechanics, and
more. Modularity is a quality function that measures the difference between the
number of edges found within clusters minus the number of edges one would
statistically expect to find based on random chance. We present a natural
generalization of modularity based on the difference between the actual and
expected number of walks within clusters, which we call walk-modularity.
Walk-modularity can be expressed in matrix form, and community detection can be
performed by finding leading eigenvectors of the walk-modularity matrix. We
demonstrate community detection on both synthetic and real-world networks and
find that walk-modularity maximization returns significantly improved results
compared to traditional modularity maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6734</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6734</id><created>2014-01-26</created><updated>2015-05-10</updated><authors><author><keyname>Conlon</keyname><forenames>David</forenames></author><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Gasarch</keyname><forenames>William</forenames></author><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Ulrich</keyname><forenames>Douglas</forenames></author><author><keyname>Zbarsky</keyname><forenames>Samuel</forenames></author></authors><title>Distinct volume subsets</title><categories>math.CO cs.CG math.MG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that $a$ and $d$ are positive integers with $a \geq 2$. Let
$h_{a,d}(n)$ be the largest integer $t$ such that any set of $n$ points in
$\mathbb{R}^d$ contains a subset of $t$ points for which all the non-zero
volumes of the ${t \choose a}$ subsets of order $a$ are distinct. Beginning
with Erd\H{o}s in 1957, the function $h_{2,d}(n)$ has been closely studied and
is known to be at least a power of $n$. We improve the best known bound for
$h_{2,d}(n)$ and show that $h_{a,d}(n)$ is at least a power of $n$ for all $a$
and $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6736</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6736</id><created>2014-01-26</created><authors><author><keyname>Tadayon</keyname><forenames>Navid</forenames></author><author><keyname>Aissa</keyname><forenames>Sonia</forenames></author></authors><title>Multi-Channel Cognitive Radio Networks: Modeling, Analysis and Synthesis</title><categories>cs.NI math.PR</categories><comments>Accepted in IEEE Journal on Selected Area in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we establish a model for multichannel cognitive radio
networks (CRNs) using the theory of priority queues. This model enables us to
conduct a performance analysis in the most general form by the derivation of
the probability mass function (PMF) of queue length at the secondary users
(SUs). In the second part, a reverse problem is considered to answer the
important top-down question of whether a service requirement can be satisfied
in a multi-channel CRN knowing the network parameters and traffic situation
with respect to the SUs and the primary users (PUs). Terming this problem as
the network synthesis, a precise conservation law is obtained, which relates
the packet waiting times of both types of users, and based on which the
achievable region of the network is also determined. Lastly, by the
introduction of a mixed strategy, the conditions for the existence of an
optimal trade-off between the interference onto the PUs and the
quality-of-service of the SUs is shown, and the optimal mixed strategy is
obtained when those conditions are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6738</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6738</id><created>2014-01-26</created><authors><author><keyname>Kim</keyname><forenames>Hyeji</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Capacity Region of the Broadcast Channel with Two Deterministic Channel
  State Components</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. Submitted to ISIT 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper establishes the capacity region of a class of broadcast channels
with random state in which each channel component is selected from two possible
functions and each receiver knows its state sequence. This channel model does
not fit into any class of broadcast channels for which the capacity region was
previously known and is useful in studying wireless communication channels when
the fading state is known only at the receivers. The capacity region is shown
to coincide with the UV outer bound and is achieved via Marton coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6757</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6757</id><created>2014-01-27</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author></authors><title>A Linear-Time Algorithm for Trust Region Problems</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental problem of maximizing a general quadratic
function over an ellipsoidal domain, also known as the trust region problem. We
give the first provable linear-time (in the number of non-zero entries of the
input) algorithm for approximately solving this problem. Specifically, our
algorithm returns an $\epsilon$-approximate solution in time
$\tilde{O}(N/\sqrt{\epsilon})$, where $N$ is the number of non-zero entries in
the input. This matches the runtime of Nesterov's accelerated gradient descent,
suitable for the special case in which the quadratic function is concave, and
the runtime of the Lanczos method which is applicable when the problem is
purely quadratic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6759</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6759</id><created>2014-01-27</created><authors><author><keyname>Benmehidi</keyname><forenames>Nadia Otmani</forenames></author><author><keyname>Arar</keyname><forenames>Meriem</forenames></author><author><keyname>Chine</keyname><forenames>Imene</forenames></author></authors><title>Modeling the behavior of reinforced concrete walls under fire,
  considering the impact of the span on firewalls</title><categories>cs.CE</categories><comments>8 pages,12 figures, 4 tables</comments><journal-ref>International Journal of Soft Computing And Software Engineering
  (JSCSE), Vol.3,No.3, pp. 600-607, 2013</journal-ref><doi>10.7321/jscse.v3.n3.91</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical modeling using computers is known to present several advantages
compared to experimental testing. The high cost and the amount of time required
to prepare and to perform a test were among the main problems on the table when
the first tools for modeling structures in fire were developed. The discipline
structures-in-fire modeling is still currently the subject of important
research efforts around the word, those research efforts led to develop many
software. In this paper, our task is oriented to the study of fire behavior and
the impact of the span reinforced concrete walls with different sections
belonging to a residential building braced by a system composed of porticoes
and sails. Regarding the design and mechanical loading (compression forces and
moments) exerted on the walls in question, we are based on the results of a
study conducted at cold. We use on this subject the software Safir witch obeys
to the Eurocode laws, to realize this study. It was found that loading,
heating, and sizing play a capital role in the state of failed walls. Our
results justify well the use of reinforced concrete walls, acting as a
firewall. Their role is to limit the spread of fire from one structure to
another structure nearby, since we get fire resistance reaching more than 10
hours depending on the loading considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6773</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6773</id><created>2014-01-27</created><authors><author><keyname>Aboua&#xef;ssa</keyname><forenames>Hassane</forenames></author><author><keyname>Kubera</keyname><forenames>Yoann</forenames></author><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author></authors><title>Dynamic Hybrid Traffic Flow Modeling</title><categories>cs.MA</categories><comments>This paper reports the works conducted in 2013 during the phase 5 of
  CISIT within the ISART project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A flow of moving agents can be observed at different scales. Thus, in traffic
modeling, three levels are generally considered: the micro, meso and macro
levels, representing respectively the interactions between vehicles, groups of
vehicles sharing common properties (such as a common destination or a common
localization) and flows of vehicles. Each approach is useful in a given
context: micro and meso models allow to simulate road networks with complex
topologies such as urban area, while macro models allow to develop control
strategies to prevent congestion in highways. However, to simulate large-scale
road networks, it can be interesting to integrate different representations,
e.g., micro and macro, in a single model. Existing models share the same
limitation: connections between levels are fixed a priori and cannot be changed
at runtime. Therefore, to be able to observe some emerging phenomena such as
congestion formation or to find the exact location of a jam in a large macro
section, a dynamic hybrid modeling approach is needed. In 2013 we started the
development of a multi-level agent-based simulator called JAM-FREE within the
ISART project. It allows to simulate large road networks efficiently using a
dynamic level of detail. This simulator relies on a multi-level agent-based
modeling framework called SIMILAR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6775</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6775</id><created>2014-01-27</created><authors><author><keyname>Hoy</keyname><forenames>Michael</forenames></author></authors><title>Methods for Collision-Free Navigation of Multiple Mobile Robots in
  Unknown Cluttered Environments</title><categories>math.OC cs.RO</categories><comments>308 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigation and guidance of autonomous vehicles is a fundamental problem in
robotics, which has attracted intensive research in recent decades. This report
is mainly concerned with provable collision avoidance of multiple autonomous
vehicles operating in unknown cluttered environments, using reactive
decentralized navigation laws, where obstacle information is supplied by some
sensor system.
  Recently, robust and decentralized variants of model predictive control based
navigation systems have been applied to vehicle navigation problems. Properties
such as provable collision avoidance under disturbance and provable convergence
to a target have been shown; however these often require significant
computational and communicative capabilities, and don't consider sensor
constraints, making real time use somewhat difficult. There also seems to be
opportunity to develop a better trade-off between tractability, optimality, and
robustness.
  The main contributions of this work are as follows; firstly, the integration
of the robust model predictive control concept with reactive navigation
strategies based on local path planning, which is applied to both holonomic and
unicycle vehicle models subjected to acceleration bounds and disturbance;
secondly, the extension of model predictive control type methods to situations
where the information about the obstacle is limited to a discrete ray-based
sensor model, for which provably safe, convergent boundary following can be
shown; and thirdly the development of novel constraints allowing decentralized
coordination of multiple vehicles using a robust model predictive control type
approach, where a single communication exchange is used per control update,
vehicles are allowed to perform planning simultaneously, and coherency
objectives are avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6785</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6785</id><created>2014-01-27</created><updated>2014-09-25</updated><authors><author><keyname>Lazi&#x107;</keyname><forenames>Ranko</forenames></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Non-Elementary Complexities for Branching VASS, MELL, and Extensions</title><categories>cs.LO</categories><comments>32 pages, revised after its presentation at CSL-LICS 2014</comments><acm-class>F.2.2; F.4.1</acm-class><journal-ref>ACM Transactions on Computational Logic, vol. 16, issue 3, article
  20, 2015</journal-ref><doi>10.1145/2733375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of reachability problems on branching extensions of
vector addition systems, which allows us to derive new non-elementary
complexity bounds for fragments and variants of propositional linear logic. We
show that provability in the multiplicative exponential fragment is Tower-hard
already in the affine case---and hence non-elementary. We match this lower
bound for the full propositional affine linear logic, proving its
Tower-completeness. We also show that provability in propositional contractive
linear logic is Ackermann-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6787</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6787</id><created>2014-01-27</created><updated>2014-04-25</updated><authors><author><keyname>Koch</keyname><forenames>Tobias</forenames></author></authors><title>On the capacity of the dither-quantized Gaussian channel</title><categories>cs.IT math.IT</categories><comments>22 pages, 1 figure. Part of this work will be presented at ISIT 2014.
  Corrected minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the capacity of the peak-and-average-power-limited
Gaussian channel when its output is quantized using a dithered, infinite-level,
uniform quantizer of step size $\Delta$. It is shown that the capacity of this
channel tends to that of the unquantized Gaussian channel when $\Delta$ tends
to zero, and it tends to zero when $\Delta$ tends to infinity. In the low
signal-to-noise ratio (SNR) regime, it is shown that, when the peak-power
constraint is absent, the low-SNR asymptotic capacity is equal to that of the
unquantized channel irrespective of $\Delta$. Furthermore, an expression for
the low-SNR asymptotic capacity for finite peak-to-average-power ratios is
given and evaluated in the low- and high-resolution limit. It is demonstrated
that, in this case, the low-SNR asymptotic capacity converges to that of the
unquantized channel when $\Delta$ tends to zero, and it tends to zero when
$\Delta$ tends to infinity. Comparing these results with achievability results
for (undithered) 1-bit quantization, it is observed that the dither reduces
capacity in the low-precision limit, and it reduces the low-SNR asymptotic
capacity unless the peak-to-average-power ratio is unbounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6790</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6790</id><created>2014-01-27</created><authors><author><keyname>Chorti</keyname><forenames>Arsenia</forenames></author><author><keyname>Papadaki</keyname><forenames>Katerina</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Optimal Power Allocation in Block Fading Gaussian Channels with Causal
  CSI and Secrecy Constraints</title><categories>cs.IT cs.CR math.IT</categories><comments>submitted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal power allocation that maximizes the secrecy capacity of block
fading Gaussian (BF-Gaussian) networks with causal channel state information
(CSI), M-block delay tolerance and a frame based power constraint is examined.
In particular, we formulate the secrecy capacity maximization as a dynamic
program. We propose suitable linear approximations of the secrecy capacity
density in the low SNR, the high SNR and the intermediate SNR regimes,
according to the overall available power budget. Our findings indicate that
when the available power resources are very low (low SNR case) the optimal
strategy is a threshold policy. On the other hand when the available power
budget is infinite (high SNR case) a constant power policy maximizes the frame
secrecy capacity. Finally, when the power budget is finite (medium SNR case),
an approximate tractable power allocation policy is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6799</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6799</id><created>2014-01-27</created><authors><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author><author><keyname>Crnojevic</keyname><forenames>Vladimir</forenames></author></authors><title>Slotted Aloha for Networked Base Stations</title><categories>cs.IT math.IT</categories><comments>conference; submitted on Dec 15, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multiple base station, multi-access systems in which the user-base
station adjacency is induced by geographical proximity. At each slot, each user
transmits (is active) with a certain probability, independently of other users,
and is heard by all base stations within the distance $r$. Both the users and
base stations are placed uniformly at random over the (unit) area. We first
consider a non-cooperative decoding where base stations work in isolation, but
a user is decoded as soon as one of its nearby base stations reads a clean
signal from it. We find the decoding probability and quantify the gains
introduced by multiple base stations. Specifically, the peak throughput
increases linearly with the number of base stations $m$ and is roughly $m/4$
larger than the throughput of a single-base station that uses standard slotted
Aloha. Next, we propose a cooperative decoding, where the mutually close base
stations inform each other whenever they decode a user inside their coverage
overlap. At each base station, the messages received from the nearby stations
help resolve collisions by the interference cancellation mechanism. Building
from our exact formulas for the non-cooperative case, we provide a heuristic
formula for the cooperative decoding probability that reflects well the actual
performance. Finally, we demonstrate by simulation significant gains of
cooperation with respect to the non-cooperative decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6803</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6803</id><created>2014-01-27</created><updated>2014-10-13</updated><authors><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Malone</keyname><forenames>David</forenames></author></authors><title>On Efficiency and Validity of Previous Homeplug MAC Performance Analysis</title><categories>cs.NI</categories><doi>10.1016/j.comnet.2015.03.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Medium Access Control protocol of Power Line Communication networks
(defined in Homeplug and IEEE 1901 standards) has received relatively modest
attention from the research community. As a consequence, there is only one
analytic model that complies with the standardised MAC procedures and considers
unsaturated conditions. We identify two important limitations of the existing
analytic model: high computational expense and predicted results just prior to
the predicted saturation point do not correspond to long-term network
performance. In this work, we present a simplification of the previously
defined analytic model of Homeplug MAC able to substantially reduce its
complexity and demonstrate that the previous performance results just before
predicted saturation correspond to a transitory phase. We determine that the
causes of previous misprediction are common analytical assumptions and the
potential occurrence of a transitory phase, that we show to be of extremely
long duration under certain circumstances. We also provide techniques, both
analytical and experimental, to correctly predict long-term behaviour and
analyse the effect of specific Homeplug/IEEE 1901 features on the magnitude of
misprediction errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6810</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6810</id><created>2014-01-27</created><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author><author><keyname>Crnojevic</keyname><forenames>Vladimir</forenames></author></authors><title>Slotted Aloha for Networked Base Stations with Spatial and Temporal
  Diversity</title><categories>cs.IT math.IT</categories><comments>extended version of a conference paper submitted on Jan 24, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider framed slotted Aloha where $m$ base stations cooperate to decode
messages from $n$ users. Users and base stations are placed uniformly at random
over an area. At each frame, each user sends multiple replicas of its packet
according to a prescribed distribution, and it is heard by all base stations
within the communication radius $r$. Base stations employ a decoding algorithm
that utilizes the successive interference cancellation mechanism, both in
space--across neighboring base stations, and in time--across different slots,
locally at each base station. We show that there exists a threshold on the
normalized load $G=n/(\tau m)$, where $\tau$ is the number of slots per frame,
below which decoding probability converges asymptotically (as
$n,m,\tau\rightarrow \infty$, $r\rightarrow 0$) to the maximal possible
value--the probability that a user is heard by at least one base station, and
we find a lower bound on the threshold. Further, we give a heuristic evaluation
of the decoding probability based on the and-or-tree analysis. Finally, we show
that the peak throughput increases linearly in the number of base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6835</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6835</id><created>2014-01-27</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM, IMJ</affiliation></author><author><keyname>Skrzypczak</keyname><forenames>Micha&#x142;</forenames></author></authors><title>On the Topological Complexity of omega-Languages of Non-Deterministic
  Petri Nets</title><categories>cs.LO cs.CC cs.FL</categories><proxy>ccsd</proxy><journal-ref>Information Processing Letters 114, 5 (2014) 229-233</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are $\Sigma_3^0$-complete languages of infinite words
accepted by non-deterministic Petri nets with B\&quot;uchi acceptance condition, or
equivalently by B\&quot;uchi blind counter automata. This shows that omega-languages
accepted by non-deterministic Petri nets are topologically more complex than
those accepted by deterministic Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1401.6840</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1401.6840</id><created>2014-01-27</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Ku&#x10d;era</keyname><forenames>Anton&#xed;n</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>Zero-Reachability in Probabilistic Multi-Counter Automata</title><categories>cs.FL</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the qualitative and quantitative zero-reachability problem in
probabilistic multi-counter systems. We identify the undecidable variants of
the problems, and then we concentrate on the remaining two cases. In the first
case, when we are interested in the probability of all runs that visit zero in
some counter, we show that the qualitative zero-reachability is decidable in
time which is polynomial in the size of a given pMC and doubly exponential in
the number of counters. Further, we show that the probability of all
zero-reaching runs can be effectively approximated up to an arbitrarily small
given error epsilon &gt; 0 in time which is polynomial in log(epsilon),
exponential in the size of a given pMC, and doubly exponential in the number of
counters. In the second case, we are interested in the probability of all runs
that visit zero in some counter different from the last counter. Here we show
that the qualitative zero-reachability is decidable and SquareRootSum-hard, and
the probability of all zero-reaching runs can be effectively approximated up to
an arbitrarily small given error epsilon &gt; 0 (these result applies to pMC
satisfying a suitable technical condition that can be verified in polynomial
time). The proof techniques invented in the second case allow to construct
counterexamples for some classical results about ergodicity in stochastic Petri
nets.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="55000" completeListSize="102538">1122234|56001</resumptionToken>
</ListRecords>
</OAI-PMH>
